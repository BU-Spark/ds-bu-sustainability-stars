Author,Title,Abstract
ANA FISZBEIN,Widespread occurrence of hybrid internal-terminal exons in human transcriptomes,"Alternative RNA processing is a major mechanism for diversifying the human transcriptome. Messenger RNA isoform differences are predominantly driven by alternative first exons, cassette internal exons and alternative last exons. Despite the importance of classifying exons to understand isoform structure, there is a lack of tools to look at isoform-specific exon usage using RNA-sequencing data. We recently observed that alternative transcription start sites often arise near annotated internal exons, creating “hybrid” exons that can be used as both first or internal exons. To investigate the creation of hybrid exons, we built the HIT (Hybrid-Internal-Terminal) exon pipeline that systematically classifies exons depending on their isoform-specific usage. Using a combination of junction reads coverage and probabilistic modeling, the HIT index identified thousands of hybrid first-internal and internal-last exons that were previously misclassified. Hybrid exons are enriched in long genes with at least ten internal exons, have longer flanking introns and strong splice sites. The usage of hybrid exons varies considerably across human tissues, but they are predominantly used in brain, testis and colon cells. Notably, genes involved in RNA splicing have the highest fraction of intra-tissue hybrid exons. Further, we found more than 100,000 inter-tissue hybrid exons that changed from internal to terminal exons across tissues. By developing the first method that can classify exons according to their isoform contexts, our findings demonstrate the existence of hybrid exons, expand the repertoire of tissue-specific terminal exons and uncover unexpected complexities of the human transcriptome."
RUBEN DRIES,The MYCN 5' UTR as a therapeutic target in neuroblastoma,"Tumor MYCN amplification is seen in high-risk neuroblastoma, yet direct targeting of this oncogenic transcription factor has been challenging. Here, we take advantage of the dependence of MYCN-amplified neuroblastoma cells on increased protein synthesis to inhibit the activity of eukaryotic translation initiation factor 4A1 (eIF4A1) using an amidino-rocaglate, CMLD012824. Consistent with the role of this RNA helicase in resolving structural barriers in 5' untranslated regions (UTRs), CMLD012824 increased eIF4A1 affinity for polypurine-rich 5' UTRs, including that of the MYCN and associated transcripts with critical roles in cell proliferation. CMLD012824-mediated clamping of eIF4A1 spanned the full lengths of mRNAs, while translational inhibition was mediated through 5' UTR binding in a cap-dependent and -independent manner. Finally, CMLD012824 led to growth inhibition in MYCN-amplified neuroblastoma models without generalized toxicity. Our studies highlight the key role of eIF4A1 in MYCN-amplified neuroblastoma and demonstrate the therapeutic potential of disrupting its function."
EVA GARRETT,Evolution of the primate vomeronasal system: fossil evidence from the Fayum,"Extant primates vary dramatically in the presence and development of the vomeronasal system (VNS), which largely detects social pheromones and anti-predator chemosignals. While the strepsirrhine VNS resembles most mammals, haplorhines either have derived VNS traits with ambiguous effects on vomeronasal function, or have lost the system entirely. While a reduced reliance on vomeronasal olfaction in haplorhines is inferred, few studies have addressed VNS variation in extinct primates to examine the timing and context of the loss of this system. We have previously identified an osteological correlate of the vomeronasal organ, the vomeronasal groove (VNG), which allows us to implement a paleontological approach toward understanding primate VNS evolution. We investigated cranial material of fossil primates for the presence or absence of a VNG using microCT scans. The VNG was present in a broad temporal and taxonomic range of primate fossils, including plesiadapiforms, adapiforms, omomyoids, crown platyrrhines, stem anthropoids, and stem catarrhines. Notably the VNG persists as a relatively small gutter in the stem catarrhine Aegyptopithecus zeuxis, but is absent in advanced stem catarrhine Saadanius hijazensis, and the Miocene cercopithecoid Victoriapithecus. We estimate that VNG loss occurred between 30-28ma, based on our sample. These dates complement estimates for the accelerated rate of deleterious mutations, and loss of function, in the TRPC2 pheromone transduction gene in catarrhines between 40-25ma. Further exploration of the VNG in fossil primates will lead to a more thorough understanding of past sensory environments and their ultimate effects on sensory specializations of extant lineages."
EVA GARRETT,Fruit scent and observer colour vision shape food-selection strategies in wild capuchin monkeys,"The senses play critical roles in helping animals evaluate foods, including fruits that can change both in colour and scent during ripening to attract frugivores. Although numerous studies have assessed the impact of colour on fruit selection, comparatively little is known about fruit scent and how olfactory and visual data are integrated during foraging. We combine 25 months of behavioural data on 75 wild, white-faced capuchins (Cebus imitator) with measurements of fruit colours and scents from 18 dietary plant species. We show that frequency of fruit-directed olfactory behaviour is positively correlated with increases in the volume of fruit odours produced during ripening. Monkeys with red-green colour blindness sniffed fruits more often, indicating that increased reliance on olfaction is a behavioural strategy that mitigates decreased capacity to detect red-green colour contrast. These results demonstrate a complex interaction among fruit traits, sensory capacities and foraging strategies, which help explain variation in primate behaviour."
EVA GARRETT,Effects of agricultural transitions on the evolution of human sensory systems,"The transition from hunting and gathering to agricultural food production, beginning around 10,000 years ago, represents a dramatic shift in how people acquire and process food. These shifts to agriculture resulted in major changes in human environments and biology. For example, previous studies have identified selective effects of agriculture in many genes, such as those involved in lactose and starch metabolism, as well as immune function. In other primates and non-primate mammals, sensory systems are often tightly linked to foraging strategy, such that dietary changes are associated with changes in the genes involved in taste, olfaction, and color vision. In this study, we investigated how the shift to agriculture, a major change in foraging strategy, influenced the evolution of human sensory genes. We used targeted capture methods to sequence 898 genes (encoding taste receptors, olfactory receptors, and cone photopigments) and 71 neutral intergenic regions in 165 individuals from two distinct geographic regions: Uganda and the Philippines. In each region, we sampled two hunter-gatherer populations and a neighboring agricultural population, thus allowing us to compare sensory genes between hunter-gatherer and paired agricultural populations across two independent transitions to agriculture. We employed allele frequency-based tests (FST, population branch statistics, and bayenv2) to identify candidate functional variants across sensory genes that may reflect subsistence strategy adaptations. Preliminary results suggest that subsistence strategy influenced subtle shifts of allele frequencies in functional variants in at least two bitter taste receptor genes and thirteen olfactory receptor genes in populations from both Uganda and the Philippines."
EVA GARRETT,Human subsistence and signatures of selection on chemosensory genes,"Chemosensation (olfaction, taste) is essential for detecting and assessing foods, such that dietary shifts elicit evolutionary changes in vertebrate chemosensory genes. The transition from hunting and gathering to agriculture dramatically altered how humans acquire food. Recent genetic and linguistic studies suggest agriculture may have precipitated olfactory degeneration. Here, we explore the effects of subsistence behaviors on olfactory (OR) and taste (TASR) receptor genes among rainforest foragers and neighboring agriculturalists in Africa and Southeast Asia. We analyze 378 functional OR and 26 functional TASR genes in 133 individuals across populations in Uganda (Twa, Sua, BaKiga) and the Philippines (Agta, Mamanwa, Manobo) with differing subsistence histories. We find no evidence of relaxed selection on chemosensory genes in agricultural populations. However, we identify subsistence-related signatures of local adaptation on chemosensory genes within each geographic region. Our results highlight the importance of culture, subsistence economy, and drift in human chemosensory perception."
CHRISTINA LAM,"The L 98-59 system: three transiting, terrestrial-size planets orbiting a nearby M dwarf","We report the Transiting Exoplanet Survey Satellite (TESS) discovery of three terrestrial-size planets transiting L 98-59 (TOI-175, TIC 307210830)—a bright M dwarf at a distance of 10.6 pc. Using the Gaia-measured distance and broadband photometry, we find that the host star is an M3 dwarf. Combined with the TESS transits from three sectors, the corresponding stellar parameters yield planet radii ranging from 0.8 R ⊕ to 1.6 R ⊕. All three planets have short orbital periods, ranging from 2.25 to 7.45 days with the outer pair just wide of a 2:1 period resonance. Diagnostic tests produced by the TESS Data Validation Report and the vetting package DAVE rule out common false-positive sources. These analyses, along with dedicated follow-up and the multiplicity of the system, lend confidence that the observed signals are caused by planets transiting L 98-59 and are not associated with other sources in the field. The L 98-59 system is interesting for a number of reasons: the host star is bright (V = 11.7 mag, K = 7.1 mag) and the planets are prime targets for further follow-up observations including precision radial-velocity mass measurements and future transit spectroscopy with the James Webb Space Telescope; the near-resonant configuration makes the system a laboratory to study planetary system dynamical evolution; and three planets of relatively similar size in the same system present an opportunity to study terrestrial planets where other variables (age, metallicity, etc.) can be held constant. L 98-59 will be observed in four more TESS sectors, which will provide a wealth of information on the three currently known planets and have the potential to reveal additional planets in the system."
LAURINA ZHANG,Gender orientation and segregation of ideas: #MeToo's impact in Hollywood,"Does the #MeToo movement affect the gender orientation of ideas and who develops which types of ideas? We examine these questions in the context of Hollywood. Since #MeToo affected the entire industry, we use variation in whether producers had past collaborations with Harvey Weinstein to discern the movement’s impact. We find that relative to non-associated producers, there is, on average, no significant change in the likelihood of developing female-protagonist stories by Weinstein-associated producers after #MeToo compared to before, despite the fact that they now work substantially more with female writers. This is because, among projects by Weinstein-associated producers, female writers are significantly less likely than male writers to work on female-protagonist stories after #MeToo. Moreover, compared to their non-associated counterparts, the depiction of female protagonists by Weinstein-associated producers after #MeToo is less traditionally feminine. Overall, our findings suggest that #MeToo may have helped mitigate the frictions and biases that prevent female talent from exploring parts of the idea space that are typically associated with men and may have shaped the nature of female-oriented works."
LAURINA ZHANG,"Scandal, social movement, and change: evidence from #MeToo in Hollywood","Social movements have the potential to effect change in firm decision-making. In this paper, we examine whether the #MeToo movement, spurred by the Harvey Weinstein scandal, led to changes in the likelihood of Hollywood producers working with female writers on new movie projects. Since #MeToo affected the entire industry, we use variation in whether producers had past collaborations with Weinstein to investigate whether and how #MeToo may spur change. We find that producers previously associated with Weinstein are, on average, about 35-percent more likely to work with female writers after the scandal than they were before, relative to non-associated producers; and the size of this effect increases with the intensity of the association. Female producers are the main drivers of our results, which may be because they are more likely than male producers to be sympathetic to the movement’s cause and face relatively low costs of enacting change. Changes made by other groups, such as production teams with the most intense association with Weinstein and less-experienced all-male teams, may be better explained by motivations to mitigate risk or guilt. We also find that producers do not sacrifice writer experience by hiring more female writers and that both experienced and novice female writers have benefited from the increased demand. Our study shows that social movements that seek to address gender inequality can, indeed, lead to meaningful change. It also provides perspective for thinking about whether, and to what extent, changes may occur in broader settings."
DAVID GUZMAN,"Caribbean Corals in Crisis: Record Thermal Stress, Bleaching, and Mortality in 2005","BACKGROUND. The rising temperature of the world's oceans has become a major threat to coral reefs globally as the severity and frequency of mass coral bleaching and mortality events increase. In 2005, high ocean temperatures in the tropical Atlantic and Caribbean resulted in the most severe bleaching event ever recorded in the basin. METHODOLOGY/PRINCIPAL FINDINGS. Satellite-based tools provided warnings for coral reef managers and scientists, guiding both the timing and location of researchers' field observations as anomalously warm conditions developed and spread across the greater Caribbean region from June to October 2005. Field surveys of bleaching and mortality exceeded prior efforts in detail and extent, and provided a new standard for documenting the effects of bleaching and for testing nowcast and forecast products. Collaborators from 22 countries undertook the most comprehensive documentation of basin-scale bleaching to date and found that over 80% of corals bleached and over 40% died at many sites. The most severe bleaching coincided with waters nearest a western Atlantic warm pool that was centered off the northern end of the Lesser Antilles. CONCLUSIONS/SIGNIFICANCE. Thermal stress during the 2005 event exceeded any observed from the Caribbean in the prior 20 years, and regionally-averaged temperatures were the warmest in over 150 years. Comparison of satellite data against field surveys demonstrated a significant predictive relationship between accumulated heat stress (measured using NOAA Coral Reef Watch's Degree Heating Weeks) and bleaching intensity. This severe, widespread bleaching and mortality will undoubtedly have long-term consequences for reef ecosystems and suggests a troubled future for tropical marine ecosystems under a warming climate."
KEVIN JAMES LANE,The long-baseline neutrino experiment: exploring fundamental symmetries of the universe,"The preponderance of matter over antimatter in the early Universe, the dynamics of the supernova bursts that produced the heavy elements necessary for life and whether protons eventually decay --- these mysteries at the forefront of particle physics and astrophysics are key to understanding the early evolution of our Universe, its current state and its eventual fate. The Long-Baseline Neutrino Experiment (LBNE) represents an extensively developed plan for a world-class experiment dedicated to addressing these questions. LBNE is conceived around three central components: (1) a new, high-intensity neutrino source generated from a megawatt-class proton accelerator at Fermi National Accelerator Laboratory, (2) a near neutrino detector just downstream of the source, and (3) a massive liquid argon time-projection chamber deployed as a far detector deep underground at the Sanford Underground Research Facility. This facility, located at the site of the former Homestake Mine in Lead, South Dakota, is approximately 1,300 km from the neutrino source at Fermilab -- a distance (baseline) that delivers optimal sensitivity to neutrino charge-parity symmetry violation and mass ordering effects. This ambitious yet cost-effective design incorporates scalability and flexibility and can accommodate a variety of upgrades and contributions. With its exceptional combination of experimental configuration, technical capabilities, and potential for transformative discoveries, LBNE promises to be a vital facility for the field of particle physics worldwide, providing physicists from around the globe with opportunities to collaborate in a twenty to thirty year program of exciting science. In this document we provide a comprehensive overview of LBNE's scientific objectives, its place in the landscape of neutrino physics worldwide, the technologies it will incorporate and the capabilities it will possess."
ELIZABETH COPPOCK,Clefts: Quite the contrary!,"Much of the previous literature on English it-clefts – sentences of the form ‘It is X that Z’ – concentrates on the nature and status of the exhaustivity inference (‘nobody/nothing other than X Z’). This paper concerns the way in which it-clefts signal contrast. We argue that it-clefts signal a type of contrast that does not merely involve a salient antecedent, as on more traditional characterizations of contrast such as those of e.g. Kiss (1998) and Rooth (1992), but also involves a conflict between the speaker’s and the hearer’s beliefs, as under the characterization of contrast given by Zimmermann (2008, 2011), which we term contrariness. Results of a felicity judgment experiment suggest that clefts do have a preference for contrariness, and one which has a gradient effect on felicity judgments: the more strongly interlocutors appear committed to an apparently false notion, the better it is to repudiate them with a cleft."
ELIZABETH COPPOCK,"Quantity superlatives in Germanic, or, ‘Life on the fault line between adjective and determiner'","This paper concerns the superlative forms of the words many, much, few, and little, and their equivalents in other Germanic languages (German, Dutch, Swedish, Norwegian, Danish, Dalecarlian, Icelandic, and Faroese). It demonstrates that every possible relationship between definiteness and interpretation is attested. It also demonstrates that agreement mismatches are found with relative readings and with proportional readings, but different kinds of agreement mismatches in each case. One consistent pattern is that a quantity superlative with adverbial morphology and neuter singular agreement features is used with relative superlatives. On the other hand, quantity superlatives with proportional readings always agree in number. I conclude that quantity superlatives are not structurally analogous to quality superlatives on either relative or proportional readings, but they depart from a plain attributive structure in different ways. On relative readings they can be akin to pseudopartitives (as in a cup of tea), while proportional readings are more closely related to partitives (as in a piece of the cake). More specifically, I suggest that the agreement features of a superlative exhibits depend on the domain from which the target is drawn (the target-domain hypothesis). When the target is a degree, as it is with adverbial superlatives and certain relative superlatives, default neuter singular emerges. Definiteness there is driven by the same process that drives definiteness with adverbial superlatives. With proportional readings, the target argument of the superlative is a subpart or subset of the domain indicated by the substance noun, hence number agreement. Subtle aspects of how the comparison class and the superlative marker are construed determine definiteness for proportional readings."
ELIZABETH COPPOCK,Absolut superlativ i samtida språkbruk,"In this article we report on a corpus study of elative superlatives in contemporary Swedish. Elative superlatives differ from ordinary superlatives in that no direct comparison with other referents is involved. Instead a referent is said to have the property expressed by the adjective to a very high degree. In Swedish, elative superlatives are formally distinct from ordinary superlatives (they lack the post-nominal clitic article) which means that they can be found in corpus searches. We show that elative superlatives have expressive function and are typically used in emphatic assertions which are intended to make the strongest possible claim in a given situation. Elative superlatives are used in all grammatical functions but with slightly different implicational properties. Contrary to what has been assumed, elative superlatives are not limited to fixed expressions and formal written language. Creative uses abound in blog texts and sports commentaries."
ELIZABETH COPPOCK,'Most' vs. 'the most' in languages where 'the more' means 'most',"This paper focuses on languages in which a superlative interpretation is typically indicated merely by a combination of a definiteness marker with a comparative marker, including French, Spanish, Italian, Romanian, and Greek ('DEF+CMP languages'). Despite ostensibly using definiteness markers to form the superlative, superlatives are not always definite-marked in these languages, and the distribution of definiteness-marking varies from language to language. To account for the cross-linguistic variation, we iden- tify conflicting pressures that all of the languages in consideration may be subject to, and suggest that different languages prioritize differently in the resolution of these conflicts. What these languages have in common, we suggest, is a mechanism of Definite Null Instantiation for the degree-type standard argument of the comparative. Among the parameters along which languages are proposed to differ is the relative importance of marking uniqueness vs. avoiding determiners with predicates of entities that are not individuals."
ELIZABETH COPPOCK,Proportional implies relative: A typological universal,"We give evidence from a geographically, genetically, and typologically diverse set of languages (drawn from 26 different language families and every continent) for the following typological universal: Regardless of the morphosyntactic strategy used by a language to form superlatives, if superlative morphosyntax can be applied to much or many, then the result can be used to express a relative reading (as in Hillary has visited the most continents (out of everyone)) but not necessarily a proportional reading (as in Hillary has visited most of the continents). Thus, no language deploys the regular superlative of much/many for the proportional but not the relative reading. We also give a rough estimate of how rare proportional readings for quantity superlatives are: about 10%. Nevertheless, we show that proportional readings arise with a diverse set of strategies for forming superlatives, and discuss the cases of Basque, Hausa, and Georgean (the last of which is most surprising)."
ELIZABETH COPPOCK,The proper treatment of egophoricity in Kathmandu Newari,"We develop a theory of so-called 'conjunct-disjunct marking', also known as 'egophoricity', in Kathmandu Newari. The signature pattern of egophoricity looks a bit like person agreement: In declaratives, there is a special marker that goes on first person verbs, but not second or third person (e.g. 'I drank-EGO too much'). But in interrogatives, the same marker goes on second person (e.g. 'Did you-EGO drink too much?'). This is called interrogative flip. Egophoric marking also interacts interestingly with the presence of evidential markers, and comes with an implication of knowing self-reference (emphasized in Newari by a restriction to volitional action). Our paper discusses two previous approaches, which we label indexical and evidential, and motivate our account, which we label egophoric. Along the way, we develop a theory of how de se attitudes are communicated."
ELIZABETH COPPOCK,Informativity in image captions vs. referring expressions,"At the intersection between computer vision and natural language processing, there has been recent progress on two natural language generation tasks: Dense Image Captioning and Referring Expression Generation for objects in complex scenes. The former aims to provide a caption for a specified object in a complex scene for the benefit of an interlocutor who may not be able to see it. The latter aims to produce a referring expression that will serve to identify a given object in a scene that the interlocutor can see. The two tasks are designed for different assumptions about the common ground between the interlocutors, and serve very different purposes, although they both associate a linguistic description with an object in a complex scene. Despite these fundamental differences, the distinction between these two tasks is sometimes overlooked. Here, we undertake a side-by-side comparison between image captioning and reference game human datasets and show that they differ systematically with respect to informativity. We hope that an understanding of the systematic differences among these human datasets will ultimately allow them to be leveraged more effectively in the associated engineering tasks."
ELIZABETH COPPOCK,Outlook-based semantics,"This paper presents and advocates an approach to the semantics of opinion statements, including matters of personal taste and moral claims. In this framework, 'outlook-based semantics', the circumstances of evaluation are not composed of a possible world and a judge (as in 'world-judge relativism'); rather, outlooks replace possible worlds in the role of circumstance of evaluation. Outlooks are refinements of worlds that settle not only matters of fact but also matters of opinion. Several virtues of the framework and advantages over existing implementations of world-judge relativism are demonstrated in this paper. First, world-judge relativism does not actually explain the 'disagreement' of 'faultless disagreement', while a straightforward explanation suggests itself in outlook-based semantics. Second, outlook-based semantics provides an account of subjective attitude verbs that can capture lack of opinionatedness. Third, outlook-based semantics unproblematically explains the connection-building role of aesthetic discourse and the group-relevance of discretionary assertions, while capturing the same effects in world-judge relativism obviates the purpose of the judge parameter. Finally, because the proposed circumstances of evaluation (outlooks) are entirely analogous to possible worlds, the framework is easy to use and extend."
ELIZABETH COPPOCK,Universals in superlative semantics,
ELIZABETH COPPOCK,Universals in superlative semantics: supplementary material,
ELIZABETH COPPOCK,Definiteness and determinacy,"This paper distinguishes between definiteness and determinacy. Definiteness is seen as a morphological category which, in English, marks a (weak) uniqueness presupposition, while determinacy consists in denoting an individual. Definite descriptions are argued to be fundamentally predicative, presupposing uniqueness but not existence, and to acquire existential import through general type-shifting operations that apply not only to definites, but also indefinites and possessives. Through these shifts, argumental definite descriptions may become either determinate (and thus denote an individual) or indeterminate (functioning as an existential quantifier). The latter option is observed in examples like ‘Anna didn’t give the only invited talk at the conference’, which, on its indeterminate reading, implies that there is nothing in the extension of ‘only invited talk at the conference’. The paper also offers a resolution of the issue of whether possessives are inherently indefinite or definite, suggesting that, like indefinites, they do not mark definiteness lexically, but like definites, they typically yield determinate readings due to a general preference for the shifting operation that produces them."
ELIZABETH COPPOCK,Defining definiteness in Turoyo,"This paper puts forth an expanded typology of definiteness marking, which includes not only ‘strong’ and ‘weak’ but also ‘super-weak’. It also proposes a methodology for identifying ‘super-weak’ definites, and applies it to Ṭuroyo, an endangered Semitic language. Data from questionnaires and interviews shows that Ṭuroyo’s definite article has a very wide distribution, including anti-uniqueness effects with exclusives, suggesting ‘super-weak’ status. Syntactic factors also affect their distribution: We find definiteness-spreading uses with demonstratives and possessives, even in non-contrastive environments, and superlative adjectives appear to compete for the article’s syntactic position. On the semantic side, we propose that Ṭuroyo’s definiteness-markers are not ‘weak’ but ‘super-weak’ articles. To explain their anaphoric uses, typical of ‘strong’ articles, we propose that the typology is arranged as a cline ordered by entailment, so that stronger articles entail weaker ones."
ELIZABETH COPPOCK,Challenge problems for a theory of degree multiplication,"This paper offers a theory of degree multiplication in natural language semantics. Motivation for the development such a theory comes from proportional readings of quantity words and rate expressions such as miles per hour. After laying out a set of ‘challenge problems’ that any good theory of degree multiplication should be able to handle, I set about solving them, borrowing mathematical tools from quantity calculus. These algebraic foundations are integrated into a compositional Montagovian framework, yielding a system that can solve, or partially solve, some of the problems."
ELIZABETH COPPOCK,Implicatures of modified numerals: quantity or quality?,"We propose a new analysis of modified numerals that allows us to: (i) predict ignorance with respect to the prejacent of at least (and thereby avoid to Bernard Schwarz's recent criticism of Coppock and Brochhagen 2013), (ii) get a three-way contrast between superlative modifiers, comparative modifiers, and numerals, without appeal to a two-sided analysis of numerals, and (iii) avoid the prediction that at least should produce quantity implciatures when only is not a grammatical alternative. With it, we reconcile Westera and Brasoveanu's (2014) findings with the achievements of the Coppock and Brochhagen account, bring that work in line with recent theorizing in inquisitive semantics using downward-closed possibilities, and show that inquisitive sincerity can interact with Horn-based quantity in a non-trivial way, something that may be fruitful to consider in other domains as well."
ELIZABETH COPPOCK,Granularity in the semantics of comparison,"This paper makes the novel observation that definite comparatives, such as the bigger circle, impose restrictions on the cardinality of the comparison class (CC) against which their truth conditions are evaluated. We show that the corpus frequency counts of definite comparatives sharply drop when the comparison class used for their interpretation is formed by more than two individuals. Two alternative theories of these distributional facts are considered and tested experimentally through an acceptability judgment task. According to the first theory, the 2-Individuals Theory, definite comparatives presuppose that the CC is of cardinality 2; under the second theory, the 2-Degrees Theory, the meaning of the comparative is evaluated against a granularity γ that maps the individuals in the CC to degrees in the relevant adjectival scale, and definite comparatives presuppose that the set of the degrees resulting from this mapping is of cardinality 2. Our experimental results show that definite comparative descriptions are most frequent and felicitous when evaluated against comparison classes with two individuals, but also that acceptability drops off with higher cardinalities in a gradient manner that is sensitive to granularity. Taken together, these findings argue against the 2-Individuals theory of definite comparatives and lend support to the 2-Degrees theory."
ELIZABETH COPPOCK,It's not what you expected! The surprising nature of cleft alternatives in French and English,"While much prior literature on the meaning of clefts—such as the English form “it is X who Z-ed”—concentrates on the nature and status of the exhaustivity inference (“nobody/nothing other than X Z”), we report on experiments examining the role of the doxastic status of alternatives on the naturalness of c'est-clefts in French and it-clefts in English. Specifically, we study the hypothesis that clefts indicate a conflict with a doxastic commitment held by some discourse participant. Results from naturalness tasks suggest that clefts are improved by a property we term “contrariness” (along the lines of Zimmermann, 2008). This property has a gradient effect on felicity judgments: the more strongly interlocutors appear committed to an apparently false notion, the better it is to repudiate them with a cleft."
ELIZABETH COPPOCK,Superlative modifiers as modified superlatives,"The superlative modifiers at least and at most are quite famous, but their cousins at best, at the latest, at the highest, etc., are less well-known. This paper is devoted to the entire family. New data is presented illustrating the productivity of the pattern, identifying a generalization delimiting it, and showing that the cousins, too, have the pragmatic effects that have attracted so much attention to at least and at most. To capture the productivity, I present a new decomposition of at least into recombinable parts. Most notable is the at-component (silent in some languages), which takes advantage of the comparison class argument of the superlative to produce the set of possibilities involved in the ignorance implicatures that superlative modifiers are known for. A side-effect is a new view on gradable predicates, accounting for uses like 88 degrees is too hot. "
ELIZABETH COPPOCK,Proportional implies relative: a typological universal,"We give evidence from a geographically, genetically, and typologically diverse set of languages (drawn from 26 different language families and every continent) for the following typological universal: Regardless of the morphosyntactic strategy used by a language to form superlatives, if superlative morphosyntax can be applied to ‘much’ or ‘many’, then the result can be used to express a relative reading (as in Hillary has visited the most continents (out of everyone)) but not necessarily a proportional reading (as in Hillary has visited most of the continents). Thus, no language deploys the regular superlative of ‘much’/‘many’ for the proportional but not the relative reading. We also give a rough estimate of how rare proportional readings for quantity superlatives are: about 10%. Nevertheless, we show that proportional readings arise with a diverse set of strategies for forming superlatives."
ELIZABETH COPPOCK,Ignorance implicatures of modified numerals,"Modified numerals, such as at least three and more than five, are known to sometimes give rise to ignorance inferences. However, there is disagreement in the literature regarding the nature of these inferences, their context dependence, and differences between at least and more than. We present a series of experiments which sheds new light on these issues. Our results show that (a) the ignorance inferences of at least are more robust than those of more than, (b) the presence and strength of the ignorance inferences triggered by both at least and more than depends on the question under discussion (QUD), and (c) whether ignorance inferences are detected in a given experimental setting depends partly on the task that participants are asked to perform (e.g., an acceptability task versus an inference task). We offer an Optimality Theoretic account of these findings. In particular, the task effect is captured by assuming that in performing an acceptability task, participants take the speaker’s perspective in order to determine whether an expression is optimal given a certain epistemic state, while in performing an inference task they take the addressee’s perspective in order to determine what the most likely epistemic state of the speaker is given a certain expression. To execute the latter task in a fully rational manner, participants have to perform higher-order reasoning about alternative expressions the speaker could have used. Under the assumption that participants do not always perform such higher-order reasoning but also often resort to so-called unidirectional optimization, the task effect finds a natural explanation. This also allows us to relate our finding to asymmetries between comprehension and production that have been found in language acquisition."
DONNA B PINCUS,Identifying and making recommendations for pediatric anxiety disorders in primary care settings: a video-based training,"INTRODUCTION: Pediatric anxiety disorders have high rates of prevalence and confer risk for later disorders if they go undetected. In primary care, they are underdiagnosed, partly because pediatricians often lack relevant training. We developed a brief, video-based training program for pediatric residents aimed at improving early identification of anxiety disorders in primary care. METHODS: Video content was consistent with the American Academy of Pediatrics Behavioral Health Competencies, as applied to the evaluation of anxiety disorders and guidance for discussing treatment options. This training can be delivered in two formats: videos (43 minutes) can be shown in a live, group-based format, or accessed via an online, asynchronous training. We tested this training program using both formats and developed surveys to evaluate knowledge about child anxiety, perceived evaluation skills, and satisfaction with the training. We also developed a video-based vignette to measure sensitivity to detecting disorders (how much the condition is interfering, diagnostic severity, and referral urgency). RESULTS: Pediatric residents from two residency programs completed the training and pre- and posttraining assessments to evaluate program efficacy. Residents' knowledge and perceived evaluation skills increased posttraining, with large effect sizes. Residents also demonstrated increased sensitivity to detecting anxiety disorders on the vignette-based assessment and reported high levels of satisfaction. DISCUSSION: Our results suggested that residents participating in this training improved their evaluation skills and that residents found the training beneficial. Video-based trainings can significantly supplement existing education. This cost-effective and minimally burdensome training program can be used to enhance resident education in a much-needed area."
DONNA B PINCUS,Predictors of treatment satisfaction among adolescents following an intensive cognitive-behavioral intervention for panic disorder,"No studies to date examine predictors of treatment satisfaction following intensive cognitive-behavioral therapy interventions among adolescents. Given the challenges to treatment adherence among adolescents, and the promise intensive interventions hold for providing rapid symptom relief and increasing access to care, data examining adolescents' satisfaction with intensive programs are needed. Twenty-four adolescents (ages 12-17) with panic disorder received an eight-day intensive cognitive-behavioral therapy intervention. Pre-treatment characteristics and clinical outcome variables were examined as predictors of satisfaction at post-treatment and three-months follow-up. Multiple regression analyses revealed that higher levels of overall symptom interference at baseline and greater reductions in agoraphobic fear during treatment predicted greater treatment satisfaction at post-treatment. Only satisfaction at post-treatment significantly predicted treatment satisfaction at follow-up, highlighting the potential influence of treatment satisfaction on long-term perceptions of treatment. Considerations for fostering treatment satisfaction in the context of intensive interventions are discussed."
HENGYE MAN,"Environmental enrichment facilitates cocaine-cue extinction, deters reacquisition of cocaine self-administration and alters AMPAR GluA1 expression and phosphorylation","This study investigated the combination of environmental enrichment (EE) with cocaine‐cue extinction training on reacquisition of cocaine self‐administration. Rats were trained under a second‐order schedule for which responses were maintained by cocaine injections and cocaine‐paired stimuli. During three weekly extinction sessions, saline was substituted for cocaine but cocaine‐paired stimuli were presented. Rats received 4‐h periods of EE at strategic time points during extinction training, or received NoEE. Additional control rats received EE or NoEE without extinction training. One week later, reacquisition of cocaine self‐administration was evaluated for 15 sessions, and then GluA1 expression, a cellular substrate for learning and memory, was measured in selected brain regions. EE provided both 24 h before and immediately after extinction training facilitated extinction learning and deterred reacquisition of cocaine self‐administration for up to 13 sessions. Each intervention by itself (EE alone or extinction alone) was ineffective, as was EE scheduled at individual time points (EE 4 h or 24 h before, or EE immediately or 6 h after, each extinction training session). Under these conditions, rats rapidly reacquired baseline rates of cocaine self‐administration. Cocaine self‐administration alone decreased total GluA1 and/or pSer845GluA1 expression in basolateral amygdala and nucleus accumbens. Extinction training, with or without EE, opposed these changes and also increased total GluA1 in ventromedial prefrontal cortex and dorsal hippocampus. EE alone increased pSer845GluA1 and EE combined with extinction training decreased pSer845GluA1 in ventromedial prefrontal cortex. EE might be a useful adjunct to extinction therapy by enabling neuroplasticity that deters relapse to cocaine self‐administration."
HENGYE MAN,3'-UTR SIRF: A database for identifying clusters of short interspersed repeats in 3' untranslated regions,"BACKGROUND:Short (~5 nucleotides) interspersed repeats regulate several aspects of post-transcriptional gene expression. Previously we developed an algorithm (REPFIND) that assigns P-values to all repeated motifs in a given nucleic acid sequence and reliably identifies clusters of short CAC-containing motifs required for mRNA localization in Xenopus oocytes.DESCRIPTION:In order to facilitate the identification of genes possessing clusters of repeats that regulate post-transcriptional aspects of gene expression in mammalian genes, we used REPFIND to create a database of all repeated motifs in the 3' untranslated regions (UTR) of genes from the Mammalian Gene Collection (MGC). The MGC database includes seven vertebrate species: human, cow, rat, mouse and three non-mammalian vertebrate species. A web-based application was developed to search this database of repeated motifs to generate species-specific lists of genes containing specific classes of repeats in their 3'-UTRs. This computational tool is called 3'-UTR SIRF (Short Interspersed Repeat Finder), and it reveals that hundreds of human genes contain an abundance of short CAC-rich and CAG-rich repeats in their 3'-UTRs that are similar to those found in mRNAs localized to the neurites of neurons. We tested four candidate mRNAs for localization in rat hippocampal neurons by in situ hybridization. Our results show that two candidate CAC-rich (Syntaxin 1B and Tubulin beta4) and two candidate CAG-rich (Sec61alpha and Syntaxin 1A) mRNAs are localized to distal neurites, whereas two control mRNAs lacking repeated motifs in their 3'-UTR remain primarily in the cell body.CONCLUSION:Computational data generated with 3'-UTR SIRF indicate that hundreds of mammalian genes have an abundance of short CA-containing motifs that may direct mRNA localization in neurons. In situ hybridization shows that four candidate mRNAs are localized to distal neurites of cultured hippocampal neurons. These data suggest that short CA-containing motifs may be part of a widely utilized genetic code that regulates mRNA localization in vertebrate cells. The use of 3'-UTR SIRF to search for new classes of motifs that regulate other aspects of gene expression should yield important information in future studies addressing cis-regulatory information located in 3'-UTRs."
HENGYE MAN,3'-UTR SIRF: A Database for Identifying Clusters of Whort Interspersed Repeats in 3' Untranslated Regions,"BACKGROUND. Short (~5 nucleotides) interspersed repeats regulate several aspects of post-transcriptional gene expression. Previously we developed an algorithm (REPFIND) that assigns P-values to all repeated motifs in a given nucleic acid sequence and reliably identifies clusters of short CAC-containing motifs required for mRNA localization in Xenopus oocytes. DESCRIPTION. In order to facilitate the identification of genes possessing clusters of repeats that regulate post-transcriptional aspects of gene expression in mammalian genes, we used REPFIND to create a database of all repeated motifs in the 3' untranslated regions (UTR) of genes from the Mammalian Gene Collection (MGC). The MGC database includes seven vertebrate species: human, cow, rat, mouse and three non-mammalian vertebrate species. A web-based application was developed to search this database of repeated motifs to generate species-specific lists of genes containing specific classes of repeats in their 3'-UTRs. This computational tool is called 3'-UTR SIRF (Short Interspersed Repeat Finder), and it reveals that hundreds of human genes contain an abundance of short CAC-rich and CAG-rich repeats in their 3'-UTRs that are similar to those found in mRNAs localized to the neurites of neurons. We tested four candidate mRNAs for localization in rat hippocampal neurons by in situ hybridization. Our results show that two candidate CAC-rich (Syntaxin 1B and Tubulin β4) and two candidate CAG-rich (Sec61α and Syntaxin 1A) mRNAs are localized to distal neurites, whereas two control mRNAs lacking repeated motifs in their 3'-UTR remain primarily in the cell body. CONCLUSION. Computational data generated with 3'-UTR SIRF indicate that hundreds of mammalian genes have an abundance of short CA-containing motifs that may direct mRNA localization in neurons. In situ hybridization shows that four candidate mRNAs are localized to distal neurites of cultured hippocampal neurons. These data suggest that short CA-containing motifs may be part of a widely utilized genetic code that regulates mRNA localization in vertebrate cells. The use of 3'-UTR SIRF to search for new classes of motifs that regulate other aspects of gene expression should yield important information in future studies addressing cis-regulatory information located in 3'-UTRs."
SWATHI KIRAN,Neural connectivity in syntactic movement processing,"Linguistic theory suggests non-canonical sentences subvert the dominant agent-verb-theme order in English via displacement of sentence constituents to argument (NP-movement) or non-argument positions (wh-movement). Both processes have been associated with the left inferior frontal gyrus and posterior superior temporal gyrus, but differences in neural activity and connectivity between movement types have not been investigated. In the current study, functional magnetic resonance imaging data were acquired from 21 adult participants during an auditory sentence-picture verification task using passive and active sentences contrasted to isolate NP-movement, and object- and subject-cleft sentences contrasted to isolate wh-movement. Then, functional magnetic resonance imaging data from regions common to both movement types were entered into a dynamic causal modeling analysis to examine effective connectivity for wh-movement and NP-movement. Results showed greater left inferior frontal gyrus activation for Wh > NP-movement, but no activation for NP > Wh-movement. Both types of movement elicited activity in the opercular part of the left inferior frontal gyrus, left posterior superior temporal gyrus, and left medial superior frontal gyrus. The dynamic causal modeling analyses indicated that neither movement type significantly modulated the connection from the left inferior frontal gyrus to the left posterior superior temporal gyrus, nor vice-versa, suggesting no connectivity differences between wh- and NP-movement. These findings support the idea that increased complexity of wh-structures, compared to sentences with NP-movement, requires greater engagement of cognitive resources via increased neural activity in the left inferior frontal gyrus, but both movement types engage similar neural networks."
SWATHI KIRAN,"Neuroplasticity of language networks in aphasia: advances, updates, and future challenges","Researchers have sought to understand how language is processed in the brain, how brain damage affects language abilities, and what can be expected during the recovery period since the early 19th century. In this review, we first discuss mechanisms of damage and plasticity in the post-stroke brain, both in the acute and the chronic phase of recovery. We then review factors that are associated with recovery. First, we review organism intrinsic variables such as age, lesion volume and location and structural integrity that influence language recovery. Next, we review organism extrinsic factors such as treatment that influence language recovery. Here, we discuss recent advances in our understanding of language recovery and highlight recent work that emphasizes a network perspective of language recovery. Finally, we propose our interpretation of the principles of neuroplasticity, originally proposed by Kleim and Jones (1) in the context of extant literature in aphasia recovery and rehabilitation. Ultimately, we encourage researchers to propose sophisticated intervention studies that bring us closer to the goal of providing precision treatment for patients with aphasia and a better understanding of the neural mechanisms that underlie successful neuroplasticity."
SWATHI KIRAN,Changes in Functional Connectivity Associated with Treatment Gains in Aphasia,
SWATHI KIRAN,"The relationship between frontotemporal effective connectivity during picture naming, behavior, and preserved cortical tissue in chronic aphasia",
SWATHI KIRAN,Intrahemispheric perfusion in chronic stroke-induced aphasia,"Stroke-induced alterations in cerebral blood flow (perfusion) may contribute to functional language impairments and recovery in chronic aphasia. Using MRI, we examined perfusion in the right and left hemispheres of 35 aphasic and 16 healthy control participants. Across 76 regions (38 per hemisphere), no significant between-subjects differences were found in the left, whereas blood flow in the right was increased in the aphasic compared to the control participants. Region-of-interest (ROI) analyses showed a varied pattern of hypo- and hyperperfused regions across hemispheres in the aphasic participants; however, there were no significant correlations between perfusion values and language abilities in these regions. These patterns may reflect autoregulatory changes in blood flow following stroke and/or increases in general cognitive effort, rather than maladaptive language processing. We also examined blood flow in perilesional tissue, finding the greatest hypoperfusion close to the lesion (within 0–6 mm), with greater hypoperfusion in this region compared to more distal regions. In addition, hypoperfusion in this region was significantly correlated with language impairment. These findings underscore the need to consider cerebral perfusion as a factor contributing to language deficits in chronic aphasia as well as recovery of language function."
SWATHI KIRAN,Technology-based rehabilitation to improve communication after acquired brain injury,"The utilization of technology has allowed for several advances in aphasia rehabilitation for individuals with acquired brain injury. Thirty-one previous studies that provide technology-based language or language and cognitive rehabilitation are examined in terms of the domains addressed, the types of treatments that were provided, details about the methods and the results, including which types of outcomes are reported. From this, we address questions about how different aspects of the delivery of treatment can influence rehabilitation outcomes, such as whether the treatment was standardized or tailored, whether the participants were prescribed homework or not, and whether intensity was varied. Results differed by these aspects of treatment delivery but ultimately the studies demonstrated consistent improvement on various outcome measures. With these aspects of technology-based treatment in mind, the ultimate goal of personalized rehabilitation is discussed."
SWATHI KIRAN,Right hemisphere grey matter volume and language functions in stroke aphasia,"The role of the right hemisphere (RH) in recovery from aphasia is incompletely understood. The present study quantified RH grey matter (GM) volume in individuals with chronic stroke-induced aphasia and cognitively healthy people using voxel-based morphometry. We compared group differences in GM volume in the entire RH and in RH regions-of-interest. Given that lesion site is a critical source of heterogeneity associated with poststroke language ability, we used voxel-based lesion symptom mapping (VLSM) to examine the relation between lesion site and language performance in the aphasic participants. Finally, using results derived from the VLSM as a covariate, we evaluated the relation between GM volume in the RH and language ability across domains, including comprehension and production processes both at the word and sentence levels and across spoken and written modalities. Between-subject comparisons showed that GM volume in the RH SMA was reduced in the aphasic group compared to the healthy controls. We also found that, for the aphasic group, increased RH volume in the MTG and the SMA was associated with better language comprehension and production scores, respectively. These data suggest that the RH may support functions previously performed by LH regions and have important implications for understanding poststroke reorganization."
SWATHI KIRAN,Towards neuroscience of the everyday world (NEW) using functional near infrared spectroscopy,"Functional near-infrared spectroscopy (fNIRS) assesses human brain activity by noninvasively measuring changes of cerebral hemoglobin concentrations caused by modulation of neuronal activity. Recent progress in signal processing and advances in system design, such as miniaturization, wearability, and system sensitivity, have strengthened fNIRS as a viable and cost-effective complement to functional magnetic resonance imaging, expanding the repertoire of experimental studies that can be performed by the neuroscience community. The availability of fNIRS and electroencephalography for routine, increasingly unconstrained, and mobile brain imaging is leading toward a new domain that we term “Neuroscience of the Everyday World” (NEW). In this light, we review recent advances in hardware, study design, and signal processing, and discuss challenges and future directions."
SWATHI KIRAN,BiLex: A computational approach to the effects of age of acquisition and language exposure on bilingual lexical access,"Lexical access in bilinguals can be modulated by multiple factors in their individual language learning history. We developed the BiLex computational model to examine the effects of L2 age of acquisition, language use and exposure on lexical retrieval in bilingual speakers. Twenty-eight Spanish-English bilinguals and five monolinguals recruited to test and validate the model were evaluated in their picture naming skills in each language and filled out a language use questionnaire. We examined whether BiLex can (i) simulate their naming performance in each language while taking into account their L2 age of acquisition, use and exposure to each language, and (ii) predict naming performance in other participants not used in model training. Our findings showed that BiLex could accurately simulate naming performance in bilinguals, suggesting that differences in L2 age of acquisition, language use and exposure can account for individual differences in bilingual lexical access."
SWATHI KIRAN,Inter- and Intra-Individual Variability in Non-Linguistic Attention in Aphasia,
SWATHI KIRAN,A lesion and connectivity-based hierarchical model of chronic aphasia recovery dissociates patients and healthy controls,"Traditional models of left hemisphere stroke recovery propose that reactivation of remaining ipsilesional tissue is optimal for language processing whereas reliance on contralesional right hemisphere homologues is less beneficial or possibly maladaptive in the chronic recovery stage. However, neuroimaging evidence for this proposal is mixed. This study aimed to elucidate patterns of effective connectivity in patients with chronic aphasia in light of healthy control connectivity patterns and in relation to damaged tissue within left hemisphere regions of interest and according to performance on a semantic decision task. Using fMRI and dynamic causal modeling, biologically-plausible models within four model families were created to correspond to potential neural recovery patterns, including Family A: Left-lateralized connectivity (i.e., no/minimal damage), Family B: Bilateral anterior-weighted connectivity (i.e., posterior damage), Family C: Bilateral posterior-weighted connectivity (i.e., anterior damage) and Family D: Right-lateralized connectivity (i.e., extensive damage). Controls exhibited a strong preference for left-lateralized network models (Family A) whereas patients demonstrated a split preference for Families A and C. At the level of connections, controls exhibited stronger left intrahemispheric task-modulated connections than did patients. Within the patient group, damage to left superior frontal structures resulted in greater right intrahemispheric connectivity whereas damage to left ventral structures resulted in heightened modulation of left frontal regions. Lesion metrics best predicted accuracy on the fMRI task and aphasia severity whereas left intrahemispheric connectivity predicted fMRI task reaction times. These results are discussed within the context of the hierarchical recovery model of chronic aphasia."
SWATHI KIRAN,Changes in task-based effective connectivity in language networks following rehabilitation in post-stroke patients with aphasia,"In this study, we examined regions in the left and right hemisphere language network that were altered in terms of the underlying neural activation and effective connectivity subsequent to language rehabilitation. Eight persons with chronic post-stroke aphasia and eight normal controls participated in the current study. Patients received a 10 week semantic feature-based rehabilitation program to improve their skills. Therapy was provided on atypical examples of one trained category while two control categories were monitored; the categories were counterbalanced across patients. In each fMRI session, two experimental tasks were conducted: (a) picture naming and (b) semantic feature verification of trained and untrained categories. Analysis of treatment effect sizes revealed that all patients showed greater improvements on the trained category relative to untrained categories. Results from this study show remarkable patterns of consistency despite the inherent variability in lesion size and activation patterns across patients. Across patients, activation that emerged as a function of rehabilitation on the trained category included bilateral IFG, bilateral SFG, LMFG, and LPCG for picture naming; and bilateral IFG, bilateral MFG, LSFG, and bilateral MTG for semantic feature verification. Analysis of effective connectivity using Dynamic Causal Modeling (DCM) indicated that LIFG was the consistently significantly modulated region after rehabilitation across participants. These results indicate that language networks in patients with aphasia resemble normal language control networks and that this similarity is accentuated by rehabilitation."
SWATHI KIRAN,Relationship between self-administered cues and rehabilitation outcomes in individuals with aphasia: understanding individual responsiveness to a technology-based rehabilitation program,"An advantage of rehabilitation administered on computers or tablets is that the tasks can be self-administered and the cueing required to complete the tasks can be monitored. Though there are many types of cueing, few studies have examined how participants’ response to rehabilitation is influenced by self-administered cueing, which is cueing that is always available but the individual decides when and which cue to administer. In this study, participants received a tablet-based rehabilitation where the tasks were selfpaced and remotely monitored by a clinician. The results of the effectiveness of this study were published previously (Des Roches et al., 2015). The current study looks at the effect of cues on accuracy and rehabilitation outcomes. Fifty-one individuals with aphasia completed a 10-week program using Constant Therapy on an iPad targeted at improving language and cognitive deficits. Three questions were examined. The first examined the effect of cues on accuracy collapsed across time. Results showed a trend where the greater the cue use, the lower the accuracy, although some participants showed the opposite effect. This analysis divided participants into profiles based on cue use and accuracy. The second question examined how each profile differed in percent cue use and on standardized measures at baseline. Results showed that the four profiles were significantly different in frequency of cues and scores on WAB-R, CLQT, BNT, and ASHA-FACS, indicating that participants with lower scores on the standardized tests used a higher percentage of cues, which were not beneficial, while participants with higher scores on the standardized tests used a lower frequency of cues, which were beneficial. The third question examined how the relationship between cues and accuracy was affected by the course of treatment. Results showed that both more and less severe participants showed a decrease in cue use and an increase in accuracy over time, though more severe participants continued to used a greater number of cues. It is possible that self-administered cues help some individuals to access information that is otherwise inaccessible, even if there is not an immediate effect. Ultimately, the results demonstrate the need for individually modifying the levels of assistance during rehabilitation."
SWATHI KIRAN,The Intensive Cognitive-Communication Rehabilitation Program for young adults with acquired brain injury,"PURPOSE: This study investigated the effects of an intensive cognitive-communication rehabilitation (ICCR) program for young individuals with chronic acquired brain injury. METHOD: ICCR included classroom lectures; metacognitive instruction, modeling, and application; technology skills training; and individual cognitive-linguistic therapy. Four individuals participated in the intensive program (6 hr with 1-hr lunch break × 4 days × 12 weeks of treatment): 3 participants completed 3 consecutive semesters, and 1 participant completed 1 semester. Two controls did not receive treatment and completed assessments before and after the 12-week treatment interval only. RESULTS: All 4 experimental participants demonstrated significant improvements on at least 1 standardized cognitive-linguistic measure, whereas controls did not. Furthermore, time point significantly predicted participants' scores on 2 of the 4 standardized outcome measures, indicating that as duration in ICCR increased, scores also increased. Participants who completed multiple semesters of ICCR also improved in their therapy and personal goals, classroom behavior, life participation, and quality of life. CONCLUSION: After ICCR, participants showed gains in their cognitive-linguistic functioning, classroom participation, and individual therapy. They also demonstrated improvements outside the classroom and in their overall well-being. There is a gap between the large population of young adults with acquired brain injury who wish to return to higher education and a lack of rehabilitation programs supporting reentry into academic environments; ICCR is a first step in reducing that gap."
SWATHI KIRAN,Representation of semantic typicality in brain activation in healthy adults and individuals with aphasia: a multi-voxel pattern analysis,"This study aimed to investigate brain regions that show different activation patterns between semantically typical and atypical items in both healthy adults and individuals with aphasia (PWA). Eighteen neurologically healthy adults and twenty-one PWA participated in an fMRI semantic feature verification task that included typical and atypical stimuli from five different semantic categories. A whole-brain searchlight multi-voxel pattern analysis (MVPA) was conducted to classify brain activation patterns between typical and atypical conditions in each participant group separately. Behavioral responses were faster and more accurate for typical vs. atypical items across both groups. The searchlight MVPA identified two significant clusters in healthy adults: left middle occipital gyrus and right calcarine cortex, but no significant clusters were found in PWA. A follow-up analysis in PWA revealed a significant association between neural classification of semantic typicality in the left middle occipital gyrus and reaction times in the fMRI task. When the typicality effect was examined for each semantic category at the univariate level, significance was identified in the visual cortex for fruits in both groups of participants. These findings suggest that semantic typicality was modulated in the visual cortex in healthy individuals, but to a lesser extent in the same region in PWA."
SWATHI KIRAN,Multi-level outcomes for young adults with acquired brain injury through a remote intensive cognitive rehabilitation approach: a pilot intervention study,"OBJECTIVE: To investigate the effects of the Intensive Cognitive and Communication Rehabilitation (ICCR) program for young adults with acquired brain injury (ABI) using a quasi-experimental pilot intervention study design while transitioning to remote implementation. METHOD: Twelve young adults with chronic ABI (treatment n = 7; control n = 5) participated in ICCR (i.e., lectures, seminars, individual cognitive rehabilitation (CR), technology training) for six hours/day, four days/week, for one or two 12-week semesters. Outcomes included classroom metrics, individual therapy performance, including Goal Attainment Scaling (GAS), standardized cognitive-linguistic assessments, and participation and health-related quality of life (QOL) measures. RESULTS: In the first semester (in-person and remote), treatment participants significantly improved in classroom exams; individual therapy (i.e., memory, writing, GAS); executive function and participation measures, but not QOL. In the second semester (remote), treatment participants significantly improved in classroom exams; essay writing; individual therapy (i.e., writing and GAS); and memory assessment, but not in participation or QOL. Treatment participants enrolled in consecutive semesters significantly improved in classroom exams, individual therapy (i.e., memory), participation and QOL, but not on standardized cognitive assessments. Controls demonstrated no significant group-level gains. CONCLUSION: These preliminary results highlight the benefit of intensive, integrated, and contextualized CR for this population and show promise for its remote delivery."
SWATHI KIRAN,Feature analysis and extraction for post aphasia recovery prediction,
SWATHI KIRAN,The cognate facilitation effect on lexical access in bilingual aphasia: evidence from the Boston naming test,"Most cognate research suggests facilitation effects in picture naming, but how these effects manifest in bilinguals after brain damage remains unclear. Additionally, whether this effect is captured in clinical measures is largely unknown. Using data from the Boston Naming Test, we examined the naming of cognates and noncognates, the extent of cognate facilitation produced, and the individual differences in bilingual language experience associated with naming outcomes in forty Spanish–English bilingual persons with aphasia (BPWA) relative to thirty-one Spanish–English healthy bilinguals (HB). Results suggest that naming performance in L1 and L2 in both groups is modulated by lexical frequency, bilingual language experience, and by language impairment in BPWA. Although the two groups showed similarities, they deviated in benefit drawn from the extent of phoneme/grapheme overlap in cognate items. HB showed an association between cognate facilitation and bilingual language experience, while cognate facilitation in BPWA was only associated with L2 language impairment."
SWATHI KIRAN,Comparison of therapy practice at home and in the clinic: a retrospective analysis of the Constant Therapy Platform data set,"BACKGROUND: Computer-based therapies can provide an affordable and practical alternative by providing frequent intervention for stroke survivors with chronic aphasia by allowing the opportunity for home exercise practice, however more evidence is needed. The goal of this retrospective analysis was to compare the time course of therapy engagement when therapy was targeted in the clinic or at home by post-stroke individuals. We examined if home users of the therapy were compliant in therapy and if this documented practice time was associated with improved outcomes similar to clinic patients who practiced under the guidance of a clinician. METHODS: A retrospective analysis of anonymously aggregated data collected for 3,686 patients with post-stroke aphasia over the course of four years (2013–2017) was conducted. Participants either received therapy delivered through Constant Therapy only at home (N = 2,100) or only in the clinic (N = 1,577). Constant Therapy includes over 70 evidence-based therapies for language and cognitive skills. This program was individualized for each patient with targeted tasks that dynamically adapted to each individual's progress. RESULTS: Patients with <60% accuracy were analyzed to determine how long it took them to reach >90% accuracy. Results showed that both home-therapy and clinic patients reached 90% accuracy on their tasks similarly (Median = 3 sessions), but the frequency of therapy was significantly different with 50% of home users receiving therapy at least every 2 days while 50% of clinic patients only had therapy once every 5 days (p < 0.001). Thus, home-therapy users were able to master tasks in a shorter time (median of 6 days) than clinic patients (median of 12 days) (p < 0.001). CONCLUSION: Outcomes of treatment are similar for home users and clinic patients indicating the potential usability of a home-based treatment program for rehabilitation for post-stroke aphasia."
SWATHI KIRAN,Multimodal neural and behavioral data predict response to rehabilitation in chronic post-stroke aphasia,"BACKGROUND: Poststroke recovery depends on multiple factors and varies greatly across individuals. Using machine learning models, this study investigated the independent and complementary prognostic role of different patient-related factors in predicting response to language rehabilitation after a stroke. METHODS: Fifty-five individuals with chronic poststroke aphasia underwent a battery of standardized assessments and structural and functional magnetic resonance imaging scans, and received 12 weeks of language treatment. Support vector machine and random forest models were constructed to predict responsiveness to treatment using pretreatment behavioral, demographic, and structural and functional neuroimaging data. RESULTS: The best prediction performance was achieved by a support vector machine model trained on aphasia severity, demographics, measures of anatomic integrity and resting-state functional connectivity (F1=0.94). This model resulted in a significantly superior prediction performance compared with support vector machine models trained on all feature sets (F1=0.82, P<0.001) or a single feature set (F1 range=0.68–0.84, P<0.001). Across random forest models, training on resting-state functional magnetic resonance imaging connectivity data yielded the best F1 score (F1=0.87). CONCLUSIONS: While behavioral, multimodal neuroimaging data and demographic information carry complementary information in predicting response to rehabilitation in chronic poststroke aphasia, functional connectivity of the brain at rest after stroke is a particularly important predictor of responsiveness to treatment, both alone and combined with other patient-related factors."
SWATHI KIRAN,LEX-BADAT: language eXperience in bilinguals with and without aphasia DATaset,
WALTER D HOPP,"Husserl, Dummett, and the linguistic turn","Michael Dummett famously holds that the “philosophy of thought” must proceed via the philosophy of language, since that is the only way to preserve the objectivity of thoughts while avoiding commitments to “mythological,” Platonic entities. Central to Dummett’s case is his thesis that all thought contents are linguistically expressible. In this paper, I will (a) argue that making the linguistic turn is neither necessary nor sufficient to avoid the problems of psychologism, (b) discuss Wayne Martin’s argument that not all thought-contents are linguistically communicable, and (c) present another, stronger argument, derived from Husserl’s early account of fulfillment, that establishes the same conclusion."
NIMI WARIBOKO,Focus: Summer 2017,
NIMI WARIBOKO,"Journal of African Christian Biography: v. 9, no. 1","[African Christians from the DACB collection are showcased here as well. We also included a short section on the Gospel writer John Mark later in the issue. This excerpt from Oden’s 2011 book the African Memory of Mark: Reassessing Early Church Tradition (InterVarsity Press.) also includes an important historiographical concept—that of African memory and how it contrasts with Western memory. This volume includes three resources made available by CEAC to JACB readers: the abovementioned lecture by Andrews Walls, a transcript of an interview with Lamin Sanneh, and a selection from We Believe, an Early African commentary on the Nicene Creed. This work, written by Christopher Hall and commissioned by Oden, illustrates the intellectual and spiritual wisdom of the early African church. These resources affirm and complement Oden’s historiographical legacy.]"
JENNIFER BALAKRISHNAN,An effective Chabauty-Kim theorem,"The Chabauty–Kim method allows one to find rational points on curves under certain technical conditions, generalising Chabauty’s proof of the Mordell conjecture for curves with Mordell–Weil rank less than their genus. We show how the Chabauty–Kim method, when these technical conditions are satisfied in depth 2, may be applied to bound the number of rational points on a curve of higher rank. This provides a non-abelian generalisation of Coleman’s effective Chabauty theorem."
JENNIFER BALAKRISHNAN,Chabauty-Coleman experiments for genus 3 hyperelliptic curves,"We describe a computation of rational points on genus 3 hyperelliptic curves C defined over ℚ whose Jacobians have Mordell-Weil rank 1. Using the method of Chabauty and Coleman, we present and implement an algorithm in Sage to compute the zero locus of two Coleman integrals and analyze the finite set of points cut out by the vanishing of these integrals. We run the algorithm on approximately 17,000 curves from a forthcoming database of genus 3 hyperelliptic curves and discuss some interesting examples where the zero set includes global points not found in C(ℚ)."
JENNIFER BALAKRISHNAN,Computational tools for quadratic Chabauty,
JENNIFER BALAKRISHNAN,Quadratic Chabauty for modular curves: algorithms and examples,"We describe how the quadratic Chabauty method may be applied to determine the set of rational points on modular curves of genus g whose Jacobians have Mordell–Weil rank g. This extends our previous work on the split Cartan curve of level 13 and allows us to consider modular curves that may have few known rational points or nontrivial local height contributions away from our working prime. We illustrate our algorithms with a number of examples where we determine the set of rational points on several modular curves of genus 2 and 3: this includes Atkin–Lehner quotients X^+_0 (N) of prime level N, the curve X_S4 (13), as well as a few other curves relevant to Mazur’s Program B."
JENNIFER BALAKRISHNAN,Variants of Lehmer's speculation for newforms,
JENNIFER BALAKRISHNAN,Even values of Ramanujan’s tau-function,
JENNIFER BALAKRISHNAN,"Rx: the official, unabridged, ""how to"" guide to 1st year: 1988-1989",
JENNIFER BALAKRISHNAN,A tale of three curves,"In this snapshot, we give a survey of some problems in the study of rational points on higher genus curves, discussing questions ranging from the era of the ancient Greeks to a few posed by mathematicians of the 20th century. To answer these questions, we describe a selection of techniques in modern number theory that can be used to determine the set of rational points on a curve."
JENNIFER BALAKRISHNAN,Explicit Coleman integration for curves,"The Coleman integral is a p-adic line integral that plays a key role in computing several important invariants in arithmetic geometry. We give an algorithm for explicit Coleman integration on curves, using the algorithms of the second author [Tui16, Tui17] to compute the action of Frobenius on p-adic cohomology. We present a collection of examples computed with our implementation. This includes integrals on a genus 55 curve, where other methods do not currently seem practical."
JENNIFER BALAKRISHNAN,Explicit quadratic Chabauty over number fields,We generalize the explicit quadratic Chabauty techniques for integral points on odd degree hyperelliptic curves and for rational points on genus 2 bielliptic curves to arbitrary number fields using restriction of scalars. This is achieved by combining equations coming from Siksek’s extension of classical Chabauty with equations defined in terms of p-adic heights attached to independent continuous idele class characters. We give several examples to show the practicality of our methods.
JENNIFER BALAKRISHNAN,Two recent p-adic approaches towards the (effective) Mordell conjecture,
JENNIFER BALAKRISHNAN,Variants of Lehmer's speculation for newforms,
SAMUEL A ISAACSON,Uniform asymptotic approximation of diffusion to a small target: Generalized reaction models,"The diffusion of a reactant to a binding target plays a key role in many biological processes. The reaction radius at which the reactant and target may interact is often a small parameter relative to the diameter of the domain in which the reactant diffuses. We develop uniform in time asymptotic expansions in the reaction radius of the full solution to the corresponding diffusion equations for two separate reactant-target interaction mechanisms: the Doi or volume reactivity model and the Smoluchowski-Collins-Kimball partial-absorption surface reactivity model. In the former, the reactant and target react with a fixed probability per unit time when within a specified separation. In the latter, upon reaching a fixed separation, they probabilistically react or the reactant reflects away from the target. Expansions of the solution to each model are constructed by projecting out the contribution of the first eigenvalue and eigenfunction to the solution of the diffusion equation and then developing matched asymptotic expansions in Laplace-transform space. Our approach offers an equivalent, but alternative, method to the pseudopotential approach we previously employed [Isaacson and Newby, Phys. Rev. E 88, 012820 (2013)PLEEE81539-375510.1103/PhysRevE.88.012820] for the simpler Smoluchowski pure-absorption reaction mechanism. We find that the resulting asymptotic expansions of the diffusion equation solutions are identical with the exception of one parameter: the diffusion-limited reaction rates of the Doi and partial-absorption models. This demonstrates that for biological systems in which the reaction radius is a small parameter, properly calibrated Doi and partial-absorption models may be functionally equivalent."
SAMUEL A ISAACSON,An unstructured mesh convergent reaction-diffusion master equation for reversible reactions,"The convergent reaction-diffusion master equation (CRDME) was recently developed to provide a lattice particle-based stochastic reaction-diffusion model that is a convergent approximation in the lattice spacing to an underlying spatially-continuous particle dynamics model. The CRDME was designed to be identical to the popular lattice reaction-diffusion master equation (RDME) model for systems with only linear reactions, while overcoming the RDME's loss of bimolecular reaction effects as the lattice spacing is taken to zero. In our original work we developed the CRDME to handle bimolecular association reactions on Cartesian grids. In this work we develop several extensions to the CRDME to facilitate the modeling of cellular processes within realistic biological domains. Foremost, we extend the CRDME to handle reversible bimolecular reactions on unstructured grids. Here we develop a generalized CRDME through discretization of the spatially continuous volume reactivity model, extending the CRDME to encompass a larger variety of particle-particle interactions. Finally, we conclude by examining several numerical examples to demonstrate the convergence and accuracy of the CRDME in approximating the volume reactivity model."
SAMUEL A ISAACSON,Modeling genetic circuit behavior in transiently transfected mammalian cells,"Binning cells by plasmid copy number is a common practice for analyzing transient transfection data. In many kinetic models of transfected cells, protein production rates are assumed to be proportional to plasmid copy number. The validity of this assumption in transiently transfected mammalian cells is not clear; models based on this assumption appear unable to reproduce experimental flow cytometry data robustly. We hypothesize that protein saturation at high plasmid copy number is a reason previous models break down and validate our hypothesis by comparing experimental data and a stochastic chemical kinetics model. The model demonstrates that there are multiple distinct physical mechanisms that can cause saturation. On the basis of these observations, we develop a novel minimal bin-dependent ODE model that assumes different parameters for protein production in cells with low versus high numbers of plasmids. Compared to a traditional Hill-function-based model, the bin-dependent model requires only one additional parameter, but fits flow cytometry input-output data for individual modules up to twice as accurately. By composing together models of individually fit modules, we use the bin-dependent model to predict the behavior of six cascades and three feed-forward circuits. The bin-dependent models are shown to provide more accurate predictions on average than corresponding (composed) Hill-function-based models and predictions of comparable accuracy to EQuIP, while still providing a minimal ODE-based model that should be easy to integrate as a subcomponent within larger differential equation circuit models. Our analysis also demonstrates that accounting for batch effects is important in developing accurate composed models."
SAMUEL A ISAACSON,Biophysical assay for tethered signaling reactions reveals tether-controlled activity for the phosphatase SHP-1,"Tethered enzymatic reactions are ubiquitous in signaling networks but are poorly understood. A previously unreported mathematical analysis is established for tethered signaling reactions in surface plasmon resonance (SPR). Applying the method to the phosphatase SHP-1 interacting with a phosphorylated tether corresponding to an immune receptor cytoplasmic tail provides five biophysical/biochemical constants from a single SPR experiment: two binding rates, two catalytic rates, and a reach parameter. Tether binding increases the activity of SHP-1 by 900-fold through a binding-induced allosteric activation (20-fold) and a more significant increase in local substrate concentration (45-fold). The reach parameter indicates that this local substrate concentration is exquisitely sensitive to receptor clustering. We further show that truncation of the tether leads not only to a lower reach but also to lower binding and catalysis. This work establishes a new framework for studying tethered signaling processes and highlights the tether as a control parameter in clustered receptor signaling."
SAMUEL A ISAACSON,Strong intracellular signal inactivation produces sharper and more robust signaling from cell membrane to nucleus,"For a chemical signal to propagate across a cell, it must navigate a tortuous environment involving a variety of organelle barriers. In this work we study mathematical models for a basic chemical signal, the arrival times at the nuclear membrane of proteins that are activated at the cell membrane and diffuse throughout the cytosol. Organelle surfaces within human B cells are reconstructed from soft X-ray tomographic images, and modeled as reflecting barriers to the molecules’ diffusion. We show that signal inactivation sharpens signals, reducing variability in the arrival time at the nuclear membrane. Inactivation can also compensate for an observed slowdown in signal propagation induced by the presence of organelle barriers, leading to arrival times at the nuclear membrane that are comparable to models in which the cytosol is treated as an open, empty region. In the limit of strong signal inactivation this is achieved by filtering out molecules that traverse non-geodesic paths."
SAMUEL A ISAACSON,Mean field limits of particle-based stochastic reaction-diffusion models,"Particle-based stochastic reaction-diffusion (PBSRD) models are a popular approach for studying biological systems involving both noise in the reaction process and diffusive transport. In this work we derive coarse-grained deterministic partial integro-differential equation (PIDE) models that provide a mean field approximation to the volume reactivity PBSRD model, a model commonly used for studying cellular processes. We formulate a weak measure-valued stochastic process (MVSP) representation for the volume reactivity PBSRD model, demonstrating for a simplified but representative system that it is consistent with the commonly used Doi Fock Space representation of the corresponding forward equation. We then prove the convergence of the general volume reactivity model MVSP to the mean field PIDEs in the large-population (i.e. thermodynamic) limit."
SAMUEL A ISAACSON,How reaction-diffusion PDEs approximate the large-population limit of stochastic particle models,"Reaction-diffusion PDEs and particle-based stochastic reaction-diffusion (PBSRD) models are commonly-used approaches for modeling the spatial dynamics of chemical and biological systems. Standard reaction-diffusion PDE models ignore the underlying stochasticity of spatial transport and reactions, and are often described as appropriate in regimes where there are large numbers of particles in a system. Recent studies have proven the rigorous large-population limit of PBSRD models, showing the resulting mean-field models (MFM) correspond to non-local systems of partial-integro differential equations. In this work we explore the rigorous relationship between standard reaction-diffusion PDE models and the derived MFM. We prove that the former can be interpreted as an asymptotic approximation to the later in the limit that bimolecular reaction kernels are short-range and averaging. As the reactive interaction length scale approaches zero, we prove the MFMs converge at second order to standard reaction-diffusion PDE models. In proving this result we also establish local well-posedness of the MFM model in time for general systems, and global well-posedness for specific reaction systems and kernels. Finally, we illustrate the agreement and disagreement between the MFM, SM and the underlying particle model for several numerical examples."
SAMUEL A ISAACSON,Reactive boundary conditions as limits of interaction potentials for Brownian and Langevin dynamics,
SAMUEL A ISAACSON,The influence of molecular reach and diffusivity on the efficacy of membrane-confined reactions,"Signaling by surface receptors often relies on tethered reactions whereby an enzyme bound to the cytoplasmic tail of a receptor catalyzes reactions on substrates within reach. The overall length and stiffness of the receptor tail, the enzyme, and the substrate determine a biophysical parameter termed the molecular reach of the reaction. This parameter determines the probability that the receptor-tethered enzyme will contact the substrate in the volume proximal to the membrane when separated by different distances within the membrane plane. In this work, we develop particle-based stochastic reaction-diffusion models to study the interplay between molecular reach and diffusion. We find that increasing the molecular reach can increase reaction efficacy for slowly diffusing receptors, whereas for rapidly diffusing receptors, increasing molecular reach reduces reaction efficacy. In contrast, if reactions are forced to take place within the two-dimensional plasma membrane instead of the three-dimensional volume proximal to it or if molecules diffuse in three dimensions, increasing molecular reach increases reaction efficacy for all diffusivities. We show results in the context of immune checkpoint receptors (PD-1 dephosphorylating CD28), a standard opposing kinase-phosphatase reaction, and a minimal two-particle model. The work highlights the importance of the three-dimensional nature of many two-dimensional membrane-confined interactions, illustrating a role for molecular reach in control-ling biochemical reactions."
SAMUEL A ISAACSON,Catalyst: fast biochemical modeling with Julia,
SAMUEL A ISAACSON,How reaction-diffusion PDEs approximate the large-population limit of stochastic particle models,"Reaction-diffusion PDEs and particle-based stochastic reaction-diffusion (PBSRD) models are commonly used approaches for modeling the spatial dynamics of chemical and biological systems. Standard reaction-diffusion PDE models ignore the underlying stochasticity of spatial transport and reactions and are often described as appropriate in regimes where there are large numbers of particles in a system. Recent studies have proven the rigorous large-population limit of PBSRD models, showing the resulting mean-field models (MFMs) correspond to nonlocal systems of partial-integro differential equations. In this work we explore the rigorous relationship between standard reaction-diffusion PDE models and the derived MFM. We prove that the former can be interpreted as an asymptotic approximation to the later in the limit that bimolecular reaction kernels are short-range and averaging. As the reactive interaction length scale approaches zero, we prove the MFMs converge at second order to standard reaction-diffusion PDE models. In proving this result we also establish local well-posedness of the MFM model in time for general systems and global well-posedness for specific reaction systems and kernels. Finally, we illustrate the agreement and disagreement between the MFM, SM, and underlying particle model for several numerical examples."
SAMUEL A ISAACSON,How retroactivity affects the behavior of incoherent feedforward loops,"An incoherent feedforward loop (IFFL) is a network motif known for its ability to accelerate responses and generate pulses. It remains an open question to understand the behavior of IFFLs in contexts with high levels of retroactivity, where an upstream transcription factor binds to numerous downstream binding sites. Here we study the behavior of IFFLs by simulating and comparing ODE models with different levels of retroactivity. We find that increasing retroactivity in an IFFL can increase, decrease, or keep the network's response time and pulse amplitude constant. This suggests that increasing retroactivity, traditionally considered an impediment to designing robust synthetic systems, could be exploited to improve the performance of IFFLs. In contrast, we find that increasing retroactivity in a negative autoregulated circuit can only slow the response. The ability of an IFFL to flexibly handle retroactivity may have contributed to its significant abundance in both bacterial and eukaryotic regulatory networks."
SAMUEL A ISAACSON,Dephosphorylation accelerates the dissociation of ZAP70 from the T cell receptor,"Protein–protein binding domains are critical in signaling networks. Src homology 2 (SH2) domains are binding domains that interact with sequences containing phosphorylated tyrosines. A subset of SH2 domain–containing proteins has tandem domains, which are thought to enhance binding affinity and specificity. However, a trade-off exists between long-lived binding and the ability to rapidly reverse signaling, which is a critical requirement of noise-filtering mechanisms such as kinetic proofreading. Here, we use modeling to show that the unbinding rate of tandem, but not single, SH2 domains can be accelerated by phosphatases. Using surface plasmon resonance, we show that the phosphatase CD45 can accelerate the unbinding rate of zeta chain–associated protein kinase 70 (ZAP70), a tandem SH2 domain–containing kinase, from biphosphorylated peptides from the T cell receptor (TCR). An important functional prediction of accelerated unbinding is that the intracellular ZAP70–TCR half-life in T cells will not be fixed but rather, dependent on the extracellular TCR–antigen half-life, and we show that this is the case in both cell lines and primary T cells. The work highlights that tandem SH2 domains can break the trade-off between signal fidelity (requiring long half-life) and signal reversibility (requiring short half-life), which is a key requirement for T cell antigen discrimination mediated by kinetic proofreading."
SAMUEL A ISAACSON,Detailed balance for particle models of reversible reactions in bounded domains,"In particle-based stochastic reaction-diffusion models, reaction rates and placement kernels are used to decide the probability per time a reaction can occur between reactant particles and to decide where product particles should be placed. When choosing kernels to use in reversible reactions, a key constraint is to ensure that detailed balance of spatial reaction fluxes holds at all points at equilibrium. In this work, we formulate a general partial-integral differential equation model that encompasses several of the commonly used contact reactivity (e.g., Smoluchowski-Collins-Kimball) and volume reactivity (e.g., Doi) particle models. From these equations, we derive a detailed balance condition for the reversible A + B ⇆ C reaction. In bounded domains with no-flux boundary conditions, when choosing unbinding kernels consistent with several commonly used binding kernels, we show that preserving detailed balance of spatial reaction fluxes at all points requires spatially varying unbinding rate functions near the domain boundary. Brownian dynamics simulation algorithms can realize such varying rates through ignoring domain boundaries during unbinding and rejecting unbinding events that result in product particles being placed outside the domain."
SAMUEL A ISAACSON,Catalyst: Fast and flexible modeling of reaction networks,"We introduce Catalyst.jl, a flexible and feature-filled Julia library for modeling and high-performance simulation of chemical reaction networks (CRNs). Catalyst supports simulating stochastic chemical kinetics (jump process), chemical Langevin equation (stochastic differential equation), and reaction rate equation (ordinary differential equation) representations for CRNs. Through comprehensive benchmarks, we demonstrate that Catalyst simulation runtimes are often one to two orders of magnitude faster than other popular tools. More broadly, Catalyst acts as both a domain-specific language and an intermediate representation for symbolically encoding CRN models as Julia-native objects. This enables a pipeline of symbolically specifying, analyzing, and modifying CRNs; converting Catalyst models to symbolic representations of concrete mathematical models; and generating compiled code for numerical solvers. Leveraging ModelingToolkit.jl and Symbolics.jl, Catalyst models can be analyzed, simplified, and compiled into optimized representations for use in numerical solvers. Finally, we demonstrate Catalyst's broad extensibility and composability by highlighting how it can compose with a variety of Julia libraries, and how existing open-source biological modeling projects have extended its intermediate representation."
SAMUEL A ISAACSON,The molecular reach of antibodies determines their SARS-CoV-2 neutralisation potency,
ORRAN KRIEGER,HIL: designing an exokernel for the data center,"We propose a new Exokernel-like layer to allow mutually untrusting physically deployed services to efficiently share the resources of a data center. We believe that such a layer offers not only efficiency gains, but may also enable new economic models, new applications, and new security-sensitive uses. A prototype (currently in active use) demonstrates that the proposed layer is viable, and can support a variety of existing provisioning tools and use cases."
ORRAN KRIEGER,A secure cloud with minimal provider trust,"Bolted is a new architecture for a bare metal cloud with the goal of providing security-sensitive customers of a cloud the same level of security and control that they can obtain in their own private data centers. It allows tenants to elastically allocate secure resources within a cloud while being protected from other previous, current, and future tenants of the cloud. The provisioning of a new server to a tenant isolates a bare metal server, only allowing it to communicate with other tenant's servers once its critical firmware and software have been attested to the tenant. Tenants, rather than the provider, control the tradeoffs between security, price, and performance. A prototype demonstrates scalable end-to-end security with small overhead compared to a less secure alternative."
ORRAN KRIEGER,EbbRT: a customizable operating system for cloud applications,"Efficient use of hardware requires operating system components be customized to the application workload. Our general purpose operating systems are ill-suited for this task. We present Genesis, a new operating system that enables per-application customizations for cloud applications. Genesis achieves this through a novel heterogeneous distributed structure, a partitioned object model, and an event-driven execution environment. This paper describes the design and prototype implementation of Genesis, and evaluates its ability to improve the performance of common cloud applications. The evaluation of the Genesis prototype demonstrates memcached, run within a VM, can outperform memcached run on an unvirtualized Linux. The prototype evaluation also demonstrates an 14% performance improvement of a V8 JavaScript engine benchmark, and a node.js webserver that achieves a 50% reduction in 99th percentile latency compared to it run on Linux."
ORRAN KRIEGER,An Experiment on Bare-Metal BigData Provisioning,"Many BigData customers use on-demand platforms in the cloud, where they can get a dedicated virtual cluster in a couple of minutes and pay only for the time they use. Increasingly, there is a demand for bare-metal bigdata solutions for applications that cannot tolerate the unpredictability and performance degradation of virtualized systems. Existing bare-metal solutions can introduce delays of 10s of minutes to provision a cluster by installing operating systems and applications on the local disks of servers. This has motivated recent research developing sophisticated mechanisms to optimize this installation. These approaches assume that using network mounted boot disks incur unacceptable run-time overhead. Our analysis suggest that while this assumption is true for application data, it is incorrect for operating systems and applications, and network mounting the boot disk and applications result in negligible run-time impact while leading to faster provisioning time."
ORRAN KRIEGER,"Hardware as a service - enabling dynamic, user-level bare metal provisioning of pools of data center resources.","We describe a “Hardware as a Service (HaaS)” tool for isolating pools of compute, storage and networking resources. The goal of HaaS is to enable dynamic and flexible, user-level provisioning of pools of resources at the so-called “bare-metal” layer. It allows experimental or untrusted services to co-exist alongside trusted services. By functioning only as a resource isolation system, users are free to choose between different system scheduling and provisioning systems and to manage isolated resources as they see fit. We describe key HaaS use cases and features. We show how HaaS can provide a valuable, and somehwat overlooked, layer in the software architecture of modern data center management. Documentation and source code for HaaS software are available at: https://github.com/CCI-MOC/haas"
ORRAN KRIEGER,Total order broadcast for fault tolerant exascale systems,"In the process of designing a new fault tolerant run-time for future exascale systems, we discovered that a total order broadcast would be necessary. That is, nodes of a supercomputer should be able to broadcast messages to other nodes even in the face of failures. All messages should be seen in the same order at all nodes. While this is a well studied problem in distributed systems, few researchers have looked at how to perform total order broadcasts at large scales for data availability. Our experience implementing a published total order broadcast algorithm showed poor scalability at tens of nodes. In this paper we present a novel algorithm for total order broadcast which scales logarithmically in the number of processes and is not delayed by most process failures. While we are motivated by the needs of our run-time we believe this primitive is of general applicability. Total order broadcasts are used often in datacenter environments and as HPC developers begins to address fault tolerance at the application level we believe they will need similar primitives."
ORRAN KRIEGER,Recursive Hardware-as-a-Service (rHaaS) and Fast Provisioning,"Hardware as a Service (HaaS) is a new service being developed by the Massachusetts Open Cloud (MOC) to allow physical servers to be allocated to clients in the same way that virtual servers are in existing IaaS clouds. This poster describes the new recursive HaaS project and the fast provisioning customization we are developing. Recursive HaaS allows a HaaS service to be layered on top of an existing one. It will allow testing of new features at performance and scale without affecting the production service. It will also allow clients to host their own HaaS on top of a base HaaS to provide, potentially customized, services to their users. An example customization we are developing is a fast provisioning service that can be used between tenants that have some degree of trust in each other. It will allow nodes to be moved between customers (and a service installed) in seconds, rather than the minutes required by base HaaS."
ORRAN KRIEGER,Using OpenStack for an Open Cloud eXchange(OCX),"We are developing a new public cloud, the Massachusetts Open Cloud (MOC) based on the model of an Open Cloud eXchange (OCX). We discuss in this paper the vision of an OCX and how we intend to realize it using the OpenStack open-source cloud platform in the MOC. A limited form of an OCX can be achieved today by layering new services on top of OpenStack. We have performed an analysis of OpenStack to determine the changes needed in order to fully realize the OCX model. We describe these proposed changes, which although significant and requiring broad community involvement will provide functionality of value to both existing single-provider clouds as well as future multi-provider ones."
ORRAN KRIEGER,SEUSS: rapid serverless deployment using environment snapshots,"Modern FaaS systems perform well in the case of repeat executions when function working sets stay small. However, these platforms are less effective when applied to more complex, large-scale and dynamic workloads. In this paper, we introduce SEUSS (serverless execution via unikernel snapshot stacks), a new system-level approach for rapidly deploying serverless functions. Through our approach, we demonstrate orders of magnitude improvements in function start times and cacheability, which improves common re-execution paths while also unlocking previously-unsupported large-scale bursty workloads."
ORRAN KRIEGER,Caching in the multiverse,"To get good performance for data stored in Object storage services like S3, data analysis clusters need to cache data locally. Recently these caches have started taking into account higher-level information from analysis framework, allowing prefetching based on predictions of future data accesses. There is, however, a broader opportunity; rather than using this information to predict one future, we can use it to select a future that is best for caching. This paper provides preliminary evidence that we can exploit the directed acyclic graph (DAG) of inter-task dependencies used by data-parallel frameworks such as Spark, PIG and Hive to improve application performance, by optimizing caching for the critical path through the DAG for the application. We present experimental results for PIG running TPC-H queries, showing completion time improvements of up to 23% vs our implementation of MRD, a state-of-the-art DAG-based prefetching system, and improvements of up to 2.5x vs LRU caching. We then discuss the broader opportunity for building a system based on this opportunity."
ORRAN KRIEGER,MultiLibOS: an OS architecture for cloud computing,"Cloud computing is resulting in fundamental changes to computing infrastructure, yet these changes have not resulted in corresponding changes to operating systems. In this paper we discuss some key changes we see in the computing infrastructure and applications of IaaS systems. We argue that these changes enable and demand a very different model of operating system. We then describe the MulitLibOS architecture we are exploring and how it helps exploit the scale and elasticity of integrated systems while still allowing for legacy software run on traditional OSes."
ORRAN KRIEGER,Towards an open cloud marketplace: vision and first steps,"As one of the most promising, emerging concepts in Information Technology (IT), cloud computing is transforming how IT is consumed and managed; yielding improved cost efficiencies, and delivering flexible, on-demand scalability by reducing computing infrastructures, platforms, and services to commodities acquired and paid-for on-demand through a set of cloud providers. Today, the transition of cloud computing from a subject of research and innovation to a critical infrastructure is proceeding at an incredibly fast pace. A potentially dangerous consequence of this speedy transition to practice is the premature adoption, and ossification, of the models, technologies, and standards underlying this critical infrastructure. This state of affairs is exacerbated by the fact that innovative research on production-scale platforms is becoming the purview of a small number of public cloud providers. Specifically, the academic research communities are effectively excluded from the opportunity to contribute meaningfully to the evolution not to mention innovation and healthy mutation of cloud computing technologies. As the dependence on our society and economy on cloud computing increases, so does the realization that the academic research community cannot be shut out from contributing to the design and evolution of this critical infrastructure. In this article we provide an alternative vision that of an Open Cloud eXchange (OCX) a public cloud marketplace, where many stakeholders, rather than just a single cloud provider, participate in implementing and operating the cloud, thus creating an ecosystem that will bring the innovation of a broader community to bear on a much healthier and more efficient cloud marketplace."
ORRAN KRIEGER,Unikernels: the next stage of Linux’s dominance,"Unikernels have demonstrated enormous advantages over Linux in many important domains, causing some to propose that the days of Linux's dominance may be coming to an end. On the contrary, we believe that unikernels' advantages represent the next natural evolution for Linux, as it can adopt the best ideas from the unikernel approach and, along with its battle-tested codebase and large open source community, continue to dominate. In this paper, we posit that an upstreamable unikernel target is achievable from the Linux kernel, and, through an early Linux unikernel prototype, demonstrate that some simple changes can bring dramatic performance advantages."
ORRAN KRIEGER,Why elasticity matters,"In this paper we proposed a new research agenda focused on elasticity. We argued that elasticity is an important area of research and hypothesized that research in this area will lead to more efficient systems with less hoarding, new applications that exploit massive cloud resources elastically, and system software and libraries that will simplify the task of developing elastic applications. We discussed some of our thoughts on a top-to-bottom cloud-scale system focused on elasticity. We argued that such a system will require: 1) a HW/IaaS layer that can quickly reallocated resources to different applications, 2) an event driven model where resource demand flows from the high level layers as transparently as possible to the lowest level of the system, and 3) a model of modularity that allows layers to be overridden as necessary and provides applications with a component model that enables the base elasticity to be exploited by new and advanced applications."
ORRAN KRIEGER,EbbRT: Elastic Building Block Runtime - case studies,"We present a new systems runtime, EbbRT, for cloud hosted applications. EbbRT takes a different approach to the role operating systems play in cloud computing. It supports stitching application functionality across nodes running commodity OSs and nodes running specialized application specific software that only execute what is necessary to accelerate core functions of the application. In doing so, it allows tradeoffs between efficiency, developer productivity, and exploitation of elasticity and scale. EbbRT, as a software model, is a framework for constructing applications as collections of standard application software and Elastic Building Blocks (Ebbs). Elastic Building Blocks are components that encapsulate runtime software objects and are implemented to exploit the raw access, scale and elasticity of IaaS resources to accelerate critical application functionality. This paper presents the EbbRT architecture, our prototype and experimental evaluation of the prototype under three different application scenarios."
ORRAN KRIEGER,EbbRT: a framework for building per-application library operating systems,"Efficient use of high speed hardware requires operating system components be customized to the application work- load. Our general purpose operating systems are ill-suited for this task. We present EbbRT, a framework for constructing per-application library operating systems for cloud applications. The primary objective of EbbRT is to enable high-performance in a tractable and maintainable fashion. This paper describes the design and implementation of EbbRT, and evaluates its ability to improve the performance of common cloud applications. The evaluation of the EbbRT prototype demonstrates memcached, run within a VM, can outperform memcached run on an unvirtualized Linux. The prototype evaluation also demonstrates an 14% performance improvement of a V8 JavaScript engine benchmark, and a node.js webserver that achieves a 50% reduction in 99th percentile latency compared to it run on Linux."
ORRAN KRIEGER,EbbRT: Elastic Building Block Runtime - overview,"EbbRT provides a lightweight runtime that enables the construction of reusable, low-level system software which can integrate with existing, general purpose systems. It achieves this by providing a library that can be linked into a process on an existing OS, and as a small library OS that can be booted directly on an IaaS node."
ORRAN KRIEGER,Slowing down for performance and energy: an OS-centric study in network driven workloads,"This paper studies three fundamental aspects of an OS that impact the performance and energy efficiency of network processing: 1) batching, 2) processor energy settings, and 3) the logic and instructions of the OS networking paths. A network device’s interrupt delay feature is used to induce batching and processor frequency is manipulated to control the speed of instruction execution. A baremetal library OS is used to explore OS path specialization. This study shows how careful use of batching and interrupt delay results in 2X energy and performance improvements across different workloads. Surprisingly, we find polling can be made energy efficient and can result in gains up to 11X over baseline Linux. We developed a methodology and a set of tools to collect system data in order to understand how energy is impacted at a fine-grained granularity. This paper identifies a number of other novel findings that have implications in OS design for networked applications and suggests a path forward to consider energy as a focal point of systems research."
ORRAN KRIEGER,Towards non-intrusive software introspection and beyond,"Continuous verification and security analysis of software systems are of paramount importance to many organizations. The state-of-the-art for such operations implements agent-based approaches to inspect the provisioned software stack for security and compliance issues. However, this approach, which runs agents on the systems being analyzed, is vulnerable to some attacks, can incur substantial performance impact, and can introduce significant complexity. In this paper, we present the design and prototype implementation of a general-purpose approach for Non-intrusive Software Introspection (NSI). By adhering to NSI, organizations hosting in the cloud can as well control the software introspection workflow with reduced trust in the provider. Experimental analysis of real-world applications demonstrates that NSI presents a lightweight and scalable approach, and has a negligible impact on the performance of applications running on the instance being introspected."
JAMES JOHNSON,First M87 Event Horizon Telescope results. III. Data processing and calibration,"We present the calibration and reduction of Event Horizon Telescope (EHT) 1.3 mm radio wavelength observations of the supermassive black hole candidate at the center of the radio galaxy M87 and the quasar 3C 279, taken during the 2017 April 5–11 observing campaign. These global very long baseline interferometric observations include for the first time the highly sensitive Atacama Large Millimeter/submillimeter Array (ALMA); reaching an angular resolution of 25 μas, with characteristic sensitivity limits of ~1 mJy on baselines to ALMA and ~10 mJy on other baselines. The observations present challenges for existing data processing tools, arising from the rapid atmospheric phase fluctuations, wide recording bandwidth, and highly heterogeneous array. In response, we developed three independent pipelines for phase calibration and fringe detection, each tailored to the specific needs of the EHT. The final data products include calibrated total intensity amplitude and phase information. They are validated through a series of quality assurance tests that show consistency across pipelines and set limits on baseline systematic errors of 2% in amplitude and 1° in phase. The M87 data reveal the presence of two nulls in correlated flux density at ~3.4 and ~8.3 Gλ and temporal evolution in closure quantities, indicating intrinsic variability of compact structure on a timescale of days, or several light-crossing times for a few billion solar-mass black hole. These measurements provide the first opportunity to image horizon-scale structure in M87."
JAMES JOHNSON,First M87 Event Horizon Telescope results. V. Physical origin of the asymmetric ring,"The Event Horizon Telescope (EHT) has mapped the central compact radio source of the elliptical galaxy M87 at 1.3 mm with unprecedented angular resolution. Here we consider the physical implications of the asymmetric ring seen in the 2017 EHT data. To this end, we construct a large library of models based on general relativistic magnetohydrodynamic (GRMHD) simulations and synthetic images produced by general relativistic ray tracing. We compare the observed visibilities with this library and confirm that the asymmetric ring is consistent with earlier predictions of strong gravitational lensing of synchrotron emission from a hot plasma orbiting near the black hole event horizon. The ring radius and ring asymmetry depend on black hole mass and spin, respectively, and both are therefore expected to be stable when observed in future EHT campaigns. Overall, the observed image is consistent with expectations for the shadow of a spinning Kerr black hole as predicted by general relativity. If the black hole spin and M87's large scale jet are aligned, then the black hole spin vector is pointed away from Earth. Models in our library of non-spinning black holes are inconsistent with the observations as they do not produce sufficiently powerful jets. At the same time, in those models that produce a sufficiently powerful jet, the latter is powered by extraction of black hole spin energy through mechanisms akin to the Blandford-Znajek process. We briefly consider alternatives to a black hole for the central compact object. Analysis of existing EHT polarization data and data taken simultaneously at other wavelengths will soon enable new tests of the GRMHD models, as will future EHT campaigns at 230 and 345 GHz."
JAMES JOHNSON,First M87 Event Horizon Telescope results. VI. The shadow and mass of the central black hole,"We present measurements of the properties of the central radio source in M87 using Event Horizon Telescope data obtained during the 2017 campaign. We develop and fit geometric crescent models (asymmetric rings with interior brightness depressions) using two independent sampling algorithms that consider distinct representations of the visibility data. We show that the crescent family of models is statistically preferred over other comparably complex geometric models that we explore. We calibrate the geometric model parameters using general relativistic magnetohydrodynamic (GRMHD) models of the emission region and estimate physical properties of the source. We further fit images generated from GRMHD models directly to the data. We compare the derived emission region and black hole parameters from these analyses with those recovered from reconstructed images. There is a remarkable consistency among all methods and data sets. We find that >50% of the total flux at arcsecond scales comes from near the horizon, and that the emission is dramatically suppressed interior to this region by a factor >10, providing direct evidence of the predicted shadow of a black hole. Across all methods, we measure a crescent diameter of 42 ± 3 μas and constrain its fractional width to be <0.5. Associating the crescent feature with the emission surrounding the black hole shadow, we infer an angular gravitational radius of GM/Dc^2 = 3.8 ± 0.4 μas. Folding in a distance measurement of {16.8}_{-0.7}^{+0.8}{Mpc} gives a black hole mass of M = 6.5 ± 0.2{| }_{stat} ± 0.7{| }_{sys} × {10}^{9} {M}_{odot }. This measurement from lensed emission near the event horizon is consistent with the presence of a central Kerr black hole, as predicted by the general theory of relativity."
JAMES JOHNSON,The Event Horizon general relativistic magnetohydrodynamic code comparison project,"Recent developments in compact object astrophysics, especially the discovery of merging neutron stars by LIGO, the imaging of the black hole in M87 by the Event Horizon Telescope, and high- precision astrometry of the Galactic Center at close to the event horizon scale by the GRAVITY experiment motivate the development of numerical source models that solve the equations of general relativistic magnetohydrodynamics (GRMHD). Here we compare GRMHD solutions for the evolution of a magnetized accretion flow where turbulence is promoted by the magnetorotational instability from a set of nine GRMHD codes: Athena++, BHAC, Cosmos++, ECHO, H-AMR, iharm3D, HARM-Noble, IllinoisGRMHD, and KORAL. Agreement among the codes improves as resolution increases, as measured by a consistently applied, specially developed set of code performance metrics. We conclude that the community of GRMHD codes is mature, capable, and consistent on these test problems."
JAMES JOHNSON,"Trade in the balance: reconciling trade and climate policy: report of the Working Group on Trade, Investment, and Climate Policy","This report outlines the general tensions between the trade and investment regime and climate policy, and outlines a framework toward making trade and investment rules more climate friendly. Members of the working group have contributed short pieces addressing a range of issues related to the intersection of trade and climate policy. The first two are by natural scientists. Anthony Janetos discusses the need to address the effects of international trade on efforts to limit the increase in global annual temperature to no more than 2oC over preindustrial levels. James J. Corbett examines the failure of the Trans Pacific Partnership (TPP) and the Transatlantic Trade and Investment Partnership (TTIP) to adequately address the environmental implications of shipping and maritime transport. The next two pieces are by economists who examine economic aspects of the trade-climate linkage. Irene Monasterolo and Marco Raberto discuss the potential impacts of including fossil fuel subsidies reduction under the TTIP. Frank Ackerman explores the economic costs of efforts to promote convergence of regulatory standards between the United States and the European Union under the TTIP. The following two contributions are by legal scholars. Brooke Güven and Lise Johnson explore the potential for international investment treaties to redirect investment flows to support climate change mitigation and adaptation, particularly with regard to China and India. Matt Porterfield provides an overview of the ways in which both existing and proposed trade and investment agreements could have either “climate positive” or “climate negative” effects on mitigation policies. The final article is by Tao Hu, a former WTO trade and environment expert advisor for China and currently at the World Wildlife Fund, arguing that the definition of environmental goods and services’ under the WTO negotiations needs to be expanded to better incorporate climate change."
JAMES JOHNSON,Gravitational test beyond the first post-Newtonian order with the shadow of the M87 black hole,"The 2017 Event Horizon Telescope (EHT) observations of the central source in M87 have led to the first measurement of the size of a black-hole shadow. This observation offers a new and clean gravitational test of the black-hole metric in the strong-field regime. We show analytically that spacetimes that deviate from the Kerr metric but satisfy weak-field tests can lead to large deviations in the predicted black-hole shadows that are inconsistent with even the current EHT measurements. We use numerical calculations of regular, parametric, non-Kerr metrics to identify the common characteristic among these different parametrizations that control the predicted shadow size. We show that the shadow-size measurements place significant constraints on deviation parameters that control the second post-Newtonian and higher orders of each metric and are, therefore, inaccessible to weak-field tests. The new constraints are complementary to those imposed by observations of gravitational waves from stellar-mass sources."
JAMES JOHNSON,First M87 Event Horizon Telescope results. IV. Imaging the central supermassive black hole,
JAMES JOHNSON,Circularity in fisheries data weakens real world prediction,"The systematic substitution of direct observational data with synthesized data derived from models during the stock assessment process has emerged as a low-cost alternative to direct data collection efforts. What is not widely appreciated, however, is how the use of such synthesized data can overestimate predictive skill when forecasting recruitment is part of the assessment process. Using a global database of stock assessments, we show that Standard Fisheries Models (SFMs) can successfully predict synthesized data based on presumed stock-recruitment relationships, however, they are generally less skillful at predicting observational data that are either raw or minimally filtered (denoised without using explicit stock-recruitment models). Additionally, we find that an equation-free approach that does not presume a specific stock-recruitment relationship is better than SFMs at predicting synthesized data, and moreover it can also predict observational recruitment data very well. Thus, while synthesized datasets are cheaper in the short term, they carry costs that can limit their utility in predicting real world recruitment."
JAMES JOHNSON,Verification of radiative transfer schemes for the EHT,"The Event Horizon Telescope (EHT) Collaboration has recently produced the first resolved images of the central supermassive black hole in the giant elliptical galaxy M87. Here we report on tests of the consistency and accuracy of the general relativistic radiative transfer codes used within the collaboration to model M87* and Sgr A*. We compare and evaluate (1) deflection angles for equatorial null geodesics in a Kerr spacetime; (2) images calculated from a series of simple, parameterized matter distributions in the Kerr metric using simplified emissivities and absorptivities; (3) for a subset of codes, images calculated from general relativistic magnetohydrodynamics simulations using different realistic synchrotron emissivities and absorptivities; (4) observables for the 2017 configuration of EHT, including visibility amplitudes and closure phases. The error in total flux is of order 1% when the codes are run with production numerical parameters. The dominant source of discrepancies for small camera distances is the location and detailed setup of the software ""camera"" that each code uses to produce synthetic images. We find that when numerical parameters are suitably chosen and the camera is sufficiently far away the images converge and that for given transfer coefficients, numerical uncertainties are unlikely to limit parameter estimation for the current generation of EHT observations. The purpose of this paper is to describe a verification and comparison of EHT radiative transfer codes. It is not to verify EHT models more generally."
JAMES JOHNSON,Monitoring the mmorphology of M87* in 2009–2017 with the Event Horizon Telescope,"The Event Horizon Telescope (EHT) has recently delivered the first resolved images of M87*, the supermassive black hole in the center of the M87 galaxy. These images were produced using 230 GHz observations performed in April 2017. Additional observations are required to investigate the persistence of the primary image feature – a ring with azimuthal brightness asymmetry – and to quantify the image variability on event horizon scales. To address this need, we analyze M87* data collected with prototype EHT arrays in 2009, 2011, 2012, and 2013. While these observations do not contain enough information to produce images, they are sufficient to constrain simple geometric models. We develop a modeling approach based on the framework utilized for the 2017 EHT data analysis and validate our procedures using synthetic data. Applying the same approach to the observational data sets, we find the M87* morphology in 2009–2017 to be consistent with a persistent asymmetric ring of 40 as diameter. The position angle of peak intensity varies in time. In particular, we find a significant difference between the position angle measured in 2013 and 2017. These variations are in broad agreement with predictions of a subset of general relativistic magnetohydrodynamic simulations. We show that quantifying the variability across multiple observational epochs has the potential to constrain physical properties of the source, such as the accretion state or the black hole spin."
JAMES JOHNSON,THEMIS: a parameter estimation framework for the Event Horizon Telescope,"The Event Horizon Telescope (EHT) provides the unprecedented ability to directly resolve the structure and dynamics of black hole emission regions on scales smaller than their horizons. This has the potential to critically probe the mechanisms by which black holes accrete and launch outflows, and the structure of supermassive black hole spacetimes. However, accessing this information is a formidable analysis challenge for two reasons. First, the EHT natively produces a variety of data types that encode information about the image structure in nontrivial ways; these are subject to a variety of systematic effects associated with very long baseline interferometry and are supplemented by a wide variety of auxiliary data on the primary EHT targets from decades of other observations. Second, models of the emission regions and their interaction with the black hole are complex, highly uncertain, and computationally expensive to construct. As a result, the scientific utilization of EHT observations requires a flexible, extensible, and powerful analysis framework. We present such a framework, Themis, which defines a set of interfaces between models, data, and sampling algorithms that facilitates future development. We describe the design and currently existing components of Themis, how Themis has been validated thus far, and present additional analyses made possible by Themis that illustrate its capabilities. Importantly, we demonstrate that Themis is able to reproduce prior EHT analyses, extend these, and do so in a computationally efficient manner that can efficiently exploit modern high-performance computing facilities. Themis has already been used extensively in the scientific analysis and interpretation of the first EHT observations of M87."
JAMES JOHNSON,First Sagittarius A* Event Horizon Telescope results. V. Testing astrophysical models of the galactic center black hole,"In this paper we provide a first physical interpretation for the Event Horizon Telescope's (EHT) 2017 observations of Sgr A*. Our main approach is to compare resolved EHT data at 230 GHz and unresolved non-EHT observations from radio to X-ray wavelengths to predictions from a library of models based on time-dependent general relativistic magnetohydrodynamics simulations, including aligned, tilted, and stellar-wind-fed simulations; radiative transfer is performed assuming both thermal and nonthermal electron distribution functions. We test the models against 11 constraints drawn from EHT 230 GHz data and observations at 86 GHz, 2.2 μm, and in the X-ray. All models fail at least one constraint. Light-curve variability provides a particularly severe constraint, failing nearly all strongly magnetized (magnetically arrested disk (MAD)) models and a large fraction of weakly magnetized models. A number of models fail only the variability constraints. We identify a promising cluster of these models, which are MAD and have inclination i ≤ 30°. They have accretion rate (5.2–9.5) × 10−9 M ⊙ yr−1, bolometric luminosity (6.8–9.2) × 1035 erg s−1, and outflow power (1.3–4.8) × 1038 erg s−1. We also find that all models with i ≥ 70° fail at least two constraints, as do all models with equal ion and electron temperature; exploratory, nonthermal model sets tend to have higher 2.2 μm flux density; and the population of cold electrons is limited by X-ray constraints due to the risk of bremsstrahlung overproduction. Finally, we discuss physical and numerical limitations of the models, highlighting the possible importance of kinetic effects and duration of the simulations."
JAMES JOHNSON,First M87 Event Horizon Telescope results. VII. Polarization of the ring,"In 2017 April, the Event Horizon Telescope (EHT) observed the near-horizon region around the supermassive black hole at the core of the M87 galaxy. These 1.3 mm wavelength observations revealed a compact asymmetric ring-like source morphology. This structure originates from synchrotron emission produced by relativistic plasma located in the immediate vicinity of the black hole. Here we present the corresponding linear-polarimetric EHT images of the center of M87. We find that only a part of the ring is significantly polarized. The resolved fractional linear polarization has a maximum located in the southwest part of the ring, where it rises to the level of ∼15%. The polarization position angles are arranged in a nearly azimuthal pattern. We perform quantitative measurements of relevant polarimetric properties of the compact emission and find evidence for the temporal evolution of the polarized source structure over one week of EHT observations. The details of the polarimetric data reduction and calibration methodology are provided. We carry out the data analysis using multiple independent imaging and modeling techniques, each of which is validated against a suite of synthetic data sets. The gross polarimetric structure and its apparent evolution with time are insensitive to the method used to reconstruct the image. These polarimetric images carry information about the structure of the magnetic fields responsible for the synchrotron emission. Their physical interpretation is discussed in an accompanying publication."
JAMES JOHNSON,First M87 Event Horizon Telescope results. VIII. Magnetic field structure near The Event Horizon,"Event Horizon Telescope (EHT) observations at 230 GHz have now imaged polarized emission around the supermassive black hole in M87 on event-horizon scales. This polarized synchrotron radiation probes the structure of magnetic fields and the plasma properties near the black hole. Here we compare the resolved polarization structure observed by the EHT, along with simultaneous unresolved observations with the Atacama Large Millimeter/submillimeter Array, to expectations from theoretical models. The low fractional linear polarization in the resolved image suggests that the polarization is scrambled on scales smaller than the EHT beam, which we attribute to Faraday rotation internal to the emission region. We estimate the average density n_e ∼ 10^4–7 cm^−3, magnetic field strength B ∼ 1–30 G, and electron temperature T_e ∼ (1–12) × 10^10 K of the radiating plasma in a simple one-zone emission model. We show that the net azimuthal linear polarization pattern may result from organized, poloidal magnetic fields in the emission region. In a quantitative comparison with a large library of simulated polarimetric images from general relativistic magnetohydrodynamic (GRMHD) simulations, we identify a subset of physical models that can explain critical features of the polarimetric EHT observations while producing a relativistic jet of sufficient power. The consistent GRMHD models are all of magnetically arrested accretion disks, where near-horizon magnetic fields are dynamically important. We use the models to infer a mass accretion rate onto the black hole in M87 of (3–20) × 10^−4 M ⊙ yr^−1."
JAMES JOHNSON,Resolving the inner parsec of the blazar J1924–2914 with the event horizon telescope,"The blazar J1924–2914 is a primary Event Horizon Telescope (EHT) calibrator for the Galactic center’s black hole Sagittarius A*. Here we present the first total and linearly polarized intensity images of this source obtained with the unprecedented 20 μas resolution of the EHT. J1924–2914 is a very compact flat-spectrum radio source with strong optical variability and polarization. In April 2017 the source was observed quasi-simultaneously with the EHT (April 5–11), the Global Millimeter VLBI Array (April 3), and the Very Long Baseline Array (April 28), giving a novel view of the source at four observing frequencies, 230, 86, 8.7, and 2.3 GHz. These observations probe jet properties from the subparsec to 100 pc scales. We combine the multifrequency images of J1924–2914 to study the source morphology. We find that the jet exhibits a characteristic bending, with a gradual clockwise rotation of the jet projected position angle of about 90° between 2.3 and 230 GHz. Linearly polarized intensity images of J1924–2914 with the extremely fine resolution of the EHT provide evidence for ordered toroidal magnetic fields in the blazar compact core."
JAMES JOHNSON,A universal power-law prescription for variability from synthetic images of black hole accretion flows,"We present a framework for characterizing the spatiotemporal power spectrum of the variability expected from the horizon-scale emission structure around supermassive black holes, and we apply this framework to a library of general relativistic magnetohydrodynamic (GRMHD) simulations and associated general relativistic ray-traced images relevant for Event Horizon Telescope (EHT) observations of Sgr A*. We find that the variability power spectrum is generically a red-noise process in both the temporal and spatial dimensions, with the peak in power occurring on the longest timescales and largest spatial scales. When both the time-averaged source structure and the spatially integrated light-curve variability are removed, the residual power spectrum exhibits a universal broken power-law behavior. On small spatial frequencies, the residual power spectrum rises as the square of the spatial frequency and is proportional to the variance in the centroid of emission. Beyond some peak in variability power, the residual power spectrum falls as that of the time-averaged source structure, which is similar across simulations; this behavior can be naturally explained if the variability arises from a multiplicative random field that has a steeper high-frequency power-law index than that of the time-averaged source structure. We briefly explore the ability of power spectral variability studies to constrain physical parameters relevant for the GRMHD simulations, which can be scaled to provide predictions for black holes in a range of systems in the optically thin regime. We present specific expectations for the behavior of the M87* and Sgr A* accretion flows as observed by the EHT."
JAMES JOHNSON,Millimeter light curves of Sagittarius A* observed during the 2017 Event Horizon Telescope campaign,"The Event Horizon Telescope (EHT) observed the compact radio source, Sagittarius A* (Sgr A*), in the Galactic Center on 2017 April 5–11 in the 1.3 mm wavelength band. At the same time, interferometric array data from the Atacama Large Millimeter/submillimeter Array and the Submillimeter Array were collected, providing Sgr A* light curves simultaneous with the EHT observations. These data sets, complementing the EHT very long baseline interferometry, are characterized by a cadence and signal-to-noise ratio previously unattainable for Sgr A* at millimeter wavelengths, and they allow for the investigation of source variability on timescales as short as a minute. While most of the light curves correspond to a low variability state of Sgr A*, the April 11 observations follow an X-ray flare and exhibit strongly enhanced variability. All of the light curves are consistent with a red-noise process, with a power spectral density (PSD) slope measured to be between −2 and −3 on timescales between 1 minute and several hours. Our results indicate a steepening of the PSD slope for timescales shorter than 0.3 hr. The spectral energy distribution is flat at 220 GHz, and there are no time lags between the 213 and 229 GHz frequency bands, suggesting low optical depth for the event horizon scale source. We characterize Sgr A*’s variability, highlighting the different behavior observed just after the X-ray flare, and use Gaussian process modeling to extract a decorrelation timescale and a PSD slope. We also investigate the systematic calibration uncertainties by analyzing data from independent data reduction pipelines."
JAMES JOHNSON,Selective dynamical imaging of interferometric data,"Recent developments in very long baseline interferometry (VLBI) have made it possible for the Event Horizon Telescope (EHT) to resolve the innermost accretion flows of the largest supermassive black holes on the sky. The sparse nature of the EHT’s (u, v)-coverage presents a challenge when attempting to resolve highly time-variable sources. We demonstrate that the changing (u, v)-coverage of the EHT can contain regions of time over the course of a single observation that facilitate dynamical imaging. These optimal time regions typically have projected baseline distributions that are approximately angularly isotropic and radially homogeneous. We derive a metric of coverage quality based on baseline isotropy and density that is capable of ranking array configurations by their ability to produce accurate dynamical reconstructions. We compare this metric to existing metrics in the literature and investigate their utility by performing dynamical reconstructions on synthetic data from simulated EHT observations of sources with simple orbital variability. We then use these results to make recommendations for imaging the 2017 EHT Sgr A* data set."
JAMES JOHNSON,"Bostonia: v. 14, no. 1-10",
JAMES JOHNSON,First Sagittarius A* Event Horizon Telescope results. VI. Testing the black hole metric,"Astrophysical black holes are expected to be described by the Kerr metric. This is the only stationary, vacuum, axisymmetric metric, without electromagnetic charge, that satisfies Einstein’s equations and does not have pathologies outside of the event horizon. We present new constraints on potential deviations from the Kerr prediction based on 2017 EHT observations of Sagittarius A* (Sgr A*). We calibrate the relationship between the geometrically defined black hole shadow and the observed size of the ring-like images using a library that includes both Kerr and non-Kerr simulations. We use the exquisite prior constraints on the mass-to-distance ratio for Sgr A* to show that the observed image size is within ∼10% of the Kerr predictions. We use these bounds to constrain metrics that are parametrically different from Kerr, as well as the charges of several known spacetimes. To consider alternatives to the presence of an event horizon, we explore the possibility that Sgr A* is a compact object with a surface that either absorbs and thermally reemits incident radiation or partially reflects it. Using the observed image size and the broadband spectrum of Sgr A*, we conclude that a thermal surface can be ruled out and a fully reflective one is unlikely. We compare our results to the broader landscape of gravitational tests. Together with the bounds found for stellar-mass black holes and the M87 black hole, our observations provide further support that the external spacetimes of all black holes are described by the Kerr metric, independent of their mass."
JAMES JOHNSON,Polarimetric properties of Event Horizon Telescope targets from ALMA,"We present the results from a full polarization study carried out with the Atacama Large Millimeter/submillimeter Array (ALMA) during the first Very Long Baseline Interferometry (VLBI) campaign, which was conducted in 2017 April in the λ3 mm and λ1.3 mm bands, in concert with the Global mm-VLBI Array (GMVA) and the Event Horizon Telescope (EHT), respectively. We determine the polarization and Faraday properties of all VLBI targets, including Sgr A*, M87, and a dozen radio-loud active galactic nuclei (AGNs), in the two bands at several epochs in a time window of 10 days. We detect high linear polarization fractions (2%–15%) and large rotation measures (RM &gt; 103.3–105.5 rad m−2), confirming the trends of previous AGN studies at millimeter wavelengths. We find that blazars are more strongly polarized than other AGNs in the sample, while exhibiting (on average) order-of-magnitude lower RM values, consistent with the AGN viewing angle unification scheme. For Sgr A* we report a mean RM of (−4.2 ± 0.3) × 105 rad m−2 at 1.3 mm, consistent with measurements over the past decade and, for the first time, an RM of (–2.1 ± 0.1) × 105 rad m−2 at 3 mm, suggesting that about half of the Faraday rotation at 1.3 mm may occur between the 3 mm photosphere and the 1.3 mm source. We also report the first unambiguous measurement of RM toward the M87 nucleus at millimeter wavelengths, which undergoes significant changes in magnitude and sign reversals on a one year timescale, spanning the range from −1.2 to 0.3 × 105 rad m−2 at 3 mm and −4.1 to 1.5 × 105 rad m−2 at 1.3 mm. Given this time variability, we argue that, unlike the case of Sgr A*, the RM in M87 does not provide an accurate estimate of the mass accretion rate onto the black hole. We put forward a two-component model, comprised of a variable compact region and a static extended region, that can simultaneously explain the polarimetric properties observed by both the EHT (on horizon scales) and ALMA (which observes the combined emission from both components). These measurements provide critical constraints for the calibration, analysis, and interpretation of simultaneously obtained VLBI data with the EHT and GMVA."
JAMES JOHNSON,"First Sagittarius A* Event Horizon Telescope results. IV. Variability, morphology, and black hole mass","In this paper we quantify the temporal variability and image morphology of the horizon-scale emission from Sgr A*, as observed by the EHT in 2017 April at a wavelength of 1.3 mm. We find that the Sgr A* data exhibit variability that exceeds what can be explained by the uncertainties in the data or by the effects of interstellar scattering. The magnitude of this variability can be a substantial fraction of the correlated flux density, reaching ∼100% on some baselines. Through an exploration of simple geometric source models, we demonstrate that ring-like morphologies provide better fits to the Sgr A* data than do other morphologies with comparable complexity. We develop two strategies for fitting static geometric ring models to the time-variable Sgr A* data; one strategy fits models to short segments of data over which the source is static and averages these independent fits, while the other fits models to the full data set using a parametric model for the structural variability power spectrum around the average source structure. Both geometric modeling and image-domain feature extraction techniques determine the ring diameter to be 51.8 ± 2.3 μas (68% credible intervals), with the ring thickness constrained to have an FWHM between ∼30% and 50% of the ring diameter. To bring the diameter measurements to a common physical scale, we calibrate them using synthetic data generated from GRMHD simulations. This calibration constrains the angular size of the gravitational radius to be 4.8_-0.7^+1.4 μas, which we combine with an independent distance measurement from maser parallaxes to determine the mass of Sgr A* to be 4.0_-0.6^+10^6 M⊙."
JAMES JOHNSON,ICAT: a novel algorithm to robustly identify cell states following perturbations in single cell transcriptomes,
JAMES JOHNSON,"Twenty questions about design behavior for sustainability, report of the International Expert Panel on behavioral science for design","How behavioral scientists, engineers, and architects can work together to advance how we all understand and practice design—in order to enhance sustainability in the built environment, and beyond."
JAMES JOHNSON,Proceedings of the Sixth International Workshop on Web Caching and Content Distribution,"OVERVIEW: The International Web Content Caching and Distribution Workshop (WCW) is a premiere technical meeting for researchers and practitioners interested in all aspects of content caching, distribution and delivery on the Internet. The 2001 WCW meeting was held on the Boston University Campus. Building on the successes of the five previous WCW meetings, WCW01 featured a strong technical program and record participation from leading researchers and practitioners in the field. This report includes all the technical papers presented at WCW'01. NOTE: Proceedings of WCW'01 are published by Elsevier. Hard copies of these proceedings can be purchased through the workshop organizers. As a service to the community, electronic copies of all WCW'01 papers are accessible through Technical Report BUCS‐TR‐2001‐017, available from the Boston University Computer Science Technical Report Archives at http://www.cs.bu.edu/techreps. [Ed.note: URL outdated. Use http://www.bu.edu/cs/research/technical-reports or http://hdl.handle.net/2144/1455 in this repository to access the reports.]"
JAMES JOHNSON,The eighteenth data release of the Sloan Digital Sky Surveys: targeting and first spectra from SDSS-V,"The eighteenth data release (DR18) of the Sloan Digital Sky Survey (SDSS) is the first one for SDSS-V, the fifth generation of the survey. SDSS-V comprises three primary scientific programs or “Mappers”: the Milky Way Mapper (MWM), the Black Hole Mapper (BHM), and the Local Volume Mapper. This data release contains extensive targeting information for the two multiobject spectroscopy programs (MWM and BHM), including input catalogs and selection functions for their numerous scientific objectives. We describe the production of the targeting databases and their calibration and scientifically focused components. DR18 also includes ∼25,000 new SDSS spectra and supplemental information for X-ray sources identified by eROSITA in its eFEDS field. We present updates to some of the SDSS software pipelines and preview changes anticipated for DR19. We also describe three value-added catalogs (VACs) based on SDSS-IV data that have been published since DR17, and one VAC based on the SDSS-V data in the eFEDS field."
JAMES JOHNSON,"First Sagittarius A* Event Horizon Telescope results. II. EHT and multiwavelength observations, data processing, and calibration","We present Event Horizon Telescope (EHT) 1.3 mm measurements of the radio source located at the position of the supermassive black hole Sagittarius A* (Sgr A*), collected during the 2017 April 5–11 campaign. The observations were carried out with eight facilities at six locations across the globe. Novel calibration methods are employed to account for Sgr A*'s flux variability. The majority of the 1.3 mm emission arises from horizon scales, where intrinsic structural source variability is detected on timescales of minutes to hours. The effects of interstellar scattering on the image and its variability are found to be subdominant to intrinsic source structure. The calibrated visibility amplitudes, particularly the locations of the visibility minima, are broadly consistent with a blurred ring with a diameter of ∼50 μas, as determined in later works in this series. Contemporaneous multiwavelength monitoring of Sgr A* was performed at 22, 43, and 86 GHz and at near-infrared and X-ray wavelengths. Several X-ray flares from Sgr A* are detected by Chandra, one at low significance jointly with Swift on 2017 April 7 and the other at higher significance jointly with NuSTAR on 2017 April 11. The brighter April 11 flare is not observed simultaneously by the EHT but is followed by a significant increase in millimeter flux variability immediately after the X-ray outburst, indicating a likely connection in the emission physics near the event horizon. We compare Sgr A*’s broadband flux during the EHT campaign to its historical spectral energy distribution and find that both the quiescent emission and flare emission are consistent with its long-term behavior."
JAMES JOHNSON,ICAT: a novel algorithm to robustly identify cell states following perturbations in single-cell transcriptomes,"MOTIVATION: The detection of distinct cellular identities is central to the analysis of single-cell RNA sequencing (scRNA-seq) experiments. However, in perturbation experiments, current methods typically fail to correctly match cell states between conditions or erroneously remove population substructure. Here, we present the novel, unsupervised algorithm Identify Cell states Across Treatments (ICAT) that employs self-supervised feature weighting and control-guided clustering to accurately resolve cell states across heterogeneous conditions. RESULTS: Using simulated and real datasets, we show ICAT is superior in identifying and resolving cell states compared with current integration workflows. While requiring no a priori knowledge of extant cell states or discriminatory marker genes, ICAT is robust to low signal strength, high perturbation severity, and disparate cell type proportions. We empirically validate ICAT in a developmental model and find that only ICAT identifies a perturbation-unique cellular response. Taken together, our results demonstrate that ICAT offers a significant improvement in defining cellular responses to perturbation in scRNA-seq data. AVAILABILITY AND IMPLEMENTATION: https://github.com/BradhamLab/icat."
JAMES JOHNSON,The polarized image of a synchrotron-emitting ring of gas orbiting a black hole,"Synchrotron radiation from hot gas near a black hole results in a polarized image. The image polarization is determined by effects including the orientation of the magnetic field in the emitting region, relativistic motion of the gas, strong gravitational lensing by the black hole, and parallel transport in the curved spacetime. We explore these effects using a simple model of an axisymmetric, equatorial accretion disk around a Schwarzschild black hole. By using an approximate expression for the null geodesics derived by Beloborodov and conservation of the Walker–Penrose constant, we provide analytic estimates for the image polarization. We test this model using currently favored general relativistic magnetohydrodynamic simulations of M87*, using ring parameters given by the simulations. For a subset of these with modest Faraday effects, we show that the ring model broadly reproduces the polarimetric image morphology. Our model also predicts the polarization evolution for compact flaring regions, such as those observed from Sgr A* with GRAVITY. With suitably chosen parameters, our simple model can reproduce the EVPA pattern and relative polarized intensity in Event Horizon Telescope images of M87*. Under the physically motivated assumption that the magnetic field trails the fluid velocity, this comparison is consistent with the clockwise rotation inferred from total intensity images."
JAMES JOHNSON,First Sagittarius A* Event Horizon Telescope results. I. The shadow of the supermassive black hole in the center of the Milky Way,"We present the first Event Horizon Telescope (EHT) observations of Sagittarius A* (Sgr A*), the Galactic center source associated with a supermassive black hole. These observations were conducted in 2017 using a global interferometric array of eight telescopes operating at a wavelength of λ = 1.3 mm. The EHT data resolve a compact emission region with intrahour variability. A variety of imaging and modeling analyses all support an image that is dominated by a bright, thick ring with a diameter of 51.8 ± 2.3 μas (68% credible interval). The ring has modest azimuthal brightness asymmetry and a comparatively dim interior. Using a large suite of numerical simulations, we demonstrate that the EHT images of Sgr A* are consistent with the expected appearance of a Kerr black hole with mass ∼4 × 106 M ⊙, which is inferred to exist at this location based on previous infrared observations of individual stellar orbits, as well as maser proper-motion studies. Our model comparisons disfavor scenarios where the black hole is viewed at high inclination (i &gt; 50°), as well as nonspinning black holes and those with retrograde accretion disks. Our results provide direct evidence for the presence of a supermassive black hole at the center of the Milky Way, and for the first time we connect the predictions from dynamical measurements of stellar orbits on scales of 103–105 gravitational radii to event-horizon-scale images and variability. Furthermore, a comparison with the EHT results for the supermassive black hole M87* shows consistency with the predictions of general relativity spanning over three orders of magnitude in central mass."
JAMES JOHNSON,The variability of the black hole image in M87 at the dynamical timescale,"The black hole images obtained with the Event Horizon Telescope (EHT) are expected to be variable at the dynamical timescale near their horizons. For the black hole at the center of the M87 galaxy, this timescale (5–61 days) is comparable to the 6 day extent of the 2017 EHT observations. Closure phases along baseline triangles are robust interferometric observables that are sensitive to the expected structural changes of the images but are free of station-based atmospheric and instrumental errors. We explored the day-to-day variability in closure-phase measurements on all six linearly independent nontrivial baseline triangles that can be formed from the 2017 observations. We showed that three triangles exhibit very low day-to-day variability, with a dispersion of ∼3°–5°. The only triangles that exhibit substantially higher variability (∼90°–180°) are the ones with baselines that cross the visibility amplitude minima on the u–v plane, as expected from theoretical modeling. We used two sets of general relativistic magnetohydrodynamic simulations to explore the dependence of the predicted variability on various black hole and accretion-flow parameters. We found that changing the magnetic field configuration, electron temperature model, or black hole spin has a marginal effect on the model consistency with the observed level of variability. On the other hand, the most discriminating image characteristic of models is the fractional width of the bright ring of emission. Models that best reproduce the observed small level of variability are characterized by thin ring-like images with structures dominated by gravitational lensing effects and thus least affected by turbulence in the accreting plasmas."
JAMES JOHNSON,Constraints on black-hole charges with the 2017 EHT observations of M87*,
JAMES JOHNSON,Prospects for beyond the standard model physics searches at the deep underground neutrino experiment: DUNE collaboration,"The Deep Underground Neutrino Experiment (DUNE) will be a powerful tool for a variety of physics topics. The high-intensity proton beams provide a large neutrino flux, sampled by a near detector system consisting of a combination of capable precision detectors, and by the massive far detector system located deep underground. This configuration sets up DUNE as a machine for discovery, as it enables opportunities not only to perform precision neutrino measurements that may uncover deviations from the present three-flavor mixing paradigm, but also to discover new particles and unveil new interactions and symmetries beyond those predicted in the Standard Model (SM). Of the many potential beyond the Standard Model (BSM) topics DUNE will probe, this paper presents a selection of studies quantifying DUNE's sensitivities to sterile neutrino mixing, heavy neutral leptons, non-standard interactions, CPT symmetry violation, Lorentz invariance violation, neutrino trident production, dark matter from both beam induced and cosmogenic sources, baryon number violation, and other new physics topics that complement those at high-energy colliders and significantly extend the present reach."
JAMES JOHNSON,Identification and reconstruction of low-energy electrons in the ProtoDUNE-SP detector,
JAMES JOHNSON,SYMBA: an end-to-end VLBI synthetic data generation pipeline,"CONTEXT: Realistic synthetic observations of theoretical source models are essential for our understanding of real observational data. In using synthetic data, one can verify the extent to which source parameters can be recovered and evaluate how various data corruption effects can be calibrated. These studies are the most important when proposing observations of new sources, in the characterization of the capabilities of new or upgraded instruments, and when verifying model-based theoretical predictions in a direct comparison with observational data. AIMS: We present the SYnthetic Measurement creator for long Baseline Arrays (SYMBA), a novel synthetic data generation pipeline for Very Long Baseline Interferometry (VLBI) observations. SYMBA takes into account several realistic atmospheric, instrumental, and calibration effects. METHODS: We used SYMBA to create synthetic observations for the Event Horizon Telescope (EHT), a millimetre VLBI array, which has recently captured the first image of a black hole shadow. After testing SYMBA with simple source and corruption models, we study the importance of including all corruption and calibration effects, compared to the addition of thermal noise only. Using synthetic data based on two example general relativistic magnetohydrodynamics (GRMHD) model images of M 87, we performed case studies to assess the image quality that can be obtained with the current and future EHT array for different weather conditions. RESULTS: Our synthetic observations show that the effects of atmospheric and instrumental corruptions on the measured visibilities are significant. Despite these effects, we demonstrate how the overall structure of our GRMHD source models can be recovered robustly with the EHT2017 array after performing calibration steps, which include fringe fitting, a priori amplitude and network calibration, and self-calibration. With the planned addition of new stations to the EHT array in the coming years, images could be reconstructed with higher angular resolution and dynamic range. In our case study, these improvements allowed for a distinction between a thermal and a non-thermal GRMHD model based on salient features in reconstructed images."
JAMES JOHNSON,The long-baseline neutrino experiment: exploring fundamental symmetries of the universe,"The preponderance of matter over antimatter in the early Universe, the dynamics of the supernova bursts that produced the heavy elements necessary for life and whether protons eventually decay --- these mysteries at the forefront of particle physics and astrophysics are key to understanding the early evolution of our Universe, its current state and its eventual fate. The Long-Baseline Neutrino Experiment (LBNE) represents an extensively developed plan for a world-class experiment dedicated to addressing these questions. LBNE is conceived around three central components: (1) a new, high-intensity neutrino source generated from a megawatt-class proton accelerator at Fermi National Accelerator Laboratory, (2) a near neutrino detector just downstream of the source, and (3) a massive liquid argon time-projection chamber deployed as a far detector deep underground at the Sanford Underground Research Facility. This facility, located at the site of the former Homestake Mine in Lead, South Dakota, is approximately 1,300 km from the neutrino source at Fermilab -- a distance (baseline) that delivers optimal sensitivity to neutrino charge-parity symmetry violation and mass ordering effects. This ambitious yet cost-effective design incorporates scalability and flexibility and can accommodate a variety of upgrades and contributions. With its exceptional combination of experimental configuration, technical capabilities, and potential for transformative discoveries, LBNE promises to be a vital facility for the field of particle physics worldwide, providing physicists from around the globe with opportunities to collaborate in a twenty to thirty year program of exciting science. In this document we provide a comprehensive overview of LBNE's scientific objectives, its place in the landscape of neutrino physics worldwide, the technologies it will incorporate and the capabilities it will possess."
JAMES JOHNSON,Scintillation light detection in the 6-m drift-length ProtoDUNE Dual Phase liquid argon TPC,"DUNE is a dual-site experiment for long-baseline neutrino oscillation studies, neutrino astrophysics and nucleon decay searches. ProtoDUNE Dual Phase (DP) is a 6  ×  6  ×  6 m 3 liquid argon time-projection-chamber (LArTPC) that recorded cosmic-muon data at the CERN Neutrino Platform in 2019-2020 as a prototype of the DUNE Far Detector. Charged particles propagating through the LArTPC produce ionization and scintillation light. The scintillation light signal in these detectors can provide the trigger for non-beam events. In addition, it adds precise timing capabilities and improves the calorimetry measurements. In ProtoDUNE-DP, scintillation and electroluminescence light produced by cosmic muons in the LArTPC is collected by photomultiplier tubes placed up to 7 m away from the ionizing track. In this paper, the ProtoDUNE-DP photon detection system performance is evaluated with a particular focus on the different wavelength shifters, such as PEN and TPB, and the use of Xe-doped LAr, considering its future use in giant LArTPCs. The scintillation light production and propagation processes are analyzed and a comparison of simulation to data is performed, improving understanding of the liquid argon properties."
JAMES JOHNSON,"The medical student: v. 5, no. 1-8",
JAMES JOHNSON,Asymmetric synthesis of nidulalin A and nidulaxanthone A: selective carbonyl desaturation using an oxoammonium salt,"Nidulaxanthone A is a dimeric, dihydroxanthone natural product that was isolated in 2020 from Aspergillus sp. Structurally, the compound features an unprecedented heptacyclic 6/6/6/6/6/6/6 ring system which is unusual for natural xanthone dimers. Biosynthetically, nidulaxanthone A originates from the monomer nidulalin A via stereoselective Diels-Alder dimerization. To expedite the synthesis of nidulalin A and study the proposed dimerization, we developed methodology involving the use of allyl triflate for chromone ester activation, followed by vinylogous addition, to rapidly forge the nidulalin A scaffold in a four-step sequence which also features ketone desaturation using Bobbitt's oxoammonium salt. An asymmetric synthesis of nidulalin A was achieved using acylative kinetic resolution (AKR) of chiral, racemic 2H-nidulalin A. Dimerization of enantioenriched nidulalin A to nidulaxanthone A was achieved using solvent-free, thermolytic conditions. Computational studies have been conducted to probe both the oxoammonium-mediated desaturation and (4 + 2) dimerization events."
JAMES JOHNSON,Broadband multi-wavelength properties of M87 during the 2017 Event Horizon Telescope campaign,"In 2017, the Event Horizon Telescope (EHT) Collaboration succeeded in capturing the first direct image of the center of the M87 galaxy. The asymmetric ring morphology and size are consistent with theoretical expectations for a weakly accreting supermassive black hole of mass ∼6.5 × 109 M ⊙. The EHTC also partnered with several international facilities in space and on the ground, to arrange an extensive, quasi-simultaneous multi-wavelength campaign. This Letter presents the results and analysis of this campaign, as well as the multi-wavelength data as a legacy data repository. We captured M87 in a historically low state, and the core flux dominates over HST-1 at high energies, making it possible to combine core flux constraints with the more spatially precise very long baseline interferometry data. We present the most complete simultaneous multi-wavelength spectrum of the active nucleus to date, and discuss the complexity and caveats of combining data from different spatial scales into one broadband spectrum. We apply two heuristic, isotropic leptonic single-zone models to provide insight into the basic source properties, but conclude that a structured jet is necessary to explain M87’s spectrum. We can exclude that the simultaneous γ-ray emission is produced via inverse Compton emission in the same region producing the EHT mm-band emission, and further conclude that the γ-rays can only be produced in the inner jets (inward of HST-1) if there are strongly particle-dominated regions. Direct synchrotron emission from accelerated protons and secondaries cannot yet be excluded."
JAMES JOHNSON,Event Horizon Telescope imaging of the archetypal blazar 3C 279 at an extreme 20 microarcsecond resolution,"3C 279 is an archetypal blazar with a prominent radio jet that show broadband flux density variability across the entire electromagnetic spectrum. We use an ultra-high angular resolution technique – global Very Long Baseline Interferometry (VLBI) at 1.3 mm (230 GHz) – to resolve the innermost jet of 3C 279 in order to study its fine-scale morphology close to the jet base where highly variable γ-ray emission is thought to originate, according to various models. The source was observed during four days in April 2017 with the Event Horizon Telescope at 230 GHz, including the phased Atacama Large Millimeter/submillimeter Array (ALMA), at an angular resolution of ∼20 μas (at a redshift of z = 0.536 this corresponds to ∼0.13 pc  ∼ 1700 Schwarzschild radii with a black hole mass M<jats:sub>BH</jats:sub> = 8 × 10^8 M_⊙). Imaging and model-fitting techniques were applied to the data to parameterize the fine-scale source structure and its variation. We find a multicomponent inner jet morphology with the northernmost component elongated perpendicular to the direction of the jet, as imaged at longer wavelengths. The elongated nuclear structure is consistent on all four observing days and across different imaging methods and model-fitting techniques, and therefore appears robust. Owing to its compactness and brightness, we associate the northern nuclear structure as the VLBI “core”. This morphology can be interpreted as either a broad resolved jet base or a spatially bent jet. We also find significant day-to-day variations in the closure phases, which appear most pronounced on the triangles with the longest baselines. Our analysis shows that this variation is related to a systematic change of the source structure. Two inner jet components move non-radially at apparent speeds of ∼15 c and ∼20 c (∼1.3 and ∼1.7 μas day^−1, respectively), which more strongly supports the scenario of traveling shocks or instabilities in a bent, possibly rotating jet. The observed apparent speeds are also coincident with the 3C 279 large-scale jet kinematics observed at longer (cm) wavelengths, suggesting no significant jet acceleration between the 1.3 mm core and the outer jet. The intrinsic brightness temperature of the jet components are ≲10^10 K, a magnitude or more lower than typical values seen at ≥7 mm wavelengths. The low brightness temperature and morphological complexity suggest that the core region of 3C 279 becomes optically thin at short (mm) wavelengths."
JAMES JOHNSON,First M87 Event Horizon Telescope results. II. Array and instrumentation,"The Event Horizon Telescope (EHT) is a very long baseline interferometry (VLBI) array that comprises millimeter- and submillimeter-wavelength telescopes separated by distances comparable to the diameter of the Earth. At a nominal operating wavelength of ~1.3 mm, EHT angular resolution (λ/D) is ~25 μas, which is sufficient to resolve nearby supermassive black hole candidates on spatial and temporal scales that correspond to their event horizons. With this capability, the EHT scientific goals are to probe general relativistic effects in the strong-field regime and to study accretion and relativistic jet formation near the black hole boundary. In this Letter we describe the system design of the EHT, detail the technology and instrumentation that enable observations, and provide measures of its performance. Meeting the EHT science objectives has required several key developments that have facilitated the robust extension of the VLBI technique to EHT observing wavelengths and the production of instrumentation that can be deployed on a heterogeneous array of existing telescopes and facilities. To meet sensitivity requirements, high-bandwidth digital systems were developed that process data at rates of 64 gigabit s−1, exceeding those of currently operating cm-wavelength VLBI arrays by more than an order of magnitude. Associated improvements include the development of phasing systems at array facilities, new receiver installation at several sites, and the deployment of hydrogen maser frequency standards to ensure coherent data capture across the array. These efforts led to the coordination and execution of the first Global EHT observations in 2017 April, and to event-horizon-scale imaging of the supermassive black hole candidate in M87."
JAMES JOHNSON,First M87 Event Horizon Telescope results. I. The shadow of the supermassive black hole,"When surrounded by a transparent emission region, black holes are expected to reveal a dark shadow caused by gravitational light bending and photon capture at the event horizon. To image and study this phenomenon, we have assembled the Event Horizon Telescope, a global very long baseline interferometry array observing at a wavelength of 1.3 mm. This allows us to reconstruct event-horizon-scale images of the supermassive black hole candidate in the center of the giant elliptical galaxy M87. We have resolved the central compact radio source as an asymmetric bright emission ring with a diameter of 42 ± 3 μas, which is circular and encompasses a central depression in brightness with a flux ratio gsim10:1. The emission ring is recovered using different calibration and imaging schemes, with its diameter and width remaining stable over four different observations carried out in different days. Overall, the observed image is consistent with expectations for the shadow of a Kerr black hole as predicted by general relativity. The asymmetry in brightness in the ring can be explained in terms of relativistic beaming of the emission from a plasma rotating close to the speed of light around a black hole. We compare our images to an extensive library of ray-traced general-relativistic magnetohydrodynamic simulations of black holes and derive a central mass of M = (6.5 ± 0.7) × 10^9 M ⊙. Our radio-wave observations thus provide powerful evidence for the presence of supermassive black holes in centers of galaxies and as the central engines of active galactic nuclei. They also present a new tool to explore gravity in its most extreme limit and on a mass scale that was so far not accessible."
JAMES JOHNSON,First Sagittarius A* Event Horizon Telescope results. III. Imaging of the Galactic center supermassive black hole,"We present the first event-horizon-scale images and spatiotemporal analysis of Sgr A* taken with the Event Horizon Telescope in 2017 April at a wavelength of 1.3 mm. Imaging of Sgr A* has been conducted through surveys over a wide range of imaging assumptions using the classical CLEAN algorithm, regularized maximum likelihood methods, and a Bayesian posterior sampling method. Different prescriptions have been used to account for scattering effects by the interstellar medium toward the Galactic center. Mitigation of the rapid intraday variability that characterizes Sgr A* has been carried out through the addition of a “variability noise budget” in the observed visibilities, facilitating the reconstruction of static full-track images. Our static reconstructions of Sgr A* can be clustered into four representative morphologies that correspond to ring images with three different azimuthal brightness distributions and a small cluster that contains diverse nonring morphologies. Based on our extensive analysis of the effects of sparse (u, v)-coverage, source variability, and interstellar scattering, as well as studies of simulated visibility data, we conclude that the Event Horizon Telescope Sgr A* data show compelling evidence for an image that is dominated by a bright ring of emission with a ring diameter of ∼50 μas, consistent with the expected “shadow” of a 4 × 106 M⊙ black hole in the Galactic center located at a distance of 8 kpc."
JAMES JOHNSON,Characterizing and mitigating intraday variability: reconstructing source structure in accreting black holes with mm-VLBI,"The extraordinary physical resolution afforded by the Event Horizon Telescope has opened a window onto the astrophysical phenomena unfolding on horizon scales in two known black holes, M87* and Sgr A*. However, with this leap in resolution has come a new set of practical complications. Sgr A* exhibits intraday variability that violates the assumptions underlying Earth aperture synthesis, limiting traditional image reconstruction methods to short timescales and data sets with very sparse (u, v) coverage. We present a new set of tools to detect and mitigate this variability. We develop a data-driven, model-agnostic procedure to detect and characterize the spatial structure of intraday variability. This method is calibrated against a large set of mock data sets, producing an empirical estimator of the spatial power spectrum of the brightness fluctuations. We present a novel Bayesian noise modeling algorithm that simultaneously reconstructs an average image and statistical measure of the fluctuations about it using a parameterized form for the excess variance in the complex visibilities not otherwise explained by the statistical errors. These methods are validated using a variety of simulated data, including general relativistic magnetohydrodynamic simulations appropriate for Sgr A* and M87*. We find that the reconstructed source structure and variability are robust to changes in the underlying image model. We apply these methods to the 2017 EHT observations of M87*, finding evidence for variability across the EHT observing campaign. The variability mitigation strategies presented are widely applicable to very long baseline interferometry observations of variable sources generally, for which they provide a data-informed averaging procedure and natural characterization of inter-epoch image consistency."
BELA SUKI,A microfluidic chamber-based approach to map the shear moduli of vascular cells and other soft materials,
BELA SUKI,Avalanches in the lung: a statistical mechanical model,
BELA SUKI,Long-Range Correlations in Rectal Temperature Fluctuations of Healthy Infants during Maturation,"BACKGROUND. Control of breathing, heart rate, and body temperature are interdependent in infants, where instabilities in thermoregulation can contribute to apneas or even life-threatening events. Identifying abnormalities in thermoregulation is particularly important in the first 6 months of life, where autonomic regulation undergoes critical development. Fluctuations in body temperature have been shown to be sensitive to maturational stage as well as system failure in critically ill patients. We thus aimed to investigate the existence of fractal-like long-range correlations, indicative of temperature control, in night time rectal temperature (Trec) patterns in maturing infants. METHODOLOGY/PRINCIPAL FINDINGS. We measured Trec fluctuations in infants every 4 weeks from 4 to 20 weeks of age and before and after immunization. Long-range correlations in the temperature series were quantified by the correlation exponent, a using detrended fluctuation analysis. The effects of maturation, room temperature, and immunization on the strength of correlation were investigated. We found that Trec fluctuations exhibit fractal long-range correlations with a mean (SD) a of 1.51 (0.11), indicating that Trec is regulated in a highly correlated and hence deterministic manner. A significant increase in a with age from 1.42 (0.07) at 4 weeks to 1.58 (0.04) at 20 weeks reflects a change in long-range correlation behavior with maturation towards a smoother and more deterministic temperature regulation, potentially due to the decrease in surface area to body weight ratio in the maturing infant. a was not associated with mean room temperature or influenced by immunization CONCLUSIONS. This study shows that the quantification of long-range correlations using a derived from detrended fluctuation analysis is an observer-independent tool which can distinguish developmental stages of night time Trec pattern in young infants, reflective of maturation of the autonomic system. Detrended fluctuation analysis may prove useful for characterizing thermoregulation in premature and other infants at risk for life-threatening events."
BELA SUKI,Avalanches and power law behavior in aortic dissection progression,Aortic dissection is a devastating cardiovascular disease known for its rapid propagation and high morbidity and mortality. The mechanisms underlying the propagation of aortic dissection are not well understood. Our study reports the discovery of avalanche-like failure of the aorta during dissection propagation that results from the local buildup of strain energy followed by a cascade failure of inhomogeneously distributed interlamellar collagen fibers. An innovative computational model was developed that successfully describes the failure mechanics of dissection propagation. Our study provides the first quantitative agreement between experiment and model prediction of the dissection propagation within the complex extracellular matrix (ECM). Our results may lead to the possibility of predicting such catastrophic events based on microscopic features of the ECM.
BELA SUKI,Local fractal dimension of collagen detects increased spatial complexity in fibrosis,"Increase of collagen content and reorganization characterizes fibrosis but quantifying the latter remains challenging. Spatially complex structures are often analyzed via the fractal dimension; however, established methods for calculating this quantity either provide a single dimension for an entire object or a spatially distributed dimension that only considers binary images. These neglect valuable information related to collagen density in images of fibrotic tissue. We sought to develop a fractal analysis that can be applied to 3-dimensional (3D) images of fibrotic tissue. A fractal dimension map for each image was calculated by determining a single fractal dimension for a small area surrounding each image pixel, using fiber thickness as the third dimension. We found that this local fractal dimension increased with age and with progression of fibrosis regardless of collagen content. Our new method of distributed 3D fractal analysis can thus distinguish between changes in collagen content and organization induced by fibrosis."
BELA SUKI,Multiscale stiffness of human emphysematous precision cut lung slices,"Emphysema is a debilitating disease that remodels the lung leading to reduced tissue stiffness. Thus, understanding emphysema progression requires assessing lung stiffness at both the tissue and alveolar scales. Here, we introduce an approach to determine multiscale tissue stiffness and apply it to precision-cut lung slices (PCLS). First, we established a framework for measuring stiffness of thin, disk-like samples. We then designed a device to verify this concept and validated its measuring capabilities using known samples. Next, we compared healthy and emphysematous human PCLS and found that the latter was 50% softer. Through computational network modeling, we discovered that this reduced macroscopic tissue stiffness was due to both microscopic septal wall remodeling and structural deterioration. Lastly, through protein expression profiling, we identified a wide spectrum of enzymes that can drive septal wall remodeling, which, together with mechanical forces, lead to rupture and structural deterioration of the emphysematous lung parenchyma."
GERALD D KIDD,"Executive function, visual attention and the cocktail party problem in musicians and non-musicians","The goal of this study was to investigate how cognitive factors influence performance in a multi-talker, “cocktail-party” like environment in musicians and non-musicians. This was achieved by relating performance in a spatial hearing task to cognitive processing abilities assessed using measures of executive function (EF) and visual attention in musicians and non-musicians. For the spatial hearing task, a speech target was presented simultaneously with two intelligible speech maskers that were either colocated with the target (0° azimuth) or were symmetrically separated from the target in azimuth (at ±15°). EF assessment included measures of cognitive flexibility, inhibition control and auditory working memory. Selective attention was assessed in the visual domain using a multiple object tracking task (MOT). For the MOT task, the observers were required to track target dots (n = 1,2,3,4,5) in the presence of interfering distractor dots. Musicians performed significantly better than non-musicians in the spatial hearing task. For the EF measures, musicians showed better performance on measures of auditory working memory compared to non-musicians. Furthermore, across all individuals, a significant correlation was observed between performance on the spatial hearing task and measures of auditory working memory. This result suggests that individual differences in performance in a cocktail party-like environment may depend in part on cognitive factors such as auditory working memory. Performance in the MOT task did not differ between groups. However, across all individuals, a significant correlation was found between performance in the MOT and spatial hearing tasks. A stepwise multiple regression analysis revealed that musicianship and performance on the MOT task significantly predicted performance on the spatial hearing task. Overall, these findings confirm the relationship between musicianship and cognitive factors including domain-general selective attention and working memory in solving the “cocktail party problem”."
IVAN FERNANDEZ-VAL,probitfe and logitfe: Bias corrections for probit and logit models with two-way fixed effects,"We present the Stata commands probitfe and logitfe, which estimate probit and logit panel data models with individual and/or time unobserved effects. Fixed effect panel data methods that estimate the unobserved effects can be severely biased because of the incidental parameter problem (Neyman and Scott, 1948). We tackle this problem by using the analytical and jackknife bias corrections derived in Fernandez-Val and Weidner (2016) for panels where the two dimensions (N and T) are moderately large. We illustrate the commands with an empirical application to international trade and a Monte Carlo simulation calibrated to this application."
IVAN FERNANDEZ-VAL,Counterfactual: an R package for counterfactual analysis,"The Counterfactual package implements the estimation and inference methods of Cher nozhukov et al. (2013) for counterfactual analysis. The counterfactual distributions considered are the result of changing either the marginal distribution of covariates related to the outcome variable of interest, or the conditional distribution of the outcome given the covariates. They can be applied to estimate quantile treatment effects and wage decompositions. This paper serves as an introduction to the package and displays basic functionality of the commands contained within."
IVAN FERNANDEZ-VAL,Generic machine learning inference on heterogenous treatment effects in randomized experiments,"We propose strategies to estimate and make inference on key features of heterogeneous effects in randomized experiments. These key features include best linear predictors of the effects using machine learning proxies, average effects sorted by impact groups, and average characteristics of most and least impacted units. The approach is valid in high dimensional settings, where the effects are proxied by machine learning methods. We post-process these proxies into the estimates of the key features. Our approach is generic, it can be used in conjunction with penalized methods, deep and shallow neural networks, canonical and new random forests, boosted trees, and ensemble methods. Our approach is agnostic and does not make unrealistic or hard-to-check assumptions; we don’t require conditions for consistency of the ML methods. Estimation and inference relies on repeated data splitting to avoid overfitting and achieve validity. For inference, we take medians of p-values and medians of confidence intervals, resulting from many different data splits, and then adjust their nominal level to guarantee uniform validity. This variational inference method is shown to be uniformly valid and quantifies the uncertainty coming from both parameter estimation and data splitting. The inference method could be of substantial independent interest in many machine learning applications. An empirical application to the impact of micro-credit on economic development illustrates the use of the approach in randomized experiments. An additional application to the impact of the gender discrimination on wages illustrates the potential use of the approach in observational studies, where machine learning methods can be used to condition flexibly on very high-dimensional controls."
IVAN FERNANDEZ-VAL,"Supplement to “program evaluation and causal inference with high-dimensional data""",
IVAN FERNANDEZ-VAL,Generic inference on quantile and quantile effect functions for discrete outcomes,"Quantile and quantile effect functions are important tools for descriptive and inferential analysis due to their natural and intuitive interpretation. Existing inference methods for these functions do not apply to discrete random variables. This paper offers a simple, practical construction of simultaneous confidence bands for quantile and quantile effect functions of possibly discrete random variables. It is based on a natural transformation of simultaneous confidence bands for distribution functions, which are readily available for many problems. The construction is generic and does not depend on the nature of the underlying problem. It works in conjunction with parametric, semiparametric, and nonparametric modeling strategies and does not depend on the sampling scheme. We apply our method to characterize the distributional impact of insurance coverage on health care utilization and obtain the distributional decomposition of the racial test score gap. Our analysis generates new, interesting empirical findings, and complements previous analyses that focused on mean effects only. In both applications, the outcomes of interest are discrete rendering existing inference methods invalid for obtaining uniform confidence bands for quantile and quantile effects functions."
IVAN FERNANDEZ-VAL,Program evaluation and causal inference with high-dimensional data,"In this paper, we provide efficient estimators and honest confidence bands for a variety of treatment effects including local average (LATE) and local quantile treatment effects (LQTE) in data-rich environments. We can handle very many control variables, endogenous receipt of treatment, heterogeneous treatment effects, and function-valued outcomes. Our framework covers the special case of exogenous receipt of treatment, either conditional on controls or unconditionally as in randomized control trials. In the latter case, our approach produces efficient estimators and honest bands for (functional) average treatment effects (ATE) and quantile treatment effects (QTE). To make informative inference possible, we assume that key reduced form predictive relationships are approximately sparse. This assumption allows the use of regularization and selection methods to estimate those relations, and we provide methods for post-regularization and post-selection inference that are uniformly valid (honest) across a wide-range of models. We show that a key ingredient enabling honest inference is the use of orthogonal or doubly robust moment conditions in estimating certain reduced form functional parameters. We illustrate the use of the proposed methods with an application to estimating the effect of 401(k) eligibility and participation on accumulated assets."
IVAN FERNANDEZ-VAL,"Individual and time effects in nonlinear panel models with large N, T","We derive fixed effects estimators of parameters and average partial effects in (possibly dynamic) nonlinear panel data models with individual and time effects. They cover logit, probit, ordered probit, Poisson and Tobit models that are important for many empirical applications in micro and macroeconomics. Our estimators use analytical and jackknife bias corrections to deal with the incidental parameter problem, and are asymptotically unbiased under asymptotic sequences where N/T converges to a constant. We develop inference methods and show that they perform well in numerical examples."
IVAN FERNANDEZ-VAL,Network and panel quantile effects via distribution regression,"This paper provides a method to construct simultaneous con fidence bands for quantile functions and quantile effects in nonlinear network and panel models with unobserved two-way effects, strictly exogenous covariates, and possibly discrete outcome variables. The method is based upon projection of simultaneous confi dence bands for distribution functions constructed from fixed effects distribution regression estimators. These fi xed effects estimators are bias corrected to deal with the incidental parameter problem. Under asymptotic sequences where both dimensions of the data set grow at the same rate, the confi dence bands for the quantile functions and effects have correct joint coverage in large samples. An empirical application to gravity models of trade illustrates the applicability of the methods to network data."
IVAN FERNANDEZ-VAL,Fixed effects estimation of large-T panel data models,"This article reviews recent advances in fixed effect estimation of panel data models for long panels, where the number of time periods is relatively large. We focus on semiparametric models with unobserved individual and time effects, where the distribution of the outcome variable conditional on covariates and unobserved effects is specified parametrically, while the distribution of the unobserved effects is left unrestricted. Compared to existing reviews on long panels (Arellano & Hahn, 2007; a section in Arellano & Bonhomme, 2011) we discuss models with both individual and time effects, split-panel Jackknife bias corrections, unbalanced panels, distribution and quantile effects, and other extensions. Understanding and correcting the incidental parameter bias caused by the estimation of many fixed effects is our main focus, and the unifying theme is that the order of this bias is given by the simple formula p/n for all models discussed, with p the number of estimated parameters and n the total sample size."
IVAN FERNANDEZ-VAL,Nonseparable multinomial choice models in cross-section and panel data,"Multinomial choice models are fundamental for empirical modeling of economic choices among discrete alternatives. We analyze identification of binary and multinomial choice models when the choice utilities are nonseparable in observed attributes and multidimensional unobserved heterogeneity with cross-section and panel data. We show that derivatives of choice probabilities with respect to continuous attributes are weighted averages of utility derivatives in cross-section models with exogenous heterogeneity. In the special case of random coefficient models with an independent additive effect, we further characterize that the probability derivative at zero is proportional to the population mean of the coefficients. We extend the identification results to models with endogenous heterogeneity using either a control function or panel data. In time stationary panel models with two periods, we find that differences over time of derivatives of choice probabilities identify utility derivatives “on the diagonal,” i.e. when the observed attributes take the same values in the two periods. We also show that time stationarity does not identify structural derivatives “off the diagonal” both in continuous and multinomial choice panel models."
IVAN FERNANDEZ-VAL,"Distribution regression with sample selection, with an application to wage decompositions in the UK","We develop a distribution regression model under endogenous sample selection. This model is a semiparametric generalization of the Heckman selection model that accommodates much rich patterns of heterogeneity in the selection process and effect of the covariates. The model applies to continuous, discrete and mixed outcomes. We study the identi fication of the model, and develop a computationally attractive two-step method to estimate the model parameters, where the fi rst step is a probit regression for the selection equation and the second step consists of multiple distribution regressions with selection corrections for the outcome equation. We construct estimators of functionals of interest such as actual and counterfactual distributions of latent and observed outcomes via plug-in rule. We derive functional central limit theorems for all the estimators and show the validity of multiplier bootstrap to carry out functional inference. We apply the methods to wage decompositions in the UK using new data. Here we decompose the difference between the male and female wage distributions into four effects: composition, wage structure, selection structure and selection sorting. We uncover positive sorting for single men and negative sorting for married women that accounts for a substantial fraction of the gender wage gap at the top of the distribution. These fi ndings can be interpreted as evidence of assortative matching in the marriage market and glass-ceiling in the labor market."
IVAN FERNANDEZ-VAL,Semiparametric estimation of structural functions in nonseparable triangular models,"This paper introduces two classes of semiparametric triangular systems with nonadditively separable unobserved heterogeneity. They are based on distribution and quantile regression modeling of the reduced-form conditional distributions of the endogenous variables. We show that these models are flexible and identify the average, distribution and quantile structural functions using a control function approach that does not require a large support condition. We propose a computationally attractive three-stage procedure to estimate the structural functions where the first two stages consist of quantile or distribution regressions. We provide asymptotic theory and uniform inference methods for each stage. In particular, we derive functional central limit theorems and bootstrap functional central limit theorems for the distribution regression estimators of the structural functions. We illustrate the implementation and applicability of our methods with numerical simulations and an empirical application to demand analysis."
IVAN FERNANDEZ-VAL,Fast algorithms for the quantile regression process,"The widespread use of quantile regression methods depends crucially on the existence of fast algorithms. Despite numerous algorithmic improvements, the computation time is still non-negligible because researchers often estimate many quantile regressions and use the bootstrap for inference. We suggest two new fast algorithms for the estimation of a sequence of quantile regressions at many quantile indexes. The first algorithm applies the preprocessing idea of Portnoy and Koenker (1997) but exploits a previously estimated quantile regression to guess the sign of the residuals. This step allows for a reduction of the effective sample size. The second algorithm starts from a previously estimated quantile regression at a similar quantile index and updates it using a single Newton-Raphson iteration. The first algorithm is exact, while the second is only asymptotically equivalent to the traditional quantile regression estimator. We also apply the preprocessing idea to the bootstrap by using the sample estimates to guess the sign of the residuals in the bootstrap sample. Simulations show that our new algorithms provide very large improvements in computation time without significant (if any) cost in the quality of the estimates. For instance, we divide by 100 the time required to estimate 99 quantile regressions with 20 regressors and 50,000 observations."
IVAN FERNANDEZ-VAL,Decomposing changes in the distribution of real hourly wages in the U.S.,"We analyze the sources of changes in the distribution of hourly wages in the United States using CPS data for the survey years 1976 to 2016. We account for the selection bias from the employment decision by modeling the distribution of annual hours of work and estimating a nonseparable model of wages which uses a control function to account for selection. This allows the inclusion of all individuals working positive hours and thus provides a fuller description of the wage distribution. We decompose changes in the distribution of wages into composition, structural and selection eﬀects. Composition eﬀects have increased wages at all quantiles but the patterns of change are generally determined by the structural eﬀects. Evidence of changes in the selection eﬀects only appear at the lower quantiles of the female wage distribution. These various components combine to produce a substantial increase in wage inequality."
IVAN FERNANDEZ-VAL,SortedEffects: sorted causal effects in R,"Chernozhukov et al. (2018) proposed the sorted effect method for nonlinear regression models. This method consists of reporting percentiles of the partial effects, the sorted effects, in addition to the average effect commonly used to summarize the heterogeneity in the partial effects. They also propose to use the sorted effects to carry out classification analysis where the observational units are classified as most and least affected if their partial effect are above or below some tail sorted effects. The R package SortedEffects implements the estimation and inference methods therein and provides tools to visualize the results. This vignette serves as an introduction to the package and displays basic functionality of the functions within."
IVAN FERNANDEZ-VAL,Nonlinear factor models for network and panel data,"Factor structures or interactive effects are convenient devices to incorporate latent variables in panel data models. We consider fixed effect estimation of nonlinear panel single-index models with factor structures in the unobservables, which include logit, probit, ordered probit and Poisson specifications. We establish that fixed effect estimators of model parameters and average partial effects have normal distributions when the two dimensions of the panel grow large, but might suffer from incidental parameter bias. We show how models with factor structures can also be applied to capture important features of network data such as reciprocity, degree heterogeneity, homophily in latent variables, and clustering. We illustrate this applicability with an empirical example to the estimation of a gravity equation of international trade between countries using a Poisson model with multiple factors."
IVAN FERNANDEZ-VAL,Fixed effect estimation of large T panel data models,"This article reviews recent advances in fi xed effect estimation of panel data models for long panels, where the number of time periods is relatively large. We focus on semiparametric models with unobserved individual and time effects, where the distribution of the outcome variable conditional on covariates and unobserved effects is specifi ed parametrically, while the distribution of the unobserved effects is left unrestricted. Compared to existing reviews on long panels (Arellano & Hahn, 2007; a section in Arellano & Bonhomme, 2011) we discuss models with both individual and time effects, split-panel Jackknife bias corrections, unbalanced panels, distribution and quantile effects, and other extensions. Understanding and correcting the incidental parameter bias caused by the estimation of many fixed effects is our main focus, and the unifying theme is that the order of this bias is given by the simple formula 𝑝/𝒏 for all models discussed, with p the number of estimated parameters and 𝒏 the total sample size."
IVAN FERNANDEZ-VAL,Nonseparable sample selection models with censored selection rules: an application to wage decompositions,"We consider identification and estimation of nonseparable sample selection models with censored selection rules. We employ a control function approach and discuss different objects of interest based on (1) local effects conditional on the control function, and (2) global effects obtained from integration over ranges of values of the control function. We provide conditions under which these objects are appropriate for the total population. We also present results regarding the estimation of counterfactual distributions. We derive conditions for identification for these different objects and suggest strategies for estimation. We also provide the associated asymptotic theory. These strategies are illustrated in an empirical investigation of the determinants of female wages and wage growth in the United Kingdom."
IVAN FERNANDEZ-VAL,quantreg. nonpar: An R Package for performing nonparametric series quantile regression,The R package quantreg.nonpar implements nonparametric quantile regression methods to estimate and make inference on partially linear quantile models. quantreg.nonpar obtains point estimates of the conditional quantile function and its derivatives based on series approximations to the nonparametric part of the model. It also provides pointwise and uniform confidence intervals over a region of covariate values and/or quantile indices for the same functions using analytical and resampling methods. This paper serves as an introduction to the package and displays basic functionality of the functions contained within.
IVAN FERNANDEZ-VAL,"QRPROCESS: stata module for quantile regression: fast algorithm, pointwise and uniform inference","This package offers fast estimation and inference procedures for the linear quantile regression model. First, qrprocess implements new algorithms that are much quicker than the built-in Stata commands, especially when a large number of quantile regressions or bootstrap replications must be estimated. Second, the commands provide analytical estimates of the variance-covariance matrix of the coefficients for several quantile regressions allowing for weights, clustering and stratification. Third, in addition to traditional pointwise confidence intervals, this command also provides functional confidence bands and tests of functional hypotheses. Fourth, predict called after qrprocess can generate monotone estimates of the conditional quantile and distribution functions obtained by rearrangement. Fifth, the new command plotprocess conveniently plots the estimated coefficients with their confidence intervals and uniform bands."
IVAN FERNANDEZ-VAL,The sorted effects method: discovering heterogeneous effects beyond their averages,"The partial (ceteris paribus) effects of interest in nonlinear and interactive linear models are heterogeneous as they can vary dramatically with the underlying observed or unobserved covariates. Despite the apparent importance of heterogeneity, a common practice in modern empirical work is to largely ignore it by reporting average partial effects (or, at best, average effects for some groups). While average effects provide very convenient scalar summaries of typical effects, by definition they fail to reflect the entire variety of the heterogeneous effects. In order to discover these effects much more fully, we propose to estimate and report sorted effects -- a collection of estimated partial effects sorted in increasing order and indexed by percentiles. By construction the sorted effect curves completely represent and help visualize the range of the heterogeneous effects in one plot. They are as convenient and easy to report in practice as the conventional average partial effects. They also serve as a basis for classification analysis, where we divide the observational units into most or least affected groups and summarize their characteristics. We provide a quantification of uncertainty (standard errors and confidence bands) for the estimated sorted effects and related classification analysis, and provide confidence sets for the most and least affected groups. The derived statistical results rely on establishing key, new mathematical results on Hadamard differentiability of a multivariate sorting operator and a related classification operator, which are of independent interest. We apply the sorted effects method and classification analysis to demonstrate several striking patterns in the gender wage gap."
IVAN FERNANDEZ-VAL,Mastering panel metrics: causal impact of democracy on growth,"We revisit the panel data analysis of Acemoglu et al. (forthcoming) on the relationship between democracy and economic growth using state-of-the-art econometric methods. We argue that panel data settings are high-dimensional, resulting in estimators to be biased to a degree that invalidates statistical inference. We remove these biases by using simple analytical and sample-splitting methods, and thereby restore valid statistical inference. We find that debiased fixed effects and Arellano-Bond estimators produce higher estimates of the long-run effect of democracy on growth, providing even stronger support for the key hypothesis of Acemoglu et al."
IVAN FERNANDEZ-VAL,Shape-enforcing operators for point and interval estimators,
IVAN FERNANDEZ-VAL,Supplement to “Semiparametric estimation of structural functions in nonseparable triangular models”,
IVAN FERNANDEZ-VAL,Parametric modeling of quantile regression coefficient functions with longitudinal data,"In ordinary quantile regression, quantiles of different order are estimated one at a time. An alternative approach, which is referred to as quantile regression coefficients modeling (qrcm), is to model quantile regression coefficients as parametric functions of the order of the quantile. In this paper, we describe how the qrcm paradigm can be applied to longitudinal data. We introduce a two-level quantile function, in which two different quantile regression models are used to describe the (conditional) distribution of the within-subject response and that of the individual effects. We propose a novel type of penalized fixed-effects estimator, and discuss its advantages over standard methods based on ℓ1 and ℓ2 penalization. We provide model identifiability conditions, derive asymptotic properties, describe goodness-of-fit measures and model selection criteria, present simulation results, and discuss an application. The proposed method has been implemented in the R package qrcm."
IVAN FERNANDEZ-VAL,Low-rank approximations of nonseparable panel models,"We provide estimation methods for panel nonseparable models based on low-rank factor structure approximations. The factor structures are estimated by matrix-completion methods to deal with the computational challenges of principal component analysis in the presence of missing data. We show that the resulting estimators are consistent in large panels, but suffer from approximation and shrinkage biases. We correct these biases using matching and difference-in-difference approaches. Numerical examples and an empirical application to the effect of election day registration on voter turnout in the U.S. illustrate the properties and usefulness of our methods."
IVAN FERNANDEZ-VAL,Hours worked and the U.S. distribution of real annual earnings 1976-2016,"We examine the impact of annual hours worked on annual earnings by decomposing changes in the real annual earnings distribution into composition, structural and hours effects. We do so via a nonseparable simultaneous model of hours, wages and earnings. Using the Current Population Survey for the survey years 1976–2019, we find that changes in the female distribution of annual hours of work are important in explaining movements in inequality in female annual earnings. This captures the substantial changes in their employment behavior over this period. Movements in the male hours distribution only affect the lower part of their earnings distribution and reflect the sensitivity of these workers’ annual hours of work to cyclical factors."
IVAN FERNANDEZ-VAL,Censored quantile instrumental variable estimation with Stata,"Many applications involve a censored dependent variable and an endogenous independent variable. Chernozhukov, Fernandez-Val, and Kowalski (2015) introduced a censored quantile instrumental variable estimator (CQIV) for use in those applications, which has been applied by Kowalski (2016), among others. In this article, we introduce a Stata command, cqiv, that simplifes application of the CQIV estimator in Stata. We summarize the CQIV estimator and algorithm, we describe the use of the cqiv command, and we provide empirical examples."
IVAN FERNANDEZ-VAL,"Supplement to ""The sorted effects method: discovering heterogeneous effects beyond their averages""","This zip file contains the replication files for the manuscript. It also contains an online appendix. The supplementary material contains 7 appendices with additional results and some omitted proofs. Appendix C introduces some notation. Appendix D includes a brief review of differential geometry. Appendix E gathers the proofs of the key mathematical results in Appendix A. Appendix F provides sufficient conditions for the u-Donsker properties in Section 4. Appendix G extends the theoretical analysis to include discrete covariates. Appendices H and I report the results of 3 numerical simulations and an empirical application to the effect of race on mortgage denials, respectively."
LUCIA LIN,Long-range coupling of prefrontal cortex and visual (MT) or polysensory (STP) cortical areas in motion perception,"To investigate how, where and when moving auditory cues interact with the perception of object-motion during self-motion, we conducted psychophysical, MEG, and fMRI experiments in which the subjects viewed nine textured objects during simulated forward self-motion. On each trial, one object was randomly assigned its own looming motion within the scene. Subjects reported which of four labeled objects had independent motion within the scene in two conditions: (1) visual information only and (2) with additional moving- auditory cue. In MEG, comparison of the two conditions showed: (i) MT activity is similar across conditions, (ii) late after the stimulus presentation there is additional activity in the auditory cue condition ventral to MT, (iii) with the auditory cue, the right auditory cortex (AC) shows early activity together with STS, (iv) these two activities have different time courses and the STS signals occur later in the epoch together with frontal activity in the right hemisphere, (v) for the visual-only condition activity in PPC (posterior parietal cortex) is stronger than in the auditory-cue condition. fMRI conducted for visual-only condition reveals activations in a network of parietal and frontal areas and in MT. In addition, Dynamic Granger Causality analysis showed for auditory cues a strong connection of the AC with STP but not with MT suggesting binding of visual and auditory information at STP. Also, while in the visual-only condition PFC is connected with MT, in the auditory-cue condition PFC is connected to STP (superior temporal polysensory) area. These results indicate that PFC allocates attention to the “object” as a whole, in STP to a moving visual-auditory object, and in MT to a moving visual object."
MARK W STANLEY,The long-baseline neutrino experiment: exploring fundamental symmetries of the universe,"The preponderance of matter over antimatter in the early Universe, the dynamics of the supernova bursts that produced the heavy elements necessary for life and whether protons eventually decay --- these mysteries at the forefront of particle physics and astrophysics are key to understanding the early evolution of our Universe, its current state and its eventual fate. The Long-Baseline Neutrino Experiment (LBNE) represents an extensively developed plan for a world-class experiment dedicated to addressing these questions. LBNE is conceived around three central components: (1) a new, high-intensity neutrino source generated from a megawatt-class proton accelerator at Fermi National Accelerator Laboratory, (2) a near neutrino detector just downstream of the source, and (3) a massive liquid argon time-projection chamber deployed as a far detector deep underground at the Sanford Underground Research Facility. This facility, located at the site of the former Homestake Mine in Lead, South Dakota, is approximately 1,300 km from the neutrino source at Fermilab -- a distance (baseline) that delivers optimal sensitivity to neutrino charge-parity symmetry violation and mass ordering effects. This ambitious yet cost-effective design incorporates scalability and flexibility and can accommodate a variety of upgrades and contributions. With its exceptional combination of experimental configuration, technical capabilities, and potential for transformative discoveries, LBNE promises to be a vital facility for the field of particle physics worldwide, providing physicists from around the globe with opportunities to collaborate in a twenty to thirty year program of exciting science. In this document we provide a comprehensive overview of LBNE's scientific objectives, its place in the landscape of neutrino physics worldwide, the technologies it will incorporate and the capabilities it will possess."
MARTIN H STEINBERG,Imputation of Missing Genotypes: An Empirical Evaluation of IMPUTE,"BACKGROUND: Imputation of missing genotypes is becoming a very popular solution for synchronizing genotype data collected with different microarray platforms but the effect of ethnic background, subject ascertainment, and amount of missing data on the accuracy of imputation are not well understood. RESULTS: We evaluated the accuracy of the program IMPUTE to generate the genotype data of partially or fully untyped single nucleotide polymorphisms (SNPs). The program uses a model-based approach to imputation that reconstructs the genotype distribution given a set of referent haplotypes and the observed data, and uses this distribution to compute the marginal probability of each missing genotype for each individual subject that is used to impute the missing data. We assembled genome-wide data from five different studies and three different ethnic groups comprising Caucasians, African Americans and Asians. We randomly removed genotype data and then compared the observed genotypes with those generated by IMPUTE. Our analysis shows 97% median accuracy in Caucasian subjects when less than 10% of the SNPs are untyped and missing genotypes are accepted regardless of their posterior probability. The median accuracy increases to 99% when we require 0.95 minimum posterior probability for an imputed genotype to be acceptable. The accuracy decreases to 86% or 94% when subjects are African Americans or Asians. We propose a strategy to improve the accuracy by leveraging the level of admixture in African Americans. CONCLUSION: Our analysis suggests that IMPUTE is very accurate in samples of Caucasians origin, it is slightly less accurate in samples of Asians background, but substantially less accurate in samples of admixed background such as African Americans. Sample size and ascertainment do not seem to affect the accuracy of imputation."
MARTIN H STEINBERG,A Hierarchical and Modular Approach to the Discovery of Robust Associations in Genome-Wide Association Studies from Pooled DNA Samples,"BACKGROUND: One of the challenges of the analysis of pooling-based genome wide association studies is to identify authentic associations among potentially thousands of false positive associations. RESULTS. We present a hierarchical and modular approach to the analysis of genome wide genotype data that incorporates quality control, linkage disequilibrium, physical distance and gene ontology to identify authentic associations among those found by statistical association tests. The method is developed for the allelic association analysis of pooled DNA samples, but it can be easily generalized to the analysis of individually genotyped samples. We evaluate the approach using data sets from diverse genome wide association studies including fetal hemoglobin levels in sickle cell anemia and a sample of centenarians and show that the approach is highly reproducible and allows for discovery at different levels of synthesis. CONCLUSION: Results from the integration of Bayesian tests and other machine learning techniques with linkage disequilibrium data suggest that we do not need to use too stringent thresholds to reduce the number of false positive associations. This method yields increased power even with relatively small samples. In fact, our evaluation shows that the method can reach almost 70% sensitivity with samples of only 100 subjects."
MARTIN H STEINBERG,RNA Editing Genes Associated with Extreme Old Age in Humans and with Lifespan in C. elegans,"BACKGROUND. The strong familiality of living to extreme ages suggests that human longevity is genetically regulated. The majority of genes found thus far to be associated with longevity primarily function in lipoprotein metabolism and insulin/IGF-1 signaling. There are likely many more genetic modifiers of human longevity that remain to be discovered. METHODOLOGY/PRINCIPAL FINDINGS. Here, we first show that 18 single nucleotide polymorphisms (SNPs) in the RNA editing genes ADARB1 and ADARB2 are associated with extreme old age in a U.S. based study of centenarians, the New England Centenarian Study. We describe replications of these findings in three independently conducted centenarian studies with different genetic backgrounds (Italian, Ashkenazi Jewish and Japanese) that collectively support an association of ADARB1 and ADARB2 with longevity. Some SNPs in ADARB2 replicate consistently in the four populations and suggest a strong effect that is independent of the different genetic backgrounds and environments. To evaluate the functional association of these genes with lifespan, we demonstrate that inactivation of their orthologues adr-1 and adr-2 in C. elegans reduces median survival by 50%. We further demonstrate that inactivation of the argonaute gene, rde-1, a critical regulator of RNA interference, completely restores lifespan to normal levels in the context of adr-1 and adr-2 loss of function. CONCLUSIONS/SIGNIFICANCE. Our results suggest that RNA editors may be an important regulator of aging in humans and that, when evaluated in C. elegans, this pathway may interact with the RNA interference machinery to regulate lifespan."
MARTIN H STEINBERG,Chronic Hyper-Hemolysis in Sickle Cell Anemia: Association of Vascular Complications and Mortality with Less Frequent Vasoocclusive Pain,"BACKGROUND. Intravascular hemolysis in sickle cell anemia could contribute to complications associated with nitric oxide deficiency, advancing age, and increased mortality. We have previously reported that intense hemolysis is associated with increased risk of vascular complications in a small cohort of adults with sickle cell disease. These observations have not been validated in other populations. METHODS. The distribution of serum lactic dehydrogenase (LDH) values was used as a surrogate measure of intravascular hemolysis in a contemporaneous patient group and an historical adult population from the Cooperative Study of Sickle Cell Disease (CSSCD), all with sickle cell anemia. Chronic hyper-hemolysis was defined by the top LDH quartile and was compared to the lowest LDH quartile. RESULTS. Hyper-hemolysis subjects had higher systolic blood pressure, higher prevalence of leg ulcers (OR 3.27, 95% CI 1.92-5.53, P<0.0001), priapism (OR 2.62, 95% CI 1.13-6.90, P=0.03) and pulmonary hypertension (OR 4.32, 95% CI 2.12-8.60, P<0.0001), while osteonecrosis (OR 0.32, 95% CI 0.19-0.54, P<0.0001) and pain (OR 0.23, 95% CI 0.09-0.55, P=0.0004) were less prevalent. Hyper-hemolysis was influenced by fetal hemoglobin and a thalassemia, and was a risk factor for early death in the CSSCD population (Hazard Ratio=1.97, P=0.02). CONCLUSIONS. Steady state LDH measurements can identify a chronic hyper-hemolysis phenotype which includes less frequent vasooclusive pain and earlier mortality. Clinicians should consider sickle cell specific therapies for these patients, as is done for those with more frequent acute pain. The findings also suggest that an important class of disease modifiers in sickle cell anemia affect the rate of hemolysis."
MARTIN H STEINBERG,"Whole genome sequences of a male and female supercentenarian, ages greater than 114 years","Supercentenarians (age 110+ years old) generally delay or escape age-related diseases and disability well beyond the age of 100 and this exceptional survival is likely to be influenced by a genetic predisposition that includes both common and rare genetic variants. In this report, we describe the complete genomic sequences of male and female supercentenarians, both age >114 years old. We show that: (1) the sequence variant spectrum of these two individuals' DNA sequences is largely comparable to existing non-supercentenarian genomes; (2) the two individuals do not appear to carry most of the well-established human longevity enabling variants already reported in the literature; (3) they have a comparable number of known disease-associated variants relative to most human genomes sequenced to-date; (4) approximately 1% of the variants these individuals possess are novel and may point to new genes involved in exceptional longevity; and (5) both individuals are enriched for coding variants near longevity-associated variants that we discovered through a large genome-wide association study. These analyses suggest that there are both common and rare longevity-associated variants that may counter the effects of disease-predisposing variants and extend lifespan. The continued analysis of the genomes of these and other rare individuals who have survived to extremely old ages should provide insight into the processes that contribute to the maintenance of health during extreme aging."
GEORGE T O'CONNOR,The Framingham Heart Study 100K SNP Genome-Wide Association Study Resource: Overview of 17 Phenotype Working Group Reports,"BACKGROUND: The Framingham Heart Study (FHS), founded in 1948 to examine the epidemiology of cardiovascular disease, is among the most comprehensively characterized multi-generational studies in the world. Many collected phenotypes have substantial genetic contributors; yet most genetic determinants remain to be identified. Using single nucleotide polymorphisms (SNPs) from a 100K genome-wide scan, we examine the associations of common polymorphisms with phenotypic variation in this community-based cohort and provide a full-disclosure, web-based resource of results for future replication studies. METHODS: Adult participants (n = 1345) of the largest 310 pedigrees in the FHS, many biologically related, were genotyped with the 100K Affymetrix GeneChip. These genotypes were used to assess their contribution to 987 phenotypes collected in FHS over 56 years of follow up, including: cardiovascular risk factors and biomarkers; subclinical and clinical cardiovascular disease; cancer and longevity traits; and traits in pulmonary, sleep, neurology, renal, and bone domains. We conducted genome-wide variance components linkage and population-based and family-based association tests. RESULTS: The participants were white of European descent and from the FHS Original and Offspring Cohorts (examination 1 Offspring mean age 32 ± 9 years, 54% women). This overview summarizes the methods, selected findings and limitations of the results presented in the accompanying series of 17 manuscripts. The presented association results are based on 70,897 autosomal SNPs meeting the following criteria: minor allele frequency ≥ 10%, genotype call rate ≥ 80%, Hardy-Weinberg equilibrium p-value ≥ 0.001, and satisfying Mendelian consistency. Linkage analyses are based on 11,200 SNPs and short-tandem repeats. Results of phenotype-genotype linkages and associations for all autosomal SNPs are posted on the NCBI dbGaP website at. CONCLUSION: We have created a full-disclosure resource of results, posted on the dbGaP website, from a genome-wide association study in the FHS. Because we used three analytical approaches to examine the association and linkage of 987 phenotypes with thousands of SNPs, our results must be considered hypothesis-generating and need to be replicated. Results from the FHS 100K project with NCBI web posting provides a resource for investigators to identify high priority findings for replication."
GEORGE T O'CONNOR,Asthma-Susceptibility Variants Identified Using Probands in Case-Control and Family-Based Analyses,"BACKGROUND: Asthma is a chronic respiratory disease whose genetic basis has been explored for over two decades, most recently via genome-wide association studies. We sought to find asthma-susceptibility variants by using probands from a single population in both family-based and case-control association designs. METHODS: We used probands from the Childhood Asthma Management Program (CAMP) in two primary genome-wide association study designs: (1) probands were combined with publicly available population controls in a case-control design, and (2) probands and their parents were used in a family-based design. We followed a two-stage replication process utilizing three independent populations to validate our primary findings. RESULTS: We found that single nucleotide polymorphisms with similar case-control and family-based association results were more likely to replicate in the independent populations, than those with the smallest p-values in either the case-control or family-based design alone. The single nucleotide polymorphism that showed the strongest evidence for association to asthma was rs17572584, which replicated in 2/3 independent populations with an overall p-value among replication populations of 3.5E-05. This variant is near a gene that encodes an enzyme that has been implicated to act coordinately with modulators of Th2 cell differentiation and is expressed in human lung. CONCLUSIONS: Our results suggest that using probands from family-based studies in case-control designs, and combining results of both family-based and case-control approaches, may be a way to augment our ability to find SNPs associated with asthma and other complex diseases."
GEORGE T O'CONNOR,Characterization of Regulatory T Cells in Urban Newborns,"BACKGROUND: In the United States, asthma prevalence is particularly high among urban children. Although the underlying immune mechanism contributing to asthma has not been identified, having impaired T regulatory (Treg) cells at birth may be a determining factor in urban children. The objective of this study was to compare Treg phenotype and function in cord blood (CB) of newborns to those in peripheral blood (PB) of a subset of participating mothers. METHODS: Treg numbers, expression, and suppressive function were quantified in subjects recruited prenatally from neighborhoods where ≥ 20% of families have incomes below the poverty line. Proportion of Treg cells and expression of naïve (CD45RA) or activated (CD45RO, CD69, and HLA-DR) markers in CD4+T cells was measured by flow cytometry. Treg suppressive capacity was determined by quantifying PHA-stimulated lymphocyte proliferation in mononuclear cell samples with and without CD25 depletion. RESULTS: In an urban cohort of 119 newborns and 82 mothers, we found that newborns had similar number of cells expressing FOXP3 as compared to the mothers but had reduced numbers of CD4+CD25+bright cells that predominantly expressed the naïve (CD45RA) rather than the activated/memory (CD45RO) phenotype found in the mothers. Additionally, the newborns had reduced mononuclear cell TGF-β production, and reduced Treg suppression of PHA-stimulated lymphocyte proliferation compared to the mothers. CONCLUSION: U.S. urban newborns have Treg cells that express FOXP3, albeit with an immature phenotype and function as compared to the mothers. Longitudinal follow-up is needed to delineate Treg cell maturation and subsequent risk for atopic diseases in this urban birth cohort."
GEORGE T O'CONNOR,"On the Analysis of Genome-Wide Association Studies in Family-Based Designs: A Universal, Robust Analysis Approach and an Application to Four Genome-Wide Association Studies","For genome-wide association studies in family-based designs, we propose a new, universally applicable approach. The new test statistic exploits all available information about the association, while, by virtue of its design, it maintains the same robustness against population admixture as traditional family-based approaches that are based exclusively on the within-family information. The approach is suitable for the analysis of almost any trait type, e.g. binary, continuous, time-to-onset, multivariate, etc., and combinations of those. We use simulation studies to verify all theoretically derived properties of the approach, estimate its power, and compare it with other standard approaches. We illustrate the practical implications of the new analysis method by an application to a lung-function phenotype, forced expiratory volume in one second (FEV1) in 4 genome-wide association studies. Author Summary In genome-wide association studies, the multiple testing problem and confounding due to population stratification have been intractable issues. Family-based designs have considered only the transmission of genotypes from founder to nonfounder to prevent sensitivity to the population stratification, which leads to the loss of information. Here we propose a novel analysis approach that combines mutually independent FBAT and screening statistics in a robust way. The proposed method is more powerful than any other, while it preserves the complete robustness of family-based association tests, which only achieves much smaller power level. Furthermore, the proposed method is virtually as powerful as population-based approaches/designs, even in the absence of population stratification. By nature of the proposed method, it is always robust as long as FBAT is valid, and the proposed method achieves the optimal efficiency if our linear model for screening test reasonably explains the observed data in terms of covariance structure and population admixture. We illustrate the practical relevance of the approach by an application in 4 genome-wide association studies."
GEORGE T O'CONNOR,Framingham heart study genome-wide association: results for pulmonary function measures,"BACKGROUND: Pulmonary function measures obtained by spirometry are used to diagnose chronic obstructive pulmonary disease (COPD) and are highly heritable. We conducted genome-wide association (GWA) analyses (Affymetrix 100K SNP GeneChip) for measures of lung function in the Framingham Heart Study. METHODS: Ten spirometry phenotypes including percent of predicted measures, mean spirometry measures over two examinations, and rates of change based on forced expiratory volume in one second (FEV1), forced vital capacity (FVC), forced expiratory flow from the 25th to 75th percentile (FEF25–75), the FEV1/FVC ratio, and the FEF25–75/FVC ratio were examined. Percent predicted phenotypes were created using each participant's latest exam with spirometry. Predicted lung function was estimated using models defined in the set of healthy never-smokers, and standardized residuals of percent predicted measures were created adjusting for smoking status, pack-years, and body mass index (BMI). All modeling was performed stratified by sex and cohort. Mean spirometry phenotypes were created using data from two examinations and adjusting for age, BMI, height, smoking and pack-years. Change in pulmonary function over time was studied using two to four examinations with spirometry to calculate slopes, which were then adjusted for age, height, smoking and pack-years. RESULTS: Analyses were restricted to 70,987 autosomal SNPs with minor allele frequency ≥ 10%, genotype call rate ≥ 80%, and Hardy-Weinberg equilibrium p-value ≥ 0.001. A SNP in the interleukin 6 receptor (IL6R) on chromosome 1 was among the best results for percent predicted FEF25–75. A non-synonymous coding SNP in glutathione S-transferase omega 2 (GSTO2) on chromosome 10 had top-ranked results studying the mean FEV1 and FVC measurements from two examinations. SNPs nearby the SOD3 and vitamin D binding protein genes, candidate genes for COPD, exhibited association to percent predicted phenotypes. CONCLUSION: GSTO2 and IL6R are credible candidate genes for association to pulmonary function identified by GWA. These and other observed associations warrant replication studies. This resource of GWA results for pulmonary function measures is publicly available at."
GEORGE T O'CONNOR,Genome-wide association of sleep and circadian phenotypes,"BACKGROUND: Numerous studies suggest genetic influences on sleepiness and circadian rhythms. The Sleep Heart Health Study collected questionnaire data on sleep habits and sleepiness from 2848 Framingham Heart Study Offspring Cohort participants. More than 700 participants were genotyped using the Affymetrix 100K SNP GeneChip, providing a unique opportunity to assess genetic linkage and association of these traits. METHODS: Sleepiness (defined as the Epworth Sleepiness Scale score), usual bedtime and usual sleep duration were assessed by self-completion questionnaire. Standardized residual measures adjusted for age, sex and BMI were analyzed. Multipoint variance components linkage analysis was performed. Association of SNPs to sleep phenotypes was analyzed with both population-based and family-based association tests, with analysis limited to 70,987 autosomal SNPs with minor allele frequency ≥10%, call rate ≥80%, and no significant deviation from Hardy-Weinberg equilibrium (p ≥ 0.001). RESULTS: Heritability of sleepiness was 0.29, bedtime 0.22, and sleep duration 0.17. Both genotype and sleep phenotype data were available for 749 subjects. Linkage analysis revealed five linkage peaks of LOD >2: four to usual bedtime, one to sleep duration. These peaks include several candidate sleep-related genes, including CSNK2A2, encoding a known component of the circadian molecular clock, and PROK2, encoding a putative transmitter of the behavioral circadian rhythm from the suprachiasmatic nucleus. Association tests identified an association of usual bedtime with a non-synonymous coding SNP in NPSR1 that has been shown to encode a gain of function mutation of the neuropeptide S receptor, whose endogenous ligand is a potent promoter of wakefulness. Each copy of the minor allele of this SNP was associated with a 15 minute later mean bedtime. The lowest p value was for association of sleepiness with a SNP located in an intron of PDE4D, which encodes a cAMP-specific phosphodiesterase widely expressed in human brain. Full association results are posted at. CONCLUSION: This analysis confirms prior reports of significant heritability of sleepiness, usual bedtime, and usual sleep duration. Several genetic loci with suggestive linkage to these traits are identified, including linkage peaks containing circadian clock-related genes. Association tests identify NPSR1 and PDE4D as possible mediators of bedtime and sleepiness."
GEORGE T O'CONNOR,The Framingham Heart Study 100K SNP Genome-Wide Association Study Resource: Overview of 17 Phenotype Working Group Reports,"BACKGROUND: The Framingham Heart Study (FHS), founded in 1948 to examine the epidemiology of cardiovascular disease, is among the most comprehensively characterized multi-generational studies in the world. Many collected phenotypes have substantial genetic contributors; yet most genetic determinants remain to be identified. Using single nucleotide polymorphisms (SNPs) from a 100K genome-wide scan, we examine the associations of common polymorphisms with phenotypic variation in this community-based cohort and provide a full-disclosure, web-based resource of results for future replication studies. METHODS: Adult participants (n = 1345) of the largest 310 pedigrees in the FHS, many biologically related, were genotyped with the 100K Affymetrix GeneChip. These genotypes were used to assess their contribution to 987 phenotypes collected in FHS over 56 years of follow up, including: cardiovascular risk factors and biomarkers; subclinical and clinical cardiovascular disease; cancer and longevity traits; and traits in pulmonary, sleep, neurology, renal, and bone domains. We conducted genome-wide variance components linkage and population-based and family-based association tests. RESULTS: The participants were white of European descent and from the FHS Original and Offspring Cohorts (examination 1 Offspring mean age 32 ± 9 years, 54% women). This overview summarizes the methods, selected findings and limitations of the results presented in the accompanying series of 17 manuscripts. The presented association results are based on 70,897 autosomal SNPs meeting the following criteria: minor allele frequency ≥ 10%, genotype call rate ≥ 80%, Hardy-Weinberg equilibrium p-value ≥ 0.001, and satisfying Mendelian consistency. Linkage analyses are based on 11,200 SNPs and short-tandem repeats. Results of phenotype-genotype linkages and associations for all autosomal SNPs are posted on the NCBI dbGaP website at. CONCLUSION: We have created a full-disclosure resource of results, posted on the dbGaP website, from a genome-wide association study in the FHS. Because we used three analytical approaches to examine the association and linkage of 987 phenotypes with thousands of SNPs, our results must be considered hypothesis-generating and need to be replicated. Results from the FHS 100K project with NCBI web posting provides a resource for investigators to identify high priority findings for replication."
GEORGE T O'CONNOR,Sleep-Disordered Breathing and Mortality: A Prospective Cohort Study,"In a cohort of 6,441 volunteers followed over an average of 8.2 years, Naresh Punjabi and colleagues find sleep-disordered breathing to be independently associated with mortality and identify predictive characteristics. BACKGROUND. Sleep-disordered breathing is a common condition associated with adverse health outcomes including hypertension and cardiovascular disease. The overall objective of this study was to determine whether sleep-disordered breathing and its sequelae of intermittent hypoxemia and recurrent arousals are associated with mortality in a community sample of adults aged 40 years or older. METHODS AND FINDINGS. We prospectively examined whether sleep-disordered breathing was associated with an increased risk of death from any cause in 6,441 men and women participating in the Sleep Heart Health Study. Sleep-disordered breathing was assessed with the apnea–hypopnea index (AHI) based on an in-home polysomnogram. Survival analysis and proportional hazards regression models were used to calculate hazard ratios for mortality after adjusting for age, sex, race, smoking status, body mass index, and prevalent medical conditions. The average follow-up period for the cohort was 8.2 y during which 1,047 participants (587 men and 460 women) died. Compared to those without sleep-disordered breathing (AHI: >5 events/h), the fully adjusted hazard ratios for all-cause mortality in those with mild (AHI: 5.0–14.9 events/h), moderate (AHI: 15.0–29.9 events/h), and severe (AHI: ≥30.0 events/h) sleep-disordered breathing were 0.93 (95% CI: 0.80–1.08), 1.17 (95% CI: 0.97–1.42), and 1.46 (95% CI: 1.14–1.86), respectively. Stratified analyses by sex and age showed that the increased risk of death associated with severe sleep-disordered breathing was statistically significant in men aged 40–70 y (hazard ratio: 2.09; 95% CI: 1.31–3.33). Measures of sleep-related intermittent hypoxemia, but not sleep fragmentation, were independently associated with all-cause mortality. Coronary artery disease–related mortality associated with sleep-disordered breathing showed a pattern of association similar to all-cause mortality. CONCLUSIONS. Sleep-disordered breathing is associated with all-cause mortality and specifically that due to coronary artery disease, particularly in men aged 40–70 y with severe sleep-disordered breathing."
GEORGE T O'CONNOR,A Genome-Wide Association Study of Pulmonary Function Measures in the Framingham Heart Study,"The ratio of forced expiratory volume in one second to forced vital capacity (FEV1/FVC) is a measure used to diagnose airflow obstruction and is highly heritable. We performed a genome-wide association study in 7,691 Framingham Heart Study participants to identify single-nucleotide polymorphisms (SNPs) associated with the FEV1/FVC ratio, analyzed as a percent of the predicted value. Identified SNPs were examined in an independent set of 835 Family Heart Study participants enriched for airflow obstruction. Four SNPs in tight linkage disequilibrium on chromosome 4q31 were associated with the percent predicted FEV1/FVC ratio with p-values of genome-wide significance in the Framingham sample (best p-value = 3.6e-09). One of the four chromosome 4q31 SNPs (rs13147758; p-value 2.3e-08 in Framingham) was genotyped in the Family Heart Study and produced evidence of association with the same phenotype, percent predicted FEV1/FVC (p-value = 2.0e-04). The effect estimates for association in the Framingham and Family Heart studies were in the same direction, with the minor allele (G) associated with higher FEV1/FVC ratio levels. Results from the Family Heart Study demonstrated that the association extended to FEV1 and dichotomous airflow obstruction phenotypes, particularly among smokers. The SNP rs13147758 was associated with the percent predicted FEV1/FVC ratio in independent samples from the Framingham and Family Heart Studies producing a combined p-value of 8.3e-11, and this region of chromosome 4 around 145.68 megabases was associated with COPD in three additional populations reported in the accompanying manuscript. The associated SNPs do not lie within a gene transcript but are near the hedgehog-interacting protein (HHIP) gene and several expressed sequence tags cloned from fetal lung. Though it is unclear what gene or regulatory effect explains the association, the region warrants further investigation. Author Summary Cigarette smoking is the primary risk factor for impaired lung function, yet only 20% of smokers develop chronic obstructive pulmonary disease (COPD). This observation, along with family studies of lung function and COPD, suggests that genetic factors influence susceptibility to cigarette smoke. We examined the relationship between common genetic variants and measures of lung function in a sample of 7,691 participants from the Framingham Heart Study and confirmed our observations in 835 participants from the Family Heart Study selected to include cases of airflow obstruction. We identified a variant on chromosome 4 that was strongly associated with FEV1/FVC in the Framingham Study and confirmed the association in the Family Heart Study. The accompanying manuscript identified the same region to be associated with COPD. Several interesting genes are present in the region that we identified, including a gene (HHIP) interacting with a biological pathway involved in lung development, but it is not yet clear which gene in the region explains the association. Our results identified a region of chromosome 4 that warrants further study to understand the genetic effects influencing lung function."
GEORGE T O'CONNOR,Eosinophil and T Cell Markers Predict Functional Decline in COPD Patients,"BACKGROUND. The major marker utilized to monitor COPD patients is forced expiratory volume in one second (FEV1). However, asingle measurement of FEV1 cannot reliably predict subsequent decline. Recent studies indicate that T lymphocytes and eosinophils are important determinants of disease stability in COPD. We therefore measured cytokine levels in the lung lavage fluid and plasma of COPD patients in order to determine if the levels of T cell or eosinophil related cytokines were predictive of the future course of the disease. METHODS. Baseline lung lavage and plasma samples were collected from COPD subjects with moderately severe airway obstruction and emphysematous changes on chest CT. The study participants were former smokers who had not had a disease exacerbation within the past six months or used steroids within the past two months. Those subjects who demonstrated stable disease over the following six months (ΔFEV1 % predicted = 4.7 ± 7.2; N = 34) were retrospectively compared with study participants who experienced a rapid decline in lung function (ΔFEV1 % predicted = -16.0 ± 6.0; N = 16) during the same time period and with normal controls (N = 11). Plasma and lung lavage cytokines were measured from clinical samples using the Luminex multiplex kit which enabled the simultaneous measurement of several T cell and eosinophil related cytokines. RESULTS AND DISCUSSION. Stable COPD participants had significantly higher plasma IL-2 levels compared to participants with rapidly progressive COPD (p = 0.04). In contrast, plasma eotaxin-1 levels were significantly lower in stable COPD subjects compared to normal controls (p < 0.03). In addition, lung lavage eotaxin-1 levels were significantly higher in rapidly progressive COPD participants compared to both normal controls (p < 0.02) and stable COPD participants (p < 0.05). CONCLUSION. These findings indicate that IL-2 and eotaxin-1 levels may be important markers of disease stability in advanced emphysema patients. Prospective studies will need to confirm whether measuring IL-2 or eotaxin-1 can identify patients at risk for rapid disease progression."
GEORGE T O'CONNOR,"The Urban Environment and Childhood Asthma (URECA) Birth Cohort Study: Design, Methods, and Study Population","BACKGROUND. The incidence and morbidity of wheezing illnesses and childhood asthma is especially high in poor urban areas. This paper describes the study design, methods, and population of the Urban Environment and Childhood Asthma (URECA) study, which was established to investigate the immunologic causes of asthma among inner-city children. METHODS AND RESULTS. URECA is an observational prospective study that enrolled pregnant women in central urban areas of Baltimore, Boston, New York City, and St. Louis and is following their offspring from birth through age 7 years. The birth cohort consists of 560 inner-city children who have at least one parent with an allergic disease or asthma, and all families live in areas in which at least 20% of the population has incomes below the poverty line. In addition, 49 inner-city children with no parental history of allergies or asthma were enrolled. The primary hypothesis is that specific urban exposures in early life promote a unique pattern of immune development (impaired antiviral and increased Th2 responses) that increases the risk of recurrent wheezing and allergic sensitization in early childhood, and of asthma by age 7 years. To track immune development, cytokine responses of blood mononuclear cells stimulated ex vivo are measured at birth and then annually. Environmental assessments include allergen and endotoxin levels in house dust, pre- and postnatal maternal stress, and indoor air nicotine and nitrogen dioxide. Nasal mucous samples are collected from the children during respiratory illnesses and analyzed for respiratory viruses. The complex interactions between environmental exposures and immune development will be assessed with respect to recurrent wheeze at age 3 years and asthma at age 7 years. CONCLUSION. The overall goal of the URECA study is to develop a better understanding of how specific urban exposures affect immune development to promote wheezing illnesses and asthma."
GEORGE T O'CONNOR,"BMQ : Boston medical quarterly: v. 5, no. 1-4",
GEORGE T O'CONNOR,First radial velocity results from the MINiature Exoplanet Radial Velocity Array (MINERVA),"The MINiature Exoplanet Radial Velocity Array (MINERVA) is a dedicated observatory of four 0.7 m robotic telescopes fiber-fed to a KiwiSpec spectrograph. The MINERVA mission is to discover super-Earths in the habitable zones of nearby stars. This can be accomplished with MINERVA's unique combination of high precision and high cadence over long time periods. In this work, we detail changes to the MINERVA facility that have occurred since our previous paper. We then describe MINERVA's robotic control software, the process by which we perform 1D spectral extraction, and our forward modeling Doppler pipeline. In the process of improving our forward modeling procedure, we found that our spectrograph's intrinsic instrumental profile is stable for at least nine months. Because of that, we characterized our instrumental profile with a time-independent, cubic spline function based on the profile in the cross dispersion direction, with which we achieved a radial velocity precision similar to using a conventional ""sum-of-Gaussians"" instrumental profile: 1.8 m s−1 over 1.5 months on the RV standard star HD 122064. Therefore, we conclude that the instrumental profile need not be perfectly accurate as long as it is stable. In addition, we observed 51 Peg and our results are consistent with the literature, confirming our spectrograph and Doppler pipeline are producing accurate and precise radial velocities."
GEORGE T O'CONNOR,The genome of the vervet ( Chlorocebus aethiops sabaeus ),"We describe a genome reference of the African green monkey or vervet (Chlorocebus aethiops). This member of the Old World monkey (OWM) superfamily is uniquely valuable for genetic investigations of simian immunodeficiency virus (SIV), for which it is the most abundant natural host species, and of a wide range of health-related phenotypes assessed in Caribbean vervets (C. a. sabaeus), whose numbers have expanded dramatically since Europeans introduced small numbers of their ancestors from West Africa during the colonial era. We use the reference to characterize the genomic relationship between vervets and other primates, the intra-generic phylogeny of vervet subspecies, and genome-wide structural variations of a pedigreed C. a. sabaeus population. Through comparative analyseswith human and rhesus macaque, we characterize at high resolution the unique chromosomal fission events that differentiate the vervets and their close relatives from most other catarrhine primates, in whom karyotype is highly conserved. We also provide a summary of transposable elements and contrast these with the rhesus macaque and human. Analysis of sequenced genomes representing each of the main vervet subspecies supports previously hypothesized relationships between these populations, which range across most of sub-Saharan Africa, while uncovering high levels of genetic diversity within each. Sequence-based analyses of major histocompatibility complex (MHC) polymorphisms reveal extremely low diversity in Caribbean C. a. sabaeus vervets, compared to vervets from putatively ancestral West African regions. In the C. a. sabaeus research population, we discover the first structural variations that are, in some cases, predicted to have a deleterious effect; future studies will determine the phenotypic impact of these variations."
GEORGE T O'CONNOR,Constructing custom-made radiotranscriptomic signatures of vascular inflammation from routine CT angiograms: a prospective outcomes validation study in COVID-19,"BACKGROUND: Direct evaluation of vascular inflammation in patients with COVID-19 would facilitate more efficient trials of new treatments and identify patients at risk of long-term complications who might respond to treatment. We aimed to develop a novel artificial intelligence (AI)-assisted image analysis platform that quantifies cytokine-driven vascular inflammation from routine CT angiograms, and sought to validate its prognostic value in COVID-19. METHODS: For this prospective outcomes validation study, we developed a radiotranscriptomic platform that uses RNA sequencing data from human internal mammary artery biopsies to develop novel radiomic signatures of vascular inflammation from CT angiography images. We then used this platform to train a radiotranscriptomic signature (C19-RS), derived from the perivascular space around the aorta and the internal mammary artery, to best describe cytokine-driven vascular inflammation. The prognostic value of C19-RS was validated externally in 435 patients (331 from study arm 3 and 104 from study arm 4) admitted to hospital with or without COVID-19, undergoing clinically indicated pulmonary CT angiography, in three UK National Health Service (NHS) trusts (Oxford, Leicester, and Bath). We evaluated the diagnostic and prognostic value of C19-RS for death in hospital due to COVID-19, did sensitivity analyses based on dexamethasone treatment, and investigated the correlation of C19-RS with systemic transcriptomic changes. FINDINGS: Patients with COVID-19 had higher C19-RS than those without (adjusted odds ratio [OR] 2·97 [95% CI 1·43-6·27], p=0·0038), and those infected with the B.1.1.7 (alpha) SARS-CoV-2 variant had higher C19-RS values than those infected with the wild-type SARS-CoV-2 variant (adjusted OR 1·89 [95% CI 1·17-3·20] per SD, p=0·012). C19-RS had prognostic value for in-hospital mortality in COVID-19 in two testing cohorts (high [≥6·99] vs low [<6·99] C19-RS; hazard ratio [HR] 3·31 [95% CI 1·49-7·33], p=0·0033; and 2·58 [1·10-6·05], p=0·028), adjusted for clinical factors, biochemical biomarkers of inflammation and myocardial injury, and technical parameters. The adjusted HR for in-hospital mortality was 8·24 (95% CI 2·16-31·36, p=0·0019) in patients who received no dexamethasone treatment, but 2·27 (0·69-7·55, p=0·18) in those who received dexamethasone after the scan, suggesting that vascular inflammation might have been a therapeutic target of dexamethasone in COVID-19. Finally, C19-RS was strongly associated (r=0·61, p=0·00031) with a whole blood transcriptional module representing dysregulation of coagulation and platelet aggregation pathways. INTERPRETATION: Radiotranscriptomic analysis of CT angiography scans introduces a potentially powerful new platform for the development of non-invasive imaging biomarkers. Application of this platform in routine CT pulmonary angiography scans done in patients with COVID-19 produced the radiotranscriptomic signature C19-RS, a marker of cytokine-driven inflammation driving systemic activation of coagulation and responsible for adverse clinical outcomes, which predicts in-hospital mortality and might allow targeted therapy. FUNDING: Engineering and Physical Sciences Research Council, British Heart Foundation, Oxford BHF Centre of Research Excellence, Innovate UK, NIHR Oxford Biomedical Research Centre, Wellcome Trust, Onassis Foundation."
ROBERT DAVEY,Butterfly genome reveals promiscuous exchange of mimicry adaptations among species,"The evolutionary importance of hybridization and introgression has long been debated1. Hybrids are usually rare and unfit, but even infrequent hybridization can aid adaptation by transferring beneficial traits between species. Here we use genomic tools to investigate introgression in Heliconius, a rapidly radiating genus of neotropical butterflies widely used in studies of ecology, behaviour, mimicry and speciation2,3,4,5. We sequenced the genome of Heliconius melpomene and compared it with other taxa to investigate chromosomal evolution in Lepidoptera and gene flow among multiple Heliconius species and races. Among 12,669 predicted genes, biologically important expansions of families of chemosensory and Hox genes are particularly noteworthy. Chromosomal organization has remained broadly conserved since the Cretaceous period, when butterflies split from the Bombyx (silkmoth) lineage. Using genomic resequencing, we show hybrid exchange of genes between three co-mimics, Heliconius melpomene, Heliconius timareta and Heliconius elevatus, especially at two genomic regions that control mimicry pattern. We infer that closely related Heliconius species exchange protective colour-pattern genes promiscuously, implying that hybridization has an important role in adaptive radiation."
ROBERT DAVEY,"High-throughput, high-resolution interferometric light microscopy of biological nanoparticles","Label-free, visible light microscopy is an indispensable tool for studying biological nanoparticles (BNPs). However, conventional imaging techniques have two major challenges: (i) weak contrast due to low-refractive-index difference with the surrounding medium and exceptionally small size and (ii) limited spatial resolution. Advances in interferometric microscopy have overcome the weak contrast limitation and enabled direct detection of BNPs, yet lateral resolution remains as a challenge in studying BNP morphology. Here, we introduce a wide-field interferometric microscopy technique augmented by computational imaging to demonstrate a 2-fold lateral resolution improvement over a large field-of-view (>100 × 100 μm2), enabling simultaneous imaging of more than 104 BNPs at a resolution of ∼150 nm without any labels or sample preparation. We present a rigorous vectorial-optics-based forward model establishing the relationship between the intensity images captured under partially coherent asymmetric illumination and the complex permittivity distribution of nanoparticles. We demonstrate high-throughput morphological visualization of a diverse population of Ebola virus-like particles and a structurally distinct Ebola vaccine candidate. Our approach offers a low-cost and robust label-free imaging platform for high-throughput and high-resolution characterization of a broad size range of BNPs."
ROBERT DAVEY,A genome-wide association study reveals variants in ARL15 that influence adiponectin levels,"The adipocyte-derived protein adiponectin is highly heritable and inversely associated with risk of type 2 diabetes mellitus (T2D) and coronary heart disease (CHD). We meta-analyzed 3 genome-wide association studies for circulating adiponectin levels (n = 8,531) and sought validation of the lead single nucleotide polymorphisms (SNPs) in 5 additional cohorts (n = 6,202). Five SNPs were genome-wide significant in their relationship with adiponectin (P≤5×10−8). We then tested whether these 5 SNPs were associated with risk of T2D and CHD using a Bonferroni-corrected threshold of P≤0.011 to declare statistical significance for these disease associations. SNPs at the adiponectin-encoding ADIPOQ locus demonstrated the strongest associations with adiponectin levels (P-combined = 9.2×10−19 for lead SNP, rs266717, n = 14,733). A novel variant in the ARL15 (ADP-ribosylation factor-like 15) gene was associated with lower circulating levels of adiponectin (rs4311394-G, P-combined = 2.9×10−8, n = 14,733). This same risk allele at ARL15 was also associated with a higher risk of CHD (odds ratio [OR] = 1.12, P = 8.5×10−6, n = 22,421) more nominally, an increased risk of T2D (OR = 1.11, P = 3.2×10−3, n = 10,128), and several metabolic traits. Expression studies in humans indicated that ARL15 is well-expressed in skeletal muscle. These findings identify a novel protein, ARL15, which influences circulating adiponectin levels and may impact upon CHD risk. Author Summary Through a meta-analysis of genome-wide association studies of 14,733 individuals, we identified common base-pair variants in the genome which influence circulating adiponectin levels. Since adiponectin is an adipocyte-derived circulating protein which has been inversely associated with risk of obesity-related diseases such as type 2 diabetes (T2D) and coronary heart disease (CHD), we next sought to understand if the identified variants influencing adiponectin levels also influence risk of T2D, CHD, and several metabolic traits. In addition to confirming that variation at the ADIPOQ locus influences adiponectin levels, our analyses point to a variant in the ARL15 (ADP-ribosylation factor-like 15) locus which decreases adiponectin levels and increases risk of CHD and T2D. Further, this same variant was associated with increased fasting insulin levels and glycated hemoglobin. While the function of ARL15 is not known, we provide insight into the tissue specificity of ARL15 expression. These results thus provide novel insights into the physiology of the adiponectin pathway and obesity-related diseases."
JIANJUN MIAO,Monetary policy and rational asset bubbles: Comments,"We revisit Galí’s (2014) analysis by extending his model to incorporate persistent bubble shocks. We find that, under adaptive learning, a stable bubbly steady state and the associated sunspot solutions under optimal monetary policy are not E-stable. When deriving the unique forward-looking minimum stable variable (MSV) solution around an unstable bubbly steady state, we obtain results that are consistent with the conventional views: leaning against the wind policy reduces bubble volatility and is optimal. Such a steady state and the associated MSV solution are E-stable."
JIANJUN MIAO,"Convergence, financial development, and policy analysis","We study the relationship among inflation, economic growth, and financial development in a Schumpeterian overlapping generations model with credit constraints. In the baseline case, money is super-neutral. When the financial development exceeds some critical level, the economy catches up and then converges to the growth rate of the world technology frontier. Otherwise, the economy converges to a poverty trap with a growth rate lower than the frontier and with inflation decreasing with the level of financial development. We then study efficient allocation and identify the sources of inefficiency in a market equilibrium. We show that a particular combination of monetary and fiscal policies can make a market equilibrium attain the efficient allocation."
JIANJUN MIAO,Asset bubbles and credit constraints,We provide a theory of rational stock price bubbles in production economies with infinitely lived agents. Firms meet stochastic investment opportunities and face endogenous credit constraints. They are not fully committed to repaying debt. Credit constraints are derived from incentive constraints in optimal contracts which ensure default never occurs in equilibrium. Stock price bubbles can emerge through a positive feedback loop mechanism and cannot be ruled out by transversality conditions. These bubbles command a liquidity premium and raise investment by raising the debt limit. Their collapse leads to a recession and a stock market crash.
JIANJUN MIAO,Aversion to ambiguity and model misspecification in dynamic stochastic environments,"Preferences that accommodate aversion to subjective uncertainty and its potential misspecification in dynamic settings are a valuable tool of analysis in many disciplines. By generalizing previous analyses, we propose a tractable approach to incorporating broadly conceived responses to uncertainty. We illustrate our approach on some stylized stochastic environments. By design, these discrete time environments have revealing continuous time limits. Drawing on these illustrations, we construct recursive representations of intertemporal preferences that allow for penalized and smooth ambiguity aversion to subjective uncertainty. These recursive representations imply continuous time limiting Hamilton–Jacobi–Bellman equations for solving control problems in the presence of uncertainty."
JIANJUN MIAO,The perils of credit booms,"We present a dynamic general equilibrium model of production economies with adverse selection in the financial market to study the interaction between funding liquidity and market liquidity and its impact on business cycles. Entrepreneurs can take on short-term collateralized debt and trade long-term assets to finance investment. Funding liquidity can erode market liquidity. High funding liquidity discourages firms from selling their good long-term assets since these good assets have to subsidize lemons when there is information asymmetry. This can cause a liquidity dry-up in the market for long-term assets and even a market breakdown, resulting in a financial crisis. Multiple equilibria can coexist. Credit booms combined with changes in beliefs can cause equilibrium regime shifts, leading to an economic crisis or expansion."
JIANJUN MIAO,Woodford's approach to robust policy analysis in a linear-quadratic framework,This paper extends Woodford's approach to the robustly optimal monetary policy to a general linear quadratic framework. We provide algorithms to solve for a time-invariant linear robustly optimal policy in a timeless perspective and for a time-invariant linear Markov perfect equilibrium under discretion. We apply our methods to a New Keynesian model of monetary policy with persistent cost-push shocks and inflation persistence. We find that the robustly optimal commitment inflation is less responsive to a cost-push shock when the shock is more persistent and that the robustly optimal discretionary policy is more responsive to lagged inflation when inflation is more persistent.
JIANJUN MIAO,Dynamic discrete choice under rational inattention,"We adopt the posterior-based approach to study dynamic discrete choice problems under rational inattention. We provide necessary and sufficient conditions to characterize the solution for the additive class of uniformly posterior-separable cost functions. We propose an efficient algorithm to solve these conditions and apply our model to explain phenomena such as status quo bias, confirmation bias, and belief polarization. A key condition for our approach to work is the concavity of the difference between the generalized entropy of the current posterior and the discounted generalized entropy of the prior beliefs about the future states."
JIANJUN MIAO,Multivariate rational inattention,"We study optimal control problems in the multivariate linear-quadratic-Gaussian framework under rational inattention and show that the multivariate attention allocation problem can be reduced to a dynamic tracking problem in information theory. We propose a general solution method to solve this problem by using rate distortion functions and semidefinite programming and derive the optimal form and dimension of signals without strong prior restrictions. We provide generalized reverse water-filling solutions for some special cases. Applying our method to solve three multivariate economic models, we obtain some results qualitatively different from the literature."
JIANJUN MIAO,Macro-financial volatility under dispersed information,"We provide a production-based asset pricing model with dispersed information and small deviations from full rational expectations. In the model, aggregate output and equity prices depend on the higher-order beliefs about aggregate demand and individual stochastic discount factors. We prove that equity price volatility becomes arbitrarily large as the volatility of idiosyncratic shocks diverges to infinity due to the interaction of signal-extraction with idiosyncratic trading decisions, while aggregate output volatility falls. We propose a two-step spectral factorization method that permits closed-form solutions in the frequency domain applicable to a wide range of models with more hidden states than signals. Our model can quantitatively match output and equity volatilities observed in US data."
JIANJUN MIAO,Intertemporal substitution and recursive smooth ambiguity preferences,"In this paper, we establish an axiomatically founded generalized recursive smooth ambiguity model that allows for a separation among intertemporal substitution, risk aversion, and ambiguity aversion. We axiomatize this model using two approaches: the second-order act approach à la Klibanoff, Marinacci, and Mukerji (2005) and the two-stage randomization approach à la Seo (2009). We characterize risk attitude and ambiguity attitude within these two approaches. We then discuss our model's application in asset pricing. Our recursive preference model nests some popular models in the literature as special cases."
JIANJUN MIAO,Firm heterogeneity and the long-run effects of dividend tax reform,"To study the long-run effect of dividend taxation on aggregate capital accumulation, we build a dynamic general equilibrium model in which there is a continuum of firms subject to idiosyncratic productivity shocks. We find that a dividend tax cut raises aggregate productivity by reducing the frictions in the reallocation of capital across firms. Our baseline model simulations show that when both dividend and capital gains tax rates are cut from 25 and 20 percent, respectively, to the same 15 percent level permanently, the aggregate long-run capital stock increases by about 4 percent. (JEL D21, E22, E62, G32, G35, H25, H32)"
JIANJUN MIAO,A bayesian dynamic stochastic general equilibrium model of stock market bubbles and business cycles,We present an estimated DSGE model of stock market bubbles and business cycles using Bayesian methods. Bubbles emerge through a positive feedback loop mechanism supported by self-fulfilling beliefs. We identify a sentiment shock which drives the movements of bubbles and is transmitted to the real economy through endogenous credit constraints. This shock explains more than 96 percent of the stock market volatility and about 25 to 45 percent of the variations in investment and output. It generates the comovements between stock prices and macroeconomic quantities and is the dominant force in driving the internet bubbles and the Great Recession.
JIANJUN MIAO,Ambiguity aversion and the variance premium,"This paper offers an ambiguity-based interpretation of the variance premium — the difference between risk-neutral and objective expectations of market return variance — as a compounding effect of both belief distortion and variance differential regarding the uncertain economic regimes. Our calibrated model can match the variance premium, the equity premium, and the risk-free rate in the data. We find that about 97% of the mean–variance premium can be attributed to ambiguity aversion. A three-way separation among ambiguity aversion, risk aversion, and intertemporal substitution, permitted by the smooth ambiguity preferences, plays a key role in our model’s quantitative performance."
JIANJUN MIAO,Bubbles and total factor productivity,This paper presents an infinite-horizon model of production economies in which firms face idiosyncratic productivity shocks and are subject to endogenous credit constraints. Credit-driven stock price bubbles can arise which can relax credit constraints and reallocate capital more efficiently among firms. The collapse of bubbles causes a fall of total factor productivity.
M. DANIELE PASERMAN,Gender differences in cooperative environments? Evidence from the U.S. Congress,"This paper uses data on bill sponsorship and cosponsorship in the U.S. House of Representatives to estimate gender differences in cooperative behavior. We employ a number of econometric methodologies to address the potential selection of female representatives into electoral districts with distinct preferences for cooperativeness, including regression discontinuity and matching. After accounting for selection, we find that among Democrats there is no significant gender gap in the number of cosponsors recruited, but women-sponsored bills tend to have fewer cosponsors from the opposite party. On the other hand, we find robust evidence that Republican women recruit more cosponsors and attract more bipartisan support on the bills that they sponsor. This is particularly true on bills that address issues more relevant for women, over which female Republicans have possibly preferences that are closer to those of Democrats. We interpret these results as evidence that cooperation is mostly driven by a commonality of interest, rather than gender per se."
M. DANIELE PASERMAN,"In the name of the son (and the daughter): intergenerational mobility in the United States, 1850-1940","This paper estimates historical intergenerational elasticities between fathers and children of both sexes in the United States using a novel empirical strategy. The key insight of our approach is that the information about socioeconomic status conveyed by first names can be used to create pseudo-links across generations. We find that both father-son and father-daughter elasticities were flat during the nineteenth century, increased sharply between 1900 and 1920, and declined slightly thereafter. We discuss the role of regional disparities in economic development, trends in inequality and returns to human capital, and the marriage market in explaining these patterns."
M. DANIELE PASERMAN,The effect of female leadership on establishment and employee outcomes: evidence from linked employer-employee data,"In this paper we use a large linked employer-employee data set on German establishments between 1993 and 2012 to investigate how the gender composition of the top layer of management affects a variety of establishment and worker outcomes. We use two different measures to identify the gender composition of the top layer based on direct survey data: the fraction of women among top managers, and the fraction of women among working proprietors. We document the following facts: a) There is a strong negative association between the fraction of women in the top layer of management and several establishment outcomes, among them business volume, investment, total wage bill per worker, total employment, and turnover; b) Establishments with a high fraction of women in the top layer of management are more likely to implement female-friendly policies, such as providing childcare facilities or promoting and mentoring female junior staff; c) The fraction of women in the top layer of management is also negatively associated with employment and wages, both male and female, full-time and part-time. However, all of these associations vanish when we include establishment fixed effects and establishment-specific time trends. This reveals a substantial sorting of female managers across establishments: small and less productive establishments that invest less, pay their employees lower wages, but are more female-friendly are more likely to be led by women."
M. DANIELE PASERMAN,"In the name of the Father: marriage and intergenerational mobility in the United States, 1850-1930","This paper constructs a continuous and consistent measure of intergenerational mobility in the United States between 1850 and 1930 by linking individuals with the same first name across pairs of decennial Censuses. One of the advantages of this methodology is that it allows to calculate intergenerational correlations not only between fathers and sons, but also between fathers-in-law and sons-in-law, something that is typically not possible with historical data. Thus, the paper sheds light on the role of marriage in the intergenerational transmis- sion of economic status from a historical perspective. We find that the father-son correlation in economic status grows throughout the period, but is consistently lower than the correlation between fathers-in-law and sons-in-law. The gap declines over time, and seems to have closed by the end of the period. We present a simple model of investment in human capital, marital sorting and intergenerational mobility that can rationalize the ?ndings."
M. DANIELE PASERMAN,Using the two-period model to understand investment in human capital,"In many textbooks, the decision to invest in human capital is presented in terms of the present discounted value of the lifetime stream of costs and benefits associated with the investment. I argue that this approach, while delivering some useful insights, also conflates subjective discount rates and market interest rates, obfuscates the role of credit market imperfections, and makes difficult the analysis of policy interventions and the effects of external shocks on human capital investment. I show instead how a simple two-period model can deliver the main insights about investment in human capital and is flexible enough to be used to model a wide variety of policy interventions."
M. DANIELE PASERMAN,Bayesian inference for duration data with unobserved and unknown heterogeneity: Monte Carlo evidence and an application,"This paper describes a semiparametric Bayesian method for analyzing duration data. The proposed estimator specifies a complete functional form for duration spells, but allows flexibility by introducing an individual heterogeneity term, which follows a Dirichlet mixture distribution. I show how to obtain predictive distributions for duration data that correctly account for the uncertainty present in the model. I also directly compare the performance of the proposed estimator with Heckman and Singer's (1984) Non Parametric Maximum Likelihood Estimator (NPMLE). The methodology is applied to the analysis of youth unemployment spells. Compared to the NPMLE, the proposed estimator reflects more accurately the uncertainty surrounding the heterogeneity distribution."
M. DANIELE PASERMAN,Gender differences in cooperative environments? Evidence from the U.S. Congress,"This paper uses data on bill co-sponsorship in the U.S. House of Representatives to estimate gender differences in cooperative behaviour. We find that among Democrats there is no significant gender gap in the number of co-sponsors recruited, but women-sponsored bills tend to have fewer co-sponsors from the opposite party. On the other hand, we find robust evidence that Republican women recruit more co-sponsors and attract more bipartisan support on the bills that they sponsor. We interpret these results as evidence that cooperation is mostly driven by a commonality of interest, rather than gender per se."
M. DANIELE PASERMAN,"Economics couldn’t predict the financial crisis, but can it predict the World Cup?",
M. DANIELE PASERMAN,"Three-generation mobility in the United States, 1850-1940: the role of maternal and paternal grandparents","This paper estimates intergenerational elasticities across three generations in the United States in the late 19th and early 20th centuries, exploring how maternal and paternal grandfathers predict the economic status of their grandsons and granddaughters. We document that the relationship between the income of grandparents and grandchildren differs by gender. The socio-economic status of grandsons is more strongly associated with the status of paternal grandfathers than maternal grandfathers. The status of maternal grandfathers is more strongly correlated with the status of granddaughters than grandsons, while the opposite is true for paternal grandfathers. We argue that the findings can be rationalized by a model of gender-specific intergenerational transmission of traits and imperfect assortative mating."
KAMIL EKINCI,Nanomechanical motion transducers for miniaturized mechanical systems,"Reliable operation of a miniaturized mechanical system requires that nanomechanical motion be transduced into electrical signals (and vice versa) with high fidelity and in a robust manner. Progress in transducer technologies is expected to impact numerous emerging and future applications of micro- and, especially, nanoelectromechanical systems (MEMS and NEMS); furthermore, high-precision measurements of nanomechanical motion are broadly used to study fundamental phenomena in physics and biology. Therefore, development of nanomechanical motion transducers with high sensitivity and bandwidth has been a central research thrust in the fields of MEMS and NEMS. Here, we will review recent progress in this rapidly-advancing area."
KAMIL EKINCI,All-electrical monitoring of bacterial antibiotic susceptibility in a microfluidic device,"The lack of rapid antibiotic susceptibility tests adversely affects the treatment of bacterial infections and contributes to increased prevalence of multidrug-resistant bacteria. Here, we describe an all-electrical approach that allows for ultrasensitive measurement of growth signals from only tens of bacteria in a microfluidic device. Our device is essentially a set of microfluidic channels, each with a nanoconstriction at one end and cross-sectional dimensions close to that of a single bacterium. Flowing a liquid bacteria sample (e.g., urine) through the microchannels rapidly traps the bacteria in the device, allowing for subsequent incubation in drugs. We measure the electrical resistance of the microchannels, which increases (or decreases) in proportion to the number of bacteria in the microchannels. The method and device allow for rapid antibiotic susceptibility tests in about 2 h. Further, the short-time fluctuations in the electrical resistance during an antibiotic susceptibility test are correlated with the morphological changes of bacteria caused by the antibiotic. In contrast to other electrical approaches, the underlying geometric blockage effect provides a robust and sensitive signal, which is straightforward to interpret without electrical models. The approach also obviates the need for a high-resolution microscope and other complex equipment, making it potentially usable in resource-limited settings."
KAMIL EKINCI,Nanomechanical Measurement of the Brownian Force Noise in a Viscous Liquid,
KAMIL EKINCI,Optimization of piezoresistive motion detection for ambient NEMS applications,
KAMIL EKINCI,Direct laser writing for cardiac tissue engineering: a microfluidic heart on a chip with integrated transducers,"We have developed a microfluidic platform for engineering cardiac microtissues in highly-controlled microenvironments. The platform is fabricated using direct laser writing (DLW) lithography and soft lithography, and contains four separate devices. Each individual device houses a cardiac microtissue and is equipped with an integrated strain actuator and a force sensor. Application of external pressure waves to the platform results in controllable time-dependent forces on the microtissues. Conversely, oscillatory forces generated by the microtissues are transduced into measurable electrical outputs. We demonstrate the capabilities of this platform by studying the response of cardiac microtissues derived from human induced pluripotent stem cells (hiPSC) under prescribed mechanical loading and pacing. This platform will be used for fundamental studies and drug screening on cardiac microtissues."
B LEE ROBERTS,First M87 Event Horizon Telescope results. III. Data processing and calibration,"We present the calibration and reduction of Event Horizon Telescope (EHT) 1.3 mm radio wavelength observations of the supermassive black hole candidate at the center of the radio galaxy M87 and the quasar 3C 279, taken during the 2017 April 5–11 observing campaign. These global very long baseline interferometric observations include for the first time the highly sensitive Atacama Large Millimeter/submillimeter Array (ALMA); reaching an angular resolution of 25 μas, with characteristic sensitivity limits of ~1 mJy on baselines to ALMA and ~10 mJy on other baselines. The observations present challenges for existing data processing tools, arising from the rapid atmospheric phase fluctuations, wide recording bandwidth, and highly heterogeneous array. In response, we developed three independent pipelines for phase calibration and fringe detection, each tailored to the specific needs of the EHT. The final data products include calibrated total intensity amplitude and phase information. They are validated through a series of quality assurance tests that show consistency across pipelines and set limits on baseline systematic errors of 2% in amplitude and 1° in phase. The M87 data reveal the presence of two nulls in correlated flux density at ~3.4 and ~8.3 Gλ and temporal evolution in closure quantities, indicating intrinsic variability of compact structure on a timescale of days, or several light-crossing times for a few billion solar-mass black hole. These measurements provide the first opportunity to image horizon-scale structure in M87."
B LEE ROBERTS,First M87 Event Horizon Telescope results. V. Physical origin of the asymmetric ring,"The Event Horizon Telescope (EHT) has mapped the central compact radio source of the elliptical galaxy M87 at 1.3 mm with unprecedented angular resolution. Here we consider the physical implications of the asymmetric ring seen in the 2017 EHT data. To this end, we construct a large library of models based on general relativistic magnetohydrodynamic (GRMHD) simulations and synthetic images produced by general relativistic ray tracing. We compare the observed visibilities with this library and confirm that the asymmetric ring is consistent with earlier predictions of strong gravitational lensing of synchrotron emission from a hot plasma orbiting near the black hole event horizon. The ring radius and ring asymmetry depend on black hole mass and spin, respectively, and both are therefore expected to be stable when observed in future EHT campaigns. Overall, the observed image is consistent with expectations for the shadow of a spinning Kerr black hole as predicted by general relativity. If the black hole spin and M87's large scale jet are aligned, then the black hole spin vector is pointed away from Earth. Models in our library of non-spinning black holes are inconsistent with the observations as they do not produce sufficiently powerful jets. At the same time, in those models that produce a sufficiently powerful jet, the latter is powered by extraction of black hole spin energy through mechanisms akin to the Blandford-Znajek process. We briefly consider alternatives to a black hole for the central compact object. Analysis of existing EHT polarization data and data taken simultaneously at other wavelengths will soon enable new tests of the GRMHD models, as will future EHT campaigns at 230 and 345 GHz."
B LEE ROBERTS,First M87 Event Horizon Telescope results. VI. The shadow and mass of the central black hole,"We present measurements of the properties of the central radio source in M87 using Event Horizon Telescope data obtained during the 2017 campaign. We develop and fit geometric crescent models (asymmetric rings with interior brightness depressions) using two independent sampling algorithms that consider distinct representations of the visibility data. We show that the crescent family of models is statistically preferred over other comparably complex geometric models that we explore. We calibrate the geometric model parameters using general relativistic magnetohydrodynamic (GRMHD) models of the emission region and estimate physical properties of the source. We further fit images generated from GRMHD models directly to the data. We compare the derived emission region and black hole parameters from these analyses with those recovered from reconstructed images. There is a remarkable consistency among all methods and data sets. We find that >50% of the total flux at arcsecond scales comes from near the horizon, and that the emission is dramatically suppressed interior to this region by a factor >10, providing direct evidence of the predicted shadow of a black hole. Across all methods, we measure a crescent diameter of 42 ± 3 μas and constrain its fractional width to be <0.5. Associating the crescent feature with the emission surrounding the black hole shadow, we infer an angular gravitational radius of GM/Dc^2 = 3.8 ± 0.4 μas. Folding in a distance measurement of {16.8}_{-0.7}^{+0.8}{Mpc} gives a black hole mass of M = 6.5 ± 0.2{| }_{stat} ± 0.7{| }_{sys} × {10}^{9} {M}_{odot }. This measurement from lensed emission near the event horizon is consistent with the presence of a central Kerr black hole, as predicted by the general theory of relativity."
B LEE ROBERTS,Magnetic-field measurement and analysis for the Muon g−2 Experiment at Fermilab,"The Fermi National Accelerator Laboratory (FNAL) Muon g−2 Experiment has measured the anomalous precession frequency aμ≡(gμ−2)/2 of the muon to a combined precision of 0.46 parts per million with data collected during its first physics run in 2018. This paper documents the measurement of the magnetic field in the muon storage ring. The magnetic field is monitored by systems and calibrated in terms of the equivalent proton spin precession frequency in a spherical water sample at 34.7∘C. The measured field is weighted by the muon distribution resulting in ˜ω′p, the denominator in the ratio ωa/˜ω′p that together with known fundamental constants yields aμ. The reported uncertainty on ˜ω′p for the Run-1 data set is 114 ppb consisting of uncertainty contributions from frequency extraction, calibration, mapping, tracking, and averaging of 56 ppb, and contributions from fast transient fields of 99 ppb."
B LEE ROBERTS,The Event Horizon general relativistic magnetohydrodynamic code comparison project,"Recent developments in compact object astrophysics, especially the discovery of merging neutron stars by LIGO, the imaging of the black hole in M87 by the Event Horizon Telescope, and high- precision astrometry of the Galactic Center at close to the event horizon scale by the GRAVITY experiment motivate the development of numerical source models that solve the equations of general relativistic magnetohydrodynamics (GRMHD). Here we compare GRMHD solutions for the evolution of a magnetized accretion flow where turbulence is promoted by the magnetorotational instability from a set of nine GRMHD codes: Athena++, BHAC, Cosmos++, ECHO, H-AMR, iharm3D, HARM-Noble, IllinoisGRMHD, and KORAL. Agreement among the codes improves as resolution increases, as measured by a consistently applied, specially developed set of code performance metrics. We conclude that the community of GRMHD codes is mature, capable, and consistent on these test problems."
B LEE ROBERTS,"Centerscope: v. 5, no. 1-5",
B LEE ROBERTS,Age effects in first language attrition: speech perception by Korean-English bilinguals,"This study investigated how bilinguals’ perception of their first language (L1) differs according to age of reduced contact with L1 after immersion in a second language (L2). Twenty-one L1 Korean-L2 English bilinguals in the United States, ranging in age of reduced contact from 3 to 15 years, and 17 control participants in Korea were tested perceptually on three L1 contrasts differing in similarity to L2 contrasts. Compared to control participants, bilinguals were less accurate on L1-specific contrasts, and their accuracy was significantly correlated with age of reduced contact, an effect most pronounced for the contrast most dissimilar to L2. These findings suggest that the earlier bilinguals are extensively exposed to L2, the less likely they are to perceive L1 sounds accurately. However, this relationship is modulated by crosslinguistic similarity, and a turning point in L2 acquisition and L1 attrition of phonology appears to occur at around age 12."
B LEE ROBERTS,"Bostonia: v. 11, no. 1-5, 7, 9-10",
B LEE ROBERTS,Gravitational test beyond the first post-Newtonian order with the shadow of the M87 black hole,"The 2017 Event Horizon Telescope (EHT) observations of the central source in M87 have led to the first measurement of the size of a black-hole shadow. This observation offers a new and clean gravitational test of the black-hole metric in the strong-field regime. We show analytically that spacetimes that deviate from the Kerr metric but satisfy weak-field tests can lead to large deviations in the predicted black-hole shadows that are inconsistent with even the current EHT measurements. We use numerical calculations of regular, parametric, non-Kerr metrics to identify the common characteristic among these different parametrizations that control the predicted shadow size. We show that the shadow-size measurements place significant constraints on deviation parameters that control the second post-Newtonian and higher orders of each metric and are, therefore, inaccessible to weak-field tests. The new constraints are complementary to those imposed by observations of gravitational waves from stellar-mass sources."
B LEE ROBERTS,"The muon (g-2) spin equations, the magic γ, what’s small and what’s not","We review the spin equations for the muon in the 1.45 T muon (g — 2) storage ring, now relocated to Fermilab. Muons are stored in a uniform 1.45 T magnetic field, and vertical focusing is provided by four sets of electrostatic quadrupoles placed symmetrically around the storage ring. The storage ring is operated at a Lorentz factor centered on the ""magic 𝛄 = 29:3""; the effect of the electric field on the muon spin precession cancels for muons at the magic momentum. We point out the relative sizes of the various terms in the spin equations, and show that for experiments that use the magic 𝛄 and electric quadrupole focusing to store the muon beam, any proposed effect that multiplies either the motional magnetic field β × 𝐸 or the muon pitching motion β • 𝐵 term, will be smaller by three or more orders of magnitude, relative to the spin precession due to the storage ring magnetic field. We use a recently proposed General Relativity correction [1] as an example, to demonstrate the smallness of any such contribution, and point out that their revised preprint [7] still contains a conceptual error, that signi cantly overestimates the magnitude of their proposed correction. We have prepared this document in the hope that future authors will nd it useful, should they wish to propose corrections from some additional term added to the Thomas equation, Eq. 13, below. Our goal is to clarify how the experiment is done, and how the small corrections due to the presence of the radial electric field and the vertical pitching motion of the muons (betatron motion) in the storage ring are taken into account."
B LEE ROBERTS,"Bostonia: v. 44, no. 2, 4",
B LEE ROBERTS,"Canvass: a crowd-sourced, natural-product screening library for exploring biological space",
B LEE ROBERTS,Verification of radiative transfer schemes for the EHT,"The Event Horizon Telescope (EHT) Collaboration has recently produced the first resolved images of the central supermassive black hole in the giant elliptical galaxy M87. Here we report on tests of the consistency and accuracy of the general relativistic radiative transfer codes used within the collaboration to model M87* and Sgr A*. We compare and evaluate (1) deflection angles for equatorial null geodesics in a Kerr spacetime; (2) images calculated from a series of simple, parameterized matter distributions in the Kerr metric using simplified emissivities and absorptivities; (3) for a subset of codes, images calculated from general relativistic magnetohydrodynamics simulations using different realistic synchrotron emissivities and absorptivities; (4) observables for the 2017 configuration of EHT, including visibility amplitudes and closure phases. The error in total flux is of order 1% when the codes are run with production numerical parameters. The dominant source of discrepancies for small camera distances is the location and detailed setup of the software ""camera"" that each code uses to produce synthetic images. We find that when numerical parameters are suitably chosen and the camera is sufficiently far away the images converge and that for given transfer coefficients, numerical uncertainties are unlikely to limit parameter estimation for the current generation of EHT observations. The purpose of this paper is to describe a verification and comparison of EHT radiative transfer codes. It is not to verify EHT models more generally."
B LEE ROBERTS,Monitoring the mmorphology of M87* in 2009–2017 with the Event Horizon Telescope,"The Event Horizon Telescope (EHT) has recently delivered the first resolved images of M87*, the supermassive black hole in the center of the M87 galaxy. These images were produced using 230 GHz observations performed in April 2017. Additional observations are required to investigate the persistence of the primary image feature – a ring with azimuthal brightness asymmetry – and to quantify the image variability on event horizon scales. To address this need, we analyze M87* data collected with prototype EHT arrays in 2009, 2011, 2012, and 2013. While these observations do not contain enough information to produce images, they are sufficient to constrain simple geometric models. We develop a modeling approach based on the framework utilized for the 2017 EHT data analysis and validate our procedures using synthetic data. Applying the same approach to the observational data sets, we find the M87* morphology in 2009–2017 to be consistent with a persistent asymmetric ring of 40 as diameter. The position angle of peak intensity varies in time. In particular, we find a significant difference between the position angle measured in 2013 and 2017. These variations are in broad agreement with predictions of a subset of general relativistic magnetohydrodynamic simulations. We show that quantifying the variability across multiple observational epochs has the potential to constrain physical properties of the source, such as the accretion state or the black hole spin."
B LEE ROBERTS,THEMIS: a parameter estimation framework for the Event Horizon Telescope,"The Event Horizon Telescope (EHT) provides the unprecedented ability to directly resolve the structure and dynamics of black hole emission regions on scales smaller than their horizons. This has the potential to critically probe the mechanisms by which black holes accrete and launch outflows, and the structure of supermassive black hole spacetimes. However, accessing this information is a formidable analysis challenge for two reasons. First, the EHT natively produces a variety of data types that encode information about the image structure in nontrivial ways; these are subject to a variety of systematic effects associated with very long baseline interferometry and are supplemented by a wide variety of auxiliary data on the primary EHT targets from decades of other observations. Second, models of the emission regions and their interaction with the black hole are complex, highly uncertain, and computationally expensive to construct. As a result, the scientific utilization of EHT observations requires a flexible, extensible, and powerful analysis framework. We present such a framework, Themis, which defines a set of interfaces between models, data, and sampling algorithms that facilitates future development. We describe the design and currently existing components of Themis, how Themis has been validated thus far, and present additional analyses made possible by Themis that illustrate its capabilities. Importantly, we demonstrate that Themis is able to reproduce prior EHT analyses, extend these, and do so in a computationally efficient manner that can efficiently exploit modern high-performance computing facilities. Themis has already been used extensively in the scientific analysis and interpretation of the first EHT observations of M87."
B LEE ROBERTS,First Sagittarius A* Event Horizon Telescope results. V. Testing astrophysical models of the galactic center black hole,"In this paper we provide a first physical interpretation for the Event Horizon Telescope's (EHT) 2017 observations of Sgr A*. Our main approach is to compare resolved EHT data at 230 GHz and unresolved non-EHT observations from radio to X-ray wavelengths to predictions from a library of models based on time-dependent general relativistic magnetohydrodynamics simulations, including aligned, tilted, and stellar-wind-fed simulations; radiative transfer is performed assuming both thermal and nonthermal electron distribution functions. We test the models against 11 constraints drawn from EHT 230 GHz data and observations at 86 GHz, 2.2 μm, and in the X-ray. All models fail at least one constraint. Light-curve variability provides a particularly severe constraint, failing nearly all strongly magnetized (magnetically arrested disk (MAD)) models and a large fraction of weakly magnetized models. A number of models fail only the variability constraints. We identify a promising cluster of these models, which are MAD and have inclination i ≤ 30°. They have accretion rate (5.2–9.5) × 10−9 M ⊙ yr−1, bolometric luminosity (6.8–9.2) × 1035 erg s−1, and outflow power (1.3–4.8) × 1038 erg s−1. We also find that all models with i ≥ 70° fail at least two constraints, as do all models with equal ion and electron temperature; exploratory, nonthermal model sets tend to have higher 2.2 μm flux density; and the population of cold electrons is limited by X-ray constraints due to the risk of bremsstrahlung overproduction. Finally, we discuss physical and numerical limitations of the models, highlighting the possible importance of kinetic effects and duration of the simulations."
B LEE ROBERTS,First M87 Event Horizon Telescope results. VII. Polarization of the ring,"In 2017 April, the Event Horizon Telescope (EHT) observed the near-horizon region around the supermassive black hole at the core of the M87 galaxy. These 1.3 mm wavelength observations revealed a compact asymmetric ring-like source morphology. This structure originates from synchrotron emission produced by relativistic plasma located in the immediate vicinity of the black hole. Here we present the corresponding linear-polarimetric EHT images of the center of M87. We find that only a part of the ring is significantly polarized. The resolved fractional linear polarization has a maximum located in the southwest part of the ring, where it rises to the level of ∼15%. The polarization position angles are arranged in a nearly azimuthal pattern. We perform quantitative measurements of relevant polarimetric properties of the compact emission and find evidence for the temporal evolution of the polarized source structure over one week of EHT observations. The details of the polarimetric data reduction and calibration methodology are provided. We carry out the data analysis using multiple independent imaging and modeling techniques, each of which is validated against a suite of synthetic data sets. The gross polarimetric structure and its apparent evolution with time are insensitive to the method used to reconstruct the image. These polarimetric images carry information about the structure of the magnetic fields responsible for the synchrotron emission. Their physical interpretation is discussed in an accompanying publication."
B LEE ROBERTS,First M87 Event Horizon Telescope results. VIII. Magnetic field structure near The Event Horizon,"Event Horizon Telescope (EHT) observations at 230 GHz have now imaged polarized emission around the supermassive black hole in M87 on event-horizon scales. This polarized synchrotron radiation probes the structure of magnetic fields and the plasma properties near the black hole. Here we compare the resolved polarization structure observed by the EHT, along with simultaneous unresolved observations with the Atacama Large Millimeter/submillimeter Array, to expectations from theoretical models. The low fractional linear polarization in the resolved image suggests that the polarization is scrambled on scales smaller than the EHT beam, which we attribute to Faraday rotation internal to the emission region. We estimate the average density n_e ∼ 10^4–7 cm^−3, magnetic field strength B ∼ 1–30 G, and electron temperature T_e ∼ (1–12) × 10^10 K of the radiating plasma in a simple one-zone emission model. We show that the net azimuthal linear polarization pattern may result from organized, poloidal magnetic fields in the emission region. In a quantitative comparison with a large library of simulated polarimetric images from general relativistic magnetohydrodynamic (GRMHD) simulations, we identify a subset of physical models that can explain critical features of the polarimetric EHT observations while producing a relativistic jet of sufficient power. The consistent GRMHD models are all of magnetically arrested accretion disks, where near-horizon magnetic fields are dynamically important. We use the models to infer a mass accretion rate onto the black hole in M87 of (3–20) × 10^−4 M ⊙ yr^−1."
B LEE ROBERTS,Resolving the inner parsec of the blazar J1924–2914 with the event horizon telescope,"The blazar J1924–2914 is a primary Event Horizon Telescope (EHT) calibrator for the Galactic center’s black hole Sagittarius A*. Here we present the first total and linearly polarized intensity images of this source obtained with the unprecedented 20 μas resolution of the EHT. J1924–2914 is a very compact flat-spectrum radio source with strong optical variability and polarization. In April 2017 the source was observed quasi-simultaneously with the EHT (April 5–11), the Global Millimeter VLBI Array (April 3), and the Very Long Baseline Array (April 28), giving a novel view of the source at four observing frequencies, 230, 86, 8.7, and 2.3 GHz. These observations probe jet properties from the subparsec to 100 pc scales. We combine the multifrequency images of J1924–2914 to study the source morphology. We find that the jet exhibits a characteristic bending, with a gradual clockwise rotation of the jet projected position angle of about 90° between 2.3 and 230 GHz. Linearly polarized intensity images of J1924–2914 with the extremely fine resolution of the EHT provide evidence for ordered toroidal magnetic fields in the blazar compact core."
B LEE ROBERTS,"Bostonia: v. 10, no. 1-10",
B LEE ROBERTS,Beam dynamics corrections to the Run-1 measurement of the muon anomalous magnetic moment at Fermilab,"This paper presents the beam dynamics systematic corrections and their uncertainties for the Run-1 dataset of the Fermilab Muon g−2 Experiment. Two corrections to the measured muon precession frequency ωma are associated with well-known effects owing to the use of electrostatic quadrupole (ESQ) vertical focusing in the storage ring. An average vertically oriented motional magnetic field is felt by relativistic muons passing transversely through the radial electric field components created by the ESQ system. The correction depends on the stored momentum distribution and the tunes of the ring, which has relatively weak vertical focusing. Vertical betatron motions imply that the muons do not orbit the ring in a plane exactly orthogonal to the vertical magnetic field direction. A correction is necessary to account for an average pitch angle associated with their trajectories. A third small correction is necessary, because muons that escape the ring during the storage time are slightly biased in initial spin phase compared to the parent distribution. Finally, because two high-voltage resistors in the ESQ network had longer than designed RC time constants, the vertical and horizontal centroids and envelopes of the stored muon beam drifted slightly, but coherently, during each storage ring fill. This led to the discovery of an important phase-acceptance relationship that requires a correction. The sum of the corrections to ωma is 0.50±0.09  ppm; the uncertainty is small compared to the 0.43 ppm statistical precision of ωma."
B LEE ROBERTS,"Bostonia, first series: v. 21, no. 1-3",
B LEE ROBERTS,A universal power-law prescription for variability from synthetic images of black hole accretion flows,"We present a framework for characterizing the spatiotemporal power spectrum of the variability expected from the horizon-scale emission structure around supermassive black holes, and we apply this framework to a library of general relativistic magnetohydrodynamic (GRMHD) simulations and associated general relativistic ray-traced images relevant for Event Horizon Telescope (EHT) observations of Sgr A*. We find that the variability power spectrum is generically a red-noise process in both the temporal and spatial dimensions, with the peak in power occurring on the longest timescales and largest spatial scales. When both the time-averaged source structure and the spatially integrated light-curve variability are removed, the residual power spectrum exhibits a universal broken power-law behavior. On small spatial frequencies, the residual power spectrum rises as the square of the spatial frequency and is proportional to the variance in the centroid of emission. Beyond some peak in variability power, the residual power spectrum falls as that of the time-averaged source structure, which is similar across simulations; this behavior can be naturally explained if the variability arises from a multiplicative random field that has a steeper high-frequency power-law index than that of the time-averaged source structure. We briefly explore the ability of power spectral variability studies to constrain physical parameters relevant for the GRMHD simulations, which can be scaled to provide predictions for black holes in a range of systems in the optically thin regime. We present specific expectations for the behavior of the M87* and Sgr A* accretion flows as observed by the EHT."
B LEE ROBERTS,Millimeter light curves of Sagittarius A* observed during the 2017 Event Horizon Telescope campaign,"The Event Horizon Telescope (EHT) observed the compact radio source, Sagittarius A* (Sgr A*), in the Galactic Center on 2017 April 5–11 in the 1.3 mm wavelength band. At the same time, interferometric array data from the Atacama Large Millimeter/submillimeter Array and the Submillimeter Array were collected, providing Sgr A* light curves simultaneous with the EHT observations. These data sets, complementing the EHT very long baseline interferometry, are characterized by a cadence and signal-to-noise ratio previously unattainable for Sgr A* at millimeter wavelengths, and they allow for the investigation of source variability on timescales as short as a minute. While most of the light curves correspond to a low variability state of Sgr A*, the April 11 observations follow an X-ray flare and exhibit strongly enhanced variability. All of the light curves are consistent with a red-noise process, with a power spectral density (PSD) slope measured to be between −2 and −3 on timescales between 1 minute and several hours. Our results indicate a steepening of the PSD slope for timescales shorter than 0.3 hr. The spectral energy distribution is flat at 220 GHz, and there are no time lags between the 213 and 229 GHz frequency bands, suggesting low optical depth for the event horizon scale source. We characterize Sgr A*’s variability, highlighting the different behavior observed just after the X-ray flare, and use Gaussian process modeling to extract a decorrelation timescale and a PSD slope. We also investigate the systematic calibration uncertainties by analyzing data from independent data reduction pipelines."
B LEE ROBERTS,Selective dynamical imaging of interferometric data,"Recent developments in very long baseline interferometry (VLBI) have made it possible for the Event Horizon Telescope (EHT) to resolve the innermost accretion flows of the largest supermassive black holes on the sky. The sparse nature of the EHT’s (u, v)-coverage presents a challenge when attempting to resolve highly time-variable sources. We demonstrate that the changing (u, v)-coverage of the EHT can contain regions of time over the course of a single observation that facilitate dynamical imaging. These optimal time regions typically have projected baseline distributions that are approximately angularly isotropic and radially homogeneous. We derive a metric of coverage quality based on baseline isotropy and density that is capable of ranking array configurations by their ability to produce accurate dynamical reconstructions. We compare this metric to existing metrics in the literature and investigate their utility by performing dynamical reconstructions on synthetic data from simulated EHT observations of sources with simple orbital variability. We then use these results to make recommendations for imaging the 2017 EHT Sgr A* data set."
B LEE ROBERTS,First Sagittarius A* Event Horizon Telescope results. VI. Testing the black hole metric,"Astrophysical black holes are expected to be described by the Kerr metric. This is the only stationary, vacuum, axisymmetric metric, without electromagnetic charge, that satisfies Einstein’s equations and does not have pathologies outside of the event horizon. We present new constraints on potential deviations from the Kerr prediction based on 2017 EHT observations of Sagittarius A* (Sgr A*). We calibrate the relationship between the geometrically defined black hole shadow and the observed size of the ring-like images using a library that includes both Kerr and non-Kerr simulations. We use the exquisite prior constraints on the mass-to-distance ratio for Sgr A* to show that the observed image size is within ∼10% of the Kerr predictions. We use these bounds to constrain metrics that are parametrically different from Kerr, as well as the charges of several known spacetimes. To consider alternatives to the presence of an event horizon, we explore the possibility that Sgr A* is a compact object with a surface that either absorbs and thermally reemits incident radiation or partially reflects it. Using the observed image size and the broadband spectrum of Sgr A*, we conclude that a thermal surface can be ruled out and a fully reflective one is unlikely. We compare our results to the broader landscape of gravitational tests. Together with the bounds found for stellar-mass black holes and the M87 black hole, our observations provide further support that the external spacetimes of all black holes are described by the Kerr metric, independent of their mass."
B LEE ROBERTS,Polarimetric properties of Event Horizon Telescope targets from ALMA,"We present the results from a full polarization study carried out with the Atacama Large Millimeter/submillimeter Array (ALMA) during the first Very Long Baseline Interferometry (VLBI) campaign, which was conducted in 2017 April in the λ3 mm and λ1.3 mm bands, in concert with the Global mm-VLBI Array (GMVA) and the Event Horizon Telescope (EHT), respectively. We determine the polarization and Faraday properties of all VLBI targets, including Sgr A*, M87, and a dozen radio-loud active galactic nuclei (AGNs), in the two bands at several epochs in a time window of 10 days. We detect high linear polarization fractions (2%–15%) and large rotation measures (RM &gt; 103.3–105.5 rad m−2), confirming the trends of previous AGN studies at millimeter wavelengths. We find that blazars are more strongly polarized than other AGNs in the sample, while exhibiting (on average) order-of-magnitude lower RM values, consistent with the AGN viewing angle unification scheme. For Sgr A* we report a mean RM of (−4.2 ± 0.3) × 105 rad m−2 at 1.3 mm, consistent with measurements over the past decade and, for the first time, an RM of (–2.1 ± 0.1) × 105 rad m−2 at 3 mm, suggesting that about half of the Faraday rotation at 1.3 mm may occur between the 3 mm photosphere and the 1.3 mm source. We also report the first unambiguous measurement of RM toward the M87 nucleus at millimeter wavelengths, which undergoes significant changes in magnitude and sign reversals on a one year timescale, spanning the range from −1.2 to 0.3 × 105 rad m−2 at 3 mm and −4.1 to 1.5 × 105 rad m−2 at 1.3 mm. Given this time variability, we argue that, unlike the case of Sgr A*, the RM in M87 does not provide an accurate estimate of the mass accretion rate onto the black hole. We put forward a two-component model, comprised of a variable compact region and a static extended region, that can simultaneously explain the polarimetric properties observed by both the EHT (on horizon scales) and ALMA (which observes the combined emission from both components). These measurements provide critical constraints for the calibration, analysis, and interpretation of simultaneously obtained VLBI data with the EHT and GMVA."
B LEE ROBERTS,"First Sagittarius A* Event Horizon Telescope results. IV. Variability, morphology, and black hole mass","In this paper we quantify the temporal variability and image morphology of the horizon-scale emission from Sgr A*, as observed by the EHT in 2017 April at a wavelength of 1.3 mm. We find that the Sgr A* data exhibit variability that exceeds what can be explained by the uncertainties in the data or by the effects of interstellar scattering. The magnitude of this variability can be a substantial fraction of the correlated flux density, reaching ∼100% on some baselines. Through an exploration of simple geometric source models, we demonstrate that ring-like morphologies provide better fits to the Sgr A* data than do other morphologies with comparable complexity. We develop two strategies for fitting static geometric ring models to the time-variable Sgr A* data; one strategy fits models to short segments of data over which the source is static and averages these independent fits, while the other fits models to the full data set using a parametric model for the structural variability power spectrum around the average source structure. Both geometric modeling and image-domain feature extraction techniques determine the ring diameter to be 51.8 ± 2.3 μas (68% credible intervals), with the ring thickness constrained to have an FWHM between ∼30% and 50% of the ring diameter. To bring the diameter measurements to a common physical scale, we calibrate them using synthetic data generated from GRMHD simulations. This calibration constrains the angular size of the gravitational radius to be 4.8_-0.7^+1.4 μas, which we combine with an independent distance measurement from maser parallaxes to determine the mass of Sgr A* to be 4.0_-0.6^+10^6 M⊙."
B LEE ROBERTS,"Cosmology intertwined: a review of the particle physics, astrophysics, and cosmology associated with the cosmological tensions and anomalies",
B LEE ROBERTS,"First Sagittarius A* Event Horizon Telescope results. II. EHT and multiwavelength observations, data processing, and calibration","We present Event Horizon Telescope (EHT) 1.3 mm measurements of the radio source located at the position of the supermassive black hole Sagittarius A* (Sgr A*), collected during the 2017 April 5–11 campaign. The observations were carried out with eight facilities at six locations across the globe. Novel calibration methods are employed to account for Sgr A*'s flux variability. The majority of the 1.3 mm emission arises from horizon scales, where intrinsic structural source variability is detected on timescales of minutes to hours. The effects of interstellar scattering on the image and its variability are found to be subdominant to intrinsic source structure. The calibrated visibility amplitudes, particularly the locations of the visibility minima, are broadly consistent with a blurred ring with a diameter of ∼50 μas, as determined in later works in this series. Contemporaneous multiwavelength monitoring of Sgr A* was performed at 22, 43, and 86 GHz and at near-infrared and X-ray wavelengths. Several X-ray flares from Sgr A* are detected by Chandra, one at low significance jointly with Swift on 2017 April 7 and the other at higher significance jointly with NuSTAR on 2017 April 11. The brighter April 11 flare is not observed simultaneously by the EHT but is followed by a significant increase in millimeter flux variability immediately after the X-ray outburst, indicating a likely connection in the emission physics near the event horizon. We compare Sgr A*’s broadband flux during the EHT campaign to its historical spectral energy distribution and find that both the quiescent emission and flare emission are consistent with its long-term behavior."
B LEE ROBERTS,The polarized image of a synchrotron-emitting ring of gas orbiting a black hole,"Synchrotron radiation from hot gas near a black hole results in a polarized image. The image polarization is determined by effects including the orientation of the magnetic field in the emitting region, relativistic motion of the gas, strong gravitational lensing by the black hole, and parallel transport in the curved spacetime. We explore these effects using a simple model of an axisymmetric, equatorial accretion disk around a Schwarzschild black hole. By using an approximate expression for the null geodesics derived by Beloborodov and conservation of the Walker–Penrose constant, we provide analytic estimates for the image polarization. We test this model using currently favored general relativistic magnetohydrodynamic simulations of M87*, using ring parameters given by the simulations. For a subset of these with modest Faraday effects, we show that the ring model broadly reproduces the polarimetric image morphology. Our model also predicts the polarization evolution for compact flaring regions, such as those observed from Sgr A* with GRAVITY. With suitably chosen parameters, our simple model can reproduce the EVPA pattern and relative polarized intensity in Event Horizon Telescope images of M87*. Under the physically motivated assumption that the magnetic field trails the fluid velocity, this comparison is consistent with the clockwise rotation inferred from total intensity images."
B LEE ROBERTS,Measurement of the anomalous precession frequency of the muon in the Fermilab Muon g − 2 Experiment,"The Muon g−2 Experiment at Fermi National Accelerator Laboratory (FNAL) has measured the muon anomalous precession frequency ωma to an uncertainty of 434 parts per billion (ppb), statistical, and 56 ppb, systematic, with data collected in four storage ring configurations during its first physics run in 2018. When combined with a precision measurement of the magnetic field of the experiment’s muon storage ring, the precession frequency measurement determines a muon magnetic anomaly of aμ(FNAL)=116592040(54)×10−11 (0.46 ppm). This article describes the multiple techniques employed in the reconstruction, analysis, and fitting of the data to measure the precession frequency. It also presents the averaging of the results from the 11 separate determinations of ωma, and the systematic uncertainties on the result."
B LEE ROBERTS,First Sagittarius A* Event Horizon Telescope results. I. The shadow of the supermassive black hole in the center of the Milky Way,"We present the first Event Horizon Telescope (EHT) observations of Sagittarius A* (Sgr A*), the Galactic center source associated with a supermassive black hole. These observations were conducted in 2017 using a global interferometric array of eight telescopes operating at a wavelength of λ = 1.3 mm. The EHT data resolve a compact emission region with intrahour variability. A variety of imaging and modeling analyses all support an image that is dominated by a bright, thick ring with a diameter of 51.8 ± 2.3 μas (68% credible interval). The ring has modest azimuthal brightness asymmetry and a comparatively dim interior. Using a large suite of numerical simulations, we demonstrate that the EHT images of Sgr A* are consistent with the expected appearance of a Kerr black hole with mass ∼4 × 106 M ⊙, which is inferred to exist at this location based on previous infrared observations of individual stellar orbits, as well as maser proper-motion studies. Our model comparisons disfavor scenarios where the black hole is viewed at high inclination (i &gt; 50°), as well as nonspinning black holes and those with retrograde accretion disks. Our results provide direct evidence for the presence of a supermassive black hole at the center of the Milky Way, and for the first time we connect the predictions from dynamical measurements of stellar orbits on scales of 103–105 gravitational radii to event-horizon-scale images and variability. Furthermore, a comparison with the EHT results for the supermassive black hole M87* shows consistency with the predictions of general relativity spanning over three orders of magnitude in central mass."
B LEE ROBERTS,Identification and reconstruction of low-energy electrons in the ProtoDUNE-SP detector,
B LEE ROBERTS,The long-baseline neutrino experiment: exploring fundamental symmetries of the universe,"The preponderance of matter over antimatter in the early Universe, the dynamics of the supernova bursts that produced the heavy elements necessary for life and whether protons eventually decay --- these mysteries at the forefront of particle physics and astrophysics are key to understanding the early evolution of our Universe, its current state and its eventual fate. The Long-Baseline Neutrino Experiment (LBNE) represents an extensively developed plan for a world-class experiment dedicated to addressing these questions. LBNE is conceived around three central components: (1) a new, high-intensity neutrino source generated from a megawatt-class proton accelerator at Fermi National Accelerator Laboratory, (2) a near neutrino detector just downstream of the source, and (3) a massive liquid argon time-projection chamber deployed as a far detector deep underground at the Sanford Underground Research Facility. This facility, located at the site of the former Homestake Mine in Lead, South Dakota, is approximately 1,300 km from the neutrino source at Fermilab -- a distance (baseline) that delivers optimal sensitivity to neutrino charge-parity symmetry violation and mass ordering effects. This ambitious yet cost-effective design incorporates scalability and flexibility and can accommodate a variety of upgrades and contributions. With its exceptional combination of experimental configuration, technical capabilities, and potential for transformative discoveries, LBNE promises to be a vital facility for the field of particle physics worldwide, providing physicists from around the globe with opportunities to collaborate in a twenty to thirty year program of exciting science. In this document we provide a comprehensive overview of LBNE's scientific objectives, its place in the landscape of neutrino physics worldwide, the technologies it will incorporate and the capabilities it will possess."
B LEE ROBERTS,Measurement of the positive muon anomalous magnetic moment to 0.46 ppm,"We present the first results of the Fermilab National Accelerator Laboratory (FNAL) Muon g-2 Experiment for the positive muon magnetic anomaly a_{μ}≡(g_{μ}-2)/2. The anomaly is determined from the precision measurements of two angular frequencies. Intensity variation of high-energy positrons from muon decays directly encodes the difference frequency ω_{a} between the spin-precession and cyclotron frequencies for polarized muons in a magnetic storage ring. The storage ring magnetic field is measured using nuclear magnetic resonance probes calibrated in terms of the equivalent proton spin precession frequency ω[over ˜]_{p}^{'} in a spherical water sample at 34.7 °C. The ratio ω_{a}/ω[over ˜]_{p}^{'}, together with known fundamental constants, determines a_{μ}(FNAL)=116 592 040(54)×10^{-11} (0.46 ppm). The result is 3.3 standard deviations greater than the standard model prediction and is in excellent agreement with the previous Brookhaven National Laboratory (BNL) E821 measurement. After combination with previous measurements of both μ^{+} and μ^{-}, the new experimental average of a_{μ}(Exp)=116 592 061(41)×10^{-11} (0.35 ppm) increases the tension between experiment and theory to 4.2 standard deviations."
B LEE ROBERTS,Scintillation light detection in the 6-m drift-length ProtoDUNE Dual Phase liquid argon TPC,"DUNE is a dual-site experiment for long-baseline neutrino oscillation studies, neutrino astrophysics and nucleon decay searches. ProtoDUNE Dual Phase (DP) is a 6  ×  6  ×  6 m 3 liquid argon time-projection-chamber (LArTPC) that recorded cosmic-muon data at the CERN Neutrino Platform in 2019-2020 as a prototype of the DUNE Far Detector. Charged particles propagating through the LArTPC produce ionization and scintillation light. The scintillation light signal in these detectors can provide the trigger for non-beam events. In addition, it adds precise timing capabilities and improves the calorimetry measurements. In ProtoDUNE-DP, scintillation and electroluminescence light produced by cosmic muons in the LArTPC is collected by photomultiplier tubes placed up to 7 m away from the ionizing track. In this paper, the ProtoDUNE-DP photon detection system performance is evaluated with a particular focus on the different wavelength shifters, such as PEN and TPB, and the use of Xe-doped LAr, considering its future use in giant LArTPCs. The scintillation light production and propagation processes are analyzed and a comparison of simulation to data is performed, improving understanding of the liquid argon properties."
B LEE ROBERTS,Broadband multi-wavelength properties of M87 during the 2017 Event Horizon Telescope campaign,"In 2017, the Event Horizon Telescope (EHT) Collaboration succeeded in capturing the first direct image of the center of the M87 galaxy. The asymmetric ring morphology and size are consistent with theoretical expectations for a weakly accreting supermassive black hole of mass ∼6.5 × 109 M ⊙. The EHTC also partnered with several international facilities in space and on the ground, to arrange an extensive, quasi-simultaneous multi-wavelength campaign. This Letter presents the results and analysis of this campaign, as well as the multi-wavelength data as a legacy data repository. We captured M87 in a historically low state, and the core flux dominates over HST-1 at high energies, making it possible to combine core flux constraints with the more spatially precise very long baseline interferometry data. We present the most complete simultaneous multi-wavelength spectrum of the active nucleus to date, and discuss the complexity and caveats of combining data from different spatial scales into one broadband spectrum. We apply two heuristic, isotropic leptonic single-zone models to provide insight into the basic source properties, but conclude that a structured jet is necessary to explain M87’s spectrum. We can exclude that the simultaneous γ-ray emission is produced via inverse Compton emission in the same region producing the EHT mm-band emission, and further conclude that the γ-rays can only be produced in the inner jets (inward of HST-1) if there are strongly particle-dominated regions. Direct synchrotron emission from accelerated protons and secondaries cannot yet be excluded."
B LEE ROBERTS,Event Horizon Telescope imaging of the archetypal blazar 3C 279 at an extreme 20 microarcsecond resolution,"3C 279 is an archetypal blazar with a prominent radio jet that show broadband flux density variability across the entire electromagnetic spectrum. We use an ultra-high angular resolution technique – global Very Long Baseline Interferometry (VLBI) at 1.3 mm (230 GHz) – to resolve the innermost jet of 3C 279 in order to study its fine-scale morphology close to the jet base where highly variable γ-ray emission is thought to originate, according to various models. The source was observed during four days in April 2017 with the Event Horizon Telescope at 230 GHz, including the phased Atacama Large Millimeter/submillimeter Array (ALMA), at an angular resolution of ∼20 μas (at a redshift of z = 0.536 this corresponds to ∼0.13 pc  ∼ 1700 Schwarzschild radii with a black hole mass M<jats:sub>BH</jats:sub> = 8 × 10^8 M_⊙). Imaging and model-fitting techniques were applied to the data to parameterize the fine-scale source structure and its variation. We find a multicomponent inner jet morphology with the northernmost component elongated perpendicular to the direction of the jet, as imaged at longer wavelengths. The elongated nuclear structure is consistent on all four observing days and across different imaging methods and model-fitting techniques, and therefore appears robust. Owing to its compactness and brightness, we associate the northern nuclear structure as the VLBI “core”. This morphology can be interpreted as either a broad resolved jet base or a spatially bent jet. We also find significant day-to-day variations in the closure phases, which appear most pronounced on the triangles with the longest baselines. Our analysis shows that this variation is related to a systematic change of the source structure. Two inner jet components move non-radially at apparent speeds of ∼15 c and ∼20 c (∼1.3 and ∼1.7 μas day^−1, respectively), which more strongly supports the scenario of traveling shocks or instabilities in a bent, possibly rotating jet. The observed apparent speeds are also coincident with the 3C 279 large-scale jet kinematics observed at longer (cm) wavelengths, suggesting no significant jet acceleration between the 1.3 mm core and the outer jet. The intrinsic brightness temperature of the jet components are ≲10^10 K, a magnitude or more lower than typical values seen at ≥7 mm wavelengths. The low brightness temperature and morphological complexity suggest that the core region of 3C 279 becomes optically thin at short (mm) wavelengths."
B LEE ROBERTS,First M87 Event Horizon Telescope results. II. Array and instrumentation,"The Event Horizon Telescope (EHT) is a very long baseline interferometry (VLBI) array that comprises millimeter- and submillimeter-wavelength telescopes separated by distances comparable to the diameter of the Earth. At a nominal operating wavelength of ~1.3 mm, EHT angular resolution (λ/D) is ~25 μas, which is sufficient to resolve nearby supermassive black hole candidates on spatial and temporal scales that correspond to their event horizons. With this capability, the EHT scientific goals are to probe general relativistic effects in the strong-field regime and to study accretion and relativistic jet formation near the black hole boundary. In this Letter we describe the system design of the EHT, detail the technology and instrumentation that enable observations, and provide measures of its performance. Meeting the EHT science objectives has required several key developments that have facilitated the robust extension of the VLBI technique to EHT observing wavelengths and the production of instrumentation that can be deployed on a heterogeneous array of existing telescopes and facilities. To meet sensitivity requirements, high-bandwidth digital systems were developed that process data at rates of 64 gigabit s−1, exceeding those of currently operating cm-wavelength VLBI arrays by more than an order of magnitude. Associated improvements include the development of phasing systems at array facilities, new receiver installation at several sites, and the deployment of hydrogen maser frequency standards to ensure coherent data capture across the array. These efforts led to the coordination and execution of the first Global EHT observations in 2017 April, and to event-horizon-scale imaging of the supermassive black hole candidate in M87."
B LEE ROBERTS,First M87 Event Horizon Telescope results. I. The shadow of the supermassive black hole,"When surrounded by a transparent emission region, black holes are expected to reveal a dark shadow caused by gravitational light bending and photon capture at the event horizon. To image and study this phenomenon, we have assembled the Event Horizon Telescope, a global very long baseline interferometry array observing at a wavelength of 1.3 mm. This allows us to reconstruct event-horizon-scale images of the supermassive black hole candidate in the center of the giant elliptical galaxy M87. We have resolved the central compact radio source as an asymmetric bright emission ring with a diameter of 42 ± 3 μas, which is circular and encompasses a central depression in brightness with a flux ratio gsim10:1. The emission ring is recovered using different calibration and imaging schemes, with its diameter and width remaining stable over four different observations carried out in different days. Overall, the observed image is consistent with expectations for the shadow of a Kerr black hole as predicted by general relativity. The asymmetry in brightness in the ring can be explained in terms of relativistic beaming of the emission from a plasma rotating close to the speed of light around a black hole. We compare our images to an extensive library of ray-traced general-relativistic magnetohydrodynamic simulations of black holes and derive a central mass of M = (6.5 ± 0.7) × 10^9 M ⊙. Our radio-wave observations thus provide powerful evidence for the presence of supermassive black holes in centers of galaxies and as the central engines of active galactic nuclei. They also present a new tool to explore gravity in its most extreme limit and on a mass scale that was so far not accessible."
B LEE ROBERTS,First Sagittarius A* Event Horizon Telescope results. III. Imaging of the Galactic center supermassive black hole,"We present the first event-horizon-scale images and spatiotemporal analysis of Sgr A* taken with the Event Horizon Telescope in 2017 April at a wavelength of 1.3 mm. Imaging of Sgr A* has been conducted through surveys over a wide range of imaging assumptions using the classical CLEAN algorithm, regularized maximum likelihood methods, and a Bayesian posterior sampling method. Different prescriptions have been used to account for scattering effects by the interstellar medium toward the Galactic center. Mitigation of the rapid intraday variability that characterizes Sgr A* has been carried out through the addition of a “variability noise budget” in the observed visibilities, facilitating the reconstruction of static full-track images. Our static reconstructions of Sgr A* can be clustered into four representative morphologies that correspond to ring images with three different azimuthal brightness distributions and a small cluster that contains diverse nonring morphologies. Based on our extensive analysis of the effects of sparse (u, v)-coverage, source variability, and interstellar scattering, as well as studies of simulated visibility data, we conclude that the Event Horizon Telescope Sgr A* data show compelling evidence for an image that is dominated by a bright ring of emission with a ring diameter of ∼50 μas, consistent with the expected “shadow” of a 4 × 106 M⊙ black hole in the Galactic center located at a distance of 8 kpc."
B LEE ROBERTS,Characterizing and mitigating intraday variability: reconstructing source structure in accreting black holes with mm-VLBI,"The extraordinary physical resolution afforded by the Event Horizon Telescope has opened a window onto the astrophysical phenomena unfolding on horizon scales in two known black holes, M87* and Sgr A*. However, with this leap in resolution has come a new set of practical complications. Sgr A* exhibits intraday variability that violates the assumptions underlying Earth aperture synthesis, limiting traditional image reconstruction methods to short timescales and data sets with very sparse (u, v) coverage. We present a new set of tools to detect and mitigate this variability. We develop a data-driven, model-agnostic procedure to detect and characterize the spatial structure of intraday variability. This method is calibrated against a large set of mock data sets, producing an empirical estimator of the spatial power spectrum of the brightness fluctuations. We present a novel Bayesian noise modeling algorithm that simultaneously reconstructs an average image and statistical measure of the fluctuations about it using a parameterized form for the excess variance in the complex visibilities not otherwise explained by the statistical errors. These methods are validated using a variety of simulated data, including general relativistic magnetohydrodynamic simulations appropriate for Sgr A* and M87*. We find that the reconstructed source structure and variability are robust to changes in the underlying image model. We apply these methods to the 2017 EHT observations of M87*, finding evidence for variability across the EHT observing campaign. The variability mitigation strategies presented are widely applicable to very long baseline interferometry observations of variable sources generally, for which they provide a data-informed averaging procedure and natural characterization of inter-epoch image consistency."
CHRISTOPHER ROBERTSON,"SIVagm infection in wild African green monkeys from South Africa: epidemiology, natural history, and evolutionary considerations","Pathogenesis studies of SIV infection have not been performed to date in wild monkeys due to difficulty in collecting and storing samples on site and the lack of analytical reagents covering the extensive SIV diversity. We performed a large scale study of molecular epidemiology and natural history of SIVagm infection in 225 free-ranging AGMs from multiple locations in South Africa. SIV prevalence (established by sequencing pol, env, and gag) varied dramatically between infant/juvenile (7%) and adult animals (68%) (p,0.0001), and between adult females (78%) and males (57%). Phylogenetic analyses revealed an extensive genetic diversity, including frequent recombination events. Some AGMs harbored epidemiological linked viruses. Viruses infecting AGMs in the Free State, which are separated from those on the coastal side by the Drakensberg Mountains, formed a separate cluster in the phylogenetic trees; this observation supports a long standing presence of SIV in AGMs, at least from the time of their speciation to their Plio-Pleistocene migration. Specific primers/probes were synthesized based on the pol sequence data and viral loads (VLs) were quantified. VLs were of 104 –106 RNA copies/ml, in the range of those observed in experimentally-infected monkeys, validating the experimental approaches in natural hosts. VLs were significantly higher (107–108 RNA copies/ml) in 10 AGMs diagnosed as acutely infected based on SIV seronegativity (Fiebig II), which suggests a very active transmission of SIVagm in the wild. Neither cytokine levels (as biomarkers of immune activation) nor sCD14 levels (a biomarker of microbial translocation) were different between SIVinfected and SIV-uninfected monkeys. This complex algorithm combining sequencing and phylogeny, VL quantification, serology, and testing of surrogate markers of microbial translocation and immune activation permits a systematic investigation of the epidemiology, viral diversity and natural history of SIV infection in wild African natural hosts."
CHRISTOPHER ROBERTSON,Predicting attitudinal and behavioral responses to COVID-19 pandemic using machine learning,"At the beginning of 2020, COVID-19 became a global problem. Despite all the efforts to emphasize the relevance of preventive measures, not everyone adhered to them. Thus, learning more about the characteristics determining attitudinal and behavioral responses to the pandemic is crucial to improving future interventions. In this study, we applied machine learning on the multinational data collected by the International Collaboration on the Social and Moral Psychology of COVID-19 (N = 51,404) to test the predictive efficacy of constructs from social, moral, cognitive, and personality psychology, as well as socio-demographic factors, in the attitudinal and behavioral responses to the pandemic. The results point to several valuable insights. Internalized moral identity provided the most consistent predictive contribution-individuals perceiving moral traits as central to their self-concept reported higher adherence to preventive measures. Similar results were found for morality as cooperation, symbolized moral identity, self-control, open-mindedness, and collective narcissism, while the inverse relationship was evident for the endorsement of conspiracy theories. However, we also found a non-neglible variability in the explained variance and predictive contributions with respect to macro-level factors such as the pandemic stage or cultural region. Overall, the results underscore the importance of morality-related and contextual factors in understanding adherence to public health recommendations during the pandemic."
CHRISTOPHER ROBERTSON,The SNO+ experiment,
CHRISTOPHER ROBERTSON,Current status and future prospects of the SNO+ experiment,"SNO+ is a large liquid scintillator-based experiment located 2 km underground at SNOLAB, Sudbury, Canada. It reuses the Sudbury Neutrino Observatory detector, consisting of a 12 m diameter acrylic vessel which will be filled with about 780 tonnes of ultra-pure liquid scintillator. Designed as a multipurpose neutrino experiment, the primary goal of SNO+ is a search for the neutrinoless double-beta decay (0νββ) of ^130Te. In Phase I, the detector will be loaded with 0.3% natural tellurium, corresponding to nearly 800 kg of ^130Te, with an expected effective Majorana neutrino mass sensitivity in the region of 55–133 meV, just above the inverted mass hierarchy. Recently, the possibility of deploying up to ten times more natural tellurium has been investigated, which would enable SNO+ to achieve sensitivity deep into the parameter space for the inverted neutrino mass hierarchy in the future. Additionally, SNO+ aims to measure reactor antineutrino oscillations, low energy solar neutrinos, and geoneutrinos, to be sensitive to supernova neutrinos, and to search for exotic physics. A first phase with the detector filled with water will begin soon, with the scintillator phase expected to start after a few months of water data taking. The 0νββ Phase I is foreseen for 2017."
DAVID A HARRIS,"D-cycloserine augmentation of exposure-based cognitive behavior therapy for anxiety, obsessive-compulsive, and posttraumatic stress disorders: a systematic review and meta-analysis of individual participant data","Importance: Whether and under which conditions D-cycloserine (DCS) augments the effects of exposure-based cognitive behavior therapy for anxiety, obsessive-compulsive, and posttraumatic stress disorders is unclear. Objective: To clarify whether DCS is superior to placebo in augmenting the effects of cognitive behavior therapy for anxiety, obsessive-compulsive, and posttraumatic stress disorders and to evaluate whether antidepressants interact with DCS and the effect of potential moderating variables. Data Sources: PubMed, EMBASE, and PsycINFO were searched from inception to February 10, 2016. Reference lists of previous reviews and meta-analyses and reports of randomized clinical trials were also checked. Study Selection: Studies were eligible for inclusion if they were (1) double-blind randomized clinical trials of DCS as an augmentation strategy for exposure-based cognitive behavior therapy and (2) conducted in humans diagnosed as having specific phobia, social anxiety disorder, panic disorder with or without agoraphobia, obsessive-compulsive disorder, or posttraumatic stress disorder. Data Extraction and Synthesis: Raw data were obtained from the authors and quality controlled. Data were ranked to ensure a consistent metric across studies (score range, 0-100). We used a 3-level multilevel model nesting repeated measures of outcomes within participants, who were nested within studies. Results: Individual participant data were obtained for 21 of 22 eligible trials, representing 1047 of 1073 eligible participants. When controlling for antidepressant use, participants receiving DCS showed greater improvement from pretreatment to posttreatment (mean difference, -3.62; 95% CI, -0.81 to -6.43; P = .01; d = -0.25) but not from pretreatment to midtreatment (mean difference, -1.66; 95% CI, -4.92 to 1.60; P = .32; d = -0.14) or from pretreatment to follow-up (mean difference, -2.98, 95% CI, -5.99 to 0.03; P = .05; d = -0.19). Additional analyses showed that participants assigned to DCS were associated with lower symptom severity than those assigned to placebo at posttreatment and at follow-up. Antidepressants did not moderate the effects of DCS. None of the prespecified patient-level or study-level moderators was associated with outcomes. Conclusions and Relevance: D-cycloserine is associated with a small augmentation effect on exposure-based therapy. This effect is not moderated by the concurrent use of antidepressants. Further research is needed to identify patient and/or therapy characteristics associated with DCS response."
DAVID A HARRIS,"Scope: v. 3, no. 1-6",
DAVID A HARRIS,"BMQ : Boston medical quarterly: v. 11, no. 1-4",
DAVID A HARRIS,"The medical student: v. 15, no. 1-8.",
DAVID A HARRIS,"Genome-wide association studies of serum magnesium, potassium, and sodium concentrations identify six loci influencing serum magnesium levels","Magnesium, potassium, and sodium, cations commonly measured in serum, are involved in many physiological processes including energy metabolism, nerve and muscle function, signal transduction, and fluid and blood pressure regulation. To evaluate the contribution of common genetic variation to normal physiologic variation in serum concentrations of these cations, we conducted genome-wide association studies of serum magnesium, potassium, and sodium concentrations using ∼2.5 million genotyped and imputed common single nucleotide polymorphisms (SNPs) in 15,366 participants of European descent from the international CHARGE Consortium. Study-specific results were combined using fixed-effects inverse-variance weighted meta-analysis. SNPs demonstrating genome-wide significant (p<5×10−8) or suggestive associations (p<4×10−7) were evaluated for replication in an additional 8,463 subjects of European descent. The association of common variants at six genomic regions (in or near MUC1, ATP2B1, DCDC5, TRPM6, SHROOM3, and MDS1) with serum magnesium levels was genome-wide significant when meta-analyzed with the replication dataset. All initially significant SNPs from the CHARGE Consortium showed nominal association with clinically defined hypomagnesemia, two showed association with kidney function, two with bone mineral density, and one of these also associated with fasting glucose levels. Common variants in CNNM2, a magnesium transporter studied only in model systems to date, as well as in CNNM3 and CNNM4, were also associated with magnesium concentrations in this study. We observed no associations with serum sodium or potassium levels exceeding p<4×10−7. Follow-up studies of newly implicated genomic loci may provide additional insights into the regulation and homeostasis of human serum magnesium levels. Author Summary Magnesium, potassium, and sodium are involved in important physiological processes. To better understand how common genetic variation may contribute to inter-individual differences in serum concentrations of these electrolytes, we evaluated single nucleotide polymorphisms (SNPs) across the genome in association with serum magnesium, potassium, and sodium levels in 15,366 participants of European descent from the CHARGE Consortium. We then verified the associations in an additional 8,463 study participants. Six different genomic regions contain variants that are reproducibly associated with serum magnesium levels, and only one of the regions had been previously known to influence serum magnesium concentrations in humans. The identified SNPs also show association with clinically defined hypomagnesemia, and some of them with traits that have been linked to serum magnesium levels, including kidney function, fasting glucose, and bone mineral density. We further provide evidence for a physiological role of magnesium transporters in humans which have previously only been studied in model systems. None of the SNPs evaluated in our study are significantly associated with serum levels of sodium or potassium. Additional studies are needed to investigate the underlying molecular mechanisms in order to help us understand the contribution of these newly identified regions to magnesium homeostasis."
DAVID A HARRIS,NFIA Haploinsufficiency Is Associated with a CNS Malformation Syndrome and Urinary Tract Defects,"Complex central nervous system (CNS) malformations frequently coexist with other developmental abnormalities, but whether the associated defects share a common genetic basis is often unclear. We describe five individuals who share phenotypically related CNS malformations and in some cases urinary tract defects, and also haploinsufficiency for the NFIA transcription factor gene due to chromosomal translocation or deletion. Two individuals have balanced translocations that disrupt NFIA. A third individual and two half-siblings in an unrelated family have interstitial microdeletions that include NFIA. All five individuals exhibit similar CNS malformations consisting of a thin, hypoplastic, or absent corpus callosum, and hydrocephalus or ventriculomegaly. The majority of these individuals also exhibit Chiari type I malformation, tethered spinal cord, and urinary tract defects that include vesicoureteral reflux. Other genes are also broken or deleted in all five individuals, and may contribute to the phenotype. However, the only common genetic defect is NFIA haploinsufficiency. In addition, previous analyses of Nfia−/− knockout mice indicate that Nfia deficiency also results in hydrocephalus and agenesis of the corpus callosum. Further investigation of the mouse Nfia+/− and Nfia−/− phenotypes now reveals that, at reduced penetrance, Nfia is also required in a dosage-sensitive manner for ureteral and renal development. Nfia is expressed in the developing ureter and metanephric mesenchyme, and Nfia+/− and Nfia−/− mice exhibit abnormalities of the ureteropelvic and ureterovesical junctions, as well as bifid and megaureter. Collectively, the mouse Nfia mutant phenotype and the common features among these five human cases indicate that NFIA haploinsufficiency contributes to a novel human CNS malformation syndrome that can also include ureteral and renal defects. Author Summary Central nervous system (CNS) and urinary tract abnormalities are common human malformations, but their variability and genetic complexity make it difficult to identify the responsible genes. Analysis of human chromosomal abnormalities associated with such disorders offers one approach to this problem. In five individuals described herein, a novel human syndrome that involves both CNS and urinary tract defects is associated with chromosomal disruption or deletion of NFIA, encoding a member of the Nuclear Factor I (NFI) family of transcription factors. This syndrome includes brain abnormalities (abnormal corpus callosum, hydrocephalus, ventriculomegaly, and Chiari type I malformation), spinal abnormalities (tethered spinal cord), and urinary tract abnormalities (vesicoureteral reflux). Nfia disruption in mice was already known to cause hydrocephalus and abnormal corpus callosum, and is now shown to exhibit renal defects and disturbed ureteral development. Other genes besides NFIA are also disrupted or deleted and may contribute to the observed phenotype. However, loss of one copy of NFIA is the only genetic defect common to all five patients. The authors thus provide evidence that genetic loss of NFIA contributes to a distinct CNS malformation syndrome with urinary tract defects of variable penetrance."
DAVID A HARRIS,"BMQ : Boston medical quarterly: v. 14, no. 1-4",
DAVID A HARRIS,Prospects for beyond the standard model physics searches at the deep underground neutrino experiment: DUNE collaboration,"The Deep Underground Neutrino Experiment (DUNE) will be a powerful tool for a variety of physics topics. The high-intensity proton beams provide a large neutrino flux, sampled by a near detector system consisting of a combination of capable precision detectors, and by the massive far detector system located deep underground. This configuration sets up DUNE as a machine for discovery, as it enables opportunities not only to perform precision neutrino measurements that may uncover deviations from the present three-flavor mixing paradigm, but also to discover new particles and unveil new interactions and symmetries beyond those predicted in the Standard Model (SM). Of the many potential beyond the Standard Model (BSM) topics DUNE will probe, this paper presents a selection of studies quantifying DUNE's sensitivities to sterile neutrino mixing, heavy neutral leptons, non-standard interactions, CPT symmetry violation, Lorentz invariance violation, neutrino trident production, dark matter from both beam induced and cosmogenic sources, baryon number violation, and other new physics topics that complement those at high-energy colliders and significantly extend the present reach."
DAVID A HARRIS,Identification and reconstruction of low-energy electrons in the ProtoDUNE-SP detector,
DAVID A HARRIS,The long-baseline neutrino experiment: exploring fundamental symmetries of the universe,"The preponderance of matter over antimatter in the early Universe, the dynamics of the supernova bursts that produced the heavy elements necessary for life and whether protons eventually decay --- these mysteries at the forefront of particle physics and astrophysics are key to understanding the early evolution of our Universe, its current state and its eventual fate. The Long-Baseline Neutrino Experiment (LBNE) represents an extensively developed plan for a world-class experiment dedicated to addressing these questions. LBNE is conceived around three central components: (1) a new, high-intensity neutrino source generated from a megawatt-class proton accelerator at Fermi National Accelerator Laboratory, (2) a near neutrino detector just downstream of the source, and (3) a massive liquid argon time-projection chamber deployed as a far detector deep underground at the Sanford Underground Research Facility. This facility, located at the site of the former Homestake Mine in Lead, South Dakota, is approximately 1,300 km from the neutrino source at Fermilab -- a distance (baseline) that delivers optimal sensitivity to neutrino charge-parity symmetry violation and mass ordering effects. This ambitious yet cost-effective design incorporates scalability and flexibility and can accommodate a variety of upgrades and contributions. With its exceptional combination of experimental configuration, technical capabilities, and potential for transformative discoveries, LBNE promises to be a vital facility for the field of particle physics worldwide, providing physicists from around the globe with opportunities to collaborate in a twenty to thirty year program of exciting science. In this document we provide a comprehensive overview of LBNE's scientific objectives, its place in the landscape of neutrino physics worldwide, the technologies it will incorporate and the capabilities it will possess."
DAVID A HARRIS,Scintillation light detection in the 6-m drift-length ProtoDUNE Dual Phase liquid argon TPC,"DUNE is a dual-site experiment for long-baseline neutrino oscillation studies, neutrino astrophysics and nucleon decay searches. ProtoDUNE Dual Phase (DP) is a 6  ×  6  ×  6 m 3 liquid argon time-projection-chamber (LArTPC) that recorded cosmic-muon data at the CERN Neutrino Platform in 2019-2020 as a prototype of the DUNE Far Detector. Charged particles propagating through the LArTPC produce ionization and scintillation light. The scintillation light signal in these detectors can provide the trigger for non-beam events. In addition, it adds precise timing capabilities and improves the calorimetry measurements. In ProtoDUNE-DP, scintillation and electroluminescence light produced by cosmic muons in the LArTPC is collected by photomultiplier tubes placed up to 7 m away from the ionizing track. In this paper, the ProtoDUNE-DP photon detection system performance is evaluated with a particular focus on the different wavelength shifters, such as PEN and TPB, and the use of Xe-doped LAr, considering its future use in giant LArTPCs. The scintillation light production and propagation processes are analyzed and a comparison of simulation to data is performed, improving understanding of the liquid argon properties."
DAVID A HARRIS,"The medical student: v. 12, no. 2-8",
DAVID A HARRIS,A new paradigm for pandemic preparedness,"PURPOSE OF REVIEW: Preparing for pandemics requires a degree of interdisciplinary work that is challenging under the current paradigm. This review summarizes the challenges faced by the field of pandemic science and proposes how to address them. RECENT FINDINGS: The structure of current siloed systems of research organizations hinders effective interdisciplinary pandemic research. Moreover, effective pandemic preparedness requires stakeholders in public policy and health to interact and integrate new findings rapidly, relying on a robust, responsive, and productive research domain. Neither of these requirements are well supported under the current system. SUMMARY: We propose a new paradigm for pandemic preparedness wherein interdisciplinary research and close collaboration with public policy and health practitioners can improve our ability to prevent, detect, and treat pandemics through tighter integration among domains, rapid and accurate integration, and translation of science to public policy, outreach and education, and improved venues and incentives for sustainable and robust interdisciplinary work."
ALLEN L GIFFORD,Pilot randomized trial of the effect of wireless telemonitoring on compliance and treatment efficacy in obstructive sleep apnea,"BACKGROUND: Obstructive sleep apnea (OSA) is a prevalent and serious medical condition characterized by repeated complete or partial obstructions of the upper airway during sleep and is prevalent in 2% to 4% of working middle-aged adults. Nasal continuous positive airway pressure (CPAP) is the gold-standard treatment for OSA. Because compliance rates with CPAP therapy are disappointingly low, effective interventions are needed to improve CPAP compliance among patients diagnosed with OSA. OBJECTIVE: The aim was to determine whether wireless telemonitoring of CPAP compliance and efficacy data, compared to usual clinical care, results in higher CPAP compliance and improved OSA outcomes. METHODS. 45 patients newly diagnosed with OSA were randomized to either telemonitored clinical care or usual clinical care and were followed for their first 2 months of treatment with CPAP therapy. CPAP therapists were not blinded to the participants' treatment group. RESULTS: 20 participants in each group received the designated intervention. Patients randomized to telemonitored clinical care used CPAP an average of 4.1 ± 1.8 hours per night, while the usual clinical care patients averaged 2.8 ± 2.2 hours per night (P = .07). Telemonitored patients used CPAP on 78% ± 22% of the possible nights, while usual care patients used CPAP on 60% ± 32% of the nights (P = .07). No statistically significant differences between the groups were found on measures of CPAP efficacy, including measures of mask leak and the Apnea-Hypopnea Index. Patients in the telemonitored group rated their likelihood to continue using CPAP significantly higher than the patients in the usual care group. Patients in both groups were highly satisfied with the care they received and rated themselves as ""not concerned"" that their CPAP data were being wirelessly monitored. CONCLUSIONS: Telemonitoring of CPAP compliance and efficacy data and rapid use of those data by the clinical sleep team to guide the collaborative (ie, patient and provider) management of CPAP treatment is as effective as usual care in improving compliance rates and outcomes in new CPAP users. This study was designed as a pilot—larger, well-powered studies are necessary to fully evaluate the clinical and economic efficacy of telemonitoring for this population."
ALLEN L GIFFORD,Evaluation of the Sustainability of an Intervention to Increase HIV Testing,"BACKGROUND Sustainability—the routinization and institutionalization of processes that improve the quality of healthcare—is difficult to achieve and not often studied. OBJECTIVE To evaluate the sustainability of increased rates of HIV testing after implementation of a multi-component intervention in two Veterans Health Administration healthcare systems. DESIGN Quasi-experimental implementation study in which the effect of transferring responsibility to conduct the provider education component of the intervention from research to operational staff was assessed. PATIENTS Persons receiving healthcare between 2005 and 2006 (intervention year) and 2006 and 2007 (sustainability year). MEASUREMENTS Monthly HIV testing rate, stratified by frequency of clinic visits RESULTS The monthly adjusted testing rate increased from 2% at baseline to 6% at the end intervention year and then declined reaching 4% at the end of the sustainability year. However, the stratified, visit-specific testing rate for persons newly exposed to the intervention (i.e., having their first through third visits during the study period) increased throughout the intervention and sustainability years. Increases in the proportion of visits by patients who remained untested despite multiple, prior exposures to the intervention accounted for the aggregate attenuation of testing during the sustainability year. Overall, the percentage of patients who received an HIV test in the sustainability year was 11.6%, in the intervention year 11.1%, and in the pre-intervention year 5.0% CONCLUSIONS Provider education combined with informatics and organizational support had a sustainable effect on HIV testing rates. The effect was most pronounced during patients' early contacts with the healthcare system."
LEV B LEVITIN,An analytical model for virtual cut-through routing,"An analytical model of a network with 2-dim torus topology and virtual cut-through routing has been considered in order to find out and analyze certain relationships between network parameters, load and performance. An exact expression for the saturation point (message generation rate at which network saturates) and expressions for the latency as a function of the message generation rate under the assumptions of the “mean field” theory have been obtained. It has been found that the saturation point is inversely proportional to the message length and to the distance between the source and destination. The theoretical results are in a good agreement with small-scale simulation experiments."
LEV B LEVITIN,Computer interconnection networks with virtual cut-through routing,"This paper considers a model of a toroidal computer interconnection network with the virtual cut-through routing. The interrelationships between network parameters, load and performance are analyzed. An exact analytical expression for the saturation point and expressions for the latency as a function of the message generation rate under the mean field theory approximation have been obtained. The theoretical results have been corroborated with the results of simulation experiments for various values of network parameters. The network behavior has been found not depending on the torus linear dimensions provided that they are at least twice as large as the message path length. The saturation point has been found to be inversely proportional to the message length in good agreement with the analytical results. A good agreement with Little’s theorem has been found if the network remains in the steady state during the experiment."
GREGORY A GRILLONE,The color of cancer: margin guidance for oral cancer resection using elastic scattering spectroscopy,"OBJECTIVES/HYPOTHESIS: To evaluate the usefulness of elastic scattering spectroscopy (ESS) as a diagnostic adjunct to frozen section analysis in patients with diagnosed squamous cell carcinoma of the oral cavity. STUDY DESIGN: Prospective analytic study. METHODS: Subjects for this single institution, institutional review board-approved study were recruited from among patients undergoing surgical resection for squamous cell cancer of the oral cavity. A portable ESS device with a contact fiberoptic probe was used to obtain spectral signals. Four to 10 spectral readings were obtained on each subject from various sites including gross tumor and normal-appearing mucosa in the surgical margin. Each reading was correlated with the histopathologic findings of biopsies taken from the exact location of the spectral readings. A diagnostic algorithm based on multidimensional pattern recognition/machine learning was developed. Sensitivity and specificity, error rate, and area under the curve were used as performance metrics for tests involving classification between disease and nondisease classes. RESULTS: Thirty-four (34) subjects were enrolled in the study. One hundred seventy-six spectral data point/biopsy specimen pairs were available for analysis. ESS distinguished normal from abnormal tissue, with a sensitivity ranging from 84% to 100% and specificity ranging from 71% to 89%, depending on how the cutoff between normal and abnormal tissue was defined (i.e., mild, moderate, or severe dysplasia). There were statistically significant differences in malignancy scores between histologically normal tissue and invasive cancer and between noninflamed tissue and inflamed tissue. CONCLUSIONS: This is the first study to evaluate the effectiveness of ESS in guiding mucosal resection margins in oral cavity cancer. ESS provides fast, real-time assessment of tissue without the need for pathology expertise. ESS appears to be effective in distinguishing between normal mucosa and invasive cancer and between ""normal"" tissue (histologically normal and mild dysplasia) and ""abnormal"" tissue (severe dysplasia and carcinoma in situ) that might require further margin resection. Further studies, however, are needed with a larger sample size to validate these findings and to determine the effectiveness of ESS in distinguishing visibly and histologically normal tissue from visibly normal but histologically abnormal tissue. LEVEL OF EVIDENCE: NA Laryngoscope, 127:S1-S9, 2017."
CUTLER CLEVELAND,"More urgency, not less: The COVID-19 pandemic’s lessons for local climate leadership","This report is the first of three that will provide community leaders, inside and outside of local government, with guidance about navigating their climate action priorities through the gauntlet of challenges created by the COVID-19 pandemic and the ensuing economic crisis. Each document, based on a synthesis of expertise and analysis of local climate action options and current research, will address a different topic: 1. Why local climate action needs more urgency, not less; 2. How the pandemic response creates opportunities and risks for local climate action, and how socially vulnerable populations can benefit from purposeful responses to the pandemic and climate change; and, 3. Which local climate actions should be a priority for federal funding."
CUTLER CLEVELAND,Climate change and the prospects for the rapid decarbonization of energy supplies,
CUTLER CLEVELAND,The effect of climate change on electricity expenditures in Massachusetts,"Climate change affects consumer expenditures by altering the consumption of and price for electricity. Previous analyses focus solely on the former, which implicitly assumes that climate-induced changes in consumption do not affect price. But this assumption is untenable because a shift in demand alters quantity and price at equilibrium. Here we present the first empirical estimates for the effect of climate change on electricity prices. Translated through the merit order dispatch of existing capacity for generating electricity, climate-induced changes in daily and monthly patterns of electricity consumption cause non-linear changes in electricity prices. A 2°C increase in global mean temperature increases the prices for and consumption of electricity in Massachusetts USA, such that the average household’s annual expenditures on electricity increase by about 12 percent. Commercial customers incur a 9 percent increase. These increases are caused largely by higher prices for electricity, whose impacts on expenditures are 1.3 and 3.6 fold larger than changes in residential and commercial consumption, respectively. This suggests that previous empirical studies understate the effects of climate change on electricity expenditures and that policy may be needed to ensure that the market generates investments in peaking capacity to satisfy climate-driven changes in summer-time consumption."
CUTLER CLEVELAND,"Climate of crisis: how cities can use climate action to close the equity gap, drive economic recovery, and improve public health","This report is the second of three that provides community leaders, inside and outside of local government, with guidance about navigating their climate-action priorities through the gauntlet of challenges created by the COVID-19 pandemic and the ensuing economic crisis. Each report, based on a synthesis of peer-reviewed research, expert interviews, and the analysis of local climate action, address a different topic: 1. More Urgency, Not Less: The COVID-19 Pandemic’s Lessons for Local Climate Leadership (Published June 2020) 2. Climate of Crisis: How Cities Can Use Climate Action to Close the Equity Gap, Drive Economic Recovery, and Improve Public Health (Published September 2020) 3. A Survey of U.S. City Climate Leaders: The Prospects for Climate Action in the COVID-19 Era (October 2020) This work is supported by The Summit Foundation and The Grantham Foundation for the Protection of the Environment."
CUTLER CLEVELAND,Carbon Free Boston: Buildings Technical Report,"OVERVIEW: Boston is known for its historic iconic buildings, from the Paul Revere House in the North End, to City Hall in Government Center, to the Old South Meeting House in Downtown Crossing, to the African Meeting House on Beacon Hill, to 200 Clarendon (the Hancock Tower) in Back Bay, to Abbotsford in Roxbury. In total, there are over 86,000 buildings that comprise more than 647 million square feet of area. Most of these buildings will still be in use in 2050. Floorspace (square footage) is almost evenly split between residential and non-residential uses, but residential buildings account for nearly 80,000 (93 percent) of the 86,000 buildings. Boston’s buildings are used for a diverse range of activities that include homes, offices, hospitals, factories, laboratories, schools, public service, retail, hotels, restaurants, and convention space. Building type strongly influences energy use; for example, restaurants, hospitals, and laboratories have high energy demands compared to other commercial uses. Boston’s building stock is characterized by thousands of turn-of-the-20th century homes and a postWorld War II building boom that expanded both residential buildings and commercial space. Boston is in the midst of another boom in building construction that is transforming neighborhoods across the city. [TRUNCATED]"
CUTLER CLEVELAND,Carbon Free Boston: Technical Summary,"OVERVIEW: This technical summary is intended to argument the rest of the Carbon Free Boston technical reports that seek to achieve this goal of deep mitigation. This document provides below: a rationale for carbon neutrality, a high level description of Carbon Free Boston’s analytical approach; a summary of crosssector strategies; a high level analysis of air quality impacts; and, a brief analysis of off-road and street light emissions."
CUTLER CLEVELAND,Carbon Free Boston: Transportation Technical Report,"OVERVIEW: Transportation connects Boston’s workers, residents and tourists to their livelihoods, health care, education, recreation, culture, and other aspects of life quality. In cities, transit access is a critical factor determining upward mobility. Yet many urban transportation systems, including Boston’s, underserve some populations along one or more of those dimensions. Boston has the opportunity and means to expand mobility access to all residents, and at the same time reduce GHG emissions from transportation. This requires the transformation of the automobile-centric system that is fueled predominantly by gasoline and diesel fuel. The near elimination of fossil fuels—combined with more transit, walking, and biking—will curtail air pollution and crashes, and dramatically reduce the public health impact of transportation. The City embarks on this transition from a position of strength. Boston is consistently ranked as one of the most walkable and bikeable cities in the nation, and one in three commuters already take public transportation. There are three general strategies to reaching a carbon-neutral transportation system: • Shift trips out of automobiles to transit, biking, and walking;1 • Reduce automobile trips via land use planning that encourages denser development and affordable housing in transit-rich neighborhoods; • Shift most automobiles, trucks, buses, and trains to zero-GHG electricity. Even with Boston’s strong transit foundation, a carbon-neutral transportation system requires a wholesale change in Boston’s transportation culture. Success depends on the intelligent adoption of new technologies, influencing behavior with strong, equitable, and clearly articulated planning and investment, and effective collaboration with state and regional partners."
CUTLER CLEVELAND,Carbon Free Boston: Energy Technical Report,"INTRODUCTION: The adoption of clean energy in Boston’s buildings and transportation systems will produce sweeping changes in the quantity and composition of the city’s demand for fuel and electricity. The demand for electricity is expected to increase by 2050, while the demand for petroleum-based liquid fuels and natural gas within the city is projected to decline significantly. The city must meet future energy demand with clean energy sources in order to meet its carbon mitigation targets. That clean energy must be procured in a way that supports the City’s goals for economic development, social equity, environmental sustainability, and overall quality of life. This chapter examines the strategies to accomplish these goals. Improved energy efficiency, district energy, and in-boundary generation of clean energy (rooftop PV) will reduce net electric power and natural gas demand substantially, but these measures will not eliminate the need for electricity and gas (or its replacement fuel) delivered into Boston. Broadly speaking, to achieve carbon neutrality by 2050, the city must therefore (1) reduce its use of fossil fuels to heat and cool buildings through cost-effective energy efficiency measures and electrification of building thermal services where feasible; and (2) over time, increase the amount of carbon-free electricity delivered to the city. Reducing energy demand though cost effective energy conservation measures will be necessary to reduce the challenges associated with expanding the electricity delivery system and sustainably sourcing renewable fuels."
CUTLER CLEVELAND,Carbon Free Boston: Waste Technical Report,"OVERVIEW: For many people, their most perceptible interaction with their environmental footprint is through the waste that they generate. On a daily basis people have numerous opportunities to decide whether to recycle, compost or throwaway. In many cases, such options may not be present or apparent. Even when such options are available, many lack the knowledge of how to correctly dispose of their waste, leading to contamination of valuable recycling or compost streams. Once collected, people give little thought to how their waste is treated. For Boston’s waste, plastic in the disposal stream acts becomes a fossil fuel used to generate electricity. Organics in the waste stream have the potential to be used to generate valuable renewable energy, while metals and electronics can be recycled to offset virgin materials. However, challenges in global recycling markets are burdening municipalities, which are experiencing higher costs to maintain their recycling. The disposal of solid waste and wastewater both account for a large and visible anthropogenic impact on human health and the environment. In terms of climate change, landfilling of solid waste and wastewater treatment generated emissions of 131.5 Mt CO2e in 2016 or about two percent of total United States GHG emissions that year. The combustion of solid waste contributed an additional 11.0 Mt CO2e, over half of which (5.9 Mt CO2e) is attributable to the combustion of plastic [1]. In Massachusetts, the GHG emissions from landfills (0.4 Mt CO2e), waste combustion (1.2 Mt CO2e), and wastewater (0.5 Mt CO2e) accounted for about 2.7 percent of the state’s gross GHG emissions in 2014 [2]. The City of Boston has begun exploring pathways to Zero Waste, a goal that seeks to systematically redesign our waste management system that can simultaneously lead to a drastic reduction in emissions from waste. The easiest way to achieve zero waste is to not generate it in the first place. This can start at the source with the decision whether or not to consume a product. This is the intent behind banning disposable items such as plastic bags that have more sustainable substitutes. When consumption occurs, products must be designed in such a way that their lifecycle impacts and waste footprint are considered. This includes making durable products, limiting the use of packaging or using organic packaging materials, taking back goods at the end of their life, and designing products to ensure compatibility with recycling systems. When reducing waste is unavoidable, efforts to increase recycling and organics diversion becomes essential for achieving zero waste. [TRUNCATED]"
CUTLER CLEVELAND,Carbon Free Boston: Offsets Technical Report,"OVERVIEW: The U.S. Environmental Protection Agency defines offsets as a specific activity or set of activities intended to reduce GHG emissions, increase the storage of carbon, or enhance GHG removals from the atmosphere [1]. From a city perspective, they provide a mechanism to negate residual GHG emissions— those the city is unable to reduce directly—by supporting projects that avoid or sequester them outside of the city’s reporting boundary. Offsetting GHG emissions is a controversial topic for cities, as the co-benefits of the investment are typically not realized locally. For this reason, offsetting emissions is considered a last resort, a strategy option available when the city has exhausted all others. However, offsets are likely to be a necessity to achieve carbon neutrality by 2050 and promote emissions reductions in the near term. While public and private sector partners pursue the more complex systems transformation, cities can utilize offsets to support short-term and relatively cost-effective reductions in emissions. Offsets can be a relatively simple, certain, and high-impact way to support the transition to a low-carbon world. This report focuses on carbon offset certificates, more often referred to as offsets. Each offset represents a metric ton of verified carbon dioxide (CO2) or equivalent emissions that is reduced, avoided, or permanently removed from the atmosphere (“sequestered”) through an action taken by the creator of the offset. The certificates can be traded and retiring (that is, not re-selling) offsets can be a useful component of an overall voluntary emissions reduction strategy, alongside activities to lower an organization’s direct and indirect emissions. In the Global Protocol for Community-Scale Greenhouse Gas Emissions Inventories (GPC), the GHG accounting system used by the City of Boston, any carbon offset certificates that the City has can be deducted from the City’s total GHG emissions."
CUTLER CLEVELAND,"Enhancing energy, climate, and environmental (data) justice in Boston",
CUTLER CLEVELAND,"Making the great transformation, November 13, 14, and 15, 2003","The conference discussants and participants analyze why transitions happen, and why they matter. Transitions are those wide-ranging changes in human organization and well being that can be convincingly attributed to a concerted set of choices that make the world that was significantly and recognizably different from the world that becomes. Transition scholars argue that that history does not just stumble along a pre-determined path, but that human ingenuity and entrepreneurship have the ability to fundamentally alter its direction. However, our ability to ‘will’ such transitions remains in doubt. These doubts cannot be removed until we have a better understanding of how transitions work."
CUTLER CLEVELAND,"Looking ahead: forecasting and planning for the longer-range future, April 1, 2, and 3, 2005","The conference allowed for many highly esteemed scholars and professionals from a broad range of fields to come together to discuss strategies designed for the 21st century and beyond. The speakers and discussants covered a broad range of subjects including: long-term policy analysis, forecasting for business and investment, the National Intelligence Council Global Trends 2020 report, Europe’s transition from the Marshal plan to the EU, forecasting global transitions, foreign policy planning, and forecasting for defense."
CUTLER CLEVELAND,A survey of North American city climate leaders: the prospects for climate action in the COVID-19 era,"In the final report of our three-part series, the Boston University Institute for Sustainable Energy presents the results of a survey of 25 U.S. and Canadian city climate leaders, conducted in July and August 2020, to assess the current priority of city climate action in the context of the pandemic. We surveyed twenty-five U.S. and Canadian city climate leaders in July and August 2020 to assess the current priority of city climate action in the context of the COVID-19 pandemic. Our key findings are summarized below. 1. Climate Action Still a Priority 2. Equity Takes Center Stage but Lacks a Clear Roadmap 3. Immediate Climate Action Can Advance Health, Equity, and Economic Recovery 4. Supporting Public Health and Mobility Increase in Importance 5. Decarbonization Programs and Ecosystem Support are Sustained 6. Community Engagement and Communication Continue to Challenge."
CUTLER CLEVELAND,How Boston’s next mayor can accelerate the city’s equitable clean energy transition,"As the Boston mayoral race heads to the primary on September 14, how can the next mayor equitably and rapidly accelerate the city’s clean energy transition to mitigate the negative impacts of climate change and fossil fuel emissions for the city and region? A new whitepaper published today by the Boston Area Research Initiative (BARI) in partnership with the local academic and nonprofit communities, including ISE experts, recommends six near-term opportunities for early leadership on clean energy. It’s part of a four-paper series created by the BARI to help advise the next mayor about the challenges confronting Boston in transportation, public safety, clean energy, and housing."
CUTLER CLEVELAND,Final report: Proposed modeling system recommendations for Boston 80/50 decarbonization,
CUTLER CLEVELAND,Carbon Free Boston: Social equity report 2019,"OVERVIEW: In January 2019, the Boston Green Ribbon Commission released its Carbon Free Boston: Summary Report, identifying potential options for the City of Boston to meet its goal of becoming carbon neutral by 2050. The report found that reaching carbon neutrality by 2050 requires three mutually-reinforcing strategies in key sectors: 1) deepen energy efficiency while reducing energy demand, 2) electrify activity to the fullest practical extent, and 3) use fuels and electricity that are 100 percent free of greenhouse gases (GHGs). The Summary Report detailed the ways in which these technical strategies will transform Boston’s physical infrastructure, including its buildings, energy supply, transportation, and waste management systems. The Summary Report also highlighted that it is how these strategies are designed and implemented that matter most in ensuring an effective and equitable transition to carbon neutrality. Equity concerns exist for every option the City has to reduce GHG emissions. The services provided by each sector are not experienced equally across Boston’s communities. Low-income families and families of color are more likely to live in residences that are in poor physical condition, leading to high utility bills, unsafe and unhealthy indoor environments, and high GHG emissions.1 Those same families face greater exposure to harmful outdoor air pollution compared to others. The access and reliability of public transportation is disproportionately worse in neighborhoods with large populations of people of color, and large swaths of vulnerable neighborhoods, from East Boston to Mattapan, do not have ready access to the city’s bike network. Income inequality is a growing national issue and is particularly acute in Boston, which consistently ranks among the highest US cities in regards to income disparities. With the release of Imagine Boston 2030, Mayor Walsh committed to make Boston more equitable, affordable, connected, and resilient. The Summary Report outlined the broad strokes of how action to reach carbon neutrality intersects with equity. A just transition to carbon neutrality improves environmental quality for all Bostonians, prioritizes socially vulnerable populations, seeks to redress current and past injustice, and creates economic and social opportunities for all. This Carbon Free Boston: Social Equity Report provides a deeper equity context for Carbon Free Boston as a whole, and for each strategy area, by demonstrating how inequitable and unjust the playing field is for socially vulnerable Bostonians and why equity must be integrated into policy design and implementation. This report summarizes the current landscape of climate action work for each strategy area and evaluates how it currently impacts inequity. Finally, this report provides guidance to the City and partners on how to do better; it lays out the attributes of an equitable approach to carbon neutrality, framed around three guiding principles: 1) plan carefully to avoid unintended consequences, 2) be intentional in design through a clear equity lens, and 3) practice inclusivity from start to finish."
CUTLER CLEVELAND,Direct air carbon capture and storage market scan,
CUTLER CLEVELAND,"Concept paper on a curriculum initiative for energy, climate change, and sustainability at Boston University","[Summary] Boston University has made important contributions to the interconnected challenges of energy, climate change, and sustainability (ECS) through its research, teaching, and campus operations. This work reveals new opportunities to expand the scope of teaching and research and place the University at the forefront of ECS in higher education. This paper describes the framework for a University-wide curriculum initiative that moves us in that direction and that complements the University’s strategic plan. The central curricular objectives are to provide every undergraduate the opportunity be touched in some way in their educational program by exposure to some aspect of the ECS challenge, and to increase opportunities for every graduate student to achieve a focused competence in ECS. The initiative has six cornerstone initiatives. The first is the Campus as a Living Lab (CALL) program in which students, faculty and staff work together and use our urban campus and its community to study and implement ECS solutions. The second is a university-wide minor degree that helps students develop an integrated perspective of the economic, environmental, and social dimensions of sustainability. The third is one or more graduate certificate programs open to all graduate students. The fourth is an annual summer faculty workshop that develops new ECS curriculum and CALL opportunities. The fifth is web-based resource that underpins the construction of a vibrant knowledge network for the BU community and beyond. Finally, an enhanced sustainability alumni network will augment professional opportunities and generate other benefits. The learning outcomes of this initiative will be realized through the collaborative work of faculty, students, and staff from all 17 colleges and schools. The initiative will leverage existing BU student resources such as the Thurman Center, Build Lab, and Innovate@BU. Benefits of this initiative, beyond the curriculum, include acceleration towards the goals of our Climate Action Plan; improving the “sustainability brand” of BU; enhancing the ability to attract students and new faculty; strengthening our alumni and campus communities; deepening our ties with the city of Boston; and the potential to spin off new social and technological innovations."
LEWIS E KAZIS,Simvastatin is associated with a reduced incidence of dementia and Parkinson's disease,"BACKGROUND: Statins are a class of medications that reduce cholesterol by inhibiting 3-hydroxy-3-methylglutaryl-coenzyme A reductase. Whether statins can benefit patients with dementia remains unclear because of conflicting results. We hypothesized that some of the confusion in the literature might arise from differences in efficacy of different statins. We used a large database to compare the action of several different statins to investigate whether some statins might be differentially associated with a reduction in the incidence of dementia and Parkinson's disease. METHODS: We analyzed data from the decision support system of the US Veterans Affairs database, which contains diagnostic, medication and demographic information on 4.5 million subjects. The association of lovastatin, simvastatin and atorvastatin with dementia was examined with Cox proportional hazard models for subjects taking statins compared with subjects taking cardiovascular medications other than statins, after adjusting for covariates associated with dementia or Parkinson's disease. RESULTS: We observed that simvastatin is associated with a significant reduction in the incidence of dementia in subjects ≥65 years, using any of three models. The first model incorporated adjustment for age, the second model included adjusted for three known risk factors for dementia, hypertension, cardiovascular disease or diabetes, and the third model incorporated adjustment for the Charlson index, which is an index that provides a broad assessment of chronic disease. Data were obtained for over 700000 subjects taking simvastatin and over 50000 subjects taking atorvastatin who were aged >64 years. Using model 3, the hazard ratio for incident dementia for simvastatin and atorvastatin are 0.46 (CI 0.44–0.48, p < 0.0001) and 0.91 (CI 0.80–1.02, p = 0.11), respectively. Lovastatin was not associated with a reduction in the incidence of dementia. Simvastatin also exhibited a reduced hazard ratio for newly acquired Parkinson's disease (HR 0.51, CI 0.4–0.55, p < 0.0001). CONCLUSION: Simvastatin is associated with a strong reduction in the incidence of dementia and Parkinson's disease, whereas atorvastatin is associated with a modest reduction in incident dementia and Parkinson's disease, which shows only a trend towards significance."
LEWIS E KAZIS,Use of angiotensin receptor blockers and risk of dementia in a predominantly male population: prospective cohort analysis,"Objective: To investigate whether angiotensin receptor blockers protect against Alzheimer's disease and dementia or reduce the progression of both diseases. Design Prospective cohort analysis. Setting Administrative database of the US Veteran Affairs, 2002-6. Population 819491 predominantly male participants (98%) aged 65 or more with cardiovascular disease. Main outcome measures Time to incident Alzheimer's disease or dementia in three cohorts (angiotensin receptor blockers, lisinopril, and other cardiovascular drugs, the ""cardiovascular comparator"") over a four year period (fiscal years 2003-6) using Cox proportional hazard models with adjustments for age, diabetes, stroke, and cardiovascular disease. Disease progression was the time to admission to a nursing home or death among participants with pre-existing Alzheimer's disease or dementia. Results Hazard rates for incident dementia in the angiotensin receptor blocker group were 0.76 (95% confidence interval 0.69 to 0.84) compared with the cardiovascular comparator and 0.81 (0.73 to 0.90) compared with the lisinopril group. Compared with the cardiovascular comparator, angiotensin receptor blockers in patients with pre-existing Alzheimer's disease were associated with a significantly lower risk of admission to a nursing home (0.51, 0.36 to 0.72) and death (0.83, 0.71 to 0.97). Angiotensin receptor blockers exhibited a dose-response as well as additive effects in combination with angiotensin converting enzyme inhibitors. This combination compared with angiotensin converting enzyme inhibitors alone was associated with a reduced risk of incident dementia (0.54, 0.51 to 0.57) and admission to a nursing home (0.33, 0.22 to 0.49). Minor differences were shown in mean systolic and diastolic blood pressures between the groups. Similar results were observed for Alzheimer's disease. Conclusions Angiotensin receptor blockers are associated with a significant reduction in the incidence and progression of Alzheimer's disease and dementia compared with angiotensin converting enzyme inhibitors or other cardiovascular drugs in a predominantly male population."
LEWIS E KAZIS,The Challenges of Multimorbidity from the Patient Perspective,"BACKGROUND Although multiple co-occurring chronic illnesses within the same individual are increasingly common, few studies have examined the challenges of multimorbidity from the patient perspective. OBJECTIVE The aim of this study is to examine the self-management learning needs and willingness to see non-physician providers of patients with multimorbidity compared to patients with single chronic illnesses. DESIGN. This research is designed as a cross-sectional survey. PARTICIPANTS Based upon ICD-9 codes, patients from a single VHA healthcare system were stratified into multimorbidity clusters or groups with a single chronic illness from the corresponding cluster. Nonproportional sampling was used to randomly select 720 patients. MEASUREMENTS Demographic characteristics, functional status, number of contacts with healthcare providers, components of primary care, self-management learning needs, and willingness to see nonphysician providers. RESULTS Four hundred twenty-two patients returned surveys. A higher percentage of multimorbidity patients compared to single morbidity patients were ""definitely"" willing to learn all 22 self-management skills, of these only 2 were not significant. Compared to patients with single morbidity, a significantly higher percentage of patients with multimorbidity also reported that they were ""definitely"" willing to see 6 of 11 non-physician healthcare providers. CONCLUSIONS Self-management learning needs of multimorbidity patients are extensive, and their preferences are consistent with team-based primary care. Alternative methods of providing support and chronic illness care may be needed to meet the needs of these complex patients."
LEWIS E KAZIS,The Quality of Care for Adults with Epilepsy: An Initial Glimpse Using the QUIET Measure,"BACKGROUND: We examined the quality of adult epilepsy care using the Quality Indicators in Epilepsy Treatment (QUIET) measure, and variations in quality based on the source of epilepsy care. METHODS: We identified 311 individuals with epilepsy diagnosis between 2004 and 2007 in a tertiary medical center in New England. We abstracted medical charts to identify the extent to which participants received quality indicator (QI) concordant care for individual QI's and the proportion of recommended care processes completed for different aspects of epilepsy care over a two year period. Finally, we compared the proportion of recommended care processes completed for those receiving care only in primary care, neurology clinics, or care shared between primary care and neurology providers. RESULTS: The mean proportion of concordant care by indicator was 55.6 (standard deviation = 31.5). Of the 1985 possible care processes, 877 (44.2%) were performed; care specific to women had the lowest concordance (37% vs. 42% [first seizure evaluation], 44% [initial epilepsy treatment], 45% [chronic care]). Individuals receiving shared care had more aspects of QI concordant care performed than did those receiving neurology care for initial treatment (53% vs. 43%; X2 = 9.0; p = 0.01) and chronic epilepsy care (55% vs. 42%; X2 = 30.2; p <0.001). CONCLUSIONS: Similar to most other chronic diseases, less than half of recommended care processes were performed. Further investigation is needed to understand whether a shared-care model enhances quality of care, and if so, how it leads to improvements in quality."
LEWIS E KAZIS,An alternative approach to measuring treatment persistence with antipsychotic agents among patients with schizophrenia in the Veterans Health Administration,"Prior studies have demonstrated the importance of treatment persistence with anti-psychotic agents in sustaining control of schizophrenic symptoms. However, the conventional approach in measuring treatment persistence tended to use only the first prescription episode even though some patients received multiple prescriptions (or multiple treatment episodes) of the same medication within one year following the initiation of the index drug. In this study, we used data from the Veterans Health Administration in the United States to assess the extent to which patients received multiple prescriptions. The study found that about a quarter of the patients had two or more treatment episodes and that levels of treatment persistence tended to vary across treatment episodes. Based on these results, we offered an alternative approach in which we calculated treatment persistence with typical and atypical antipsychotic agents separately for patients with one, two, or three treatment episodes. Considering that patients with different number of treatment episodes might differ in disease profiles, this treatment episode-specific approach offered a fair comparison of the levels of treatment persistence across patients with different number of treatment episodes. Future research needs to extend the analyses beyond two antipsychotic classes to individual antipsychotic agents. A more comprehensive assessment using appropriate analytic methods should help physicians make prescription choices that will ultimately improve the care of patients with schizophrenia."
LEWIS E KAZIS,Measurement of treatment adherence with antipsychotic agents in patients with schizophrenia,"The importance of medication adherence in sustaining control of schizophrenic symptoms has generated a great deal of interest in comparing levels of treatment adherence with different antipsychotic agents. However, the bulk of the research has yielded results that are often inconsistent. In this prospective, observational study, we assessed the measurement properties of 3 commonly used, pharmacy-based measures of treatment adherence with antipsychotic agents in schizophrenia using data from the Veterans Health Administration during 2000 to 2005. Patients were selected if they were on antipsychotics and diagnosed with schizophrenia (N = 18,425). A gap of ≥30 days (with no filled index medication) was used to define discontinuation of treatment as well as medication ""episodes,"" or the number of times a patient returned to the same index agent after discontinuation of treatment within a 1-year period. The study found that the 3 existing measures differed in their approaches in measuring treatment adherence, suggesting that studies using these different measures would generate different levels of treatment adherence across antipsychotic agents. Considering the measurement problems associated with each existing approach, we offered a new, medication episode-specific approach, which would provide a fairer comparison of the levels of treatment adherence across different antipsychotic agents."
LEWIS E KAZIS,A web-based nutrition program reduces health care costs in employees with cardiac risk factors: before and after cost analysis,"BACKGROUND: Rising health insurance premiums represent a rapidly increasing burden on employer-sponsors of health insurance and their employees. Some employers have become proactive in managing health care costs by providing tools to encourage employees to directly manage their health and prevent disease. One example of such a tool is DASH for Health, an Internet-based nutrition and exercise behavior modification program. This program was offered as a free, opt-in benefit to US-based employees of the EMC Corporation. OBJECTIVE: The aim was to determine whether an employer-sponsored, Internet-based diet and exercise program has an effect on health care costs. METHODS. There were 15,237 total employees and spouses who were included in our analyses, of whom 1967 enrolled in the DASH for Health program (DASH participants). Using a retrospective, quasi-experimental design, study year health care costs among DASH participants and non-participants were compared, controlling for baseline year costs, risk, and demographic variables. The relationship between how often a subject visited the DASH website and health care costs also was examined. These relationships were examined among all study subjects and among a subgroup of 735 subjects with cardiovascular conditions (diabetes, hypertension, hyperlipidemia). Multiple linear regression analysis examined the relationship of program use to health care costs, comparing study year costs among DASH participants and non-participants and then examining the effects of increased website use on health care costs. Analyses were repeated among the cardiovascular condition subgroups. RESULTS: Overall, program use was not associated with changes in health care costs. However, among the cardiovascular risk study subjects, health care costs were US$827 lower, on average, during the study year (P = .05; t 729 = 1.95). Among 1028 program users, increased website use was significantly associated with lower health care costs among those who visited the website at least nine times during the study year (US$14 decrease per visit; P = .04; t 1022 = 2.05), with annual savings highest among 80 program users with targeted conditions (US$55 decrease per visit; P< .001; t 74 = 2.71). CONCLUSIONS: An employer-sponsored, Internet-based diet and exercise program shows promise as a low-cost benefit that contributes to lower health care costs among persons at higher risk for above-average health care costs and utilization."
LYNN A ROSENBERG,Quantitative Ultrasound in Relation to Risk Factors for Low Bone Mineral Density in South African Pre-Menopausal Women,"SUMMARY. The study describes the association between risk factors and quantitative ultrasound bone measures in black and mixed-race pre-menopausal South African women. Despite some differences between the two study groups, the findings generally lend support to the use of ultrasound for epidemiological studies of bone mass in resource-limited settings. INTRODUCTION. Quantitative ultrasound at the calcaneus is a convenient and inexpensive method of estimating bone strength well suited to community-based research in countries with limited resources. This study determines, in a large sample of pre-menopausal South African women, whether characteristics associated with quantitative ultrasound measures are similar to those shown to be associated with bone mineral density as measured by dual X-ray absorptiometry. METHODS. This cross-sectional study included 3,493 women (1,598 black and 1,895 mixed race), aged 18–44 living in Cape Town. Study nurses administered structured interviews on reproductive history, lifestyle factors, and measured height and weight. Calcaneus quantitative ultrasound measurements were obtained using the Sahara device. Adjusted means of ultrasound measures according to categories of risk factors were obtained using multivariable regression analysis. RESULTS. Associations between quantitative ultrasound measures and age, body mass index, age at menarche, parity, and primary school physical activity were similar to those known for bone mineral density as measured by dual X-ray absorptiometry. There were no clear associations between quantitative ultrasound measures and educational level, alcohol use, cigarette smoking, and current calcium intake. CONCLUSION. The data give qualified support to the use of quantitative ultrasound as an epidemiological tool in large studies of bone strength in pre-menopausal women."
CHRISTOPHER MOORE,Focus: Summer 2018,
CHRISTOPHER MOORE,Nonlinear properties of medial entorhinal cortex neurons reveal frequency selectivity during multi-sinusoidal stimulation,"The neurons in layer II of the medial entorhinal cortex are part of the grid cell network involved in the representation of space. Many of these neurons are likely to be stellate cells with specific oscillatory and firing properties important for their function. A fundamental understanding of the nonlinear basis of these oscillatory properties is critical for the development of theories of grid cell firing. In order to evaluate the behavior of stellate neurons, measurements of their quadratic responses were used to estimate a second order Volterra kernel. This paper uses an operator theory, termed quadratic sinusoidal analysis (QSA), which quantitatively determines that the quadratic response accounts for a major part of the nonlinearity observed at membrane potential levels characteristic of normal synaptic events. Practically, neurons were probed with multi-sinusoidal stimulations to determine a Hermitian operator that captures the quadratic function in the frequency domain. We have shown that the frequency content of the stimulation plays an important role in the characteristics of the nonlinear response, which can distort the linear response as well. Stimulations with enhanced low frequency amplitudes evoked a different nonlinear response than broadband profiles. The nonlinear analysis was also applied to spike frequencies and it was shown that the nonlinear response of subthreshold membrane potential at resonance frequencies near the threshold is similar to the nonlinear response of spike trains."
CHRISTOPHER MOORE,Buildout and integration of an automated high-throughput CLIA laboratory for SARS-CoV-2 testing on a large urban campus,"In 2019, the first cases of SARS-CoV-2 were detected in Wuhan, China, and by early 2020 the first cases were identified in the United States. SARS-CoV-2 infections increased in the US causing many states to implement stay-at-home orders and additional safety precautions to mitigate potential outbreaks. As policies changed throughout the pandemic and restrictions lifted, there was an increase in demand for COVID-19 testing which was costly, difficult to obtain, or had long turn-around times. Some academic institutions, including Boston University (BU), created an on-campus COVID-19 screening protocol as part of a plan for the safe return of students, faculty, and staff to campus with the option for in-person classes. At BU, we put together an automated high-throughput clinical testing laboratory with the capacity to run 45,000 individual tests weekly by Fall of 2020, with a purpose-built clinical testing laboratory, a multiplexed reverse transcription PCR (RT-qPCR) test, robotic instrumentation, and trained staff. There were many challenges including supply chain issues for personal protective equipment and testing materials in addition to equipment that were in high demand. The BU Clinical Testing Laboratory (CTL) was operational at the start of Fall 2020 and performed over 1 million SARS-CoV-2 PCR tests during the 2020-2021 academic year."
CHRISTOPHER MOORE,Prospects for beyond the standard model physics searches at the deep underground neutrino experiment: DUNE collaboration,"The Deep Underground Neutrino Experiment (DUNE) will be a powerful tool for a variety of physics topics. The high-intensity proton beams provide a large neutrino flux, sampled by a near detector system consisting of a combination of capable precision detectors, and by the massive far detector system located deep underground. This configuration sets up DUNE as a machine for discovery, as it enables opportunities not only to perform precision neutrino measurements that may uncover deviations from the present three-flavor mixing paradigm, but also to discover new particles and unveil new interactions and symmetries beyond those predicted in the Standard Model (SM). Of the many potential beyond the Standard Model (BSM) topics DUNE will probe, this paper presents a selection of studies quantifying DUNE's sensitivities to sterile neutrino mixing, heavy neutral leptons, non-standard interactions, CPT symmetry violation, Lorentz invariance violation, neutrino trident production, dark matter from both beam induced and cosmogenic sources, baryon number violation, and other new physics topics that complement those at high-energy colliders and significantly extend the present reach."
CHRISTOPHER MOORE,Identification and reconstruction of low-energy electrons in the ProtoDUNE-SP detector,
CHRISTOPHER MOORE,The long-baseline neutrino experiment: exploring fundamental symmetries of the universe,"The preponderance of matter over antimatter in the early Universe, the dynamics of the supernova bursts that produced the heavy elements necessary for life and whether protons eventually decay --- these mysteries at the forefront of particle physics and astrophysics are key to understanding the early evolution of our Universe, its current state and its eventual fate. The Long-Baseline Neutrino Experiment (LBNE) represents an extensively developed plan for a world-class experiment dedicated to addressing these questions. LBNE is conceived around three central components: (1) a new, high-intensity neutrino source generated from a megawatt-class proton accelerator at Fermi National Accelerator Laboratory, (2) a near neutrino detector just downstream of the source, and (3) a massive liquid argon time-projection chamber deployed as a far detector deep underground at the Sanford Underground Research Facility. This facility, located at the site of the former Homestake Mine in Lead, South Dakota, is approximately 1,300 km from the neutrino source at Fermilab -- a distance (baseline) that delivers optimal sensitivity to neutrino charge-parity symmetry violation and mass ordering effects. This ambitious yet cost-effective design incorporates scalability and flexibility and can accommodate a variety of upgrades and contributions. With its exceptional combination of experimental configuration, technical capabilities, and potential for transformative discoveries, LBNE promises to be a vital facility for the field of particle physics worldwide, providing physicists from around the globe with opportunities to collaborate in a twenty to thirty year program of exciting science. In this document we provide a comprehensive overview of LBNE's scientific objectives, its place in the landscape of neutrino physics worldwide, the technologies it will incorporate and the capabilities it will possess."
CHRISTOPHER MOORE,Scintillation light detection in the 6-m drift-length ProtoDUNE Dual Phase liquid argon TPC,"DUNE is a dual-site experiment for long-baseline neutrino oscillation studies, neutrino astrophysics and nucleon decay searches. ProtoDUNE Dual Phase (DP) is a 6  ×  6  ×  6 m 3 liquid argon time-projection-chamber (LArTPC) that recorded cosmic-muon data at the CERN Neutrino Platform in 2019-2020 as a prototype of the DUNE Far Detector. Charged particles propagating through the LArTPC produce ionization and scintillation light. The scintillation light signal in these detectors can provide the trigger for non-beam events. In addition, it adds precise timing capabilities and improves the calorimetry measurements. In ProtoDUNE-DP, scintillation and electroluminescence light produced by cosmic muons in the LArTPC is collected by photomultiplier tubes placed up to 7 m away from the ionizing track. In this paper, the ProtoDUNE-DP photon detection system performance is evaluated with a particular focus on the different wavelength shifters, such as PEN and TPB, and the use of Xe-doped LAr, considering its future use in giant LArTPCs. The scintillation light production and propagation processes are analyzed and a comparison of simulation to data is performed, improving understanding of the liquid argon properties."
CHRISTOPHER MOORE,CD4+ and CD8+ T cells and antibodies are associated with protection against Delta vaccine breakthrough infection: a nested case-control study within the PITCH study,"Defining correlates of protection against severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) vaccine breakthrough infection informs vaccine policy for booster doses and future vaccine designs. Existing studies demonstrate humoral correlates of protection, but the role of T cells in protection is still unclear. In this study, we explore antibody and T cell immune responses associated with protection against Delta variant vaccine breakthrough infection in a well-characterized cohort of UK Healthcare Workers (HCWs). We demonstrate evidence to support a role for CD4+ and CD8+ T cells as well as antibodies against Delta vaccine breakthrough infection. In addition, our results suggest a potential role for cross-reactive T cells in vaccine breakthrough."
CHRISTOPHER MOORE,Omicron infection following vaccination enhances a broad spectrum of immune responses dependent on infection history,"Pronounced immune escape by the SARS-CoV-2 Omicron variant has resulted in many individuals possessing hybrid immunity, generated through a combination of vaccination and infection. Concerns have been raised that omicron breakthrough infections in triple-vaccinated individuals result in poor induction of omicron-specific immunity, and that prior SARS-CoV-2 infection is associated with immune dampening. Taking a broad and comprehensive approach, we characterize mucosal and blood immunity to spike and non-spike antigens following BA.1/BA.2 infections in triple mRNA-vaccinated individuals, with and without prior SARS-CoV-2 infection. We find that most individuals increase BA.1/BA.2/BA.5-specific neutralizing antibodies following infection, but confirm that the magnitude of increase and post-omicron titres are higher in the infection-naive. In contrast, significant increases in nasal responses, including neutralizing activity against BA.5 spike, are seen regardless of infection history. Spike-specific T cells increase only in infection-naive vaccinees; however, post-omicron T cell responses are significantly higher in the previously-infected, who display a maximally induced response with a highly cytotoxic CD8+ phenotype following their 3rd mRNA vaccine dose. Responses to non-spike antigens increase significantly regardless of prior infection status. These findings suggest that hybrid immunity induced by omicron breakthrough infections is characterized by significant immune enhancement that can help protect against future omicron variants."
LESLIE S KAUFMAN,"Leveraging big data and analytics to improve food, energy, and water system sustainability",
LESLIE S KAUFMAN,"Corals of the genus Porites are a locally abundant component of the epibiont community on mangrove prop roots at Calabash Caye, Turneffe Atoll, Belize","Mangroves are generally regarded as inhospitable for corals, but recent reports suggest they provide ecological refuge for some species. We surveyed diverse mangrove habitats on Turneffe Atoll, Belize, documenting 127 colonies of Porites divaricata (Thin Finger Coral) along 1858 m of mangrove prop roots at Calabash Caye and a much more diverse coral assemblage at Crooked Creek. At Calabash, corals were highly clumped, and varied widely in size and morphology, including large well-arborized colonies, encrusting forms with few branches, and new recruits with no branches, suggesting an age-structuredpopulation exhibiting extensive morphological plasticity. The data described here contributeto an emerging picture of mangroves as potentially critical habitat for many Caribbeancoral species."
LESLIE S KAUFMAN,"Genomic comparison of the temperate coral Astrangia poculata with tropical corals yields insights into winter quiescence, innate immunity, and sexual reproduction","Facultatively symbiotic corals provide important experimental models to explore the establishment, maintenance, and breakdown of the mutualism between corals and members of the algal family Symbiodiniaceae. The temperate coral Astrangia poculata is one such model as it is not only facultatively symbiotic, but also occurs across a broad temperature and latitudinal gradient. Here, we report the de novo chromosome-scale assembly and annotation of the A. poculata genome. Though widespread segmental/tandem duplications of genomic regions were detected, we did not find strong evidence of a whole genome duplication (WGD) event. Comparison of the gene arrangement between A. poculata and the tropical coral Acropora millepora revealed 56.38% of the orthologous genes were conserved in syntenic blocks despite ~415 million years of divergence. Gene families related to sperm hyperactivation and innate immunity, including lectins, were found to contain more genes in A. millepora relative to A. poculata. Sperm hyperactivation in A. millepora is expected given the extreme requirements of gamete competition during mass spawning events in tropical corals, while lectins are important in the establishment of coral-algal symbiosis. By contrast, gene families involved in sleep promotion, feeding suppression, and circadian sleep/wake cycle processes were expanded in A. poculata. These expanded gene families may play a role in A. poculata’s ability to enter a dormancy-like state (“winter quiescence”) to survive freezing temperatures at the northern edges of the species’ range."
RONALD B CORLEY,Accumulation of Marginal Zone B Cells and Accelerated Loss of Follicular Dendritic Cells in NF-κB p50-Deficient Mice,"BACKGROUND: Marginal zone (MZ) B cells play important roles in the early phases of humoral immune responses. In addition to possessing an inherent capacity to rapidly differentiate into antibody secreting cells, MZ B cells also help to regulate the fate of both T-independent and T-dependent blood-borne antigens in the spleen. For T-dependent antigens, MZ B cells bind IgM-antigen complexes in a complement-dependent manner. Once MZ B cells bind IgM-containing immune complexes (IgM-IC), they transport them into B cell follicles for deposition onto follicular dendritic cells (FDCs), an important component of secreted IgM's ability to enhance adaptive immune responses. To further define the requirement for MZ B cells in IgM-IC deposition, mice deficient in the NF-κB protein p50, which have been reported to lack MZ B cells, were analyzed for their ability to trap IgM-IC onto FDCs. RESULTS: Mice (2 months of age) deficient in p50 (p50-/-) had small numbers of MZ B cells, as determined by cell surface phenotype and localization in the splenic MZ. These cells bound high levels of IgM-IC both in vivo and in vitro. Subsequent to the binding of IgM-IC by the MZ B cells in p50-/- mice, small amounts of IgM-IC were found localized on FDCs, suggesting that the MZ B cells retained their ability to transport these complexes into splenic follicles. Strikingly, MZ B cells accumulated with age in p50-/- mice. By 6 months of age, p50-/- mice contained normal numbers of these cells as defined by CD21/CD23 profile and high level expression of CD1d, CD9, and IgM, and by their positioning around the marginal sinus. However, FDCs from these older p50-/- mice exhibited a reduced capacity to trap IgM-IC and retain complement components. CONCLUSION: These results demonstrate that while the p50 component of the NF-κB transcription complex plays an important role in the early development of MZ B cells, MZ B cells can develop and accumulate in mice lacking this protein. These results highlight the interface between genetic deficiencies and age, and suggest that different transcription factors may play distinct roles in the development and maintenance of cell populations at different ages."
CHRISTOPHER CHEN,Mechanical response of cardiac microtissues to acute localized injury,"After a myocardial infarction (MI), the heart undergoes changes including local remodeling that can lead to regional abnormalities in mechanical and electrical properties, ultimately increasing the risk of arrhythmias and heart failure. Although these responses have been successfully recapitulated in animal models of MI, local changes in tissue and cell-level mechanics caused by MI remain difficult to study in vivo. Here, we developed an in vitro cardiac microtissue (CMT) injury system that through acute focal injury recapitulates aspects of the regional responses seen following an MI. With a pulsed laser, cell death was induced in the center of the microtissue causing a loss of calcium signaling and a complete loss of contractile function in the injured region and resulting in a 39% reduction in the CMT's overall force production. After 7 days, the injured area remained void of cardiomyocytes (CMs) and showed increased expression of vimentin and fibronectin, two markers for fibrotic remodeling. Interestingly, although the injured region showed minimal recovery, calcium amplitudes in uninjured regions returned to levels comparable with control. Furthermore, overall force production returned to preinjury levels despite the lack of contractile function in the injured region. Instead, uninjured regions exhibited elevated contractile function, compensating for the loss of function in the injured region, drawing parallels to changes in tissue-level mechanics seen in vivo. Overall, this work presents a new in vitro model to study cardiac tissue remodeling and electromechanical changes after injury.NEW & NOTEWORTHY We report an in vitro cardiac injury model that uses a high-powered laser to induce regional cell death and a focal fibrotic response within a human-engineered cardiac microtissue. The model captures the effects of acute injury on tissue response, remodeling, and electromechanical recovery in both the damaged region and surrounding healthy tissue, modeling similar changes to contractile function observed in vivo following myocardial infarction."
CHRISTOPHER CHEN,The Event Horizon general relativistic magnetohydrodynamic code comparison project,"Recent developments in compact object astrophysics, especially the discovery of merging neutron stars by LIGO, the imaging of the black hole in M87 by the Event Horizon Telescope, and high- precision astrometry of the Galactic Center at close to the event horizon scale by the GRAVITY experiment motivate the development of numerical source models that solve the equations of general relativistic magnetohydrodynamics (GRMHD). Here we compare GRMHD solutions for the evolution of a magnetized accretion flow where turbulence is promoted by the magnetorotational instability from a set of nine GRMHD codes: Athena++, BHAC, Cosmos++, ECHO, H-AMR, iharm3D, HARM-Noble, IllinoisGRMHD, and KORAL. Agreement among the codes improves as resolution increases, as measured by a consistently applied, specially developed set of code performance metrics. We conclude that the community of GRMHD codes is mature, capable, and consistent on these test problems."
CHRISTOPHER CHEN,A non-canonical Notch complex regulates adherens junctions and vascular barrier function,"The vascular barrier that separates blood from tissues is actively regulated by the endothelium and is essential for transport, inflammation, and haemostasis1. Haemodynamic shear stress plays a critical role in maintaining endothelial barrier function2, but how this occurs remains unknown. Here we use an engineered organotypic model of perfused microvessels to show that activation of the transmembrane receptor NOTCH1 directly regulates vascular barrier function through a non-canonical, transcription-independent signalling mechanism that drives assembly of adherens junctions, and confirm these findings in mouse models. Shear stress triggers DLL4-dependent proteolytic activation of NOTCH1 to expose the transmembrane domain of NOTCH1. This domain mediates establishment of the endothelial barrier; expression of the transmembrane domain of NOTCH1 is sufficient to rescue defects in barrier function induced by knockout of NOTCH1. The transmembrane domain restores barrier function by catalysing the formation of a receptor complex in the plasma membrane consisting of vascular endothelial cadherin, the transmembrane protein tyrosine phosphatase LAR, and the RAC1 guanidine-exchange factor TRIO. This complex activates RAC1 to drive assembly of adherens junctions and establish barrier function. Canonical transcriptional signalling via Notch is highly conserved in metazoans and is required for many processes in vascular development, including arterial–venous differentiation3, angiogenesis4 and remodelling5. We establish the existence of a non-canonical cortical NOTCH1 signalling pathway that regulates vascular barrier function, and thus provide a mechanism by which a single receptor might link transcriptional programs with adhesive and cytoskeletal remodelling."
CHRISTOPHER CHEN,NRXN3 Is a Novel Locus for Waist Circumference: A Genome-Wide Association Study from the CHARGE Consortium,"Central abdominal fat is a strong risk factor for diabetes and cardiovascular disease. To identify common variants influencing central abdominal fat, we conducted a two-stage genome-wide association analysis for waist circumference (WC). In total, three loci reached genome-wide significance. In stage 1, 31,373 individuals of Caucasian descent from eight cohort studies confirmed the role of FTO and MC4R and identified one novel locus associated with WC in the neurexin 3 gene [NRXN3 (rs10146997, p = 6.4×10−7)]. The association with NRXN3 was confirmed in stage 2 by combining stage 1 results with those from 38,641 participants in the GIANT consortium (p = 0.009 in GIANT only, p = 5.3×10−8 for combined analysis, n = 70,014). Mean WC increase per copy of the G allele was 0.0498 z-score units (0.65 cm). This SNP was also associated with body mass index (BMI) [p = 7.4×10−6, 0.024 z-score units (0.10 kg/m2) per copy of the G allele] and the risk of obesity (odds ratio 1.13, 95% CI 1.07–1.19; p = 3.2×10−5 per copy of the G allele). The NRXN3 gene has been previously implicated in addiction and reward behavior, lending further evidence that common forms of obesity may be a central nervous system-mediated disorder. Our findings establish that common variants in NRXN3 are associated with WC, BMI, and obesity. Author Summary Obesity is a major health concern worldwide. In the past two years, genome-wide association studies of DNA markers known as SNPs (single nucleotide polymorphisms) have identified two novel genetic factors that may help scientists better understand why some people may be more susceptible to obesity. Similarly, this paper describes results from a large scale genome-wide association analysis for obesity susceptibility genes that includes 31,373 individuals from 8 separate studies. We uncovered a new gene influencing waist circumference, the neurexin 3 gene (NRXN3), which has been previously implicated in studies of addiction and reward behavior. These findings lend further evidence that our genes may influence our desire and consumption of food and, in turn, our susceptibility to obesity."
CHRISTOPHER CHEN,"Sarc-Graph: automated segmentation, tracking, and analysis of sarcomeres in hiPSC-derived cardiomyocytes","A better fundamental understanding of human induced pluripotent stem cell-derived cardiomyocytes (hiPSC-CMs) has the potential to advance applications ranging from drug discovery to cardiac repair. Automated quantitative analysis of beating hiPSC-CMs is an important and fast developing component of the hiPSC-CM research pipeline. Here we introduce ""Sarc-Graph,"" a computational framework to segment, track, and analyze sarcomeres in fluorescently tagged hiPSC-CMs. Our framework includes functions to segment z-discs and sarcomeres, track z-discs and sarcomeres in beating cells, and perform automated spatiotemporal analysis and data visualization. In addition to reporting good performance for sarcomere segmentation and tracking with little to no parameter tuning and a short runtime, we introduce two novel analysis approaches. First, we construct spatial graphs where z-discs correspond to nodes and sarcomeres correspond to edges. This makes measuring the network distance between each sarcomere (i.e., the number of connecting sarcomeres separating each sarcomere pair) straightforward. Second, we treat tracked and segmented components as fiducial markers and use them to compute the approximate deformation gradient of the entire tracked population. This represents a new quantitative descriptor of hiPSC-CM function. We showcase and validate our approach with both synthetic and experimental movies of beating hiPSC-CMs. By publishing Sarc-Graph, we aim to make automated quantitative analysis of hiPSC-CM behavior more accessible to the broader research community."
CHRISTOPHER CHEN,Optical calibration of the SNO+ detector in the water phase with deployed sources,"SNO+ is a large-scale liquid scintillator experiment with the primary goal of searching for neutrinoless double beta decay, and is located approximately 2 km underground in SNOLAB, Sudbury, Canada. The detector acquired data for two years as a pure water Cherenkov detector, starting in May 2017. During this period, the optical properties of the detector were measured in situ using a deployed light diffusing sphere, with the goal of improving the detector model and the energy response systematic uncertainties. The measured parameters included the water attenuation coefficients, effective attenuation coefficients for the acrylic vessel, and the angular response of the photomultiplier tubes and their surrounding light concentrators, all across different wavelengths. The calibrated detector model was validated using a deployed tagged gamma source, which showed a 0.6% variation in energy scale across the primary target volume."
CHRISTOPHER CHEN,The impact of ear growth on identification rates using an ear biometric system in young infants,"BACKGROUND: Accurate patient identification is essential for delivering longitudinal care. Our team developed an ear biometric system (SEARCH) to improve patient identification. To address how ear growth affects matching rates longitudinally, we constructed an infant cohort, obtaining ear image sets monthly to map a 9-month span of observations. This analysis had three main objectives: 1) map trajectory of ear growth during the first 9 months of life; 2) determine the impact of ear growth on matching accuracy; and 3) explore computer vision techniques to counter a loss of accuracy.   METHODOLOGY: Infants were enrolled from an urban clinic in Lusaka, Zambia. Roughly half were enrolled at their first vaccination visit and ~half at their last vaccination. Follow-up visits for each patient occurred monthly for 6 months. At each visit, we collected four images of the infant’s ears, and the child’s weight. We analyze ear area versus age and change in ear area versus age. We conduct pair-wise comparisons for all age intervals. RESULTS: From 227 enrolled infants we acquired age-specific datasets for 6 days through 9 months. Maximal ear growth occurred between 6 days and 14 weeks. Growth was significant until 6 months of age, after which further growth appeared minimal. Examining look-back performance to the 6-month visit, baseline pair-wise comparisons yielded identification rates that ranged 46.9–75%. Concatenating left and right ears per participant improved identification rates to 61.5–100%. Concatenating images captured on adjacent visits further improved identification rates to 90.3–100%. Lastly, combining these two approaches improved identification to 100%. All matching strategies showed the weakest matching rates during periods of maximal growth (i.e., <6 months). CONCLUSION: By quantifying the effect that ear growth has on performance of the SEARCH platform, we show that ear identification is a feasible solution for patient identification in an infant population 6 months and above."
CHRISTOPHER CHEN,Multiple Independent Loci at Chromosome 15q25.1 Affect Smoking Quantity: a Meta-Analysis and Comparison with Lung Cancer and COPD,"Recently, genetic association findings for nicotine dependence, smoking behavior, and smoking-related diseases converged to implicate the chromosome 15q25.1 region, which includes the CHRNA5-CHRNA3-CHRNB4 cholinergic nicotinic receptor subunit genes. In particular, association with the nonsynonymous CHRNA5 SNP rs16969968 and correlates has been replicated in several independent studies. Extensive genotyping of this region has suggested additional statistically distinct signals for nicotine dependence, tagged by rs578776 and rs588765. One goal of the Consortium for the Genetic Analysis of Smoking Phenotypes (CGASP) is to elucidate the associations among these markers and dichotomous smoking quantity (heavy versus light smoking), lung cancer, and chronic obstructive pulmonary disease (COPD). We performed a meta-analysis across 34 datasets of European-ancestry subjects, including 38,617 smokers who were assessed for cigarettes-per-day, 7,700 lung cancer cases and 5,914 lung-cancer-free controls (all smokers), and 2,614 COPD cases and 3,568 COPD-free controls (all smokers). We demonstrate statistically independent associations of rs16969968 and rs588765 with smoking (mutually adjusted p-values<10−35 and >10−8 respectively). Because the risk alleles at these loci are negatively correlated, their association with smoking is stronger in the joint model than when each SNP is analyzed alone. Rs578776 also demonstrates association with smoking after adjustment for rs16969968 (p<10−6). In models adjusting for cigarettes-per-day, we confirm the association between rs16969968 and lung cancer (p<10−20) and observe a nominally significant association with COPD (p = 0.01); the other loci are not significantly associated with either lung cancer or COPD after adjusting for rs16969968. This study provides strong evidence that multiple statistically distinct loci in this region affect smoking behavior. This study is also the first report of association between rs588765 (and correlates) and smoking that achieves genome-wide significance; these SNPs have previously been associated with mRNA levels of CHRNA5 in brain and lung tissue. Author Summary Nicotine binds to cholinergic nicotinic receptors, which are composed of a variety of subunits. Genetic studies for smoking behavior and smoking-related diseases have implicated a genomic region that encodes the alpha5, alpha3, and beta4 subunits. We examined genetic data across this region for over 38,000 smokers, a subset of which had been assessed for lung cancer or chronic obstructive pulmonary disease. We demonstrate strong evidence that there are at least two statistically independent loci in this region that affect risk for heavy smoking. One of these loci represents a change in the protein structure of the alpha5 subunit. This work is also the first to report strong evidence of association between smoking and a group of genetic variants that are of biological interest because of their links to expression of the alpha5 cholinergic nicotinic receptor subunit gene. These advances in understanding the genetic influences on smoking behavior are important because of the profound public health burdens caused by smoking and nicotine addiction."
CHRISTOPHER CHEN,Adhesive and mechanical regulation of mesenchymal stem cell differentiation in human bone marrow and periosteum-derived progenitor cells,"It has previously been demonstrated that cell shape can influence commitment of human bone marrow-derived mesenchymal stem cells (hBMCs) to adipogenic, osteogenic, chondrogenic, and other lineages. Human periosteum-derived cells (hPDCs) exhibit multipotency similar to hBMCs, but hPDCs may offer enhanced potential for osteogenesis and chondrogenesis given their apparent endogenous role in bone and cartilage repair in vivo. Here, we examined whether hPDC differentiation is regulated by adhesive and mechanical cues comparable to that reported for hBMC differentiation. When cultured in the appropriate induction media, hPDCs at high cell seeding density demonstrated enhanced levels of adipogenic or chondrogenic markers as compared with hPDCs at low cell seeding density. Cell seeding density correlated inversely with projected area of cell spreading, and directly limiting cell spreading with micropatterned substrates promoted adipogenesis or chondrogenesis while substrates promoting cell spreading supported osteogenesis. Interestingly, cell seeding density influenced differentiation through both changes in cell shape and non-shape-mediated effects: density-dependent adipogenesis and chondrogenesis were regulated primarily by cell shape whereas non-shape effects strongly influenced osteogenic potential. Inhibition of cytoskeletal contractility by adding the Rho kinase inhibitor Y27632 further enhanced adipogenic differentiation and discouraged osteogenic differentiation of hPDCs. Together, our results suggest that multipotent lineage decisions of hPDCs are impacted by cell adhesive and mechanical cues, though to different extents than hBMCs. Thus, future studies of hPDCs and other primary stem cell populations with clinical potential should consider varying biophysical metrics for more thorough optimization of stem cell differentiation."
CHRISTOPHER CHEN,Cellular forces and matrix assembly coordinate fibrous tissue repair,"Planar in vitro models have been invaluable tools to identify the mechanical basis of wound closure. Although these models may recapitulate closure dynamics of epithelial cell sheets, they fail to capture how a wounded fibrous tissue rebuilds its 3D architecture. Here we develop a 3D biomimetic model for soft tissue repair and demonstrate that fibroblasts ensconced in a collagen matrix rapidly close microsurgically induced defects within 24 h. Traction force microscopy and time-lapse imaging reveal that closure of gaps begins with contractility-mediated whole-tissue deformations. Subsequently, tangentially migrating fibroblasts along the wound edge tow and assemble a progressively thickening fibronectin template inside the gap that provide the substrate for cells to complete closure. Unlike previously reported mechanisms based on lamellipodial protrusions and purse-string contraction, our data reveal a mode of stromal closure in which coordination of tissue-scale deformations, matrix assembly and cell migration act together to restore 3D tissue architecture"
CHRISTOPHER CHEN,Force-FAK signaling coupling at individual focal adhesions coordinates mechanosensing and microtissue repair,"How adhesive forces are transduced and integrated into biochemical signals at focal adhesions (FAs) is poorly understood. Using cells adhering to deformable micropillar arrays, we demonstrate that traction force and FAK localization as well as traction force and Y397-FAK phosphorylation are linearly coupled at individual FAs on stiff, but not soft, substrates. Similarly, FAK phosphorylation increases linearly with external forces applied to FAs using magnetic beads. This mechanosignaling coupling requires actomyosin contractility, talin-FAK binding, and full-length vinculin that binds talin and actin. Using an in vitro 3D biomimetic wound healing model, we show that force-FAK signaling coupling coordinates cell migration and tissue-scale forces to promote microtissue repair. A simple kinetic binding model of talin-FAK interactions under force can recapitulate the experimental observations. This study provides insights on how talin and vinculin convert forces into FAK signaling events regulating cell migration and tissue repair."
CHRISTOPHER CHEN,Reconstituting the dynamics of endothelial cells and fibroblasts in wound closure,"The formation of healthy vascularized granulation tissue is essential for rapid wound closure and the prevention of chronic wounds in humans, yet how endothelial cells and fibroblasts coordinate during this process has been difficult to study. Here, we have developed an in vitro system that reveals how human endothelial and stromal cells in a 3D matrix respond during wound healing and granulation tissue formation. By creating incisions in engineered cultures composed of human umbilical vein endothelial cells and human lung fibroblasts embedded within a 3D matrix, we observed that these tissues are able to close the wound within approximately 4 days. Live tracking of cells during wound closure revealed that the process is mediated primarily by fibroblasts. The fibroblasts migrate circumferentially around the wound edge during early phases of healing, while contracting the wound. The fibroblast-derived matrix is, then, deposited into the void, facilitating fibroblast migration toward the wound center and filling of the void. Interestingly, the endothelial cells remain at the periphery of the wound rather than actively sprouting into the healing region to restore the vascular network. This study captures the dynamics of endothelial and fibroblast-mediated closure of three-dimensional wounds, which results in the repopulation of the wound with the cell-derived extracellular matrix representative of early granulation tissue, thus presenting a model for future studies to investigate factors regulating vascularized granulation tissue formation."
CHRISTOPHER CHEN,A biomimetic pancreatic cancer on-chip reveals endothelial ablation via ALK7 signaling,"Pancreatic ductal adenocarcinoma (PDAC) is an aggressive, lethal malignancy that invades adjacent vasculatures and spreads to distant sites before clinical detection. Although invasion into the peripancreatic vasculature is one of the hallmarks of PDAC, paradoxically, PDAC tumors also exhibit hypovascularity. How PDAC tumors become hypovascular is poorly understood. We describe an organotypic PDAC-on-a-chip culture model that emulates vascular invasion and tumor-blood vessel interactions to better understand PDAC-vascular interactions. The model features a 3D matrix containing juxtaposed PDAC and perfusable endothelial lumens. PDAC cells invaded through intervening matrix, into vessel lumen, and ablated the endothelial cells, leaving behind tumor-filled luminal structures. Endothelial ablation was also observed in in vivo PDAC models. We also identified the activin-ALK7 pathway as a mediator of endothelial ablation by PDAC. This tumor-on-a-chip model provides an important in vitro platform for investigating the process of PDAC-driven endothelial ablation and may provide a mechanism for tumor hypovascularity."
CHRISTOPHER CHEN,Myosin IIA-mediated forces regulate multicellular integrity during vascular sprouting,"Angiogenic sprouting is a critical process involved in vascular network formation within tissues. During sprouting, tip cells and ensuing stalk cells migrate collectively into the extracellular matrix while preserving cell-cell junctions, forming patent structures that support blood flow. Although several signaling pathways have been identified as controlling sprouting, it remains unclear to what extent this process is mechanoregulated. To address this question, we investigated the role of cellular contractility in sprout morphogenesis, using a biomimetic model of angiogenesis. Three-dimensional maps of mechanical deformations generated by sprouts revealed that mainly leader cells, not stalk cells, exert contractile forces on the surrounding matrix. Surprisingly, inhibiting cellular contractility with blebbistatin did not affect the extent of cellular invasion but resulted in cell-cell dissociation primarily between tip and stalk cells. Closer examination of cell-cell junctions revealed that blebbistatin impaired adherens-junction organization, particularly between tip and stalk cells. Using CRISPR/Cas9-mediated gene editing, we further identified NMIIA as the major isoform responsible for regulating multicellularity and cell contractility during sprouting. Together, these studies reveal a critical role for NMIIA-mediated contractile forces in maintaining multicellularity during sprouting and highlight the central role of forces in regulating cell-cell adhesions during collective motility."
CHRISTOPHER CHEN,Transient support from fibroblasts is sufficient to drive functional vascularization in engineered tissues,"Formation of capillary blood vasculature is a critical requirement for native as well as engineered organs and can be induced in vitro by co-culturing endothelial cells with fibroblasts. However, whether these fibroblasts are required only in the initial morphogenesis of endothelial cells or needed throughout is unknown, and the ability to remove these stromal cells after assembly could be useful for clinical translation. In this study, we introduce a technique termed CAMEO (Controlled Apoptosis in Multicellular Tissues for Engineered Organogenesis), whereby fibroblasts are selectively ablated on demand, and utilize it to probe the dispensability of fibroblasts in vascular morphogenesis. The presence of fibroblasts is shown to be necessary only during the first few days of endothelial cell morphogenesis, after which they can be ablated without significantly affecting the structural and functional features of the developed vasculature. Furthermore, we demonstrate the use of CAMEO to vascularize a construct containing primary human hepatocytes that improved tissue function. In conclusion, this study suggests that transient, initial support from fibroblasts is sufficient to drive vascular morphogenesis in engineered tissues, and this strategy of engineering-via-elimination may provide a new general approach for achieving desired functions and cell compositions in engineered organs."
CHRISTOPHER CHEN,The SNO+ experiment,
CHRISTOPHER CHEN,Buildout and integration of an automated high-throughput CLIA laboratory for SARS-CoV-2 testing on a large urban campus,"In 2019, the first cases of SARS-CoV-2 were detected in Wuhan, China, and by early 2020 the first cases were identified in the United States. SARS-CoV-2 infections increased in the US causing many states to implement stay-at-home orders and additional safety precautions to mitigate potential outbreaks. As policies changed throughout the pandemic and restrictions lifted, there was an increase in demand for COVID-19 testing which was costly, difficult to obtain, or had long turn-around times. Some academic institutions, including Boston University (BU), created an on-campus COVID-19 screening protocol as part of a plan for the safe return of students, faculty, and staff to campus with the option for in-person classes. At BU, we put together an automated high-throughput clinical testing laboratory with the capacity to run 45,000 individual tests weekly by Fall of 2020, with a purpose-built clinical testing laboratory, a multiplexed reverse transcription PCR (RT-qPCR) test, robotic instrumentation, and trained staff. There were many challenges including supply chain issues for personal protective equipment and testing materials in addition to equipment that were in high demand. The BU Clinical Testing Laboratory (CTL) was operational at the start of Fall 2020 and performed over 1 million SARS-CoV-2 PCR tests during the 2020-2021 academic year."
CHRISTOPHER CHEN,Current status and future prospects of the SNO+ experiment,"SNO+ is a large liquid scintillator-based experiment located 2 km underground at SNOLAB, Sudbury, Canada. It reuses the Sudbury Neutrino Observatory detector, consisting of a 12 m diameter acrylic vessel which will be filled with about 780 tonnes of ultra-pure liquid scintillator. Designed as a multipurpose neutrino experiment, the primary goal of SNO+ is a search for the neutrinoless double-beta decay (0νββ) of ^130Te. In Phase I, the detector will be loaded with 0.3% natural tellurium, corresponding to nearly 800 kg of ^130Te, with an expected effective Majorana neutrino mass sensitivity in the region of 55–133 meV, just above the inverted mass hierarchy. Recently, the possibility of deploying up to ten times more natural tellurium has been investigated, which would enable SNO+ to achieve sensitivity deep into the parameter space for the inverted neutrino mass hierarchy in the future. Additionally, SNO+ aims to measure reactor antineutrino oscillations, low energy solar neutrinos, and geoneutrinos, to be sensitive to supernova neutrinos, and to search for exotic physics. A first phase with the detector filled with water will begin soon, with the scintillator phase expected to start after a few months of water data taking. The 0νββ Phase I is foreseen for 2017."
CHRISTOPHER CHEN,Prospects for beyond the standard model physics searches at the deep underground neutrino experiment: DUNE collaboration,"The Deep Underground Neutrino Experiment (DUNE) will be a powerful tool for a variety of physics topics. The high-intensity proton beams provide a large neutrino flux, sampled by a near detector system consisting of a combination of capable precision detectors, and by the massive far detector system located deep underground. This configuration sets up DUNE as a machine for discovery, as it enables opportunities not only to perform precision neutrino measurements that may uncover deviations from the present three-flavor mixing paradigm, but also to discover new particles and unveil new interactions and symmetries beyond those predicted in the Standard Model (SM). Of the many potential beyond the Standard Model (BSM) topics DUNE will probe, this paper presents a selection of studies quantifying DUNE's sensitivities to sterile neutrino mixing, heavy neutral leptons, non-standard interactions, CPT symmetry violation, Lorentz invariance violation, neutrino trident production, dark matter from both beam induced and cosmogenic sources, baryon number violation, and other new physics topics that complement those at high-energy colliders and significantly extend the present reach."
CHRISTOPHER CHEN,Identification and reconstruction of low-energy electrons in the ProtoDUNE-SP detector,
CHRISTOPHER CHEN,The long-baseline neutrino experiment: exploring fundamental symmetries of the universe,"The preponderance of matter over antimatter in the early Universe, the dynamics of the supernova bursts that produced the heavy elements necessary for life and whether protons eventually decay --- these mysteries at the forefront of particle physics and astrophysics are key to understanding the early evolution of our Universe, its current state and its eventual fate. The Long-Baseline Neutrino Experiment (LBNE) represents an extensively developed plan for a world-class experiment dedicated to addressing these questions. LBNE is conceived around three central components: (1) a new, high-intensity neutrino source generated from a megawatt-class proton accelerator at Fermi National Accelerator Laboratory, (2) a near neutrino detector just downstream of the source, and (3) a massive liquid argon time-projection chamber deployed as a far detector deep underground at the Sanford Underground Research Facility. This facility, located at the site of the former Homestake Mine in Lead, South Dakota, is approximately 1,300 km from the neutrino source at Fermilab -- a distance (baseline) that delivers optimal sensitivity to neutrino charge-parity symmetry violation and mass ordering effects. This ambitious yet cost-effective design incorporates scalability and flexibility and can accommodate a variety of upgrades and contributions. With its exceptional combination of experimental configuration, technical capabilities, and potential for transformative discoveries, LBNE promises to be a vital facility for the field of particle physics worldwide, providing physicists from around the globe with opportunities to collaborate in a twenty to thirty year program of exciting science. In this document we provide a comprehensive overview of LBNE's scientific objectives, its place in the landscape of neutrino physics worldwide, the technologies it will incorporate and the capabilities it will possess."
CHRISTOPHER CHEN,Tevatron-for-LHC report: preparations for discoveries,"This is the ""TeV4LHC"" report of the ""Physics Landscapes"" Working Group, focused on facilitating the start-up of physics explorations at the LHC by using the experience gained at the Tevatron. We present experimental and theoretical results that can be employed to probe various scenarios for physics beyond the Standard Model."
CHRISTOPHER CHEN,Scintillation light detection in the 6-m drift-length ProtoDUNE Dual Phase liquid argon TPC,"DUNE is a dual-site experiment for long-baseline neutrino oscillation studies, neutrino astrophysics and nucleon decay searches. ProtoDUNE Dual Phase (DP) is a 6  ×  6  ×  6 m 3 liquid argon time-projection-chamber (LArTPC) that recorded cosmic-muon data at the CERN Neutrino Platform in 2019-2020 as a prototype of the DUNE Far Detector. Charged particles propagating through the LArTPC produce ionization and scintillation light. The scintillation light signal in these detectors can provide the trigger for non-beam events. In addition, it adds precise timing capabilities and improves the calorimetry measurements. In ProtoDUNE-DP, scintillation and electroluminescence light produced by cosmic muons in the LArTPC is collected by photomultiplier tubes placed up to 7 m away from the ionizing track. In this paper, the ProtoDUNE-DP photon detection system performance is evaluated with a particular focus on the different wavelength shifters, such as PEN and TPB, and the use of Xe-doped LAr, considering its future use in giant LArTPCs. The scintillation light production and propagation processes are analyzed and a comparison of simulation to data is performed, improving understanding of the liquid argon properties."
CHRISTOPHER CHEN,Probing the subcellular nanostructure of engineered human cardiomyocytes in 3D tissue,"The structural and functional maturation of human induced pluripotent stem cell-derived cardiomyocytes (hiPSC-CMs) is essential for pharmaceutical testing, disease modeling, and ultimately therapeutic use. Multicellular 3D-tissue platforms have improved the functional maturation of hiPSC-CMs, but probing cardiac contractile properties in a 3D environment remains challenging, especially at depth and in live tissues. Using small-angle X-ray scattering (SAXS) imaging, we show that hiPSC-CMs matured and examined in a 3D environment exhibit a periodic spatial arrangement of the myofilament lattice, which has not been previously detected in hiPSC-CMs. The contractile force is found to correlate with both the scattering intensity (R 2 = 0.44) and lattice spacing (R 2 = 0.46). The scattering intensity also correlates with lattice spacing (R 2 = 0.81), suggestive of lower noise in our structural measurement than in the functional measurement. Notably, we observed decreased myofilament ordering in tissues with a myofilament mutation known to lead to hypertrophic cardiomyopathy (HCM). Our results highlight the progress of human cardiac tissue engineering and enable unprecedented study of structural maturation in hiPSC-CMs."
CHRISTOPHER CHEN,Direct laser writing for cardiac tissue engineering: a microfluidic heart on a chip with integrated transducers,"We have developed a microfluidic platform for engineering cardiac microtissues in highly-controlled microenvironments. The platform is fabricated using direct laser writing (DLW) lithography and soft lithography, and contains four separate devices. Each individual device houses a cardiac microtissue and is equipped with an integrated strain actuator and a force sensor. Application of external pressure waves to the platform results in controllable time-dependent forces on the microtissues. Conversely, oscillatory forces generated by the microtissues are transduced into measurable electrical outputs. We demonstrate the capabilities of this platform by studying the response of cardiac microtissues derived from human induced pluripotent stem cells (hiPSC) under prescribed mechanical loading and pacing. This platform will be used for fundamental studies and drug screening on cardiac microtissues."
CHRISTOPHER CHEN,Event Horizon Telescope imaging of the archetypal blazar 3C 279 at an extreme 20 microarcsecond resolution,"3C 279 is an archetypal blazar with a prominent radio jet that show broadband flux density variability across the entire electromagnetic spectrum. We use an ultra-high angular resolution technique – global Very Long Baseline Interferometry (VLBI) at 1.3 mm (230 GHz) – to resolve the innermost jet of 3C 279 in order to study its fine-scale morphology close to the jet base where highly variable γ-ray emission is thought to originate, according to various models. The source was observed during four days in April 2017 with the Event Horizon Telescope at 230 GHz, including the phased Atacama Large Millimeter/submillimeter Array (ALMA), at an angular resolution of ∼20 μas (at a redshift of z = 0.536 this corresponds to ∼0.13 pc  ∼ 1700 Schwarzschild radii with a black hole mass M<jats:sub>BH</jats:sub> = 8 × 10^8 M_⊙). Imaging and model-fitting techniques were applied to the data to parameterize the fine-scale source structure and its variation. We find a multicomponent inner jet morphology with the northernmost component elongated perpendicular to the direction of the jet, as imaged at longer wavelengths. The elongated nuclear structure is consistent on all four observing days and across different imaging methods and model-fitting techniques, and therefore appears robust. Owing to its compactness and brightness, we associate the northern nuclear structure as the VLBI “core”. This morphology can be interpreted as either a broad resolved jet base or a spatially bent jet. We also find significant day-to-day variations in the closure phases, which appear most pronounced on the triangles with the longest baselines. Our analysis shows that this variation is related to a systematic change of the source structure. Two inner jet components move non-radially at apparent speeds of ∼15 c and ∼20 c (∼1.3 and ∼1.7 μas day^−1, respectively), which more strongly supports the scenario of traveling shocks or instabilities in a bent, possibly rotating jet. The observed apparent speeds are also coincident with the 3C 279 large-scale jet kinematics observed at longer (cm) wavelengths, suggesting no significant jet acceleration between the 1.3 mm core and the outer jet. The intrinsic brightness temperature of the jet components are ≲10^10 K, a magnitude or more lower than typical values seen at ≥7 mm wavelengths. The low brightness temperature and morphological complexity suggest that the core region of 3C 279 becomes optically thin at short (mm) wavelengths."
CHRISTOPHER CHEN,First M87 Event Horizon Telescope results. II. Array and instrumentation,"The Event Horizon Telescope (EHT) is a very long baseline interferometry (VLBI) array that comprises millimeter- and submillimeter-wavelength telescopes separated by distances comparable to the diameter of the Earth. At a nominal operating wavelength of ~1.3 mm, EHT angular resolution (λ/D) is ~25 μas, which is sufficient to resolve nearby supermassive black hole candidates on spatial and temporal scales that correspond to their event horizons. With this capability, the EHT scientific goals are to probe general relativistic effects in the strong-field regime and to study accretion and relativistic jet formation near the black hole boundary. In this Letter we describe the system design of the EHT, detail the technology and instrumentation that enable observations, and provide measures of its performance. Meeting the EHT science objectives has required several key developments that have facilitated the robust extension of the VLBI technique to EHT observing wavelengths and the production of instrumentation that can be deployed on a heterogeneous array of existing telescopes and facilities. To meet sensitivity requirements, high-bandwidth digital systems were developed that process data at rates of 64 gigabit s−1, exceeding those of currently operating cm-wavelength VLBI arrays by more than an order of magnitude. Associated improvements include the development of phasing systems at array facilities, new receiver installation at several sites, and the deployment of hydrogen maser frequency standards to ensure coherent data capture across the array. These efforts led to the coordination and execution of the first Global EHT observations in 2017 April, and to event-horizon-scale imaging of the supermassive black hole candidate in M87."
CHRISTOPHER CHEN,First M87 Event Horizon Telescope results. I. The shadow of the supermassive black hole,"When surrounded by a transparent emission region, black holes are expected to reveal a dark shadow caused by gravitational light bending and photon capture at the event horizon. To image and study this phenomenon, we have assembled the Event Horizon Telescope, a global very long baseline interferometry array observing at a wavelength of 1.3 mm. This allows us to reconstruct event-horizon-scale images of the supermassive black hole candidate in the center of the giant elliptical galaxy M87. We have resolved the central compact radio source as an asymmetric bright emission ring with a diameter of 42 ± 3 μas, which is circular and encompasses a central depression in brightness with a flux ratio gsim10:1. The emission ring is recovered using different calibration and imaging schemes, with its diameter and width remaining stable over four different observations carried out in different days. Overall, the observed image is consistent with expectations for the shadow of a Kerr black hole as predicted by general relativity. The asymmetry in brightness in the ring can be explained in terms of relativistic beaming of the emission from a plasma rotating close to the speed of light around a black hole. We compare our images to an extensive library of ray-traced general-relativistic magnetohydrodynamic simulations of black holes and derive a central mass of M = (6.5 ± 0.7) × 10^9 M ⊙. Our radio-wave observations thus provide powerful evidence for the presence of supermassive black holes in centers of galaxies and as the central engines of active galactic nuclei. They also present a new tool to explore gravity in its most extreme limit and on a mass scale that was so far not accessible."
RANDALL P ELLIS,Diagnosis-based risk adjustment for Medicare capitation payments.,"Using 1991-92 data for a 5-percent Medicare sample, we develop, estimate, and evaluate risk-adjustment models that utilize diagnostic information from both inpatient and ambulatory claims to adjust payments for aged and disabled Medicare enrollees. Hierarchical coexisting conditions (HCC) models achieve greater explanatory power than diagnostic cost group (DCG) models by taking account of multiple coexisting medical conditions. Prospective models predict average costs of individuals with chronic conditions nearly as well as concurrent models. All models predict medical costs far more accurately than the current health maintenance organization (HMO) payment formula."
RANDALL P ELLIS,Risk adjustment of Medicare capitation payments using the CMS-HCC model,"This article describes the CMS hierarchical condition categories (HCC) model implemented in 2004 to adjust Medicare capitation payments to private health care plans for the health expenditure risk of their enrollees. We explain the model's principles, elements, organization, calibration, and performance. Modifications to reduce plan data reporting burden and adaptations for disabled, institutionalized, newly enrolled, and secondary payer subpopulations are discussed."
RANDALL P ELLIS,"Development and assessment of a new framework for disease surveillance, prediction, and risk adjustment: the diagnostic items classification system","IMPORTANCE: Current disease risk-adjustment formulas in the US rely on diagnostic classification frameworks that predate the International Classification of Diseases, Tenth Revision, Clinical Modification (ICD-10-CM). OBJECTIVE: To develop an ICD-10-CM-based classification framework for predicting diverse health care payment, quality, and performance outcomes. DESIGN SETTING AND PARTICIPANTS: Physician teams mapped all ICD-10-CM diagnoses into 3 types of diagnostic items (DXIs): main effect DXIs that specify diseases; modifiers, such as laterality, timing, and acuity; and scaled variables, such as body mass index, gestational age, and birth weight. Every diagnosis was mapped to at least 1 DXI. Stepwise and weighted least-squares estimation predicted cost and utilization outcomes, and their performance was compared with models built on (1) the Agency for Healthcare Research and Quality Clinical Classifications Software Refined (CCSR) categories, and (2) the Health and Human Services Hierarchical Condition Categories (HHS-HCC) used in the Affordable Care Act Marketplace. Each model's performance was validated using R 2, mean absolute error, the Cumming prediction measure, and comparisons of actual to predicted outcomes by spending percentiles and by diagnostic frequency. The IBM MarketScan Commercial Claims and Encounters Database, 2016 to 2018, was used, which included privately insured, full- or partial-year eligible enrollees aged 0 to 64 years in plans with medical, drug, and mental health/substance use coverage. MAIN OUTCOMES AND MEASURES: Fourteen concurrent outcomes were predicted: overall and plan-paid health care spending (top-coded and not top-coded); enrollee out-of-pocket spending; hospital days and admissions; emergency department visits; and spending for 6 types of services. The primary outcome was annual health care spending top-coded at $250 000. RESULTS: A total of 65 901 460 person-years were split into 90% estimation/10% validation samples (n = 6 604 259). In all, 3223 DXIs were created: 2435 main effects, 772 modifiers, and 16 scaled items. Stepwise regressions predicting annual health care spending (mean [SD], $5821 [$17 653]) selected 76% of the main effect DXIs with no evidence of overfitting. Validated R 2 was 0.589 in the DXI model, 0.539 for CCSR, and 0.428 for HHS-HCC. Use of DXIs reduced underpayment for enrollees with rare (1-in-a-million) diagnoses by 83% relative to HHS-HCCs. CONCLUSIONS: In this diagnostic modeling study, the new DXI classification system showed improved predictions over existing diagnostic classification systems for all spending and utilization outcomes considered."
RANDALL P ELLIS,"Risk selection, risk adjustment and choice: concepts and lessons from the Americas","Interest has grown worldwide in risk adjustment and risk sharing due to their potential to contain costs, improve fairness, and reduce selection problems in health care markets. Significant steps have been made in the empirical development of risk adjustment models, and in the theoretical foundations of risk adjustment and risk sharing. This literature has often modeled the effects of risk adjustment without highlighting the institutional setting, regulations, and diverse selection problems that risk adjustment is intended to fix. Perhaps because of this, the existing literature and their recommendations for optimal risk adjustment or optimal payment systems are sometimes confusing. In this paper, we present a unified way of thinking about the organizational structure of health care systems, which enables us to focus on two key dimensions of markets that have received less attention: what choices are available that may lead to selection problems, and what financial or regulatory tools other than risk adjustment are used to influence these choices. We specifically examine the health care systems, choices, and problems in four countries: the US, Canada, Chile, and Colombia, and examine the relationship between selection-related efficiency and fairness problems and the choices that are allowed in each country, and discuss recent regulatory reforms that affect choices and selection problems. In this sample, countries and insurance programs with more choices have more selection problems."
RANDALL P ELLIS,Evaluating the impact of Brazil’s central audit program on municipal provision of health services,"We evaluate the success of Brazil’s Corregedoria-Geral da União’s (CGU) anti-corruption program in fostering better outcomes in the health sector using panel data from 5560 Brazilian municipalities over the period from 2000 to 2011. Since 2003, the program has randomly selected municipalities to be investigated each year, and immediately disclosed its findings. We examine two mechanisms through which this program could matter: a deterrent effect whereby municipalities react to the threat of being audited, and an auditing effect, whereby municipalities change behavior only when actually audited. A regression discontinuity approach on four outcomes likely to react quickly to corruption changes finds no improvement due to the deterrent or auditing effect, while difference-in-difference models suggest statistically significant but a small short-run effect of actually being audited on the infant mortality rate. Overall, we do not find any meaningful effect of the anticorruption audit program on the health indicators studied."
RANDALL P ELLIS,Mispricing in the Medicare advantage risk adjustment model,"The Centers for Medicare and Medicaid Services (CMS) implemented hierarchical condition category (HCC) models in 2004 to adjust payments to Medicare Advantage (MA) plans to reflect enrollees’ expected health care costs. We use Verisk Health’s diagnostic cost group (DxCG) Medicare models, refined “descendants” of the same HCC framework with 189 comprehensive clinical categories available to CMS in 2004, to reveal 2 mispricing errors resulting from CMS’ implementation. One comes from ignoring all diagnostic information for “new enrollees” (those with less than 12 months of prior claims). Another comes from continuing to use the simplified models that were originally adopted in response to assertions from some capitated health plans that submitting the claims-like data that facilitate richer models was too burdensome. Even the main CMS model being used in 2014 recognizes only 79 condition categories, excluding many diagnoses and merging conditions with somewhat heterogeneous costs. Omitted conditions are typically lower cost or “vague” and not easily audited from simplified data submissions. In contrast, DxCG Medicare models use a comprehensive, 394-HCC classification system. Applying both models to Medicare’s 2010-2011 fee-for-service 5% sample, we find mispricing and lower predictive accuracy for the CMS implementation. For example, in 2010, 13% of beneficiaries had at least 1 higher cost DxCG-recognized condition but no CMS-recognized condition; their 2011 actual costs averaged US$6628, almost one-third more than the CMS model prediction. As MA plans must now supply encounter data, CMS should consider using more refined and comprehensive (DxCG-like) models."
RANDALL P ELLIS,"Diagnostic category prevalence in 3 classification systems across the transition to the International Classification of Diseases, Tenth Revision, Clinical Modification","IMPORTANCE: On October 1, 2015, the US transitioned to the International Classification of Diseases, Tenth Revision, Clinical Modification (ICD-10-CM) for recording diagnoses, symptoms, and procedures. It is unknown whether this transition was associated with changes in diagnostic category prevalence based on diagnosis classification systems commonly used for payment and quality reporting. OBJECTIVE: To assess changes in diagnostic category prevalence associated with the ICD-10-CM transition. Design, Setting, and Participants: This interrupted time series analysis and cross-sectional study examined level and trend changes in diagnostic category prevalence associated with the ICD-10-CM transition and clinically reviewed a subset of diagnostic categories with changes of 20% or more. Data included insurance claim diagnoses from the IBM MarketScan Commercial Database from January 1, 2010, to December 31, 2017, for more than 18 million people aged 0 to 64 years with private insurance. Diagnoses were mapped using 3 common diagnostic classification systems: World Health Organization (WHO) disease chapters, Department of Health and Human Services Hierarchical Condition Categories (HHS-HCCs), and Agency for Healthcare Research and Quality Clinical Classification System (AHRQ-CCS). Data were analyzed from December 1, 2018, to January 21, 2020. EXPOSURES: US implementation of ICD-10-CM. Main Outcomes and Measures: Monthly rates of individuals with at least 1 diagnosis in a diagnostic classification category per 10 000 eligible members. Results: The analytic sample contained information on 2.1 billion enrollee person-months with 3.4 billion clinically assigned diagnoses; the mean (range) monthly sample size was 22.1 (18.4 to 27.1 ) million individuals. While diagnostic category prevalence changed minimally for WHO disease chapters, the ICD-10-CM transition was associated with level changes of 20% or more among 20 of 127 HHS-HCCs (15.7%) and 46 of 282 AHRQ-CCS categories (16.3%) and with trend changes of 20% or more among 12 of 127 of HHS-HCCs (9.4%) and 27 of 282 of AHRQ-CCS categories (9.6%). For HHS-HCCs, monthly rates of individuals with any acute myocardial infarction diagnosis increased 131.5% (95% CI, 124.1% to 138.8%), primarily because HHS added non-ST-segment-elevation myocardial infarction diagnoses to this category. The HHS-HCC for diabetes with chronic complications increased by 92.4% (95% CI, 84.2% to 100.5%), primarily from including new diabetes-related hypoglycemia and hyperglycemia codes, and the rate for completed pregnancy with complications decreased by 54.5% (95% CI, -58.7% to -50.2%) partly due to removing vaginal birth after cesarean delivery as a complication. CONCLUSIONS AND RELEVANCE: These findings suggest that the ICD-10-CM transition was associated with large prevalence changes for many diagnostic categories. Diagnostic classification systems developed using ICD-9-CM may need to be refined using ICD-10-CM data to avoid unintended consequences for disease surveillance, performance assessment, and risk-adjusted payments."
RANDALL P ELLIS,Managed competition in the United States: how well is it promoting equity and efficiency?,"Managed competition frameworks aim to control healthcare costs and promote access to high-quality health insurance and services through a combination of public policies and market forces. In the United States, managed competition delivery systems are varied and diffused across a patchwork of divided markets and populations. This, coupled with extremely high national health spending per capita, makes a more unified managed competition strategy an appealing alternative to a currently struggling healthcare system. We examine the relative effectiveness of three existing programmes in the U.S. that each rely upon some principles of managed competition: health insurance exchanges instituted by the Affordable Care Act, Medicaid managed care organisations, and Medicare Advantage plans. Although each programme leverages some competitive features, each faces significant hurdles as a candidate for expansion. We highlight these challenges with a survey of academic health economists, and find that provider and insurer consolidation, highly segmented markets, and failing to incentivise competitive efficiencies all dampen the success of existing programmes. Although managed competition for all is a potentially desirable framework for future health reform in the U.S., successful expansion relies on addressing fundamental issues revealed by imperfect existing programmes."
RANDALL P ELLIS,Measuring efficiency of health plan payment systems in managed competition health insurance markets,"Adverse selection in health insurance markets leads to two types of inefficiency. On the demand side, adverse selection leads to plan price distortions resulting in inefficient sorting of consumers across health plans. On the supply side, adverse selection creates incentives for plans to inefficiently distort benefits to attract profitable enrollees. Reinsurance, risk adjustment, and premium categories address these problems. Building on prior research on health plan payment system evaluation, we develop measures of the efficiency consequences of price and benefit distortions under a given payment system. Our measures are based on explicit economic models of insurer behavior under adverse selection, incorporate multiple features of plan payment systems, and can be calculated prior to observing actual insurer and consumer behavior. We illustrate the use of these measures with data from a simulated market for individual health insurance."
RANDALL P ELLIS,Health care demand elasticities by type of service,"We estimate within-year price elasticities of demand for detailed health care services using an instrumental variable strategy, in which individual monthly cost shares are instrumented by employer-year-plan-month average cost shares. A specification using backward myopic prices gives more plausible and stable results than using forward myopic prices. Using 171 million person-months spanning 73 employers from 2008 to 2014, we estimate that the overall demand elasticity by backward myopic consumers is -0.44, with higher elasticities of demand for pharmaceuticals (-0.44), specialists visits (-0.32), MRIs (-0.29) and mental health/substance abuse (-0.26), and lower elasticities for prevention visits (-0.02) and emergency rooms (-0.04). Demand response is lower for children, in larger firms, among hourly waged employees, and for sicker people. Overall the method appears promising for estimating elasticities for highly disaggregated services although the approach does not work well on services that are very expensive or persistent."
RANDALL P ELLIS,Risk-adjustment simulation: plans may have incentives to distort mental health and substance use coverage,"Under the Affordable Care Act, the risk-adjustment program is designed to compensate health plans for enrolling people with poorer health status so that plans compete on cost and quality rather than the avoidance of high-cost individuals. This study examined health plan incentives to limit covered services for mental health and substance use disorders under the risk-adjustment system used in the health insurance Marketplaces. Through a simulation of the program on a population constructed to reflect Marketplace enrollees, we analyzed the cost consequences for plans enrolling people with mental health and substance use disorders. Our assessment points to systematic underpayment to plans for people with these diagnoses. We document how Marketplace risk adjustment does not remove incentives for plans to limit coverage for services associated with mental health and substance use disorders. Adding mental health and substance use diagnoses used in Medicare Part D risk adjustment is one potential policy step toward addressing this problem in the Marketplaces."
RANDALL P ELLIS,Scope and incentives for risk selection in health insurance markets with regulated competition: a conceptual framework and international comparison,"In health insurance markets with regulated competition, regulators face the challenge of preventing risk selection. This paper provides a framework for analyzing the scope (i.e., potential actions by insurers and consumers) and incentives for risk selection in such markets. Our approach consists of three steps. First, we describe four types of risk selection: (a) selection by consumers in and out of the market, (b) selection by consumers between high- and low-value plans, (c) selection by insurers via plan design, and (d) selection by insurers via other channels such as marketing, customer service, and supplementary insurance. In a second step, we develop a conceptual framework of how regulation and features of health insurance markets affect the scope and incentives for risk selection along these four dimensions. In a third step, we use this framework to compare nine health insurance markets with regulated competition in Australia, Europe, Israel, and the United States."
JOHN BERNARDO,MAGIC and H.E.S.S. detect VHE gamma rays from the blazar OT081 for the first time: a deep multiwavelength study,
JOHN BERNARDO,Broadband multi-wavelength properties of M87 during the 2017 Event Horizon Telescope campaign,"In 2017, the Event Horizon Telescope (EHT) Collaboration succeeded in capturing the first direct image of the center of the M87 galaxy. The asymmetric ring morphology and size are consistent with theoretical expectations for a weakly accreting supermassive black hole of mass ∼6.5 × 109 M ⊙. The EHTC also partnered with several international facilities in space and on the ground, to arrange an extensive, quasi-simultaneous multi-wavelength campaign. This Letter presents the results and analysis of this campaign, as well as the multi-wavelength data as a legacy data repository. We captured M87 in a historically low state, and the core flux dominates over HST-1 at high energies, making it possible to combine core flux constraints with the more spatially precise very long baseline interferometry data. We present the most complete simultaneous multi-wavelength spectrum of the active nucleus to date, and discuss the complexity and caveats of combining data from different spatial scales into one broadband spectrum. We apply two heuristic, isotropic leptonic single-zone models to provide insight into the basic source properties, but conclude that a structured jet is necessary to explain M87’s spectrum. We can exclude that the simultaneous γ-ray emission is produced via inverse Compton emission in the same region producing the EHT mm-band emission, and further conclude that the γ-rays can only be produced in the inner jets (inward of HST-1) if there are strongly particle-dominated regions. Direct synchrotron emission from accelerated protons and secondaries cannot yet be excluded."
JOHN BERNARDO,Polarized blazar X-rays imply particle acceleration in shocks,"Most of the light from blazars, active galactic nuclei with jets of magnetized plasma that point nearly along the line of sight, is produced by high-energy particles, up to around 1 TeV. Although the jets are known to be ultimately powered by a supermassive black hole, how the particles are accelerated to such high energies has been an unanswered question. The process must be related to the magnetic field, which can be probed by observations of the polarization of light from the jets. Measurements of the radio to optical polarization-the only range available until now-probe extended regions of the jet containing particles that left the acceleration site days to years earlier1-3, and hence do not directly explore the acceleration mechanism, as could X-ray measurements. Here we report the detection of X-ray polarization from the blazar Markarian 501 (Mrk 501). We measure an X-ray linear polarization degree ΠX of around 10%, which is a factor of around 2 higher than the value at optical wavelengths, with a polarization angle parallel to the radio jet. This points to a shock front as the source of particle acceleration and also implies that the plasma becomes increasingly turbulent with distance from the shock."
JOHN BERNARDO,X-Ray Polarization Observations of BL Lacertae,"Blazars are a class of jet-dominated active galactic nuclei with a typical double-humped spectral energy distribution. It is of common consensus that the synchrotron emission is responsible for the low frequency peak, while the origin of the high frequency hump is still debated. The analysis of X-rays and their polarization can provide a valuable tool to understand the physical mechanisms responsible for the origin of high-energy emission of blazars. We report the first observations of BL Lacertae (BL Lac) performed with the Imaging X-ray Polarimetry Explorer, from which an upper limit to the polarization degree Π X &lt; 12.6% was found in the 2–8 keV band. We contemporaneously measured the polarization in radio, infrared, and optical wavelengths. Our multiwavelength polarization analysis disfavors a significant contribution of proton-synchrotron radiation to the X-ray emission at these epochs. Instead, it supports a leptonic origin for the X-ray emission in BL Lac."
JAMES A HAMILTON,Genetic Disruption of Myostatin Reduces the Development of Proatherogenic Dyslipidemia and Atherogenic Lesions In Ldlr Null Mice,"OBJECTIVE: Insulin-resistant states, such as obesity and type 2 diabetes, contribute substantially to accelerated atherogenesis. Null mutations of myostatin (Mstn) are associated with increased muscle mass and decreased fat mass. In this study, we determined whether Mstn disruption could prevent the development of insulin resistance, proatherogenic dyslipidemia, and atherogenesis. RESEARCH DESIGN AND METHODS: C57BL/6 Ldlr−/− mice were cross-bred with C57BL/6 Mstn−/− mice for >10 generations to generate Mstn−/−/Ldlr−/− double-knockout mice. The effects of high-fat/high-cholesterol diet on body composition, plasma lipids, systemic and tissue-specific insulin sensitivity, hepatic steatosis, as well as aortic atheromatous lesion were characterized in Mstn−/−/Ldlr−/− mice in comparison with control Mstn+/+/Ldlr−/− mice. RESULTS: Compared with Mstn+/+/Ldlr−/− controls, Mstn−/−/ Ldlr−/− mice were resistant to diet-induced obesity, and had greatly improved insulin sensitivity, as indicated by 42% higher glucose infusion rate and 90% greater muscle [3H]-2-deoxyglucose uptake during hyperinsulinemic-euglycemic clamp. Mstn−/−/Ldlr−/− mice were protected against diet-induced hepatic steatosis and had 56% higher rate of hepatic fatty acid β-oxidation than controls. Mstn−/−/Ldlr−/− mice also had 36% lower VLDL secretion rate and were protected against diet-induced dyslipidemia, as indicated by 30–60% lower VLDL and LDL cholesterol, free fatty acids, and triglycerides. Magnetic resonance angiography and en face analyses demonstrated 41% reduction in aortic atheromatous lesions in Ldlr−/− mice with Mstn deletion. CONCLUSIONS: Inactivation of Mstn protects against the development of insulin resistance, proatherogenic dyslipidemia, and aortic atherogenesis in Ldlr−/− mice. Myostatin may be a useful target for drug development for prevention and treatment of obesity and its associated type 2 diabetes and atherosclerosis."
JAMES A HAMILTON,"Concussion, microvascular injury, and early tauopathy in young athletes after impact head injury and an impact concussion mouse model","The mechanisms underpinning concussion, traumatic brain injury, and chronic traumatic encephalopathy, and the relationships between these disorders, are poorly understood. We examined post-mortem brains from teenage athletes in the acute-subacute period after mild closed-head impact injury and found astrocytosis, myelinated axonopathy, microvascular injury, perivascular neuroinflammation, and phosphorylated tau protein pathology. To investigate causal mechanisms, we developed a mouse model of lateral closed-head impact injury that uses momentum transfer to induce traumatic head acceleration. Unanaesthetized mice subjected to unilateral impact exhibited abrupt onset, transient course, and rapid resolution of a concussion-like syndrome characterized by altered arousal, contralateral hemiparesis, truncal ataxia, locomotor and balance impairments, and neurobehavioural deficits. Experimental impact injury was associated with axonopathy, blood–brain barrier disruption, astrocytosis, microgliosis (with activation of triggering receptor expressed on myeloid cells, TREM2), monocyte infiltration, and phosphorylated tauopathy in cerebral cortex ipsilateral and subjacent to impact. Phosphorylated tauopathy was detected in ipsilateral axons by 24 h, bilateral axons and soma by 2 weeks, and distant cortex bilaterally at 5.5 months post-injury. Impact pathologies co-localized with serum albumin extravasation in the brain that was diagnostically detectable in living mice by dynamic contrast-enhanced MRI. These pathologies were also accompanied by early, persistent, and bilateral impairment in axonal conduction velocity in the hippocampus and defective long-term potentiation of synaptic neurotransmission in the medial prefrontal cortex, brain regions distant from acute brain injury. Surprisingly, acute neurobehavioural deficits at the time of injury did not correlate with blood–brain barrier disruption, microgliosis, neuroinflammation, phosphorylated tauopathy, or electrophysiological dysfunction. Furthermore, concussion-like deficits were observed after impact injury, but not after blast exposure under experimental conditions matched for head kinematics. Computational modelling showed that impact injury generated focal point loading on the head and seven-fold greater peak shear stress in the brain compared to blast exposure. Moreover, intracerebral shear stress peaked before onset of gross head motion. By comparison, blast induced distributed force loading on the head and diffuse, lower magnitude shear stress in the brain. We conclude that force loading mechanics at the time of injury shape acute neurobehavioural responses, structural brain damage, and neuropathological sequelae triggered by neurotrauma. These results indicate that closed-head impact injuries, independent of concussive signs, can induce traumatic brain injury as well as early pathologies and functional sequelae associated with chronic traumatic encephalopathy. These results also shed light on the origins of concussion and relationship to traumatic brain injury and its aftermath."
JAMES A HAMILTON,Prospects for beyond the standard model physics searches at the deep underground neutrino experiment: DUNE collaboration,"The Deep Underground Neutrino Experiment (DUNE) will be a powerful tool for a variety of physics topics. The high-intensity proton beams provide a large neutrino flux, sampled by a near detector system consisting of a combination of capable precision detectors, and by the massive far detector system located deep underground. This configuration sets up DUNE as a machine for discovery, as it enables opportunities not only to perform precision neutrino measurements that may uncover deviations from the present three-flavor mixing paradigm, but also to discover new particles and unveil new interactions and symmetries beyond those predicted in the Standard Model (SM). Of the many potential beyond the Standard Model (BSM) topics DUNE will probe, this paper presents a selection of studies quantifying DUNE's sensitivities to sterile neutrino mixing, heavy neutral leptons, non-standard interactions, CPT symmetry violation, Lorentz invariance violation, neutrino trident production, dark matter from both beam induced and cosmogenic sources, baryon number violation, and other new physics topics that complement those at high-energy colliders and significantly extend the present reach."
JAMES A HAMILTON,Identification and reconstruction of low-energy electrons in the ProtoDUNE-SP detector,
JAMES A HAMILTON,Scintillation light detection in the 6-m drift-length ProtoDUNE Dual Phase liquid argon TPC,"DUNE is a dual-site experiment for long-baseline neutrino oscillation studies, neutrino astrophysics and nucleon decay searches. ProtoDUNE Dual Phase (DP) is a 6  ×  6  ×  6 m 3 liquid argon time-projection-chamber (LArTPC) that recorded cosmic-muon data at the CERN Neutrino Platform in 2019-2020 as a prototype of the DUNE Far Detector. Charged particles propagating through the LArTPC produce ionization and scintillation light. The scintillation light signal in these detectors can provide the trigger for non-beam events. In addition, it adds precise timing capabilities and improves the calorimetry measurements. In ProtoDUNE-DP, scintillation and electroluminescence light produced by cosmic muons in the LArTPC is collected by photomultiplier tubes placed up to 7 m away from the ionizing track. In this paper, the ProtoDUNE-DP photon detection system performance is evaluated with a particular focus on the different wavelength shifters, such as PEN and TPB, and the use of Xe-doped LAr, considering its future use in giant LArTPCs. The scintillation light production and propagation processes are analyzed and a comparison of simulation to data is performed, improving understanding of the liquid argon properties."
ELIZABETH D BARNETT,"Seroprevalence of Hepatitis E among Boston Area Travelers, 2009-2010","We determined the prevalence of IgG antibodies to hepatitis E virus (anti-HEV IgG) among travelers attending Boston-area travel health clinics from 2009 to 2010. Pre-travel samples were available for 1,356 travelers, with paired pre- and post-travel samples for 450 (33%). Eighty of 1,356 (6%) pre-travel samples were positive for anti-HEV IgG. Compared with participants who had never lived in nor traveled to a highly endemic country, the pre-travel prevalence odds ratio (POR) of anti-HEV IgG among participants born in or with a history of previous travel to a highly endemic country was increased (POR = 4.8, 95% CI = 2.3–10.3 and POR = 2.6, 95% CI = 1.4–5.0, respectively). Among participants with previous travel to a highly endemic country, anti-HEV IgG was associated with age > 40 years (POR = 3.7, 95% CI = 1.3–10.2) and travel history to ≥ 3 highly endemic countries (POR = 2.7, 95% CI = 1.2–5.9). Two participants may have contracted HEV infection during their 2009–2010 trip."
ELIZABETH D BARNETT,Travelers’ diarrhea and other gastrointestinal symptoms among Boston-area international travelers,"INTRODUCTION: Travelers' diarrhea (TD) and non-TD gastrointestinal (GI) symptoms are common among international travelers. In a study of short-term travelers from Switzerland to developing countries, the most common symptom experienced was severe diarrhea (8.5%) followed by vomiting or abdominal cramps (4%).1 GI illnesses were the most frequently reported diagnoses (34%) among ill-returned travelers to GeoSentinel clinics.2 Of those returning to U.S. GeoSentinel clinics, acute diarrhea (30%) was the most common diagnosis.3 In one cohort of U.S. travelers, 46% reported diarrhea.4 GI illnesses can last from 2 days to weeks or longer,5 disrupting plans during travel or after returning home. Eighty percent of those who experienced diarrhea during travel treated themselves with medication and 6% sought medical care. METHODS: The Boston Area Travel Medicine Network (BATMN) is a research collaboration of travel clinics in the greater Boston area representing urban-, suburban-, academic-, and university-affiliated facilities. A convenience sample of travelers ≥ 18 years of age attending three BATMN clinics between 2009 and 2011 for pre-travel consultations completed pre-travel surveys, at least one survey weekly during travel, and a post-travel survey 2–4 weeks after return. Travelers were asked to complete a survey at the end of each week of their trip. Institutional review board approvals were obtained at all sites and the Centers for Disease Control and Prevention, and participants provided written informed consent. Information collected included demographic and trip characteristics, vaccines and medications recommended/prescribed before travel, medications taken during travel, dietary practices during travel (consumption of tap water, ice in drinks, unpasteurized dairy products, and salads), symptoms experienced, and impact of illness during and after travel. Vaccinations, prescriptions, and travel health advice given during the pre-travel consultation were recorded by a clinician, and the remainder of the surveys were completed by the traveler. Data were entered into a password-protected database (CS Pro, U.S. Census Bureau, Washington, DC). RESULTS: We enrolled 987 travelers; 628 (64%) completed all three parts (pre-, during, and post-travel) and were included in the study. Comparison of the 628 to the 359 who did not complete all three parts (noncompleters) revealed no differences, except that completion rates were higher for white travelers than all other racial/ethnic groups (P < 0.001) and for older travelers (median age 47 years versus 32 years in noncompleters, P < 0.001).11 Of those 628 travelers, 208 (33%) experienced TD, 45 (7%) experienced non-TD GI symptoms, 147 (23%) experienced non-GI symptoms, and 228 (36%) did not experience any symptoms during or after travel. Of the 208 with TD, 140 (67%) reported diarrhea as their only symptom, whereas 33 (16%) also experienced nausea/vomiting, 23 (11%) abdominal pain, and 27 (13%) fever (Table 1). Of the 45 who reported non-TD GI symptoms, 21 (47%) experienced nausea/vomiting, 19 (42%) experienced constipation, and 10 (22%) experienced abdominal pain during or after travel (Table 2). Almost all travelers (99%) received advice about food and water precautions and diarrhea management during pre-travel consultation."
ELIZABETH D BARNETT,Self-reported illness among Boston-area international travelers: A prospective study,"BACKGROUND: The Boston Area Travel Medicine Network surveyed travelers on travel-related health problems. METHODS: Travelers were recruited 2009-2011 during pre-travel consultation at three clinics. The investigation included pre-travel data, weekly during-travel diaries, and a post-travel questionnaire. We analyzed demographics, trip characteristics, health problems experienced, and assessed the relationship between influenza vaccination, influenza prevention advice, and respiratory symptoms. RESULTS:Of 987 enrolled travelers, 628 (64%) completed all surveys, of which 400 (64%) reported health problems during and/or after travel; median trip duration was 12 days. Diarrhea affected the most people during travel (172) while runny/stuffy nose affected the most people after travel (95). Of those with health problems during travel, 25% stopped or altered plans; 1% were hospitalized. After travel, 21% stopped planned activities, 23% sought physician or other health advice; one traveler was hospitalized. Travelers who received influenza vaccination and influenza prevention advice had lower rates of respiratory symptoms than those that received influenza prevention advice alone (18% vs 28%, P = 0.03). CONCLUSIONS:A large proportion of Boston-area travelers reported health problems despite pre-travel consultation, resulting in inconveniences. The combination of influenza prevention advice and influenza immunization was associated with fewer respiratory symptoms than those who received influenza prevention advice alone."
ELIZABETH D BARNETT,Filariasis in Travelers Presenting to the GeoSentinel Surveillance Network,"BACKGROUND. As international travel increases, there is rising exposure to many pathogens not traditionally encountered in the resource-rich countries of the world. Filarial infections, a great problem throughout the tropics and subtropics, are relatively rare among travelers even to filaria-endemic regions of the world. The GeoSentinel Surveillance Network, a global network of medicine/travel clinics, was established in 1995 to detect morbidity trends among travelers. PRINCIPAL FINDINGS. We examined data from the GeoSentinel database to determine demographic and travel characteristics associated with filaria acquisition and to understand the differences in clinical presentation between nonendemic visitors and those born in filaria-endemic regions of the world. Filarial infections comprised 0.62% (n = 271) of all medical conditions reported to the GeoSentinel Network from travelers; 37% of patients were diagnosed with Onchocerca volvulus, 25% were infected with Loa loa, and another 25% were diagnosed with Wuchereria bancrofti. Most infections were reported from immigrants and from those immigrants returning to their county of origin (those visiting friends and relatives); the majority of filarial infections were acquired in sub-Saharan Africa. Among the patients who were natives of filaria-nonendemic regions, 70.6% acquired their filarial infection with exposure greater than 1 month. Moreover, nonendemic visitors to filaria-endemic regions were more likely to present to GeoSentinel sites with clinically symptomatic conditions compared with those who had lifelong exposure. SIGNIFICANCE. Codifying the filarial infections presenting to the GeoSentinel Surveillance Network has provided insights into the clinical differences seen among filaria-infected expatriates and those from endemic regions and demonstrated that O. volvulus infection can be acquired with short-term travel. Author Summary As international travel increases, there is rising exposure to many pathogens not traditionally encountered in the resource-rich countries of the world. The GeoSentinel Surveillance Network, a global network of medicine/travel clinics, was established in 1995 to detect morbidity trends among travelers. Filarial infections (parasitic worm infections that cause, among others, onchocerciasis [river blindness], lymphatic filariasis [e.g. elephantiasis, lymphedema, hydrocele] and loiasis [African eyeworm]) comprised 0.62% (n = 271) of the 43,722 medical conditions reported to the GeoSentinel Network between 1995 and 2004. Immigrants from filarial-endemic regions comprised the group most likely to have acquired a filarial infection; sub-Saharan Africa was the region of the world where the majority of filarial infections were acquired. Long-term travel (greater than 1 month) was more likely to be associated with acquisition of one of the filarial infections than shorter-term travel."
ALAN P MARSCHER,First M87 Event Horizon Telescope results. III. Data processing and calibration,"We present the calibration and reduction of Event Horizon Telescope (EHT) 1.3 mm radio wavelength observations of the supermassive black hole candidate at the center of the radio galaxy M87 and the quasar 3C 279, taken during the 2017 April 5–11 observing campaign. These global very long baseline interferometric observations include for the first time the highly sensitive Atacama Large Millimeter/submillimeter Array (ALMA); reaching an angular resolution of 25 μas, with characteristic sensitivity limits of ~1 mJy on baselines to ALMA and ~10 mJy on other baselines. The observations present challenges for existing data processing tools, arising from the rapid atmospheric phase fluctuations, wide recording bandwidth, and highly heterogeneous array. In response, we developed three independent pipelines for phase calibration and fringe detection, each tailored to the specific needs of the EHT. The final data products include calibrated total intensity amplitude and phase information. They are validated through a series of quality assurance tests that show consistency across pipelines and set limits on baseline systematic errors of 2% in amplitude and 1° in phase. The M87 data reveal the presence of two nulls in correlated flux density at ~3.4 and ~8.3 Gλ and temporal evolution in closure quantities, indicating intrinsic variability of compact structure on a timescale of days, or several light-crossing times for a few billion solar-mass black hole. These measurements provide the first opportunity to image horizon-scale structure in M87."
ALAN P MARSCHER,First M87 Event Horizon Telescope results. V. Physical origin of the asymmetric ring,"The Event Horizon Telescope (EHT) has mapped the central compact radio source of the elliptical galaxy M87 at 1.3 mm with unprecedented angular resolution. Here we consider the physical implications of the asymmetric ring seen in the 2017 EHT data. To this end, we construct a large library of models based on general relativistic magnetohydrodynamic (GRMHD) simulations and synthetic images produced by general relativistic ray tracing. We compare the observed visibilities with this library and confirm that the asymmetric ring is consistent with earlier predictions of strong gravitational lensing of synchrotron emission from a hot plasma orbiting near the black hole event horizon. The ring radius and ring asymmetry depend on black hole mass and spin, respectively, and both are therefore expected to be stable when observed in future EHT campaigns. Overall, the observed image is consistent with expectations for the shadow of a spinning Kerr black hole as predicted by general relativity. If the black hole spin and M87's large scale jet are aligned, then the black hole spin vector is pointed away from Earth. Models in our library of non-spinning black holes are inconsistent with the observations as they do not produce sufficiently powerful jets. At the same time, in those models that produce a sufficiently powerful jet, the latter is powered by extraction of black hole spin energy through mechanisms akin to the Blandford-Znajek process. We briefly consider alternatives to a black hole for the central compact object. Analysis of existing EHT polarization data and data taken simultaneously at other wavelengths will soon enable new tests of the GRMHD models, as will future EHT campaigns at 230 and 345 GHz."
ALAN P MARSCHER,First M87 Event Horizon Telescope results. VI. The shadow and mass of the central black hole,"We present measurements of the properties of the central radio source in M87 using Event Horizon Telescope data obtained during the 2017 campaign. We develop and fit geometric crescent models (asymmetric rings with interior brightness depressions) using two independent sampling algorithms that consider distinct representations of the visibility data. We show that the crescent family of models is statistically preferred over other comparably complex geometric models that we explore. We calibrate the geometric model parameters using general relativistic magnetohydrodynamic (GRMHD) models of the emission region and estimate physical properties of the source. We further fit images generated from GRMHD models directly to the data. We compare the derived emission region and black hole parameters from these analyses with those recovered from reconstructed images. There is a remarkable consistency among all methods and data sets. We find that >50% of the total flux at arcsecond scales comes from near the horizon, and that the emission is dramatically suppressed interior to this region by a factor >10, providing direct evidence of the predicted shadow of a black hole. Across all methods, we measure a crescent diameter of 42 ± 3 μas and constrain its fractional width to be <0.5. Associating the crescent feature with the emission surrounding the black hole shadow, we infer an angular gravitational radius of GM/Dc^2 = 3.8 ± 0.4 μas. Folding in a distance measurement of {16.8}_{-0.7}^{+0.8}{Mpc} gives a black hole mass of M = 6.5 ± 0.2{| }_{stat} ± 0.7{| }_{sys} × {10}^{9} {M}_{odot }. This measurement from lensed emission near the event horizon is consistent with the presence of a central Kerr black hole, as predicted by the general theory of relativity."
ALAN P MARSCHER,Probing the innermost regions of AGN jets and their magnetic fields with RadioAstron. III. Blazar S5 0716+71 at Microarcsecond Resolution,"We present RadioAstron Space VLBI imaging observations of the BL Lac object S5 0716+71 made on 2015 January 3–4 at a frequency of 22 GHz (wavelength λ = 1.3 cm). The observations were made in the framework of the AGN Polarization Key Science Program. The source was detected on projected space–ground baselines up to 70,833 km (5.6 Earth diameters) for both parallel-hand and cross-hand interferometric visibilities. We have used these detections to obtain a full-polarimetric image of the blazar at an unprecedented angular resolution of 24 μas, the highest for this source to date. This enabled us to estimate the size of the radio core to be <12 × 5 μas and to reveal a complex structure and a significant curvature of the blazar jet in the inner 100 μas, which is an indication that the jet viewing angle lies inside the opening angle of the jet conical outflow. Fairly highly (15%) linearly polarized emission is detected in a jet region 19 μas in size, located 58 μas downstream from the core. The highest brightness temperature in the source frame is estimated to be >2.2 × 1013 K for the blazar core. This implies that the inverse-Compton limit must be violated in the rest frame of the source, even for the largest Doppler factor δ ~ 25 reported for 0716+714."
ALAN P MARSCHER,Modeling the time-dependent polarization of blazars,"Linear polarization is an extremely valuable observational tool for probing the dynamic physical conditions of blazar jets. Some patterns are seen in the data, suggestive of order that can be explained by shock waves and helical magnetic field components. However, much disorder is apparent, which implies that turbulence plays a major role as well, especially in the fluctuations of flux and polarization, and perhaps particle acceleration. Here, we present some actual flux and polarization versus time data, plus simulations of model jets. We analyze the output of the simulations in a manner that can be compared with observational data. The results suggest that the ratio of turbulent to ordered magnetic fields varies with time."
ALAN P MARSCHER,""" Orphan"" γ-ray flares and stationary sheaths of blazar jets","Blazars exhibit flares across the entire electromagnetic spectrum. Many γ-ray flares are highly correlated with flares detected at longer wavelengths; however, a small subset appears to occur in isolation, with little or no correlated variability at longer wavelengths. These ""orphan"" γ-ray flares challenge current models of blazar variability, most of which are unable to reproduce this type of behavior. MacDonald et al. have developed the Ring of Fire model to explain the origin of orphan γ-ray flares from within blazar jets. In this model, electrons contained within a blob of plasma moving relativistically along the spine of the jet inverse-Compton scatter synchrotron photons emanating off of a ring of shocked sheath plasma that enshrouds the jet spine. As the blob propagates through the ring, the scattering of the ring photons by the blob electrons creates an orphan γ-ray flare. This model was successfully applied to modeling a prominent orphan γ-ray flare observed in the blazar PKS 1510−089. To further support the plausibility of this model, MacDonald et al. presented a stacked radio map of PKS 1510−089 containing the polarimetric signature of a sheath of plasma surrounding the spine of the jet. In this paper, we extend our modeling and stacking techniques to a larger sample of blazars: 3C 273, 4C 71.01, 3C 279, 1055+018, CTA 102, and 3C 345, the majority of which have exhibited orphan γ-ray flares. We find that the model can successfully reproduce these flares, while our stacked maps reveal the existence of jet sheaths within these blazars."
ALAN P MARSCHER,Faraday conversion in turbulent blazar jets,"Low (≲1%) levels of circular polarization (CP) detected at radio frequencies in the relativistic jets of some blazars can provide insight into the underlying nature of the jet plasma. CP can be produced through linear birefringence, in which initially linearly polarized emission produced in one region of the jet is altered by Faraday rotation as it propagates through other regions of the jet with varying magnetic field orientation. Marscher has begun a study of jets with such magnetic geometries using the turbulent extreme multi-zone (TEMZ) model, in which turbulent plasma crossing a standing shock in the jet is represented by a collection of thousands of individual plasma cells, each with distinct magnetic field orientations. Here we develop a radiative transfer scheme that allows the numerical TEMZ code to produce simulated images of the time-dependent linearly and circularly polarized intensity at different radio frequencies. In this initial study, we produce synthetic polarized emission maps that highlight the linear and circular polarization expected within the model."
ALAN P MARSCHER,High-resolution polarization imaging of the Fermi blazar 3C 279,"Ever since the discovery by the Fermi mission that active galactic nuclei (AGN) produce copious amounts of high-energy emission, its origin has remained elusive. Using high-frequency radio interferometry (VLBI) polarization imaging, we could probe the magnetic field topology of the compact high-energy emission regions in blazars. A case study for the blazar 3C 279 reveals the presence of multiple g -ray emission regions. Pass 8 Fermi-Large Area Telescope (LAT) data are used to investigate the flux variations in the GeV regime; six g -ray flares were observed in the source during November 2013 to August 2014. We use the 43 GHz VLBI data to study the morphological changes in the jet. Ejection of a new component (NC2) during the first three g -ray flares suggests the VLBI core as the possible site of the high-energy emission. A delay between the last three flares and the ejection of a new component (NC3) indicates that highenergy emission in this case is located upstream of the 43 GHz core (closer to the black hole)."
ALAN P MARSCHER,Kinematics of Parsec-scale jets of Gamma-ray blazars at 43GHz within the VLBA-BU-BLAZAR program,"We analyze the parsec-scale jet kinematics from 2007 June to 2013 January of a sample of γ-ray bright blazars monitored roughly monthly with the Very Long Baseline Array at 43 GHz. In a total of 1929 images, we measure apparent speeds of 252 emission knots in 21 quasars, 12 BL Lacertae objects (BLLacs), and 3 radio galaxies, ranging from 0.02c to 78c; 21% of the knots are quasi-stationary. Approximately one-third of the moving knots execute non-ballistic motions, with the quasars exhibiting acceleration along the jet within 5 pc (projected) of the core, and knots in BLLacs tending to decelerate near the core. Using the apparent speeds of the components and the timescales of variability from their light curves, we derive the physical parameters of 120 superluminal knots, including variability Doppler factors, Lorentz factors, and viewing angles. We estimate the half-opening angle of each jet based on the projected opening angle and scatter of intrinsic viewing angles of knots. We determine characteristic values of the physical parameters for each jet and active galactic nucleus class based on the range of values obtained for individual features. We calculate the intrinsic brightness temperatures of the cores, T core, at all b, int epochs, finding that the radio galaxies usually maintain equipartition conditions in the cores, while ∼30% of T core b, int measurements in the quasars and BLLacs deviate from equipartition values by a factor >10. This probably occurs during transient events connected with active states. In the Appendix, we briefly describe the behavior of each blazar during the period analyzed."
ALAN P MARSCHER,Implications of the VHE gamma-ray detection of the Quasar 3C279,"The MAGIC collaboration recently reported the detection of the quasar 3C279 at > 100 GeV gamma-ray energies. Here we present simultaneous optical (BVRI) and X-ray (RXTE PCA) data from the day of the VHE detection and discuss the implications of the snap-shot spectral energy distribution for jet models of blazars. A one-zone synchrotron-self-Compton origin of the entire SED, including the VHE gamma-ray emission can be ruled out. The VHE emission could, in principle, be interpreted as Compton upscattering of external radiation (e.g., from the broad-line regions). However, such an interpretation would require either an unusually low magnetic field of B ~ 0.03 G or an unrealistically high Doppler factor of Gamma ~ 140. In addition, such a model fails to reproduce the observed X-ray flux. This as well as the lack of correlated variability in the optical with the VHE gamma-ray emission and the substantial gamma-gamma opacity of the BLR radiation field to VHE gamma-rays suggests a multi-zone model. In particular, an SSC model with an emission region far outside the BLR reproduces the simultaneous X-ray -- VHE gamma-ray spectrum of 3C279. Alternatively, a hadronic model is capable of reproducing the observed SED of 3C279 reasonably well. However, the hadronic model requires a rather extreme jet power of L_j ~ 10^{49} erg s^{-1}, compared to a requirement of L_j ~ 2 X 10^{47} erg s^{-1} for a multi-zone leptonic model."
ALAN P MARSCHER,3 mm GMVA observations of total and polarized emission from blazar and radio galaxy core regions,"We present total and linearly polarized 3 mm Global mm-VLBI Array images of a sample of blazars and radio galaxies from the VLBA-BU-BLAZAR 7 mm monitoring program designed to probe the innermost regions of active galactic nuclei (AGN) jets and locate the sites of gamma-ray emission observed by the Fermi-LAT. The lower opacity at 3 mm and improved angular resolution, on the order of 50 microarcseconds, allow us to distinguish features in the jet not visible in the 7 mm VLBA data. We also compare two different methods used for the calibration of instrumental polarisation and we analyze the resulting images for some of the sources in the sample."
ALAN P MARSCHER,A multi-band study of the remarkable jet in quasar 4C+19.44,"We present arcsecond-resolution data in the radio, IR, optical, and X-ray for 4C+19.44 (=PKS 1354+195), the longest and straightest quasar jet with deep X-ray observations. We report results from radio images with half to one arcsecond angular resolution at three frequencies, plus Hubble Space Telescope and Spitzer data. The Chandra data allow us to measure the X-ray spectral index in 10 distinct regions along the 18'' jet and compare with the radio index. The radio and X-ray spectral indices of the jet regions are consistent with a value of 𝛼 =0.80 throughout the jet, to within 2σ uncertainties. The X-ray jet structure to the south extends beyond the prominent radio jet and connects to the southern radio lobe, and there is extended X-ray emission, both in the direction of the unseen counter-jet, and also coincident with the northern radio lobe. This jet is remarkable because its straight appearance over a large distance allows the geometry factors to be taken as fixed along the jet. Using the model of inverse Compton scattering of the cosmic microwave background (iC/CMB) by relativistic electrons, we find that the magnetic field strengths and Doppler factors are relatively constant along the jet. If instead the X-rays are synchrotron emission, they must arise from a population of electrons distinct from the particles producing the radio synchrotron spectrum."
ALAN P MARSCHER,Impact of ordered and disordered magnetic fields on multiwavelength emission of blazars,"We present a detailed analysis of the effects of magnetic field topology on the spectral energy distribution (SED) and spectral variability patterns (SVPs) of blazars. In order to study these effects, we have extended our time-dependent leptonic jet model (in the internal shock scenario) to include the dependence of the synchrotron emissivity on the angle between the photon direction and the magnetic field in the plasma frame. We have explored the effects of different magnetic field geometries, such as parallel, perpendicular, oblique, toroidal, and helical, on the simulated SEDs and SVPs of a generic blazar for both purely ordered and disordered components of fields. These considerations provide either upper or lower limits to the impact on blazar emission, depending on the fraction of a disordered component present and the viewing angle. The results of our work point out some of the signatures that the orientations can leave on the SEDs and SVPs of a blazar. For example, in the case of a purely oblique field, if the magnetic field is aligned along the line of sight (in the plasma frame), it results in an annulment of the synchrotron component while keeping the flux level of the high-energy component intact. On the other hand, in the presence of a disordered component, the impact of an oblique field is reduced, and the same effect is not observed."
ALAN P MARSCHER,The Event Horizon general relativistic magnetohydrodynamic code comparison project,"Recent developments in compact object astrophysics, especially the discovery of merging neutron stars by LIGO, the imaging of the black hole in M87 by the Event Horizon Telescope, and high- precision astrometry of the Galactic Center at close to the event horizon scale by the GRAVITY experiment motivate the development of numerical source models that solve the equations of general relativistic magnetohydrodynamics (GRMHD). Here we compare GRMHD solutions for the evolution of a magnetized accretion flow where turbulence is promoted by the magnetorotational instability from a set of nine GRMHD codes: Athena++, BHAC, Cosmos++, ECHO, H-AMR, iharm3D, HARM-Noble, IllinoisGRMHD, and KORAL. Agreement among the codes improves as resolution increases, as measured by a consistently applied, specially developed set of code performance metrics. We conclude that the community of GRMHD codes is mature, capable, and consistent on these test problems."
ALAN P MARSCHER,Gravitational test beyond the first post-Newtonian order with the shadow of the M87 black hole,"The 2017 Event Horizon Telescope (EHT) observations of the central source in M87 have led to the first measurement of the size of a black-hole shadow. This observation offers a new and clean gravitational test of the black-hole metric in the strong-field regime. We show analytically that spacetimes that deviate from the Kerr metric but satisfy weak-field tests can lead to large deviations in the predicted black-hole shadows that are inconsistent with even the current EHT measurements. We use numerical calculations of regular, parametric, non-Kerr metrics to identify the common characteristic among these different parametrizations that control the predicted shadow size. We show that the shadow-size measurements place significant constraints on deviation parameters that control the second post-Newtonian and higher orders of each metric and are, therefore, inaccessible to weak-field tests. The new constraints are complementary to those imposed by observations of gravitational waves from stellar-mass sources."
ALAN P MARSCHER,First M87 Event Horizon Telescope results. IV. Imaging the central supermassive black hole,
ALAN P MARSCHER,Event horizon telescope observations of the jet launching and collimation in Centaurus A,"Very-long-baseline interferometry (VLBI) observations of active galactic nuclei at millimetre wavelengths have the power to reveal the launching and initial collimation region of extragalactic radio jets, down to 10-100 gravitational radii (r_g ≡ GM/c^2) scales in nearby sources1. Centaurus A is the closest radio-loud source to Earth2. It bridges the gap in mass and accretion rate between the supermassive black holes (SMBHs) in Messier 87 and our Galactic Centre. A large southern declination of −43° has, however, prevented VLBI imaging of Centaurus A below a wavelength of 1 cm thus far. Here we show the millimetre VLBI image of the source, which we obtained with the Event Horizon Telescope at 228 GHz. Compared with previous observations3, we image the jet of Centaurus A at a tenfold higher frequency and sixteen times sharper resolution and thereby probe sub-lightday structures. We reveal a highly collimated, asymmetrically edge-brightened jet as well as the fainter counterjet. We find that the source structure of Centaurus A resembles the jet in Messier 87 on ~500 r_g scales remarkably well. Furthermore, we identify the location of Centaurus A's SMBH with respect to its resolved jet core at a wavelength of 1.3 mm and conclude that the source's event horizon shadow4 should be visible at terahertz frequencies. This location further supports the universal scale invariance of black holes over a wide range of masses^5,6."
ALAN P MARSCHER,Verification of radiative transfer schemes for the EHT,"The Event Horizon Telescope (EHT) Collaboration has recently produced the first resolved images of the central supermassive black hole in the giant elliptical galaxy M87. Here we report on tests of the consistency and accuracy of the general relativistic radiative transfer codes used within the collaboration to model M87* and Sgr A*. We compare and evaluate (1) deflection angles for equatorial null geodesics in a Kerr spacetime; (2) images calculated from a series of simple, parameterized matter distributions in the Kerr metric using simplified emissivities and absorptivities; (3) for a subset of codes, images calculated from general relativistic magnetohydrodynamics simulations using different realistic synchrotron emissivities and absorptivities; (4) observables for the 2017 configuration of EHT, including visibility amplitudes and closure phases. The error in total flux is of order 1% when the codes are run with production numerical parameters. The dominant source of discrepancies for small camera distances is the location and detailed setup of the software ""camera"" that each code uses to produce synthetic images. We find that when numerical parameters are suitably chosen and the camera is sufficiently far away the images converge and that for given transfer coefficients, numerical uncertainties are unlikely to limit parameter estimation for the current generation of EHT observations. The purpose of this paper is to describe a verification and comparison of EHT radiative transfer codes. It is not to verify EHT models more generally."
ALAN P MARSCHER,Monitoring the mmorphology of M87* in 2009–2017 with the Event Horizon Telescope,"The Event Horizon Telescope (EHT) has recently delivered the first resolved images of M87*, the supermassive black hole in the center of the M87 galaxy. These images were produced using 230 GHz observations performed in April 2017. Additional observations are required to investigate the persistence of the primary image feature – a ring with azimuthal brightness asymmetry – and to quantify the image variability on event horizon scales. To address this need, we analyze M87* data collected with prototype EHT arrays in 2009, 2011, 2012, and 2013. While these observations do not contain enough information to produce images, they are sufficient to constrain simple geometric models. We develop a modeling approach based on the framework utilized for the 2017 EHT data analysis and validate our procedures using synthetic data. Applying the same approach to the observational data sets, we find the M87* morphology in 2009–2017 to be consistent with a persistent asymmetric ring of 40 as diameter. The position angle of peak intensity varies in time. In particular, we find a significant difference between the position angle measured in 2013 and 2017. These variations are in broad agreement with predictions of a subset of general relativistic magnetohydrodynamic simulations. We show that quantifying the variability across multiple observational epochs has the potential to constrain physical properties of the source, such as the accretion state or the black hole spin."
ALAN P MARSCHER,THEMIS: a parameter estimation framework for the Event Horizon Telescope,"The Event Horizon Telescope (EHT) provides the unprecedented ability to directly resolve the structure and dynamics of black hole emission regions on scales smaller than their horizons. This has the potential to critically probe the mechanisms by which black holes accrete and launch outflows, and the structure of supermassive black hole spacetimes. However, accessing this information is a formidable analysis challenge for two reasons. First, the EHT natively produces a variety of data types that encode information about the image structure in nontrivial ways; these are subject to a variety of systematic effects associated with very long baseline interferometry and are supplemented by a wide variety of auxiliary data on the primary EHT targets from decades of other observations. Second, models of the emission regions and their interaction with the black hole are complex, highly uncertain, and computationally expensive to construct. As a result, the scientific utilization of EHT observations requires a flexible, extensible, and powerful analysis framework. We present such a framework, Themis, which defines a set of interfaces between models, data, and sampling algorithms that facilitates future development. We describe the design and currently existing components of Themis, how Themis has been validated thus far, and present additional analyses made possible by Themis that illustrate its capabilities. Importantly, we demonstrate that Themis is able to reproduce prior EHT analyses, extend these, and do so in a computationally efficient manner that can efficiently exploit modern high-performance computing facilities. Themis has already been used extensively in the scientific analysis and interpretation of the first EHT observations of M87."
ALAN P MARSCHER,First Sagittarius A* Event Horizon Telescope results. V. Testing astrophysical models of the galactic center black hole,"In this paper we provide a first physical interpretation for the Event Horizon Telescope's (EHT) 2017 observations of Sgr A*. Our main approach is to compare resolved EHT data at 230 GHz and unresolved non-EHT observations from radio to X-ray wavelengths to predictions from a library of models based on time-dependent general relativistic magnetohydrodynamics simulations, including aligned, tilted, and stellar-wind-fed simulations; radiative transfer is performed assuming both thermal and nonthermal electron distribution functions. We test the models against 11 constraints drawn from EHT 230 GHz data and observations at 86 GHz, 2.2 μm, and in the X-ray. All models fail at least one constraint. Light-curve variability provides a particularly severe constraint, failing nearly all strongly magnetized (magnetically arrested disk (MAD)) models and a large fraction of weakly magnetized models. A number of models fail only the variability constraints. We identify a promising cluster of these models, which are MAD and have inclination i ≤ 30°. They have accretion rate (5.2–9.5) × 10−9 M ⊙ yr−1, bolometric luminosity (6.8–9.2) × 1035 erg s−1, and outflow power (1.3–4.8) × 1038 erg s−1. We also find that all models with i ≥ 70° fail at least two constraints, as do all models with equal ion and electron temperature; exploratory, nonthermal model sets tend to have higher 2.2 μm flux density; and the population of cold electrons is limited by X-ray constraints due to the risk of bremsstrahlung overproduction. Finally, we discuss physical and numerical limitations of the models, highlighting the possible importance of kinetic effects and duration of the simulations."
ALAN P MARSCHER,First M87 Event Horizon Telescope results. VII. Polarization of the ring,"In 2017 April, the Event Horizon Telescope (EHT) observed the near-horizon region around the supermassive black hole at the core of the M87 galaxy. These 1.3 mm wavelength observations revealed a compact asymmetric ring-like source morphology. This structure originates from synchrotron emission produced by relativistic plasma located in the immediate vicinity of the black hole. Here we present the corresponding linear-polarimetric EHT images of the center of M87. We find that only a part of the ring is significantly polarized. The resolved fractional linear polarization has a maximum located in the southwest part of the ring, where it rises to the level of ∼15%. The polarization position angles are arranged in a nearly azimuthal pattern. We perform quantitative measurements of relevant polarimetric properties of the compact emission and find evidence for the temporal evolution of the polarized source structure over one week of EHT observations. The details of the polarimetric data reduction and calibration methodology are provided. We carry out the data analysis using multiple independent imaging and modeling techniques, each of which is validated against a suite of synthetic data sets. The gross polarimetric structure and its apparent evolution with time are insensitive to the method used to reconstruct the image. These polarimetric images carry information about the structure of the magnetic fields responsible for the synchrotron emission. Their physical interpretation is discussed in an accompanying publication."
ALAN P MARSCHER,First M87 Event Horizon Telescope results. VIII. Magnetic field structure near The Event Horizon,"Event Horizon Telescope (EHT) observations at 230 GHz have now imaged polarized emission around the supermassive black hole in M87 on event-horizon scales. This polarized synchrotron radiation probes the structure of magnetic fields and the plasma properties near the black hole. Here we compare the resolved polarization structure observed by the EHT, along with simultaneous unresolved observations with the Atacama Large Millimeter/submillimeter Array, to expectations from theoretical models. The low fractional linear polarization in the resolved image suggests that the polarization is scrambled on scales smaller than the EHT beam, which we attribute to Faraday rotation internal to the emission region. We estimate the average density n_e ∼ 10^4–7 cm^−3, magnetic field strength B ∼ 1–30 G, and electron temperature T_e ∼ (1–12) × 10^10 K of the radiating plasma in a simple one-zone emission model. We show that the net azimuthal linear polarization pattern may result from organized, poloidal magnetic fields in the emission region. In a quantitative comparison with a large library of simulated polarimetric images from general relativistic magnetohydrodynamic (GRMHD) simulations, we identify a subset of physical models that can explain critical features of the polarimetric EHT observations while producing a relativistic jet of sufficient power. The consistent GRMHD models are all of magnetically arrested accretion disks, where near-horizon magnetic fields are dynamically important. We use the models to infer a mass accretion rate onto the black hole in M87 of (3–20) × 10^−4 M ⊙ yr^−1."
ALAN P MARSCHER,Resolving the inner parsec of the blazar J1924–2914 with the event horizon telescope,"The blazar J1924–2914 is a primary Event Horizon Telescope (EHT) calibrator for the Galactic center’s black hole Sagittarius A*. Here we present the first total and linearly polarized intensity images of this source obtained with the unprecedented 20 μas resolution of the EHT. J1924–2914 is a very compact flat-spectrum radio source with strong optical variability and polarization. In April 2017 the source was observed quasi-simultaneously with the EHT (April 5–11), the Global Millimeter VLBI Array (April 3), and the Very Long Baseline Array (April 28), giving a novel view of the source at four observing frequencies, 230, 86, 8.7, and 2.3 GHz. These observations probe jet properties from the subparsec to 100 pc scales. We combine the multifrequency images of J1924–2914 to study the source morphology. We find that the jet exhibits a characteristic bending, with a gradual clockwise rotation of the jet projected position angle of about 90° between 2.3 and 230 GHz. Linearly polarized intensity images of J1924–2914 with the extremely fine resolution of the EHT provide evidence for ordered toroidal magnetic fields in the blazar compact core."
ALAN P MARSCHER,Multi-Wavelength and Multi-Messenger Studies Using the Next-Generation Event Horizon Telescope,"The next-generation Event Horizon Telescope (ngEHT) will provide us with the best opportunity to investigate supermassive black holes (SMBHs) at the highest possible resolution and sensitivity. With respect to the existing Event Horizon Telescope (EHT) array, the ngEHT will provide increased sensitivity and uv-coverage (with the addition of new stations), wider frequency coverage (from 86 GHz to 345 GHz and higher), finer resolution (&lt;15 micro-arcseconds), and better monitoring capabilities. The ngEHT will offer a unique opportunity to deeply investigate the physics around SMBHs, such as the disk-jet connection, the mechanisms responsible for high-energy photon and neutrino events, and the role of magnetic fields in shaping relativistic jets, as well as the nature of binary SMBH systems. In this white paper we describe some ngEHT science cases in the context of multi-wavelength studies and synergies."
ALAN P MARSCHER,"X-Ray, UV, and radio timing observations of the radio galaxy 3C 120","We report the results of monitoring of the radio galaxy 3C 120 with the Neil Gehrels Swift Observatory, Very Long Baseline Array, and Metsähovi Radio Observatory. The UV-optical continuum spectrum and R-band polarization can be explained by a superposition of an inverted-spectrum source with a synchrotron component containing a disordered magnetic field. The UV-optical and X-ray light curves include dips and flares, while several superluminal knots appear in the parsec-scale jet. The recovery time of the second dip was longer at UV-optical wavelengths, in conflict with a model in which the inner accretion disk (AD) is disrupted during a dip and then refilled from outer to inner radii. We favor an alternative scenario in which occasional polar alignments of the magnetic field in the disk and corona cause the flux dips and formation of shocks in the jet. Similar to observations of Seyfert galaxies, intra-band time lags of flux variations are longer than predicted by the standard AD model. This suggests that scattering or some other reprocessing occurs. The 37 GHz light curve is well-correlated with the optical-UV variations, with a ∼20 day delay. A radio flare in the jet occurred in a superluminal knot 0.14 milliarcseconds downstream of the 43 GHz ""core,"" which places the site of the preceding X-ray/UV/optical flare within the core 0.5–1.3 pc from the black hole. The inverted UV-optical flare spectrum can be explained by a nearly monoenergetic electron distribution with energy similar to the minimum energy inferred in the TeV γ-ray emitting regions of some BL Lacertae objects."
ALAN P MARSCHER,Frequency and time dependence of linear polarization in turbulent jets of blazars,"Time-variable polarization is an extremely valuable observational tool to probe the dynamical physical conditions of blazar jets. Since 2008, we have been monitoring the flux and linear polarization of a sample of gamma-ray bright blazars at optical frequencies. Some of the observations were performed on nightly or intra-night time-scales in four optical bands, providing information on the frequency and time dependence of the polarization. The observed behavior is similar to that found in simulations of turbulent plasma in a relativistic jet that contains a standing shock and/or a helical background magnetic field. Similar simulations predict the characteristics of X-ray synchrotron polarization of blazars that will be measured in the future by the Imaging X-ray Polarimetry Explorer (IXPE)."
ALAN P MARSCHER,Jet kinematics in the transversely stratified jet of 3C 84,"3C 84 (NGC 1275) is one of the brightest radio sources in the millimetre radio bands, which led to a plethora of very-long-baseline interferometry (VLBI) observations at numerous frequencies over the years. They reveal a two-sided jet structure, with an expanding but not well-collimated parsec-scale jet, pointing southward. High-resolution millimetre-VLBI observations allow the study and imaging of the jet base on a sub-parsec scale. This could facilitate the investigation of the nature of the jet origin, also in view of the previously detected two-railed jet structure and east-west oriented core region seen withRadioAstronat 22 GHz. We produced VLBI images of this core and inner jet region, observed over the past twenty years at 15, 43, and 86 GHz. We determined the kinematics of the inner jet and ejected features at 43 and 86 GHz and compared their ejection times with radio andγ-ray variability. For the moving jet features, we find an average velocity ofβappavg = 0.055−0.22c (μavg = 0.04 − 0.18 mas yr−1). From the time-averaged VLBI images at the three frequencies, we measured the transverse jet width along the bulk flow. On the ≤1.5 parsec scale, we find a clear trend of the jet width being frequency dependent, with the jet being narrower at higher frequencies. This stratification is discussed in the context of a spine-sheath scenario, and we compare it to other possible interpretations. From quasi-simultaneous observations at 43 and 86 GHz, we obtain spectral index maps, revealing a time-variable orientation of the spectral index gradient due to structural variability of the inner jet."
ALAN P MARSCHER,Multiwavelength variability power spectrum analysis of the blazars 3C 279 and PKS 1510–089 on multiple timescales,"We present the results of variability power spectral density (PSD) analysis using multiwavelength radio to GeV γ-ray light curves covering timescales of decades/years to days/minutes for the blazars 3C 279 and PKS 1510−089. The PSDs are modeled as single power laws, and the best-fit spectral shape is derived using the “power spectral response” method. With more than 10 yr of data obtained with weekly/daily sampling intervals, most of the PSDs cover ∼2–4 decades in temporal frequency; moreover, in the optical band, the PSDs cover ∼6 decades for 3C 279 due to the availability of intranight light curves. Our main results are the following: (1) on timescales ranging from decades to days, the synchrotron and the inverse-Compton spectral components, in general, exhibit red-noise (slope ∼2) and flicker-noise (slope ∼1) type variability, respectively; (2) the slopes of γ-ray variability PSDs obtained using a 3 hr integration bin and 3 weeks total duration exhibit a range between ∼1.4 and ∼2.0 (mean slope = 1.60 ± 0.70), consistent within errors with the slope on longer timescales; (3) comparisons of fractional variability indicate more power on timescales ≤100 days at γ-ray frequencies compared to longer wavelengths, in general (except between the γ-ray and optical wavelengths for PKS 1510−089); (4) the normalization of intranight optical PSDs for 3C 279 appears to be a simple extrapolation from longer timescales, indicating a continuous (single) process driving the variability at optical wavelengths; and (5) the emission at optical/infrared wavelengths may involve a combination of disk and jet processes for PKS 1510−089."
ALAN P MARSCHER,A universal power-law prescription for variability from synthetic images of black hole accretion flows,"We present a framework for characterizing the spatiotemporal power spectrum of the variability expected from the horizon-scale emission structure around supermassive black holes, and we apply this framework to a library of general relativistic magnetohydrodynamic (GRMHD) simulations and associated general relativistic ray-traced images relevant for Event Horizon Telescope (EHT) observations of Sgr A*. We find that the variability power spectrum is generically a red-noise process in both the temporal and spatial dimensions, with the peak in power occurring on the longest timescales and largest spatial scales. When both the time-averaged source structure and the spatially integrated light-curve variability are removed, the residual power spectrum exhibits a universal broken power-law behavior. On small spatial frequencies, the residual power spectrum rises as the square of the spatial frequency and is proportional to the variance in the centroid of emission. Beyond some peak in variability power, the residual power spectrum falls as that of the time-averaged source structure, which is similar across simulations; this behavior can be naturally explained if the variability arises from a multiplicative random field that has a steeper high-frequency power-law index than that of the time-averaged source structure. We briefly explore the ability of power spectral variability studies to constrain physical parameters relevant for the GRMHD simulations, which can be scaled to provide predictions for black holes in a range of systems in the optically thin regime. We present specific expectations for the behavior of the M87* and Sgr A* accretion flows as observed by the EHT."
ALAN P MARSCHER,Unraveling the innermost jet structure of OJ 287 with the first GMVA + ALMA observations,"We present the first very long baseline interferometric (VLBI) observations of the blazar OJ 287 carried out jointly with the Global Millimeter VLBI Array (GMVA) and the phased Atacama Large Millimeter/submillimeter Array (ALMA) at 3.5 mm on 2017 April 2. The participation of phased ALMA has not only improved the GMVA north–south resolution by a factor of ∼3, but has also enabled fringe detections with signal-to-noise ratios up to 300 at baselines longer than 2 Gλ. The high sensitivity has motivated us to image the data with newly developed regularized maximum likelihood imaging methods, revealing the innermost jet structure with unprecedentedly high angular resolution. Our images reveal a compact and twisted jet extending along the northwest direction, with two bends within the inner 200 μas, resembling a precessing jet in projection. The component at the southeastern end shows a compact morphology and high brightness temperature, and is identified as the VLBI core. An extended jet feature that lies at ∼200 μas northwest of the core shows a conical shape, in both total and linearly polarized intensity, and a bimodal distribution of the linear polarization electric vector position angle. We discuss the nature of this feature by comparing our observations with models and simulations of oblique and recollimation shocks with various magnetic field configurations. Our high-fidelity images also enabled us to search for possible jet features from the secondary supermassive black hole (SMBH) and test the SMBH binary hypothesis proposed for this source."
ALAN P MARSCHER,Kinematics of parsec-scale jets of gamma-ray blazars at 43 GHz during 10 yr of the VLBA-BU-BLAZAR program,"We analyze the parsec-scale jet kinematics from 2007 June to 2018 December of a sample of γ-ray bright blazars monitored roughly monthly with the Very Long Baseline Array (VLBA) at 43 GHz under the VLBA-BU-BLAZAR program. We implement a novel piecewise linear fitting method to derive the kinematics of 521 distinct emission knots from a total of 3705 total intensity images in 22 quasars, 13 BL Lacertae objects, and 3 radio galaxies. Apparent speeds of these components range from 0.01c to 78c, and 18.6% of knots (other than the “core”) are quasi-stationary. One-fifth of moving knots exhibit nonballistic motion, with acceleration along the jet within 5 pc of the core (projected) and deceleration farther out. These accelerations occur mainly at locations coincident with quasi-stationary features. We calculate the physical parameters of 273 knots with statistically significant motion, including their Doppler factors, Lorentz factors, and viewing angles. We determine the typical values of these parameters for each jet and the average for each subclass of active galactic nuclei. We investigate the variability of the position angle of each jet over the 10 yr of monitoring. The fluctuations in position of the quasi-stationary components in radio galaxies tend to be parallel to the jet, while no directional preference is seen in the components of quasars and BL Lacertae objects. We find a connection between γ-ray states of blazars and their parsec-scale jet properties, with blazars with brighter 43 GHz cores typically reaching higher γ-ray maxima during flares."
ALAN P MARSCHER,Millimeter light curves of Sagittarius A* observed during the 2017 Event Horizon Telescope campaign,"The Event Horizon Telescope (EHT) observed the compact radio source, Sagittarius A* (Sgr A*), in the Galactic Center on 2017 April 5–11 in the 1.3 mm wavelength band. At the same time, interferometric array data from the Atacama Large Millimeter/submillimeter Array and the Submillimeter Array were collected, providing Sgr A* light curves simultaneous with the EHT observations. These data sets, complementing the EHT very long baseline interferometry, are characterized by a cadence and signal-to-noise ratio previously unattainable for Sgr A* at millimeter wavelengths, and they allow for the investigation of source variability on timescales as short as a minute. While most of the light curves correspond to a low variability state of Sgr A*, the April 11 observations follow an X-ray flare and exhibit strongly enhanced variability. All of the light curves are consistent with a red-noise process, with a power spectral density (PSD) slope measured to be between −2 and −3 on timescales between 1 minute and several hours. Our results indicate a steepening of the PSD slope for timescales shorter than 0.3 hr. The spectral energy distribution is flat at 220 GHz, and there are no time lags between the 213 and 229 GHz frequency bands, suggesting low optical depth for the event horizon scale source. We characterize Sgr A*’s variability, highlighting the different behavior observed just after the X-ray flare, and use Gaussian process modeling to extract a decorrelation timescale and a PSD slope. We also investigate the systematic calibration uncertainties by analyzing data from independent data reduction pipelines."
ALAN P MARSCHER,Selective dynamical imaging of interferometric data,"Recent developments in very long baseline interferometry (VLBI) have made it possible for the Event Horizon Telescope (EHT) to resolve the innermost accretion flows of the largest supermassive black holes on the sky. The sparse nature of the EHT’s (u, v)-coverage presents a challenge when attempting to resolve highly time-variable sources. We demonstrate that the changing (u, v)-coverage of the EHT can contain regions of time over the course of a single observation that facilitate dynamical imaging. These optimal time regions typically have projected baseline distributions that are approximately angularly isotropic and radially homogeneous. We derive a metric of coverage quality based on baseline isotropy and density that is capable of ranking array configurations by their ability to produce accurate dynamical reconstructions. We compare this metric to existing metrics in the literature and investigate their utility by performing dynamical reconstructions on synthetic data from simulated EHT observations of sources with simple orbital variability. We then use these results to make recommendations for imaging the 2017 EHT Sgr A* data set."
ALAN P MARSCHER,The bright gamma-ray flare of 3C 279 in 2015 June: AGILE detection and multifrequency follow-up observations,"We report the AGILE detection and the results of the multifrequency follow-up observations of a bright γ-ray flare of the blazar 3C 279 in 2015 June. We use AGILE and Fermi gamma-ray data, together with Swift X-ray andoptical-ultraviolet data, and ground-based GASP-WEBT optical observations, including polarization information, to study the source variability and the overall spectral energy distribution during the γ-ray flare. The γ-ray flaring data, compared with as yet unpublished simultaneous optical data that will allow constraints on the big blue bump disk luminosity, show very high Compton dominance values of ~100, with the ratio of γ-ray to optical emission rising by a factor of three in a few hours. The multiwavelength behavior of the source during the flare challenges one-zone leptonic theoretical models. The new observations during the 2015 June flare are also compared with already published data and nonsimultaneous historical 3C 279 archival data."
ALAN P MARSCHER,Optical emission and particle acceleration in a quasi-stationary component in the jet of OJ 287,"We analyze the linear polarization of the relativistic jet in BL Lacertae object OJ 287 as revealed by multi-epoch Very Long Baseline Array images at 43 GHz and monitoring observations at optical bands. The electric-vector position angle of the optical polarization matches that at 43 GHz at locations that are often in the compact millimeter-wave ""core"" or, at other epochs, coincident with a bright, quasi-stationary emission feature ~0.2 mas (~0.9 pc projected on the sky) downstream from the core. This implies that electrons with high enough energies to emit optical synchrotron and γ-ray inverse Compton radiation are accelerated both in the core and at the downstream feature, the latter of which lies ≥10 pc from the central engine. The polarization vector in the stationary feature is nearly parallel to the jet axis, as expected for a conical standing shock capable of accelerating electrons to GeV energies."
ALAN P MARSCHER,First Sagittarius A* Event Horizon Telescope results. VI. Testing the black hole metric,"Astrophysical black holes are expected to be described by the Kerr metric. This is the only stationary, vacuum, axisymmetric metric, without electromagnetic charge, that satisfies Einstein’s equations and does not have pathologies outside of the event horizon. We present new constraints on potential deviations from the Kerr prediction based on 2017 EHT observations of Sagittarius A* (Sgr A*). We calibrate the relationship between the geometrically defined black hole shadow and the observed size of the ring-like images using a library that includes both Kerr and non-Kerr simulations. We use the exquisite prior constraints on the mass-to-distance ratio for Sgr A* to show that the observed image size is within ∼10% of the Kerr predictions. We use these bounds to constrain metrics that are parametrically different from Kerr, as well as the charges of several known spacetimes. To consider alternatives to the presence of an event horizon, we explore the possibility that Sgr A* is a compact object with a surface that either absorbs and thermally reemits incident radiation or partially reflects it. Using the observed image size and the broadband spectrum of Sgr A*, we conclude that a thermal surface can be ruled out and a fully reflective one is unlikely. We compare our results to the broader landscape of gravitational tests. Together with the bounds found for stellar-mass black holes and the M87 black hole, our observations provide further support that the external spacetimes of all black holes are described by the Kerr metric, independent of their mass."
ALAN P MARSCHER,Probing the innermost regions of AGN jets and their magnetic fields with RadioAstron. V. Space and ground millimeter-VLBI imaging of OJ 287,"We present the first polarimetric space very long baseline interferometry (VLBI) observations of OJ 287, observed with RadioAstron at 22 GHz during a perigee session on 2014 April 4 and five near-in-time snapshots, together with contemporaneous ground VLBI observations at 15, 43, and 86 GHz. Ground-space fringes were obtained up to a projected baseline of 3.9 Earth diameters during the perigee session, and at a record 15.1 Earth diameters during the snapshot sessions, allowing us to image the innermost jet at an angular resolution of ∼50μ as, the highest ever achieved at 22 GHz for OJ 287. Comparison with ground-based VLBI observations reveals a progressive jet bending with increasing angular resolution that agrees with predictions from a supermassive binary black hole model, although other models cannot be ruled out. Spectral analyses suggest that the VLBI core is dominated by the internal energy of the emitting particles during the onset of a multiwavelength flare, while the parsec-scale jet is consistent with being in equipartition between the particles and magnetic field. Estimated minimum brightness temperatures from the visibility amplitudes show a continued rising trend with projected baseline length up to 1013 K, reconciled with the inverse-Compton limit through Doppler boosting for a jet closely oriented to the line of sight. The observed electric vector position angle suggests that the innermost jet has a predominantly toroidal magnetic field, which, together with marginal evidence of a gradient in rotation measure across the jet width, indicates that the VLBI core is threaded by a helical magnetic field, in agreement with jet formation models."
ALAN P MARSCHER,Polarimetric properties of Event Horizon Telescope targets from ALMA,"We present the results from a full polarization study carried out with the Atacama Large Millimeter/submillimeter Array (ALMA) during the first Very Long Baseline Interferometry (VLBI) campaign, which was conducted in 2017 April in the λ3 mm and λ1.3 mm bands, in concert with the Global mm-VLBI Array (GMVA) and the Event Horizon Telescope (EHT), respectively. We determine the polarization and Faraday properties of all VLBI targets, including Sgr A*, M87, and a dozen radio-loud active galactic nuclei (AGNs), in the two bands at several epochs in a time window of 10 days. We detect high linear polarization fractions (2%–15%) and large rotation measures (RM &gt; 103.3–105.5 rad m−2), confirming the trends of previous AGN studies at millimeter wavelengths. We find that blazars are more strongly polarized than other AGNs in the sample, while exhibiting (on average) order-of-magnitude lower RM values, consistent with the AGN viewing angle unification scheme. For Sgr A* we report a mean RM of (−4.2 ± 0.3) × 105 rad m−2 at 1.3 mm, consistent with measurements over the past decade and, for the first time, an RM of (–2.1 ± 0.1) × 105 rad m−2 at 3 mm, suggesting that about half of the Faraday rotation at 1.3 mm may occur between the 3 mm photosphere and the 1.3 mm source. We also report the first unambiguous measurement of RM toward the M87 nucleus at millimeter wavelengths, which undergoes significant changes in magnitude and sign reversals on a one year timescale, spanning the range from −1.2 to 0.3 × 105 rad m−2 at 3 mm and −4.1 to 1.5 × 105 rad m−2 at 1.3 mm. Given this time variability, we argue that, unlike the case of Sgr A*, the RM in M87 does not provide an accurate estimate of the mass accretion rate onto the black hole. We put forward a two-component model, comprised of a variable compact region and a static extended region, that can simultaneously explain the polarimetric properties observed by both the EHT (on horizon scales) and ALMA (which observes the combined emission from both components). These measurements provide critical constraints for the calibration, analysis, and interpretation of simultaneously obtained VLBI data with the EHT and GMVA."
ALAN P MARSCHER,"First Sagittarius A* Event Horizon Telescope results. IV. Variability, morphology, and black hole mass","In this paper we quantify the temporal variability and image morphology of the horizon-scale emission from Sgr A*, as observed by the EHT in 2017 April at a wavelength of 1.3 mm. We find that the Sgr A* data exhibit variability that exceeds what can be explained by the uncertainties in the data or by the effects of interstellar scattering. The magnitude of this variability can be a substantial fraction of the correlated flux density, reaching ∼100% on some baselines. Through an exploration of simple geometric source models, we demonstrate that ring-like morphologies provide better fits to the Sgr A* data than do other morphologies with comparable complexity. We develop two strategies for fitting static geometric ring models to the time-variable Sgr A* data; one strategy fits models to short segments of data over which the source is static and averages these independent fits, while the other fits models to the full data set using a parametric model for the structural variability power spectrum around the average source structure. Both geometric modeling and image-domain feature extraction techniques determine the ring diameter to be 51.8 ± 2.3 μas (68% credible intervals), with the ring thickness constrained to have an FWHM between ∼30% and 50% of the ring diameter. To bring the diameter measurements to a common physical scale, we calibrate them using synthetic data generated from GRMHD simulations. This calibration constrains the angular size of the gravitational radius to be 4.8_-0.7^+1.4 μas, which we combine with an independent distance measurement from maser parallaxes to determine the mass of Sgr A* to be 4.0_-0.6^+10^6 M⊙."
ALAN P MARSCHER,The X-ray polarization view of Mrk~421 in an average flux state as observed by the Imaging X-ray Polarimetry Explorer,"Particle acceleration mechanisms in supermassive black hole jets, such as shock acceleration, magnetic reconnection, and turbulence, are expected to have observable signatures in the multi-wavelength polarization properties of blazars. The recent launch of the Imaging X-ray Polarimetry Explorer (IXPE) enables us, for the first time, to use polarization in the X-ray band (2-8 keV) to probe the properties of the jet synchrotron emission in high-frequency-peaked BL Lac objects (HSPs). We report the discovery of X-ray linear polarization (degree 𝛱_x =15% and electric-vector position angle ψ_x = 35° ± 4°) from the jet of the HSP Mrk~421 in an average X-ray flux state. At the same time, the degree of polarization at optical, infrared, and millimeter wavelengths was found to be lower by at least a factor of 3. During the IXPE pointing, the X-ray flux of the source increased by a factor of 2.2, while the polarization behavior was consistent with no variability. The higher level of 𝛱_x compared to longer wavelengths, and the absence of significant polarization variability, suggest a shock as the most likely X-ray emission site in the jet of Mrk 421 during the observation. The multiwavelength polarization properties are consistent with an energy-stratified electron population, where the particles emitting at longer wavelengths are located farther from the acceleration site, where they experience a more disordered magnetic field."
ALAN P MARSCHER,Polarization properties of the weakly magnetized neutron star X-ray binary GS 1826-238 in the high soft state,"The launch of the Imaging X-ray Polarimetry Explorer (IXPE) on 2021 December 9 has opened a new window in X-ray astronomy. We report here the results of the first IXPE observation of a weakly magnetized neutron star, GS 1826-238, performed on 2022 March 29-31 when the source was in a high soft state. An upper limit (99.73% confidence level) of 1.3% for the linear polarization degree is obtained over the IXPE 2-8 keV energy range. Coordinated INTEGRAL and NICER observations were carried out simultaneously with IXPE. The spectral parameters obtained from the fits to the broad-band spectrum were used as inputs for Monte Carlo simulations considering different possible geometries of the X-ray emitting region. Comparing the IXPE upper limit with these simulations, we can put constraints on the geometry and inclination angle of GS 1826-238."
ALAN P MARSCHER,Unraveling the complex behavior of Mrk 421 with simultaneous X-Ray and VHE observations during an extreme flaring activity in 2013 April,"We report on a multiband variability and correlation study of the TeV blazar Mrk 421 during an exceptional flaring activity observed from 2013 April 11 to 19. The study uses, among others, data from GLAST-AGILE Support Program (GASP) of the Whole Earth Blazar Telescope (WEBT), Swift, Nuclear Spectroscopic Telescope Array (NuSTAR), Fermi Large Area Telescope, Very Energetic Radiation Imaging Telescope Array System (VERITAS), and Major Atmospheric Gamma Imaging Cherenkov (MAGIC). The large blazar activity and the 43 hr of simultaneous NuSTAR and MAGIC/VERITAS observations permitted variability studies on 15 minute time bins over three X-ray bands (3–7 keV, 7–30 keV, and 30–80 keV) and three very-high-energy (VHE; >0.1 TeV) gamma-ray bands (0.2–0.4 TeV, 0.4–0.8 TeV, and >0.8 TeV). We detected substantial flux variations on multi-hour and sub-hour timescales in all of the X-ray and VHE gamma-ray bands. The characteristics of the sub-hour flux variations are essentially energy independent, while the multi-hour flux variations can have a strong dependence on the energy of the X-rays and the VHE gamma-rays. The three VHE bands and the three X-ray bands are positively correlated with no time lag, but the strength and characteristics of the correlation change substantially over time and across energy bands. Our findings favor multi-zone scenarios for explaining the achromatic/chromatic variability of the fast/slow components of the light curves, as well as the changes in the flux–flux correlation on day-long timescales. We interpret these results within a magnetic reconnection scenario, where the multi-hour flux variations are dominated by the combined emission from various plasmoids of different sizes and velocities, while the sub-hour flux variations are dominated by the emission from a single small plasmoid moving across the magnetic reconnection layer."
ALAN P MARSCHER,Multiwavelength observations of a VHE gamma-ray flare from PKS 1510-089 in 2015,"CONTEXT: PKS 1510−089 is one of only a few flat spectrum radio quasars detected in the very-high-energy (VHE, > 100 GeV) gamma-ray band. AIMS: We study the broadband spectral and temporal properties of the PKS 1510−089 emission during a high gamma-ray state. METHODS: We performed VHE gamma-ray observations of PKS 1510−089 with the Major Atmospheric Gamma Imaging Cherenkov (MAGIC) telescopes during a long, high gamma-ray state in May 2015. In order to perform broadband modeling of the source, we have also gathered contemporaneous multiwavelength data in radio, IR, optical photometry and polarization, UV, X-ray, and GeV gamma-ray ranges. We construct a broadband spectral energy distribution (SED) in two periods, selected according to VHE gamma-ray state. RESULTS: PKS 1510−089 was detected by MAGIC during a few day-long observations performed in the middle of a long, high optical and gamma-ray state, showing for the first time a significant VHE gamma-ray variability. Similarly to the optical and gamma-ray high state of the source detected in 2012, it was accompanied by a rotation of the optical polarization angle and the emission of a new jet component observed in radio. However, owing to large uncertainty on the knot separation time, the association with the VHE gamma-ray emission cannot be firmly established. The spectral shape in the VHE band during the flare is similar to those obtained during previous measurements of the source. The observed flux variability sets constraints for the first time on the size of the region from which VHE gamma rays are emitted. We model the broadband SED in the framework of the external Compton scenario and discuss the possible emission site in view of multiwavelength data and alternative emission models."
ALAN P MARSCHER,The great Markarian 421 flare of 2010 February: multiwavelength variability and correlation studies,"We report on variability and correlation studies using multiwavelength observations of the blazar Mrk 421 during the month of 2010 February, when an extraordinary flare reaching a level of ~27 Crab Units above 1 TeV was measured in very high energy (VHE) γ-rays with the Very Energetic Radiation Imaging Telescope Array System (VERITAS) observatory. This is the highest flux state for Mrk 421 ever observed in VHE γ-rays. Data are analyzed from a coordinated campaign across multiple instruments, including VHE γ-ray (VERITAS, Major Atmospheric Gamma-ray Imaging Cherenkov), high-energy γ-ray (Fermi-LAT), X-ray (Swift, Rossi X-ray Timing Experiment, MAXI), optical (including the GASP-WEBT collaboration and polarization data), and radio (Metsähovi, Owens Valley Radio Observatory, University of Michigan Radio Astronomy Observatory). Light curves are produced spanning multiple days before and after the peak of the VHE flare, including over several flare ""decline"" epochs. The main flare statistics allow 2 minute time bins to be constructed in both the VHE and optical bands enabling a cross-correlation analysis that shows evidence for an optical lag of ~25–55 minutes, the first time-lagged correlation between these bands reported on such short timescales. Limits on the Doppler factor (δ ≳ 33) and the size of the emission region (𝛿^-1R_B≲ 3.8×10^13cm) are obtained from the fast variability observed by VERITAS during the main flare. Analysis of 10 minute binned VHE and X-ray data over the decline epochs shows an extraordinary range of behavior in the flux–flux relationship, from linear to quadratic to lack of correlation to anticorrelation. Taken together, these detailed observations of an unprecedented flare seen in Mrk 421 are difficult to explain with the classic single-zone synchrotron self-Compton model."
ALAN P MARSCHER,"A fast, very-high-energy γ-ray flare from BL Lacertae during a period of multi-wavelength activity in June 2015","The mechanisms producing fast variability of the γ-ray emission in active galactic nuclei (AGNs) are under debate. The MAGIC telescopes detected a fast, very-high-energy (VHE, E  &gt;  100 GeV) γ-ray flare from BL Lacertae on 2015 June 15. The flare had a maximum flux of (1.5 ± 0.3) × 10−10 photons cm−2 s−1 and halving time of 26 ± 8 min. The MAGIC observations were triggered by a high state in the optical and high-energy (HE, E  &gt;  100 MeV) γ-ray bands. In this paper we present the MAGIC VHE γ-ray data together with multi-wavelength data from radio, optical, X-rays, and HE γ rays from 2015 May 1 to July 31. Well-sampled multi-wavelength data allow us to study the variability in detail and compare it to the other epochs when fast, VHE γ-ray flares have been detected from this source. Interestingly, we find that the behaviour in radio, optical, X-rays, and HE γ-rays is very similar to two other observed VHE γ-ray flares. In particular, also during this flare there was an indication of rotation of the optical polarization angle and of activity at the 43 GHz core. These repeating patterns indicate a connection between the three events. We also test modelling of the spectral energy distribution based on constraints from the light curves and VLBA observations, with two different geometrical setups of two-zone inverse Compton models. In addition we model the γ-ray data with the star-jet interaction model. We find that all of the tested emission models are compatible with the fast VHE γ-ray flare, but all have some tension with the multi-wavelength observations."
ALAN P MARSCHER,The X-ray polarization view of Mrk 421 in an average flux state as observed by the imaging X-ray polarimetry explorer,"Particle acceleration mechanisms in supermassive black hole jets, such as shock acceleration, magnetic reconnection, and turbulence, are expected to have observable signatures in the multiwavelength polarization properties of blazars. The recent launch of the Imaging X-Ray Polarimetry Explorer (IXPE) enables us, for the first time, to use polarization in the X-ray band (2–8 keV) to probe the properties of the jet synchrotron emission in high-synchrotron-peaked BL Lac objects (HSPs). We report the discovery of X-ray linear polarization (degree Πx = 15% ± 2% and electric vector position angle ψ x = 35° ± 4°) from the jet of the HSP Mrk 421 in an average X-ray flux state. At the same time, the degree of polarization at optical, infrared, and millimeter wavelengths was found to be lower by at least a factor of 3. During the IXPE pointing, the X-ray flux of the source increased by a factor of 2.2, while the polarization behavior was consistent with no variability. The higher level of Πx compared to longer wavelengths, and the absence of significant polarization variability, suggest a shock is the most likely X-ray emission site in the jet of Mrk 421 during the observation. The multiwavelength polarization properties are consistent with an energy-stratified electron population, where the particles emitting at longer wavelengths are located farther from the acceleration site, where they experience a more disordered magnetic field."
ALAN P MARSCHER,Polarized x-rays from a magnetar,"Magnetars are neutron stars with ultrastrong magnetic fields, which can be observed in x-rays. Polarization measurements could provide information on their magnetic fields and surface properties. We observed polarized x-rays from the magnetar 4U 0142+61 using the Imaging X-ray Polarimetry Explorer and found a linear polarization degree of 13.5 ± 0.8% averaged over the 2- to 8-kilo-electron volt band. The polarization changes with energy: The degree is 15.0 ± 1.0% at 2 to 4 kilo-electron volts, drops below the instrumental sensitivity ~4 to 5 kilo-electron volts, and rises to 35.2 ± 7.1% at 5.5 to 8 kilo-electron volts. The polarization angle also changes by 90° at ~4 to 5 kilo-electron volts. These results are consistent with a model in which thermal radiation from the magnetar surface is reprocessed by scattering off charged particles in the magnetosphere."
ALAN P MARSCHER,Determination of X-ray pulsar geometry with IXPE polarimetry,
ALAN P MARSCHER,Vela pulsar wind nebula X-rays are polarized to near the synchrotron limit,"Pulsar wind nebulae are formed when outflows of relativistic electrons and positrons hit the surrounding supernova remnant or interstellar medium at a shock front. The Vela pulsar wind nebula is powered by a young pulsar (B0833-45, aged 11,000 years)1 and located inside an extended structure called Vela X, which is itself inside the supernova remnant2. Previous X-ray observations revealed two prominent arcs that are bisected by a jet and counter jet3,4. Radio maps have shown high linear polarization of 60% in the outer regions of the nebula5. Here we report an X-ray observation of the inner part of the nebula, where polarization can exceed 60% at the leading edge-approaching the theoretical limit of what can be produced by synchrotron emission. We infer that, in contrast with the case of the supernova remnant, the electrons in the pulsar wind nebula are accelerated with little or no turbulence in a highly uniform magnetic field."
ALAN P MARSCHER,X-Ray polarization detection of Cassiopeia A with IXPE,"We report on a ∼5σ detection of polarized 3–6 keV X-ray emission from the supernova remnant Cassiopeia A (Cas A) with the Imaging X-ray Polarimetry Explorer (IXPE). The overall polarization degree of 1.8% ± 0.3% is detected by summing over a large region, assuming circular symmetry for the polarization vectors. The measurements imply an average polarization degree for the synchrotron component of ∼2.5%, and close to 5% for the X-ray synchrotron-dominated forward shock region. These numbers are based on an assessment of the thermal and nonthermal radiation contributions, for which we used a detailed spatial-spectral model based on Chandra X-ray data. A pixel-by-pixel search for polarization provides a few tentative detections from discrete regions at the ∼ 3σ confidence level. Given the number of pixels, the significance is insufficient to claim a detection for individual pixels, but implies considerable turbulence on scales smaller than the angular resolution. Cas A’s X-ray continuum emission is dominated by synchrotron radiation from regions within ≲1017 cm of the forward and reverse shocks. We find that (i) the measured polarization angle corresponds to a radially oriented magnetic field, similar to what has been inferred from radio observations; (ii) the X-ray polarization degree is lower than in the radio band (∼5%). Since shock compression should impose a tangential magnetic-field structure, the IXPE results imply that magnetic fields are reoriented within ∼1017 cm of the shock. If the magnetic-field alignment is due to locally enhanced acceleration near quasi-parallel shocks, the preferred X-ray polarization angle suggests a size of 3 × 1016 cm for cells with radial magnetic fields."
ALAN P MARSCHER,Observations of 4U 1626–67 with the imaging X-ray polarimetry explorer,"We present measurements of the polarization of X-rays in the 2–8 keV band from the pulsar in the ultracompact low-mass X-ray binary 4U 1626–67 using data from the Imaging X-Ray Polarimetry Explorer (IXPE). The 7.66 s pulsations were clearly detected throughout the IXPE observations as well as in the NICER soft X-ray observations, which we used as the basis for our timing analysis and to constrain the spectral shape over the 0.4–10 keV energy band. Chandra HETGS high-resolution X-ray spectra were also obtained near the times of the IXPE observations for firm spectral modeling. We found an upper limit on the pulse-averaged linear polarization of &lt;4% (at 95% confidence). Similarly, there was no significant detection of polarized flux in pulse phase intervals when subdividing the bandpass by energy. However, spectropolarimetric modeling over the full bandpass in pulse phase intervals provided a marginal detection of polarization of the power-law spectral component at the 4.8% ± 2.3% level (90% confidence). We discuss the implications concerning the accretion geometry onto the pulsar, favoring two-component models of the pulsed emission."
ALAN P MARSCHER,The X-ray polarimetry view of the accreting pulsar Cen X-3,Cen X-3 is the first X-ray pulsar discovered 50 years ago. Radiation from such objects is expected to be highly polarized due to birefringence of plasma and vacuum associated with propagation of photons in presence of the strong magnetic field. Here we present results of the observations of Cen X-3 performed with the Imaging X-ray Polarimetry Explorer. The source exhibited significant flux variability and was observed in two states different by a factor of ~20 in flux. In the low-luminosity state no significant polarization was found either in pulse phase-averaged (with the 3𝜎 upper limit of 12%) or phase-resolved data (the 3𝜎 upper limits are 20-30%). In the bright state the polarization degree of 5.8％ ± 0.3% and polarization angle of $49.°6 ±1.°5 with significance of about 20σ was measured from the spectro-polarimetric analysis of the phase-averaged data. The phase-resolved analysis showed a significant anti-correlation between the flux and the polarization degree as well as strong variations of the polarization angle. The fit with the rotating vector model indicates a position angle of the pulsar spin axis of about 49° and a magnetic obliquity of 17°. The detected relatively low polarization can be explained if the upper layers of the neutron star surface are overheated by the accreted matter and the conversion of the polarization modes occurs within the transition region between the upper hot layer and a cooler underlying atmosphere. A fraction of polarization signal can also be produced by reflection of radiation from the neutron star surface and the accretion curtain.
ALAN P MARSCHER,Limits on X-ray polarization at the core of Centaurus A as observed with the imaging X-Ray polarimetry explorer,"We present measurements of the polarization of X-rays in the 2–8 keV band from the nucleus of the radio galaxy Centaurus A (Cen A), using a 100 ks observation from the Imaging X-ray Polarimetry Explorer (IXPE). Nearly simultaneous observations of Cen A were also taken with the Swift, NuSTAR, and INTEGRAL observatories. No statistically significant degree of polarization is detected with IXPE. These observations have a minimum detectable polarization at 99% confidence (MDP99) of 6.5% using a weighted, spectral model-independent calculation in the 2–8 keV band. The polarization angle ψ is consequently unconstrained. Spectral fitting across three orders of magnitude in X-ray energy (0.3–400 keV) demonstrates that the SED of Cen A is well described by a simple power law with moderate intrinsic absorption (N H ∼ 1023 cm−2) and a Fe Kα emission line, although a second unabsorbed power law is required to account for the observed spectrum at energies below 2 keV. This spectrum suggests that the reprocessing material responsible for this emission line is optically thin and distant from the central black hole. Our upper limits on the X-ray polarization are consistent with the predictions of Compton scattering, although the specific seed photon population responsible for the production of the X-rays cannot be identified. The low polarization degree, variability in the core emission, and the relative lack of variability in the Fe Kα emission line support a picture where electrons are accelerated in a region of highly disordered magnetic fields surrounding the innermost jet."
ALAN P MARSCHER,Multiwavelength variability and correlation studies of Mrk 421 during historically low X-ray and γ-ray activity in 2015–2016,"We report a characterization of the multi-band flux variability and correlations of the nearby (z=0.031) blazar Markarian 421 (Mrk 421) using data from Metsähovi, Swift, Fermi-LAT, MAGIC, FACT and other collaborations and instruments from November 2014 till June 2016. Mrk 421 did not show any prominent flaring activity, but exhibited periods of historically low activity above 1 TeV (F_> 1TeV < 1.7× 10^−12 ph cm^−2 s^−1) and in the 2-10 keV (X-ray) band (F_2 − 10 keV < 3.6 × 10^−11 erg cm^−2 s^−1), during which the Swift-BAT data suggests an additional spectral component beyond the regular synchrotron emission.The highest flux variability occurs in X-rays and very-high-energy (E > 0.1 TeV) γ-rays, which, despite the low activity, show a significant positive correlation with no time lag. The HRkeV and HRTeV show the harder-when-brighter trend observed in many blazars, but the trend flattens at the highest fluxes, which suggests a change in the processes dominating the blazar variability. Enlarging our data set with data from years 2007 to 2014, we measured a positive correlation between the optical and the GeV emission over a range of about 60 days centered at time lag zero, and a positive correlation between the optical/GeV and the radio emission over a range of about 60 days centered at a time lag of 43^+9_-6 days.This observation is consistent with the radio-bright zone being located about 0.2 parsec downstream from the optical/GeV emission regions of the jet. The flux distributions are better described with a LogNormal function in most of the energy bands probed, indicating that the variability in Mrk 421 is likely produced by a multiplicative process."
ALAN P MARSCHER,"First Sagittarius A* Event Horizon Telescope results. II. EHT and multiwavelength observations, data processing, and calibration","We present Event Horizon Telescope (EHT) 1.3 mm measurements of the radio source located at the position of the supermassive black hole Sagittarius A* (Sgr A*), collected during the 2017 April 5–11 campaign. The observations were carried out with eight facilities at six locations across the globe. Novel calibration methods are employed to account for Sgr A*'s flux variability. The majority of the 1.3 mm emission arises from horizon scales, where intrinsic structural source variability is detected on timescales of minutes to hours. The effects of interstellar scattering on the image and its variability are found to be subdominant to intrinsic source structure. The calibrated visibility amplitudes, particularly the locations of the visibility minima, are broadly consistent with a blurred ring with a diameter of ∼50 μas, as determined in later works in this series. Contemporaneous multiwavelength monitoring of Sgr A* was performed at 22, 43, and 86 GHz and at near-infrared and X-ray wavelengths. Several X-ray flares from Sgr A* are detected by Chandra, one at low significance jointly with Swift on 2017 April 7 and the other at higher significance jointly with NuSTAR on 2017 April 11. The brighter April 11 flare is not observed simultaneously by the EHT but is followed by a significant increase in millimeter flux variability immediately after the X-ray outburst, indicating a likely connection in the emission physics near the event horizon. We compare Sgr A*’s broadband flux during the EHT campaign to its historical spectral energy distribution and find that both the quiescent emission and flare emission are consistent with its long-term behavior."
ALAN P MARSCHER,Exploring the connection between Parsec-scale jet activity and broadband outbursts in 3C 279,"We use a combination of high-resolution very long baseline interferometry (VLBI) radio and multiwavelength flux density and polarization observations to constrain the physics of the dissipation mechanism powering the broadband flares in 3C 279 during an episode of extreme flaring activity in 2013–2014. Six bright flares superimposed on a long-term outburst are detected at γ-ray energies. Four of the flares have optical and radio counterparts. The two modes of flaring activity (faster flares sitting on top of a long-term outburst) present at radio, optical, and γ-ray frequencies are missing in X-rays. X-ray counterparts are only observed for two flares. The first three flares are accompanied by ejection of a new VLBI component (NC2), suggesting the 43 GHz VLBI core as the site of energy dissipation. Another new component, NC3, is ejected after the last three flares, which suggests that the emission is produced upstream from the core (closer to the black hole). The study therefore indicates multiple sites of energy dissipation in the source. An anticorrelation is detected between the optical percentage polarization (PP) and optical/γ-ray flux variations, while the PP has a positive correlation with optical/γ-ray spectral indices. Given that the mean polarization is inversely proportional to the number of cells in the emission region, the PP versus optical/γ-ray anticorrelation could be due to more active cells during the outburst than at other times. In addition to the turbulent component, our analysis suggests the presence of a combined turbulent and ordered magnetic field, with the ordered component transverse to the jet axis."
ALAN P MARSCHER,The complex variability of blazars: time-scales and periodicity analysis in S4 0954+65,"Among active galactic nuclei, blazars show extreme variability properties. We here investigate the case of the BL Lac object S4 0954+65 with data acquired in 2019–2020 by the Transiting Exoplanet Survey Satellite (TESS) and by the Whole Earth Blazar Telescope (WEBT) Collaboration. The 2-min cadence optical light curves provided by TESS during three observing sectors of nearly 1 month each allow us to study the fast variability in great detail. We identify several characteristic short-term time-scales, ranging from a few hours to a few days. However, these are not persistent, as they differ in the various TESS sectors. The long-term photometric and polarimetric optical and radio monitoring undertaken by the WEBT brings significant additional information, revealing that (i) in the optical, long-term flux changes are almost achromatic, while the short-term ones are strongly chromatic; (ii) the radio flux variations at 37 GHz follow those in the optical with a delay of about 3 weeks; (iii) the range of variation of the polarization degree and angle is much larger in the optical than in the radio band, but the mean polarization angles are similar; (iv) the optical long-term variability is characterized by a quasi-periodicity of about 1 month. We explain the source behaviour in terms of a rotating inhomogeneous helical jet, whose pitch angle can change in time."
ALAN P MARSCHER,The magnetic field structure in CTA 102 from high-resolution mm-VLBI observations during the flaring state in 2016-2017,"CONTEXT: Investigating the magnetic field structure in the innermost regions of relativistic jets is fundamental to understanding the crucial physical processes giving rise to jet formation, as well as to their extraordinary radiation output up to γ-ray energies. AIMS: We study the magnetic field structure of the quasar CTA 102 with 3 and 7 mm VLBI polarimetric observations, reaching an unprecedented resolution (∼50 μas). We also investigate the variability and physical processes occurring in the source during the observing period, which coincides with a very active state of the source over the entire electromagnetic spectrum. METHODS: We perform the Faraday rotation analysis using 3 and 7 mm data and we compare the obtained rotation measure (RM) map with the polarization evolution in 7 mm VLBA images. We study the kinematics and variability at 7 mm and infer the physical parameters associated with variability. From the analysis of γ-ray and X-ray data, we compute a minimum Doppler factor value required to explain the observed high-energy emission. RESULTS: Faraday rotation analysis shows a gradient in RM with a maximum value of ∼6 × 104⁴ rad m⁻² and intrinsic electric vector position angles (EVPAs) oriented around the centroid of the core, suggesting the presence of large-scale helical magnetic fields. Such a magnetic field structure is also visible in 7 mm images when a new superluminal component is crossing the core region. The 7 mm EVPA orientation is different when the component is exiting the core or crossing a stationary feature at ∼0.1 mas. The interaction between the superluminal component and a recollimation shock at ∼0.1 mas could have triggered the multi-wavelength flares. The variability Doppler factor associated with such an interaction is large enough to explain the high-energy emission and the remarkable optical flare occurred very close in time."
ALAN P MARSCHER,The jet collimation profile at high resolution in BL Lacertae,"CONTEXT: Controversial studies on the jet collimation profile of BL Lacertae (BL Lac), the eponymous blazar of the BL Lac objects class, complicate the scenario in this already puzzling class of objects. Understanding the jet geometry in connection with the jet kinematics and the physical conditions in the surrounding medium is fundamental for better constraining the formation, acceleration, and collimation mechanisms in extragalactic jets. AIMS: With the aim of investigating the jet geometry in the innermost regions of the BL Lac jet, and resolving the controversy, we explore the radio jet in this source using high-resolution millimeter-wave VLBI data. METHODS: We collect 86 GHz GMVA and 43 GHz VLBA data to obtain stacked images that we use to infer the jet collimation profile by means of two comparable methods. We analyze the kinematics at 86 GHz, and we discuss it in the context of the jet expansion. Finally, we consider a possible implication of the Bondi sphere in shaping the jet of BL Lac. RESULTS: The jet in BL Lac expands with an overall conical geometry. A higher expanding rate region is observed between ∼5 and 10 pc (de-projected) from the black hole. Such a region is associated with the decrease in brightness usually observed in high-frequency VLBI images of BL Lac. The jet retrieves the original jet expansion around 17 pc, where the presence of a recollimation shock is supported by both the jet profile and the 15 GHz kinematics (MOJAVE survey). The change in the jet expansion profile occurring at ∼5 pc could be associated with a change in the external pressure at the location of the Bondi radius (∼3.3 × 10^5R_S)."
ALAN P MARSCHER,Multiwavelength behaviour of the blazar 3C 279: decade-long study from γ-ray to radio,"We report the results of decade-long (2008–2018) γ-ray to 1 GHz radio monitoring of the blazar 3C 279, including GASP/WEBT, Fermi and Swift data, as well as polarimetric and spectroscopic data. The X-ray and γ-ray light curves correlate well, with no delay ≳ 3 h, implying general cospatiality of the emission regions. The γ-ray–optical flux–flux relation changes with activity state, ranging from a linear to a more complex dependence. The behaviour of the Stokes parameters at optical and radio wavelengths, including 43 GHz Very Long Baseline Array images, supports either a predominantly helical magnetic field or motion of the radiating plasma along a spiral path. Apparent speeds of emission knots range from 10 to 37c, with the highest values requiring bulk Lorentz factors close to those needed to explain γ-ray variability on very short time-scales. The Mg ii emission line flux in the ‘blue’ and ‘red’ wings correlates with the optical synchrotron continuum flux density, possibly providing a variable source of seed photons for inverse Compton scattering. In the radio bands, we find progressive delays of the most prominent light-curve maxima with decreasing frequency, as expected from the frequency dependence of the τ = 1 surface of synchrotron self-absorption. The global maximum in the 86 GHz light curve becomes less prominent at lower frequencies, while a local maximum, appearing in 2014, strengthens toward decreasing frequencies, becoming pronounced at ∼5 GHz. These tendencies suggest different Doppler boosting of stratified radio-emitting zones in the jet."
ALAN P MARSCHER,Investigating the multiwavelength behaviour of the flat spectrum radio quasar CTA 102 during 2013–2017,"We present a multiwavelength study of the flat-spectrum radio quasar CTA 102 during 2013–2017. We use radio-to-optical data obtained by the Whole Earth Blazar Telescope, 15 GHz data from the Owens Valley Radio Observatory, 91 and 103 GHz data from the Atacama Large Millimeter Array, near-infrared data from the Rapid Eye Monitor telescope, as well as data from the Swift (optical-UV and X-rays) and Fermi (γ-rays) satellites to study flux and spectral variability and the correlation between flux changes at different wavelengths. Unprecedented γ-ray flaring activity was observed during 2016 November–2017 February, with four major outbursts. A peak flux of (2158 ± 63) × 10−8 ph cm−2 s−1, corresponding to a luminosity of (2.2 ± 0.1) × 1050 erg s−1, was reached on 2016 December 28. These four γ-ray outbursts have corresponding events in the near-infrared, optical, and UV bands, with the peaks observed at the same time. A general agreement between X-ray and γ-ray activity is found. The γ-ray flux variations show a general, strong correlation with the optical ones with no time lag between the two bands and a comparable variability amplitude. This γ-ray/optical relationship is in agreement with the geometrical model that has successfully explained the low-energy flux and spectral behaviour, suggesting that the long-term flux variations are mainly due to changes in the Doppler factor produced by variations of the viewing angle of the emitting regions. The difference in behaviour between radio and higher energy emission would be ascribed to different viewing angles of the jet regions producing their emission."
ALAN P MARSCHER,The beamed jet and quasar core of the distant blazar 4C 71.07,"The object 4C 71.07 is a high-redshift blazar whose spectral energy distribution shows a prominent big blue bump and a strong Compton dominance. We present the results of a 2-yr multiwavelength campaign led by the Whole Earth Blazar Telescope (WEBT) to study both the quasar core and the beamed jet of this source. The WEBT data are complemented by ultraviolet and X-ray data from Swift, and by γ-ray data by Fermi. The big blue bump is modelled by using optical and near-infrared mean spectra obtained during the campaign, together with optical and ultraviolet quasar templates. We give prescriptions to correct the source photometry in the various bands for the thermal contribution, in order to derive the non-thermal jet flux. The role of the intergalactic medium absorption is analysed in both the ultraviolet and X-ray bands. We provide opacity values to deabsorb ultraviolet data, and derive a best-guess value for the hydrogen column density of N_{ H}^{best}=6.3 × 10^{20} cm^{-2} through the analysis of X-ray spectra. We estimate the disc and jet bolometric luminosities, accretion rate, and black hole mass. Light curves do not show persistent correlations among flux changes at different frequencies. We study the polarimetric behaviour and find no correlation between polarization degree and flux, even when correcting for the dilution effect of the big blue bump. Similarly, wide rotations of the electric vector polarization angle do not seem to be connected with the source activity."
ALAN P MARSCHER,The detection of the blazar S4 0954+65 at very-high-energy with the MAGIC telescopes during an exceptionally high optical state,"The very-high-energy (VHE, ≳ 100 GeV) γ-ray MAGIC observations of the blazar S4 0954+65, were triggered by an exceptionally high flux state of emission in the optical. This blazar has a disputed redshift of z=0.368 or z ≥ 0.45 and an uncertain classification among blazar subclasses. The exceptional source state described here makes for an excellent opportunity to understand physical processes in the jet of S4 0954+65 and thus contribute to its classification. We investigate the multiwavelength (MWL) light curve and spectral energy distribution (SED) of the S4 0954+65 blazar during an enhanced state in February 2015 and put it in context with possible emission scenarios. We collect photometric data in radio, optical, X-ray, and γ-ray. We study both the optical polarization and the inner parsec-scale jet behavior with 43 GHz data. Observations with the MAGIC telescopes led to the first detection of S4 0954+65 at VHE. Simultaneous data with Fermi-LAT at high energy γ-ray (HE, 100 MeV < E < 100 GeV) also show a period of increased activity. Imaging at 43 GHz reveals the emergence of a new feature in the radio jet in coincidence with the VHE flare. Simultaneous monitoring of the optical polarization angle reveals a rotation of approximately 100°. The broadband spectrum can be modeled with an emission mechanism commonly invoked for flat spectrum radio quasars, i.e. inverse Compton scattering on an external soft photon field from the dust torus, also known as external Compton. The light curve and SED phenomenology is consistent with an interpretation of a blob propagating through a helical structured magnetic field and eventually crossing a standing shock in the jet, a scenario typically applied to flat spectrum radio quasars (FSRQs) and low-frequency peaked BL Lac objects (LBL)."
ALAN P MARSCHER,Investigation of the correlation patterns and the Compton dominance variability of Mrk 421 in 2017,"Aims. We present a detailed characterisation and theoretical interpretation of the broadband emission of the paradigmatic TeV blazar Mrk 421, with a special focus on the multi-band flux correlations. Methods. The dataset has been collected through an extensive multi-wavelength campaign organised between 2016 December and 2017 June. The instruments involved are MAGIC, FACT, Fermi-LAT, Swift, GASP-WEBT, OVRO, Medicina, and Metsähovi. Additionally, four deep exposures (several hours long) with simultaneous MAGIC and NuSTAR observations allowed a precise measurement of the falling segments of the two spectral components. Results. The very-high-energy (VHE; E >  100 GeV) gamma rays and X-rays are positively correlated at zero time lag, but the strength and characteristics of the correlation change substantially across the various energy bands probed. The VHE versus X-ray fluxes follow different patterns, partly due to substantial changes in the Compton dominance for a few days without a simultaneous increase in the X-ray flux (i.e., orphan gamma-ray activity). Studying the broadband spectral energy distribution (SED) during the days including NuSTAR observations, we show that these changes can be explained within a one-zone leptonic model with a blob that increases its size over time. The peak frequency of the synchrotron bump varies by two orders of magnitude throughout the campaign. Our multi-band correlation study also hints at an anti-correlation between UV-optical and X-ray at a significance higher than 3σ. A VHE flare observed on MJD 57788 (2017 February 4) shows gamma-ray variability on multi-hour timescales, with a factor ten increase in the TeV flux but only a moderate increase in the keV flux. The related broadband SED is better described by a two-zone leptonic scenario rather than by a one-zone scenario. We find that the flare can be produced by the appearance of a compact second blob populated by high energetic electrons spanning a narrow range of Lorentz factors, from γ′min=2×104 to γ′max=6×105."
ALAN P MARSCHER,The polarized image of a synchrotron-emitting ring of gas orbiting a black hole,"Synchrotron radiation from hot gas near a black hole results in a polarized image. The image polarization is determined by effects including the orientation of the magnetic field in the emitting region, relativistic motion of the gas, strong gravitational lensing by the black hole, and parallel transport in the curved spacetime. We explore these effects using a simple model of an axisymmetric, equatorial accretion disk around a Schwarzschild black hole. By using an approximate expression for the null geodesics derived by Beloborodov and conservation of the Walker–Penrose constant, we provide analytic estimates for the image polarization. We test this model using currently favored general relativistic magnetohydrodynamic simulations of M87*, using ring parameters given by the simulations. For a subset of these with modest Faraday effects, we show that the ring model broadly reproduces the polarimetric image morphology. Our model also predicts the polarization evolution for compact flaring regions, such as those observed from Sgr A* with GRAVITY. With suitably chosen parameters, our simple model can reproduce the EVPA pattern and relative polarized intensity in Event Horizon Telescope images of M87*. Under the physically motivated assumption that the magnetic field trails the fluid velocity, this comparison is consistent with the clockwise rotation inferred from total intensity images."
ALAN P MARSCHER,Observation of a sudden cessation of a very-high-energy gamma-ray flare in PKS 1510-089 with H.E.S.S. and MAGIC in May 2016,"The flat spectrum radio quasar (FSRQ) PKS 1510-089 is known for its complex multiwavelength behavior, and is one of only a few FSRQs detected at very high energy (VHE, E >100 GeV) 𝛾-rays. VHE 𝛾-ray observations with H.E.S.S. and MAGIC during late May and early June 2016 resulted in the detection of an unprecedented flare, which reveals for the first time VHE 𝛾-ray intranight variability in this source. While a common variability timescale of 1.5 hr is found, there is a significant deviation near the end of the flare with a timescale of ∼ 20 min marking the cessation of the event. The peak flux is nearly two orders of magnitude above the low-level emission. For the first time, curvature is detected in the VHE 𝛾-ray spectrum of PKS 1510-089, which is fully explained through absorption by the extragalactic background light. Optical R-band observations with ATOM reveal a counterpart of the 𝛾-ray flare, even though the detailed flux evolution differs from the VHE lightcurve. Interestingly, a steep flux decrease is observed at the same time as the cessation of the VHE flare. In the high energy (HE, E >100 MeV) 𝛾-ray band only a moderate flux increase is observed with Fermi-LAT, while the HE 𝛾-ray spectrum significantly hardens up to a photon index of 1.6. A search for broad-line region (BLR) absorption features in the 𝛾-ray spectrum indicates that the emission region is located outside of the BLR. Radio VLBI observations reveal a fast moving knot interacting with a standing jet feature around the time of the flare. As the standing feature is located ∼ 50 pc from the black hole, the emission region of the flare may have been located at a significant distance from the black hole. If this correlation is indeed true, VHE 𝛾 rays have been produced far down the jet where turbulent plasma crosses a standing shock."
ALAN P MARSCHER,Multi-wavelength characterization of the blazar S5 0716+714 during an unprecedented outburst phase,"CONTEXT: The BL Lac object S5 0716+714, a highly variable blazar, underwent an impressive outburst in January 2015 (Phase A), followed by minor activity in February (Phase B). The MAGIC observations were triggered by the optical flux observed in Phase A, corresponding to the brightest ever reported state of the source in the R-band. AIMS: The comprehensive dataset collected is investigated in order to shed light on the mechanism of the broadband emission. METHODS: Multi-wavelength light curves have been studied together with the broadband spectral energy distributions (SEDs). The sample includes data from Effelsberg, OVRO, Metsähovi, VLBI, CARMA, IRAM, SMA, Swift-UVOT, KVA, Tuorla, Steward, RINGO3, KANATA, AZT-8+ST7, Perkins, LX-200, Swift-XRT, NuSTAR, Fermi-LAT and MAGIC. RESULTS: The flaring state of Phase A was detected in all the energy bands, providing for the first time a multi-wavelength sample of simultaneous data from the radio band to the very-high-energy (VHE, E > 100 GeV). In the constructed SED, the Swift-XRT+NuSTAR data constrain the transition between the synchrotron and inverse Compton components very accurately, while the second peak is constrained from 0.1 GeV to 600 GeV by Fermi+MAGIC data. The broadband SED cannot be described with a one-zone synchrotron self-Compton model as it severely underestimates the optical flux in order to reproduce the X-ray to γ-ray data. Instead we use a two-zone model. The electric vector position angle (EVPA) shows an unprecedented fast rotation. An estimation of the redshift of the source by combined high-energy (HE, 0.1 GeV < E < 100 GeV) and VHE data provides a value of z = 0.31 ± 0.02stats ± 0.05sys, confirming the literature value. CONCLUSIONS: The data show the VHE emission originating in the entrance and exit of a superluminal knot in and out of a recollimation shock in the inner jet. A shock–shock interaction in the jet seems responsible for the observed flares and EVPA swing. This scenario is also consistent with the SED modeling."
ALAN P MARSCHER,New jet feature in the parsec-scale jet of the blazar OJ 287 connected to the 2017 teraelectronvolt flaring activity,"CONTEXT: In February 2017 the blazar OJ 287, one of the best super-massive binary-black-hole-system candidates, was detected for the first time at very high energies (VHEs;E &gt; 100 GeV) with the ground-basedγ-ray observatory VERITAS. AIMS: Very high energyγrays are thought to be produced in the near vicinity of the central engine in active galactic nuclei. For this reason, and with the main goal of providing useful information for the characterization of the physical mechanisms connected with the observed teraelectronvolt flaring event, we investigate the parsec-scale source properties by means of high-resolution very long baseline interferometry observations. METHODS: We use 86 GHz Global Millimeter-VLBI Array (GMVA) observations from 2015 to 2017 and combine them with additional multiwavelength radio observations at different frequencies from other monitoring programs. We investigate the source structure by modeling the brightness distribution with two-dimensional Gaussian components in the visibility plane. RESULTS: In the GMVA epoch following the source VHE activity, we find a new jet feature (labeled K) at ∼0.2 mas from the core region and located in between two quasi-stationary components (labeled S1 and S2). Multiple periods of enhanced activity are detected at different radio frequencies before and during the VHE flaring state. CONCLUSIONS: Based on the findings of this work, we identify as a possible trigger for the VHE flaring emission during the early months of 2017 the passage of a new jet feature through a recollimation shock (represented by the model-fit component S1) in a region of the jet located at a de-projected distance of ∼10 pc from the radio core."
ALAN P MARSCHER,Accretion geometry of the neutron star low mass X-ray binary Cyg X-2 from X-ray polarization measurements,"We report spectro-polarimetric results of an observational campaign of the bright neutron star low-mass X-ray binary Cyg X-2 simultaneously observed by IXPE, NICER and INTEGRAL. Consistently with previous results, the broad-band spectrum is characterized by a lower-energy component, attributed to the accretion disc with kT_in ≈ 1 keV, plus unsaturated Comptonization in thermal plasma with temperature kT_e = 3 keV and optical depth 𝛕 ≈ 4, assuming a slab geometry. We measure the polarization degree in the 2-8 keV band P=1.8 ± 0.3 per cent and polarization angle φ = 140°± 4°, consistent with the previous X-ray polarimetric measurements by OSO-8 as well as with the direction of the radio jet which was earlier observed from the source. While polarization of the disc spectral component is poorly constrained with the IXPE data, the Comptonized emission has a polarization degree P =4.0 ± 0.7 per cent and a polarization angle aligned with the radio jet. Our results strongly favour a spreading layer at the neutron star surface as the main source of the polarization signal. However, we cannot exclude a significant contribution from reflection off the accretion disc, as indicated by the presence of the iron fluorescence line."
ALAN P MARSCHER,First Sagittarius A* Event Horizon Telescope results. I. The shadow of the supermassive black hole in the center of the Milky Way,"We present the first Event Horizon Telescope (EHT) observations of Sagittarius A* (Sgr A*), the Galactic center source associated with a supermassive black hole. These observations were conducted in 2017 using a global interferometric array of eight telescopes operating at a wavelength of λ = 1.3 mm. The EHT data resolve a compact emission region with intrahour variability. A variety of imaging and modeling analyses all support an image that is dominated by a bright, thick ring with a diameter of 51.8 ± 2.3 μas (68% credible interval). The ring has modest azimuthal brightness asymmetry and a comparatively dim interior. Using a large suite of numerical simulations, we demonstrate that the EHT images of Sgr A* are consistent with the expected appearance of a Kerr black hole with mass ∼4 × 106 M ⊙, which is inferred to exist at this location based on previous infrared observations of individual stellar orbits, as well as maser proper-motion studies. Our model comparisons disfavor scenarios where the black hole is viewed at high inclination (i &gt; 50°), as well as nonspinning black holes and those with retrograde accretion disks. Our results provide direct evidence for the presence of a supermassive black hole at the center of the Milky Way, and for the first time we connect the predictions from dynamical measurements of stellar orbits on scales of 103–105 gravitational radii to event-horizon-scale images and variability. Furthermore, a comparison with the EHT results for the supermassive black hole M87* shows consistency with the predictions of general relativity spanning over three orders of magnitude in central mass."
ALAN P MARSCHER,VHE gamma-ray detection of FSRQ QSO B1420+326 and modeling of its enhanced broadband state in 2020,"CONTEXT: QSO B1420+326 is a blazar classified as a Flat Spectrum Radio Quasar (FSRQ). In the beginning of 2020 it underwent an enhanced flux state. An extensive multiwavelength campaign allowed us to trace the evolution of the flare. AIMS: We search for VHE gamma-ray emission from QSO B1420+326 during this flaring state. We aim to characterize and model the broadband emission of the source over different phases of the flare. METHODS: The source was observed with a number of instruments in radio, near infrared, optical (including polarimetry and spectroscopy), ultra-violet, X-ray and gamma-ray bands. We use dedicated optical spectroscopy results to estimate the accretion disk and the dust torus luminosity. We perform spectral energy distribution modeling in the framework of combined Synchrotron-Self-Compton and External Compton scenario in which the electron energy distribution is partially determined from acceleration and cooling processes. RESULTS: During the enhanced state the flux of both SED components drastically increased and the peaks were shifted to higher energies. Follow up observations with the MAGIC telescopes led to the detection of very-high-energy gamma-ray emission from this source, making it one of only a handful of FSRQs known in this energy range. Modeling allows us to constrain the evolution of the magnetic field and electron energy distribution in the emission region. The gamma-ray flare was accompanied by a rotation of the optical polarization vector during a low polarization state. Also, a new, superluminal radio knot contemporaneously appeared in the radio image of the jet. The optical spectroscopy shows a prominent FeII bump with flux evolving together with the continuum emission and a MgII line with varying equivalent width."
ALAN P MARSCHER,Stochastic modeling of multiwavelength variability of the classical BL Lac Object OJ287 on timescales ranging from decades to hours,"We present the results of our power spectral density analysis for the BL Lac object OJ 287, utilizing the Fermi-LAT survey at high-energy γ-rays, Swift-XRT in X-rays, several ground-based telescopes and the Kepler satellite in the optical, and radio telescopes at GHz frequencies. The light curves are modeled in terms of continuous-time autoregressive moving average (CARMA) processes. Owing to the inclusion of the Kepler data, we were able to construct for the first time the optical variability power spectrum of a blazar without any gaps across ~6 dex in temporal frequencies. Our analysis reveals that the radio power spectra are of a colored-noise type on timescales ranging from tens of years down to months, with no evidence for breaks or other spectral features. The overall optical power spectrum is also consistent with a colored noise on the variability timescales ranging from 117 years down to hours, with no hints of any quasi-periodic oscillations. The X-ray power spectrum resembles the radio and optical power spectra on the analogous timescales ranging from tens of years down to months. Finally, the γ-ray power spectrum is noticeably different from the radio, optical, and X-ray power spectra of the source: we have detected a characteristic relaxation timescale in the Fermi-LAT data, corresponding to ~150 days, such that on timescales longer than this, the power spectrum is consistent with uncorrelated (white) noise, while on shorter variability timescales there is correlated (colored) noise."
ALAN P MARSCHER,Mapping the circumnuclear regions of the Circinus galaxy with the imaging X-ray polarimetry explorer,"We report on the Imaging X-ray Polarimetry Explorer (IXPE) observation of the closest and X-ray brightest Compton-thick active galactic nucleus (AGN), the Circinus galaxy. We find the source to be significantly polarized in the 2--6 keV band. From previous studies, the X-ray spectrum is known to be dominated by reflection components, both neutral (torus) and ionized (ionization cones). Our analysis indicates that the polarization degree is 28 ± 7 per cent (at 68 per cent confidence level) for the neutral reflector, with a polarization angle of 18° ± 5°, roughly perpendicular to the radio jet. The polarization of the ionized reflection is unconstrained. A comparison with Monte Carlo simulations of the polarization expected from the torus shows that the neutral reflector is consistent with being an equatorial torus with a half-opening angle of 45°-55°. This is the first X-ray polarization detection in a Seyfert galaxy, demonstrating the power of X-ray polarimetry in probing the geometry of the circumnuclear regions of AGNs, and confirming the basic predictions of standard Unification Models."
ALAN P MARSCHER,The variability of the black hole image in M87 at the dynamical timescale,"The black hole images obtained with the Event Horizon Telescope (EHT) are expected to be variable at the dynamical timescale near their horizons. For the black hole at the center of the M87 galaxy, this timescale (5–61 days) is comparable to the 6 day extent of the 2017 EHT observations. Closure phases along baseline triangles are robust interferometric observables that are sensitive to the expected structural changes of the images but are free of station-based atmospheric and instrumental errors. We explored the day-to-day variability in closure-phase measurements on all six linearly independent nontrivial baseline triangles that can be formed from the 2017 observations. We showed that three triangles exhibit very low day-to-day variability, with a dispersion of ∼3°–5°. The only triangles that exhibit substantially higher variability (∼90°–180°) are the ones with baselines that cross the visibility amplitude minima on the u–v plane, as expected from theoretical modeling. We used two sets of general relativistic magnetohydrodynamic simulations to explore the dependence of the predicted variability on various black hole and accretion-flow parameters. We found that changing the magnetic field configuration, electron temperature model, or black hole spin has a marginal effect on the model consistency with the observed level of variability. On the other hand, the most discriminating image characteristic of models is the fractional width of the bright ring of emission. Models that best reproduce the observed small level of variability are characterized by thin ring-like images with structures dominated by gravitational lensing effects and thus least affected by turbulence in the accreting plasmas."
ALAN P MARSCHER,Constraints on black-hole charges with the 2017 EHT observations of M87*,
ALAN P MARSCHER,MAGIC and H.E.S.S. detect VHE gamma rays from the blazar OT081 for the first time: a deep multiwavelength study,
ALAN P MARSCHER,SYMBA: an end-to-end VLBI synthetic data generation pipeline,"CONTEXT: Realistic synthetic observations of theoretical source models are essential for our understanding of real observational data. In using synthetic data, one can verify the extent to which source parameters can be recovered and evaluate how various data corruption effects can be calibrated. These studies are the most important when proposing observations of new sources, in the characterization of the capabilities of new or upgraded instruments, and when verifying model-based theoretical predictions in a direct comparison with observational data. AIMS: We present the SYnthetic Measurement creator for long Baseline Arrays (SYMBA), a novel synthetic data generation pipeline for Very Long Baseline Interferometry (VLBI) observations. SYMBA takes into account several realistic atmospheric, instrumental, and calibration effects. METHODS: We used SYMBA to create synthetic observations for the Event Horizon Telescope (EHT), a millimetre VLBI array, which has recently captured the first image of a black hole shadow. After testing SYMBA with simple source and corruption models, we study the importance of including all corruption and calibration effects, compared to the addition of thermal noise only. Using synthetic data based on two example general relativistic magnetohydrodynamics (GRMHD) model images of M 87, we performed case studies to assess the image quality that can be obtained with the current and future EHT array for different weather conditions. RESULTS: Our synthetic observations show that the effects of atmospheric and instrumental corruptions on the measured visibilities are significant. Despite these effects, we demonstrate how the overall structure of our GRMHD source models can be recovered robustly with the EHT2017 array after performing calibration steps, which include fringe fitting, a priori amplitude and network calibration, and self-calibration. With the planned addition of new stations to the EHT array in the coming years, images could be reconstructed with higher angular resolution and dynamic range. In our case study, these improvements allowed for a distinction between a thermal and a non-thermal GRMHD model based on salient features in reconstructed images."
ALAN P MARSCHER,A detailed kinematic study of 3C 84 and its connection to γ-rays,"3C 84 (NGC 1275) is the bright radio core of the Perseus cluster. Even in the absence of strong relativistic effects, the source has been detected at γ-rays up to TeV energies. Despite its intensive study, the physical processes responsible for the high-energy emission in the source remain unanswered. We present a detailed kinematics study of the source and its connection to γ-ray emission. The subparsec-scale radio structure is dominated by slow-moving features in both the eastern and western lanes of the jet. The jet appears to have accelerated to its maximum speed within less than 125,000 gravitational radii. The fastest reliably detected speed in the jet was ∼0.9c. This leads to a minimum viewing angle to the source of ≳42° and a maximum Doppler factor of ≲1.5. Our analysis suggests the presence of multiple high-energy sites in the source. If γ-rays are associated with kinematic changes in the jet, they are being produced in both eastern and western lanes in the jet. Three γ-ray flares are contemporaneous with epochs where the slowly moving emission region splits into two subregions. We estimate the significance of these events being associated to be ∼2σ–3σ. We tested our results against theoretical predictions for magnetic-reconnection-induced mini-jets and turbulence and find them compatible."
ALAN P MARSCHER,Spatially resolved origin of mm-wave linear polarization in the nuclear region of 3C 84,"We report results from a deep polarization imaging of the nearby radio galaxy 3C 84 (NGC 1275). The source was observed with the Global Millimeter VLBI Array (GMVA) at 86 GHz at an ultra-high angular resolution of 50μas (corresponding to 250R𝒔). We also add complementary multi-wavelength data from the Very Long Baseline Array (VLBA; 15 & 43 GHz) and from the Atacama Large Millimeter/submillimeter Array (ALMA; 97.5, 233.0, and 343.5 GHz). At 86 GHz, we measure a fractional linear polarization of ~ 2% in the VLBI core region. The polarization morphology suggests that the emission is associated with an underlying limb-brightened jet. The fractional linear polarization is lower at 43 and 15 GHz (~ 0.3-0.7% and < 0.1%, respectively). This suggests an increasing linear polarization degree towards shorter wavelengths on VLBI scales. We also obtain a large rotation measure (RM) of ~ 10⁵⁻⁶ rad/m² in the core at ≳43 GHz. Moreover, the VLBA 43 GHz observations show a variable RM in the VLBI core region during a small flare in 2015. Faraday depolarization and Faraday conversion in an inhomogeneous and mildly relativistic plasma could explain the observed linear polarization characteristics and the previously measured frequency dependence of the circular polarization. Our Faraday depolarization modeling suggests that the RM most likely originates from an external screen with a highly uniform RM distribution. To explain the large RM value, the uniform RM distribution, and the RM variability, we suggest that the Faraday rotation is caused by a boundary layer in a transversely stratified jet. Based on the RM and the synchrotron spectrum of the core, we provide an estimate for the magnetic field strength and the electron density of the jet plasma."
ALAN P MARSCHER,Multiwavelength observations of the blazar BL Lacertae: a new fast TeV gamma-ray flare,"Combined with measurements made by very-long-baseline interferometry, the observations of fast TeV gamma-ray flares probe the structure and emission mechanism of blazar jets. However, only a handful of such flares have been detected to date, and only within the last few years have these flares been observed from lower-frequency-peaked BL Lac objects and flat-spectrum radio quasars. We report on a fast TeV gamma-ray flare from the blazar BL Lacertae observed by the Very Energetic Radiation Imaging Telescope Array System (VERITAS). with a rise time of ~2.3 hr and a decay time of ~36 min. The peak flux above 200 GeV is (4.2 ± 0.6) × 10⁻⁶ photon m⁻² s⁻¹ measured with a 4-minute-binned light curve, corresponding to ~180% of the flux that is observed from the Crab Nebula above the same energy threshold. Variability contemporaneous with the TeV gamma-ray flare was observed in GeV gamma-ray, X-ray, and optical flux, as well as in optical and radio polarization. Additionally, a possible moving emission feature with superluminal apparent velocity was identified in Very Long Baseline Array observations at 43 GHz, potentially passing the radio core of the jet around the time of the gamma-ray flare. We discuss the constraints on the size, Lorentz factor, and location of the emitting region of the flare, and the interpretations with several theoretical models that invoke relativistic plasma passing stationary shocks."
ALAN P MARSCHER,ALMA full polarization observations of PKS 1830−211 during its record-breaking flare of 2019,"We report Atacama Large Millimeter Array (ALMA) Band 6 full-polarization observations of the lensed blazar PKS 1830−211 during its record-breaking radio and gamma-ray flare in the spring of 2019. The observations were taken close to the peak of the gamma activity and show a clear difference in polarization state between the two time-delayed images. The leading image has a fractional polarization about three times lower than the trailing image, implying that significant depolarization occurred during the flare. In addition, we observe clear intra-hour variability of the polarization properties between the two lensed images, with a quasi-linear increase in the differential electric-vector position angle at a rate of about two degrees per hour, associated with changes in the relative fractional polarization of ∼10%. This variability, combined with the lower polarization close to the peak of gamma activity, is in agreement with models of magnetic turbulence to explain polarization variability in blazar jets. Finally, the comparison of results from the full and differential polarization analysis confirms that the differential polarization technique (Martí-Vidal et al. 2016, A&A, 593, A61) can provide useful information on the polarization state of sources like gravitationally lensed radio-loud quasars."
ALAN P MARSCHER,Broadband multi-wavelength properties of M87 during the 2017 Event Horizon Telescope campaign,"In 2017, the Event Horizon Telescope (EHT) Collaboration succeeded in capturing the first direct image of the center of the M87 galaxy. The asymmetric ring morphology and size are consistent with theoretical expectations for a weakly accreting supermassive black hole of mass ∼6.5 × 109 M ⊙. The EHTC also partnered with several international facilities in space and on the ground, to arrange an extensive, quasi-simultaneous multi-wavelength campaign. This Letter presents the results and analysis of this campaign, as well as the multi-wavelength data as a legacy data repository. We captured M87 in a historically low state, and the core flux dominates over HST-1 at high energies, making it possible to combine core flux constraints with the more spatially precise very long baseline interferometry data. We present the most complete simultaneous multi-wavelength spectrum of the active nucleus to date, and discuss the complexity and caveats of combining data from different spatial scales into one broadband spectrum. We apply two heuristic, isotropic leptonic single-zone models to provide insight into the basic source properties, but conclude that a structured jet is necessary to explain M87’s spectrum. We can exclude that the simultaneous γ-ray emission is produced via inverse Compton emission in the same region producing the EHT mm-band emission, and further conclude that the γ-rays can only be produced in the inner jets (inward of HST-1) if there are strongly particle-dominated regions. Direct synchrotron emission from accelerated protons and secondaries cannot yet be excluded."
ALAN P MARSCHER,Event Horizon Telescope imaging of the archetypal blazar 3C 279 at an extreme 20 microarcsecond resolution,"3C 279 is an archetypal blazar with a prominent radio jet that show broadband flux density variability across the entire electromagnetic spectrum. We use an ultra-high angular resolution technique – global Very Long Baseline Interferometry (VLBI) at 1.3 mm (230 GHz) – to resolve the innermost jet of 3C 279 in order to study its fine-scale morphology close to the jet base where highly variable γ-ray emission is thought to originate, according to various models. The source was observed during four days in April 2017 with the Event Horizon Telescope at 230 GHz, including the phased Atacama Large Millimeter/submillimeter Array (ALMA), at an angular resolution of ∼20 μas (at a redshift of z = 0.536 this corresponds to ∼0.13 pc  ∼ 1700 Schwarzschild radii with a black hole mass M<jats:sub>BH</jats:sub> = 8 × 10^8 M_⊙). Imaging and model-fitting techniques were applied to the data to parameterize the fine-scale source structure and its variation. We find a multicomponent inner jet morphology with the northernmost component elongated perpendicular to the direction of the jet, as imaged at longer wavelengths. The elongated nuclear structure is consistent on all four observing days and across different imaging methods and model-fitting techniques, and therefore appears robust. Owing to its compactness and brightness, we associate the northern nuclear structure as the VLBI “core”. This morphology can be interpreted as either a broad resolved jet base or a spatially bent jet. We also find significant day-to-day variations in the closure phases, which appear most pronounced on the triangles with the longest baselines. Our analysis shows that this variation is related to a systematic change of the source structure. Two inner jet components move non-radially at apparent speeds of ∼15 c and ∼20 c (∼1.3 and ∼1.7 μas day^−1, respectively), which more strongly supports the scenario of traveling shocks or instabilities in a bent, possibly rotating jet. The observed apparent speeds are also coincident with the 3C 279 large-scale jet kinematics observed at longer (cm) wavelengths, suggesting no significant jet acceleration between the 1.3 mm core and the outer jet. The intrinsic brightness temperature of the jet components are ≲10^10 K, a magnitude or more lower than typical values seen at ≥7 mm wavelengths. The low brightness temperature and morphological complexity suggest that the core region of 3C 279 becomes optically thin at short (mm) wavelengths."
ALAN P MARSCHER,First M87 Event Horizon Telescope results. II. Array and instrumentation,"The Event Horizon Telescope (EHT) is a very long baseline interferometry (VLBI) array that comprises millimeter- and submillimeter-wavelength telescopes separated by distances comparable to the diameter of the Earth. At a nominal operating wavelength of ~1.3 mm, EHT angular resolution (λ/D) is ~25 μas, which is sufficient to resolve nearby supermassive black hole candidates on spatial and temporal scales that correspond to their event horizons. With this capability, the EHT scientific goals are to probe general relativistic effects in the strong-field regime and to study accretion and relativistic jet formation near the black hole boundary. In this Letter we describe the system design of the EHT, detail the technology and instrumentation that enable observations, and provide measures of its performance. Meeting the EHT science objectives has required several key developments that have facilitated the robust extension of the VLBI technique to EHT observing wavelengths and the production of instrumentation that can be deployed on a heterogeneous array of existing telescopes and facilities. To meet sensitivity requirements, high-bandwidth digital systems were developed that process data at rates of 64 gigabit s−1, exceeding those of currently operating cm-wavelength VLBI arrays by more than an order of magnitude. Associated improvements include the development of phasing systems at array facilities, new receiver installation at several sites, and the deployment of hydrogen maser frequency standards to ensure coherent data capture across the array. These efforts led to the coordination and execution of the first Global EHT observations in 2017 April, and to event-horizon-scale imaging of the supermassive black hole candidate in M87."
ALAN P MARSCHER,First M87 Event Horizon Telescope results. I. The shadow of the supermassive black hole,"When surrounded by a transparent emission region, black holes are expected to reveal a dark shadow caused by gravitational light bending and photon capture at the event horizon. To image and study this phenomenon, we have assembled the Event Horizon Telescope, a global very long baseline interferometry array observing at a wavelength of 1.3 mm. This allows us to reconstruct event-horizon-scale images of the supermassive black hole candidate in the center of the giant elliptical galaxy M87. We have resolved the central compact radio source as an asymmetric bright emission ring with a diameter of 42 ± 3 μas, which is circular and encompasses a central depression in brightness with a flux ratio gsim10:1. The emission ring is recovered using different calibration and imaging schemes, with its diameter and width remaining stable over four different observations carried out in different days. Overall, the observed image is consistent with expectations for the shadow of a Kerr black hole as predicted by general relativity. The asymmetry in brightness in the ring can be explained in terms of relativistic beaming of the emission from a plasma rotating close to the speed of light around a black hole. We compare our images to an extensive library of ray-traced general-relativistic magnetohydrodynamic simulations of black holes and derive a central mass of M = (6.5 ± 0.7) × 10^9 M ⊙. Our radio-wave observations thus provide powerful evidence for the presence of supermassive black holes in centers of galaxies and as the central engines of active galactic nuclei. They also present a new tool to explore gravity in its most extreme limit and on a mass scale that was so far not accessible."
ALAN P MARSCHER,Polarized x-rays constrain the disk-jet geometry in the black hole x-ray binary Cygnus X-1,"A black hole x-ray binary (XRB) system forms when gas is stripped from a normal star and accretes onto a black hole, which heats the gas sufficiently to emit x-rays. We report a polarimetric observation of the XRB Cygnus X-1 using the Imaging X-ray Polarimetry Explorer. The electric field position angle aligns with the outflowing jet, indicating that the jet is launched from the inner x-ray-emitting region. The polarization degree is 4.01 ± 0.20% at 2 to 8 kiloelectronvolts, implying that the accretion disk is viewed closer to edge-on than the binary orbit. These observations reveal that hot x-ray-emitting plasma is spatially extended in a plane perpendicular to, not parallel to, the jet axis."
ALAN P MARSCHER,Two flares with one shock: the interesting case of 3C 454.3,"The quasar 3C 454.3 is a blazar known for its rapid and violent outbursts seen across the electromagnetic spectrum. Using γ-ray, X-ray, multiband optical, and very-long-baseline interferometric data we investigate the nature of two such events that occurred in 2013 and 2014 accompanied by strong variations in optical polarization, including a ~230° electric vector position angle (EVPA) rotation. Our results suggest that a single disturbance was responsible for both flaring events. We interpret the disturbance as a shock propagating down the jet. Under this interpretation the 2013 flare originated most likely due to changes in the viewing angle caused by perhaps a bent or helical trajectory of the shock upstream of the radio core. The 2014 flare and optical polarization behavior are the result of the shock exiting the 43 GHz radio core, suggesting that shock crossings are one of the possible mechanisms for EVPA rotations."
ALAN P MARSCHER,Radio and γ-ray activity in the jet of the blazar S5 0716+714,"We explore the connection between the γ-ray and radio emission in the jet of the blazar 0716+714 by using 15, 37, and 230 GHz radio and 0.1–200 GeV γ-ray light curves spanning 10.5 yr (2008–2019). We find significant positive and negative correlations between radio and γ-ray fluxes in different time ranges. The time delays between radio and γ-ray emission suggest that the observed γ-ray flares originated from multiple regions upstream of the radio core, within a few parsecs from the central engine. Using time-resolved 43 GHz Very Long Baseline Array maps we identified 14 jet components moving downstream along the jet. Their apparent speeds range from 6c to 26c, and they show notable variations in their position angles upstream from the stationary component (∼0.53 mas from the core). The brightness temperature declines as a function of distance from the core according to a power law that becomes shallower at the location of the stationary component. We also find that the periods at which significant correlations between radio and γ-ray emission occur overlap with the times when the jet was oriented to the north. Our results indicate that the passage of a propagating disturbance (or shock) through the radio core and the orientation of the jet might be responsible for the observed correlation between the radio and γ-ray variability. We present a scenario that connects the positive correlation and the unusual anticorrelation by combining the production of a flare and a dip at γ-rays by a strong moving shock at different distances from the jet apex."
ALAN P MARSCHER,Polarized blazar X-rays imply particle acceleration in shocks,"Most of the light from blazars, active galactic nuclei with jets of magnetized plasma that point nearly along the line of sight, is produced by high-energy particles, up to around 1 TeV. Although the jets are known to be ultimately powered by a supermassive black hole, how the particles are accelerated to such high energies has been an unanswered question. The process must be related to the magnetic field, which can be probed by observations of the polarization of light from the jets. Measurements of the radio to optical polarization-the only range available until now-probe extended regions of the jet containing particles that left the acceleration site days to years earlier1-3, and hence do not directly explore the acceleration mechanism, as could X-ray measurements. Here we report the detection of X-ray polarization from the blazar Markarian 501 (Mrk 501). We measure an X-ray linear polarization degree ΠX of around 10%, which is a factor of around 2 higher than the value at optical wavelengths, with a polarization angle parallel to the radio jet. This points to a shock front as the source of particle acceleration and also implies that the plasma becomes increasingly turbulent with distance from the shock."
ALAN P MARSCHER,X-Ray Polarization Observations of BL Lacertae,"Blazars are a class of jet-dominated active galactic nuclei with a typical double-humped spectral energy distribution. It is of common consensus that the synchrotron emission is responsible for the low frequency peak, while the origin of the high frequency hump is still debated. The analysis of X-rays and their polarization can provide a valuable tool to understand the physical mechanisms responsible for the origin of high-energy emission of blazars. We report the first observations of BL Lacertae (BL Lac) performed with the Imaging X-ray Polarimetry Explorer, from which an upper limit to the polarization degree Π X &lt; 12.6% was found in the 2–8 keV band. We contemporaneously measured the polarization in radio, infrared, and optical wavelengths. Our multiwavelength polarization analysis disfavors a significant contribution of proton-synchrotron radiation to the X-ray emission at these epochs. Instead, it supports a leptonic origin for the X-ray emission in BL Lac."
ALAN P MARSCHER,The 2016 June optical and gamma-ray outburst and optical microvariability of the blazar 3C 454.3,"The quasar 3C 454.3 underwent a uniquely structured multifrequency outburst in 2016 June. The blazar was observed in the optical R-band by several ground-based telescopes in photometric and polarimetric modes, at γ-ray frequencies by the Fermi Large Area Telescope, and at 43 GHz with the Very Long Baseline Array. The maximum flux density was observed on 2016 June 24 at both optical and γ-ray frequencies, reaching {S}_{\mathrm{opt}}^{\max }=18.91\pm 0.08 mJy and {S}_{\gamma }^{\max }=22.20\pm 0.18\times {10}^{-6} ph cm−2 s−1, respectively. The 2016 June outburst possessed a precipitous decay at both γ-ray and optical frequencies, with the source decreasing in flux density by a factor of 4 over a 24 hr period in the R-band. Intraday variability was observed throughout the outburst, with flux density changes between 1 and 5 mJy over the course of a night. The precipitous decay featured statistically significant quasiperiodic microvariability oscillations with an amplitude of ~2%–3% about the mean trend and a characteristic period of 36 minutes. The optical degree of polarization jumped from ~3% to nearly 20% during the outburst, while the position angle varied by ~120°. A knot was ejected from the 43 GHz core on 2016 February 25, moving at an apparent speed {v}_{\mathrm{app}}=20.3c\pm 0.8c. From the observed minimum timescale of variability {\tau }_{\mathrm{opt}}^{\min }\approx 2\,\mathrm{hr} and derived Doppler factor δ = 22.6, we find the size of the emission region r lesssim 2.6 × 1015 cm. If the quasiperiodic microvariability oscillations are caused by periodic variations of the Doppler factor of emission from a turbulent vortex, we derive the rotational speed of the vortex to be ~0.2c."
ALAN P MARSCHER,Emission-line variability during a nonthermal outburst in the gamma-ray bright quasar 1156+295,"We present multi-epoch optical spectra of the γ-ray bright blazar 1156+295 (4C +29.45, Ton 599) obtained with the 4.3 m Lowell Discovery Telescope. During a multiwavelength outburst in late 2017, when the γ-ray flux increased to 2.5 × 10−6 phot cm−2 s−1 and the quasar was first detected at energies ≥100 GeV, the flux of the Mg ii λ2798 emission line changed, as did that of the Fe emission complex at shorter wavelengths. These emission-line fluxes increased along with the highly polarized optical continuum flux, which is presumably synchrotron radiation from the relativistic jet, with a relative time delay of ≲2 weeks. This implies that the line-emitting clouds lie near the jet, which points almost directly toward the line of sight. The emission-line radiation from such clouds, which are located outside the canonical accretion-disk related broad-line region, may be a primary source of seed photons that are up-scattered to γ-ray energies by relativistic electrons in the jet."
ALAN P MARSCHER,First Sagittarius A* Event Horizon Telescope results. III. Imaging of the Galactic center supermassive black hole,"We present the first event-horizon-scale images and spatiotemporal analysis of Sgr A* taken with the Event Horizon Telescope in 2017 April at a wavelength of 1.3 mm. Imaging of Sgr A* has been conducted through surveys over a wide range of imaging assumptions using the classical CLEAN algorithm, regularized maximum likelihood methods, and a Bayesian posterior sampling method. Different prescriptions have been used to account for scattering effects by the interstellar medium toward the Galactic center. Mitigation of the rapid intraday variability that characterizes Sgr A* has been carried out through the addition of a “variability noise budget” in the observed visibilities, facilitating the reconstruction of static full-track images. Our static reconstructions of Sgr A* can be clustered into four representative morphologies that correspond to ring images with three different azimuthal brightness distributions and a small cluster that contains diverse nonring morphologies. Based on our extensive analysis of the effects of sparse (u, v)-coverage, source variability, and interstellar scattering, as well as studies of simulated visibility data, we conclude that the Event Horizon Telescope Sgr A* data show compelling evidence for an image that is dominated by a bright ring of emission with a ring diameter of ∼50 μas, consistent with the expected “shadow” of a 4 × 106 M⊙ black hole in the Galactic center located at a distance of 8 kpc."
ALAN P MARSCHER,Time-dependent spectral modelling of Markarian 421 during a violent outburst in 2010,"We present the results of extensive modelling of the spectral energy distributions (SEDs) of the closest blazar (z = 0.031) Markarian 421 (Mrk 421) during a giant outburst in 2010 February. The source underwent rapid flux variations in both X-rays and very high energy (VHE) gamma rays as it evolved from a low-flux state on 2010 February 13–15 to a high-flux state on 2010 February 17. During this period, the source exhibited significant spectral hardening from X-rays to VHE gamma rays while exhibiting a ‘harder when brighter’ behaviour in these energy bands. We reproduce the broad-band SED using a time-dependent multizone leptonic jet model with radiation feedback. We find that an injection of the leptonic particle population with a single power-law energy distribution at shock fronts followed by energy losses in an inhomogeneous emission region is suitable for explaining the evolution of Mrk 421 from low- to high-flux state in 2010 February. The spectral states are successfully reproduced by a combination of a few key physical parameters, such as the maximum and minimum cut-offs and power-law slope of the electron injection energies, magnetic field strength, and bulk Lorentz factor of the emission region. The simulated light curves and spectral evolution of Mrk 421 during this period imply an almost linear correlation between X-ray flux at 1–10 keV energies and VHE gamma-ray flux above 200 GeV, as has been previously exhibited by this source. Through this study, a general trend that has emerged for the role of physical parameters is that, as the flare evolves from a low- to a high-flux state, higher bulk kinetic energy is injected into the system with a harder particle population and a lower magnetic field strength."
ALAN P MARSCHER,Linear polarization signatures of particle acceleration in high-synchrotron-peak blazars,"Blazars whose synchrotron spectral energy distribution (SED) peaks at X-ray energies need to accelerate electrons to energies in the &gt;100 GeV range in relativistic plasma jets at distances of parsecs from the central engine. Compton scattering by the same electrons can explain high luminosities at very high photon energies (&gt;100 GeV) from the same objects. Turbulence combined with a standing conical shock can accomplish this. Such a scenario can also qualitatively explain the level and variability of linear polarization observed at optical frequencies in these objects. Multi-wavelength polarization measurements, including those at X-ray energies by the Imaging X-ray Polarimetry Explorer (IXPE), find that the degree of polarization is several times higher at X-ray than at optical wavelengths, in general agreement with the turbulence-plus-shock picture. Some detailed properties of the observed polarization can be naturally explained by this scenario, while others pose challenges that may require modifications to the model."
ALAN P MARSCHER,Characterizing and mitigating intraday variability: reconstructing source structure in accreting black holes with mm-VLBI,"The extraordinary physical resolution afforded by the Event Horizon Telescope has opened a window onto the astrophysical phenomena unfolding on horizon scales in two known black holes, M87* and Sgr A*. However, with this leap in resolution has come a new set of practical complications. Sgr A* exhibits intraday variability that violates the assumptions underlying Earth aperture synthesis, limiting traditional image reconstruction methods to short timescales and data sets with very sparse (u, v) coverage. We present a new set of tools to detect and mitigate this variability. We develop a data-driven, model-agnostic procedure to detect and characterize the spatial structure of intraday variability. This method is calibrated against a large set of mock data sets, producing an empirical estimator of the spatial power spectrum of the brightness fluctuations. We present a novel Bayesian noise modeling algorithm that simultaneously reconstructs an average image and statistical measure of the fluctuations about it using a parameterized form for the excess variance in the complex visibilities not otherwise explained by the statistical errors. These methods are validated using a variety of simulated data, including general relativistic magnetohydrodynamic simulations appropriate for Sgr A* and M87*. We find that the reconstructed source structure and variability are robust to changes in the underlying image model. We apply these methods to the 2017 EHT observations of M87*, finding evidence for variability across the EHT observing campaign. The variability mitigation strategies presented are widely applicable to very long baseline interferometry observations of variable sources generally, for which they provide a data-informed averaging procedure and natural characterization of inter-epoch image consistency."
FRANK J KOROM,Folkloristics and Indian folklore,
FRANK J KOROM,"Introduction: Place, space and identity: the cultural, economic and aesthetic politics of Tibetan diaspora",
FRANK J KOROM,Introduction - South Asian nationalisms,"This article intends to raise questions related to nationalism in South Asia, while also addressing the rationale for this special issue. Is nationalism a monolithic construct based on a European precedent or is it something much larger that is developed pluralistically in a variety of contexts around the world? If the latter is true, which is our position, then how do we go about studying the various versions of global nationalism? We argue that good comparison is based on both similarity and difference. To make a case for multiple versions of nationalism, the articles included herein focus on the Indian Subcontinent. Each article looks at a particular country belonging to the South Asian Association for Regional Cooperation (SAARC), the intergovernmental group representing the geopolitical union of states in South Asia, which was founded in Dhaka, Bangladesh in 1985. The overall purpose of this collection of articles is to highlight the varieties of nationalism found in the region, with the goal of interrogating the idea of a singular form of nationalism inherited by postcolonial societies from their European colonizers."
FRANK J KOROM,Animal slaughter and religious nationalism in Bhutan,
FRANK J KOROM,New Year’s day performances as nationalist discourse in Bangladesh,
FRANK J KOROM,Editors' note,
EDWARD LOECHLER,Architecture of Y-Family DNA Polymerases Relevant to Translesion DNA Synthesis as Revealed in Structural and Molecular Modeling Studies,"DNA adducts, which block replicative DNA polymerases (DNAPs), are often bypassed by lesion-bypass DNAPs, which are mostly in the Y-Family. Y-Family DNAPs can do non-mutagenic or mutagenic dNTP insertion, and understanding this difference is important, because mutations transform normal into tumorigenic cells. Y-Family DNAP architecture that dictates mechanism, as revealed in structural and modeling studies, is considered. Steps from adduct blockage of replicative DNAPs, to bypass by a lesion-bypass DNAP, to resumption of synthesis by a replicative DNAP are described. Catalytic steps and protein conformational changes are considered. One adduct is analyzed in greater detail: the major benzo[a]pyrene adduct (B[a]P-N2-dG), which is bypassed non-mutagenically (dCTP insertion) by Y-family DNAPs in the IV/κ-class and mutagenically (dATP insertion) by V/η-class Y-Family DNAPs. Important architectural differences between IV/κ-class versus V/η-class DNAPs are discussed, including insights gained by analyzing ~400 sequences each for bacterial DNAPs IV and V, along with sequences from eukaryotic DNAPs kappa, eta and iota. The little finger domains of Y-Family DNAPs do not show sequence conservation; however, their structures are remarkably similar due to the presence of a core of hydrophobic amino acids, whose exact identity is less important than the hydrophobic amino acid spacing."
EDWARD LOECHLER,Genetic Effects of Oxidative DNA Damages: Comparative Mutagenesis of the Imidazole Ring-Opened Formamidopyrimidines (Fapy Lesions) and 8-Oxo-Purines in Simian Kidney Cells,"Fapy·dG and 8-oxo-7,8-dihydro-2′-deoxyguanosine (8-oxo-dG) are formed in DNA by hydroxyl radical damage. In order to study replication past these lesions in cells, we constructed a single-stranded shuttle vector containing the lesion in 5′-TGT and 5′-TGA sequence contexts. Replication of the modified vector in simian kidney (COS-7) cells showed that Fapy·dG is mutagenic inducing primarily targeted Fapy·G→T transversions. In the 5′-TGT sequence mutational frequency of Fapy·dG was ^∼30%, whereas in the 5′-TGA sequence it was ^∼8%. In parallel studies 8-oxo-dG was found to be slightly less mutagenic than Fapy·dG, though it also exhibited a similar context effect: 4-fold G→T transversions (24% versus 6%) occurred in the 5′-TGT sequence relative to 5′-TGA. To investigate a possible structural basis for the higher G→T mutations induced by both lesions when their 3′ neighbor was T, we carried out a molecular modeling investigation in the active site of DNA polymerase β, which is known to incorporate both dCTP (no mutation) and dATP (G→T substitution) opposite 8-oxo-G. In pol β, the syn-8-oxo-G:dATP pair showed greater stacking with the 3′-T:A base pair in the 5′-TGT sequence compared with the 3′-A:T in the 5′-TGA sequence, whereas stacking for the anti-8-oxo-G:dCTP pair was similar in both 5′-TGT and 5′-TGA sequences. Similarly, syn-Fapy·G:dATP pairing showed greater stacking in the 5′-TGT sequence compared with the 5′-TGA sequence, while stacking for anti-Fapy·G:dCTP pairs was similar in the two sequences. Thus, for both lesions less efficient base stacking between the lesion:dATP pair and the 3′-A:T base pair in the 5′-TGA sequence might cause lower G→T mutational frequencies in the 5′-TGA sequence compared to 5′-TGT. The corresponding lesions derived from 2′-deoxyadenosine, Fapy·dA and 8-oxo-dA, were not detectably mutagenic in the 5′-TAT sequence, and were only weakly mutagenic (<1%) in the 5′-TAA sequence context, where both lesions induced targeted A→C transversions. To our knowledge this is the first investigation using extrachromosomal probes containing a Fapy·dG or Fapy·dA site-specifically incorporated, which showed unequivocally that in simian kidney cells Fapy·G→T substitutions occur at a higher frequency than 8-oxo-G→T and that Fapy·dA is very weakly mutagenic, as is 8-oxo-dA."
BIN GU,Informational cascades and software adoption on the Internet: an empirical investigation,"Online users often need to make adoption decisions without accurate information about the product values. An informational cascade occurs when it is optimal for an online user, having observed others' actions, to follow the adoption decision of the preceding individual without regard to his own information. Informational cascades are often rational for individual decision making; however, it may lead to adoption of inferior products. With easy availability of information about other users' choices, the Internet offers an ideal environment for informational cascades. In this paper, we empirically examine informational cascades in the context of online software adoption. We find user behavior in adopting software products is consistent with the predictions of the informational cascades literature. Our results demonstrate that online users' choices of software products exhibit distinct jumps and drops with changes in download ranking, as predicted by informational cascades theory. Furthermore, we find that user reviews have no impact on user adoption of the most popular product, while having an increasingly positive impact on the adoption of lower ranking products. The phenomenon persists after controlling for alternative explanations such as network effects, word-of-mouth effects, and product diffusion. Our results validate informational cascades as an important driver for decision making on the Internet. The finding also offers an explanation for the mixed results reported in prior studies with regard to the influence of online user reviews on product sales. We show that the mixed results could be due to the moderating effect of informational cascades."
BIN GU,Uncertainty-reduction or reciprocity? Understanding the effects of a platform-initiated reviewer incentive program on regular review generation,"To stimulate product reviews, many e-commerce platforms have launched reviewer incentive programs in which free product samples are provided to reviewers in exchange for their ratings of the samples. This study focuses on an unexplored aspect of reviewer incentive programs—the impact of participating in such programs on reviewers’ ratings of products they purchased normally (i.e., regular ratings). We find that after reviewers join the program and receive free product samples, their average regular rating increases by 2.25% (i.e., 0.093 more stars on the five-star scale). Our follow-up analyses indicate that the observed regular-rating increase can be attributed to an uncertainty-reduction effect evoked by the free product samples, as opposed to a reciprocity effect. We further delve into the underlying mechanism by analyzing the reviewers’ regular ratings at a granular, product-category level. Consistent with our theorization of the uncertainty-reduction effect, our findings reveal that reviewers’ regular-rating increase is driven by improved assessment and knowledge about products sharing common attributes with the sampled products, resulting in better post-purchase outcomes. Our results demonstrate that apart from motivating the feedback for the sampled products, free product sampling can reduce reviewers’ product uncertainty and trigger evident change in their regular ratings for the purchased products."
BIN GU,Gender wage gap in online gig economy and gender differences in job preferences,"We explore whether there is a gender wage gap in the gig economy and examine to what degree gender differences in job application strategy could account for the gap. With a large-scale dataset from a leading online labor market, we show that females only earn around 81.4% of the hourly wage of their male counterparts. We further investigate three main aspects of job application strategy, namely bid timing, job selection, and avoidance of monitoring. After matching males with females using the propensity score matching method, we find that females tend to bid later and prefer jobs with a lower budget. In particular, the observed gender difference in bid timing can explain 7.6% of the difference in hourly wage, which could account for 41% of the gender wage gap (i.e. 18.6%) observed by us. Moreover, taking advantage of a natural experiment wherein the platform rolled out the monitoring system, we find that females are less willing to bid for monitored jobs than males. To further quantify the economic value of the gender difference in avoidance of monitoring, we run a field experiment on Amazon Mechanical Turk (AMT), which suggests that females tend to have a higher willingness to pay (WTP) for the avoidance of monitoring. The gender difference in WTP for the avoidance of monitoring can explain 8.1% of the difference in hourly wage, namely, 44% of the observed gender wage gap. Overall, our study reveals the important role of job application strategies in the persistent gender wage gap."
BIN GU,Product-driven entrepreneurs and online crowdfunding,"Advancements in information technology is known for enabling new business models and new market mechanisms. Online crowdfunding is one such new mechanism through which entrepreneurs can advertise their potential products and attract investors from the mass. In this study, we advance the existing theory on online crowdfunding markets by recognizing that online crowdfunding provides not only a venue of fundraising to entrepreneurs but also a venue for them to obtain demand information before production and to signal their intention. We formulate a spatial competition model between profit-driven entrepreneurs and product-driven entrepreneurs and find that on average profit-driven entrepreneurs earn higher profits, but their advantage is constrained by the mechanism of the crowdfunding campaign, and product-driven entrepreneurs earn a significant fraction of the market. Comparing to the Keep-it-all funding scheme we used in the baseline model, the All-or-nothing scheme is more favorable for product-driven entrepreneur, under which the two type entrepreneurs earn equal market shares. We further discuss model implications for consumer satisfaction of the platform and find that including more product-driven entrepreneurs, or adopting All-or-nothing funding scheme improves the overall quality of the platform, but the effects on design popularity and consumer welfare are subtle."
BIN GU,Internet-of-things enabled supply chain planning and coordination with big data services: certain theoretic implications,"Recent advances in information technology have led to profound changes in global manufacturing. This study focuses on the theoretical and practical challenges and opportunities arising from the Internet of Things (IoT) as it enables new ways of supply-chain operations partially based on big-data analytics and changes in the nature of industries. We intend to reveal the acting principle of the IoT and its implications for big-data analytics on the supply chain operational performance, particularly with regard to dynamics of operational coordination and optimization for supply chains by leveraging big data obtained from smart connected products (SCPs), and the governance mechanism of big-data sharing. Building on literature closely related to our focal topic, we analyze and deduce the substantial influence of disruptive technologies and emerging business models including the IoT, big data analytics and SCPs on many aspects of supply chains, such as consumers value judgment, products development, resources allocation, operations optimization, revenue management and network governance. Furthermore, we propose several research directions and corresponding research schemes in the new situations. This study aims to promote future researches in the field of big data-driven supply chain management with the IoT, help firms improve data-driven operational decisions, and provide government a reference to advance and regulate the development of the IoT and big data industry."
BIN GU,Workplace flexibility and worker resilience: the role of teleworkability in the COVID-19 pandemic,"The unexpected outbreak of COVID-19 has thrown a spotlight on the importance of telework. With the massive lockdown order, teleworkability, i.e., whether workers’ jobs are teleworkable, plays an important role in determining whether workers can maintain their productivity during the pandemic, which in turn has consequences for their resilience to the COVID-induced labor market disruptions. However, the impact of teleworkability is likely to be heterogeneous, varying by internet infrastructure, job characteristics, and worker characteristics, such as gender. In this paper, we examine the average and heterogeneous impact of teleworkability on workers’ resilience to the COVID-induced labor market disruptions in terms of the unemployment rate, work absence rate, and layoff rate. To do this, we compile a rich dataset, including data on different implementation dates of the stay-at-home order across U.S. counties, employment data from Current Population Survey (CPS), broadband coverage data from Federal Communications Commission (FCC), and occupation-based teleworkability and automatability measure based on surveys from O*NET. Using stay-at-home order as a measure of labor market disruption, and leveraging the staggered implementation of the stay-at-home order across counties, we find that teleworkability can offset the increase in the unemployment rate due to the stay-at-home order by 51.5%, that in work absence rate by 54.9%, and that in layoff rate by 51.7%. We further show that the positive effect of teleworkability on workers’ resilience is i) stronger for those living in areas with higher broadband coverage; ii) stronger for those whose jobs are at risk of being automated; iii) stronger for females without kids than their male counterparts. Our study contributes to the emerging literature on how to enhance societal resilience in facing a pandemic by underscoring the nuanced impact of teleworkability."
BIN GU,The dynamics of online word-of-mouth and product sales—an empirical investigation of the movie industry,"There are growing interests in understanding how word-of-mouth (WOM) on the Internet is generated and how it influences consumers’ purchase decisions at retail outlets. A unique aspect of the WOM effect is the presence of a positive feedback mechanism between WOM and retail sales. We characterize the process through a dynamic simultaneous equation system, in which we separate the effect of online WOM as both a precursor to and an outcome of retail sales. We apply our approach to the movie industry, showing that both a movie's box office revenue and WOM valence significantly influence WOM volume. WOM volume in turn leads to higher box office performance. This positive feedback mechanism highlights the importance of WOM in generating and sustaining retail revenue."
BIN GU,Do online reviews matter? — An empirical investigation of panel data,"This study examines the persuasive effect and awareness effect of online user reviews on movies' daily box office performance. In contrast to earlier studies that take online user reviews as an exogenous factor, we consider reviews both influencing and influenced by movie sales. The consideration of the endogenous nature of online user reviews significantly changes the analysis. Our result shows that the rating of online user reviews has no significant impact on movies' box office revenues after accounting for the endogeneity, indicating that online user reviews have little persuasive effect on consumer purchase decisions. Nevertheless, we find that box office sales are significantly influenced by the volume of online posting, suggesting the importance of awareness effect. The finding of awareness effect for online user reviews is surprising as online reviews under the analysis are posted to the same website and are not expected to increase product awareness. We attribute the effect to online user reviews as an indicator of the intensity of underlying word-of-mouth that plays a dominant role in driving box office revenues."
BIN GU,Justifying contingent information technology investments: balancing the need for speed of action with certainty before action,"Executives need to master different mechanisms for analyzing their firms' investment opportunities in uncertain, difficult times. Rapidly changing business conditions require firms to move quickly, with total commitment and the rapid deployment of capital, resources, and management attention, often in several directions at the same time. However, high levels of strategic uncertainty and environmental risk, combined with limits on available funding, require firms to limit their commitment. In brief, we require high levels of strategic commitment to numerous projects, while simultaneously preserving our flexibility and withholding commitment. Whereas achieving both is clearly impossible, techniques exist that enable executives (1) to identify and to delimit their range of investment alternatives that must be considered, and to do so rapidly and reliably, (2) to divide investments into discrete stages that can be implemented sequentially, (3) to determine which chunks can safely and profitably be developed as strategic options, with value that can be captured when subsequent stage investments are made later; and (4) to quantify and to estimate the value of these strategic options with a significant degree of accuracy, so that selections can be made from a portfolio of investment alternatives. This paper also avoids restrictions of common option valuation models by providing a technique that is general enough to be used when the data required by common models are not available or the assumptions are not satisfied."
BIN GU,Managing artificial intelligence,"Managing artificial intelligence (AI) marks the dawn of a new age of information technology management. Managing AI involves communicating, leading, coordinating, and controlling an ever-evolving frontier of computational advancements that references human intelligence in addressing ever more complex decisionmaking problems. It means making decisions about three related, interdependent facets of AI—autonomy, learning, and inscrutability—in the ongoing quest to push the frontiers of performance and scope of AI. We demonstrate how the frontiers of AI have shifted with time, and explain how the seven exemplar studies included in this special issue are helping us learn about management at the frontiers of AI. We close by speculating about future frontiers in managing AI and what role information systems scholarship has in exploring and shaping this future."
BIN GU,IT workforce research curation,
JANICE M WEINBERG,Prenatal Exposure to Tetrachloroethylene-Contaminated Drinking Water and the Risk of Congenital Anomalies: A Retrospective Cohort Study,"BACKGROUND: Prior animal and human studies of prenatal exposure to solvents including tetrachloroethylene (PCE) have shown increases in the risk of certain congenital anomalies among exposed offspring. OBJECTIVES: This retrospective cohort study examined whether PCE contamination of public drinking water supplies in Massachusetts influenced the occurrence of congenital anomalies among children whose mothers were exposed around the time of conception. METHODS: The study included 1,658 children whose mothers were exposed to PCE-contaminated drinking water and a comparable group of 2,999 children of unexposed mothers. Mothers completed a self-administered questionnaire to gather information on all of their prior births, including the presence of anomalies, residential histories and confounding variables. PCE exposure was estimated using EPANET water distribution system modeling software that incorporated a fate and transport model. RESULTS: Children whose mothers had high exposure levels around the time of conception had an increased risk of congenital anomalies. The adjusted odds ratio of all anomalies combined among children with prenatal exposure in the uppermost quartile was 1.5 (95% CI: 0.9, 2.5). No meaningful increases in the risk were seen for lower exposure levels. Increases were also observed in the risk of neural tube defects (OR: 3.5, 95% CI: 0.8, 14.0) and oral clefts (OR 3.2, 95% CI: 0.7, 15.0) among offspring with any prenatal exposure. CONCLUSION: The results of this study suggest that the risk of certain congenital anomalies is increased among the offspring of women who were exposed to PCE-contaminated drinking water around the time of conception. Because these results are limited by the small number of children with congenital anomalies that were based on maternal reports, a follow-up investigation should be conducted with a larger number of affected children who are identified by independent records."
JANICE M WEINBERG,"Association of urinary phthalate metabolite concentrations with body mass index and waist circumference: a cross-sectional study of NHANES data, 1999–2002","BACKGROUND: Although diet and activity are key factors in the obesity epidemic, laboratory studies suggest that endocrine disrupting chemicals may also affect obesity. METHODS: We analyzed associations between six phthalate metabolites measured in urine and body mass index (BMI) and waist circumference (WC) in National Health and Nutrition Examination Survey (NHANES) participants aged 6–80. We included 4369 participants from NHANES 1999–2002, with data on mono-ethyl (MEP), mono-2-ethylhexyl (MEHP), mono-n-butyl (MBP), and mono-benzyl (MBzP) phthalate; 2286 also had data on mono-2-ethyl-5-hydroxyhexyl (MEHHP) and mono-2-ethyl-5-oxohexyl (MEOHP) phthalate (2001–2002). Using multiple regression, we computed mean BMI and WC within phthalate quartiles in eight age/gender specific models. RESULTS: The most consistent associations were in males aged 20–59; BMI and WC increased across quartiles of MBzP (adjusted mean BMI = 26.7, 27.2, 28.4, 29.0, p-trend = 0.0002), and positive associations were also found for MEOHP, MEHHP, MEP, and MBP. In females, BMI and WC increased with MEP quartile in adolescent girls (adjusted mean BMI = 22.9, 23.8, 24.1, 24.7, p-trend = 0.03), and a similar but less strong pattern was seen in 20–59 year olds. In contrast, MEHP was inversely related to BMI in adolescent girls (adjusted mean BMI = 25.4, 23.8, 23.4, 22.9, p-trend = 0.02) and females aged 20–59 (adjusted mean BMI = 29.9, 29.9, 27.9, 27.6, p-trend = 0.02). There were no important associations among children, but several inverse associations among 60–80 year olds. CONCLUSION: This exploratory, cross-sectional analysis revealed a number of interesting associations with different phthalate metabolites and obesity outcomes, including notable differences by gender and age subgroups. Effects of endocrine disruptors, such as phthalates, may depend upon endogenous hormone levels, which vary dramatically by age and gender. Individual phthalates also have different biologic and hormonal effects. Although our study has limitations, both of these factors could explain some of the variation in the observed associations. These preliminary data support the need for prospective studies in populations at risk for obesity."
JANICE M WEINBERG,Exposure to Polyfluoroalkyl Chemicals and Attention Deficit/Hyperactivity Disorder in U.S. Children 12-15 Years of Age,"BACKGROUND. Polyfluoroalkyl chemicals (PFCs) have been widely used in consumer products. Exposures in the United States and in world populations are widespread. PFC exposures have been linked to various health impacts, and data in animals suggest that PFCs may be potential developmental neurotoxicants. OBJECTIVES. We evaluated the associations between exposures to four PFCs and parental report of diagnosis of attention deficit/hyperactivity disorder (ADHD). METHODS. Data were obtained from the National Health and Nutrition Examination Survey (NHANES) 1999-2000 and 2003-2004 for children 12-15 years of age. Parental report of a previous diagnosis by a doctor or health care professional of ADHD in the child was the primary outcome measure. Perfluorooctane sulfonic acid (PFOS), perfluorooctanoic acid (PFOA), perfluorononanoic acid (PFNA), and perfluorohexane sulfonic acid (PFHxS) levels were measured in serum samples from each child. RESULTS. Parents reported that 48 of 571 children included in the analysis had been diagnosed with ADHD. The adjusted odds ratio (OR) for parentally reported ADHD in association with a 1-μg/L increase in serum PFOS (modeled as a continuous predictor) was 1.03 [95% confidence interval (CI), 1.01-1.05]. Adjusted ORs for 1-μg/L increases in PFOA and PFHxS were also statistically significant (PFOA: OR = 1.12; 95% CI, 1.01-1.23; PFHxS: OR = 1.06; 95% CI, 1.02-1.11), and we observed a nonsignificant positive association with PFNA (OR = 1.32; 95% CI, 0.86-2.02). CONCLUSIONS. Our results, using cross-sectional data, are consistent with increased odds of ADHD in children with higher serum PFC levels. Given the extremely prevalent exposure to PFCs, follow-up of these data with cohort studies is needed."
JANICE M WEINBERG,Association between Residences in U.S. Northern Latitudes and Rheumatoid Arthritis: A Spatial Analysis of the Nurses' Health Study,"BACKGROUND. The etiology of rheumatoid arthritis (RA) remains largely unknown, although epidemiologic studies suggest genetic and environmental factors may play a role. Geographic variation in incident RA has been observed at the regional level. OBJECTIVE. Spatial analyses are a useful tool for confirming existing exposure hypotheses or generating new ones. To further explore the association between location and RA risk, we analyzed individual-level data from U.S. women in the Nurses' Health Study, a nationwide cohort study. METHODS. Participants included 461 incident RA cases and 9,220 controls with geocoded addresses; participants were followed from 1988 to 2002. We examined spatial variation using addresses at baseline in 1988 and at the time of case diagnosis or the censoring of controls. Generalized additive models (GAMs) were used to predict a continuous risk surface by smoothing on longitude and latitude while adjusting for known risk factors. Permutation tests were conducted to evaluate the overall importance of location and to identify, within the entire study area, those locations of statistically significant risk. RESULTS. A statistically significant area of increased RA risk was identified in the northeast United States (p-value = 0.034). Risk was generally higher at northern latitudes, and it increased slightly when we used the nurses' 1988 locations compared with those at the time of diagnosis or censoring. Crude and adjusted models produced similar results. CONCLUSIONS. Spatial analyses suggest women living in higher latitudes may be at greater risk for RA. Further, RA risk may be greater for locations that occur earlier in residential histories. These results illustrate the usefulness of GAM methods in generating hypotheses for future investigation and supporting existing hypotheses."
JANICE M WEINBERG,Renal Hyperfiltration and the Development of Microalbuminuria in Type 1 Diabetes,"OBJECTIVE: The purpose of this study was to examine prospectively whether renal hyperfiltration is associated with the development of microalbuminuria in patients with type 1 diabetes, after taking into account known risk factors. RESEARCH DESIGN AND METHODS: The study group comprised 426 participants with normoalbuminuria from the First Joslin Kidney Study, followed for 15 years. Glomerular filtration rate was estimated by serum cystatin C, and hyperfiltration was defined as exceeding the 97.5th percentile of the sex-specific distribution of a similarly aged, nondiabetic population (134 and 149 ml/min per 1.73 m2 for men and women, respectively). The outcome was time to microalbuminuria development (multiple albumin excretion rate >30 μg/min). Hazard ratios (HRs) for microalbuminuria were calculated at 5, 10, and 15 years. RESULTS: Renal hyperfiltration was present in 24% of the study group and did not increase the risk of developing microalbuminuria. The unadjusted HR for microalbuminuria comparing those with and without hyperfiltration at baseline was 0.8 (95% CI 0.4–1.7) during the first 5 years, 1.0 (0.6–1.7) during the first 10 years, and 0.8 (0.5–1.4) during 15 years of follow-up. The model adjusted for baseline known risk factors including A1C, age at diagnosis of diabetes, diabetes duration, and cigarette smoking resulted in similar HRs. In addition, incorporating changes in hyperfiltration status during follow-up had minimal impact on the HRs for microalbuminuria. CONCLUSION;S Renal hyperfiltration does not have an impact on the development of microalbuminuria in type 1 diabetes during 5, 10, or 15 years of follow-up."
JANICE M WEINBERG,"Spatial analysis of learning and developmental disorders in upper Cape Cod, Massachusetts using generalized additive models","The spatial variability of three indicators of learning and developmental disability (LDD) was assessed for Cape Cod, Massachusetts. Maternal reports of receiving special education services, attention deficit hyperactivity disorder, and educational attainment were available for a birth cohort from 1969-1983. Using generalized additive models and residential history, maps of the odds of LDD were produced that also controlled for known risk factors. While results were not statistically significant, they suggest that children living in certain parts of Cape Cod were more likely to have a LDD. The spatial variation may be due to variation in the physical and social environment."
JANICE M WEINBERG,"Spatial-temporal analysis of breast cancer in upper Cape Cod, Massachusetts","INTRODUCTION. The reasons for elevated breast cancer rates in the upper Cape Cod area of Massachusetts remain unknown despite several epidemiological studies that investigated possible environmental risk factors. Data from two of these population-based case-control studies provide geocoded residential histories and information on confounders, creating an invaluable dataset for spatial-temporal analysis of participants' residency over five decades. METHODS. The combination of statistical modeling and mapping is a powerful tool for visualizing disease risk in a spatial-temporal analysis. Advances in geographic information systems (GIS) enable spatial analytic techniques in public health studies previously not feasible. Generalized additive models (GAMs) are an effective approach for modeling spatial and temporal distributions of data, combining a number of desirable features including smoothing of geographical location, residency duration, or calendar years; the ability to estimate odds ratios (ORs) while adjusting for confounders; selection of optimum degree of smoothing (span size); hypothesis testing; and use of standard software. We conducted a spatial-temporal analysis of breast cancer case-control data using GAMs and GIS to determine the association between participants' residential history during 1947–1993 and the risk of breast cancer diagnosis during 1983–1993. We considered geographic location alone in a two-dimensional space-only analysis. Calendar year, represented by the earliest year a participant lived in the study area, and residency duration in the study area were modeled individually in one-dimensional time-only analyses, and together in a two-dimensional time-only analysis. We also analyzed space and time together by applying a two-dimensional GAM for location to datasets of overlapping calendar years. The resulting series of maps created a movie which allowed us to visualize changes in magnitude, geographic size, and location of elevated breast cancer risk for the 40 years of residential history that was smoothed over space and time. RESULTS. The space-only analysis showed statistically significant increased areas of breast cancer risk in the northern part of upper Cape Cod and decreased areas of breast cancer risk in the southern part (p-value = 0.04; ORs: 0.90–1.40). There was also a significant association between breast cancer risk and calendar year (p-value = 0.05; ORs: 0.53–1.38), with earlier calendar years resulting in higher risk. The results of the one-dimensional analysis of residency duration and the two-dimensional analysis of calendar year and duration showed that the risk of breast cancer increased with increasing residency duration, but results were not statistically significant. When we considered space and time together, the maps showed a large area of statistically significant elevated risk for breast cancer near the Massachusetts Military Reservation (p-value range:0.02–0.05; ORs range: 0.25–2.5). This increased risk began with residences in the late 1940s and remained consistent in size and location through the late 1950s. CONCLUSION. Spatial-temporal analysis of the breast cancer data may help identify new exposure hypotheses that warrant future epidemiologic investigations with detailed exposure models. Our methods allow us to visualize breast cancer risk, adjust for known confounders including age at diagnosis or index year, family history of breast cancer, parity and age at first live- or stillbirth, and test for the statistical significance of location and time. Despite the advantages of GAMs, analyses are for exploratory purposes and there are still methodological issues that warrant further research. This paper illustrates that GAM methods are a suitable alternative to widely-used cluster detection methods and may be preferable when residential histories from existing epidemiological studies are available."
SOLOMON R EISENBERG,Experimental Validation of the Influence of White Matter Anisotropy on the Intracranial EEG Forward Solution,"Forward solutions with different levels of complexity are employed for localization of current generators, which are responsible for the electric and magnetic fields measured from the human brain. The influence of brain anisotropy on the forward solution is poorly understood. The goal of this study is to validate an anisotropic model for the intracranial electric forward solution by comparing with the directly measured 'gold standard'. Dipolar sources are created at known locations in the brain and intracranial electroencephalogram (EEG) is recorded simultaneously. Isotropic models with increasing level of complexity are generated along with anisotropic models based on Diffusion tensor imaging (DTI). A Finite Element Method based forward solution is calculated and validated using the measured data. Major findings are (1) An anisotropic model with a linear scaling between the eigenvalues of the electrical conductivity tensor and water self-diffusion tensor in brain tissue is validated. The greatest improvement was obtained when the stimulation site is close to a region of high anisotropy. The model with a global anisotropic ratio of 10:1 between the eigenvalues (parallel: tangential to the fiber direction) has the worst performance of all the anisotropic models. (2) Inclusion of cerebrospinal fluid as well as brain anisotropy in the forward model is necessary for an accurate description of the electric field inside the skull. The results indicate that an anisotropic model based on the DTI can be constructed non-invasively and shows an improved performance when compared to the isotropic models for the calculation of the intracranial EEG forward solution. ELECTRONIC SUPPLEMENTARY MATERIAL. The online version of this article (doi:10.1007/s10827-009-0205-z) contains supplementary material, which is available to authorized users."
BENJAMIN SOVACOOL,Social media enables people-centric climate action in the hard-to-decarbonise building sector,"The building and construction sector accounts for around 39% of global carbon dioxide emissions and remains a hard-to-abate sector. We use a data-driven analysis of global high-level climate action on emissions reduction in the building sector using 256,717 English-language tweets across a 13-year time frame (2009-2021). Using natural language processing and network analysis, we show that public sentiments and emotions on social media are reactive to these climate policy actions. Between 2009-2012, discussions around green building-led emission reduction efforts were highly influential in shaping the online public perceptions of climate action. From 2013 to 2016, communication around low-carbon construction and energy efficiency significantly influenced the online narrative. More significant interactions on net-zero transition, climate tech, circular economy, mass timber housing and climate justice in 2017-2021 shaped the online climate action discourse. We find positive sentiments are more prominent and recurrent and comprise a larger share of the social media conversation. However, we also see a rise in negative sentiment by 30-40% following popular policy events like the IPCC report launches, the Paris Agreement and the EU Green Deal. With greater online engagement and information diffusion, social and environmental justice topics emerge in the online discourse. Continuing such shifts in online climate discourse is pivotal to a more just and people-centric transition in such hard-to-decarbonise sectors."
BENJAMIN SOVACOOL,Conspiracy spillovers and geoengineering,"Geoengineering techniques such as solar radiation management (SRM) could be part of a future technology portfolio to limit global temperature change. However, there is public opposition to research and deployment of SRM technologies. We use 814,924 English-language tweets containing #geoengineering globally over 13 years (2009-2021) to explore public emotions, perceptions, and attitudes toward SRM using natural language processing, deep learning, and network analysis. We find that specific conspiracy theories influence public reactions toward geoengineering, especially regarding ""chemtrails"" (whereby airplanes allegedly spray poison or modify weather through contrails). Furthermore, conspiracies tend to spillover, shaping regional debates in the UK, USA, India, and Sweden and connecting with broader political considerations. We also find that positive emotions rise on both the global and country scales following events related to SRM governance, and negative and neutral emotions increase following SRM projects and announcements of experiments. Finally, we also find that online toxicity shapes the breadth of spillover effects, further influencing anti-SRM views."
BENJAMIN SOVACOOL,"Decarbonization, population disruption and resource inventories in the global energy transition","We develop a novel approach to analysing decarbonisation strategies by linking global resource inventories with demographic systems. Our 'mine-town systems' approach establishes an empirical basis for examining the spatial extent of the transition and demographic effects of changing energy systems. The research highlights an urgent need for targeted macro-level planning as global markets see a decline in thermal coal and a ramp up of other mining commodities. Our findings suggest that ramping up energy transition metals (ETM) could be more disruptive to demographic systems than ramping down coal. The data shows asymmetry in the distribution of risks: mine-town systems within the United States are most sensitive to coal phase-out, while systems in Australia and Canada are most sensitive to ETM phase-in. A complete phase-out of coal could disrupt demographic systems with a minimum of 33.5 million people, and another 115.7 million people if all available ETM projects enter production."
BENJAMIN SOVACOOL,Ordering theories: typologies and conceptual frameworks for sociotechnical change,"What theories or concepts are most useful at explaining socio technical change? How can - or cannot - these be integrated? To provide an answer, this study presents the results from 35 semi-structured research interviews with social science experts who also shared more than two hundred articles, reports and books on the topic of the acceptance, adoption, use, or diffusion of technology. This material led to the identification of 96 theories and conceptual approaches spanning 22 identified disciplines. The article begins by explaining its research terms and methods before honing in on a combination of fourteen theories deemed most relevant and useful by the material. These are: Sociotechnical Transitions, Social Practice Theory, Discourse Theory, Domestication Theory, Large Technical Systems, Social Construction of Technology, Sociotechnical Imaginaries, Actor-Network Theory, Social Justice Theory, Sociology of Expectations, Sustainable Development, Values Beliefs Norms Theory, Lifestyle Theory, and the Unified Theory of Acceptance and Use of Technology. It then positions these theories in terms of two distinct typologies. Theories can be placed into five general categories of being centered on agency, structure, meaning, relations or norms. They can also be classified based on their assumptions and goals rooted in functionalism, interpretivism, humanism or conflict. The article lays out tips for research methodology before concluding with insights about technology itself, analytical processes associated with technology, and the framing and communication of results. An interdisciplinary theoretical and conceptual inventory has much to offer students, analysts and scholars wanting to study technological change and society."
BENJAMIN SOVACOOL,Determining our climate policy future: expert opinions about negative emissions and solar radiation management pathways,"Negative emissions technologies and solar radiation management techniques could contribute towards climate stability, either by removing carbon dioxide from the atmosphere and storing it permanently or reflecting sunlight away from the atmosphere. Despite concerns about them, such options are increasingly being discussed as crucial complements to traditional climate change mitigation and adaptation. Expectations around negative emissions and solar radiation management and their associated risks and costs shape public and private discussions of how society deals with the climate crisis. In this study, we rely on a large expert survey (N = 74) to critically examine the future potential of both negative emission options (e.g., carbon dioxide removal) and solar radiation management techniques. We designed a survey process that asked a pool of prominent experts questions about (i) the necessity of adopting negative emissions or solar radiation management options, (ii) the desirability of such options when ranked against each other, (iii) estimations of future efficacy in terms of temperature reductions achieved or gigatons of carbon removed, (iv) expectations about future scaling, commercialization, and deployment targets, and (v) potential risks and barriers. Unlike other elicitation processes where experts are more positive or have high expectations about novel options, our results are more critical and cautionary. We find that some options (notably afforestation and reforestation, ecosystem restoration, and soil carbon sequestration) are envisioned frequently as necessary, desirable, feasible, and affordable, with minimal risks and barriers (compared to other options). This contrasts with other options envisaged as unnecessary risky or costly, notably ocean alkalization or fertilization, space-based reflectors, high-altitude sunshades, and albedo management via clouds. Moreover, only the options of afforestation and reforestation and soil carbon sequestration are expected to be widely deployed before 2035, which raise very real concerns about climate and energy policy in the near- to mid-term."
BENJAMIN SOVACOOL,"A fair trade? Expert perceptions of equity, innovation, and public awareness in China's future emissions trading scheme","How can the Chinese emissions trading scheme (ETS) be redesigned or improved to better address issues of fairness and equity, innovation and learning, and awareness and social acceptance? In order to meet its 2030 carbon emission reduction pledges, the Chinese government has announced plans for a fully implementable national carbon ETS after 2020. This scheme is set to become the world's most significant carbon trading market and it could cover half of all Chinese CO2 emissions (as much as 4 billion tons of carbon dioxide). In this study, we qualitatively analyze the Chinese ETS through the lens of three interconnected themes-equity, innovation, and awareness-which are disaggregated into six specific dimensions. We then explore these themes and dimensions with a mixed methods and original research design involving a survey of 68 Chinese experts as well as 34 semi-structured research interviews with respondents from local governments, financial institutions, technology service companies, universities, industries, and civil society groups. We find that uneven economic and social growth could exacerbate any initial permits allocation scheme that could be a cornerstone for an ETS. Substantial technological and institutional uncertainties exist that could also hamper development and enforcement. Low or negative awareness among the public and private sector were identified as also being significant barriers for ETS implementation."
BENJAMIN SOVACOOL,Examining the synergies and tradeoffs of net-zero climate protection with the sustainable development goals,"This article discusses and illuminates the synergies and jeopardies or tradeoffs that exist between the 17 Sustainable Development Goals (SDGs) and net-zero or future climate protection options such as greenhouse gas removal (GGR) technologies and solar radiation management (SRM) deployment approaches, respectively. Through a large-scale expert-interview exercise (N = 125), the study finds firstly that numerous synergies and tradeoffs exist between GGR, SRM, and the SDGs. More specifically, we reveal that GGR deployment could enhance the attainment of 16 of the 17 SDGs, but this comes with possible tradeoffs with 12 of the SDGs. SRM deployment could not only enhance the attainment of 16 of the 17 SDGs, but also create possible tradeoffs with (a different) 12 SDGs. The findings further support the understanding of the complexity of SRM and GGR proposals and help policymakers and industrial pioneers understand, navigate, and benchmark between geoengineering approaches using sustainable development goals."
BENJAMIN SOVACOOL,A research agenda to better understand the human dimensions of energy transitions,"The Social Sciences and Humanities (SSH) have a key role to play in understanding which factors and policies would motivate, encourage and enable different actors to adopt a wide range of sustainable energy behaviours and support the required system changes and policies. The SSH can provide critical insights into how consumers could be empowered to consistently engage in sustainable energy behaviour, support and adopt new technologies, and support policies and changes in energy systems. Furthermore, they can increase our understanding of how organisations such as private and public institutions, and groups and associations of people can play a key role in the sustainable energy transition. We identify key questions to be addressed that have been identified by the Platform for Energy Research in the Socio-economic Nexus (PERSON, see person.eu), including SSH scholars who have been studying energy issues for many years. We identify three main research themes. The first research theme involves understanding which factors encourage different actors to engage in sustainable energy behaviour. The second research theme focuses on understanding which interventions can be effective in encouraging sustainable energy behaviour of different actors, and which factors enhance their effects. The third research theme concerns understanding which factors affect public and policy support for energy policy and changes in energy systems, and how important public concerns can best be addressed as to reduce or prevent resistance."
BENJAMIN SOVACOOL,"Imagining sustainable energy and mobility transitions: valence, temporality, and radicalism in 38 visions of a low-carbon future","Based on an extensive synthesis of semi-structured interviews, media content analysis, and reviews, this article conducts a qualitative meta-analysis of more than 560 sources of evidence to identify 38 visions associated with seven different low-carbon innovations - automated mobility, electric vehicles, smart meters, nuclear power, shale gas, hydrogen, and the fossil fuel divestment movement - playing a key role in current deliberations about mobility or low-carbon energy supply and use. From this material, it analyzes such visions based on rhetorical features such as common problems and functions, storylines, discursive struggles, and rhetorical effectiveness. It also analyzes visions based on typologies or degrees of valence (utopian vs. dystopian), temporality (proximal vs. distant), and radicalism (incremental vs. transformative). The article is motivated by the premise that tackling climate change via low-carbon energy systems (and practices) is one of the most significant challenges of the twenty-first century, and that effective decarbonization will require not only new energy technologies, but also new ways of understanding language, visions, and discursive politics surrounding emerging innovations and transitions."
BENJAMIN SOVACOOL,"Reconfiguration, contestation, and decline: conceptualizing mature large technical systems","Large technical systems (LTS) are integral to modern lifestyles but arduous to analyze. In this paper, we advance a conceptualization of LTS using the notion of mature ""phases,"" drawing from insights into innovation studies, science and technology studies, political science, the sociology of infrastructure, history of technology, and governance. We begin by defining LTS as a unit of analysis and explaining its conceptual utility and novelty, situating it among other prominent sociotechnical theories. Next, we argue that after LTS have moved through the (overlapping) phases proposed by Thomas Hughes of invention, expansion, growth, momentum, and style, mature LTS undergo the additional (overlapping) phases of reconfiguration, contestation (subject to pressures such as drift and crisis), and eventually stagnation and decline. We illustrate these analytical phases with historical case studies and the conceptual literature, and close by suggesting future research to refine and develop the LTS framework, particularly related to more refined typologies, temporal dimensions, and a broadening of system users. We aim to contribute to theoretical debates about the coevolution of LTS as well as empirical discussions about system-related use, sociotechnical change, and policy-making."
FALLOU NGOM,"Images of inside the home of manuscript owner, El-hadji Lamine Bayo","Images of inside the home of manuscript owner, El-hadji Lamine Bayo, in Belfort, Ziguinchor, Senegal."
FALLOU NGOM,Responses from El-hadji Lamine Bayo interview (listing),"Responses from the interview conducted by Mr Ibrahima Yaffa between the manuscript owner, Abdou Karim Thiam, and and Arabiatou Biaye (second wife to manuscript author Nimbaly Thiam) describing the relationship between the manuscripts with people and places, and detailed information about the original physical collection (per manuscript)."
FALLOU NGOM,Images of digitization work at Abdou Khader Cisse's home,"Images and a short video clip of the manuscript digitization work done in January 2018 in the home of the manuscript owner (Abdou Khadre Cisse) in the neighborhood of Kandialang in Ziguinchor, Senegal."
FALLOU NGOM,Images taken inside of Abdou Khader Cisse's home,"Images taken in the home of the manuscript owner (Abdou Khadre Cisse) in the neighborhood of Kandialang in Ziguinchor, Senegal, for the manuscript digitization work done in January 2018."
FALLOU NGOM,Images taken outside of Abdou Khader Cisse's home,"Images taken outside of the home of the manuscript owner (Abdou Khadre Cisse) in the neighborhood of Kandialang in Ziguinchor, Senegal, for the manuscript digitization work done in January 2018."
FALLOU NGOM,"Digital Preservation of Mandinka Ajami Materials of Casamance, Senegal","This poster outlines the international research collaboration between Boston University, the West African Research Center, and local experts in Senegal during a 15 month project --funded by the British Library’s Endangered Archives Programme-- to digitize, curate, and preserve over 14,000 manuscript pages written in Mandinka Ajami and Arabic. Technical workflows and challenges, as well as future research opportunities in this area will be discussed. Ultimately, research outputs from this project, along with documentation will be archived and made available at all three institutions, including the ""African Ajami Library"", an open access repository of aggregated Ajami materials from all over the continent."
FALLOU NGOM,"Images outside of the home of manuscript owner, El-hadji Lamine Bayo","Images taken outside of the home of manuscript owner, El-hadji Lamine Bayo, in Belfort, Ziguinchor, Senegal."
FALLOU NGOM,Digitization images of El-hadji Lamine Bayo's manuscript collection,"Images of the manuscript digitization and interview work done on January 9, 2018 with manuscript owner, El-hadji Lamine Bayo, at the Hotel Nema-Kadior in Ziguinchor, Senegal."
FALLOU NGOM,West African manuscripts in Arabic and African languages and digital preservation,"West African manuscripts are numerous and varied in forms and contents. There are thousands of them across West Africa. A significant portion of them are documents written in Arabic and Ajami (African languages written in Arabic script). They deal with both religious and nonreligious subjects. The development of these manuscript traditions dates back to the early days of Islam in West Africa, in the 11th century. In addition to these Arabic and Ajami manuscripts, there have been others written in indigenous scripts. These include those in the Vai script invented in Liberia; Tifinagh, the traditional writing system of the Amazigh (Berber) people; and the N’KO script invented in Guinea for Mande languages. While the writings in indigenous scripts are rare less numerous and widespread, they nonetheless constitute an important component of West Africa’s written heritage. Though the efforts devoted to the preservation of West African manuscripts are limited compared to other world regions, interest in preserving them has increased. Some of the initial preservation efforts of West African manuscripts are the collections of colonial officers. Academics later supplemented these collections. These efforts resulted in important print and digital repositories of West African manuscripts in Africa, Europe, and America. Until recently, most of the cataloguing and digital preservation efforts of West African manuscripts have focused on those written in Arabic. However, there has been an increasing interest in West African manuscripts written in Ajami and indigenous scripts. Important West African manuscripts in Arabic, Ajami, and indigenous scripts have now been digitized and preserved, though the bulk remain uncatalogued and unknown beyond the communities of their owners."
FALLOU NGOM,Images taken inside of Abdou Karim Thiam's home,"Images taken in the home of the manuscript owner (Abdou Karim Thiam) in the neighborhood of Kandialang in Ziguinchor, Senegal, for the manuscript digitization work done in January 2018."
FALLOU NGOM,Responses from Abdou Karim Thiam interview (listing),"Responses from the interview conducted by Mr Ibrahima Yaffa between the manuscript owner, Abdou Karim Thiam, and and Arabiatou Biaye (second wife to manuscript author Nimbaly Thiam) describing the relationship between the manuscripts with people and places, and detailed information about the original physical collection (per manuscript)."
FALLOU NGOM,Images of digitization work at Abdou Karim Thiam's home,"Images of the manuscript digitization and interview work done in January 2018 in the home of the manuscript owner (Abdou Karim Thiam) in the neighborhood of Kandialang in Ziguinchor, Senegal."
FALLOU NGOM,Images taken outside of Abdou Karim Thiam's home,"Images taken outside of the home of the manuscript owner (Abdou Karim Thiam) in the neighborhood of Kandialang in Ziguinchor, Senegal, for the manuscript digitization work done in January 2018."
FALLOU NGOM,Responses from Abdou Khadre Cisse interview (listing),"Responses from the interview conducted by Mr Ibrahima Yaffa between the manuscript owner, Abdou Khadre Cisse and his brother Cherif Cisse describing the relationship between the manuscripts with people and places, and detailed information about the original physical collection (per manuscript)."
FALLOU NGOM,"Africa 2060: good news from Africa, April 16, 2010","This report provides commentary reflecting upon and information pertaining to the substance of the conference. An introductory overview looks at the major issues discussed at the event, which are placed within the larger literature on Africa’s future. Four short essays prepared by Boston University graduate students provide readers with more specific reflections and highlights of each conference session and the main issues discussed by panelists. The final section presents analyses of key trends and projections related to societal, economic, and governance issues for Africa and a commentary on what this information tells us about the drivers that will determine the continent’s future."
FALLOU NGOM,Beyond African orality: digital preservation of Mandinka ʿAjamī archives of Casamance,"This article focuses on the digital preservation of African sources written in Mandinka ʿAjamī, i.e. the enriched form of the Arabic script used to write the Mandinka language for centuries. ʿAjamī writing has been utilized to document intellectual traditions, histories, belief systems, and cultures of non-Arab Muslims around the world. ʿAjamī texts have played critical roles in the spread of Islam in Africa and continue to be used for both religious and non-religious writings. However, African ʿAjamī texts such as those of the Mandinka people of Casamance in southern Senegal are not well known beyond local communities. ʿAjamī texts in Mandinka and other Mande languages are among the least documented. Only a few Mande ʿAjamī texts are available to scholars. Thanks to the British Library’s Endangered Archives Programme (EAP), Africa’s rich written heritage in ʿAjamī and other scripts previously unavailable to academics is being preserved and made universally accessible."
FALLOU NGOM,Photographs from WARC Digitization Workshop,"Photographs taken during the WARC Digitization Workshop from January 4-6, 2018 in Dakar, Senegal."
FALLOU NGOM,"Mamadou Lo, Un aspect de la poésie “Wolofal” Mouride: l’éducation morale et spirituelle de l’Aspirant (al Murid) dans la production de Sëriñ Mbay Jaxate","The book’s author, Mamadou Lo, has a dual education. He is as well versed in the Senegalese French-based education system as he is in the Murid Islamic education system. He has served as a humanities teacher in the Senegalese education system and as an Education and Training Inspector until his retirement. He is one of the early members of the Hizbut Tarqiyyah, a Murid organization born out of the Murid students’ organization called Dahira des Étudiants Mourides de l’ucad (Université Cheikh Anta Diop), which was founded in the 1980s. He joined the organization in the 1990s."
FALLOU NGOM,Ajami scripts in the Senegalese speech community,"Wolofal (from Wolof: Wolof language or ethnic group and ‘-al’: causative morpheme) is an Ajami writing (a generic term commonly used to refer to non-Arabic languages written with Arabic scripts) used to transliterate Wolof in Senegal.It results from the early Islamization of the major Muslim ethnic groups in the country, especially the Pulaar, the Wolof and the Mandinka. Although Senegal is considered to be a French-speaking country, ironically over 50% of the Senegalese people are thought to be illiterate in French. French literacy is restricted to the minority educated group mostly found in urban areas. Because the literacy rate in French is very small in the country, especially among older people, Wolofal remains a major means of written communication among people who are illiterate in French and who have attended Qurʾānic schools. It is used by these people to write letters, run their informal businesses and read religious poems and writings. This paper is based upon fieldwork conducted in Senegal in the summer of 2004. It discusses the orthographic system of Wolofal (com-pared to Arabic) and provides a sociolinguistic profile of communities in which it serves as major means of written communication."
FALLOU NGOM,Ajami sources and knowledge production about Africa in the 21st century,"The emergence of Ajami traditions in Africa mirrors the development of traditions of writing European languages based on the Latin orthography. Just like the Latin script spread throughout the world through Christianity and was modified to write numerous European languages, so too the Arabic script spread through Islam and was modified to write numerous African languages. Many Ajami traditions initially emerged as part of the pedagogies to disseminate Islam to the illiterate African masses. However, their usage expanded to encompass other areas of knowledge, just as the Latin script flourished from the church environment to encompass other secular domains of knowledge of different European communities that had modified the script to meet their written communication needs. Recent discoveries indicate that West African Ajami traditions go as far back as the sixteenth century. The Berber Ajami tradition is thought to have begun at least centuries earlier. The materials that emerged in Ajami traditions represent an important and underexplored source of knowledge on Africa. They are rich and varied and encompass both religious and secular manuscripts. The religious materials include prayers, talismanic protective devices, didactic materials in poetry and prose, elegies, hagiographies, translations of works on Islamic metaphysics, jurisprudence, Sufism, and translations of the Quran into African languages. The non‐religious documents encompass commercial and administrative record‐keeping, family genealogies, records of local events (such as foundations of villages, births, deaths, and weddings), biographies, political and social satires, advertisements, road signs, public announcements and speeches, personal correspondences, traditional treatment of illnesses and medicinal plants, local customs and traditions, and texts on diplomatic matters and history. Pre‐colonial Ajami documents are difficult to find, partly because many have not survived due to poor conservation conditions, and partly because of their neglect due to the enduring emphasis on European colonial archives and Arabic sources. The recently uncovered Ajami materials were largely produced during the colonial and post‐colonial era. They provide fresh insights on various aspects of local histories, cultures, and belief systems. In this paper I will focus on selected West Ajami materials (which include chronograms commonly used to date local events, genealogies, and diplomatic correspondences) to demonstrate their potential to enrich and advance scholarly inquiries on Africa in the 21st century."
FALLOU NGOM,Daily steps in getting ready to photograph manuscripts,Handout documenting each step needed to prepare camera to digitize manuscript pages in the field. Prepared for the WARC Digitization Workshop.
FALLOU NGOM,Questions for manuscript owners and their collections (EAP 1042),List of questions to be asked of each manuscript owner about their collection of manuscripts.
FALLOU NGOM,"The Trans-Saharan Book Trade: Manuscript Culture, Arabic Literacy and Intellectual History in Muslim Africa, edited by Graziano Krätli and Ghislaine Lydon",
FALLOU NGOM,Fallou Ngom and Roger Brisson leading a digitization training at the West African Research Center,"Roger Brisson (4th from left) and Fallou Ngom (standing right) at the West African Research Center in Dakar, training the Senegal fieldwork team for the digital preservation of Wolof Ajami manuscripts. This project (EAP 334) was funded by the British Library's Endangered Archives Programme <https://eap.bl.uk/project/EAP334>."
FALLOU NGOM,"Introduction:ʿAjamī literacies of Africa: the Hausa, Fula, Mandinka, and Wolof traditions","African ʿAjamī literatures hold a wealth of knowledge on the history and intellectual traditions of the region but are largely unknown to the larger public. Our special issue seeks to enhance a broader understanding of this important part of the Islamic world, exploring the ʿAjamī literatures and literacies of four main language groups of Muslim West Africa: Hausa, Mandinka, Fula, and Wolof. Through increasing access to primary sources in ʿAjamī and utilizing an innovative multimedia approach, our research contributes to an interpretive and comparative analysis of African ʿAjamī literacy, with its multiple purposes, forms, and custodians. Our Editorial Introduction to the special issue discusses the building blocks and historical development of ʿAjamī cultures in West Africa, outlines the longitudinal collaborative research initiatives that our special issue draws upon, and explores the challenges and opportunities for participatory knowledge-making that accompany the rise of digital technologies in the study of African literatures and literacies."
JAIMIE GRADUS,The Association between Adjustment Disorder Diagnosed at Psychiatric Treatment Facilities and Completed Suicide,"Adjustment disorder is a diagnosis given following a significant psychosocial stressor from which an individual has difficulty recovering. The individual's reaction to this event must exceed what would be observed among similar people experiencing the same stressor. Adjustment disorder is associated with suicidal ideation and suicide attempt. However the association between adjustment disorder and completed suicide has yet to be examined. The current study is a population-based case control study examining this association in the population of Denmark aged 15 to 90 years. All suicides in Denmark from 1994 to 2006 were included, resulting in 9,612 cases. For each case, up to 30 controls were matched on gender, exact date of birth, and calendar time, yielding 199,306 controls. Adjustment disorder diagnosis was found in 7.6% of suicide cases and 0.52% of controls. Conditional logistic regression analyses revealed that those diagnosed with adjustment disorder had 12 times the rate of suicide as those without an adjustment disorder diagnosis, after controlling for history of depression diagnosis, marital status, income, and the matched factors."
WILLIAM W GRIMES,"International remittance rails as infrastructures: embeddedness, innovation and financial access in developing economies","Remittances to developing economies constitute one of their most important and consistent forms of capital inflow, but have long been limited by costs and risks associated with trans-border payments. New digital platforms lower these, with significant implications for financial inclusion and economic development. Constituting an important element of developing countries’ engagement with international finance, remittances engender new opportunities for empowerment and vulnerability. This article analyzes recent developments in international remittances to developing countries through the lens of infrastructure. The infrastructural perspective reveals important junction points between diverse money transfer pathways and institutions, depicting their spatial configuration and relationality as well as their potential to affect power differentials, and allowing for a socially embedded view of digital disruption. Drawing on examples in Africa and Asia, we show that the new generation of remittance infrastructures are best understood as assemblages of multiple elements, conjoining monopolistic trunks that depend on local innovations to traverse the ‘last mile’ to reach end-users. The vibrancy and indispensability of local networks and innovation, along with competition among core platforms, allow for significant agency and economic opportunity even among communities beset by poverty."
WILLIAM W GRIMES,Leaving the nest: the rise of regional financial arrangements and the future of global governance,"This article examines the impact of regional financial arrangements (RFAs) on the global liquidity regime. It argues that the design of RFAs could potentially alter the global regime, whether by strengthening it and making it more coherent or by decentring the International Monetary Fund (IMF) and destabilizing it. To determine possible outcomes, this analysis deploys a ‘middle‐up’ approach that focuses on the institutional design of these RFAs. It first draws on the rational design of institutions framework to identify the internal characteristics of RFAs that are most relevant to their capabilities and capacities. It then applies these insights to the interactions of RFAs with the IMF, building on Aggarwal's (1998) concept of ‘nested’ versus ‘parallel’ institutions, to create an analytical lens through which to assess the nature and sustainability of nested linkages. Through an analysis of the Chiang Mai Initiative Multilateralization (CMIM) and the Latin American Reserve Fund (FLAR), the article demonstrates the usefulness of this lens. It concludes by considering three circumstances in which fault lines created by these RFAs’ institutional design could be activated, permitting an institution to ‘leave the nest’, including changing intentions of principals, creation of parallel capabilities and facilities, and failure of the global regime to address regional needs in a crisis."
WILLIAM W GRIMES,Manifesting the embedded developmental state: the role of South Korea’s National Pension Service in managing financial crisis,"Financial liberalization has noticeably reduced the role of the state in effectively influencing the economy in post-developmental states. Yet many studies have found that the legacies of the developmental model continue to influence the policies, institutions, and socioeconomic challenges that are faced by the states that previously adopted the model. These studies, however, do not clearly identify when and how such legacies may be manifested in state behavior. This paper contributes to filling this gap in the literature by arguing that financial crises can serve as a trigger to more clearly reveal the structural evidence of the legacy in institutions that were previously established and utilized for developmental objectives. By conducting a rigorous case analysis using historical and market data on the crisis responses of South Korea’s public pension fund, this paper finds that South Korea’s developmental legacy remains passively embedded in the governance structure of the pension fund in non-crisis times but manifests during financial crises."
WILLIAM W GRIMES,The varieties of financial statecraft and middle powers: assessing South Korea’s strategic involvement in regional financial cooperation,"In recent years, the financial statecraft literature has expanded from a focus on great powers to encompass the behavior of emerging powers. While offering an important corrective, the literature does not yet adequately address the full variety of the emerging powers’ strategies of financial statecraft. In particular, we argue that regional middle powers behave differently from regional great powers even when they have similar capacities at the global level. For instance, both India and South Korea are categorized as emerging powers in the financial statecraft literature and deploy regional strategies to reduce their financial vulnerability. Yet their financial statecraft strategies have clearly differed in practice. India has sought to challenge the global status quo and influence its neighbors, while South Korea has pursued more modest and defensive goals. Drawing on the middle power literature, we posit that middle powers’ relative position within their home regions explains such differences among the financial statecraft of emerging powers. To demonstrate the utility of this approach, we examine South Korea’s financial statecraft in the Asia-Pacific region. We find that its position as a regional middle power effectively explains its patterns of bilateral and regional cooperation in the monetary sphere."
DONALD M THEA,"Effectiveness of 4% chlorhexidine umbilical cord care on neonatal mortality in Southern Province, Zambia (ZamCAT): a cluster-randomised controlled trial","BACKGROUND: Chlorhexidine umbilical cord washes reduce neonatal mortality in south Asian populations with high neonatal mortality rates and predominantly home-based deliveries. No data exist for sub-Saharan African populations with lower neonatal mortality rates or mostly facility-based deliveries. We compared the e ect of chlorhexidine with dry cord care on neonatal mortality rates in Zambia. METHODS: We undertook a cluster-randomised controlled trial in Southern Province, Zambia, with 90 health facility- based clusters. We enrolled women who were in their second or third trimester of pregnancy, aged at least 15 years, and who would remain in the catchment area for follow-up of 28 days post-partum. Newborn babies received clean dry cord care (control) or topical application of 10 mL of a 4% chlorhexidine solution once per day until 3 days after cord drop (intervention), according to cluster assignment. We used strati ed, restricted randomisation to divide clusters into urban or two rural groups (located <40 km or ≥40 km to referral facility), and randomly assigned clusters (1:1) to use intervention (n=45) or control treatment (n=45). Sites, participants, and eld monitors were aware of their study assignment. The primary outcomes were all-cause neonatal mortality within 28 days post-partum and all-cause neonatal mortality within 28 days post-partum among babies who survived the rst 24 h of life. Analysis was by intention to treat. Neonatal mortality rate was compared with generalised estimating equations. This study is registered at http://ClinicalTrials.gov (NCT01241318). FINDINGS: From Feb 15, 2011, to Jan 30, 2013, we screened 42356 pregnant women and enrolled 39679 women (mean 436·2 per cluster [SD 65·3]), who had 37856 livebirths and 723 stillbirths; 63·8% of deliveries were facility-based. Of livebirths, 18 450 (99·7%) newborn babies in the chlorhexidine group and 19 308 (99·8%) newborn babies in the dry cord care group were followed up to day 28 or death. 16660 (90·0%) infants in the chlorhexidine group had chlorhexidine applied within 24 h of birth. We found no signi cant di erence in neonatal mortality rate between the chlorhexidine group (15·2 deaths per 1000 livebirths) and the dry cord care group (13·6 deaths per 1000 livebirths; risk ratio [RR] 1·12, 95% CI 0·88–1·44). Eliminating day 0 deaths yielded similar ndings (RR 1·12, 95% CI 0·86–1·47). INTERPRETATION: Despite substantial reductions previously reported in south Asia, chlorhexidine cord applications did not signi cantly reduce neonatal mortality rates in Zambia. Chlorhexidine cord applications do not seem to provide clear bene ts for newborn babies in settings with predominantly facility-based deliveries and lower (<30 deaths per 1000 livebirths) neonatal mortality rates."
DONALD M THEA,Restriction of HIV-1 Genotypes in Breast Milk Does Not Account for the Population Transmission Genetic Bottleneck That Occurs following Transmission,"BACKGROUND. Breast milk transmission of HIV-1 remains a major route of pediatric infection. Defining the characteristics of viral variants to which breastfeeding infants are exposed is important for understanding the genetic bottleneck that occurs in the majority of mother-to-child transmissions. The blood-milk epithelial barrier markedly restricts the quantity of HIV-1 in breast milk, even in the absence of antiretroviral drugs. The basis of this restriction and the genetic relationship between breast milk and blood variants are not well established. METHODOLOGY/PRINCIPAL FINDINGS. We compared 356 HIV-1 subtype C gp160 envelope (env) gene sequences from the plasma and breast milk of 13 breastfeeding women. A trend towards lower viral population diversity and divergence in breast milk was observed, potentially indicative of clonal expansion within the breast. No differences in potential N-linked glycosylation site numbers or in gp160 variable loop amino acid lengths were identified. Genetic compartmentalization was evident in only one out of six subjects in whom contemporaneously obtained samples were studied. However, in samples that were collected 10 or more days apart, six of seven subjects were classified as having compartmentalized viral populations, highlighting the necessity of contemporaneous sampling for genetic compartmentalization studies. We found evidence of CXCR4 co-receptor using viruses in breast milk and blood in nine out of the thirteen subjects, but no evidence of preferential localization of these variants in either tissue. CONCLUSIONS/SIGNIFICANCE. Despite marked restriction of HIV-1 quantities in milk, our data indicate intermixing of virus between blood and breast milk. Thus, we found no evidence that a restriction in viral genotype diversity in breast milk accounts for the genetic bottleneck observed following transmission. In addition, our results highlight the rapidity of HIV-1 env evolution and the importance of sample timing in analyses of gene flow."
DONALD M THEA,4E10-Resistant HIV-1 Isolated from Four Subjects with Rare Membrane-Proximal External Region Polymorphisms,"Human antibody 4E10 targets the highly conserved membrane-proximal external region (MPER) of the HIV-1 transmembrane glycoprotein, gp41, and has extraordinarily broad neutralizing activity. It is considered by many to be a prototype for vaccine development. In this study, we describe four subjects infected with viruses carrying rare MPER polymorphisms associated with resistance to 4E10 neutralization. In one case resistant virus carrying a W680G substitution was transmitted from mother to infant. We used site-directed mutagenesis to demonstrate that the W680G substitution is necessary for conferring the 4E10-resistant phenotype, but that it is not sufficient to transfer the phenotype to a 4E10-sensitive Env. Our third subject carried Envs with a W680R substitution causing variable resistance to 4E10, indicating that residues outside the MPER are required to confer the phenotype. A fourth subject possessed a F673L substitution previously associated with 4E10 resistance. For all three subjects with W680 polymorphisms, we observed additional residues in the MPER that co-varied with position 680 and preserved charged distributions across this region. Our data provide important caveats for vaccine development targeting the MPER. Naturally occurring Env variants described in our study also represent unique tools for probing the structure-function of HIV-1 envelope."
DONALD M THEA,The Mucosae-Associated Epithelial Chemokine (MEC/CCL28) Modulates Immunity in HIV Infection,BACKGROUND. CCL28 (MEC) binds to CCR3 and CCR10 and recruits IgA-secreting plasma cells (IgA-ASC) in the mucosal lamina propria (MLP). Mucosal HIV-specific IgA are detected in HIV-infection and exposure. The CCL28 circuit was analyzed in HIV-infected and-exposed individuals and in HIV-unexposed controls; the effect of CCL28 administration on gastrointestinal MLP IgA-ASC was verified in a mouse model. METHODOLOGY/FINDINGS. CCL28 was augmented in breast milk (BM) plasma and saliva of HIV-infected and –exposed individuals; CCR3+ and CCR10+ B lymphocytes were increased in these same individuals. Additionally: 1) CCL28 concentration in BM was associated with longer survival in HIV vertically-infected children; and 2) gastro-intestinal mucosal IgA-ASC were significantly increased in VSV-immunized mice receiving CCL28. CONCLUSIONS. CCL28 mediates mucosal immunity in HIV exposure and infection. CCL28-including constructs should be considered in mucosal vaccines to prevent HIV infection of the gastro-intestinal MLP via modulation of IgA-ASC.
DONALD M THEA,Differential Effects of Early Weaning for HIV-Free Survival of Children Born to HIV-Infected Mothers by Severity of Maternal Disease,"BACKGROUND. We previously reported no benefit of early weaning for HIV-free survival of children born to HIV-infected mothers in intent-to-treat analyses. Since early weaning was poorly accepted, we conducted a secondary analysis to investigate whether beneficial effects may have been hidden. METHODS. 958 HIV-infected women in Lusaka, Zambia, were randomized to abrupt weaning at 4 months (intervention) or to continued breastfeeding (control). Children were followed to 24 months with regular HIV PCR tests and examinations to determine HIV infection or death. Detailed behavioral data were collected on when all breastfeeding ended. Most participants were recruited before antiretroviral treatment (ART) became available. We compared outcomes among mother-child pairs who weaned earlier or later than intended by study design adjusting for potential confounders. RESULTS. Of infants alive, uninfected and still breastfeeding at 4 months in the intervention group, 16.1% who weaned as instructed acquired HIV or died by 24 months compared to 16.0% who did not comply (p=0.98). Children of women with less severe disease during pregnancy (not eligible for ART) had worse outcomes if their mothers weaned as instructed (RH=2.60 95% CI: 1.06-6.36) compared to those who continued breastfeeding. Conversely, children of mothers with more severe disease (eligible for ART but did not receive it) who weaned early had better outcomes (p-value interaction=0.002). In the control group, weaning before 15 months was associated with 3.94-fold (95% CI: 1.65-9.39) increase in HIV infection or death among infants of mothers with less severe disease. CONCLUSION. Incomplete adherence did not mask a benefit of early weaning. On the contrary, for women with less severe disease, early weaning was harmful and continued breastfeeding resulted in better outcomes. For women with more advanced disease, ART should be given during pregnancy for maternal health and to reduce transmission, including through breastfeeding. TRIAL REGISTRATION. Clinical trials.gov NCT00310726"
DONALD M THEA,High Uptake of Exclusive Breastfeeding and Reduced Early Post-Natal HIV Transmission,"BACKGROUND. Empirical data showing the clear benefits of exclusive breastfeeding (EBF) for HIV prevention are needed to encourage implementation of lactation support programs for HIV-infected women in low resource settings among whom replacement feeding is unsafe. We conducted a prospective, observational study in Lusaka, Zambia, to test the hypothesis that EBF is associated with a lower risk of postnatal HIV transmission than non-EBF. METHODS AND RESULTS. As part of a randomized trial of early weaning, 958 HIV-infected women and their infants were recruited and all were encouraged to breastfeed exclusively to 4 months. Single-dose nevirapine was provided to prevent transmission. Regular samples were collected from infants to 24 months of age and tested by PCR. Detailed measurements of actual feeding behaviors were collected to examine, in an observational analysis, associations between feeding practices and postnatal HIV transmission. Uptake of EBF was high with 84% of women reporting only EBF cumulatively to 4 months. Post-natal HIV transmission before 4 months was significantly lower (p = 0.004) among EBF (0.040 95% CI: 0.024–0.055) than non-EBF infants (0.102 95% CI: 0.047–0.157); time-dependent Relative Hazard (RH) of transmission due to non-EBF = 3.48 (95% CI: 1.71–7.08). There were no significant differences in the severity of disease between EBF and non-EBF mothers and the association remained significant (RH = 2.68 95% CI: 1.28–5.62) after adjusting for maternal CD4 count, plasma viral load, syphilis screening results and low birth weight. CONCLUSIONS. Non-EBF more than doubles the risk of early postnatal HIV transmission. Programs to support EBF should be expanded universally in low resource settings. EBF is an affordable, feasible, acceptable, safe and sustainable practice that also reduces HIV transmission providing HIV-infected women with a means to protect their children's lives. TRIAL REGISTRATION. ClinicalTrials.gov NCT00310726"
DONALD M THEA,Community Case Management of Fever Due to Malaria and Pneumonia in Children Under Five in Zambia: A Cluster Randomized Controlled Trial,"In a cluster randomized trial, Kojo Yeboah-Antwi and colleagues find that integrated management of malaria and pneumonia in children under five by community health workers is both feasible and effective. BACKGROUND. Pneumonia and malaria, two of the leading causes of morbidity and mortality among children under five in Zambia, often have overlapping clinical manifestations. Zambia is piloting the use of artemether-lumefantrine (AL) by community health workers (CHWs) to treat uncomplicated malaria. Valid concerns about potential overuse of AL could be addressed by the use of malaria rapid diagnostics employed at the community level. Currently, CHWs in Zambia evaluate and treat children with suspected malaria in rural areas, but they refer children with suspected pneumonia to the nearest health facility. This study was designed to assess the effectiveness and feasibility of using CHWs to manage nonsevere pneumonia and uncomplicated malaria with the aid of rapid diagnostic tests (RDTs). METHODS AND FINDINGS. Community health posts staffed by CHWs were matched and randomly allocated to intervention and control arms. Children between the ages of 6 months and 5 years were managed according to the study protocol, as follows. Intervention CHWs performed RDTs, treated test-positive children with AL, and treated those with nonsevere pneumonia (increased respiratory rate) with amoxicillin. Control CHWs did not perform RDTs, treated all febrile children with AL, and referred those with signs of pneumonia to the health facility, as per Ministry of Health policy. The primary outcomes were the use of AL in children with fever and early and appropriate treatment with antibiotics for nonsevere pneumonia. A total of 3,125 children with fever and/or difficult/fast breathing were managed over a 12-month period. In the intervention arm, 27.5% (265/963) of children with fever received AL compared to 99.1% (2066/2084) of control children (risk ratio 0.23, 95% confidence interval 0.14–0.38). For children classified with nonsevere pneumonia, 68.2% (247/362) in the intervention arm and 13.3% (22/203) in the control arm received early and appropriate treatment (risk ratio 5.32, 95% confidence interval 2.19–8.94). There were two deaths in the intervention and one in the control arm. CONCLUSIONS. The potential for CHWs to use RDTs, AL, and amoxicillin to manage both malaria and pneumonia at the community level is promising and might reduce overuse of AL, as well as provide early and appropriate treatment to children with nonsevere pneumonia."
DONALD M THEA,Why do Nigerian manufacturing firms take action on AIDS?,"OBJECTIVE: To identify differences between manufacturing firms in Nigeria that have undertaken HIV/AIDS prevention activities and those that have not as a step toward improving the targeting of HIV policies and interventions. METHODS: A survey of a representative sample of registered manufacturing firms in Nigeria, stratified by location, workforce size, and industrial sector. The survey was administered to managers of 232 firms representing most major industrial areas and sectors in March-April 2001. RESULTS: 45.3 percent of the firms’ managers received information about HIV/AIDS from a source outside the firm in 2000; 7.7 percent knew of an employee who was HIV-positive at the time of the survey; and 13.6 percent knew of an employee who had left the firm and/or died in service due to AIDS. Only 31.7 percent of firms took any action to prevent HIV among employees in 2000, and 23.9 percent had discussed the epidemic as a potential business concern. The best correlates of having taken action on HIV were knowledge of an HIV-positive employee or having lost an employee to AIDS (odds ratio [OR] 6.36, 95% confidence interval [CI]: 2.30, 17.57) and receiving information about the disease from an outside source (OR 7.83, 95% CI: 3.46, 17.69). CONCLUSIONS: Despite a nationwide HIV seroprevalence of 5.8 percent, as of 2001 most Nigerian manufacturing firm managers did not regard HIV/AIDS as a serious problem and had neither taken any action on it nor discussed it as a business issue. Providing managers with accurate, relevant information about the epidemic and practical prevention interventions might strengthen the business response to AIDS in countries like Nigeria."
JUDITH G GONYEA,Fractured identity: a framework for understanding young Asian American women's self-harm and suicidal behaviors,"Despite the high suicide rate among young Asian American women, the reasons for this phenomenon remain unclear. This qualitative study explored the family experiences of 16 young Asian American women who are children of immigrants and report a history of self-harm and/or suicidal behaviors. Our findings suggest that the participants experienced multiple types of ""disempowering parenting styles"" that are characterized as: abusive, burdening, culturally disjointed, disengaged, and gender-prescriptive parenting. Tied to these family dynamics is the double bind that participants suffer. Exposed to multiple types of negative parenting, the women felt paralyzed by opposing forces, caught between a deep desire to satisfy their parents' expectations as well as societal expectations and to simultaneously rebel against the image of ""the perfect Asian woman."" Torn by the double bind, these women developed a ""fractured identity,"" which led to the use of ""unsafe coping"" strategies. Trapped in a ""web of pain,"" the young women suffered alone and engaged in self-harm and suicidal behaviors."
IAIN COCKBURN,The economics of reproducibility in preclinical research,"Low reproducibility rates within life science research undermine cumulative knowledge production and contribute to both delays and costs of therapeutic drug development. An analysis of past studies indicates that the cumulative (total) prevalence of irreproducible preclinical research exceeds 50%, resulting in approximately US$28,000,000,000 (US$28B)/year spent on preclinical research that is not reproducible—in the United States alone. We outline a framework for solutions and a plan for long-term improvements in reproducibility rates that will help to accelerate the discovery of life-saving therapies and cures."
STEVEN DEAN,The pulsating white dwarf G117-B15A: still the most stable optical clock known,"The pulsating hydrogen atmosphere white dwarf star G 117-B15A has been observed since 1974. Its main pulsation period at 215.19738823(63) s, observed in optical light curves, varies by only (5.12 ± 0.82) × 10−15 s s−1 and shows no glitches, as pulsars do. The observed rate of period change corresponds to a change of the pulsation period by 1 s in 6.2 million yr. We demonstrate that this exceptional optical clock can continue to put stringent limits on fundamental physics, such as constraints on interaction from hypothetical dark matter particles, as well as to search for the presence of external substellar companions."
STEVEN DEAN,The concept of terminal vascular patterns,
STEVEN DEAN,Butterfly genome reveals promiscuous exchange of mimicry adaptations among species,"The evolutionary importance of hybridization and introgression has long been debated1. Hybrids are usually rare and unfit, but even infrequent hybridization can aid adaptation by transferring beneficial traits between species. Here we use genomic tools to investigate introgression in Heliconius, a rapidly radiating genus of neotropical butterflies widely used in studies of ecology, behaviour, mimicry and speciation2,3,4,5. We sequenced the genome of Heliconius melpomene and compared it with other taxa to investigate chromosomal evolution in Lepidoptera and gene flow among multiple Heliconius species and races. Among 12,669 predicted genes, biologically important expansions of families of chemosensory and Hox genes are particularly noteworthy. Chromosomal organization has remained broadly conserved since the Cretaceous period, when butterflies split from the Bombyx (silkmoth) lineage. Using genomic resequencing, we show hybrid exchange of genes between three co-mimics, Heliconius melpomene, Heliconius timareta and Heliconius elevatus, especially at two genomic regions that control mimicry pattern. We infer that closely related Heliconius species exchange protective colour-pattern genes promiscuously, implying that hybridization has an important role in adaptive radiation."
STEVEN DEAN,Symmetry breaking and friction in few layer phosphorene,
DAVIDSON HAMER,Components of clean delivery kits and newborn mortality in the Zambia Chlorhexidine Application Trial (ZamCAT): an observational study,"BACKGROUND: Infection, a leading cause of neonatal death in low- and middle-income countries, is often caused by pathogens acquired during childbirth. Clean delivery kits (CDKs) have shown efficacy in reducing infection-related perinatal and neonatal mortality. However, there remain gaps in our current knowledge, including the effect of individual components, timeline of protection, and benefit of CDKs in home and facility-based deliveries. METHODS AND FINDINGS: A post-hoc, secondary analysis was performed using non-randomized data from the Zambia Chlorhexidine Application Trial (ZamCAT), a community-based, cluster-randomized controlled trial of chlorhexidine umbilical cord care in Southern Province of Zambia from February 2011 to January 2013. CDKs, containing soap, gloves, cord clamp, plastic sheet, razor blade, matches, and candles, were provided to all participants. Field monitors made home-based visit to each participant 4 days post-partum, during which CDK use and newborn outcomes were ascertained. Logistic regression was used to study the association between different CDK components and newborn mortality rate (NMR).Of 38,579 deliveries recorded during the study, 36,996 newborns were analyzed after excluding stillbirths and missing information. Gloves, cord clamp, and plastic sheets were the most frequently used CDK item combinations in both home and facility deliveries. Each of the 7 CDK components was associated with lower NMR in users versus non-users. Adjusted logistic regression showed that use of gloves (OR: 0.33, CI: 0.24-0.46), cord clamp (OR: 0.51, CI: 0.38-0.68), plastic sheets (OR: 0.46, CI: 0.34-0.63), and razor blades (OR: 0.69, CI: 0.53-0.89) were associated with lower risk of newborn mortality. Use of gloves and cord clamp was associated with reduced risk of immediate newborn death (<24 hours). Reduction in risk of early newborn death (1-7 days) was associated with use of gloves, cord clamp, plastic sheets, and razor blades. In examining perinatal mortality, similar patterns were observed. There was no significant reduction in risk of late newborn mortality (7-28 days) with CDK use. Study limitations included potential for potential recall bias of CDK use and inability to establish causality as a secondary observational study. CONCLUSIONS: CDK use was associated with reductions in early newborn mortality at both home and facility deliveries, especially when certain kit components were used. While causality could not be established in this non-randomized secondary analysis, given these beneficial associations, scaling up the use of CDKs in rural areas of sub-Saharan Africa may improve neonatal outcomes."
DAVIDSON HAMER,Impact of a community-based package of interventions on child development in Zambia: a cluster-randomised controlled trial,"BACKGROUND: Community-based programmes are a critical platform for improving child health and development. We tested the impact of a community based early childhood intervention package in rural Zambia. Methods: We conducted a non-blinded cluster randomised controlled trial in Southern Province, Zambia. 30 clusters of villages were matched based on population density and distance from the nearest health centre, and randomly assigned to intervention (15 clusters and 268 caregiver–child dyads) or control (15 clusters and 258 caregiver–child dyads). Caregivers were eligible if they had a child aged 6–12 months at baseline. In intervention clusters, health workers screened children for infections and malnutrition, and invited caregivers to attend fortnightly group meetings covering a nutrition and child development curriculum. 220 intervention and 215 control dyads were evaluated after 1 year. The primary outcomes were stunting and INTERGROWTH-21st neurodevelopmental assessment (NDA) scores. Weight-for-age and height-for-age z-scores based on WHO growth standards were also analysed. Secondary outcomes were child illness symptoms, dietary intake and caregiver–child interactions based on self-report. Impact was estimated using intention to-treat analysis. RESULTS: The intervention package was associated with a 0.12 SD increase in weight-for-age (95% CI−0.14 to 0.38), a 0.15 SD increase in height-for-age (95% CI −0.18 to 0.48) and a reduction in stunting (OR 0.68; 95% CI 0.36 to 1.28), whereas there was no measurable impact on NDA score. Children receiving the intervention package had fewer symptoms, a more diverse diet and more caregiver interactions. CONCLUSIONS: In settings like Zambia, community based early childhood programmes appear to be feasible and appreciated by caregivers, as evidenced by high rates of uptake. The intervention package improved parenting behaviours and had a small positive, though statistically insignificant, impact on child development. Given the short time frame of the project, larger developmental impact is likely if differential parenting behaviours persist."
DAVIDSON HAMER,"Seroprevalence of Hepatitis E among Boston Area Travelers, 2009-2010","We determined the prevalence of IgG antibodies to hepatitis E virus (anti-HEV IgG) among travelers attending Boston-area travel health clinics from 2009 to 2010. Pre-travel samples were available for 1,356 travelers, with paired pre- and post-travel samples for 450 (33%). Eighty of 1,356 (6%) pre-travel samples were positive for anti-HEV IgG. Compared with participants who had never lived in nor traveled to a highly endemic country, the pre-travel prevalence odds ratio (POR) of anti-HEV IgG among participants born in or with a history of previous travel to a highly endemic country was increased (POR = 4.8, 95% CI = 2.3–10.3 and POR = 2.6, 95% CI = 1.4–5.0, respectively). Among participants with previous travel to a highly endemic country, anti-HEV IgG was associated with age > 40 years (POR = 3.7, 95% CI = 1.3–10.2) and travel history to ≥ 3 highly endemic countries (POR = 2.7, 95% CI = 1.2–5.9). Two participants may have contracted HEV infection during their 2009–2010 trip."
DAVIDSON HAMER,Guidelines for the prevention and treatment of travelers’ diarrhea: a graded expert panel report,"BACKGROUND: Travelers' diarrhea causes significant morbidity including some sequelae, lost travel time and opportunity cost to both travelers and countries receiving travelers. Effective prevention and treatment are needed to reduce these negative impacts. METHODS: This critical appraisal of the literature and expert consensus guideline development effort asked several key questions related to antibiotic and non-antibiotic prophylaxis and treatment, utility of available diagnostics, impact of multi-drug resistant (MDR) colonization associated with travel and travelers' diarrhea, and how our understanding of the gastrointestinal microbiome should influence current practice and future research. Studies related to these key clinical areas were assessed for relevance and quality. Based on this critical appraisal, guidelines were developed and voted on using current standards for clinical guideline development methodology. RESULTS: New definitions for severity of travelers' diarrhea were developed. A total of 20 graded recommendations on the topics of prophylaxis, diagnosis, therapy and follow-up were developed. In addition, three non-graded consensus-based statements were adopted. CONCLUSION: Prevention and treatment of travelers' diarrhea requires action at the provider, traveler and research community levels. Strong evidence supports the effectiveness of antimicrobial therapy in most cases of moderate to severe travelers' diarrhea, while either increasing intake of fluids only or loperamide or bismuth subsalicylate may suffice for most cases of mild diarrhea. Further studies are needed to address knowledge gaps regarding optimal therapies, the individual, community and global health risks of MDR acquisition, manipulation of the microbiome in prevention and treatment and the utility of laboratory testing in returning travelers with persistent diarrhea."
DAVIDSON HAMER,"Vitamin A and zinc supplementation among pregnant women to prevent placental malaria: a randomized, double-blind, placebo-controlled trial in Tanzania","BACKGROUND: Malaria causes nearly 200 million clinical cases and approximately half a million deaths each year, primarily in sub-Saharan Africa.1 The risk of malaria increases during pregnancy,2 a period during which its prevention is especially important. Not only do pregnant women experience greater severity of illness compared with nonpregnant women,2 but studies have shown strong associations between prenatal malaria and maternal anemia,2 fetal loss, low birthweight, and infant mortality.2 Improving preventive measures that specifically target malaria in pregnancy is a global health priority.3 METHODS: Study design and participants. This randomized, doubleblind, placebo-controlled trial was implemented at 8 antenatal care clinics in the urban Temeke and Ilala districts of Dar es Salaam, Tanzania. The trial was registered RESULTS: A total of 2,500 screened participants were enrolled in the trial. The trial profile is shown in Figure 1. It was not possible to collect placentas from 875 participants for the following reasons: miscarriages (fetal loss before 28 weeks of gestation) (N = 234), delivery outside of Dar es Salaam or at a non-study hospital (N = 577), or withdrawal from the study (N = 34). Of the remaining 1,589 women, 1,404 placental samples were obtained (88%); histology results were available for 1,361 participants. PCR results were available for 1,158 participants, and 1,404 participants had either histology or PCR results available. CONCLUSION: This study is the first to examine the impact of vitamin A and zinc supplementation starting in early pregnancy on placental malaria. We observed that supplementation with 25 mg zinc per day from the first trimester until delivery was associated with a 36% (95% CI = 9–56%) reduced risk of histopathology-positive placental infection, but not PCRpositive infection. Vitamin A supplementation had no impact on placental malaria, but was associated with an increased risk for severe anemia."
DAVIDSON HAMER,Etiology of severe pneumonia in Ecuadorian children,"INTRODUCTION: In Latin America, community-acquired pneumonia remains a major cause of morbidity and mortality among children. Few studies have examined the etiology of pneumonia in Ecuador. METHODS: This observational study was part of a randomized, double blind, placebo-controlled clinical trial conducted among children aged 2–59 months with severe pneumonia in Quito, Ecuador. Nasopharyngeal and blood samples were tested for bacterial and viral etiology by polymerase chain reaction. Risk factors for specific respiratory pathogens were also evaluated. RESULTS: Among 406 children tested, 159 (39.2%) had respiratory syncytial virus (RSV), 71 (17.5%) had human metapneumovirus (hMPV), and 62 (15.3%) had adenovirus. Streptococcus pneumoniae was identified in 37 (9.2%) samples and Mycoplasma pneumoniae in three (0.74%) samples. The yearly circulation pattern of RSV (P = 0.0003) overlapped with S. pneumoniae, (P = 0.03) with most cases occurring in the rainy season. In multivariable analysis, risk factors for RSV included younger age (adjusted odds ratio [aOR] = 1.9, P = 0.01) and being underweight (aOR = 1.8, P = 0.04). Maternal education (aOR = 0.82, P = 0.003), pulse oximetry (aOR = 0.93, P = 0.005), and rales (aOR = 0.25, P = 0.007) were associated with influenza A. Younger age (aOR = 3.5, P = 0.007) and elevated baseline respiratory rate were associated with HPIV-3 infection (aOR = 0.94, P = 0.03). CONCLUSION: These results indicate the importance of RSV and influenza, and potentially modifiable risk factors including undernutrition and future use of a RSV vaccine, when an effective vaccine becomes available."
DAVIDSON HAMER,Using Electronic Drug Monitor Feedback to Improve Adherence to Antiretroviral Therapy Among HIV-Positive Patients in China,"Effective antiretroviral therapy (ART) requires excellent adherence. Little is known about how to improve ART adherence in many HIV/AIDS-affected countries, including China. We therefore assessed an adherence intervention among HIV-positive patients in southwestern China. Eighty subjects were enrolled and monitored for 6 months. Sixty-eight remaining subjects were randomized to intervention/control arms. In months 7–12, intervention subjects were counseled using EDM feedback; controls continued with standard of care. Among randomized subjects, mean adherence and CD4 count were 86.8 vs. 83.8% and 297 vs. 357 cells/μl in intervention vs. control subjects, respectively. At month 12, among 64 subjects who completed the trial, mean adherence had risen significantly among intervention subjects to 96.5% but remained unchanged in controls. Mean CD4 count rose by 90 cells/μl and declined by 9 cells/μl among intervention and control subjects, respectively. EDM feedback as a counseling tool appears promising for management of HIV and other chronic diseases."
DAVIDSON HAMER,Paediatric malaria case-management with artemether-lumefantrine in Zambia: a repeat cross-sectional study,"BACKGROUND:Zambia was the first African country to change national antimalarial treatment policy to artemisinin-based combination therapy - artemether-lumefantrine. An evaluation during the early implementation phase revealed low readiness of health facilities and health workers to deliver artemether-lumefantrine, and worryingly suboptimal treatment practices. Improvements in the case-management of uncomplicated malaria two years after the initial evaluation and three years after the change of policy in Zambia are reported.METHODS:Data collected during the health facility surveys undertaken in 2004 and 2006 at all outpatient departments of government and mission facilities in four Zambian districts were analysed. The surveys were cross-sectional, using a range of quality of care assessment methods. The main outcome measures were changes in health facility and health worker readiness to deliver artemether-lumefantrine, and changes in case-management practices for children below five years of age presenting with uncomplicated malaria as defined by national guidelines.RESULTS:In 2004, 94 health facilities, 103 health workers and 944 consultations for children with uncomplicated malaria were evaluated. In 2006, 104 facilities, 135 health workers and 1125 consultations were evaluated using the same criteria of selection. Health facility and health worker readiness improved from 2004 to 2006: availability of artemether-lumefantrine from 51% (48/94) to 60% (62/104), presence of artemether-lumefantrine dosage wall charts from 20% (19/94) to 75% (78/104), possession of guidelines from 58% (60/103) to 92% (124/135), and provision of in-service training from 25% (26/103) to 41% (55/135). The proportions of children with uncomplicated malaria treated with artemether-lumefantrine also increased from 2004 to 2006: from 1% (6/527) to 27% (149/552) in children weighing 5 to 9 kg, and from 11% (42/394) to 42% (231/547) in children weighing 10 kg or more. In both weight groups and both years, 22% (441/2020) of children with uncomplicated malaria were not prescribed any antimalarial drug.CONCLUSION:Although significant improvements in malaria case-management have occurred over two years in Zambia, the quality of treatment provided at the point of care is not yet optimal. Strengthening weak health systems and improving the delivery of effective interventions should remain high priority in all countries implementing new treatment policies for malaria."
DAVIDSON HAMER,Amplicon residues in research laboratories masquerade as COVID-19 in surveillance tests,"Asymptomatic surveillance testing together with COVID-19-related research can lead to positive SARS-CoV-2 tests resulting not from true infections, but non-infectious, non-hazardous by-products of research (amplicons). Amplicons can be widespread and persistent in lab environments and can be difficult to distinguish for true infections. We discuss prevention and mitigation strategies."
DAVIDSON HAMER,Travel-Associated Zika Virus Disease Acquired in the Americas Through February 2016,"BACKGROUND: Zika virus has spread rapidly in the Americas and has been imported into many nonendemic countries by travelers. OBJECTIVE: To describe clinical manifestations and epidemiology of Zika virus disease in travelers exposed in the Americas. DESIGN: Descriptive, using GeoSentinel records. SETTING: 63 travel and tropical medicine clinics in 30 countries. PATIENTS: Ill returned travelers with a confirmed, probable, or clinically suspected diagnosis of Zika virus disease seen between January 2013 and 29 February 2016. MEASUREMENTS: Frequencies of demographic, trip, and clinical characteristics and complications. RESULTS: Starting in May 2015, 93 cases of Zika virus disease were reported. Common symptoms included exanthema (88%), fever (76%), and arthralgia (72%). Fifty-nine percent of patients were exposed in South America; 71% were diagnosed in Europe. Case status was established most commonly by polymerase chain reaction (PCR) testing of blood and less often by PCR testing of other body fluids or serology and plaque-reduction neutralization testing. Two patients developed Guillain–Barre syndrome, and 3 of 4 pregnancies had adverse outcomes (microcephaly, major fetal neurologic abnormalities, and intrauterine fetal death). LIMITATION: Surveillance data collected by specialized clinics may not be representative of all ill returned travelers, and denominator data are unavailable. CONCLUSION: These surveillance data help characterize the clinical manifestations and adverse outcomes of Zika virus disease among travelers infected in the Americas and show a need for global standardization of diagnostic testing. The serious fetal complications observed in this study highlight the importance of travel advisories and prevention measures for pregnant women and their partners. Travelers are sentinels for global Zika virus circulation and may facilitate further transmission."
DAVIDSON HAMER,Paediatric malaria case-management with Artemether-Lumefantrine in Zambia: a repeat cross-sectional study,"BACKGROUND Zambia was the first African country to change national antimalarial treatment policy to artemisinin-based combination therapy – artemether-lumefantrine. An evaluation during the early implementation phase revealed low readiness of health facilities and health workers to deliver artemether-lumefantrine, and worryingly suboptimal treatment practices. Improvements in the case-management of uncomplicated malaria two years after the initial evaluation and three years after the change of policy in Zambia are reported. METHODS Data collected during the health facility surveys undertaken in 2004 and 2006 at all outpatient departments of government and mission facilities in four Zambian districts were analysed. The surveys were cross-sectional, using a range of quality of care assessment methods. The main outcome measures were changes in health facility and health worker readiness to deliver artemether-lumefantrine, and changes in case-management practices for children below five years of age presenting with uncomplicated malaria as defined by national guidelines. RESULTS. In 2004, 94 health facilities, 103 health workers and 944 consultations for children with uncomplicated malaria were evaluated. In 2006, 104 facilities, 135 health workers and 1125 consultations were evaluated using the same criteria of selection. Health facility and health worker readiness improved from 2004 to 2006: availability of artemether-lumefantrine from 51% (48/94) to 60% (62/104), presence of artemether-lumefantrine dosage wall charts from 20% (19/94) to 75% (78/104), possession of guidelines from 58% (60/103) to 92% (124/135), and provision of in-service training from 25% (26/103) to 41% (55/135). The proportions of children with uncomplicated malaria treated with artemether-lumefantrine also increased from 2004 to 2006: from 1% (6/527) to 27% (149/552) in children weighing 5 to 9 kg, and from 11% (42/394) to 42% (231/547) in children weighing 10 kg or more. In both weight groups and both years, 22% (441/2020) of children with uncomplicated malaria were not prescribed any antimalarial drug. CONCLUSION Although significant improvements in malaria case-management have occurred over two years in Zambia, the quality of treatment provided at the point of care is not yet optimal. Strengthening weak health systems and improving the delivery of effective interventions should remain high priority in all countries implementing new treatment policies for malaria."
DAVIDSON HAMER,From Chloroquine to Artemether-Lumefantrine: The Process of Drug Policy Change in Zambia,"BACKGROUND: Following the recognition that morbidity and mortality due to malaria had dramatically increased in the last three decades, in 2002 the government of Zambia reviewed its efforts to prevent and treat malaria. Convincing evidence of the failing efficacy of chloroquine resulted in the initiation of a process that eventually led to the development and implementation of a new national drug policy based on artemisinin-based combination therapy (ACT). METHODS: All published and unpublished documented evidence dealing with the antimalarial drug policy change was reviewed. These data were supplemented by the authors' observations of the policy change process. The information has been structured to capture the timing of events, the challenges encountered, and the resolutions reached in order to achieve implementation of the new treatment policy. RESULTS: A decision was made to change national drug policy to artemether-lumefantrine (AL) in the first quarter of 2002, with a formal announcement made in October 2002. During this period, efforts were undertaken to identify funding for the procurement of AL and to develop new malaria treatment guidelines, training materials, and plans for implementation of the policy. In order to avoid a delay in implementation, the policy change decision required a formal adoption within existing legislation. Starting with donated drug, a phased deployment of AL began in January 2003 with initial use in seven districts followed by scaling up to 28 districts in the second half of 2003 and then to all 72 districts countrywide in early 2004. CONCLUSION: Drug policy changes are not without difficulties and demand a sustained international financing strategy for them to succeed. The Zambian experience demonstrates the need for a harmonized national consensus among many stakeholders and a political commitment to ensure that new policies are translated into practice quickly. To guarantee effective policies requires more effort and recognition that this becomes a health system and not a drug issue. This case study attempts to document the successful experience of change to ACT in Zambia and provides a realistic overview of some of the painful experiences and important lessons learnt."
DAVIDSON HAMER,"Dataset for ""A Qualitative Assessment of Community Acceptability and Use of a Locally Developed Children’s Book to Increase Shared Reading and Parent-Child Interactions in Rural Zambia""",
DAVIDSON HAMER,"Dataset for ""Barriers and facilitators to facility-based delivery in rural Zambia: A qualitative study of women’s perceptions after implementation of an improved Maternity Waiting Homes intervention""","Objectives: Women in sub-Saharan Africa face well-documented barriers to facility-based deliveries. An improved maternity waiting homes (MWH) model was implemented in rural Zambia to bring pregnant women closer to facilities for delivery. We qualitatively assessed whether MWHs changed perceived barriers to facility delivery among remote-living women. Design: We administered in-depth interviews (IDIs) to a randomly-selected subsample of women in intervention (n=78) and control (n=80) groups who participated in the primary quasi-experimental evaluation of an improved MWH model. The IDIs explored perceptions and preferences of delivery location. We conducted content analysis to understand perceived barriers and facilitators to facility delivery. Setting and participants: Participants lived in villages 10+ kilometers from the health facility and had delivered a baby in the previous 12 months. Intervention: The improved MWH model was implemented at 20 rural health facilities. Results: Over 96% of participants in the intervention arm and 90% in the control arm delivered their last baby at a health facility. Key barriers to facility delivery were distance and transportation, and costs associated with delivery. Facilitators included no user fees, penalties for home delivery, desire for safe delivery, and availability of MWHs. Most themes were similar between study arms. Both discussed the role MWHs have in improving access to facility-based delivery. Intervention arm participants expressed that the improved MWH model encourages use and helps overcome the distance barrier. Control arm participants either expressed a desire for an improved MWH model or did not consider it in their decision-making. Conclusions: Even in areas with high facility-based delivery rates in rural Zambia, barriers to access persist. MWHs may be useful to address the distance challenge, but no single intervention is likely to address all barriers experienced by rural, low-resourced populations. MWHs should be considered in a broader systems approach to improving access in remote areas. Trial Registration: ClinicalTrials.gov Identifier: NCT02620436"
DAVIDSON HAMER,MAHMAZ maternity waiting home: setup cost dataset,"These datasets detail 1) the setup costs expended to set up 10 maternity waiting homes in rural Zambia and 2) the monthly occupancy of the maternity waiting homes. The former includes the date of purchase, cost category, and the purchase amount in Kwacha. The latter describes how many patients visited the maternity waiting home in the last year of our project. We utilized this data to create a manuscript describing the setup costs of these homes, and the cost per admission to the homes, to serve as a guide for future implementors."
DAVIDSON HAMER,"Dataset for ""If we build it, will they come? Results of a quasi-experimental study assessing the impact of maternity waiting homes on facility-based childbirth and maternity care in Zambia""",
DAVIDSON HAMER,"Effectiveness of 4% chlorhexidine umbilical cord care on neonatal mortality in Southern Province, Zambia (ZamCAT): a cluster-randomised controlled trial","BACKGROUND: Chlorhexidine umbilical cord washes reduce neonatal mortality in south Asian populations with high neonatal mortality rates and predominantly home-based deliveries. No data exist for sub-Saharan African populations with lower neonatal mortality rates or mostly facility-based deliveries. We compared the e ect of chlorhexidine with dry cord care on neonatal mortality rates in Zambia. METHODS: We undertook a cluster-randomised controlled trial in Southern Province, Zambia, with 90 health facility- based clusters. We enrolled women who were in their second or third trimester of pregnancy, aged at least 15 years, and who would remain in the catchment area for follow-up of 28 days post-partum. Newborn babies received clean dry cord care (control) or topical application of 10 mL of a 4% chlorhexidine solution once per day until 3 days after cord drop (intervention), according to cluster assignment. We used strati ed, restricted randomisation to divide clusters into urban or two rural groups (located <40 km or ≥40 km to referral facility), and randomly assigned clusters (1:1) to use intervention (n=45) or control treatment (n=45). Sites, participants, and eld monitors were aware of their study assignment. The primary outcomes were all-cause neonatal mortality within 28 days post-partum and all-cause neonatal mortality within 28 days post-partum among babies who survived the rst 24 h of life. Analysis was by intention to treat. Neonatal mortality rate was compared with generalised estimating equations. This study is registered at http://ClinicalTrials.gov (NCT01241318). FINDINGS: From Feb 15, 2011, to Jan 30, 2013, we screened 42356 pregnant women and enrolled 39679 women (mean 436·2 per cluster [SD 65·3]), who had 37856 livebirths and 723 stillbirths; 63·8% of deliveries were facility-based. Of livebirths, 18 450 (99·7%) newborn babies in the chlorhexidine group and 19 308 (99·8%) newborn babies in the dry cord care group were followed up to day 28 or death. 16660 (90·0%) infants in the chlorhexidine group had chlorhexidine applied within 24 h of birth. We found no signi cant di erence in neonatal mortality rate between the chlorhexidine group (15·2 deaths per 1000 livebirths) and the dry cord care group (13·6 deaths per 1000 livebirths; risk ratio [RR] 1·12, 95% CI 0·88–1·44). Eliminating day 0 deaths yielded similar ndings (RR 1·12, 95% CI 0·86–1·47). INTERPRETATION: Despite substantial reductions previously reported in south Asia, chlorhexidine cord applications did not signi cantly reduce neonatal mortality rates in Zambia. Chlorhexidine cord applications do not seem to provide clear bene ts for newborn babies in settings with predominantly facility-based deliveries and lower (<30 deaths per 1000 livebirths) neonatal mortality rates."
DAVIDSON HAMER,"Burden of Malaria in Pregnancy in Jharkhand State, India","BACKGROUND. Past studies in India included only symptomatic pregnant women and thus may have overestimated the proportion of women with malaria. Given the large population at risk, a cross sectional study was conducted in order to better define the burden of malaria in pregnancy in Jharkhand, a malaria-endemic state in central-east India. METHODS Cross-sectional surveys at antenatal clinics and delivery units were performed over a 12-month period at two district hospitals in urban and semi-urban areas, and a rural mission hospital. Malaria was diagnosed by Giemsa-stained blood smear and/or rapid diagnostic test using peripheral or placental blood. RESULTS 2,386 pregnant women were enrolled at the antenatal clinics and 718 at the delivery units. 1.8% (43/2382) of the antenatal clinic cohort had a positive diagnostic test for malaria (53.5% Plasmodium falciparum, 37.2% Plasmodium vivax, and 9.3% mixed infections). Peripheral parasitaemia was more common in pregnant women attending antenatal clinics in rural sites (adjusted relative risk [aRR] 4.31, 95%CI 1.84-10.11) and in those who were younger than 20 years (aRR 2.68, 95%CI 1.03-6.98). Among delivery unit participants, 1.7% (12/717) had peripheral parasitaemia and 2.4% (17/712) had placental parasitaemia. Women attending delivery units were more likely to be parasitaemic if they were in their first or second pregnancy (aRR 3.17, 95%CI 1.32-7.61), had fever in the last week (aRR 5.34, 95%CI 2.89-9.90), or had rural residence (aRR 3.10, 95%CI 1.66-5.79). Malaria control measures including indoor residual spraying (IRS) and untreated bed nets were common, whereas insecticide-treated bed nets (ITN) and malaria chemoprophylaxis were rarely used. CONCLUSION The prevalence of malaria among pregnant women was relatively low. However, given the large at-risk population in this malaria-endemic region of India, there is a need to enhance ITN availability and use for prevention of malaria in pregnancy, and to improve case management of symptomatic pregnant women."
DAVIDSON HAMER,Malaria at Parturition in Nigeria: Current Status and Delivery Outcome,"Background. To evaluate the current status of malaria at parturition and its impact on delivery outcome in Nigeria. Methods. A total of 2500 mother-neonate pairs were enrolled at 4 sites over a 12-month period. Maternal and placental blood smears for malaria parasitaemia and haematocrit were determined. Results. Of the 2500 subjects enrolled, 625 were excluded from analysis because of breach in study protocol. The mean age of the remaining 1875 mothers was 29.0 ± 5.1 years. The prevalence of parasitaemia was 17% and 14% in the peripheral blood and placenta of the parturient women, respectively. Peripheral blood parasitaemia was negatively associated with increasing parity (P<.0001). Maternal age <20 years was significantly associated with both peripheral blood and placental parasitaemia. After adjusting for covariates only age <20 years was associated with placental parasitaemia. Peripheral blood parasitaemia in the women was associated with anaemia (PCV ≤30%) lower mean hematocrit (P<.0001). lower mean birth weight (P<.001) and a higher proportion of low birth weight babies (LBW), (P = .025). Conclusion. In Nigeria, maternal age 20 years was the most important predisposing factor to malaria at parturition. The main impacts on pregnancy outcome were a twofold increase in rate of maternal anaemia and higher prevalence of LBW."
DAVIDSON HAMER,Travelers’ diarrhea and other gastrointestinal symptoms among Boston-area international travelers,"INTRODUCTION: Travelers' diarrhea (TD) and non-TD gastrointestinal (GI) symptoms are common among international travelers. In a study of short-term travelers from Switzerland to developing countries, the most common symptom experienced was severe diarrhea (8.5%) followed by vomiting or abdominal cramps (4%).1 GI illnesses were the most frequently reported diagnoses (34%) among ill-returned travelers to GeoSentinel clinics.2 Of those returning to U.S. GeoSentinel clinics, acute diarrhea (30%) was the most common diagnosis.3 In one cohort of U.S. travelers, 46% reported diarrhea.4 GI illnesses can last from 2 days to weeks or longer,5 disrupting plans during travel or after returning home. Eighty percent of those who experienced diarrhea during travel treated themselves with medication and 6% sought medical care. METHODS: The Boston Area Travel Medicine Network (BATMN) is a research collaboration of travel clinics in the greater Boston area representing urban-, suburban-, academic-, and university-affiliated facilities. A convenience sample of travelers ≥ 18 years of age attending three BATMN clinics between 2009 and 2011 for pre-travel consultations completed pre-travel surveys, at least one survey weekly during travel, and a post-travel survey 2–4 weeks after return. Travelers were asked to complete a survey at the end of each week of their trip. Institutional review board approvals were obtained at all sites and the Centers for Disease Control and Prevention, and participants provided written informed consent. Information collected included demographic and trip characteristics, vaccines and medications recommended/prescribed before travel, medications taken during travel, dietary practices during travel (consumption of tap water, ice in drinks, unpasteurized dairy products, and salads), symptoms experienced, and impact of illness during and after travel. Vaccinations, prescriptions, and travel health advice given during the pre-travel consultation were recorded by a clinician, and the remainder of the surveys were completed by the traveler. Data were entered into a password-protected database (CS Pro, U.S. Census Bureau, Washington, DC). RESULTS: We enrolled 987 travelers; 628 (64%) completed all three parts (pre-, during, and post-travel) and were included in the study. Comparison of the 628 to the 359 who did not complete all three parts (noncompleters) revealed no differences, except that completion rates were higher for white travelers than all other racial/ethnic groups (P < 0.001) and for older travelers (median age 47 years versus 32 years in noncompleters, P < 0.001).11 Of those 628 travelers, 208 (33%) experienced TD, 45 (7%) experienced non-TD GI symptoms, 147 (23%) experienced non-GI symptoms, and 228 (36%) did not experience any symptoms during or after travel. Of the 208 with TD, 140 (67%) reported diarrhea as their only symptom, whereas 33 (16%) also experienced nausea/vomiting, 23 (11%) abdominal pain, and 27 (13%) fever (Table 1). Of the 45 who reported non-TD GI symptoms, 21 (47%) experienced nausea/vomiting, 19 (42%) experienced constipation, and 10 (22%) experienced abdominal pain during or after travel (Table 2). Almost all travelers (99%) received advice about food and water precautions and diarrhea management during pre-travel consultation."
DAVIDSON HAMER,Effect of Incentives on Insecticide-Treated Bed Net Use in Sub-Saharan Africa: A Cluster Randomized Trial in Madagascar,"BACKGROUND Insecticide-treated bed nets (ITNs) have been shown to reduce morbidity and mortality due to malaria in sub-Saharan Africa. Strategies using incentives to increase ITN use could be more efficient than traditional distribution campaigns. To date, behavioural incentives have been studied mostly in developed countries. No study has yet looked at the effect of incentives on the use of ITNs. Reported here are the results of a cluster randomized controlled trial testing household-level incentives for ITN use following a free ITN distribution campaign in Madagascar. METHODS The study took place from July 2007 until February 2008. Twenty-one villages were randomized to either intervention or control clusters. Households in both clusters received a coupon redeemable for one ITN. After one month, intervention households received a bonus for ITN use, determined by visual confirmation of a mounted ITN. Data were collected at baseline, one month and six months. Both unadjusted and adjusted results, using cluster specific methods, are presented. RESULTS At baseline, 8.5% of households owned an ITN and 6% were observed to have a net mounted over a bed in the household. At one month, there were no differences in ownership between the intervention and control groups (99.5% vs. 99.4%), but net use was substantially higher in the intervention group (99% vs. 78%), with an adjusted risk ratio of 1.24 (95% CI: 1.10 to 1.40; p <0.001). After six months, net ownership had decreased in the intervention compared to the control group (96.7% vs. 99.7%), with an adjusted risk ratio of 0.97 (p <0.01). There was no difference between the groups in terms of ITN use at six months; however, intervention households were more likely to use a net that they owned (96% vs. 90%; p <0.001). CONCLUSIONS Household-level incentives have the potential to significantly increase the use of ITNs in target households in the immediate-term, but, over time, the use of ITNs is similar to households that did not receive incentives. Providing incentives for behaviour change is a promising tool that can complement traditional ITN distribution programmes and improve the effectiveness of ITN programmes in protecting vulnerable populations, especially in the short-term."
DAVIDSON HAMER,"Intermittent Preventive Treatment with Sulphadoxine-Pyrimethamine Is Effective in Preventing Maternal and Placental Malaria in Ibadan, South-Western Nigeria","BACKGROUND Intermittent preventive treatment with sulphadoxine-pyrimethamine (IPT-SP) is currently the recommended regimen for prevention of malaria in pregnancy in endemic areas. This study sets out to evaluate the effectiveness of IPT-SP in the prevention of maternal and placental malaria in parturient mothers in Ibadan, Nigeria, where the risk of malaria is present all year round. METHOD During a larger study evaluating the epidemiology of congenital malaria, the effect of malaria prophylaxis was examined in 983 parturient mothers. Five hundred and ninety eight mothers (60.8%) received IPT-SP, 214 (21.8%) received pyrimethamine (PYR) and 171 (17.4%) did not take any chemoprophylactic agent (NC). RESULTS The prevalence of maternal parasitaemia in the IPT-SP, PYR and NC groups was 10.4%, 15.9% and 17% respectively (p = 0.021). The prevalence of placental parasitaemia was 10.5% in the IPT-SP, 16.8% PYR and 17% NC groups, respectively (p = 0.015). The prevalence of maternal anaemia (haematocrit <30%) was 5.7% vs. 8.9% vs. 13.4% among the IPT-SP, PYR and NC groups respectively (p <0.0001) while that of pre-term delivery (GA <37 weeks) was 10.5%, 19.2% and 25.3% among IPT-SP, PYR and NC groups respectively (p <0.0001). Babies born to mothers in the IPT-SP, PYR and NC groups had mean birth weights of 3204 ± 487.16, 3075 ± 513.24 and 3074 ± 505.92 respectively (ρ <0.0001). There was a trend towards a lower proportion of low birth weight babies in the IPT-SP group (p = 0.095). CONCLUSION IPT-SP is effective in preventing maternal and placental malaria as well as improving pregnancy outcomes among parturient women in Ibadan, Nigeria. The implementation of the recently adopted IPT-SP strategy should be pursued with vigour as it holds great promise for reducing the burden of malaria in pregnancy in Nigeria."
DAVIDSON HAMER,Community Case Management of Fever Due to Malaria and Pneumonia in Children Under Five in Zambia: A Cluster Randomized Controlled Trial,"In a cluster randomized trial, Kojo Yeboah-Antwi and colleagues find that integrated management of malaria and pneumonia in children under five by community health workers is both feasible and effective. BACKGROUND. Pneumonia and malaria, two of the leading causes of morbidity and mortality among children under five in Zambia, often have overlapping clinical manifestations. Zambia is piloting the use of artemether-lumefantrine (AL) by community health workers (CHWs) to treat uncomplicated malaria. Valid concerns about potential overuse of AL could be addressed by the use of malaria rapid diagnostics employed at the community level. Currently, CHWs in Zambia evaluate and treat children with suspected malaria in rural areas, but they refer children with suspected pneumonia to the nearest health facility. This study was designed to assess the effectiveness and feasibility of using CHWs to manage nonsevere pneumonia and uncomplicated malaria with the aid of rapid diagnostic tests (RDTs). METHODS AND FINDINGS. Community health posts staffed by CHWs were matched and randomly allocated to intervention and control arms. Children between the ages of 6 months and 5 years were managed according to the study protocol, as follows. Intervention CHWs performed RDTs, treated test-positive children with AL, and treated those with nonsevere pneumonia (increased respiratory rate) with amoxicillin. Control CHWs did not perform RDTs, treated all febrile children with AL, and referred those with signs of pneumonia to the health facility, as per Ministry of Health policy. The primary outcomes were the use of AL in children with fever and early and appropriate treatment with antibiotics for nonsevere pneumonia. A total of 3,125 children with fever and/or difficult/fast breathing were managed over a 12-month period. In the intervention arm, 27.5% (265/963) of children with fever received AL compared to 99.1% (2066/2084) of control children (risk ratio 0.23, 95% confidence interval 0.14–0.38). For children classified with nonsevere pneumonia, 68.2% (247/362) in the intervention arm and 13.3% (22/203) in the control arm received early and appropriate treatment (risk ratio 5.32, 95% confidence interval 2.19–8.94). There were two deaths in the intervention and one in the control arm. CONCLUSIONS. The potential for CHWs to use RDTs, AL, and amoxicillin to manage both malaria and pneumonia at the community level is promising and might reduce overuse of AL, as well as provide early and appropriate treatment to children with nonsevere pneumonia."
DAVIDSON HAMER,Self-reported illness among Boston-area international travelers: A prospective study,"BACKGROUND: The Boston Area Travel Medicine Network surveyed travelers on travel-related health problems. METHODS: Travelers were recruited 2009-2011 during pre-travel consultation at three clinics. The investigation included pre-travel data, weekly during-travel diaries, and a post-travel questionnaire. We analyzed demographics, trip characteristics, health problems experienced, and assessed the relationship between influenza vaccination, influenza prevention advice, and respiratory symptoms. RESULTS:Of 987 enrolled travelers, 628 (64%) completed all surveys, of which 400 (64%) reported health problems during and/or after travel; median trip duration was 12 days. Diarrhea affected the most people during travel (172) while runny/stuffy nose affected the most people after travel (95). Of those with health problems during travel, 25% stopped or altered plans; 1% were hospitalized. After travel, 21% stopped planned activities, 23% sought physician or other health advice; one traveler was hospitalized. Travelers who received influenza vaccination and influenza prevention advice had lower rates of respiratory symptoms than those that received influenza prevention advice alone (18% vs 28%, P = 0.03). CONCLUSIONS:A large proportion of Boston-area travelers reported health problems despite pre-travel consultation, resulting in inconveniences. The combination of influenza prevention advice and influenza immunization was associated with fewer respiratory symptoms than those who received influenza prevention advice alone."
DAVIDSON HAMER,Availability and utilization of malaria prevention strategies in pregnancy in Eastern India,"BACKGROUND. Malaria in pregnancy in India, as elsewhere, is responsible for maternal anemia and adverse pregnancy outcomes such as low birth weight and preterm birth. It is not known whether prevention and treatment strategies for malaria in pregnancy (case management, insecticide-treated bednets, intermittent preventive therapy) are widely utilized in India. METHODS. This cross-sectional study was conducted during 2006-2008 in two states of India, Jharkhand and Chhattisgarh, at 7 facilities representing a range of rural and urban populations and areas of more versus less stable malaria transmission. 280 antenatal visits (40/site) were observed by study personnel coupled with exit interviews of pregnant women to assess emphasis upon, availability and utilization of malaria prevention practices by health workers and pregnant women. The facilities were assessed for the availability of antimalarials, lab supplies and bednets. RESULTS. All participating facilities were equipped to perform malaria blood smears; none used rapid diagnostic tests. Chloroquine, endorsed for chemoprophylaxis during pregnancy by the government at the time of the study, was stocked regularly at all facilities although the quantity stocked varied. Availability of alternative antimalarials for use in pregnancy was less consistent. In Jharkhand, no health worker recommended bednet use during the antenatal visit yet over 90% of pregnant women had bednets in their household. In Chhattisgarh, bednets were available at all facilities but only 14.4% of health workers recommended their use. 40% of the pregnant women interviewed had bednets in their household. Only 1.4% of all households owned an insecticide-treated bednet; yet 40% of all women reported their households had been sprayed with insecticide. Antimalarial chemoprophylaxis with chloroquine was prescribed in only 2 (0.7%) and intermittent preventive therapy prescribed in only one (0.4%) of the 280 observed visits. CONCLUSIONS. A disconnect remains between routine antenatal practices in India and known strategies to prevent and treat malaria in pregnancy. Prevention strategies, in particular the use of insecticide-treated bednets, are underutilized. Gaps highlighted by this study combined with recent estimates of the prevalence of malaria during pregnancy in these areas should be used to revise governmental policy and target increased educational efforts among health care workers and pregnant women."
DAVIDSON HAMER,Buildout and integration of an automated high-throughput CLIA laboratory for SARS-CoV-2 testing on a large urban campus,"In 2019, the first cases of SARS-CoV-2 were detected in Wuhan, China, and by early 2020 the first cases were identified in the United States. SARS-CoV-2 infections increased in the US causing many states to implement stay-at-home orders and additional safety precautions to mitigate potential outbreaks. As policies changed throughout the pandemic and restrictions lifted, there was an increase in demand for COVID-19 testing which was costly, difficult to obtain, or had long turn-around times. Some academic institutions, including Boston University (BU), created an on-campus COVID-19 screening protocol as part of a plan for the safe return of students, faculty, and staff to campus with the option for in-person classes. At BU, we put together an automated high-throughput clinical testing laboratory with the capacity to run 45,000 individual tests weekly by Fall of 2020, with a purpose-built clinical testing laboratory, a multiplexed reverse transcription PCR (RT-qPCR) test, robotic instrumentation, and trained staff. There were many challenges including supply chain issues for personal protective equipment and testing materials in addition to equipment that were in high demand. The BU Clinical Testing Laboratory (CTL) was operational at the start of Fall 2020 and performed over 1 million SARS-CoV-2 PCR tests during the 2020-2021 academic year."
BRUCE J SCHULMAN,"Bostonia: 1999-2000, no. 1-4",
BRUCE J SCHULMAN,The (new) American political tradition,"The year 2023 marked the seventy-fifth anniversary of the publication of Richard Hofstadter's The American Political Tradition: And the Men Who Made It—a bestselling book that captured the imagination of many of Hofstadter's fellow Americans in the early postwar period and, at the same time, defined the terms of argument for much of academic history for the next generation.1 It was also the book my high school teacher, Mr. Backfish, assigned in eleventh-grade Advanced Placement (AP) American History that helped transform me into a historian. So different was it from the dry, conventional textbook in both its riveting, sometimes acerbic prose and its unsentimental view of venerated figures in the American past. I still have my original copy (Figure 1)."
LOUIS C GERSTENFELD,Identification of known and novel long non-coding RNAs potentially responsible for the effects of BMD GWAS loci,"Osteoporosis, characterized by low bone mineral density (BMD), is the most common complex disease affecting bone and constitutes a major societal health problem. Genome-wide association studies (GWASs) have identified over 1100 associations influencing BMD. It has been shown that perturbations to long non-coding RNAs (lncRNAs) influence BMD and the activities of bone cells; however, the extent to which lncRNAs are involved in the genetic regulation of BMD is unknown. Here, we combined the analysis of allelic imbalance (AI) in human acetabular bone fragments with a transcriptome-wide association study (TWAS) and expression quantitative trait loci (eQTL) colocalization analysis using data from the Genotype-Tissue Expression (GTEx) project to identify lncRNAs potentially responsible for GWAS associations. We identified 27 lncRNAs in bone that are located in proximity to a BMD GWAS association and harbor SNPs demonstrating AI. Using GTEx data we identified an additional 31 lncRNAs whose expression was associated (FDR correction<0.05) with BMD through TWAS and had a colocalizing eQTL (regional colocalization probability (RCP)>0.1). The 58 lncRNAs are located in 43 BMD associations. To further support a causal role for the identified lncRNAs, we show that 23 of the 58 lncRNAs are differentially expressed as a function of osteoblast differentiation. Our approach identifies lncRNAs that are potentially responsible for BMD GWAS associations and suggest that lncRNAs play a role in the genetics of osteoporosis."
LOUIS C GERSTENFELD,Transcriptional Analysis of Fracture Healing and the Induction of Embryonic Stem Cell-Related Genes,"Fractures are among the most common human traumas. Fracture healing represents a unique temporarily definable post-natal process in which to study the complex interactions of multiple molecular events that regulate endochondral skeletal tissue formation. Because of the regenerative nature of fracture healing, it is hypothesized that large numbers of post-natal stem cells are recruited and contribute to formation of the multiple cell lineages that contribute to this process. Bayesian modeling was used to generate the temporal profiles of the transcriptome during fracture healing. The temporal relationships between ontologies that are associated with various biologic, metabolic, and regulatory pathways were identified and related to developmental processes associated with skeletogenesis, vasculogenesis, and neurogenesis. The complement of all the expressed BMPs, Wnts, FGFs, and their receptors were related to the subsets of transcription factors that were concurrently expressed during fracture healing. We further defined during fracture healing the temporal patterns of expression for 174 of the 193 genes known to be associated with human genetic skeletal disorders. In order to identify the common regulatory features that might be present in stem cells that are recruited during fracture healing to other types of stem cells, we queried the transcriptome of fracture healing against that seen in embryonic stem cells (ESCs) and mesenchymal stem cells (MSCs). Approximately 300 known genes that are preferentially expressed in ESCs and ~350 of the known genes that are preferentially expressed in MSCs showed induction during fracture healing. Nanog, one of the central epigenetic regulators associated with ESC stem cell maintenance, was shown to be associated in multiple forms or bone repair as well as MSC differentiation. In summary, these data present the first temporal analysis of the transcriptome of an endochondral bone formation process that takes place during fracture healing. They show that neurogenesis as well as vasculogenesis are predominant components of skeletal tissue formation and suggest common pathways are shared between post-natal stem cells and those seen in ESCs."
LOUIS C GERSTENFELD,Role of Fas and Treg cells in fracture healing as characterized in the Fas‐deficient (lpr) mouse model of lupus,"Previous studies showed that loss of tumor necrosis factora (TNFa) signaling delayed fracture healing by delaying chondrocyte apoptosis and cartilage resorption. Mechanistic studies showed that TNFa induced Fas expression within chondrocytes; however, the degree to which chondrocyte apoptosis ismediated by TNFa alone or dependent on the induction of Fas is unclear. This questionwas addressed by assessing fracture healing in Fas‐deficient B6.MRL/Faslpr/Jmice. Loss of Fas delayed cartilage resorption but also lowered bone fraction in the calluses. The reduced bone fraction was related to elevated rates of coupled bone turnover in the B6.MRL/Faslpr/J calluses, as evidenced by higher osteoclast numbers and increased osteogenesis. Analysis of the apoptoticmarker caspase 3 showed fewer positive chondrocytes and osteoclasts in calluses of B6.MRL/Faslpr/J mice. To determine if an active autoimmune state contributed to increased bone turnover, the levels of activated T cells and Treg cellswere assessed. B6.MRL/Faslpr/J mice had elevated Treg cells in both spleens and bones of B6.MRL/Faslpr/J but decreased percentage of activated T cells in bone tissues. Fracture led to 30% to 60% systemic increase in Treg cells in bothwild‐type and B6.MRL/Faslpr/J bone tissues during the period of cartilage formation and resorption but either decreased (wild type) or left unchanged (B6.MRL/Faslpr/J) the numbers of activated T cells in bone. These results show that an active autoimmune state is inhibited during the period of cartilage resorption and suggest that iTreg cells play a functional role in this process. These data show that loss of Fas activity specifically in chondrocytes prolonged the life span of chondrocytes and that Fas synergized with TNFa signaling tomediate chondrocyte apoptosis. Conversely, loss of Fas systemically led to increased osteoclast numbers during later periods of fracture healing and increased osteogenesis. These findings suggest that retention of viable chondrocytes locally inhibits osteoclast activity or matrix proteolysis during cartilage resorption."
LOUIS C GERSTENFELD,Whorl: 2024,
JAMES S PANEK,Synthesis of isochromene-type scaffolds via single-flask Diels-Alder-[4 + 2]-annulation sequence of a silyl-substituted diene with menadione,"A sequential Diels-Alder reaction/silicon-directed [4 + 2]-annulation was developed to assemble hydroisochromene-type ring systems from menadione 2. In the first step, a Diels-Alder of the 1-silyl-substituted butadiene 1 with 2 furnished an intermediate cyclic allylsilane. Subsequently, TMSOTf promoted a [4 + 2]-annulation through trapping of an oxonium, generated by condensation between an aldehyde and the TBS protected alcohol resulted in the formation of a cis-fused hydroisochromene 13."
JAMES S PANEK,Diastereodivergent synthesis of chiral tetrahydropyrrolodiazepinediones via a one-pot intramolecular aza-Michael/lactamization sequence,"A modular and diastereodivergent synthesis of tetrahydro-1H-pyrrolo[1,2d]diazepine-(2,5)-diones is presented. The tetrahydropyrrolodiazepinedione scaffold is obtained via a base-mediated three-step isomerization/tandem cyclization of amino acid-coupled homoallylic amino esters. Diastereoselectivity of the process is mediated by the interplay of a kinetic cyclization event and a propensity for thermodynamic epimerization at two labile chiral centers, giving rise to two distinct major diastereomers dependent on starting material stereochemistry and reaction conditions selected. Herein, we present a synthetic and computational study for this tandem process on a variety of amino ester substrates."
JAMES S PANEK,Discovery of macrocyclic inhibitors of apurinic/apyrimidinic endonuclease 1,"Apurinic/apyrimidinic endonuclease 1 (APE1) is an essential base excision repair enzyme that is upregulated in a number of cancers, contributes to resistance of tumors treated with DNA-alkylating or -oxidizing agents, and has recently been identified as an important therapeutic target. In this work, we identified hot spots for binding of small organic molecules experimentally in high resolution crystal structures of APE1 and computationally through the use of FTMAP analysis (http://ftmap.bu.edu). Guided by these hot spots, a library of drug-like macrocycles was docked and then screened for inhibition of APE1 endonuclease activity. In an iterative process, hot-spot-guided docking, characterization of inhibition of APE1 endonuclease, and cytotoxicity of cancer cells were used to design next generation macrocycles. To assess target selectivity in cells, selected macrocycles were analyzed for modulation of DNA damage. Taken together, our studies suggest that macrocycles represent a promising class of compounds for inhibition of APE1 in cancer cells."
JAMES S PANEK,Identification of a kavain analog with efficient anti-inflammatory effects,"Kavain, a compound derived from Piper methysticum, has demonstrated anti-inflammatory properties. To optimize its drug properties, identification and development of new kavain-derived compounds was undertaken. A focused library of analogs was synthesized and their effects on Porphyromonas gingivalis (P. gingivalis) elicited inflammation were evaluated in vitro and in vivo. The library contained cyclohexenones (5,5-dimethyl substituted cyclohexenones) substituted with a benzoate derivative at the 3-position of the cyclohexanone. The most promising analog identifed was a methylated derivative of kavain, Kava-205Me (5,5-dimethyl-3-oxocyclohex-1-en-1-yl 4-methylbenzoate.) In an in vitro assay of anti-inflammatory effects, murine macrophages (BMM) and THP-1 cells were infected with P. gingivalis (MOI = 20:1) and a panel of cytokines were measured. Both cell types treated with Kava-205Me (10 to 200 μg/ml) showed significantly and dose-dependently reduced TNF-α secretion induced by P. gingivalis. In BMM, Kava-205Me also reduced secretion of other cytokines involved in the early phase of inflammation, including IL-12, eotaxin, RANTES, IL-10 and interferon-γ (p < 0.05). In vivo, in an acute model of P. gingivalis-induced calvarial destruction, administration of Kava-205Me significantly improved the rate of healing associated with reduced soft tissue inflammation and osteoclast activation. In an infective arthritis murine model induced by injection of collagen-antibody (ArthriomAb) + P. gingivalis, administration of Kava-205Me was able to reduce efficiently paw swelling and joint destruction. These results highlight the strong anti-inflammatory properties of Kava-205Me and strengthen the interest of testing such compounds in the management of P. gingivalis elicited inflammation, especially in the management of periodontitis."
JAMES S PANEK,Kava -241 reduced periodontal destruction in a collagen antibiody primed Porphyromonas gingivalis model of periodontis,"AIM: The aim of this study was to evaluate the effect of Kava-241, an optimized Piper methysticum Kava compound, on periodontal destruction in a collagen antibody primed oral gavage model of periodontitis. METHODS: Experimental periodontitis was induced by oral gavage of Porphyromonas gingivalis (P. gingivalis) + type II collagen antibody (AB) in mice during 15 days. Mice were treated with Kava-241 concomitantly or prior to P. gingivalis gavage and compared to untreated mice. Comprehensive histomorphometric analyses were performed. RESULTS: Oral gavage with P. gingivalis induced mild epithelial down-growth and alveolar bone loss, while oral gavage with additional AB priming had greater tissular destruction in comparison with gavage alone (p < .05). Kava-241 treatment significantly (p < .05) reduced epithelial down-growth (72%) and alveolar bone loss (36%) in P. gingivalis+AB group. This Kava-241 effect was associated to a reduction in inflammatory cell counts within soft tissues and an increase in fibroblasts (p < .05). CONCLUSION: Priming with type II collagen antibody with oral gavage is a fast and reproducible model of periodontal destruction adequate for the evaluation of novel therapeutics. The effect of Kava-241 shows promise in the prevention and treatment of inflammation and alveolar bone loss associated with periodontitis. Further experiments are required to determine molecular pathways targeted by this therapeutic agent."
JAMES S PANEK,"Enantioselective multicomponent condensation reactions of phenols, aldehydes, and boronates catalyzed by chiral biphenols","Chiral diols and biphenols catalyze the multicomponent condensation reaction of phenols, aldehydes, and alkenyl or aryl boronates. The condensation products are formed in good yields and enantioselectivities. The reaction proceeds via an initial Friedel-Crafts alkylation of the aldehyde and phenol to yield an ortho-quinone methide that undergoes an enantioselective boronate addition. A cyclization pathway was discovered while exploring the scope of the reaction that provides access to chiral 2,4-diaryl chroman products, the core of which is a structural motif found in natural products."
JAMES S PANEK,Multicomponent condensation reactions via ortho-Quinone Methides,"Iron(III) salts promote the condensation of aldehydes or acetals with electron-rich phenols to generate ortho-quinone methides that undergo Diels-Alder condensations with alkenes. The reaction sequence occurs in a single vessel to afford benzopyrans in up to 95% yield. The reaction was discovered while investigating a two-component strategy using 2-(hydroxy(phenyl)methyl)phenols to access the desired ortho-quinone methide in a Diels-Alder condensation. The two-component condensation also afforded the corresponding benzopyran products in yields up to 97%. Taken together, the two- and three-component strategies using ortho-quinone methide intermediates provide efficient access to benzopyrans in good yields and selectivities."
RICARDO MUNARRIZ,"Streptozotocin-induced diabetes in the rat is associated with changes in vaginal hemodynamics, morphology and biochemical markers","BACKGROUND. Diabetes is associated with declining sexual function in women. However, the effects of diabetes on genital tissue structure, innervation and function remains poorly characterized. In control and streptozotocin-treated female rats, we investigated the effects of diabetes on vaginal blood flow, tissue morphology, and expression of arginase I, endothelial nitric oxide synthase (eNOS) and cGMP-dependent protein kinase (PKG), key enzymes that regulate smooth muscle relaxation. We further related these changes with estrogen receptor alpha (ERα) and androgen receptor (AR) expression. RESULTS. In addition to significantly elevated blood glucose levels, diabetic rats had decreased mean body weight, lower levels of plasma estradiol, and higher plasma testosterone concentration, compared to age-matched controls. Eight weeks after administration of buffer (control) or 65 mg/kg of streptozotocin (diabetic), the vaginal blood flow response to pelvic nerve stimulation was significantly reduced in diabetic rats. Histological examination of vaginal tissue from diabetic animals showed reduced epithelial thickness and atrophy of the muscularis layer. Diabetic animals also had reduced vaginal levels of eNOS and arginase I, but elevated levels of PKG, as assessed by Western blot analyses. These alterations were accompanied by a reduction in both ERα and AR in nuclear extracts of vaginal tissue from diabetic animals. CONCLUSION. In ovariectomized (estrogen deficient) animals, previous reports from our lab and others have documented changes in blood flow, tissue structure, ERα, arginase I and eNOS that parallel those observed in diabetic rats. We hypothesize that diabetes may lead to multiple disruptions in sex steroid hormone synthesis, metabolism and action. These pathological events may cause dramatic changes in tissue structure and key enzymes that regulate cell growth and smooth muscle contractility, ultimately affecting the genital response during sexual arousal."
THOMAS G BIFANO,Simultaneous multiplane imaging with reverberation multiphoton microscopy,"Multiphoton microscopy (MPM) has gained enormous popularity over the years for its capacity to provide high resolution images from deep within scattering samples1. However, MPM is generally based on single-point laser-focus scanning, which is intrinsically slow. While imaging speeds as fast as video rate have become routine for 2D planar imaging, such speeds have so far been unattainable for 3D volumetric imaging without severely compromising microscope performance. We demonstrate here 3D volumetric (multiplane) imaging at the same speed as 2D planar (single plane) imaging, with minimal compromise in performance. Specifically, multiple planes are acquired by near-instantaneous axial scanning while maintaining 3D micron-scale resolution. Our technique, called reverberation MPM, is well adapted for large-scale imaging in scattering media with low repetition-rate lasers, and can be implemented with conventional MPM as a simple add-on."
ANN C MCKEE,Clinicopathological evaluation of chronic traumatic encephalopathy in players of American football,"IMPORTANCE: Players of American football may be at increased risk of long-term neurological conditions, particularly chronic traumatic encephalopathy (CTE). OBJECTIVE: To determine the neuropathological and clinical features of deceased football players with CTE. DESIGN, SETTING, AND PARTICIPANTS: Case series of 202 football players whose brains were donated for research. Neuropathological evaluations and retrospective telephone clinical assessments (including head trauma history) with informants were performed blinded. Online questionnaires ascertained athletic and military history. EXPOSURES: Participation in American football at any level of play. MAIN OUTCOMES AND MEASURES: Neuropathological diagnoses of neurodegenerative diseases, including CTE, based on defined diagnostic criteria; CTE neuropathological severity (stages I to IV or dichotomized into mild [stages I and II] and severe [stages III and IV]); informant-reported athletic history and, for players who died in 2014 or later, clinical presentation, including behavior, mood, and cognitive symptoms and dementia. RESULTS: Among 202 deceased former football players (median age at death, 66 years [interquartile range, 47-76 years]), CTE was neuropathologically diagnosed in 177 players (87%; median age at death, 67 years [interquartile range, 52-77 years]; mean years of football participation, 15.1 [SD, 5.2]), including 0 of 2 pre–high school, 3 of 14 high school (21%), 48 of 53 college (91%), 9 of 14 semiprofessional (64%), 7 of 8 Canadian Football League (88%), and 110 of 111 National Football League (99%) players. Neuropathological severity of CTE was distributed across the highest level of play, with all 3 former high school players having mild pathology and the majority of former college (27 [56%]), semiprofessional (5 [56%]), and professional (101 [86%]) players having severe pathology. Among 27 participants with mild CTE pathology, 26 (96%) had behavioral or mood symptoms or both, 23 (85%) had cognitive symptoms, and 9 (33%) had signs of dementia. Among 84 participants with severe CTE pathology, 75 (89%) had behavioral or mood symptoms or both, 80 (95%) had cognitive symptoms, and 71 (85%) had signs of dementia. CONCLUSIONS AND RELEVANCE: In a convenience sample of deceased football players who donated their brains for research, a high proportion had neuropathological evidence of CTE, suggesting that CTE may be related to prior participation in football."
ANN C MCKEE,Acyl Peptide Hydrolase Degrades Monomeric and Oligomeric Amyloid-Beta Peptide,"BACKGROUND The abnormal accumulation of amyloid-beta peptide is believed to cause malfunctioning of neurons in the Alzheimer's disease brain. Amyloid-beta exists in different assembly forms in the aging mammalian brain including monomers, oligomers, and aggregates, and in senile plaques, fibrils. Recent findings suggest that soluble amyloid-beta oligomers may represent the primary pathological species in Alzheimer's disease and the most toxic form that impairs synaptic and thus neuronal function. We previously reported the isolation of a novel amyloid-beta-degrading enzyme, acyl peptide hydrolase, a serine protease that degrades amyloid-beta, and is different in structure and activity from other amyloid-beta-degrading enzymes. RESULTS Here we report the further characterization of acyl peptide hydrolase activity using mass spectrometry. Acyl peptide hydrolase cleaves the amyloid-beta peptide at amino acids 13, 14 and 19. In addition, by real-time PCR we found elevated acyl peptide hydrolase expression in brain areas rich in amyloid plaques suggesting that this enzyme's levels are responsive to increases in amyloid-beta levels. Lastly, tissue culture experiments using transfected CHO cells expressing APP751 bearing the V717F mutation indicate that acyl peptide hydrolase preferentially degrades dimeric and trimeric forms of amyloid-beta. CONCLUSION These data suggest that acyl peptide hydrolase is involved in the degradation of oligomeric amyloid-beta, an activity that, if induced, might present a new tool for therapy aimed at reducing neurodegeneration in the Alzheimer's brain."
ANN C MCKEE,"Concussion, microvascular injury, and early tauopathy in young athletes after impact head injury and an impact concussion mouse model","The mechanisms underpinning concussion, traumatic brain injury, and chronic traumatic encephalopathy, and the relationships between these disorders, are poorly understood. We examined post-mortem brains from teenage athletes in the acute-subacute period after mild closed-head impact injury and found astrocytosis, myelinated axonopathy, microvascular injury, perivascular neuroinflammation, and phosphorylated tau protein pathology. To investigate causal mechanisms, we developed a mouse model of lateral closed-head impact injury that uses momentum transfer to induce traumatic head acceleration. Unanaesthetized mice subjected to unilateral impact exhibited abrupt onset, transient course, and rapid resolution of a concussion-like syndrome characterized by altered arousal, contralateral hemiparesis, truncal ataxia, locomotor and balance impairments, and neurobehavioural deficits. Experimental impact injury was associated with axonopathy, blood–brain barrier disruption, astrocytosis, microgliosis (with activation of triggering receptor expressed on myeloid cells, TREM2), monocyte infiltration, and phosphorylated tauopathy in cerebral cortex ipsilateral and subjacent to impact. Phosphorylated tauopathy was detected in ipsilateral axons by 24 h, bilateral axons and soma by 2 weeks, and distant cortex bilaterally at 5.5 months post-injury. Impact pathologies co-localized with serum albumin extravasation in the brain that was diagnostically detectable in living mice by dynamic contrast-enhanced MRI. These pathologies were also accompanied by early, persistent, and bilateral impairment in axonal conduction velocity in the hippocampus and defective long-term potentiation of synaptic neurotransmission in the medial prefrontal cortex, brain regions distant from acute brain injury. Surprisingly, acute neurobehavioural deficits at the time of injury did not correlate with blood–brain barrier disruption, microgliosis, neuroinflammation, phosphorylated tauopathy, or electrophysiological dysfunction. Furthermore, concussion-like deficits were observed after impact injury, but not after blast exposure under experimental conditions matched for head kinematics. Computational modelling showed that impact injury generated focal point loading on the head and seven-fold greater peak shear stress in the brain compared to blast exposure. Moreover, intracerebral shear stress peaked before onset of gross head motion. By comparison, blast induced distributed force loading on the head and diffuse, lower magnitude shear stress in the brain. We conclude that force loading mechanics at the time of injury shape acute neurobehavioural responses, structural brain damage, and neuropathological sequelae triggered by neurotrauma. These results indicate that closed-head impact injuries, independent of concussive signs, can induce traumatic brain injury as well as early pathologies and functional sequelae associated with chronic traumatic encephalopathy. These results also shed light on the origins of concussion and relationship to traumatic brain injury and its aftermath."
ANN C MCKEE,The long-baseline neutrino experiment: exploring fundamental symmetries of the universe,"The preponderance of matter over antimatter in the early Universe, the dynamics of the supernova bursts that produced the heavy elements necessary for life and whether protons eventually decay --- these mysteries at the forefront of particle physics and astrophysics are key to understanding the early evolution of our Universe, its current state and its eventual fate. The Long-Baseline Neutrino Experiment (LBNE) represents an extensively developed plan for a world-class experiment dedicated to addressing these questions. LBNE is conceived around three central components: (1) a new, high-intensity neutrino source generated from a megawatt-class proton accelerator at Fermi National Accelerator Laboratory, (2) a near neutrino detector just downstream of the source, and (3) a massive liquid argon time-projection chamber deployed as a far detector deep underground at the Sanford Underground Research Facility. This facility, located at the site of the former Homestake Mine in Lead, South Dakota, is approximately 1,300 km from the neutrino source at Fermilab -- a distance (baseline) that delivers optimal sensitivity to neutrino charge-parity symmetry violation and mass ordering effects. This ambitious yet cost-effective design incorporates scalability and flexibility and can accommodate a variety of upgrades and contributions. With its exceptional combination of experimental configuration, technical capabilities, and potential for transformative discoveries, LBNE promises to be a vital facility for the field of particle physics worldwide, providing physicists from around the globe with opportunities to collaborate in a twenty to thirty year program of exciting science. In this document we provide a comprehensive overview of LBNE's scientific objectives, its place in the landscape of neutrino physics worldwide, the technologies it will incorporate and the capabilities it will possess."
VINCENT C. SMITH,A genome-wide association study reveals variants in ARL15 that influence adiponectin levels,"The adipocyte-derived protein adiponectin is highly heritable and inversely associated with risk of type 2 diabetes mellitus (T2D) and coronary heart disease (CHD). We meta-analyzed 3 genome-wide association studies for circulating adiponectin levels (n = 8,531) and sought validation of the lead single nucleotide polymorphisms (SNPs) in 5 additional cohorts (n = 6,202). Five SNPs were genome-wide significant in their relationship with adiponectin (P≤5×10−8). We then tested whether these 5 SNPs were associated with risk of T2D and CHD using a Bonferroni-corrected threshold of P≤0.011 to declare statistical significance for these disease associations. SNPs at the adiponectin-encoding ADIPOQ locus demonstrated the strongest associations with adiponectin levels (P-combined = 9.2×10−19 for lead SNP, rs266717, n = 14,733). A novel variant in the ARL15 (ADP-ribosylation factor-like 15) gene was associated with lower circulating levels of adiponectin (rs4311394-G, P-combined = 2.9×10−8, n = 14,733). This same risk allele at ARL15 was also associated with a higher risk of CHD (odds ratio [OR] = 1.12, P = 8.5×10−6, n = 22,421) more nominally, an increased risk of T2D (OR = 1.11, P = 3.2×10−3, n = 10,128), and several metabolic traits. Expression studies in humans indicated that ARL15 is well-expressed in skeletal muscle. These findings identify a novel protein, ARL15, which influences circulating adiponectin levels and may impact upon CHD risk. Author Summary Through a meta-analysis of genome-wide association studies of 14,733 individuals, we identified common base-pair variants in the genome which influence circulating adiponectin levels. Since adiponectin is an adipocyte-derived circulating protein which has been inversely associated with risk of obesity-related diseases such as type 2 diabetes (T2D) and coronary heart disease (CHD), we next sought to understand if the identified variants influencing adiponectin levels also influence risk of T2D, CHD, and several metabolic traits. In addition to confirming that variation at the ADIPOQ locus influences adiponectin levels, our analyses point to a variant in the ARL15 (ADP-ribosylation factor-like 15) locus which decreases adiponectin levels and increases risk of CHD and T2D. Further, this same variant was associated with increased fasting insulin levels and glycated hemoglobin. While the function of ARL15 is not known, we provide insight into the tissue specificity of ARL15 expression. These results thus provide novel insights into the physiology of the adiponectin pathway and obesity-related diseases."
VINCENT C. SMITH,Event Horizon Telescope imaging of the archetypal blazar 3C 279 at an extreme 20 microarcsecond resolution,"3C 279 is an archetypal blazar with a prominent radio jet that show broadband flux density variability across the entire electromagnetic spectrum. We use an ultra-high angular resolution technique – global Very Long Baseline Interferometry (VLBI) at 1.3 mm (230 GHz) – to resolve the innermost jet of 3C 279 in order to study its fine-scale morphology close to the jet base where highly variable γ-ray emission is thought to originate, according to various models. The source was observed during four days in April 2017 with the Event Horizon Telescope at 230 GHz, including the phased Atacama Large Millimeter/submillimeter Array (ALMA), at an angular resolution of ∼20 μas (at a redshift of z = 0.536 this corresponds to ∼0.13 pc  ∼ 1700 Schwarzschild radii with a black hole mass M<jats:sub>BH</jats:sub> = 8 × 10^8 M_⊙). Imaging and model-fitting techniques were applied to the data to parameterize the fine-scale source structure and its variation. We find a multicomponent inner jet morphology with the northernmost component elongated perpendicular to the direction of the jet, as imaged at longer wavelengths. The elongated nuclear structure is consistent on all four observing days and across different imaging methods and model-fitting techniques, and therefore appears robust. Owing to its compactness and brightness, we associate the northern nuclear structure as the VLBI “core”. This morphology can be interpreted as either a broad resolved jet base or a spatially bent jet. We also find significant day-to-day variations in the closure phases, which appear most pronounced on the triangles with the longest baselines. Our analysis shows that this variation is related to a systematic change of the source structure. Two inner jet components move non-radially at apparent speeds of ∼15 c and ∼20 c (∼1.3 and ∼1.7 μas day^−1, respectively), which more strongly supports the scenario of traveling shocks or instabilities in a bent, possibly rotating jet. The observed apparent speeds are also coincident with the 3C 279 large-scale jet kinematics observed at longer (cm) wavelengths, suggesting no significant jet acceleration between the 1.3 mm core and the outer jet. The intrinsic brightness temperature of the jet components are ≲10^10 K, a magnitude or more lower than typical values seen at ≥7 mm wavelengths. The low brightness temperature and morphological complexity suggest that the core region of 3C 279 becomes optically thin at short (mm) wavelengths."
VINCENT C. SMITH,First M87 Event Horizon Telescope results. II. Array and instrumentation,"The Event Horizon Telescope (EHT) is a very long baseline interferometry (VLBI) array that comprises millimeter- and submillimeter-wavelength telescopes separated by distances comparable to the diameter of the Earth. At a nominal operating wavelength of ~1.3 mm, EHT angular resolution (λ/D) is ~25 μas, which is sufficient to resolve nearby supermassive black hole candidates on spatial and temporal scales that correspond to their event horizons. With this capability, the EHT scientific goals are to probe general relativistic effects in the strong-field regime and to study accretion and relativistic jet formation near the black hole boundary. In this Letter we describe the system design of the EHT, detail the technology and instrumentation that enable observations, and provide measures of its performance. Meeting the EHT science objectives has required several key developments that have facilitated the robust extension of the VLBI technique to EHT observing wavelengths and the production of instrumentation that can be deployed on a heterogeneous array of existing telescopes and facilities. To meet sensitivity requirements, high-bandwidth digital systems were developed that process data at rates of 64 gigabit s−1, exceeding those of currently operating cm-wavelength VLBI arrays by more than an order of magnitude. Associated improvements include the development of phasing systems at array facilities, new receiver installation at several sites, and the deployment of hydrogen maser frequency standards to ensure coherent data capture across the array. These efforts led to the coordination and execution of the first Global EHT observations in 2017 April, and to event-horizon-scale imaging of the supermassive black hole candidate in M87."
VINCENT C. SMITH,First M87 Event Horizon Telescope results. I. The shadow of the supermassive black hole,"When surrounded by a transparent emission region, black holes are expected to reveal a dark shadow caused by gravitational light bending and photon capture at the event horizon. To image and study this phenomenon, we have assembled the Event Horizon Telescope, a global very long baseline interferometry array observing at a wavelength of 1.3 mm. This allows us to reconstruct event-horizon-scale images of the supermassive black hole candidate in the center of the giant elliptical galaxy M87. We have resolved the central compact radio source as an asymmetric bright emission ring with a diameter of 42 ± 3 μas, which is circular and encompasses a central depression in brightness with a flux ratio gsim10:1. The emission ring is recovered using different calibration and imaging schemes, with its diameter and width remaining stable over four different observations carried out in different days. Overall, the observed image is consistent with expectations for the shadow of a Kerr black hole as predicted by general relativity. The asymmetry in brightness in the ring can be explained in terms of relativistic beaming of the emission from a plasma rotating close to the speed of light around a black hole. We compare our images to an extensive library of ray-traced general-relativistic magnetohydrodynamic simulations of black holes and derive a central mass of M = (6.5 ± 0.7) × 10^9 M ⊙. Our radio-wave observations thus provide powerful evidence for the presence of supermassive black holes in centers of galaxies and as the central engines of active galactic nuclei. They also present a new tool to explore gravity in its most extreme limit and on a mass scale that was so far not accessible."
TIBOR PALFAI,"Development of a tailored, telehealth intervention to address chronic pain and heavy drinking among people with HIV infection: integrating perspectives of patients in HIV care.","BACKGROUND: Chronic pain and heavy drinking commonly co-occur and can infuence the course of HIV. There have been no interventions designed to address both of these conditions among people living with HIV (PLWH), and none that have used telehealth methods. The purpose of this study was to better understand pain symptoms, patterns of alcohol use, treatment experiences, and technology use among PLWH in order to tailor a telehealth intervention that addresses these conditions SUBJECTS: Ten participants with moderate or greater chronic pain and heavy drinking were recruited from a cohort of patients engaged in HIV-care (Boston Alcohol Research Collaborative on HIV/AIDS Cohort) and from an integrated HIV/primary care clinic at a large urban hospital. METHODS: One-on-one interviews were conducted with participants to understand experiences and treatment of HIV, chronic pain, and alcohol use. Participants’ perceptions of the infuence of alcohol on HIV and chronic pain were explored as was motivation to change drinking. Technology use and treatment preferences were examined in the fnal section of the interview. Interviews were recorded, transcribed and uploaded into NVivo® v12 software for analysis. A codebook was developed based on interviews followed by thematic analysis in which specifc meanings were assigned to codes. RESULTS: A number of themes were identifed that had implications for intervention tailoring including: resilience in coping with HIV; autonomy in health care decision-making; coping with pain, stress, and emotion; understanding treatment rationale; depression and social withdrawal; motives to drink and refrain from drinking; technology use and capacity; and preference for intervention structure and style. Ratings of intervention components indicated that participants viewed each of the proposed intervention content areas as “helpful” to “very helpful”. Videoconferencing was viewed as an acceptable modality for intervention delivery CONCLUSIONS: Results helped specify treatment targets and provided information about how to enhance intervention delivery. The interviews supported the view that videoconferencing is an acceptable telehealth method of addressing chronic pain and heavy drinking among PLWH."
TIBOR PALFAI,When wanting to change is not enough: automatic appetitive processes moderate the effects of a brief alcohol intervention in hazardous-drinking college students.,"BACKGROUND: Research indicates that brief motivational interventions are efficacious treatments for hazardous drinking. Little is known, however, about the psychological processes that may moderate intervention success. Based on growing evidence that drinking behavior may be influenced by automatic (nonvolitional) mental processes, the current study examined whether automatic alcohol-approach associations moderated the effect of a brief motivational intervention. Specifically, we examined whether the efficacy of a single-session intervention designed to increase motivation to reduce alcohol consumption would be moderated by the strength of participants' automatic alcohol-approach associations. METHODS: Eighty-seven undergraduate hazardous drinkers participated for course credit. Participants completed an Implicit Association Test to measure automatic alcohol-approach associations, a baseline measure of readiness to change drinking behavior, and measures of alcohol involvement. Participants were then randomly assigned to either a brief (15-minute) motivational intervention or a control condition. Participants completed a measure of readiness to change drinking at the end of the first session and returned for a follow-up session six weeks later in which they reported on their drinking over the previous month. RESULTS: Compared with the control group, those in the intervention condition showed higher readiness to change drinking at the end of the baseline session but did not show decreased drinking quantity at follow-up. Automatic alcohol-approach associations moderated the effects of the intervention on change in drinking quantity. Among participants in the intervention group, those with weak automatic alcohol-approach associations showed greater reductions in the amount of alcohol consumed per occasion at follow-up compared with those with strong automatic alcohol-approach associations. Automatic appetitive associations with alcohol were not related with change in amount of alcohol consumed per occasion in control participants. Furthermore, among participants who showed higher readiness to change, those who exhibited weaker alcohol-approach associations showed greater reductions in drinking quantity compared with those who exhibited stronger alcohol-approach associations. CONCLUSIONS: The results support the idea that automatic mental processes may moderate the influence of brief motivational interventions on quantity of alcohol consumed per drinking occasion. The findings suggest that intervention efficacy may be improved by utilizing implicit measures to identify those who may be responsive to brief interventions and by developing intervention elements to address the influence of automatic processes on drinking behavior."
MICHAEL E HASSELMO,3D multimodal dataset and token-based pose optimization,
MICHAEL E HASSELMO,Impact of optogenetic pulse design on CA3 learning and replay: a neural model.,"Optogenetic manipulation of hippocampal circuitry is an important tool for investigating learning in vivo. Numerous approaches to pulse design have been employed to elicit desirable circuit and behavioral outcomes. Here, we systematically test the outcome of different single-pulse waveforms in a rate-based model of hippocampal memory function at the level of mnemonic replay extension and de novo synaptic weight formation in CA3 and CA1. Lower-power waveforms with long forward or forward and backward ramps yield more natural sequence replay dynamics and induce synaptic plasticity that allows for more natural memory replay timing, in contrast to square or backward ramps. These differences between waveform shape and amplitude are preserved with the addition of noise in membrane potential, light scattering, and protein expression, improving the potential validity of predictions for in vivo work. These results inform future optogenetic experimental design choices in the field of learning and memory."
MICHAEL E HASSELMO,"Individual differences in human path integration abilities correlate with gray matter volume in retrosplenial cortex, hippocampus, and medial prefrontal cortex","Humans differ in their individual navigational abilities. These individual differences may exist in part because successful navigation relies on several disparate abilities, which rely on different brain structures. One such navigational capability is path integration, the updating of position and orientation, in which navigators track distances, directions, and locations in space during movement. Although structural differences related to landmark-based navigation have been examined, gray matter volume related to path integration ability has not yet been tested. Here, we examined individual differences in two path integration paradigms: (1) a location tracking task and (2) a task tracking translational and rotational self-motion. Using voxel-based morphometry, we related differences in performance in these path integration tasks to variation in brain morphology in 26 healthy young adults. Performance in the location tracking task positively correlated with individual differences in gray matter volume in three areas critical for path integration: the hippocampus, the retrosplenial cortex, and the medial prefrontal cortex. These regions are consistent with the path integration system known from computational and animal models and provide novel evidence that morphological variability in retrosplenial and medial prefrontal cortices underlies individual differences in human path integration ability. The results for tracking rotational self-motion-but not translation or location-demonstrated that cerebellum gray matter volume correlated with individual performance. Our findings also suggest that these three aspects of path integration are largely independent. Together, the results of this study provide a link between individual abilities and the functional correlates, computational models, and animal models of path integration."
MICHAEL E HASSELMO,Flexible resonance in prefrontal networks with strong feedback inhibition,"Oscillations are ubiquitous features of brain dynamics that undergo task-related changes in synchrony, power, and frequency. The impact of those changes on target networks is poorly understood. In this work, we used a biophysically detailed model of prefrontal cortex (PFC) to explore the effects of varying the spike rate, synchrony, and waveform of strong oscillatory inputs on the behavior of cortical networks driven by them. Interacting populations of excitatory and inhibitory neurons with strong feedback inhibition are inhibition-based network oscillators that exhibit resonance (i.e., larger responses to preferred input frequencies). We quantified network responses in terms of mean firing rates and the population frequency of network oscillation; and characterized their behavior in terms of the natural response to asynchronous input and the resonant response to oscillatory inputs. We show that strong feedback inhibition causes the PFC to generate internal (natural) oscillations in the beta/gamma frequency range (>15 Hz) and to maximize principal cell spiking in response to external oscillations at slightly higher frequencies. Importantly, we found that the fastest oscillation frequency that can be relayed by the network maximizes local inhibition and is equal to a frequency even higher than that which maximizes the firing rate of excitatory cells; we call this phenomenon population frequency resonance. This form of resonance is shown to determine the optimal driving frequency for suppressing responses to asynchronous activity. Lastly, we demonstrate that the natural and resonant frequencies can be tuned by changes in neuronal excitability, the duration of feedback inhibition, and dynamic properties of the input. Our results predict that PFC networks are tuned for generating and selectively responding to beta- and gamma-rhythmic signals due to the natural and resonant properties of inhibition-based oscillators. They also suggest strategies for optimizing transcranial stimulation and using oscillatory networks in neuromorphic engineering."
MICHAEL E HASSELMO,Coincidence detection of place and temporal context in a network model of spiking hippocampal neurons,"Recent advances in single-neuron biophysics have enhanced our understanding of information processing on the cellular level, but how the detailed properties of individual neurons give rise to large-scale behavior remains unclear. Here, we present a model of the hippocampal network based on observed biophysical properties of hippocampal and entorhinal cortical neurons. We assembled our model to simulate spatial alternation, a task that requires memory of the previous path through the environment for correct selection of the current path to a reward site. The convergence of inputs from entorhinal cortex and hippocampal region CA3 onto CA1 pyramidal cells make them potentially important for integrating information about place and temporal context on the network level. Our model shows how place and temporal context information might be combined in CA1 pyramidal neurons to give rise to splitter cells, which fire selectively based on a combination of place and temporal context. The model leads to a number of experimentally testable predictions that may lead to a better understanding of the biophysical basis of information processing in the hippocampus. Author Summary. Understanding how behavior is connected to cellular and network processes is one of the most important challenges in neuroscience, and computational modeling allows one to directly formulate hypotheses regarding the interactions between these scales. We present a model of the hippocampal network, an area of the brain important for spatial navigation and episodic memory, memory of ""what, when, and where."" We show how the model, which consists of neurons and connections based on biophysical properties known from experiments, can guide a virtual rat through the spatial alternation task by storing a memory of the previous path through an environment. Our model shows how neurons that fire selectively based on both the current location and past trajectory of the animal (dubbed ""splitter cells"") might emerge from a newly discovered biophysical interaction in these cells. Our model is not intended to be comprehensive, but rather to contain just enough detail to achieve performance of the behavioral task. Goals of this approach are to present a scenario by which the gap between biophysics and behavior can be bridged and to provide a framework for the formulation of experimentally testable hypotheses."
MICHAEL E HASSELMO,The Influence of Markov Decision Process Structure on the Possible Strategic Use of Working Memory and Episodic Memory,"Researchers use a variety of behavioral tasks to analyze the effect of biological manipulations on memory function. This research will benefit from a systematic mathematical method for analyzing memory demands in behavioral tasks. In the framework of reinforcement learning theory, these tasks can be mathematically described as partially-observable Markov decision processes. While a wealth of evidence collected over the past 15 years relates the basal ganglia to the reinforcement learning framework, only recently has much attention been paid to including psychological concepts such as working memory or episodic memory in these models. This paper presents an analysis that provides a quantitative description of memory states sufficient for correct choices at specific decision points. Using information from the mathematical structure of the task descriptions, we derive measures that indicate whether working memory (for one or more cues) or episodic memory can provide strategically useful information to an agent. In particular, the analysis determines which observed states must be maintained in or retrieved from memory to perform these specific tasks. We demonstrate the analysis on three simplified tasks as well as eight more complex memory tasks drawn from the animal and human literature (two alternation tasks, two sequence disambiguation tasks, two non-matching tasks, the 2-back task, and the 1-2-AX task). The results of these analyses agree with results from quantitative simulations of the task reported in previous publications and provide simple indications of the memory demands of the tasks which can require far less computation than a full simulation of the task. This may provide a basis for a quantitative behavioral stoichiometry of memory tasks."
MICHAEL E HASSELMO,Analyses of Markov Decision Process Structure Regarding the Possible Strategic Use of Interacting Memory Systems,"Behavioral tasks are often used to study the different memory systems present in humans and animals. Such tasks are usually designed to isolate and measure some aspect of a single memory system. However, it is not necessarily clear that any given task actually does isolate a system or that the strategy used by a subject in the experiment is the one desired by the experimenter. We have previously shown that when tasks are written mathematically as a form of partially observable Markov decision processes, the structure of the tasks provide information regarding the possible utility of certain memory systems. These previous analyses dealt with the disambiguation problem: given a specific ambiguous observation of the environment, is there information provided by a given memory strategy that can disambiguate that observation to allow a correct decision? Here we extend this approach to cases where multiple memory systems can be strategically combined in different ways. Specifically, we analyze the disambiguation arising from three ways by which episodic-like memory retrieval might be cued (by another episodic-like memory, by a semantic association, or by working memory for some earlier observation). We also consider the disambiguation arising from holding earlier working memories, episodic-like memories or semantic associations in working memory. From these analyses we can begin to develop a quantitative hierarchy among memory systems in which stimulus-response memories and semantic associations provide no disambiguation while the episodic memory system provides the most flexible disambiguation, with working memory at an intermediate level."
MICHAEL E HASSELMO,Working Memory Performance Correlates with Prefrontal-Hippocampal Theta Interactions but not with Prefrontal Neuron Firing Rates,"Performance of memory tasks is impaired by lesions to either the medial prefrontal cortex (mPFC) or the hippocampus (HPC); although how these two areas contribute to successful performance is not well understood. mPFC unit activity is temporally affected by hippocampal-theta oscillations, with almost half the mPFC population entrained to theta in behaving animals, pointing to theta interactions as the mechanism enabling collaborations between these two areas. mPFC neurons respond to sensory stimuli and responses in working memory tasks, though the function of these correlated firing rate changes remains unclear because similar responses are reported during mPFC dependent and independent tasks. Using a DNMS task we compared error trials vs. correct trials and found almost all mPFC cells fired at similar rates during both error and correct trials (92%), however theta-entrainment of mPFC neurons declined during error performance as only 17% of cells were theta-entrained (during correct trials 46% of the population was theta-entrained). Across the population, error and correct trials did not differ in firing rate, but theta-entrainment was impaired. Periods of theta-entrainment and firing rate changes appeared to be independent variables, and only theta-entrainment was correlated with successful performance, indicating mPFC-HPC theta-range interactions are the key to successful DNMS performance."
MICHAEL E HASSELMO,Differences in visual-spatial input may underlie different compression properties of firing fields for grid cell modules in medial entorhinal cortex,"Firing fields of grid cells in medial entorhinal cortex show compression or expansion after manipulations of the location of environmental barriers. This compression or expansion could be selective for individual grid cell modules with particular properties of spatial scaling. We present a model for differences in the response of modules to barrier location that arise from different mechanisms for the influence of visual features on the computation of location that drives grid cell firing patterns. These differences could arise from differences in the position of visual features within the visual field. When location was computed from the movement of visual features on the ground plane (optic flow) in the ventral visual field, this resulted in grid cell spatial firing that was not sensitive to barrier location in modules modeled with small spacing between grid cell firing fields. In contrast, when location was computed from static visual features on walls of barriers, i.e. in the more dorsal visual field, this resulted in grid cell spatial firing that compressed or expanded based on the barrier locations in modules modeled with large spacing between grid cell firing fields. This indicates that different grid cell modules might have differential properties for computing location based on visual cues, or the spatial radius of sensitivity to visual cues might differ between modules."
MICHAEL E HASSELMO,Comparison of properties of medial entorhinal cortex layer II neurons in two anatomical dimensions with and without cholinergic activation,"Mechanisms underlying grid cell firing in the medial entorhinal cortex (MEC) still remain unknown. Computational modeling studies have suggested that cellular properties such as spike frequency adaptation and persistent firing might underlie the grid cell firing. Recent in vivo studies also suggest that cholinergic activation influences grid cell firing. Here we investigated the anatomical distribution of firing frequency adaptation, the medium spike after hyperpolarization potential (mAHP), subthreshold membrane potential oscillations, sag potential, input resistance and persistent firing, in MEC layer II principal cells using in vitro whole-cell patch clamp recordings in rats. Anatomical distributions of these properties were compared along both the dorso-ventral and medio-lateral axes, both with and without the cholinergic receptor agonist carbachol. We found that spike frequency adaptation is significantly stronger in ventral than in dorsal neurons both with and without carbachol. Spike frequency adaptation was significantly correlated with the duration of the mAHP, which also showed a gradient along the dorso-ventral axis. In carbachol, we found that about 50% of MEC layer II neurons show persistent firing which lasted more than 30 seconds. Persistent firing of MEC layer II neurons might contribute to grid cell firing by providing the excitatory drive. Dorso-ventral differences in spike frequency adaptation we report here are opposite from previous predictions by a computational model. We discuss an alternative mechanism as to how dorso-ventral differences in spike frequency adaptation could contribute to different scales of grid spacing."
MICHAEL E HASSELMO,Evaluation of the Oscillatory Interference Model of Grid Cell Firing through Analysis and Measured Period Variance of Some Biological Oscillators,"Models of the hexagonally arrayed spatial activity pattern of grid cell firing in the literature generally fall into two main categories: continuous attractor models or oscillatory interference models. Burak and Fiete (2009, PLoS Comput Biol) recently examined noise in two continuous attractor models, but did not consider oscillatory interference models in detail. Here we analyze an oscillatory interference model to examine the effects of noise on its stability and spatial firing properties. We show analytically that the square of the drift in encoded position due to noise is proportional to time and inversely proportional to the number of oscillators. We also show there is a relatively fixed breakdown point, independent of many parameters of the model, past which noise overwhelms the spatial signal. Based on this result, we show that a pair of oscillators are expected to maintain a stable grid for approximately t = 5µ3/(4πσ)2 seconds where µ is the mean period of an oscillator in seconds and σ2 its variance in seconds2. We apply this criterion to recordings of individual persistent spiking neurons in postsubiculum (dorsal presubiculum) and layers III and V of entorhinal cortex, to subthreshold membrane potential oscillation recordings in layer II stellate cells of medial entorhinal cortex and to values from the literature regarding medial septum theta bursting cells. All oscillators examined have expected stability times far below those seen in experimental recordings of grid cells, suggesting the examined biological oscillators are unfit as a substrate for current implementations of oscillatory interference models. However, oscillatory interference models can tolerate small amounts of noise, suggesting the utility of circuit level effects which might reduce oscillator variability. Further implications for grid cell models are discussed. Author Summary For many animals, including rats, accurate spatial memory over relatively large areas is important in order to find food and shelter. Just as unique points in time can be efficiently represented by combinations of repeating elements like hours, days, and months, points in space can be represented as combinations of elements that repeat at different spatial scales. Just such a code has been identified in the brains of rats and it shows an intriguing triangular spacing of encoded locations. Two different explanations have been developed as to what general mechanism in the brain might be able to generate this unusual code. However, to date there is not conclusive experimental evidence indicating whether either of the two explanations is correct. Here we show in detail that one of the explanations, called oscillatory interference, has specific requirements regarding the amount of variability in the system that implements it. We then report data experimentally examining candidate systems to evaluate their levels of noise. The large amount of noise that we find presents a challenge to the currently suggested biological implementations of oscillatory interference, but it does not provide support for the alternative explanation."
MICHAEL E HASSELMO,Cognitive computation using neural representations of time and space in the Laplace domain,"Memory for the past makes use of a record of what happened when---a function over past time. Time cells in the hippocampus and temporal context cells in the entorhinal cortex both code for events as a function of past time, but with very different receptive fields. Time cells in the hippocampus can be understood as a compressed estimate of events as a function of the past. Temporal context cells in the entorhinal cortex can be understood as the Laplace transform of that function, respectively. Other functional cell types in the hippocampus and related regions, including border cells, place cells, trajectory coding, splitter cells, can be understood as coding for functions over space or past movements or their Laplace transforms. More abstract quantities, like distance in an abstract conceptual space or numerosity could also be mapped onto populations of neurons coding for the Laplace transform of functions over those variables. Quantitative cognitive models of memory and evidence accumulation can also be specified in this framework allowing constraints from both behavior and neurophysiology. More generally, the computational power of the Laplace domain could be important for efficiently implementing data-independent operators, which could serve as a basis for neural models of a very broad range of cognitive computations."
MICHAEL E HASSELMO,Internally generated time in the rodent hippocampus is logarithmically compressed,"The Weber-Fechner law proposes that our perceived sensory input increases with physical input on a logarithmic scale. Hippocampal 'time cells' carry a record of recent experience by firing sequentially during a circumscribed period of time after a triggering stimulus. Different cells have 'time fields' at different delays up to at least tens of seconds. Past studies suggest that time cells represent a compressed timeline by demonstrating that fewer time cells fire late in the delay and their time fields are wider. This paper asks whether the compression of time cells obeys the Weber-Fechner Law. Time cells were studied with a hierarchical Bayesian model that simultaneously accounts for the firing pattern at the trial level, cell level, and population level. This procedure allows separate estimates of the within-trial receptive field width and the across-trial variability. After isolating across-trial variability, time field width increased linearly with delay. Further, the time cell population was distributed evenly along a logarithmic time axis. These findings provide strong quantitative evidence that the neural temporal representation in rodent hippocampus is logarithmically compressed and obeys a neural Weber-Fechner Law."
MICHAEL E HASSELMO,"During running in place, grid cells integrate elapsed time and distance run","The spatial scale of grid cells may be provided by self-generated motion information or by external sensory information from environmental cues. To determine whether grid cell activity reflects distance traveled or elapsed time independent of external information, we recorded grid cells as animals ran in place on a treadmill. Grid cell activity was only weakly influenced by location, but most grid cells and other neurons recorded from the same electrodes strongly signaled a combination of distance and time, with some signaling only distance or time. Grid cells were more sharply tuned to time and distance than non-grid cells. Many grid cells exhibited multiple firing fields during treadmill running, parallel to the periodic firing fields observed in open fields, suggesting a common mode of information processing. These observations indicate that, in the absence of external dynamic cues, grid cells integrate self-generated distance and time information to encode a representation of experience."
MICHAEL E HASSELMO,"Linking Cellular Mechanisms to Behavior: Entorhinal Persistent Spiking and Membrane Potential Oscillations May Underlie Path Integration, Grid Cell Firing, and Episodic Memory","The entorhinal cortex plays an important role in spatial memory and episodic memory functions. These functions may result from cellular mechanisms for integration of the afferent input to entorhinal cortex. This article reviews physiological data on persistent spiking and membrane potential oscillations in entorhinal cortex then presents models showing how both these cellular mechanisms could contribute to properties observed during unit recording, including grid cell firing, and how they could underlie behavioural functions including path integration. The interaction of oscillations and persistent firing could contribute to encoding and retrieval of trajectories through space and time as a mechanism relevant to episodic memory."
MICHAEL E HASSELMO,Sources of the Spatial Code Within the Hippocampus,"Neurons in the hippocampus are thought to provide information on an animal's location within its environment. Input to the hippocampus comes via afferents from the entorhinal cortex, which are separated into several major pathways serving different hippocampal regions. Recent studies show the significance of individual afferent pathways in location perception, enhancing our understanding of hippocampal function."
MICHAEL E HASSELMO,Segregation of cortical head direction cell assemblies on alternating theta cycles,"High-level cortical systems for spatial navigation, including entorhinal grid cells, critically depend on input from the head direction system. We examined spiking rhythms and modes of synchrony between neurons participating in head direction networks for evidence of internal processing, independent of direct sensory drive, which may be important for grid cell function. We found that head direction networks of rats were segregated into at least two populations of neurons firing on alternate theta cycles (theta cycle skipping) with fixed synchronous or anti-synchronous relationships. Pairs of anti-synchronous theta cycle skipping neurons exhibited larger differences in head direction tuning, with a minimum difference of 40 degrees of head direction. Septal inactivation preserved the head direction signal, but eliminated theta cycle skipping of head direction cells and grid cell spatial periodicity. We propose that internal mechanisms underlying cycle skipping in head direction networks may be critical for downstream spatial computation by grid cells."
CURTIS E WOODCOCK,Near-real-time monitoring of insect defoliation using landsat time series,"Introduced insects and pathogens impact millions of acres of forested land in the United States each year, and large-scale monitoring efforts are essential for tracking the spread of outbreaks and quantifying the extent of damage. However, monitoring the impacts of defoliating insects presents a significant challenge due to the ephemeral nature of defoliation events. Using the 2016 gypsy moth (Lymantria dispar) outbreak in Southern New England as a case study, we present a new approach for near-real-time defoliation monitoring using synthetic images produced from Landsat time series. By comparing predicted and observed images, we assessed changes in vegetation condition multiple times over the course of an outbreak. Initial measures can be made as imagery becomes available, and season-integrated products provide a wall-to-wall assessment of potential defoliation at 30 m resolution. Qualitative and quantitative comparisons suggest our Landsat Time Series (LTS) products improve identification of defoliation events relative to existing products and provide a repeatable metric of change in condition. Our synthetic-image approach is an important step toward using the full temporal potential of the Landsat archive for operational monitoring of forest health over large extents, and provides an important new tool for understanding spatial and temporal dynamics of insect defoliators."
CURTIS E WOODCOCK,Implications of land use change on the national terrestrial carbon budget of Georgia,"BACKGROUND: Globally, the loss of forests now contributes almost 20% of carbon dioxide emissions to the atmosphere. There is an immediate need to reduce the current rates of forest loss, and the associated release of carbon dioxide, but for many areas of the world these rates are largely unknown. The Soviet Union contained a substantial part of the world's forests and the fate of those forests and their effect on carbon dynamics remain unknown for many areas of the former Eastern Bloc. For Georgia, the political and economic transitions following independence in 1991 have been dramatic. In this paper we quantify rates of land use changes and their effect on the terrestrial carbon budget for Georgia. A carbon book-keeping model traces changes in carbon stocks using historical and current rates of land use change. Landsat satellite images acquired circa 1990 and 2000 were analyzed to detect changes in forest cover since 1990. RESULTS: The remote sensing analysis showed that a modest forest loss occurred, with approximately 0.8% of the forest cover having disappeared after 1990. Nevertheless, growth of Georgian forests still contribute a current national sink of about 0.3 Tg of carbon per year, which corresponds to 31% of the country anthropogenic carbon emissions. CONCLUSIONS: We assume that the observed forest loss is mainly a result of illegal logging, but we have not found any evidence of large-scale clear-cutting. Instead local harvesting of timber for household use is likely to be the underlying driver of the observed logging. The Georgian forests are a currently a carbon sink and will remain as such until about 2040 if the current rate of deforestation persists. Forest protection efforts, combined with economic growth, are essential for reducing the rate of deforestation and protecting the carbon sink provided by Georgian forests."
CURTIS E WOODCOCK,"ART and ARTMAP Neural Networks for Applications: Self-Organizing Learning, Recognition, and Prediction","ART and ARTMAP neural networks for adaptive recognition and prediction have been applied to a variety of problems. Applications include parts design retrieval at the Boeing Company, automatic mapping from remote sensing satellite measurements, medical database prediction, and robot vision. This chapter features a self-contained introduction to ART and ARTMAP dynamics and a complete algorithm for applications. Computational properties of these networks are illustrated by means of remote sensing and medical database examples. The basic ART and ARTMAP networks feature winner-take-all (WTA) competitive coding, which groups inputs into discrete recognition categories. WTA coding in these networks enables fast learning, that allows the network to encode important rare cases but that may lead to inefficient category proliferation with noisy training inputs. This problem is partially solved by ART-EMAP, which use WTA coding for learning but distributed category representations for test-set prediction. In medical database prediction problems, which often feature inconsistent training input predictions, the ARTMAP-IC network further improves ARTMAP performance with distributed prediction, category instance counting, and a new search algorithm. A recently developed family of ART models (dART and dARTMAP) retains stable coding, recognition, and prediction, but allows arbitrarily distributed category representation during learning as well as performance."
CURTIS E WOODCOCK,ART Neural Networks for Remote Sensing: Vegetation Classification from Landsat TM and Terrain Data,"A new methodology for automatic mapping from Landsat Thematic Mapper (TM) and terrain data, based on the fuzzy ARTMAP neural network, is developed. System capabilities are tested on a challenging remote sensing classification problem, using spectral and terrain features for vegetation classification in the Cleveland National Forest. After training at the pixel level, system capabilities arc tested at the stand level, using sites not seen during training. Results are compared to those of maximum likelihood classifiers, as well as back propagation neural networks and K Nearest Neighbor algorithms. ARTMAP dynamics arc fast, stable, and scalable, overcoming common limitations of back propagation, which did not give satisfactory performance. Best results arc obtained using a hybrid system based on a convex combination of fuzzy ARTMAP and maximum likelihood predictions. Fuzzy ARTMAP automatically constructs a minimal number of recognition categories to meet accuracy criteria. A voting strategy improves prediction by training the system several times. on different orderings of an Input set. Voting assigns confidence estimates to competing predictions."
CURTIS E WOODCOCK,Art Neural Networks for Remote Sensing: Vegetation Classification from Landsat TM and Terrain Data,"A new methodology for automatic mapping from Landsat Thematic Mapper (TM) and terrain data, based on the fuzzy ARTMAP neural network, is developed. System capabilities are tested on a challenging remote sensing classification problem, using spectral and terrain features for vegetation classification in the Cleveland National Forest. After training at the pixel level, system performance is tested at the stand level, using sites not seen during training. Results are compared to those of maximum likelihood classifiers, as well as back propagation neural networks and K Nearest Neighbor algorithms. ARTMAP dynamics are fast, stable, and scalable, overcoming common limitations of back propagation, which did not give satisfactory performance. Best results are obtained using a hybrid system based on a convex combination of fuzzy ARTMAP and maximum likelihood predictions. A prototype remote sensing example introduces each aspect of data processing and fuzzy ARTMAP classification. The example shows how the network automatically constructs a minimal number of recognition categories to meet accuracy criteria. A voting strategy improves prediction and assigns confidence estimates by training the system several times on different orderings of an input set."
CURTIS E WOODCOCK,Implications of land use change on the National Terrestrial Carbon Budget of Georgia,"BACKGROUND: Globally, the loss of forests now contributes almost 20% of carbon dioxide emissions to the atmosphere. There is an immediate need to reduce the current rates of forest loss, and the associated release of carbon dioxide, but for many areas of the world these rates are largely unknown. The Soviet Union contained a substantial part of the world's forests and the fate of those forests and their effect on carbon dynamics remain unknown for many areas of the former Eastern Bloc. For Georgia, the political and economic transitions following independence in 1991 have been dramatic. In this paper we quantify rates of land use changes and their effect on the terrestrial carbon budget for Georgia. A carbon book-keeping model traces changes in carbon stocks using historical and current rates of land use change. Landsat satellite images acquired circa 1990 and 2000 were analyzed to detect changes in forest cover since 1990. RESULTS: The remote sensing analysis showed that a modest forest loss occurred, with approximately 0.8% of the forest cover having disappeared after 1990. Nevertheless, growth of Georgian forests still contribute a current national sink of about 0.3 Tg of carbon per year, which corresponds to 31% of the country anthropogenic carbon emissions. CONCLUSIONS: We assume that the observed forest loss is mainly a result of illegal logging, but we have not found any evidence of large-scale clear-cutting. Instead local harvesting of timber for household use is likely to be the underlying driver of the observed logging. The Georgian forests are a currently a carbon sink and will remain as such until about 2040 if the current rate of deforestation persists. Forest protection efforts, combined with economic growth, are essential for reducing the rate of deforestation and protecting the carbon sink provided by Georgian forests."
CURTIS E WOODCOCK,Monitoring shifting cultivation in Laos with Landsat time series,"Shifting cultivation is an agricultural practice in which plots of land are cultivated temporarily, then abandoned, and vegetation is allowed to regenerate during fallow periods. Shifting cultivation is usually associated with cutting and burning forests and thus it is an important driver of forest disturbance in the tropics. However, studies of shifting cultivation are limited, and current area estimates of shifting cultivation are highly uncertain. Although Southeast Asia is a hotspot of shifting cultivation, there are no national maps of shifting cultivation in Southeast Asia at moderate or high resolution (less than or equal to 30 m). Monitoring shifting cultivation is challenging because it is highly dynamic, small-scale and results in complex post-disturbance landscapes. In this study, we monitored shifting cultivation using Landsat time series on Google Earth Engine for the entire country of Laos from 1991 to 2020. First, CCDC-SMA (Continuous Change Detection and Classification - Spectral Mixture Analysis) was used to detect forest disturbances. Then, these disturbances were attributed by combining time series analysis, object-based image analysis (OBIA), and post-disturbance land cover classification. Forest disturbances were assigned to Shifting cultivation, New plantation, Deforestation, Severe Drought, and Subtle Disturbance annually from 1991 to 2020 at a 30-m resolution. The major forest disturbances in 1991–2020 were mapped with an overall accuracy of 85%. Shifting cultivation is mapped with a producer’s accuracy of 88% and a user’s accuracy of 80%. The margin of error of the sampling-based area estimates of Shifting cultivation is 5.9%. Shifting cultivation is the main land use in Laos, accounting for 32.9% ± 1.9% of Laos over the past 30 years. To study changes in shifting cultivation over time, the area of shifting cultivation was estimated at 5-year intervals between 1991 and 2020 with all margins of error <17%. Results show that the area of slash-and-burn activities in Laos increased in 2015–2020. Our study provides an effective approach for monitoring shifting cultivation, which can be potentially applied into other regions. Our results not only provide valuable information for land management in Laos, but also can support analysis of spatial-temporal patterns of shifting cultivation and estimation of carbon emissions associated with shifting cultivation for REDD+ reporting."
CURTIS E WOODCOCK,Satellite data reveals a recent increase in shifting cultivation and associated carbon emissions in Laos,"Although shifting cultivation is the major land use type in Laos, the spatial-temporal patterns and the associated carbon emissions of shifting cultivation in Laos are largely unknown. This study provides a nationwide analysis of the spatial-temporal patterns of shifting cultivation and estimations of the associated carbon emissions in Laos over the last three decades. This study found that shifting cultivation has been expanding and intensifying in Laos, especially in the last 5 years. The newly cultivated land from 2016 to 2020 accounted for 4.5% (±1.2%) of the total land area of Laos. Furthermore, the length of fallow periods has been continuously declining, indicating that shifting cultivation is becoming increasingly intensive. Combining biomass derived from Global Ecosystem Dynamics Investigation and shifting cultivation maps and area estimates, we found that the net carbon emissions from shifting cultivation declined in 2001–2015 but increased in 2016–2020. The largest carbon source is conversion from intact forests to shifting cultivation, which contributed to 89% of the total emissions from 2001 to 2020. In addition, there were increased emissions from intensified use of fallow lands. This research provides useful information for policymakers in Laos to understand the changes in shifting cultivation and improve land use management. This study not only supports Reducing Emissions from Deforestation and Forest Degradation reporting for Laos but also provides a methodology for tracking carbon emissions and removals of shifting cultivation."
CURTIS E WOODCOCK,Need and vision for global medium-resolution Landsat and Sentinel-2 data products,
PRITIRAJ MOHANTY,Wireless transfer of power by a 35-GHz metamaterial split-ring resonator rectenna,Wireless transfer of power via high frequency microwave radiation using a miniature split ring resonator rectenna is reported. RF power is converted into DC power by integrating a rectification circuit with the split ring resonator. The near-field behavior of the rectenna is investigated with microwave radiation in the frequency range between 20-40 GHz with a maximum power level of 17 dBm. The observed resonance peaks match those predicted by simulation. Polarization studies show the expected maximum in signal when the electric field is polarized along the edge of the split ring resonator with the gap and minimum for perpendicular orientation. The efficiency of the rectenna is on the order of 1% for a frequency of 37.2 GHz. By using a cascading array of 9 split ring resonators the output power was increased by a factor of 20.
PRITIRAJ MOHANTY,Autoassociative memory and pattern recognition in micromechanical oscillator network,"Towards practical realization of brain-inspired computing in a scalable physical system, we investigate a network of coupled micromechanical oscillators. We numerically simulate this array of all-to-all coupled nonlinear oscillators in the presence of stochasticity and demonstrate its ability to synchronize and store information in the relative phase differences at synchronization. Sensitivity of behavior to coupling strength, frequency distribution, nonlinearity strength, and noise amplitude is investigated. Our results demonstrate that neurocomputing in a physically realistic network of micromechanical oscillators with silicon-based fabrication process can be robust against noise sources and fabrication process variations. This opens up tantalizing prospects for hardware realization of a low-power brain-inspired computing architecture that captures complexity on a scalable manufacturing platform."
PRITIRAJ MOHANTY,Wireless actuation of bulk acoustic modes in micromechanical resonators,We report wireless actuation of a Lamb wave micromechanical resonator from a distance of over 1 m with an efficiency of over 15%. Wireless actuation of conventional micromechanical resonators can have broad impact in a number of applications from wireless communication and implantable biomedical devices to distributed sensor networks.
PRITIRAJ MOHANTY,Perspective: Melanoma diagnosis and monitoring: Sunrise for melanoma therapy but early detection remains in the shade,"Melanoma is one of the most dangerous forms of cancer. The five-year survival rate is 98% if it is detected early. However, this rate plummets to 63% for regional disease and 17% when tumors have metastasized, that is, spread to distant sites. Furthermore, the incidence of melanoma has been rising by about 3% per year, whereas the incidence of cancers that are more common is decreasing. A handful of targeted therapies have recently become available that have finally shown real promise for treatment, but for reasons that remain unclear only a fraction of patients respond long term. These drugs often increase survival by only a few months in metastatic patient groups before relapse occurs. More effective treatment may be possible if a diagnosis can be made when the tumor burden is still low. Here, an overview of the current state-of-the-art is provided along with an argument for newer technologies towards early point-of-care diagnosis of melanoma."
PRITIRAJ MOHANTY,Detection of the melanoma biomarker TROY using silicon nanowire field-effect transistors,"Antibody-functionalized silicon nanowire field-effect transistors have been shown to exhibit excellent analyte detection sensitivity enabling sensing of analyte concentrations at levels not readily accessible by other methods. One example where accurate measurement of small concentrations is necessary is detection of serum biomarkers, such as the recently discovered tumor necrosis factor receptor superfamily member TROY (TNFRSF19), which may serve as a biomarker for melanoma. TROY is normally only present in brain but it is aberrantly expressed in primary and metastatic melanoma cells and shed into the surrounding environment. In this study, we show the detection of different concentrations of TROY in buffer solution using top-down fabricated silicon nanowires. We demonstrate the selectivity of our sensors by comparing the signal with that obtained from bovine serum albumin in buffer solution. Both the signal size and the reaction kinetics serve to distinguish the two signals. Using a fast-mixing two-compartment reaction model, we are able to extract the association and dissociation rate constants for the reaction of TROY with the antibody immobilized on the sensor surface."
PRITIRAJ MOHANTY,Micromechanical resonator with dielectric nonlinearity,"Nonlinear response of dielectric polarization to electric field in certain media is the foundation of nonlinear optics. Optically, such nonlinearities are observed at high light intensities, achievable by laser, where atomic-scale field strengths exceeding 106–108 V/m can be realized. Nonlinear optics includes a host of fascinating phenomena such as higher harmonic frequency generation, sum and difference frequency generation, four-wave mixing, self-focusing, optical phase conjugation, and optical rectification. Even though nonlinear optics has been studied for more than five decades, such studies in analogous acoustic or microwave frequency ranges are yet to be realized. Here, we demonstrate a nonlinear dielectric resonator composed of a silicon micromechanical resonator with an aluminum nitride piezoelectric layer, a material known to have a nonlinear optical susceptibility. Using a novel multiport approach, we demonstrate second and third-harmonic generation, sum and difference frequency generation, and four-wave mixing. Our demonstration of a nonlinear dielectric resonator opens up unprecedented possibilities for exploring nonlinear dielectric effects in engineered structures with an equally broad range of effects such as those observed in nonlinear optics. Furthermore, integration of a nonlinear dielectric layer on a chip-scale silicon micromechanical resonator offers tantalizing prospects for novel applications, such as ultra high harmonic generation, frequency multipliers, microwave frequency-comb generators, and nonlinear microwave signal processing."
PRITIRAJ MOHANTY,Wireless actuation of micromechanical resonators,"The wireless transfer of power is of fundamental and technical interest, with applications ranging from the remote operation of consumer electronics and implanted biomedical devices and sensors to the actuation of devices for which hard-wired power sources are neither desirable nor practical. In particular, biomedical devices that are implanted in the body or brain require small-footprint power receiving elements for wireless charging, which can be accomplished by micromechanical resonators. Moreover, for fundamental experiments, the ultralow-power wireless operation of micromechanical resonators in the microwave range can enable the performance of low-temperature studies of mechanical systems in the quantum regime, where the heat carried by the electrical wires in standard actuation techniques is detrimental to maintaining the resonator in a quantum state. Here we demonstrate the successful actuation of micron-sized silicon-based piezoelectric resonators with resonance frequencies ranging from 36 to 120 MHz at power levels of nanowatts and distances of ~3 feet, including comprehensive polarization, distance and power dependence measurements. Our unprecedented demonstration of the wireless actuation of micromechanical resonators via electric-field coupling down to nanowatt levels may enable a multitude of applications that require the wireless control of sensors and actuators based on micromechanical resonators, which was inaccessible until now."
PRITIRAJ MOHANTY,Quantum friction of micromechanical resonators at low temperatures,"Dissipation of micro- and nanoscale mechanical structures is dominated by quantum-mechanical tunneling of two-level defects intrinsically present in the system. We find that at high frequencies-usually, for smaller, micron-scale structures-a novel mechanism of phonon pumping of two-level defects gives rise to weakly temperature-dependent internal friction, Q-1, concomitant to the effects observed in recent experiments. Because of their size, comparable to or shorter than the emitted phonon wavelength, these structures suffer from superradiance-enhanced dissipation by the collective relaxation of a large number of two-level defects contained within the wavelength."
PRITIRAJ MOHANTY,Channel-width dependent enhancement in nanoscale field effect transistor,"We report the observation of channel-width dependent enhancement in nanoscale field effect transistors containing lithographically-patterned silicon nanowires as the conduction channel. These devices behave as conventional metal-oxide-semiconductor field-effect transistors in reverse source drain bias. Reduction of nanowire width below 200 nm leads to dramatic change in the threshold voltage. Due to increased surface-to-volume ratio, these devices show higher transconductance per unit width at smaller width. Our devices with nanoscale channel width demonstrate extreme sensitivity to surface field profile, and therefore can be used as logic elements in computation and as ultrasensitive sensors of surface-charge in chemical and biological species."
PRITIRAJ MOHANTY,Sensing of the melanoma biomarker TROY using silicon nanowire field-effect transistors,"Antibody-functionalized silicon nanowire field-effect transistors have been shown to exhibit excellent analyte detection sensitivity enabling sensing of analyte concentrations at levels not readily accessible by other methods. One example where accurate measurement of small concentrations is necessary is detection of serum biomarkers, such as the recently discovered tumor necrosis factor receptor superfamily member TROY (TNFRSF19), which may serve as a biomarker for melanoma. TROY is normally only present in brain but it is aberrantly expressed in primary and metastatic melanoma cells and shed into the surrounding environment. In this study, we show the detection of different concentrations of TROY in buffer solution using top-down fabricated silicon nanowires. We demonstrate the selectivity of our sensors by comparing the signal with that obtained from bovine serum albumin in buffer solution. Both the signal size and the reaction kinetics serve to distinguish the two signals. Using a fast-mixing two-compartment reaction model we are able to extract the association and dissociation rate constants for the reaction of TROY with the antibody immobilized on the sensor surface."
PRITIRAJ MOHANTY,Phase cascade lattice rectifier array: an exactly solvable nonlinear network circuit,"An exact analysis of a 2-D lattice network consisting of N × N sites with rectifier and AC source elements with controllable phases reveals a method for generating ripple-free DC power without the use of any filtering circuit elements. A phase cascade configuration is described in which the current ripple in a load resistor goes to zero in the large N limit, enhancing the rectification efficiency without requiring any additional capacitor or inductor based filters. The integrated modular configuration is qualitatively different from conventional rectenna arrays in which the source, rectifier and filter systems are physically disjoint. Nonlinear networks in the large N limit of source-rectifier arrays are potentially of interest to a fast evolving field of distributed power networks."
PRITIRAJ MOHANTY,Nanoscale field effect transistor for biomolecular signal amplification,We report amplification of biomolecular recognition signal in lithographically defined silicon nanochannel devices. The devices are configured as field effect transistors (FET) in the reversed source-drain bias region. The measurement of the differential conductance of the nanowire channels in the FET allows sensitive detection of changes in the surface potential due to biomolecular binding. Narrower silicon channels demonstrate higher sensitivity to binding due to increased surface-to-volume ratio. The operation of the device in the negative source-drain region demonstrates signal amplification. The equivalence between protein binding and change in the surface potential is described.
PRITIRAJ MOHANTY,Field Effect Transistor Nanosensor for Breast Cancer Diagnostics,"Silicon nanochannel field effect transistor (FET) biosensors are one of the most promising technologies in the development of highly sensitive and label-free analyte detection for cancer diagnostics. With their exceptional electrical properties and small dimensions, silicon nanochannels are ideally suited for extraordinarily high sensitivity. In fact, the high surface-to-volume ratios of these systems make single molecule detection possible. Further, FET biosensors offer the benefits of high speed, low cost, and high yield manufacturing, without sacrificing the sensitivity typical for traditional optical methods in diagnostics. Top down manufacturing methods leverage advantages in Complementary Metal Oxide Semiconductor (CMOS) technologies, making richly multiplexed sensor arrays a reality. Here, we discuss the fabrication and use of silicon nanochannel FET devices as biosensors for breast cancer diagnosis and monitoring."
PRITIRAJ MOHANTY,Silicon-based nanochannel glucose sensor,"Silicon nanochannel biological field effect transistors have been developed for glucose detection. The device is nanofabricated from a silicon-on-insulator wafer with a top-down approach and surface functionalized with glucose oxidase. The differential conductance of silicon nanowires, tuned with source-drain bias voltage, is demonstrated to be sensitive to the biocatalyzed oxidation of glucose. The glucose biosensor response is linear in the 0.5–8mM concentration range with 3–5min response time. This silicon nanochannel-based glucose biosensor technology offers the possibility of high density, high quality glucose biosensor integration with silicon-based circuitry."
PRITIRAJ MOHANTY,Dynamical response of nanomechanical oscillators in immiscible viscous fluid for in vitro biomolecular recognition,"Dynamical response of nanomechanical cantilever structures immersed in a viscous fluid is important to in vitro single-molecule force spectroscopy, biomolecular recognition of disease-specific proteins, and the study of microscopic protein dynamics. Here we study the stochastic response of biofunctionalized nanomechanical cantilever beams in a viscous fluid. Using the fluctuation-dissipation theorem we derive an exact expression for the spectral density of displacement and a linear approximation for resonance frequency shift. We find that in a viscous solution the frequency shift of the nanoscale cantilever is determined by surface stress generated by biomolecular interaction with negligible contributions from mass loading due to the biomolecules."
PRITIRAJ MOHANTY,Optical wireless information transfer with nonlinear micromechanical resonators,"Wireless transfer of information is the basis of modern communication. It includes cellular, WiFi, Bluetooth, and GPS systems, all of which use electromagnetic radio waves with frequencies ranging from typically 100 MHz to a few GHz. However, several long-standing challenges with standard radio-wave wireless transmission still exist, including keeping secure transmission of data from potential compromise. Here, we demonstrate wireless information transfer using a line-of-sight optical architecture with a micromechanical element. In this fundamentally new approach, a laser beam encoded with information impinges on a nonlinear micromechanical resonator located a distance from the laser. The force generated by the radiation pressure of the laser light on the nonlinear micromechanical resonator produces a sideband modulation signal, which carries the precise information encoded in the subtle changes in the radiation pressure. Using this, we demonstrate data and image transfer with one hundred percent fidelity with a single 96-by-270 μm silicon resonator element in an optical frequency band. This mechanical approach relies only on the momentum of the incident photons and is therefore able to use any portion of the optical frequency band—a band that is 10 000 times wider than the radio frequency band. Our line-of-sight architecture using highly scalable micromechanical resonators offers new possibilities in wireless communication. Due to their small size, these resonators can be easily arrayed while maintaining a small form factor to provide redundancy and parallelism."
PRITIRAJ MOHANTY,Micromechanical resonator driven by radiation pressure force,"Radiation pressure exerted by light on any surface is the pressure generated by the momentum of impinging photons. The associated force - fundamentally, a quantum mechanical aspect of light - is usually too small to be useful, except in large-scale problems in astronomy and astrodynamics. In atomic and molecular optics, radiation pressure can be used to trap or cool atoms and ions. Use of radiation pressure on larger objects such as micromechanical resonators has been so far limited to its coupling to an acoustic mode, sideband cooling, or levitation of microscopic objects. In this Letter, we demonstrate direct actuation of a radio-frequency micromechanical plate-type resonator by the radiation pressure force generated by a standard laser diode at room temperature. Using two independent methods, the magnitude of the resonator's response to forcing by radiation pressure is found to be proportional to the intensity of the incident light."
PRITIRAJ MOHANTY,Micromechanical microphone using sideband modulation of nonlinear resonators,
PRITIRAJ MOHANTY,Measurement of nonlinear piezoelectric coefficients using a micromechanical resonator,"We describe and demonstrate a method by which the nonlinear piezoelectric properties of a piezoelectric material may be measured by detecting the force that it applies on a suspended micromechanical resonator at one of its mechanical resonance frequencies. Resonators are used in countless applications; this method could provide a means for better-characterizing material behaviors within real MEMS devices. Further, special devices can be designed to probe this nonlinear behavior at specific frequencies with enhanced signal sizes. The resonators used for this experiment are actuated using a 1-μm-thick layer of aluminum nitride. When driven at large amplitudes, the piezoelectric layer generates harmonics, which are measurable in the response of the resonator. In this experiment, we measured the second-order piezoelectric coefficient of aluminum nitride to be −(23.1±14.1)×10^−22m/V^2."
PRITIRAJ MOHANTY,Alanine aminotransferase assay biosensor platform using silicon nanowire field effect transistors,"Frequent monitoring of serum alanine aminotransferase (ALT) activity is essential to prevent drug-induced liver injury (DILI). Current ALT assays are restricted to centralized clinical laboratories, making frequent patient monitoring logistically difficult. To address this, we demonstrated the capability of commercial foundry manufactured silicon nanowire field effect transistor (SiNW-FET) biosensors in a form factor that enables frequent near-patient monitoring. Here, we designed an ALT assay, by coupling the ALT-catalyzed production of pyruvate to the reduction of ferricyanide, enabling both spectrophotometric and electrical measurement of ALT activity. The two methods yield comparable ALT activity detection across a dynamic range wide enough to monitor patients at risk for DILI. This study demonstrates kinetic activity measurement of an endogenous enzyme using uncoupled SiNW-FETs, and commercial manufacturing of SiNW-FET sensor arrays for use in a portable biosensor platform."
DAVID FRIED,"Institutional Repositories, Policies, and Disruption","For many librarians, institutional repositories (IRs) promised significant change for academic libraries. We envisioned enlarging collection development scope to include locally produced scholarship and an expansion of library services to embrace scholarly publication and distribution. However, at the University of Rochester, as at many other institutions, this transformational technology was introduced in the conservative, controlled manner associated with stereotypical librarian culture, and so these expected changes never materialized. In this case study, we focus on the creation of our institutional repository (a potentially disruptive technology) and how its success was hampered by our organizational culture, manifested as a lengthy and complicated set of policies. In the following pages, we briefly describe our repository project, talk about our original policies, look at the ways those policies impeded our project, and discuss the disruption of those policies and the benefits in user uptake that resulted."
DAVID FRIED,Electricity and firm productivity: a general-equilibrium approach,"Many policymakers view power outages as a major constraint on firm productivity in developing countries. Yet empirical studies find modest short-run effects of outages on firm performance. This paper builds a dynamic macroeconomic model to study the long-run general-equilibrium effects of power outages on productivity. Outages lower productivity in the model by creating idle resources, depressing the scale of incumbent firms and reducing entry of new firms. Consistent with the empirical literature, the model predicts small shortrun effects of eliminating outages. However, the long-run general-equilibrium effects are much larger, supporting the view that eliminating outages is an important development objective."
DAVID FRIED,The eighteenth data release of the Sloan Digital Sky Surveys: targeting and first spectra from SDSS-V,"The eighteenth data release (DR18) of the Sloan Digital Sky Survey (SDSS) is the first one for SDSS-V, the fifth generation of the survey. SDSS-V comprises three primary scientific programs or “Mappers”: the Milky Way Mapper (MWM), the Black Hole Mapper (BHM), and the Local Volume Mapper. This data release contains extensive targeting information for the two multiobject spectroscopy programs (MWM and BHM), including input catalogs and selection functions for their numerous scientific objectives. We describe the production of the targeting databases and their calibration and scientifically focused components. DR18 also includes ∼25,000 new SDSS spectra and supplemental information for X-ray sources identified by eROSITA in its eFEDS field. We present updates to some of the SDSS software pipelines and preview changes anticipated for DR19. We also describe three value-added catalogs (VACs) based on SDSS-IV data that have been published since DR17, and one VAC based on the SDSS-V data in the eFEDS field."
DAVID FRIED,Prospects for beyond the standard model physics searches at the deep underground neutrino experiment: DUNE collaboration,"The Deep Underground Neutrino Experiment (DUNE) will be a powerful tool for a variety of physics topics. The high-intensity proton beams provide a large neutrino flux, sampled by a near detector system consisting of a combination of capable precision detectors, and by the massive far detector system located deep underground. This configuration sets up DUNE as a machine for discovery, as it enables opportunities not only to perform precision neutrino measurements that may uncover deviations from the present three-flavor mixing paradigm, but also to discover new particles and unveil new interactions and symmetries beyond those predicted in the Standard Model (SM). Of the many potential beyond the Standard Model (BSM) topics DUNE will probe, this paper presents a selection of studies quantifying DUNE's sensitivities to sterile neutrino mixing, heavy neutral leptons, non-standard interactions, CPT symmetry violation, Lorentz invariance violation, neutrino trident production, dark matter from both beam induced and cosmogenic sources, baryon number violation, and other new physics topics that complement those at high-energy colliders and significantly extend the present reach."
DAVID FRIED,Identification and reconstruction of low-energy electrons in the ProtoDUNE-SP detector,
DAVID FRIED,Scintillation light detection in the 6-m drift-length ProtoDUNE Dual Phase liquid argon TPC,"DUNE is a dual-site experiment for long-baseline neutrino oscillation studies, neutrino astrophysics and nucleon decay searches. ProtoDUNE Dual Phase (DP) is a 6  ×  6  ×  6 m 3 liquid argon time-projection-chamber (LArTPC) that recorded cosmic-muon data at the CERN Neutrino Platform in 2019-2020 as a prototype of the DUNE Far Detector. Charged particles propagating through the LArTPC produce ionization and scintillation light. The scintillation light signal in these detectors can provide the trigger for non-beam events. In addition, it adds precise timing capabilities and improves the calorimetry measurements. In ProtoDUNE-DP, scintillation and electroluminescence light produced by cosmic muons in the LArTPC is collected by photomultiplier tubes placed up to 7 m away from the ionizing track. In this paper, the ProtoDUNE-DP photon detection system performance is evaluated with a particular focus on the different wavelength shifters, such as PEN and TPB, and the use of Xe-doped LAr, considering its future use in giant LArTPCs. The scintillation light production and propagation processes are analyzed and a comparison of simulation to data is performed, improving understanding of the liquid argon properties."
CHANTAL E STERN,Altered intrinsic functional coupling between core neurocognitive networks in Parkinson's disease,"Parkinson's disease (PD) is largely attributed to disruptions in the nigrostriatal dopamine system. These neurodegenerative changes may also have a more global effect on intrinsic brain organization at the cortical level. Functional brain connectivity between neurocognitive systems related to cognitive processing is critical for effective neural communication, and is disrupted across neurological disorders. Three core neurocognitive networks have been established as playing a critical role in the pathophysiology of many neurological disorders: the default-mode network (DMN), the salience network (SN), and the central executive network (CEN). In healthy adults, DMN-CEN interactions are anti-correlated while SN-CEN interactions are strongly positively correlated even at rest, when individuals are not engaging in any task. These intrinsic between-network interactions at rest are necessary for efficient suppression of the DMN and activation of the CEN during a range of cognitive tasks. To identify whether these network interactions are disrupted in individuals with PD, we used resting state functional magnetic resonance imaging (rsfMRI) to compare between-network connectivity between 24 PD participants and 20 age-matched controls (MC). In comparison to the MC, individuals with PD showed significantly less SN-CEN coupling and greater DMN-CEN coupling during rest. Disease severity, an index of striatal dysfunction, was related to reduced functional coupling between the striatum and SN. These results demonstrate that individuals with PD have a dysfunctional pattern of interaction between core neurocognitive networks compared to what is found in healthy individuals, and that interaction between the SN and the striatum is even more profoundly disrupted in those with greater disease severity."
CHANTAL E STERN,Functional correlates of optic flow motion processing in Parkinson’s disease,"The visual input created by the relative motion between an individual and the environment, also called optic flow, influences the sense of self-motion, postural orientation, veering of gait, and visuospatial cognition. An optic flow network comprising visual motion areas V6, V3A, and MT+, as well as visuo-vestibular areas including posterior insula vestibular cortex (PIVC) and cingulate sulcus visual area (CSv), has been described as uniquely selective for parsing egomotion depth cues in humans. Individuals with Parkinson’s disease (PD) have known behavioral deficits in optic flow perception and visuospatial cognition compared to age- and education-matched control adults (MC). The present study used functional magnetic resonance imaging (fMRI) to investigate neural correlates related to impaired optic flow perception in PD. We conducted fMRI on 40 non-demented participants (23 PD and 17 MC) during passive viewing of simulated optic flow motion and random motion. We hypothesized that compared to the MC group, PD participants would show abnormal neural activity in regions comprising this optic flow network. MC participants showed robust activation across all regions in the optic flow network, consistent with studies in young adults, suggesting intact optic flow perception at the neural level in healthy aging. PD participants showed diminished activity compared to MC particularly within visual motion area MT+ and the visuo-vestibular region CSv. Further, activation in visuo-vestibular region CSv was associated with disease severity. These findings suggest that behavioral reports of impaired optic flow perception and visuospatial performance may be a result of impaired neural processing within visual motion and visuo-vestibular regions in PD."
CHANTAL E STERN,"Individual differences in human path integration abilities correlate with gray matter volume in retrosplenial cortex, hippocampus, and medial prefrontal cortex","Humans differ in their individual navigational abilities. These individual differences may exist in part because successful navigation relies on several disparate abilities, which rely on different brain structures. One such navigational capability is path integration, the updating of position and orientation, in which navigators track distances, directions, and locations in space during movement. Although structural differences related to landmark-based navigation have been examined, gray matter volume related to path integration ability has not yet been tested. Here, we examined individual differences in two path integration paradigms: (1) a location tracking task and (2) a task tracking translational and rotational self-motion. Using voxel-based morphometry, we related differences in performance in these path integration tasks to variation in brain morphology in 26 healthy young adults. Performance in the location tracking task positively correlated with individual differences in gray matter volume in three areas critical for path integration: the hippocampus, the retrosplenial cortex, and the medial prefrontal cortex. These regions are consistent with the path integration system known from computational and animal models and provide novel evidence that morphological variability in retrosplenial and medial prefrontal cortices underlies individual differences in human path integration ability. The results for tracking rotational self-motion-but not translation or location-demonstrated that cerebellum gray matter volume correlated with individual performance. Our findings also suggest that these three aspects of path integration are largely independent. Together, the results of this study provide a link between individual abilities and the functional correlates, computational models, and animal models of path integration."
CHANTAL E STERN,Resting state connectivity between medial temporal lobe regions and intrinsic cortical networks predicts performance in a path integration task,"Humans differ in their individual navigational performance, in part because successful navigation relies on several diverse abilities. One such navigational capability is path integration, the updating of position and orientation during movement, typically in a sparse, landmark-free environment. This study examined the relationship between path integration abilities and functional connectivity to several canonical intrinsic brain networks. Intrinsic networks within the brain reflect past inputs and communication as well as structural architecture. Individual differences in intrinsic connectivity have been observed for common networks, suggesting that these networks can inform our understanding of individual spatial abilities. Here, we examined individual differences in intrinsic connectivity using resting state magnetic resonance imaging (rsMRI). We tested path integration ability using a loop closure task, in which participants viewed a single video of movement in a circle trajectory in a sparse environment, and then indicated whether the video ended in the same location in which it started. To examine intrinsic brain networks, participants underwent a resting state scan. We found that better performance in the loop task was associated with increased connectivity during rest between the central executive network (CEN) and posterior hippocampus, parahippocampal cortex (PHC) and entorhinal cortex. We also found that connectivity between PHC and the default mode network (DMN) during rest was associated with better loop closure performance. The results indicate that interactions between medial temporal lobe (MTL) regions and intrinsic networks that involve prefrontal cortex (PFC) are important for path integration and navigation."
CHANTAL E STERN,Salience and default mode network coupling predicts cognition in aging and Parkinson’s disease,"OBJECTIVES: Cognitive impairment is common in Parkinson’s disease (PD). Three neurocognitive networks support efficient cognition: the salience network, the default mode network, and the central executive network. The salience network is thought to switch between activating and deactivating the default mode and central executive networks. Anti-correlated interactions between the salience and default mode networks in particular are necessary for efficient cognition. Our previous work demonstrated altered functional coupling between the neurocognitive networks in non-demented individuals with PD compared to age-matched control participants. Here, we aim to identify associations between cognition and functional coupling between these neurocognitive networks in the same group of participants. METHODS: We investigated the extent to which intrinsic functional coupling among these neurocognitive networks is related to cognitive performance across three neuropsychological domains: executive functioning, psychomotor speed, and verbal memory. Twenty-four non-demented individuals with mild to moderate PD and 20 control participants were scanned at rest and evaluated on three neuropsychological domains. RESULTS: PD participants were impaired on tests from all three domains compared to control participants. Our imaging results demonstrated that successful cognition across healthy aging and Parkinson’s disease participants was related to anti-correlated coupling between the salience and default mode networks. Individuals with poorer performance scores across groups demonstrated more positive salience network/default-mode network coupling. CONCLUSIONS: Successful cognition relies on healthy coupling between the salience and default mode networks, which may become dysfunctional in PD. These results can help inform non-pharmacological interventions (repetitive transcranial magnetic stimulation) targeting these specific networks before they become vulnerable in early stages of Parkinson’s disease."
MEERS M OPPENHEIM,Hybrid simulations of coupled Farley-Buneman/gradient drift instabilities in the equatorial E region ionosphere,"Plasma irregularities in the equatorial E region ionosphere are classified as Type I or Type II, based on coherent radar spectra. Type I irregularities are attributed to the Farley‐Buneman instability and Type II to the gradient drift instability that cascades to meter‐scale irregularities detected by radars. This work presents the first kinetic simulations of coupled Farley‐Buneman and gradient drift turbulence in the equatorial E region ionosphere for a range of zeroth‐order vertical electric fields, using a new approach to solving the electrostatic potential equation. The simulation models a collisional quasi‐neutral plasma with a warm, inertialess electron fluid and a distribution of NO+ ions. A 512 m wave with a maximum/minimum of ±0.25 of the background density perturbs the plasma. The density wave creates an electrostatic field that adds to the zeroth‐order vertical and ambipolar fields, and drives Farley‐Buneman turbulence even when these fields are below the instability threshold. Wave power spectra show that Type II irregularities develop in all simulation runs and that Type I irregularities with wavelengths of a few meters develop in the trough of the background wave in addition to Type II irregularities as the zeroth‐order electric field magnitude increases. Linear fluid theory predicts the growth of Type II irregularities reasonably well, but it does not fully capture the simultaneous growth of Type I irregularities in the region of peak total electric field. The growth of localized Type I irregularities represents a parametric instability in which the electric field of the large‐scale background wave drives pure Farley‐Buneman turbulence. These results help explain observations of meter‐scale irregularities advected by kilometer‐scale waves."
MEERS M OPPENHEIM,Photoelectron-induced waves: A likely source of 150 km radar echoes and enhanced electron modes,"VHF radars near the geomagnetic equator receive coherent reflections from plasma density irregularities between 130 and 160 km in altitude during the daytime. Though researchers first discovered these 150 km echoes over 50 years ago and use them to monitor vertical plasma drifts, the underlying mechanism that creates them remains a mystery. This paper uses large‐scale kinetic simulations to show that photoelectrons can drive electron waves, which then enhance ion density irregularities that radars could observe as 150 km echoes. This model explains why 150 km echoes exist only during the day and why they appear at their lowest altitudes near noon. It predicts the spectral structure observed by Chau (2004) and suggests observations that can further evaluate this mechanism. It also shows the types and strength of electron modes that photoelectron‐wave interactions generate in a magnetized plasma."
MEERS M OPPENHEIM,Formation of plasma around a small meteoroid: 1. Kinetic theory,"This paper calculates the spatial distribution of the plasma responsible for radar head echoes by applying the kinetic theory developed in the companion paper. This results in a set of analytic expressions for the plasma density as a function of distance from the meteoroid. It shows that at distances less than a collisional mean free path from the meteoroid surface, the plasma density drops in proportion to 1/R where R is the distance from the meteoroid center; and, at distances much longer than the mean‐free‐path behind the meteoroid, the density diminishes at a rate proportional to 1/R2. The results of this paper should be used for modeling and analysis of radar head echoes."
MEERS M OPPENHEIM,Simulations of secondary Farley-Buneman instability driven by a kilometer-scale primary wave: anomalous transport and formation of flat-topped electric fields,"Since the 1950s, high frequency and very high frequency radars near the magnetic equator have frequently detected strong echoes caused ultimately by the Farley‐Buneman instability (FBI) and the gradient drift instability (GDI). In the 1980s, coordinated rocket and radar campaigns made the astonishing observation of flat‐topped electric fields coincident with both meter‐scale irregularities and the passage of kilometer‐scale waves. The GDI in the daytime E region produces kilometer‐scale primary waves with polarization electric fields large enough to drive meter‐scale secondary FBI waves. The meter‐scale waves propagate nearly vertically along the large‐scale troughs and crests and act as VHF tracers for the large‐scale dynamics. This work presents a set of hybrid numerical simulations of secondary FBIs, driven by a primary kilometer‐scale GDI‐like wave. Meter‐scale density irregularities develop in the crest and trough of the kilometer‐scale wave, where the total electric field exceeds the FBI threshold, and propagate at an angle near the direction of total Hall drift determined by the combined electric fields. The meter‐scale irregularities transport plasma across the magnetic field, producing flat‐topped electric fields similar to those observed in rocket data and reducing the large‐scale wave electric field to just above the FBI threshold value. The self‐consistent reduction in driving electric field helps explain why echoes from the FBI propagate near the plasma acoustic speed."
MEERS M OPPENHEIM,Effects of ion magnetization on the Farley-Buneman instability in the solar chromosphere,"Intense heating in the quiet-Sun chromosphere raises the temperature from 4000 to 6500 K but, despite decades of study, the underlying mechanism remains a mystery. This study continues to explore the possibility that the Farley–Buneman instability contributes to chromospheric heating. This instability occurs in weakly ionized collisional plasmas in which electrons are magnetized, but ions are not. A mixture of metal ions generate the plasma density in the coolest parts of the chromosphere; while some ions are weakly magnetized, others are demagnetized by neutral collisions. This paper incorporates the effects of multiple, arbitrarily magnetized species of ions to the theory of the Farley–Buneman instability and examines the ramifications on instability in the chromosphere. The inclusion of magnetized ions introduces new restrictions on the regions in which the instability can occur in the chromosphere—in fact, it confines the instability to the regions in which heating is observed. For a magnetic field of 30 G, the minimum ambient electric field capable of driving the instability is 13.5 V/m at the temperature minimum."
MEERS M OPPENHEIM,Mesospheric anomalous diffusion during noctilucent cloud scenarios,"The Andenes specular meteor radar shows meteor trail diffusion rates increasing on average by about 10 % at times and locations where a lidar observes noctilucent clouds (NLCs). This high-latitude effect has been attributed to the presence of charged NLC after exploring possible contributions from thermal tides. To make this claim, the current study evaluates data from three stations at high, middle, and low latitudes for the years 2012 to 2016 to show that NLC influence on the meteor trail diffusion is independent of thermal tides. The observations also show that the meteor trail diffusion enhancement during NLC cover exists only at high latitudes and near the peaks of NLC layers. This paper discusses a number of possible explanations for changes in the regions with NLCs and leans towards the hypothesis that the relative abundance of background electron density plays the leading role. A more accurate model of the meteor trail diffusion around NLC particles would help researchers determine mesospheric temperature and neutral density profiles from meteor radars at high latitudes."
MEERS M OPPENHEIM,Nonlinear effects of electron‐electron collisions on ISR temperature measurements,"Incoherent scatter radars (ISR) estimate the electron and ion temperatures in the ionosphere by fitting measured spectra of ion‐acoustic waves to forward models. For radars looking at aspect angles within 5° off perpendicular to the Earth's magnetic field, the magnetic field constrains electron movement and Coulomb collisions add an additional source of damping that narrows the spectra. Fitting the collisionally narrowed spectra to collisionless forward models leads to errors or underestimates of the plasma temperatures. This paper presents the first fully kinetic particle‐in‐cell (PIC) simulations of ISR spectra with collisional damping by velocity‐dependent electron‐electron and electron‐ion collisions. For aspect angles between 0.5° and 2° off perpendicular, the damping effects of electron‐ion and electron‐electron collisions in the PIC simulations are the same and the resulting spectra are narrower than what current theories and models predict. For aspect angles larger than 3° away from perpendicular, the simulations with electron‐ion collisions match collisionless ISR theory well, but spectra with electron‐electron collisions are narrower than theory predicts at aspect angles as large as 5° away from perpendicular. At aspect angles less than 5° the PIC simulations produce narrower spectra than previous simulations using single‐particle displacement statistics that include both electron‐ion and electron‐electron collisions. The narrowing of spectra by electron‐electron collisions in the PIC code between 3° and 5° away from perpendicular is currently neglected when fitting measured spectra from the Jicamarca and Millstone Hill radars, leading to underestimates of electron temperatures by as much as 25% at small aspect angles."
MEERS M OPPENHEIM,Formation of plasma around a small meteoroid: simulation and theory,"High‐power large‐aperture radars detect meteors by reflecting radio waves off dense plasma that surrounds a hypersonic meteoroid as it ablates in the Earth's atmosphere. If the plasma density profile around the meteoroid is known, the plasma's radar cross section can be used to estimate meteoroid properties such as mass, density, and composition. This paper presents head echo plasma density distributions obtained via two numerical simulations of a small ablating meteoroid and compares the results to an analytical solution found in Dimant and Oppenheim (2017a, https://doi.org/10.1002/2017JA023960, 2017b, https://doi.org/10.1002/2017JA023963). The first simulation allows ablated meteoroid particles to experience only a single collision to match an assumption in the analytical solution, while the second is a more realistic simulation by allowing multiple collisions. The simulation and analytical results exhibit similar plasma density distributions. At distances much less than λT, the average distance an ablated particle travels from the meteoroid before a collision with an atmospheric particle, the plasma density falls off as 1/R, where R is the distance from the meteoroid center. At distances substantially greater than λT, the plasma density profile has an angular dependence, falling off as 1/R^2 directly behind the meteoroid, 1/R^3 in a plane perpendicular to the meteoroid's path that contains the meteoroid center, and exp - 1.5(𝑅/λ𝙏)2/3/𝑅 in front of the meteoroid. When used for calculating meteoroid masses, this new plasma density model can give masses that are orders of magnitude different than masses calculated from a spherically symmetric Gaussian distribution, which has been used to calculate masses in the past."
MEERS M OPPENHEIM,Mesospheric anomalous diffusion during noctilucent clouds,"The Andenes specular meteor radar shows meteor-trail diffusion rates increasing on average by ~ 20% at times and locations where a lidar observes noctilucent clouds (NLCs). This high-latitude effect has been attributed to the presence of charged NLC but this study shows that such behaviors result predominantly from thermal tides. To make this claim, the current study evaluates data from three stations, at high-, mid-, and low-latitudes, for the years 2012 to 2016, comparing diffusion to show that thermal tides correlate strongly with the presence of NLCs. This data also shows that the connection between meteor-trail diffusion and thermal tide occurs at all altitudes in the mesosphere, while the NLC influence exists only at high-latitudes and at around peak of NLC layer. This paper discusses a number of possible explanations for changes in the regions with NLCs and leans towards the hypothesis that relative abundance of background electron density plays the leading role. A more accurate model of the meteor trail diffusion around NLC particles would help researchers determine mesospheric temperature and neutral density profiles from meteor radars."
MEERS M OPPENHEIM,ISR spectra simulations with electron-ion Coulomb collisions,"Incoherent scatter radars (ISR) rely on Thomson scattering of very high frequency or ultrahigh frequency radio waves off electrons in the ionosphere and measure the backscattered power spectra in order to estimate altitude profiles of plasma density, electron temperature, ion temperature, and ion drift speed. These spectra result from the collective behavior of coupled ion and electron dynamics, and, for most cases, existing theories predict these well. However, when the radar points nearly perpendicular to the Earth's magnetic field, the motion of the plasma across the field lines becomes complex and Coulomb collisions between electrons and ions become important in interpreting ISR measurements. This paper presents the first fully kinetic, self‐consistent, particle‐in‐cell simulations of ISR spectra with electron‐ion Coulomb collisions. We implement a grid‐based Coulomb collision algorithm in the Electrostatic Parallel Particle‐in‐Cell simulator and obtain ISR spectra from simulations both with and without collisions. For radar directions greater than 5° away from perpendicular to the magnetic field, both sets of simulations match collisionless ISR theory well. For angles between 3° and 5°, the collisional simulation is well described by a simplified Brownian motion collision process. At angles less than 3° away from perpendicular the Brownian motion model fails, and the collisional simulation qualitatively agrees with previous single particle simulations. For radar directions exactly perpendicular to the magnetic field the simulated collisional spectra match those from the Brownian motion collision theory, in agreement with previous single particle simulations."
MEERS M OPPENHEIM,Solar flare effects on 150‐km echoes observed over Jicamarca: WACCM‐X simulations,"Jicamarca Radio Observatory observations and Whole Atmosphere Community Climate Model with thermosphere‐ionosphere eXtension (WACCM‐X) simulations are used to investigate the effects of the 7 September 2005 X‐17 solar flare on 150‐km echoes, electron densities, and vertical plasma drifts. The solar flare produces a remarkably similar response in the observed 150‐km echoes and simulated electron densities. The results provide additional evidence of the relationship between the background electron density and the layering structure that is seen in 150‐km echoes. The simulations also capture a similar rapid decrease in vertical plasma drift velocity that is seen in the observations. The simulated change in vertical plasma drift is, however, weaker than the observed decrease at the longitude of Jicamarca, though it is stronger east of Jicamarca. The effect of the solar flare on the vertical plasma drifts is primarily attributed to changes in conductivity due to the enhanced ionization during the solar flare."
AZIZA AHMED,"Dobbs v. Jackson women’s health: undermining public health, facilitating reproductive coercion","Dobbs v. Jackson Women’s Health continues a trajectory of U.S. Supreme Court jurisprudence that undermines the normative foundation of public health — the idea that the state is obligated to provide a robust set of supports for healthcare services and the underlying social determinants of health. Dobbs furthers a longstanding ideology of individual responsibility in public health, neglecting collective responsibility for better health outcomes. Such an ideology on individual responsibility not only enables a shrinking of public health infrastructure for reproductive health, it facilitates the rise of reproductive coercion and a criminal legal response to pregnancy and abortion. This commentary situates Dobbs in the context of a long historical shift in public health that increasingly places burdens on individuals for their own reproductive health care, moving away from the possibility of a robust state public health infrastructure."
NEIL W KOWALL,Clinicopathological evaluation of chronic traumatic encephalopathy in players of American football,"IMPORTANCE: Players of American football may be at increased risk of long-term neurological conditions, particularly chronic traumatic encephalopathy (CTE). OBJECTIVE: To determine the neuropathological and clinical features of deceased football players with CTE. DESIGN, SETTING, AND PARTICIPANTS: Case series of 202 football players whose brains were donated for research. Neuropathological evaluations and retrospective telephone clinical assessments (including head trauma history) with informants were performed blinded. Online questionnaires ascertained athletic and military history. EXPOSURES: Participation in American football at any level of play. MAIN OUTCOMES AND MEASURES: Neuropathological diagnoses of neurodegenerative diseases, including CTE, based on defined diagnostic criteria; CTE neuropathological severity (stages I to IV or dichotomized into mild [stages I and II] and severe [stages III and IV]); informant-reported athletic history and, for players who died in 2014 or later, clinical presentation, including behavior, mood, and cognitive symptoms and dementia. RESULTS: Among 202 deceased former football players (median age at death, 66 years [interquartile range, 47-76 years]), CTE was neuropathologically diagnosed in 177 players (87%; median age at death, 67 years [interquartile range, 52-77 years]; mean years of football participation, 15.1 [SD, 5.2]), including 0 of 2 pre–high school, 3 of 14 high school (21%), 48 of 53 college (91%), 9 of 14 semiprofessional (64%), 7 of 8 Canadian Football League (88%), and 110 of 111 National Football League (99%) players. Neuropathological severity of CTE was distributed across the highest level of play, with all 3 former high school players having mild pathology and the majority of former college (27 [56%]), semiprofessional (5 [56%]), and professional (101 [86%]) players having severe pathology. Among 27 participants with mild CTE pathology, 26 (96%) had behavioral or mood symptoms or both, 23 (85%) had cognitive symptoms, and 9 (33%) had signs of dementia. Among 84 participants with severe CTE pathology, 75 (89%) had behavioral or mood symptoms or both, 80 (95%) had cognitive symptoms, and 71 (85%) had signs of dementia. CONCLUSIONS AND RELEVANCE: In a convenience sample of deceased football players who donated their brains for research, a high proportion had neuropathological evidence of CTE, suggesting that CTE may be related to prior participation in football."
NEIL W KOWALL,"Concussion, microvascular injury, and early tauopathy in young athletes after impact head injury and an impact concussion mouse model","The mechanisms underpinning concussion, traumatic brain injury, and chronic traumatic encephalopathy, and the relationships between these disorders, are poorly understood. We examined post-mortem brains from teenage athletes in the acute-subacute period after mild closed-head impact injury and found astrocytosis, myelinated axonopathy, microvascular injury, perivascular neuroinflammation, and phosphorylated tau protein pathology. To investigate causal mechanisms, we developed a mouse model of lateral closed-head impact injury that uses momentum transfer to induce traumatic head acceleration. Unanaesthetized mice subjected to unilateral impact exhibited abrupt onset, transient course, and rapid resolution of a concussion-like syndrome characterized by altered arousal, contralateral hemiparesis, truncal ataxia, locomotor and balance impairments, and neurobehavioural deficits. Experimental impact injury was associated with axonopathy, blood–brain barrier disruption, astrocytosis, microgliosis (with activation of triggering receptor expressed on myeloid cells, TREM2), monocyte infiltration, and phosphorylated tauopathy in cerebral cortex ipsilateral and subjacent to impact. Phosphorylated tauopathy was detected in ipsilateral axons by 24 h, bilateral axons and soma by 2 weeks, and distant cortex bilaterally at 5.5 months post-injury. Impact pathologies co-localized with serum albumin extravasation in the brain that was diagnostically detectable in living mice by dynamic contrast-enhanced MRI. These pathologies were also accompanied by early, persistent, and bilateral impairment in axonal conduction velocity in the hippocampus and defective long-term potentiation of synaptic neurotransmission in the medial prefrontal cortex, brain regions distant from acute brain injury. Surprisingly, acute neurobehavioural deficits at the time of injury did not correlate with blood–brain barrier disruption, microgliosis, neuroinflammation, phosphorylated tauopathy, or electrophysiological dysfunction. Furthermore, concussion-like deficits were observed after impact injury, but not after blast exposure under experimental conditions matched for head kinematics. Computational modelling showed that impact injury generated focal point loading on the head and seven-fold greater peak shear stress in the brain compared to blast exposure. Moreover, intracerebral shear stress peaked before onset of gross head motion. By comparison, blast induced distributed force loading on the head and diffuse, lower magnitude shear stress in the brain. We conclude that force loading mechanics at the time of injury shape acute neurobehavioural responses, structural brain damage, and neuropathological sequelae triggered by neurotrauma. These results indicate that closed-head impact injuries, independent of concussive signs, can induce traumatic brain injury as well as early pathologies and functional sequelae associated with chronic traumatic encephalopathy. These results also shed light on the origins of concussion and relationship to traumatic brain injury and its aftermath."
JEAN MORRISON,Search for heavy neutrinos with the T2K near detector ND280,"This paper reports on the search for heavy neutrinos with masses in the range 140<MN<493  MeV/c^2 using the off-axis near detector ND280 of the T2K experiment. These particles can be produced from kaon decays in the standard neutrino beam and then subsequently decay in ND280. The decay modes under consideration are N→ℓ±απ∓ and N→ℓ+αℓ−β(−)ν(α,β=e,μ). A search for such events has been made using the Time Projection Chambers of ND280, where the background has been reduced to less than two events in the current dataset in all channels. No excess has been observed in the signal region. A combined Bayesian statistical approach has been applied to extract upper limits on the mixing elements of heavy neutrinos to electron-, muon- and tau- flavored currents (U^2e, U^2μ, U^2τ) as a function of the heavy neutrino mass, e.g., U^2e<10−9 at 90% C.L. for a mass of 390  MeV/c^2. These constraints are competitive with previous experiments."
VIVEK GOYAL,Seeing around corners with edge-resolved transient imaging,"Non-line-of-sight (NLOS) imaging is a rapidly growing field seeking to form images of objects outside the field of view, with potential applications in autonomous navigation, reconnaissance, and even medical imaging. The critical challenge of NLOS imaging is that diffuse reflections scatter light in all directions, resulting in weak signals and a loss of directional information. To address this problem, we propose a method for seeing around corners that derives angular resolution from vertical edges and longitudinal resolution from the temporal response to a pulsed light source. We introduce an acquisition strategy, scene response model, and reconstruction algorithm that enable the formation of 2.5-dimensional representations-a plan view plus heights-and a 180∘ field of view for large-scale scenes. Our experiments demonstrate accurate reconstructions of hidden rooms up to 3 meters in each dimension despite a small scan aperture (1.5-centimeter radius) and only 45 measurement locations."
VIVEK GOYAL,Necessary and sufficient conditions for sparsity pattern recovery,"The paper considers the problem of detecting the sparsity pattern of a k -sparse vector in \BBR n from m random noisy measurements. A new necessary condition on the number of measurements for asymptotically reliable detection with maximum-likelihood (ML) estimation and Gaussian measurement matrices is derived. This necessary condition for ML detection is compared against a sufficient condition for simple maximum correlation (MC) or thresholding algorithms. The analysis shows that the gap between thresholding and ML can be described by a simple expression in terms of the total signal-to-noise ratio (SNR), with the gap growing with increasing SNR. Thresholding is also compared against the more sophisticated Lasso and orthogonal matching pursuit (OMP) methods. At high SNRs, it is shown that the gap between Lasso and OMP over thresholding is described by the range of powers of the nonzero component values of the unknown signals. Specifically, the key benefit of Lasso and OMP over thresholding is the ability of Lasso and OMP to detect signals with relatively small components."
VIVEK GOYAL,A few photons among many: unmixing signal and noise for photon-efficient active imaging,"Conventional LIDAR systems require hundreds or thousands of photon detections per pixel to form accurate depth and reflectivity images. Recent photon-efficient computational imaging methods are remarkably effective with only 1.0 to 3.0 detected photons per pixel, but they are not demonstrated at signal-to-background ratio (SBR) below 1.0 because their imaging accuracies degrade significantly in the presence of high background noise. We introduce a new approach to depth and reflectivity estimation that emphasizes the unmixing of contributions from signal and noise sources. At each pixel in an image, short-duration range gates are adaptively determined and applied to remove detections likely to be due to noise. For pixels with too few detections to perform this censoring accurately, data are combined from neighboring pixels to improve depth estimates, where the neighborhood formation is also adaptive to scene content. Algorithm performance is demonstrated on experimental data at varying levels of noise. Results show improved performance of both reflectivity and depth estimates over state-of-the-art methods, especially at low SBR. In particular, accurate imaging is demonstrated with SBR as low as 0.04. This validation of a photon-efficient, noise-tolerant method demonstrates the viability of rapid, long-range, and low-power LIDAR imaging."
VIVEK GOYAL,Reduced damage in electron microscopy by using interaction-free measurement and conditional reillumination,"Interaction-free measurement (IFM) has been proposed as a means of high-resolution, low-damage imaging of radiation-sensitive samples, such as biomolecules and proteins. The basic setup for IFM is a Mach-Zehnder interferometer, and recent progress in nanofabricated electron-diffraction gratings has made it possible to incorporate a Mach-Zehnder interferometer in a transmission electron microscope (TEM). Therefore, the limits of performance of IFM with such an interferometer and a shot-noise limited electron source (such as that in a TEM) are of interest. In this paper, we compare the error probability and sample damage for ideal IFM and classical imaging schemes, through theoretical analysis and numerical simulation. We consider a sample that is either completely transparent or completely opaque at each pixel. In our analysis, we also evaluate the impact of an additional detector for scattered electrons. Inclusion of the scattering detector results in reduction of error by up to an order of magnitude, for both IFM and classical schemes. We also investigate a sample reillumination scheme based on updating priors after each round of illumination and find that this scheme further reduces error. Implementation of these methods is likely achievable with existing instrumentation and would result in improved resolution in low-dose electron microscopy."
VIVEK GOYAL,Asymptotic analysis of MAP estimation via the replica method and applications to compressed sensing,"The replica method is a nonrigorous but well-known technique from statistical physics used in the asymptotic analysis of large, random, nonlinear problems. This paper applies the replica method, under the assumption of replica symmetry, to study estimators that are maximum a posteriori (MAP) under a postulated prior distribution. It is shown that with random linear measurements and Gaussian noise, the replica-symmetric prediction of the asymptotic behavior of the postulated MAP estimate of an -dimensional vector “decouples” as scalar postulated MAP estimators. The result is based on applying a hardening argument to the replica analysis of postulated posterior mean estimators of Tanaka and of Guo and Verdú. The replica-symmetric postulated MAP analysis can be readily applied to many estimators used in compressed sensing, including basis pursuit, least absolute shrinkage and selection operator (LASSO), linear estimation with thresholding, and zero norm-regularized estimation. In the case of LASSO estimation, the scalar estimator reduces to a soft-thresholding operator, and for zero norm-regularized estimation, it reduces to a hard threshold. Among other benefits, the replica method provides a computationally tractable method for precisely predicting various performance metrics including mean-squared error and sparsity pattern recovery probability."
VIVEK GOYAL,An information-theoretic characterization of channels that die,"Given the possibility of communication systems failing catastrophically, we investigate limits to communicating over channels that fail at random times. These channels are finite-state semi-Markov channels. We show that communication with arbitrarily small probability of error is not possible. Making use of results in finite blocklength channel coding, we determine sequences of blocklengths that optimize transmission volume communicated at fixed maximum message error probabilities. We provide a partial ordering of communication channels. A dynamic programming formulation is used to show the structural result that channel state feedback does not improve performance."
VIVEK GOYAL,Photon-efficient computational 3-D and reflectivity imaging with single-photon detectors,"Capturing depth and reflectivity images at low light levels from active illumination of a scene has wide-ranging applications. Conventionally, even with detectors sensitive to individual photons, hundreds of photon detections are needed at each pixel to mitigate Poisson noise. We develop a robust method for estimating depth and reflectivity using fixed dwell time per pixel and on the order of one detected photon per pixel averaged over the scene. Our computational image formation method combines physically accurate single-photon counting statistics with exploitation of the spatial correlations present in real-world reflectivity and 3-D structure. Experiments conducted in the presence of strong background light demonstrate that our method is able to accurately recover scene depth and reflectivity, while traditional imaging methods based on maximum likelihood (ML) estimation or approximations thereof lead to noisier images. For depth, performance compares favorably to signal-independent noise removal algorithms such as median filtering or block-matching and 3-D filtering (BM3D) applied to the pixelwise ML estimate; for reflectivity, performance is similar to signal-dependent noise removal algorithms such as Poisson nonlocal sparse PCA and BM3D with variance-stabilizing transformation. Our framework increases photon efficiency 100-fold over traditional processing and also improves, somewhat, upon first-photon imaging under a total acquisition time constraint in raster-scanned operation. Thus, our new imager will be useful for rapid, low-power, and noise-tolerant active optical imaging, and its fixed dwell time will facilitate parallelization through use of a detector array."
VIVEK GOYAL,Hybrid approximate message passing,"Gaussian and quadratic approximations of message passing algorithms on graphs have attracted considerable recent attention due to their computational simplicity, analytic tractability, and wide applicability in optimization and statistical inference problems. This paper presents a systematic framework for incorporating such approximate message passing (AMP) methods in general graphical models. The key concept is a partition of dependencies of a general graphical model into strong and weak edges, with the weak edges representing interactions through aggregates of small, linearizable couplings of variables. AMP approximations based on the Central Limit Theorem can be readily applied to aggregates of many weak edges and integrated with standard message passing updates on the strong edges. The resulting algorithm, which we call hybrid generalized approximate message passing (HyGAMP), can yield significantly simpler implementations of sum-product and max-sum loopy belief propagation. By varying the partition of strong and weak edges, a performance--complexity trade-off can be achieved. Group sparsity and multinomial logistic regression problems are studied as examples of the proposed methodology."
VIVEK GOYAL,Photon-efficient imaging with a single-photon camera.,"Reconstructing a scene's 3D structure and reflectivity accurately with an active imaging system operating in low-light-level conditions has wide-ranging applications, spanning biological imaging to remote sensing. Here we propose and experimentally demonstrate a depth and reflectivity imaging system with a single-photon camera that generates high-quality images from ∼1 detected signal photon per pixel. Previous achievements of similar photon efficiency have been with conventional raster-scanning data collection using single-pixel photon counters capable of ∼10-ps time tagging. In contrast, our camera's detector array requires highly parallelized time-to-digital conversions with photon time-tagging accuracy limited to ∼ns. Thus, we develop an array-specific algorithm that converts coarsely time-binned photon detections to highly accurate scene depth and reflectivity by exploiting both the transverse smoothness and longitudinal sparsity of natural scenes. By overcoming the coarse time resolution of the array, our framework uniquely achieves high photon efficiency in a relatively short acquisition time."
VIVEK GOYAL,Compressively sampling the optical transmission matrix of a multimode fibre,"The measurement of the optical transmission matrix (TM) of an opaque material is an advanced form of space-variant aberration correction. Beyond imaging, TM-based methods are emerging in a range of fields, including optical communications, micro-manipulation, and computing. In many cases, the TM is very sensitive to perturbations in the configuration of the scattering medium it represents. Therefore, applications often require an up-to-the-minute characterisation of the fragile TM, typically entailing hundreds to thousands of probe measurements. Here, we explore how these measurement requirements can be relaxed using the framework of compressive sensing, in which the incorporation of prior information enables accurate estimation from fewer measurements than the dimensionality of the TM we aim to reconstruct. Examples of such priors include knowledge of a memory effect linking the input and output fields, an approximate model of the optical system, or a recent but degraded TM measurement. We demonstrate this concept by reconstructing the full-size TM of a multimode fibre supporting 754 modes at compression ratios down to ∼5% with good fidelity. We show that in this case, imaging is still possible using TMs reconstructed at compression ratios down to ∼1% (eight probe measurements). This compressive TM sampling strategy is quite general and may be applied to a variety of other scattering samples, including diffusers, thin layers of tissue, fibre optics of any refractive profile, and reflections from opaque walls. These approaches offer a route towards the measurement of high-dimensional TMs either quickly or with access to limited numbers of measurements."
VIVEK GOYAL,Malleable coding: compressed palimpsests,"A malleable coding scheme considers not only compression efficiency but also the ease of alteration, thus encouraging some form of recycling of an old compressed version in the formation of a new one. Malleability cost is the difficulty of synchronizing compressed versions, and malleable codes are of particular interest when representing information and modifying the representation are both expensive. We examine the trade-off between compression efficiency and malleability cost under a malleability metric defined with respect to a string edit distance. This problem introduces a metric topology to the compressed domain. We characterize the achievable rates and malleability as the solution of a subgraph isomorphism problem. This can be used to argue that allowing conditional entropy of the edited message given the original message to grow linearly with block length creates an exponential increase in code length."
VIVEK GOYAL,Malleable coding with fixed reuse,"In cloud computing, storage area networks, remote backup storage, and similar settings, stored data is modified with updates from new versions. Representing information and modifying the representation are both expensive. Therefore it is desirable for the data to not only be compressed but to also be easily modified during updates. A malleable coding scheme considers both compression efficiency and ease of alteration, promoting codeword reuse. We examine the trade-off between compression efficiency and malleability cost-the difficulty of synchronizing compressed versions-measured as the length of a reused prefix portion. Through a coding theorem, the region of achievable rates and malleability is expressed as a single-letter optimization. Relationships to common information problems are also described."
VIVEK GOYAL,Benefiting from disorder: source coding for unordered data,"The order of letters is not always relevant in a communication task. This paper discusses the implications of order irrelevance on source coding, presenting results in several major branches of source coding theory: lossless coding, universal lossless coding, rate-distortion, high-rate quantization, and universal lossy coding. The main conclusions demonstrate that there is a significant rate savings when order is irrelevant. In particular, lossless coding of n letters from a finite alphabet requires Theta(log n) bits and universal lossless coding requires n + o(n) bits for many countable alphabet sources. However, there are no universal schemes that can drive a strong redundancy measure to zero. Results for lossy coding include distribution-free expressions for the rate savings from order irrelevance in various high-rate quantization schemes. Rate-distortion bounds are given, and it is shown that the analogue of the Shannon lower bound is loose at all finite rates."
VIVEK GOYAL,Beyond thresholding: analysis and improvements for deterministic parameter estimation,"Hard-threshold estimators are popular in signal processing applications. We provide a detailed study of using hard-threshold estimators for estimating an unknown deterministic signal when additive white Gaussian noise corrupts observations. The analysis, depending heavily on Cramér-Rao bounds, motivates piecewise-linear estimation as a simple improvement to hard thresholding. We compare the performance of two piecewise-linear estimators to a hard-threshold estimator. When either piecewise-linear estimator is optimized for the decay rate of the basis coefficients, its performance is better than the best possible with hard thresholding."
VIVEK GOYAL,Predicting dead time distortion for high-flux single-photon lidar,"Detector and electronics dead times distort photon detection histograms at high flux, but can be mitigated by probabilistic modeling identifying the sequence of detections as a Markov chain."
VIVEK GOYAL,Two-dimensional non-line-of-sight scene estimation from a single edge occluder,
VIVEK GOYAL,On-off random access channels: a compressed sensing framework,"This paper considers a simple on-off random multiple access channel, where n users communicate simultaneously to a single receiver over m degrees of freedom. Each user transmits with probability 𝛌, where typically 𝛌n < m << n, and the receiver must detect which users transmitted. We show that when the codebook has i.i.d. Gaussian entries, detecting which users transmitted is mathematically equivalent to a certain sparsity detection problem considered in compressed sensing. Using recent sparsity results, we derive upper and lower bounds on the capacities of these channels. We show that common sparsity detection algorithms, such as lasso and orthogonal matching pursuit (OMP), can be used as tractable multiuser detection schemes and have significantly better performance than single-user detection. These methods do achieve some near-far resistance but--at high signal-to-noise ratios (SNRs)--may achieve capacities far below optimal maximum likelihood detection. We then present a new algorithm, called sequential OMP, that illustrates that iterative detection combined with power ordering or power shaping can significantly improve the high SNR performance. Sequential OMP is analogous to successive interference cancellation in the classic multiple access channel. Our results thereby provide insight into the roles of power control and multiuser detection on random-access signalling."
VIVEK GOYAL,SWAGGER: sparsity within and across groups for general estimation and recovery,"Penalty functions or regularization terms that promote structured solutions to optimization problems are of great interest in many fields. Proposed in this work is a nonconvex structured sparsity penalty that promotes one-sparsity within arbitrary overlapping groups in a vector. This allows one to enforce mutual exclusivity between components within solutions to optimization problems. We show multiple example use cases (including a total variation variant), demonstrate synergy between it and other regularizers, and propose an algorithm to efficiently solve problems regularized or constrained by the proposed penalty."
VIVEK GOYAL,"Time-resolved focused ion beam microscopy: modeling, estimation methods, and analyses","In a focused ion beam (FIB) microscope, source particles interact with a small volume of a sample to generate secondary electrons that are detected, pixel by pixel, to produce a micrograph. Randomness of the number of incident particles causes excess variation in the micrograph, beyond the variation in the underlying particle-sample interaction. We recently demonstrated that joint processing of multiple time-resolved measurements from a single pixel can mitigate this effect of source shot noise in helium ion microscopy. This paper is focused on establishing a rigorous framework for understanding the potential for this approach. It introduces idealized continuous- and discrete-time abstractions of FIB microscopy with direct electron detection and estimation-theoretic limits of imaging performance under these measurement models. Novel estimators for use with continuous-time measurements are introduced and analyzed, and estimators for use with discrete-time measurements are analyzed and shown to approach their continuous-time counterparts as time resolution is increased. Simulated FIB microscopy results are consistent with theoretical analyses and demonstrate that substantial improvements over conventional FIB microscopy image formation are made possible by time-resolved measurement."
VIVEK GOYAL,Dead time compensation for high-flux ranging,"Dead time effects have been considered a major limitation for fast data acquisition in various time-correlated single photon counting applications, since a commonly adopted approach for dead time mitigation is to operate in the low-flux regime where dead time effects can be ignored. Through the application of lidar ranging, this paper explores the empirical distribution of detection times in the presence of dead time and demonstrates that an accurate statistical model can result in reduced ranging error with shorter data acquisition time when operating in the high-flux regime. Specifically, we show that the empirical distribution of detection times converges to the stationary distribution of a Markov chain. Depth estimation can then be performed by passing the empirical distribution through a filter matched to the stationary distribution. Moreover, based on the Markov chain model, we formulate the recovery of arrival distribution from detection distribution as a nonlinear inverse problem and solve it via provably convergent mathematical optimization. By comparing per-detection Fisher information for depth estimation from high- and low-flux detection time distributions, we provide an analytical basis for possible improvement of ranging performance resulting from the presence of dead time. Finally, we demonstrate the effectiveness of our formulation and algorithm via simulations of lidar ranging."
VIVEK GOYAL,Nonlinear digital post-processing to mitigate jitter in sampling,"This paper describes several new algorithms for estimating the parameters of a periodic bandlimited signal from samples corrupted by jitter (timing noise) and additive noise. Both classical (non-random) and Bayesian formulations are considered: an Expectation-Maximization (EM) algorithm is developed to compute the maximum likelihood (ML) estimator for the classical estimation framework, and two Gibbs samplers are proposed to approximate the Bayes least squares (BLS) estimate for parameters independently distributed according to a uniform prior. Simulations are performed to demonstrate the significant performance improvement achievable using these algorithms as compared to linear estimators. The ML estimator is also compared to the Cramer-Rao lower bound to determine the range of jitter for which the estimator is approximately efficient. These simulations provide evidence that the nonlinear algorithms derived here can tolerate 1.4-2 times more jitter than linear estimators, reducing on-chip ADC power consumption by 50-75 percent."
VIVEK GOYAL,Online beam current estimation in particle beam microscopy,
VIVEK GOYAL,Fast computational periscopy in challenging ambient light conditions through optimized preconditioning,"Non-line-of-sight (NLOS) imaging is a rapidly advancing technology that provides asymmetric vision: seeing without being seen. Though limited in accuracy, resolution, and depth recovery compared to active methods, the capabilities of passive methods are especially surprising because they typically use only a single, inexpensive digital camera. One of the largest challenges in passive NLOS imaging is ambient background light, which limits the dynamic range of the measurement while carrying no useful information about the hidden part of the scene. In this work we propose a new reconstruction approach that uses an optimized linear transformation to balance the rejection of uninformative light with the retention of informative light, resulting in fast (video-rate) reconstructions of hidden scenes from photographs of a blank wall under high ambient light conditions."
VIVEK GOYAL,Multi-depth computational periscopy with an ordinary camera,"We demonstrate non-line-of-sight imaging of multi-depth scenes using only a single photograph from an ordinary digital camera. The hidden scene, comprising two images at different depths, is partially occluded from a visible wall by an opaque occluding object. The distance from the visible wall to the hidden surfaces, and the images they contain, are recovered."
VIVEK GOYAL,"Double your corners, double your fun: the doorway camera","In a built environment, wanting to see without direct line of sight is often due to being outside a doorway. The two vertical edges of the doorway provide occlusions that can be exploited for non-line-of-sight imaging by forming corner cameras. While each corner camera can separately yield a robust 1D reconstruction, joint processing suggests novelties in both forward modeling and inversion. The resulting doorway camera provides accurate and robust 2D reconstructions of the hidden scene. This work provides a novel inversion algorithm to jointly estimate two views of change in the hidden scene, using the temporal difference between photographs acquired on the visible side of the doorway. Successful reconstruction is demonstrated in a variety of real and rendered scenarios, including different hidden scenes and lighting conditions. A Cramer-Rao bound analysis is used to demonstrate the 2D resolving power of the doorway camera over other passive acquisition strategies and to motivate the novel biangular reconstruction grid."
VIVEK GOYAL,Edge-resolved transient imaging,"We demonstrate 2.5-dimensional, 180◦ field-of-view non-line-of-sight recon- structions of large-scale scenes using time-correlated single-photon detection and pulsed illumination along an arc at a small opening where a vertical wall edge meets a floor plane."
VIVEK GOYAL,Prevention beats removal: avoiding stripe artifacts from current variation in particle beam microscopy through time-resolved sensing,
VIVEK GOYAL,Optimal stopping times for estimating Bernoulli parameters with applications to active imaging,"We address the problem of estimating the parameter of a Bernoulli process. This arises in many applications, including photon-efficient active imaging where each illumination period is regarded as a single Bernoulli trial. We introduce a framework within which to minimize the mean-squared error (MSE) subject to an upper bound on the mean number of trials. This optimization has several simple and intuitive properties when the Bernoulli parameter has a beta prior. In addition, by exploiting typical spatial correlation using total variation regularization, we extend the developed framework to a rectangular array of Bernoulli processes representing the pixels in a natural scene. In simulations inspired by realistic active imaging scenarios, we demonstrate a 4.26 dB reduction in MSE due to the adaptive acquisition, as an average over many independent experiments and invariant to a factor of 3.4 variation in trial budget."
VIVEK GOYAL,Team decision making with social learning: human subject experiments,"We demonstrate that human decision-making agents do social learning whether it is beneficial or not. Specifically, we consider binary Bayesian hypothesis testing with multiple agents voting sequentially for a team decision, where each one observes earlier-acting agents' votes as well as a conditionally independent and identically distributed private signal. While the best strategy (for the team objective) is to ignore the votes of earlier-acting agents, human agents instead tend to be affected by others' decisions. Furthermore, they are almost equally affected in the team setting as when they are incentivized only for individual correctness. These results suggest that votes of earlier-acting agents should be withheld (not shared as public signals) to improve team decision-making performance; humans are insufficiently rational to innately apply the optimal decision rules that would ignore the public signals."
VIVEK GOYAL,Source shot noise mitigation in scanned beam microscopy,
VIVEK GOYAL,Scalar quantization with random thresholds,"The distortion-rate performance of certain randomly-designed scalar quantizers is determined. The central results are the mean-squared error distortion and output entropy for quantizing a uniform random variable with thresholds drawn independently from a uniform distribution. The distortion is at most 6 times that of an optimal (deterministically-designed) quantizer, and for a large number of levels the output entropy is reduced by approximately (1-gamma)/(ln 2) bits, where gamma is the Euler-Mascheroni constant. This shows that the high-rate asymptotic distortion of these quantizers in an entropy-constrained context is worse than the optimal quantizer by at most a factor of 6 exp(-2(1-gamma))."
VIVEK GOYAL,Malleable coding for updatable cloud caching,"In software-as-a-service applications provisioned through cloud computing, locally cached data are often modified with updates from new versions. In some cases, with each edit, one may want to preserve both the original and new versions. In this paper, we focus on cases in which only the latest version must be preserved. Furthermore, it is desirable for the data to not only be compressed but to also be easily modified during updates, since representing information and modifying the representation both incur cost. We examine whether it is possible to have both compression efficiency and ease of alteration, in order to promote codeword reuse. In other words, we study the feasibility of a malleable and efficient coding scheme. The tradeoff between compression efficiency and malleability cost-the difficulty of synchronizing compressed versions-is measured as the length of a reused prefix portion. The region of achievable rates and malleability is found. Drawing from prior work on common information problems, we show that efficient data compression may not be the best engineering design principle when storing software-as-a-service data. In the general case, goals of efficiency and malleability are fundamentally in conflict."
VIVEK GOYAL,High-flux single-photon lidar,"In time-correlated single-photon counting (TCSPC), photons that arrive during the detector and timing electronics dead times are missed, causing distortion of the detection time distribution. Conventional wisdom holds that TCSPC should be performed with detections in fewer than 5% of illumination cycles to avoid substantial distortion. This requires attenuation and leads to longer acquisition times if the incident flux is too high. Through the example of ranging with a single-photon lidar system, this work demonstrates that accurately modeling the sequence of detection times as a Markov chain allows for measurements at much higher incident flux without attenuation. Our probabilistic model is validated by the close match between the limiting distribution of the Markov chain and both simulated and experimental data, so long as issues of calibration and afterpulsing are minimal. We propose an algorithm that corrects for the distortion in detection histograms caused by dead times without assumptions on the form of the transient light intensity. Our histogram correction yields substantially improved depth imaging performance, and modest additional improvement is achieved with a parametric model assuming a single depth per pixel. We show results for depth and flux estimation with up to 5 photoelectrons per illumination cycle on average, facilitating an increase in time efficiency of more than two orders of magnitude. The use of identical TCSPC equipment in other fields suggests that our modeling and histogram correction could likewise enable high-flux acquisitions in fluorescence lifetime microscopy or quantum optics applications."
VIVEK GOYAL,Intersensor collaboration in distributed quantization networks,"Several key results in distributed source coding offer the intuition that little improvement in compression can be gained from intersensor communication when the information is coded in long blocks. However, when sensors are restricted to code their observations in small blocks (e.g., one) or desire fidelity of a computation applied to source realizations, intelligent collaboration between sensors can greatly reduce distortion. For networks where sensors are allowed to ""chat"" using a side channel that is unobservable at the fusion center, we provide asymptotically-exact characterization of distortion performance and optimal quantizer design in the high-resolution (low-distortion) regime using a framework called distributed functional scalar quantization (DFSQ). The key result is that chatting can dramatically improve performance even when intersensor communication is at very low rate. We also solve the rate allocation problem when communication links have heterogeneous costs and provide a detailed example to demonstrate the theoretical and practical gains from chatting. This example for maximum computation gives insight on the gap between chatting and distributed networks, and how to optimize the intersensor communication."
VIVEK GOYAL,Quantization of prior probabilities for collaborative distributed hypothesis testing,"This paper studies the quantization of prior probabilities, drawn from an ensemble, in distributed detection with data fusion by combination of binary local decisions. Design and performance equivalences between a team of N agents and a more powerful single agent are obtained. Effects of identical quantization and diverse quantization on mean Bayes risk are compared. It is shown that when agents using diverse quantizers interact to agree on a perceived common risk, the effective number quantization levels is increased. With this collaboration, optimal diverse regular quantization with K cells per quantizer performs as well as optimal identical quantization with N ( K -1)+1 cells per quantizer. Similar results are obtained for the maximum Bayes risk error criterion."
VIVEK GOYAL,Time-stampless adaptive nonuniform sampling for stochastic signals,"In this paper, we introduce a time-stampless adaptive nonuniform sampling (TANS) framework, in which time increments between samples are determined by a function of the m most recent increments and sample values. Since only past samples are used in computing time increments, it is not necessary to save sampling times (time stamps) for use in the reconstruction process. We focus on two TANS schemes for discrete-time stochastic signals: a greedy method, and a method based on dynamic programming. We analyze the performances of these schemes by computing (or bounding) their trade-offs between sampling rate and expected reconstruction distortion for autoregressive and Markovian signals. Simulation results support the analysis of the sampling schemes. We show that, by opportunistically adapting to local signal characteristics, TANS may lead to improved power efficiency in some applications."
VIVEK GOYAL,Distributed hypothesis testing with social learning and symmetric fusion,"We study the utility of social learning in a distributed detection model with agents sharing the same goal: a collective decision that optimizes an agreed upon criterion. We show that social learning is helpful in some cases but is provably futile (and thus essentially a distraction) in other cases. Specifically, we consider Bayesian binary hypothesis testing performed by a distributed detection and fusion system, where all decision-making agents have binary votes that carry equal weight. Decision-making agents in the team sequentially make local decisions based on their own private signals and all precedent local decisions. It is shown that the optimal decision rule is not affected by precedent local decisions when all agents observe conditionally independent and identically distributed private signals. Perfect Bayesian reasoning will cancel out all effects of social learning. When the agents observe private signals with different signal-to-noise ratios, social learning is again futile if the team decision is only approved by unanimity. Otherwise, social learning can strictly improve the team performance. Furthermore, the order in which agents make their decisions affects the team decision."
VIVEK GOYAL,Social teaching: being informative vs. being right in sequential decision making,"We consider sequential Bayesian binary hypothesis testing where each individual agent makes a binary decision motivated only by minimization of her own perception of the Bayes risk. The information available to each agent is an initial belief, a private signal, and decisions of all earlier-acting agents; it is follows that each agent should apply a standard Bayesian update of her belief as in social learning. The effect of the set of initial beliefs on the decision-making performance of the last agent is studied. In general, the optimal initial beliefs are not equal to the actual prior probability. When the private signals are described by Gaussian likelihoods, they also are not haphazard, but rather follow a systematic pattern: The earlier-acting agents should act as if the prior probability is larger than it is in reality when the true prior probability is small, and vice versa. We interpret this as being open-minded toward the unlikely hypothesis. Such open-mindedness increases but does not maximize the mutual information between the true hypothesis and a decision."
VIVEK GOYAL,Keep ballots secret: on the futility of social learning in decision making by voting,"We show that social learning is not useful in a model of team binary decision making by voting, where each vote carries equal weight. Specifically, we consider Bayesian binary hypothesis testing where agents have any conditionally-independent observation distribution and their local decisions are fused by any L-out-of-N fusion rule. The agents make local decisions sequentially, with each allowed to use its own private signal and all precedent local decisions. Though social learning generally occurs in that precedent local decisions affect an agent's belief, optimal team performance is obtained when all precedent local decisions are ignored. Thus, social learning is futile, and secret ballots are optimal. This conclusion contrasts with typical studies of social learning because we include a fusion center rather than concentrating on the performance of the latest-acting agents."
VIVEK GOYAL,Distributed scalar quantization for computing: high-resolution analysis and extensions,"Communication of quantized information is frequently followed by a computation. We consider situations of distributed functional scalar quantization: distributed scalar quantization of (possibly correlated) sources followed by centralized computation of a function. Under smoothness conditions on the sources and function, companding scalar quantizer designs are developed to minimize mean-squared error (MSE) of the computed function as the quantizer resolution is allowed to grow. Striking improvements over quantizers designed without consideration of the function are possible and are larger in the entropy-constrained setting than in the fixed-rate setting. As extensions to the basic analysis, we characterize a large class of functions for which regular quantization suffices, consider certain functions for which asymptotic optimality is achieved without arbitrarily fine quantization, and allow limited collaboration between source encoders. In the entropy-constrained setting, a single bit per sample communicated between encoders can have an arbitrarily large effect on functional distortion. In contrast, such communication has very little effect in the fixed-rate setting."
VIVEK GOYAL,Performance analysis of low-flux least-squares single-pixel imaging,"A single-pixel camera is able to computationally form spatially resolved images using one photodetector and a spatial light modulator. The images it produces in low-light-level operation are imperfect, even when the number of measurements exceeds the number of pixels, because its photodetection measurements are corrupted by Poisson noise. Conventional performance analysis for single-pixel imaging generates estimates of mean-square error (MSE) from Monte Carlo simulations, which require long computational times. In this letter, we use random matrix theory to develop a closed-form approximation to the MSE of the widely used least-squares inversion method for Poisson noise-limited single-pixel imaging. We present numerical experiments that validate our approximation and a motivating example showing how our framework can be used to answer practical optical design questions for a single-pixel camera."
VIVEK GOYAL,Photon-efficient super-resolution laser radar,"The resolution achieved in photon-efficient active optical range imaging systems can be low due to non-idealities such as propagation through a diffuse scattering medium. We propose a constrained optimization-based frame- work to address extremes in scarcity of photons and blurring by a forward imaging kernel. We provide two algorithms for the resulting inverse problem: a greedy algorithm, inspired by sparse pursuit algorithms; and a convex optimization heuristic that incorporates image total variation regularization. We demonstrate that our framework outperforms existing deconvolution imaging techniques in terms of peak signal-to-noise ratio. Since our proposed method is able to super-resolve depth features using small numbers of photon counts, it can be useful for observing fine-scale phenomena in remote sensing through a scattering medium and through-the-skin biomedical imaging applications."
VIVEK GOYAL,Distributed functional scalar quantization simplified,"Distributed functional scalar quantization (DFSQ) theory provides optimality conditions and predicts performance of data acquisition systems in which a computation on acquired data is desired. We address two limitations of previous works: prohibitively expensive decoder design and a restriction to source distributions with bounded support. We show that a much simpler decoder has equivalent asymptotic performance to the conditional expectation estimator studied previously, thus reducing decoder design complexity. The simpler decoder features decoupled communication and computation blocks. Moreover, we extend the DFSQ framework with the simpler decoder to source distributions with unbounded support. Finally, through simulation results, we demonstrate that performance at moderate coding rates is well predicted by the asymptotic analysis, and we give new insight on the rate of convergence."
VIVEK GOYAL,Simultaneously sparse solutions to linear inverse problems with multiple system matrices and a single observation vector,"A problem that arises in slice-selective magnetic resonance imaging (MRI) radio-frequency (RF) excitation pulse design is abstracted as a novel linear inverse problem with a simultaneous sparsity constraint. Multiple unknown signal vectors are to be determined, where each passes through a different system matrix and the results are added to yield a single observation vector. Given the matrices and lone observation, the objective is to find a simultaneously sparse set of unknown vectors that approximately solves the system. We refer to this as the multiple-system single-output (MSSO) simultaneous sparse approximation problem. This manuscript contrasts the MSSO problem with other simultaneous sparsity problems and conducts an initial exploration of algorithms with which to solve it. Greedy algorithms and techniques based on convex relaxation are derived and compared empirically. Experiments involve sparsity pattern recovery in noiseless and noisy settings and MRI RF pulse design."
VIVEK GOYAL,On the estimation of nonrandom signal coefficients from jittered samples,"This paper examines the problem of estimating the parameters of a bandlimited signal from samples corrupted by random jitter (timing noise) and additive, independent identically distributed (i.i.d.) Gaussian noise, where the signal lies in the span of a finite basis. For the presented classical estimation problem, the Cramér-Rao lower bound (CRB) is computed, and an Expectation-Maximization (EM) algorithm approximating the maximum likelihood (ML) estimator is developed. Simulations are performed to study the convergence properties of the EM algorithm and compare the performance both against the CRB and a basic linear estimator. These simulations demonstrate that by postprocessing the jittered samples with the proposed EM algorithm, greater jitter can be tolerated, potentially reducing on-chip ADC power consumption substantially."
VIVEK GOYAL,Robustness of time-resolved measurement to unknown and variable beam current in particle beam microscopy,"Variations in the intensity of the incident beam can cause significant inaccuracies in microscopes that use focused beams of electrons or ions. Existing mitigation methods depend on the artifacts having characteristic spatial structures explained by the raster scan pattern and temporal correlation of the beam current variations. We show that recently introduced time-resolved measurement methods create robustness to beam current variations that improve significantly upon existing methods while not depending on separability of artifact structure from underlying image content. These advantages are illustrated through Monte Carlo simulations representative of both helium ion microscopy (higher secondary electron yield) and scanning electron microscopy (lower secondary electron yield). Notably, this demonstrates that when the beam current variation is appreciable, time-resolved measurements provide a novel benefit in particle beam microscopy that extends to low secondary electron yields."
VIVEK GOYAL,Bayesian post-processing methods for jitter mitigation in sampling,"Minimum mean-square error (MMSE) estimators of signals from samples corrupted by jitter (timing noise) and additive noise are nonlinear, even when the signal parameters and additive noise have normal distributions. This paper develops a stochastic algorithm based on Gibbs sampling and slice sampling to approximate the optimal MMSE estimator in this Bayesian formulation. Simulations demonstrate that this nonlinear algorithm can improve significantly upon the linear MMSE estimator, as well as the EM algorithm approximation to the maximum likelihood (ML) estimator used in classical estimation. Effective off-chip postprocessing to mitigate jitter enables greater jitter to be tolerated, potentially reducing on-chip ADC power consumption."
VIVEK GOYAL,Optimal quantization for compressive sensing under message passing reconstruction,We consider the optimal quantization of compressive sensing measurements along with estimation from quantized samples using generalized approximate message passing (GAMP). GAMP is an iterative reconstruction scheme inspired by the belief propagation algorithm on bipartite graphs which generalizes approximate message passing (AMP) for arbitrary measurement channels. Its asymptotic error performance can be accurately predicted and tracked through the state evolution formalism. We utilize these results to design mean-square optimal scalar quantizers for GAMP signal reconstruction and empirically demonstrate the superior error performance of the resulting quantizers.
VIVEK GOYAL,Estimation from quantized Gaussian measurements: when and how to use dither,"Subtractive dither is a powerful method for removing the signal dependence of quantization noise for coarsely quantized signals. However, estimation from dithered measurements often naively applies the sample mean or midrange, even when the total noise is not well described with a Gaussian or uniform distribution. We show that the generalized Gaussian distribution approximately describes subtractively dithered, quantized samples of a Gaussian signal. Furthermore, a generalized Gaussian fit leads to simple estimators based on order statistics that match the performance of more complicated maximum likelihood estimators requiring iterative solvers. The order statistics-based estimators outperform both the sample mean and midrange for nontrivial sums of Gaussian and uniform noise. Additional analysis of the generalized Gaussian approximation yields rules of thumb for determining when and how to apply dither to quantized measurements. Specifically, we find subtractive dither to be beneficial when the ratio between the Gaussian standard deviation and quantization interval length is roughly less than one-third. When that ratio is also greater than 0.822/K^0.930 for the number of measurements K > 20, estimators we present are more efficient than the midrange."
VIVEK GOYAL,On palimpsests in neural memory: an information theory viewpoint,"The finite capacity of neural memory and the reconsolidation phenomenon suggest it is important to be able to update stored information as in a palimpsest, where new information overwrites old information. Moreover, changing information in memory is metabolically costly. In this paper, we suggest that information-theoretic approaches may inform the fundamental limits in constructing such a memory system. In particular, we define malleable coding, that considers not only representation length but also ease of representation update, thereby encouraging some form of recycling to convert an old codeword into a new one. Malleability cost is the difficulty of synchronizing compressed versions, and malleable codes are of particular interest when representing information and modifying the representation are both expensive. We examine the tradeoff between compression efficiency and malleability cost, under a malleability metric defined with respect to a string edit distance. This introduces a metric topology to the compressed domain. We characterize the exact set of achievable rates and malleability as the solution of a subgraph isomorphism problem. This is all done within the optimization approach to biology framework."
VIVEK GOYAL,Concentric permutation source codes,"Permutation codes are a class of structured vector quantizers with a computationally-simple encoding procedure based on sorting the scalar components. Using a codebook comprising several permutation codes as subcodes preserves the simplicity of encoding while increasing the number of rate-distortion operating points, improving the convex hull of operating points, and increasing design complexity. We show that when the subcodes are designed with the same composition, optimization of the codebook reduces to a lower-dimensional vector quantizer design within a single cone. Heuristics for reducing design complexity are presented, including an optimization of the rate allocation in a shape-gain vector quantizer with gain-dependent wrapped spherical shape codebook."
VIVEK GOYAL,Frame permutation quantization,"Frame permutation quantization (FPQ) is a new vector quantization technique using finite frames. In FPQ, a vector is encoded using a permutation source code to quantize its frame expansion. This means that the encoding is a partial ordering of the frame expansion coefficients. Compared to ordinary permutation source coding, FPQ produces a greater number of possible quantization rates and a higher maximum rate. Various representations for the partitions induced by FPQ are presented, and reconstruction algorithms based on linear programming, quadratic programming, and recursive orthogonal projection are derived. Implementations of the linear and quadratic programming algorithms for uniform and Gaussian sources show performance improvements over entropy-constrained scalar quantization for certain combinations of vector dimension and coding rate. Monte Carlo evaluation of the recursive algorithm shows that mean-squared error (MSE) decays as for an M-element frame, which is consistent with previous results on optimal decay of MSE. Reconstruction using the canonical dual frame is also studied, and several results relate properties of the analysis frame to whether linear reconstruction techniques provide consistent reconstructions."
VIVEK GOYAL,Quantum-inspired computational imaging,"Computational imaging combines measurement and computational methods with the aim of forming images even when the measurement conditions are weak, few in number, or highly indirect. The recent surge in quantum-inspired imaging sensors, together with a new wave of algorithms allowing on-chip, scalable and robust data processing, has induced an increase of activity with notable results in the domain of low-light flux imaging and sensing. We provide an overview of the major challenges encountered in low-illumination (e.g., ultrafast) imaging and how these problems have recently been addressed for imaging applications in extreme conditions. These methods provide examples of the future imaging solutions to be developed, for which the best results are expected to arise from an efficient codesign of the sensors and data analysis tools."
VIVEK GOYAL,Computational multi-depth single-photon imaging,"We present an imaging framework that is able to accurately reconstruct multiple depths at individual pixels from single-photon observations. Our active imaging method models the single-photon detection statistics from multiple reflectors within a pixel, and it also exploits the fact that a multi-depth profile at each pixel can be expressed as a sparse signal. We interpret the multi-depth reconstruction problem as a sparse deconvolution problem using single-photon observations, create a convex problem through discretization and relaxation, and use a modified iterative shrinkage-thresholding algorithm to efficiently solve for the optimal multi-depth solution. We experimentally demonstrate that the proposed framework is able to accurately reconstruct the depth features of an object that is behind a partially-reflecting scatterer and 4 m away from the imager with root mean-square error of 11 cm, using only 19 signal photon detections per pixel in the presence of moderate background light. In terms of root mean-square error, this is a factor of 4.2 improvement over the conventional method of Gaussian-mixture fitting for multi-depth recovery."
VIVEK GOYAL,Single-photon depth imaging using a union-of-subspaces model,"Light detection and ranging systems reconstruct scene depth from time-of-flight measurements. For low light-level depth imaging applications, such as remote sensing and robot vision, these systems use single-photon detectors that resolve individual photon arrivals. Even so, they must detect a large number of photons to mitigate Poisson shot noise and reject anomalous photon detections from background light. We introduce a novel framework for accurate depth imaging using a small number of detected photons in the presence of an unknown amount of background light that may vary spatially. It employs a Poisson observation model for the photon detections plus a union-of-subspaces constraint on the discrete-time flux from the scene at any single pixel. Together, they enable a greedy signal-pursuit algorithm to rapidly and simultaneously converge on accurate estimates of scene depth and background flux, without any assumptions on spatial correlations of the depth or background flux. Using experimental single-photon data, we demonstrate that our proposed framework recovers depth features with 1.7 cm absolute error, using 15 photons per image pixel and an illumination pulse with 6.7-cm scaled root-mean-square length. We also show that our framework outperforms the conventional pixelwise log-matched filtering, which is a computationally-efficient approximation to the maximum-likelihood solution, by a factor of 6.1 in absolute depth error."
VIVEK GOYAL,Addressing neon gas field ion source instability through online beam current estimation,
VIVEK GOYAL,Secondary electron count imaging in SEM,"Scanning electron microscopy (SEM) is a versatile technique used to image samples at the nanoscale. Conventional imaging by this technique relies on finding the average intensity of the signal generated on a detector by secondary electrons (SEs) emitted from the sample and is subject to noise due to variations in the voltage signal from the detector. This noise can result in degradation of the SEM image quality for a given imaging dose. SE count imaging, which uses the direct count of SEs detected from the sample instead of the average signal intensity, would overcome this limitation and lead to improvement in SEM image quality. In this paper, we implement an SE count imaging scheme by synchronously outcoupling the detector and beam scan signals from the microscope and using custom code to count detected SEs. We demonstrate a ∼30% increase in the image signal-to-noise-ratio due to SE counting compared to conventional imaging. The only external hardware requirement for this imaging scheme is an oscilloscope fast enough to accurately sample the detector signal for SE counting, making the scheme easily implementable on any SEM."
VIVEK GOYAL,Online beam current estimation in particle beam microscopy through time-resolved measurement,
VIVEK GOYAL,Beyond binomial and negative binomial: adaptation in Bernoulli parameter estimation,"Estimating the parameter of a Bernoulli process arises in many applications, including photon-efficient active imaging where each illumination period is regarded as a single Bernoulli trial. Motivated by acquisition efficiency when multiple Bernoulli processes (e.g., multiple pixels) are of interest, we formulate the allocation of trials under a constraint on the mean as an optimal resource allocation problem. An oracle-aided trial allocation demonstrates that there can be a significant advantage from varying the allocation for different processes and inspires the introduction of a simple trial allocation gain quantity. Motivated by achieving this gain without an oracle, we present a trellis-based framework for representing and optimizing stopping rules. Considering the convenient case of Beta priors, three implementable stopping rules with similar performances are explored, and the simplest of these is shown to asymptotically achieve the oracle-aided trial allocation. These approaches are further extended to estimating functions of a Bernoulli parameter. In simulations inspired by realistic active imaging scenarios, we demonstrate significant mean-squared error improvements up to 4.36 dB for the estimation of p and up to 1.86 dB for the estimation of log p."
VIVEK GOYAL,Computational periscopy with an ordinary digital camera,"Computing the amounts of light arriving from different directions enables a diffusely reflecting surface to play the part of a mirror in a periscope—that is, perform non-line-of-sight imaging around an obstruction. Because computational periscopy has so far depended on light-travel distances being proportional to the times of flight, it has mostly been performed with expensive, specialized ultrafast optical systems^1,2,3,4,5,6,7,8,9,10,11,12. Here we introduce a two-dimensional computational periscopy technique that requires only a single photograph captured with an ordinary digital camera. Our technique recovers the position of an opaque object and the scene behind (but not completely obscured by) the object, when both the object and scene are outside the line of sight of the camera, without requiring controlled or time-varying illumination. Such recovery is based on the visible penumbra of the opaque object having a linear dependence on the hidden scene that can be modelled through ray optics. Non-line-of-sight imaging using inexpensive, ubiquitous equipment may have considerable value in monitoring hazardous environments, navigation and detecting hidden adversaries."
VIVEK GOYAL,A few good photons: unmixing to mitigate high ambient light levels in active imaging,"Recent photon-efficient LIDAR methods are effective with 1.0 detected photon per pixel, half from background. With a novel emphasis on unmixing signal and background contributions, we demonstrate accurate imaging with 25 times as much background."
VIVEK GOYAL,Robustness of time- resolved measurement to unknown and variable beam current in particle beam microscopy,"Variations in the intensity of the incident beam can cause significant inaccuracies in microscopes that use focused beams of electrons or ions. Existing mitigation methods depend on the artifacts having characteristic spatial structures explained by the raster scan pattern and temporal correlation of the beam current variations. We show that recently introduced time-resolved measurement methods create robustness to beam current variations that improve significantly upon existing methods while not depending on separability of artifact structure from underlying image content. These advantages are illustrated through Monte Carlo simulations representative of both helium ion microscopy (higher secondary electron yield) and scanning electron microscopy (lower secondary electron yield). Notably, this demonstrates that when the beam current variation is appreciable, time-resolved measurements provide a novel benefit in particle beam microscopy that extends to low secondary electron yields."
VIVEK GOYAL,"Edge-resolved transient imaging: performance analyses, optimizations, and simulations","Edge-resolved transient imaging (ERTI) is a method for non-line-of-sight imaging that combines the use of direct time of flight for measuring distances with the azimuthal angular resolution afforded by a vertical edge occluder. Recently conceived and demonstrated for the first time, no performance analyses or optimizations of ERTI have appeared in published papers. This paper explains how the difficulty of detection of hidden scene objects with ERTI depends on a variety of parameters, including illumination power, acquisition time, ambient light, visible-side reflectivity, hidden-side reflectivity, target range, and target azimuthal angular position. Based on this analysis, optimization of the acquisition process is introduced whereby the illumination dwell times are varied to counteract decreasing signal-to-noise ratio at deeper angles into the hidden volume. Inaccuracy caused by a coaxial approximation is also analyzed and simulated."
VIVEK GOYAL,Beliefs in decision-making cascades,"This work explores a social learning problem with agents having nonidentical noise variances and mismatched beliefs. We consider an N-agent binary hypothesis test in which each agent sequentially makes a decision based not only on a private observation, but also on preceding agents' decisions. In addition, the agents have their own beliefs instead of the true prior, and have nonidentical noise variances in the private signal. We focus on the Bayes risk of the last agent, where preceding agents are selfish. We first derive the optimal decision rule by recursive belief update and conclude, counterintuitively, that beliefs deviating from the true prior could be optimal in this setting. The effect of nonidentical noise levels in the two-agent case is also considered and analytical properties of the optimal belief curves are given. Next, we consider a predecessor selection problem wherein the subsequent agent of a certain belief chooses a predecessor from a set of candidates with varying beliefs. We characterize the decision region for choosing such a predecessor and argue that a subsequent agent with beliefs varying from the true prior often ends up selecting a suboptimal predecessor, indicating the need for a social planner. Lastly, we discuss an augmented intelligence design problem that uses a model of human behavior from cumulative prospect theory and investigate its near-optimality and suboptimality."
VIVEK GOYAL,"Advances in single-photon lidar for autonomous vehicles: working principles, challenges, and recent advances","The safety and success of autonomous vehicles (AVs) depend on their ability to accurately map and respond to their surroundings in real time. One of the most promising recent technologies for depth mapping is single-photon lidar (SPL), which measures the time of flight of individual photons. The long-range capabilities (kilometers), excellent depth resolution (centimeters), and use of low-power (eye-safe) laser sources renders this modality a strong candidate for use in AVs. While presenting unique opportunities, the remarkable sensitivity of single-photon detectors introduces several signal processing challenges. The discrete nature of photon counting and the particular design of the detection devices means the acquired signals cannot be treated as arising in a linear system with additive Gaussian noise. Moreover, the number of useful photon detections may be small despite a large data volume, thus requiring careful modeling and algorithmic design for real-time performance. This article discusses the main working principles of SPL and summarizes recent advances in signal processing techniques for this modality, highlighting promising applications in AVs as well as a number of challenges for vehicular lidar that cannot be solved by better hardware alone."
VIVEK GOYAL,Non-line-of-sight imaging over 1.43 km,"Non-line-of-sight (NLOS) imaging has the ability to reconstruct hidden objects from indirect light paths that scatter multiple times in the surrounding environment, which is of considerable interest in a wide range of applications. Whereas conventional imaging involves direct line-of-sight light transport to recover the visible objects, NLOS imaging aims to reconstruct the hidden objects from the indirect light paths that scatter multiple times, typically using the information encoded in the time-of-flight of scattered photons. Despite recent advances, NLOS imaging has remained at short-range realizations, limited by the heavy loss and the spatial mixing due to the multiple diffuse reflections. Here, both experimental and conceptual innovations yield hardware and software solutions to increase the standoff distance of NLOS imaging from meter to kilometer range, which is about three orders of magnitude longer than previous experiments. In hardware, we develop a high-efficiency, low-noise NLOS imaging system at near-infrared wavelength based on a dual-telescope confocal optical design. In software, we adopt a convex optimizer, equipped with a tailored spatial-temporal kernel expressed using three-dimensional matrix, to mitigate the effect of the spatial-temporal broadening over long standoffs. Together, these enable our demonstration of NLOS imaging and real-time tracking of hidden objects over a distance of 1.43 km. The results will open venues for the development of NLOS imaging techniques and relevant applications to real-world conditions."
VIVEK GOYAL,Dithered depth imaging,"Single-photon lidar (SPL) is a promising technology for depth measurement at long range or from weak reflectors because of the sensitivity to extremely low light levels. However, constraints on the timing resolution of existing arrays of single-photon avalanche diode (SPAD) detectors limit the precision of resulting depth estimates. In this work, we describe an implementation of subtractively-dithered SPL that can recover high-resolution depth estimates despite the coarse resolution of the detector. Subtractively-dithered measurement is achieved by adding programmable delays into the photon timing circuitry that introduce relative time shifts between the illumination and detection that are shorter than the time bin duration. Careful modeling of the temporal instrument response function leads to an estimator that outperforms the sample mean and results in depth estimates with up to 13 times lower root mean-squared error than if dither were not used. The simple implementation and estimation suggest that globally dithered SPAD arrays could be used for high spatial- and temporal-resolution depth sensing."
VIVEK GOYAL,Convolutional neural network denoising of focused ion beam micrographs,"Most research on deep learning algorithms for image denoising has focused on signal-independent additive noise. Focused ion beam (FIB) microscopy with direct secondary electron detection has an unusual Neyman Type A (compound Poisson) measurement model, and sample damage poses fundamental challenges in obtaining training data. Model-based estimation is difficult and ineffective because of the nonconvexity of the negative log likelihood. In this paper, we develop deep learning-based denoising methods for FIB micrographs using synthetic training data generated from natural images. To the best of our knowledge, this is the first attempt in the literature to solve this problem with deep learning. Our results show that the proposed methods slightly outperform a total variation-regularized model-based method that requires time-resolved measurements that are not conventionally available. Improvements over methods using conventional measurements and less accurate noise modeling are dramatic - around 10 dB in peak signal-to-noise ratio."
VIVEK GOYAL,Source shot noise mitigation in focused ion beam microscopy by time-resolved measurement,"Focused ion beam microscopy suffers from source shot noise - random variation in the number of incident ions in any fixed dwell time - along with random variation in the number of detected secondary electrons per incident ion. This multiplicity of sources of randomness increases the variance of the measurements and thus worsens the trade-off between incident ion dose and image accuracy. Repeated measurement with low dwell time, without changing the total ion dose, is a way to introduce time resolution to this form of microscopy. Through theoretical analyses and Monte Carlo simulations, we show that three ways to process time-resolved measurements result in mean-squared error (MSE) improvements compared to the conventional method of having no time resolution. In particular, maximum likelihood estimation provides reduction in MSE or reduction in required dose by a multiplicative factor approximately equal to the secondary electron yield. This improvement factor is similar to complete mitigation of source shot noise. Experiments with a helium ion microscope are consistent with the analyses and suggest accuracy improvement for a fixed source dose by a factor of about 4."
VIVEK GOYAL,Beliefs and expertise in sequential decision making,"This work explores a sequential decision making problem with agents having diverse expertise and mismatched beliefs. We consider an N-agent sequential binary hypothesis test in which each agent sequentially makes a decision based not only on a private observation, but also on previous agents’ decisions. In addition, the agents have their own beliefs instead of the true prior, and have varying expertise in terms of the noise variance in the private signal. We focus on the risk of the last-acting agent, where precedent agents are selfish. Thus, we call this advisor(s)-advisee sequential decision making. We first derive the optimal decision rule by recursive belief update and conclude, counterintuitively, that beliefs deviating from the true prior could be optimal in this setting. The impact of diverse noise levels (which means diverse expertise levels) in the two-agent case is also considered and the analytical properties of the optimal belief curves are given. These curves, for certain cases, resemble probability weighting functions from cumulative prospect theory, and so we also discuss the choice of Prelec weighting functions as an approximation for the optimal beliefs, and the possible psychophysical optimality of human beliefs. Next, we consider an advisor selection problem where in the advisee of a certain belief chooses an advisor from a set of candidates with varying beliefs. We characterize the decision region for choosing such an advisor and argue that an advisee with beliefs varying from the true prior often ends up selecting a suboptimal advisor, indicating the need for a social planner. We close with a discussion on the implications of the study toward designing artificial intelligence systems for augmenting human intelligence."
VIVEK GOYAL,Occlusion-based computational periscopy with consumer cameras,"The ability to form images of scenes hidden from direct view would be advantageous in many applications – from improved motion planning and collision avoidance in autonomous navigation to enhanced danger anticipation for first-responders in search-and-rescue missions. Recent techniques for imaging around corners have mostly relied on time-of-flight measurements of light propagation, necessitating the use of expensive, specialized optical systems. In this work, we demonstrate how to form images of hidden scenes from intensity-only measurements of the light reaching a visible surface from the hidden scene. Our approach exploits the penumbra cast by an opaque occluding object onto a visible surface. Specifically, we present a physical model that relates the measured photograph to the radiosity of the hidden scene and the visibility function due to the opaque occluder. For a given scene–occluder setup, we characterize the parts of the hidden region for which the physical model is well-conditioned for inversion – i.e., the computational field of view (CFOV) of the imaging system. This concept of CFOV is further verified through the Cram´er–Rao bound of the hidden-scene estimation problem. Finally, we present a two-step computational method for recovering the occluder and the scene behind it. We demonstrate the effectiveness of the proposed method using both synthetic and experimentally measured data."
VIVEK GOYAL,Offline secondary electron counting and conditional re-illumination in SEM,
VIVEK GOYAL,Shot noise-mitigated secondary electron imaging with ion count-aided microscopy,"Modern science is dependent on imaging on the nanoscale, often achieved through processes that detect secondary electrons created by a highly focused incident charged particle beam. Multiple types of measurement noise limit the ultimate trade-off between the image quality and the incident particle dose, which can preclude useful imaging of dose-sensitive samples. Existing methods to improve image quality do not fundamentally mitigate the noise sources. Furthermore, barriers to assigning a physically meaningful scale make the images qualitative. Here, we introduce ion count-aided microscopy (ICAM), which is a quantitative imaging technique that uses statistically principled estimation of the secondary electron yield. With a readily implemented change in data collection, ICAM substantially reduces source shot noise. In helium ion microscopy, we demonstrate 3[Formula: see text] dose reduction and a good match between these empirical results and theoretical performance predictions. ICAM facilitates imaging of fragile samples and may make imaging with heavier particles more attractive."
VIVEK GOYAL,Continuous-time modeling and analysis of particle beam metrology,
VIVEK GOYAL,Non-line-of-sight snapshots and background mapping with an active corner camera,"The ability to form reconstructions beyond line-of-sight view could be transformative in a variety of fields, including search and rescue, autonomous vehicle navigation, and reconnaissance. Most existing active non-line-of-sight (NLOS) imaging methods use data collection steps in which a pulsed laser is directed at several points on a relay surface, one at a time. The prevailing approaches include raster scanning of a rectangular grid on a vertical wall opposite the volume of interest to generate a collection of confocal measurements. These and a recent method that uses a horizontal relay surface are inherently limited by the need for laser scanning. Methods that avoid laser scanning to operate in a snapshot mode are limited to treating the hidden scene of interest as one or two point targets. In this work, based on more complete optical response modeling yet still without multiple illumination positions, we demonstrate accurate reconstructions of foreground objects while also introducing the capability of mapping the stationary scenery behind moving objects. The ability to count, localize, and characterize the sizes of hidden objects, combined with mapping of the stationary hidden scene, could greatly improve indoor situational awareness in a variety of applications."
VIVEK GOYAL,MEGS: a penalty for mutually exclusive group sparsity,
VIVEK GOYAL,The role of detection times in reflectivity estimation with single-photon lidar,
VIVEK GOYAL,Denoising particle beam micrographs with plug-and-play methods,
VIVEK GOYAL,"Absorption-based hyperspectral thermal ranging: performance analyses, optimization, and simulations","The wavelength dependence of atmospheric absorption creates range cues in hyperspectral measurements that can be exploited for passive ranging using only thermal emissions. In this work, we present fundamental limits on absorption-based ranging under a model of known air temperature and wavelength-dependent attenuation coefficient, with object temperature and emissivity unknown; reflected solar and environmental radiance is omitted from our analysis. Fisher information computations illustrate how performance limits depend on atmospheric conditions such as air temperature and humidity; temperature contrast in the scene; spectral resolution of measurement; and distance. These results should prove valuable in sensor system design."
GREGORY MELCHOR-BARZ,Sonicization of gender in Tanzania kwaya congregational music,"In this article, I introduce issues related to the embodiment of gendered sound in contemporary Tanzanian Christian choral communities (East Africa). By pulling back the layers of meaning that frequently veil congregational singing, I suggest that a focus on the routinely reiterated sounds produced by kwayas (KiSwahili for “choir”), that participate within that greater congregational space leads to a normalization of the performance of a localized gendering process—the sounding of sopranos, for example—that I label “sonic gendering.” This proposal confirms Judith Butler’s admonition that it is through rearticulation and repetition, such as when a kwaya continually affirms sonic gendering daily, that constitutive gender norms are reworked within a given cultural context (2011[1993], ix). I suggest that everyday singing in a kwaya facilitates the re-performing, re-consumption, and continuous re-embodiment of a process of gendering."
AKIHIRO KANAMORI,Cantor and continuity,"Georg Cantor (1845-1919), with his seminal work on sets and number, brought forth a new field of inquiry, set theory, and ushered in a way of proceeding in mathematics, one at base infinitary, topological, and combinatorial. While this was the thrust, his work at the beginning was embedded in issues and concerns of real analysis and contributed fundamentally to its 19th Century rigorization, a development turning on limits and continuity. And a continuing engagement with limits and continuity would be very much part of Cantor's mathematical journey, even as dramatically new conceptualizations emerged. Evolutionary accounts of Cantor's work mostly underscore his progressive ascent through settheoretic constructs to transfinite number, this as the storied beginnings of set theory. In this article, we consider Cantor's work with a steady focus on continuity, putting it first into the context of rigorization and then pursuing the increasingly set-theoretic constructs leading to its further elucidations."
AKIHIRO KANAMORI,Kriesel and Wittgenstein,"Georg Kreisel (15 September 1923 - 1 March 2015) was a formidable mathematical logician during a formative period when the subject was becoming a sophisticated field at the crossing of mathematics and logic. Both with his technical sophistication for his time and his dialectical engagement with mandates, aspirations and goals, he inspired wide-ranging investigation in the metamathematics of constructivity, proof theory and generalized recursion theory. Kreisel's mathematics and interactions with colleagues and students have been memorably described in Kreiseliana ([Odifreddi, 1996]). At a different level of interpersonal conceptual interaction, Kreisel during his life time had extended engagement with two celebrated logicians, the mathematical Kurt Gödel and the philosophical Ludwig Wittgenstein. About Gödel, with modern mathematical logic palpably emanating from his work, Kreisel has reflected and written over a wide mathematical landscape. About Wittgenstein on the other hand, with an early personal connection established Kreisel would return as if with an anxiety of influence to their ways of thinking about logic and mathematics, ever in a sort of dialectic interplay. In what follows we draw this out through his published essays—and one letter—both to elicit aspects of influence in his own terms and to set out a picture of Kreisel's evolving thinking about logic and mathematics in comparative relief."
AKIHIRO KANAMORI,Laver and set theory,"In this commemorative article, the work of Richard Laver is surveyed in its full range and extent."
AKIHIRO KANAMORI,"Bostonia: 1993-1994, no. 2-3",
GAEL I ORSMOND,Parents' future visions for their autistic transition-age youth: hopes and expectations,"Researchers have documented that young adults with autism spectrum disorder have poor outcomes in employment, post-secondary education, social participation, independent living, and community participation. There is a need to further explore contributing factors to such outcomes to better support successful transitions to adulthood. Parents play a critical role in transition planning, and parental expectations appear to impact young adult outcomes for autistic individuals. The aim of this study was to explore how parents express their future visions (i.e. hopes and expectations) for their autistic transition-age youth. Data were collected through focus groups and individual interviews with 18 parents. Parents' hopes and expectations focused on eight primary domains. In addition, parents often qualified or tempered their stated hope with expressions of fears, uncertainty, realistic expectations, and the perceived lack of guidance. We discuss our conceptualization of the relations among these themes and implications for service providers and research."
GAEL I ORSMOND,Adult siblings who have a brother or sister with autism: between-family and within-family variations in sibling relationships,"Prior research on the sibling relationship in the context of autism spectrum disorder (ASD) has included only one sibling per family. We used multi-level modeling to examine aspects of the sibling relationship in 207 adults who have a brother or sister with ASD from 125 families, investigating variability in sibling relationship quality and pessimism within and between families. We found that there was greater variability in aspects of the sibling relationship with the brother or sister with ASD within families than between families. Sibling individual-level factors were associated with positive affect in the sibling relationship, while family-level factors were associated with the sibling’s pessimism about their brother or sister’s future. The findings illustrate the unique experiences of siblings within families."
GAEL I ORSMOND,Social participation of families with children with autism spectrum disorder in a science museum,"This article describes a qualitative research study undertaken as a collaboration between museum and occupational therapy (OT) researchers to better understand museum experiences for families with a child or children impacted by autism spectrum disorder (ASD). Inclusion for visitors with ASD is an issue that museums are increasingly considering, and the social dimension of inclusion can be particularly relevant for this audience. The construct of social participation, used in OT, provides a promising avenue for museum professionals to think about inclusion. Social participation situates social and community experiences within the context of peoples’ diverse motivations and the strategies they use to navigate environments. This study took these multiple factors into account when observing families’ museum visits—including analysis of their motivations for visiting, environmental features that influenced their visit, family strategies used before and during the visit, and the families’ definitions of a successful visit. Learning more about these factors that are associated with social participation can inform future efforts to improve museum inclusion for families with children with ASD."
THOMAS GILMORE,Disulfide-mediated stabilization of the IκB kinase binding domain of NF-κB essential modulator (NEMO),"Human NEMO (NF-κB essential modulator) is a 419 residue scaffolding protein that, together with catalytic subunits IKKα and IKKβ, forms the IκB kinase (IKK) complex, a key regulator of NF-κB pathway signaling. NEMO is an elongated homodimer comprising mostly α-helix. It has been shown that a NEMO fragment spanning residues 44-111, which contains the IKKα/β binding site, is structurally disordered in the absence of bound IKKβ. Herein we show that enforcing dimerization of NEMO1-120 or NEMO44-111 constructs through introduction of one or two interchain disulfide bonds, through oxidation of the native Cys54 residue and/or at position 107 through a Leu107Cys mutation, induces a stable α-helical coiled-coil structure that is preorganized to bind IKKβ with high affinity. Chemical and thermal denaturation studies showed that, in the context of a covalent dimer, the ordered structure was stabilized relative to the denatured state by up to 3 kcal/mol. A full-length NEMO-L107C protein formed covalent dimers upon treatment of mammalian cells with H2O2. Furthermore, NEMO-L107C bound endogenous IKKβ in A293T cells, reconstituted TNF-induced NF-κB signaling in NEMO-deficient cells, and interacted with TRAF6. Our results indicate that the IKKβ binding domain of NEMO possesses an ordered structure in the unbound state, provided that it is constrained within a dimer as is the case in the constitutively dimeric full-length NEMO protein. The stability of the NEMO coiled coil is maintained by strong interhelix interactions in the region centered on residue 54. The disulfide-linked constructs we describe herein may be useful for crystallization of NEMO's IKKβ binding domain in the absence of bound IKKβ, thereby facilitating the structural characterization of small-molecule inhibitors."
THOMAS GILMORE,"Sea anemone model has a single Toll-like receptor that can function in pathogen detection, NF-κB signal transduction, and development","In organisms from insects to vertebrates, Toll-like receptors (TLRs) are primary pathogen detectors that activate downstream pathways, specifically those that direct expression of innate immune effector genes. TLRs also have roles in development in many species. The sea anemone Nematostella vectensis is a useful cnidarian model to study the origins of TLR signaling because its genome encodes a single TLR and homologs of many downstream signaling components, including the NF-κB pathway. We have characterized the single N. vectensis TLR (Nv-TLR) and demonstrated that it can activate canonical NF-κB signaling in human cells. Furthermore, we show that the intracellular Toll/IL-1 receptor (TIR) domain of Nv-TLR can interact with the human TLR adapter proteins MAL and MYD88. We demonstrate that the coral pathogen Vibrio coralliilyticus causes a rapidly lethal disease in N. vectensis and that heat-inactivated V. coralliilyticus and bacterial flagellin can activate a reconstituted Nv-TLR–to–NF-κB pathway in human cells. By immunostaining of anemones, we show that Nv-TLR is expressed in a subset of cnidocytes and that many of these Nv-TLR–expressing cells also express Nv-NF-κB. Additionally, the nematosome, which is a Nematostella-specific multicellular structure, expresses Nv-TLR and many innate immune pathway homologs and can engulf V. coralliilyticus. Morpholino knockdown indicates that Nv-TLR also has an essential role during early embryonic development. Our characterization of this primitive TLR and identification of a bacterial pathogen for N. vectensis reveal ancient TLR functions and provide a model for studying the molecular basis of cnidarian disease and immunity."
THOMAS GILMORE,Transcription factor NF-κB is modulated by symbiotic status in a sea anemone model of cnidarian bleaching.,"Transcription factor NF-κB plays a central role in immunity from fruit flies to humans, and NF-κB activity is altered in many human diseases. To investigate a role for NF-κB in immunity and disease on a broader evolutionary scale we have characterized NF-κB in a sea anemone (Exaiptasia pallida; called Aiptasia herein) model for cnidarian symbiosis and dysbiosis (i.e., ""bleaching""). We show that the DNA-binding site specificity of Aiptasia NF-κB is similar to NF-κB proteins from a broad expanse of organisms. Analyses of NF-κB and IκB kinase proteins from Aiptasia suggest that non-canonical NF-κB processing is an evolutionarily ancient pathway, which can be reconstituted in human cells. In Aiptasia, NF-κB protein levels, DNA-binding activity, and tissue expression increase when loss of the algal symbiont Symbiodinium is induced by heat or chemical treatment. Kinetic analysis of NF-κB levels following loss of symbiosis show that NF-κB levels increase only after Symbiodinium is cleared. Moreover, introduction of Symbiodinium into naïve Aiptasia larvae results in a decrease in NF-κB expression. Our results suggest that Symbiodinium suppresses NF-κB in order to enable establishment of symbiosis in Aiptasia. These results are the first to demonstrate a link between changes in the conserved immune regulatory protein NF-κB and cnidarian symbiotic status."
THOMAS GILMORE,Inhibition of oncogenic transcription factor REL by the natural product derivative calafianin monomer 101 induces proliferation arrest and apoptosis in human B-lymphoma cell lines,"Increased activity of transcription factor NF-κB has been implicated in many B-cell lymphomas. We investigated effects of synthetic compound calafianin monomer (CM101) on biochemical and biological properties of NF-κB. In human 293 cells, CM101 selectively inhibited DNA binding by overexpressed NF-κB subunits REL (human c-Rel) and p65 as compared to NF-κB p50, and inhibition of REL and p65 DNA binding by CM101 required a conserved cysteine residue. CM101 also inhibited DNA binding by REL in human B-lymphoma cell lines, and the sensitivity of several B-lymphoma cell lines to CM101-induced proliferation arrest and apoptosis correlated with levels of cellular and nuclear REL. CM101 treatment induced both phosphorylation and decreased expression of anti-apoptotic protein Bcl-XL, a REL target gene product, in sensitive B-lymphoma cell lines. Ectopic expression of Bcl-XL protected SUDHL-2 B-lymphoma cells against CM101-induced apoptosis, and overexpression of a transforming mutant of REL decreased the sensitivity of BJAB B-lymphoma cells to CM101-induced apoptosis. Lipopolysaccharide-induced activation of NF-κB signaling upstream components occurred in RAW264.7 macrophages at CM101 concentrations that blocked NF-κB DNA binding. Direct inhibitors of REL may be useful for treating B-cell lymphomas in which REL is active, and may inhibit B-lymphoma cell growth at doses that do not affect some immune-related responses in normal cells."
THOMAS GILMORE,Scaffold proteins as dynamic integrators of biological processes,
THOMAS GILMORE,An innate ability: how do basal invertebrates manage their chronic exposure to microbes?,"Homologs of mammalian innate immune sensing and downstream pathway proteins have been discovered in a variety of basal invertebrates, including cnidarians and sponges, as well as some single-celled protists. Although the structures of these proteins vary among the basal organisms, many of the activities found in their mammalian counterparts are conserved. This is especially true for the Toll-like receptor (TLR) and cGAS-STING pathways that lead to downstream activation of transcription factor NF-κB. In this short perspective, we describe the evidence that TLR and cGAS-STING signaling to NF-κB is also involved in immunity in basal animals, as well as in the maintenance of microbial symbionts. Different from terrestrial animals, immunity in many marine invertebrates might have a constitutively active state (to protect against continual exposure to resident or waterborne microbes), as well as a hyperactive state that can be induced by pathogens at both transcriptional and posttranscriptional levels. Research on basal immunity may be important for (1) understanding different approaches that organisms take to sensing and protecting against microbes, as well as in maintaining microbial symbionts; (2) the identification of novel antimicrobial effector genes and processes; and (3) the molecular pathways that are being altered in basal marine invertebrates in the face of the effects of a changing environment."
THOMAS GILMORE,Two alleles of NF-kappaB in the sea anemone Nematostella vectensis are widely dispersed in nature and encode proteins with distinct activities,"BACKGROUND: NF-kappaB is an evolutionarily conserved transcription factor that controls the expression of genes involved in many key organismal processes, including innate immunity, development, and stress responses. NF-kappaB proteins contain a highly conserved DNA-binding/dimerization domain called the Rel homology domain. METHODS/PRINCIPAL FINDINGS: We characterized two NF-kappaB alleles in the sea anemone Nematostella vectensis that differ at nineteen single-nucleotide polymorphisms (SNPs). Ten of these SNPs result in amino acid substitutions, including six within the Rel homology domain. Both alleles are found in natural populations of Nematostella. The relative abundance of the two NF-kappaB alleles differs between populations, and departures from Hardy-Weinberg equilibrium within populations indicate that the locus may be under selection. The proteins encoded by the two Nv-NF-kappaB alleles have different molecular properties, in part due to a Cys/Ser polymorphism at residue 67, which resides within the DNA recognition loop. In nearly all previously characterized NF-kappaB proteins, the analogous residue is fixed for Cys, and conversion of human RHD proteins from Cys to Ser at this site has been shown to increase DNA-binding ability and increase resistance to inhibition by thiol-reactive compounds. However, the naturally-occurring Nematostella variant with Cys at position 67 binds DNA with a higher affinity than the Ser variant. On the other hand, the Ser variant activates transcription in reporter gene assays more effectively, and it is more resistant to inhibition by a thiol-reactive compound. Reciprocal Cys<->Ser mutations at residue 67 of the native Nv-NF-kappaB proteins affect DNA binding as in human NF-kappaB proteins, e.g., a Cys->Ser mutation increases DNA binding of the native Cys variant. CONCLUSIONS/SIGNIFICANCE: These results are the first demonstration of a naturally occurring and functionally significant polymorphism in NF-kappaB in any species. The functional differences between these alleles and their uneven distribution in the wild suggest that different genotypes could be favored in different environments, perhaps environments that vary in their levels of peroxides or thiol-reactive compounds."
THOMAS GILMORE,CRISPR/Cas9-based editing of a sensitive transcriptional regulatory element to achieve cell type-specific knockdown of the NEMO scaffold protein,"The use of alternative promoters for the cell type-specific expression of a given mRNA/protein is a common cell strategy. NEMO is a scaffold protein required for canonical NF-κB signaling. Transcription of the NEMO gene is primarily controlled by two promoters: one (promoter B) drives NEMO transcription in most cell types and the second (promoter D) is largely responsible for NEMO transcription in liver cells. Herein, we have used a CRISPR/Cas9-based approach to disrupt a core sequence element of promoter B, and this genetic editing essentially eliminates expression of NEMO mRNA and protein in 293T human kidney cells. By cell subcloning, we have isolated targeted 293T cell lines that express no detectable NEMO protein, have defined genomic alterations at promoter B, and do not support activation of canonical NF-κB signaling in response to treatment with tumor necrosis factor. Nevertheless, noncanonical NF-κB signaling is intact in these NEMO-deficient cells. Expression of ectopic wildtype NEMO, but not certain human NEMO disease mutants, in the edited cells restores downstream NF-κB signaling in response to tumor necrosis factor. Targeting of the promoter B element does not substantially reduce NEMO expression (from promoter D) in the human SNU423 liver cancer cell line. Thus, we have created a strategy for selectively eliminating cell typespecific expression from an alternative promoter and have generated 293T cell lines with a functional knockout of NEMO. The implications of these findings for further studies and for therapeutic approaches to target canonical NF-κB signaling are discussed."
THOMAS GILMORE,Intraspecific variation in oxidative stress tolerance in a model cnidarian: differences in peroxide sensitivity between and within populations of Nematostella vectensis,"Nematostella vectensis is a member of the phylum Cnidaria, a lineage that includes anemones, corals, hydras, and jellyfishes. This estuarine anemone is an excellent model system for investigating the evolution of stress tolerance because it is easy to collect in its natural habitat and to culture in the laboratory, and it has a sequenced genome. Additionally, there is evidence of local adaptation to environmental stress in different N. vectensis populations, and abundant protein-coding polymorphisms have been identified, including polymorphisms in proteins that are implicated in stress responses. N. vectensis can tolerate a wide range of environmental parameters, and has recently been shown to have substantial intraspecific variation in temperature preference. We investigated whether different clonal lines of anemones also exhibit differential tolerance to oxidative stress. N. vectensis populations are continually exposed to reactive oxygen species (ROS) generated during cellular metabolism and by other environmental factors. Fifteen clonal lines of N. vectensis collected from four different estuaries were exposed to hydrogen peroxide. Pronounced differences in survival and regeneration were apparent between clonal lines collected from Meadowlands, NJ, Baruch, SC, and Kingsport, NS, as well as among 12 clonal lines collected from a single Cape Cod marsh. To our knowledge, this is the first example of intraspecific variability in oxidative stress resistance in cnidarians or in any marine animal. As oxidative stress often accompanies heat stress in marine organisms, resistance to oxidative stress could strongly influence survival in warming oceans. For example, while elevated temperatures trigger bleaching in corals, oxidative stress is thought to be the proximal trigger of bleaching at the cellular level."
THOMAS GILMORE,CRISPR/Cas9-based editing of a sensitive transcriptional regulatory element to achieve cell type-specific knockdown of the NEMO scaffold protein,"The use of alternative promoters for the cell type-specific expression of a given mRNA/protein is a common cell strategy. NEMO is a scaffold protein required for canonical NF-κB signaling. Transcription of the NEMO gene is primarily controlled by two promoters: one (promoter B) drives NEMO transcription in most cell types and the second (promoter A) is largely responsible for NEMO transcription in liver cells. Herein, we have used a CRISPR/Cas9-based approach to disrupt a core sequence element of promoter B, and this genetic editing essentially eliminates expression of NEMO mRNA and protein in 293T human kidney cells. By cell subcloning, we have isolated targeted 293T cell lines that express no detectable NEMO protein, have defined genomic alterations at promoter B, and do not support canonical NF-κB signaling in response to treatment with tumor necrosis factor (TNF). Nevertheless, non-canonical NF-κB signaling is intact in these NEMO-deficient cells. Expression of ectopic NEMO in the edited cells restores downstream NF-κB signaling in response to TNF. Targeting of the promoter B element does not substantially reduce NEMO expression (from promoter A) in the human SNU-423 liver cancer cell line. We have also used homology directed repair (HDR) to fix the promoter B element in a 293T cell clone. Overall, we have created a strategy for selectively eliminating cell type-specific expression from an alternative promoter and have generated 293T cell lines with a functional knockout of NEMO. The implications of these findings for further studies and for therapeutic approaches to target canonical NF-κB signaling are discussed."
THOMAS GILMORE,Two Alleles of NF-κB in the Sea Anemone Nematostella vectensis Are Widely Dispersed in Nature and Encode Proteins with Distinct Activities,"BACKGROUND. NF-κB is an evolutionarily conserved transcription factor that controls the expression of genes involved in many key organismal processes, including innate immunity, development, and stress responses. NF-κB proteins contain a highly conserved DNA-binding/dimerization domain called the Rel homology domain. METHODS/PRINCIPAL FINDINGS. We characterized two NF-κB alleles in the sea anemone Nematostella vectensis that differ at nineteen single-nucleotide polymorphisms (SNPs). Ten of these SNPs result in amino acid substitutions, including six within the Rel homology domain. Both alleles are found in natural populations of Nematostella. The relative abundance of the two NF-κB alleles differs between populations, and departures from Hardy-Weinberg equilibrium within populations indicate that the locus may be under selection. The proteins encoded by the two Nv-NF-κB alleles have different molecular properties, in part due to a Cys/Ser polymorphism at residue 67, which resides within the DNA recognition loop. In nearly all previously characterized NF-κB proteins, the analogous residue is fixed for Cys, and conversion of human RHD proteins from Cys to Ser at this site has been shown to increase DNA-binding ability and increase resistance to inhibition by thiol-reactive compounds. However, the naturally-occurring Nematostella variant with Cys at position 67 binds DNA with a higher affinity than the Ser variant. On the other hand, the Ser variant activates transcription in reporter gene assays more effectively, and it is more resistant to inhibition by a thiol-reactive compound. Reciprocal Cys<->Ser mutations at residue 67 of the native Nv-NF-κB proteins affect DNA binding as in human NF-κB proteins, e.g., a Cys->Ser mutation increases DNA binding of the native Cys variant. CONCLUSIONS/SIGNIFICANCE. These results are the first demonstration of a naturally occurring and functionally significant polymorphism in NF-κB in any species. The functional differences between these alleles and their uneven distribution in the wild suggest that different genotypes could be favored in different environments, perhaps environments that vary in their levels of peroxides or thiol-reactive compounds."
THOMAS GILMORE,Varied effects of algal symbionts on transcription factor NF-κB in a sea anemone and a coral: possible roles in symbiosis and thermotolerance,"Many cnidarians, including the reef-building corals, undergo symbiotic mutualisms with photosynthetic dinoflagellate algae of the family Symbiodiniaceae. These partnerships are sensitive to temperature extremes, which cause symbiont loss and increased coral mortality. Previous studies have implicated host immunity and specifically immunity transcription factor NF-κB as having a role in the maintenance of the cnidarian-algal symbiosis. Here we have further investigated a possible role for NF-κB in establishment and loss of symbiosis in various strains of the anemone Exaiptasia (Aiptasia) and in the coral Pocillopora damicornis. Our results show that NF-κB expression is reduced in Aiptasia larvae and adults that host certain algae strains. Treatment of Aiptasia larvae with a known symbiosis-promoting cytokine, transforming growth factor β, also led to decreased NF-κB expression. We also show that aposymbiotic Aiptasia (with high NF-κB expression) have increased survival following infection with the pathogenic bacterium Serratia marcescens as compared to symbiotic Aiptasia (low NF-κB expression). Furthermore, a P. damicornis coral colony hosting Durusdinium spp. (formerly clade D) symbionts had higher basal NF-κB expression and decreased heat-induced bleaching as compared to two individuals hosting Cladocopium spp. (formerly clade C) symbionts. Lastly, genome-wide gene expression profiling and genomic promoter analysis identified putative NF-κB target genes that may be involved in thermal bleaching, symbiont maintenance, and/or immune protection in P. damicornis. Our results provide further support for the hypothesis that modulation of NF-κB and immunity plays a role in some, but perhaps not all, cnidarian-Symbiodiniaceae partnerships as well as in resistance to pathogens and bleaching."
THOMAS GILMORE,Starvation decreases immunity and immune regulatory factor NF-κB in the starlet sea anemone Nematostella vectensis,"Lack of proper nutrition (malnutrition) or the complete absence of all food (starvation) have important consequences on the physiology of all organisms. In many cases, nutritional status affects immunity, but, for the most part, the relationship between nutrition and immunity has been limited to studies in vertebrates and terrestrial invertebrates. Herein, we describe a positive correlation between nutrition and immunity in the sea anemone Nematostella vectensis. Gene expression profiling of adult fed and starved anemones showed downregulation of many genes involved in nutrient metabolism and cellular respiration, as well as immune-related genes, in starved animals. Starved adult anemones also had reduced protein levels and DNA-binding activity of immunity-related transcription factor NF-κB. Starved juvenile anemones had increased sensitivity to bacterial infection and also had lower NF-κB protein levels, as compared to fed controls. Weighted Gene Correlation Network Analysis (WGCNA) revealed significantly correlated gene networks that were inversely associated with starvation. Based on the WGCNA and a reporter gene assay, we identified TRAF3 as a likely NF-κB target gene in N. vectensis. Overall, these experiments demonstrate a correlation between nutrition and immunity in a basal marine metazoan, and the results have implications for the survival of marine organisms as they encounter changing environments."
PETER C WEBER,The long-baseline neutrino experiment: exploring fundamental symmetries of the universe,"The preponderance of matter over antimatter in the early Universe, the dynamics of the supernova bursts that produced the heavy elements necessary for life and whether protons eventually decay --- these mysteries at the forefront of particle physics and astrophysics are key to understanding the early evolution of our Universe, its current state and its eventual fate. The Long-Baseline Neutrino Experiment (LBNE) represents an extensively developed plan for a world-class experiment dedicated to addressing these questions. LBNE is conceived around three central components: (1) a new, high-intensity neutrino source generated from a megawatt-class proton accelerator at Fermi National Accelerator Laboratory, (2) a near neutrino detector just downstream of the source, and (3) a massive liquid argon time-projection chamber deployed as a far detector deep underground at the Sanford Underground Research Facility. This facility, located at the site of the former Homestake Mine in Lead, South Dakota, is approximately 1,300 km from the neutrino source at Fermilab -- a distance (baseline) that delivers optimal sensitivity to neutrino charge-parity symmetry violation and mass ordering effects. This ambitious yet cost-effective design incorporates scalability and flexibility and can accommodate a variety of upgrades and contributions. With its exceptional combination of experimental configuration, technical capabilities, and potential for transformative discoveries, LBNE promises to be a vital facility for the field of particle physics worldwide, providing physicists from around the globe with opportunities to collaborate in a twenty to thirty year program of exciting science. In this document we provide a comprehensive overview of LBNE's scientific objectives, its place in the landscape of neutrino physics worldwide, the technologies it will incorporate and the capabilities it will possess."
ANITA H PATTERSON,"The American legacy of ""Prufrock""","The essay cluster brings together leading Eliot and modernist scholars to commemorate the centenary of the publication of “The Love Song of J. Alfred Prufrock.” Together, they reexamine the circumstances surrounding the poem’s original publication, recontextualize its allusions, and reconstruct its reception over the past century. Patterson examines the American roots of Eliot’s ironic love song, often considered through the lenses of European poetry and philosophy. Dickey returns to the poem’s early reception to challenge the now established narrative that “Prufrock” shocked early readers, showing how often his contemporaries associated the poem with Decadent or Aesthetic precedents. Ricks returns to the poem’s first publication in Poetry Magazine to understand how the poem’s first readers would have encountered the text and how this context would have mediated the reader’s experience. Cuda situates Eliot’s poem vis-à-vis current discourses on late modernism and demonstrates how lateness and belatedness feature centrally in the poem. Finally, Schuchard examines Eliot’s literary and religious allusions, showing that his allusive method is in full force even in his first poetic masterpiece."
VICKERY E TRINKAUS-RANDALL,Tri-Nucleotide Receptors Play a Critical Role in Epithelial Cell Wound Repair,"The cornea plays a major role in the refraction of light to the retina. Therefore, the integrity and transparency of the corneal epithelium are critical to vision. Following injury, a combination of rapid signal transduction events and long-term cell migration are essential for wound closure. We have demonstrated previously that injury resulted in the release of nucleotides that induce the propagation of a Ca2+ wave to neighboring cells. This suggests that nucleotides and their receptors are critical components of wound healing. Epidermal growth factor (EGF) and integrins also have been shown to play a role in injury. In this study, we demonstrate that pretreatment of cells with ATP and UTP inhibited the immediate wound response, while BzATP, ADP, and UDP did not affect this response. Tri-nucleotide pretreatment also reduced the EGF induced Ca2+ response. Additionally, lower EC50 concentrations of ATP and UTP triggered migration of cells that was enhanced further with EGF and was inhibited by the tripeptide, RGD. Results indicate that the desensitization induced by ATP and UTP was specific. While ADP and UDP cause a homologous desensitization of their own signal, they did not cause an inhibition of the wound response nor does BzATP. Neither Ca2+ wave propagation nor cell migration occurred in response to β,γ-MeATP. Together these results lead us to hypothesize that corneal epithelial wound repair is mediated by both P2Y2 and P2Y4 receptors."
LUCIA M VAINA,Functional roles of 10 Hz alpha-band power modulating engagement and disengagement of cortical networks in a complex visual motion task,"Alpha band power, particularly at the 10 Hz frequency, is significantly involved in sensory inhibition, attention modulation, and working memory. However, the interactions between cortical areas and their relationship to the different functional roles of the alpha band oscillations are still poorly understood. Here we examined alpha band power and the cortico-cortical interregional phase synchrony in a psychophysical task involving the detection of an object moving in depth by an observer in forward self-motion. Wavelet filtering at the 10 Hz frequency revealed differences in the profile of cortical activation in the visual processing regions (occipital and parietal lobes) and in the frontoparietal regions. The alpha rhythm driving the visual processing areas was found to be asynchronous with the frontoparietal regions. These findings suggest a decoupling of the 10 Hz frequency into separate functional roles: sensory inhibition in the visual processing regions and spatial attention in the frontoparietal regions."
LUCIA M VAINA,Spared ability to perceive direction of locomotor heading and scene-relative object movement despite inability to perceive relative motion,"BACKGROUND: All contemporary models of perception of locomotor heading from optic flow (the characteristic patterns of retinal motion that result from self-movement) begin with relative motion. Therefore it would be expected that an impairment on perception of relative motion should impact on the ability to judge heading and other 3D motion tasks. MATERIAL AND METHODS: We report two patients with occipital lobe lesions whom we tested on a battery of motion tasks. Patients were impaired on all tests that involved relative motion in plane (motion discontinuity, form from differences in motion direction or speed). Despite this they retained the ability to judge their direction of heading relative to a target. A potential confound is that observers can derive information about heading from scale changes bypassing the need to use optic flow. Therefore we ran further experiments in which we isolated optic flow and scale change. RESULTS: Patients' performance was in normal ranges on both tests. The finding that ability to perceive heading can be retained despite an impairment on ability to judge relative motion questions the assumption that heading perception proceeds from initial processing of relative motion. Furthermore, on a collision detection task, SS and SR's performance was significantly better for simulated forward movement of the observer in the 3D scene, than for the static observer. This suggests that in spite of severe deficits on relative motion in the frontoparlel (xy) plane, information from self-motion helped identification objects moving along an intercept 3D relative motion trajectory. CONCLUSIONS: This result suggests a potential use of a flow parsing strategy to detect in a 3D world the trajectory of moving objects when the observer is moving forward. These results have implications for developing rehabilitation strategies for deficits in visually guided navigation."
LUCIA M VAINA,Improving the nulling beamformer using subspace suppression,"Magnetoencephalography (MEG) captures the magnetic fields generated by neuronal current sources with sensors outside the head. In MEG analysis these current sources are estimated from the measured data to identify the locations and time courses of neural activity. Since there is no unique solution to this so-called inverse problem, multiple source estimation techniques have been developed. The nulling beamformer (NB), a modified form of the linearly constrained minimum variance (LCMV) beamformer, is specifically used in the process of inferring interregional interactions and is designed to eliminate shared signal contributions, or cross-talk, between regions of interest (ROIs) that would otherwise interfere with the connectivity analyses. The nulling beamformer applies the truncated singular value decomposition (TSVD) to remove small signal contributions from a ROI to the sensor signals. However, ROIs with strong crosstalk will have high separating power in the weaker components, which may be removed by the TSVD operation. To address this issue we propose a new method, the nulling beamformer with subspace suppression (NBSS). This method, controlled by a tuning parameter, reweights the singular values of the gain matrix mapping from source to sensor space such that components with high overlap are reduced. By doing so, we are able to measure signals between nearby source locations with limited cross-talk interference, allowing for reliable cortical connectivity analysis between them. In two simulations, we demonstrated that NBSS reduces cross-talk while retaining ROIs' signal power, and has higher separating power than both the minimum norm estimate (MNE) and the nulling beamformer without subspace suppression. We also showed that NBSS successfully localized the auditory M100 event-related field in primary auditory cortex, measured from a subject undergoing an auditory localizer task, and suppressed cross-talk in a nearby region in the superior temporal sulcus."
LUCIA M VAINA,Select and cluster: a method for finding functional networks of clustered voxels in fMRI,"Extracting functional connectivity patterns among cortical regions in fMRI datasets is a challenge stimulating the development of effective data-driven or model based techniques. Here, we present a novel data-driven method for the extraction of significantly connected functional ROIs directly from the preprocessed fMRI data without relying on a priori knowledge of the expected activations. This method finds spatially compact groups of voxels which show a homogeneous pattern of significant connectivity with other regions in the brain. The method, called Select and Cluster (S&C), consists of two steps: first, a dimensionality reduction step based on a blind multiresolution pairwise correlation by which the subset of all cortical voxels with significant mutual correlation is selected and the second step in which the selected voxels are grouped into spatially compact and functionally homogeneous ROIs by means of a Support Vector Clustering (SVC) algorithm. The S&C method is described in detail. Its performance assessed on simulated and experimental fMRI data is compared to other methods commonly used in functional connectivity analyses, such as Independent Component Analysis (ICA) or clustering. S&C method simplifies the extraction of functional networks in fMRI by identifying automatically spatially compact groups of voxels (ROIs) involved in whole brain scale activation networks."
LUCIA M VAINA,Partial volume correction for PET quantification and its impact on brain network in Alzheimer's disease,"Amyloid positron emission tomography (PET) imaging is a valuable tool for research and diagnosis in Alzheimer’s disease (AD). Partial volume effects caused by the limited spatial resolution of PET scanners degrades the quantitative accuracy of PET image. In this study, we have applied a method to evaluate the impact of a joint-entropy based partial volume correction (PVC) technique on brain networks learned from a clinical dataset of AV-45 PET image and compare network properties of both uncorrected and corrected image-based brain networks. We also analyzed the region-wise SUVRs of both uncorrected and corrected images. We further performed classification tests on different groups using the same set of algorithms with same parameter settings. PVC has sometimes been avoided due to increased noise sensitivity in image registration and segmentation, however, our results indicate that appropriate PVC may enhance the brain network structure analysis for AD progression and improve classification performance."
LUCIA M VAINA,Functional and anatomical profile of visual motion impairments in stroke patients correlate with fMRI in normal subjects,"We used six psychophysical tasks to measure sensitivity to different types of global motion in 45 healthy adults and in 57 stroke patients who had recovered from the initial results of the stroke, but a large subset of them had enduring deficits on selective visual motion perception tasks. The patients were divided into four groups on the basis of the location of their cortical lesion: occipito‐temporal, occipito‐parietal, rostro‐dorsal parietal, or frontal‐prefrontal. The six tasks were: direction discrimination, speed discrimination, motion coherence, motion discontinuity, two‐dimensional form‐from‐motion, and motion coherence – radial. We found both qualitative and quantitative differences among the motion impairments in the four groups: patients with frontal lesions or occipito‐temporal lesions were not impaired on any task. The other two groups had substantial impairments, most severe in the group with occipito‐parietal damage. We also tested eight healthy control subjects on the same tasks while they were scanned by functional magnetic resonance imaging. The BOLD signal provoked by the different tasks correlated well with the locus of the lesions that led to impairments among the different tasks. The results highlight the advantage of using psychophysical techniques and a variety of visual tasks with neurological patients to tease apart the contribution of different cortical areas to motion processing."
LUCIA M VAINA,Motion sequence analysis in the presence of figural cues,"The perception of 3-D structure in dynamic sequences is believed to be subserved primarily through the use of motion cues. However, real-world sequences contain many figural shape cues besides the dynamic ones. We hypothesize that if figural cues are perceptually significant during sequence analysis, then inconsistencies in these cues over time would lead to percepts of non-rigidity in sequences showing physically rigid objects in motion. We develop an experimental paradigm to test this hypothesis and present results with two patients with impairments in motion perception due to focal neurological damage, as well as two control subjects. Consistent with our hypothesis, the data suggest that figural cues strongly influence the perception of structure in motion sequences, even to the extent of inducing non-rigid percepts in sequences where motion information alone would yield rigid structures. Beyond helping to probe the issue of shape perception, our experimental paradigm might also serve as a possible perceptual assessment tool in a clinical setting."
LUCIA M VAINA,Dissociation of first- and second-order motion systems by perceptual learning,"Previous studies investigating transfer of perceptual learning between luminance-defined (LD) motion and texture-contrast-defined (CD) motion tasks have found little or no transfer from LD to CD motion tasks but nearly perfect transfer from CD to LD motion tasks. Here, we introduce a paradigm that yields a clean double dissociation: LD training yields no transfer to the CD task, but more interestingly, CD training yields no transfer to the LD task. Participants were trained in two variants of a global motion task. In one (LD) variant, motion was defined by tokens that differed from the background in mean luminance. In the other (CD) variant, motion was defined by tokens that had mean luminance equal to the background but differed from the background in texture contrast. The task was to judge whether the signal tokens were moving to the right or to the left. Task difficulty was varied by manipulating the proportion of tokens that moved coherently across the four frames of the stimulus display. Performance in each of the LD and CD variants of the task was measured as training proceeded. In each task, training produced substantial improvement in performance in the trained task; however, in neither case did this improvement show any significant transfer to the nontrained task."
LUCIA M VAINA,A method for selecting an efficient diagnostic protocol for classification of perceptive and cognitive impairments in neurological patients,"An important and unresolved problem in the assessment of perceptual and cognitive deficits in neurological patients is how to choose from the many existing behavioral tests, a subset that is sufficient for an appropriate diagnosis. This problem has to be dealt with in clinical trials, as well as in rehabilitation settings and often even at bedside in acute care hospitals. The need for efficient, cost effective and accurate diagnostic-evaluations, in the context of clinician time constraints and concerns for patients’ fatigue in long testing sessions, make it imperative to select a set of tests that will provide the best classification of the patient’s deficits. However, the small sample size of the patient population complicates the selection methodology and the potential accuracy of the classifier. We propose a method that allows for ordering tests based on having progressive increases in classification using cross-validation to assess the classification power of the chosen test set. This method applies forward linear regression to find an ordering of the tests with leave-one-out cross-validation to quantify, without biasing to the training set, the classification power of the chosen tests."
LUCIA M VAINA,A computerized perimeter for assessing modality-specific visual field loss,"The characterization of visual field loss provides a valuable diagnostic metric for studying the effects of damage to the retina, optic nerve or visual cortex. We describe a tool, the Quadrant Vision Perimeter (QVp), to rapidly and accurately measure visual fields. In addition to measuring the location of visual deficits, the tool can assess modality-specific field loss (e.g., impaired detection of luminance, motion, depth and color) and severity of the deficit. We present validation and normalization for parameters of visual attributes, as well as exemplar comparisons of visual fields obtained automatically using QVp to standardized perimeters for three stroke patients. Patient visual fields are compared among visual features to assess modality-specific deficits, and over time, to measure fine changes in visual fields, due either to spontaneous recovery or visual degradation."
LUCIA M VAINA,Reorganization of retinotopic maps after occipital lobe infarction,"We studied patient JS, who had a right occipital infarct that encroached on visual areas V1, V2v, and VP. When tested psychophysically, he was very impaired at detecting the direction of motion in random dot displays where a variable proportion of dots moving in one direction (signal) were embedded in masking motion noise (noise dots). The impairment on this motion coherence task was especially marked when the display was presented to the upper left (affected) visual quadrant, contralateral to his lesion. However, with extensive training, by 11 months his threshold fell to the level of healthy participants. Training on the motion coherence task generalized to another motion task, the motion discontinuity task, on which he had to detect the presence of an edge that was defined by the difference in the direction of the coherently moving dots (signal) within the display. He was much better at this task at 8 than 3 months, and this improvement was associated with an increase in the activation of the human MT complex (hMT^+) and in the kinetic occipital region as shown by repeated fMRI scans. We also used fMRI to perform retinotopic mapping at 3, 8, and 11 months after the infarct. We quantified the retinotopy and areal shifts by measuring the distances between the center of mass of functionally defined areas, computed in spherical surface-based coordinates. The functionally defined retinotopic areas V1, V2v, V2d, and VP were initially smaller in the lesioned right hemisphere, but they increased in size between 3 and 11 months. This change was not found in the normal, left hemisphere of the patient or in either hemispheres of the healthy control participants. We were interested in whether practice on the motion coherence task promoted the changes in the retinotopic maps. We compared the results for patient JS with those from another patient (PF) who had a comparable lesion but had not been given such practice. We found similar changes in the maps in the lesioned hemisphere of PF. However, PF was only scanned at 3 and 7 months, and the biggest shifts in patient JS were found between 8 and 11 months. Thus, it is important to carry out a prospective study with a trained and untrained group so as to determine whether the patterns of reorganization that we have observed can be further promoted by training."
LUCIA M VAINA,Different motion cues are used to estimate time-to-arrival for frontoparallel and looming trajectories,"Estimation of time-to-arrival for moving objects is critical to obstacle interception and avoidance, as well as to timing actions such as reaching and grasping moving objects. The source of motion information that conveys arrival time varies with the trajectory of the object raising the question of whether multiple context-dependent mechanisms are involved in this computation. To address this question we conducted a series of psychophysical studies to measure observers’ performance on time-to-arrival estimation when object trajectory was specified by angular motion (“gap closure” trajectories in the frontoparallel plane), looming (colliding trajectories, TTC) or both (passage courses, TTP). We measured performance of time-to-arrival judgments in the presence of irrelevant motion, in which a perpendicular motion vector was added to the object trajectory. Data were compared to models of expected performance based on the use of different components of optical information. Our results demonstrate that for gap closure, performance depended only on the angular motion, whereas for TTC and TTP, both angular and looming motion affected performance. This dissociation of inputs suggests that gap closures are mediated by a separate mechanism than that used for the detection of time-to-collision and time-to-passage. We show that existing models of TTC and TTP estimation make systematic errors in predicting subject performance, and suggest that a model which weights motion cues by their relative time-to-arrival provides a better account of performance."
LUCIA M VAINA,Global flow impacts time-to-passage judgments based on local motion cues,"We assessed the effect of the coherence of optic flow on time-to-passage judgments in order to investigate the strategies that observers use when local expansion information is reduced or lacking. In the standard display, we presented a cloud of dots whose image expanded consistent with constant observer motion. The dots themselves, however, did not expand and were thus devoid of object expansion cues. Only the separations between the dots expanded. Subjects had to judge which of two colored target dots, presented at different simulated depths and lateral displacements would pass them first. Image velocities of the target dots were chosen so as to correlate with time-to-passage only some of the time. When optic flow was mainly incoherent, subjects’ responses were biased and relied on image velocities rather than on global flow analysis. However, the bias induced by misleading image velocity cues diminished as a function of the coherence of the optic flow. We discuss the results in the context of a global tau mechanism and settle a debate whether local expansion cues or optic flow analysis are the basis for time-to-passage estimation"
LUCIA M VAINA,Two mechanisms for optic flow and scale change processing of looming,"The detection of looming, the motion of objects in depth, underlies many behavioral tasks, including the perception of self-motion and time-to-collision. A number of studies have demonstrated that one of the most important cues for looming detection is optic flow, the pattern of motion across the retina. Schrater et al. have suggested that changes in spatial frequency over time, or scale changes, may also support looming detection in the absence of optic flow (P. R. Schrater, D. C. Knill, & E. P. Simoncelli, 2001). Here we used an adaptation paradigm to determine whether the perception of looming from optic flow and scale changes is mediated by single or separate mechanisms. We show first that when the adaptation and test stimuli were the same (both optic flow or both scale change), observer performance was significantly impaired compared to a dynamic (non-motion, non-scale change) null adaptation control. Second, we found no evidence of cross-cue adaptation, either from optic flow to scale change, or vice versa. Taken together, our data suggest that optic flow and scale changes are processed by separate mechanisms, providing multiple pathways for the detection of looming."
LUCIA M VAINA,Neuropsychological evidence for three distinct motion mechanisms,"We describe psychophysical performance of two stroke patients with lesions in distinct cortical regions in the left hemisphere. Both patients were selectively impaired on direction discrimination in several local and global second-order but not first-order motion tasks. However, only patient FD was impaired on a specific bi-stable motion task where the direction of motion is biased by object similarity. We suggest that this bi-stable motion task may be mediated by a high-level attention or position based mechanism indicating a separate neurological substrate for a high-level attention or position-based mechanism. Therefore, these results provide evidence for the existence of at least three motion mechanisms in the human visual system: a low-level first- and second-order motion mechanism and a high-level attention or position-based mechanism."
LUCIA M VAINA,Differential cortical activation during the perception of moving objects along different trajectories,"Detection of 3D object motion trajectories depends on the integration of two distinct visual cues: translational displacement and looming. Electrophysiological studies have identified distinct neuronal populations whose activity depends on the precise motion cues present in the stimulus. This distinction, however, has been less clear in humans, and it is confounded by differences in the behavioral task being performed. We analyzed whole-brain fMRI while subjects performed a common time-to-arrival task for objects moving along three trajectories: moving directly towards the observer (collision course), with trajectories parallel to the line of sight (passage course), and with trajectories perpendicular to the line of sight (gap closure). We found that there was substantial overlap in the pattern of activation associated with each of the three tasks, with differences among conditions limited to the human motion area (hMT+), which showed greater activation extent in the gap closure condition than for either collision or passage courses. These results support a common substrate for temporal judgments of an object’s time-to-arrival, wherein the special cases of object motion directly toward, or perpendicular to, the observer represent two extremes within the broader continuum of 3D passage trajectories relative to the observer."
LUCIA M VAINA,Visual attributes of subliminal priming images impact conscious perception of facial expressions,"We investigated, in young healthy participants, how the affective content of subliminally presented priming images and their specific visual attributes impacted conscious perception of facial expressions. The priming images were broadly categorized as aggressive, pleasant, or neutral and further subcategorized by the presence of a face and by the centricity (egocentric or allocentric vantage-point) of the image content. Participants responded to the emotion portrayed in a pixelated target-face by indicating via key-press if the expression was angry or neutral. Response time to the neutral target face was significantly slower when preceded by face primes, compared to non-face primes (p < 0.05, Bonferroni corrected). In contrast, faster RTs were observed when angry target faces were preceded by face compared to non-face primes. In addition, participants’ performance was worse when a priming image contained an egocentric face compared to when it contained either an allocentric face or an egocentric non-face. The results suggest a significant impact of the visual features of the priming image on conscious perception of face expression."
LUCIA M VAINA,Peripheral visual localization is degraded by globally incongruent auditory-spatial attention cues,"Global auditory-spatial orienting cues help the detection of weak visual stimuli, but it is not clear whether crossmodal attention cues also enhance the resolution of visuospatial discrimination. Here, we hypothesized that if anywhere, crossmodal modulations of visual localization should emerge in the periphery where the receptive fields are large. Subjects were presented with trials where a Visual Target, defined by a cluster of low-luminance dots, was shown for 220 ms at 25°–35° eccentricity in either the left or right hemifield. The Visual Target was either Uncued or it was presented 250 ms after a crossmodal Auditory Cue that was simulated either from the same or the opposite hemifield than the Visual Target location. After a whole-screen visual mask displayed for 800 ms, a pair of vertical Reference Bars was presented ipsilateral to the Visual Target. In a two-alternative forced choice task, subjects were asked to determine which of these two bars was closer to the center of the Visual Target. When the Auditory Cue and Visual Target were hemispatially incongruent, the speed and accuracy of visual localization performance was significantly impaired. However, hemispatially congruent Auditory Cues did not improve the localization of Visual Targets when compared to the Uncued condition. Further analyses suggested that the crossmodal Auditory Cues decreased the sensitivity (d′) of the Visual Target localization without affecting post-perceptual decision biases. Our results suggest that in the visual periphery, the detrimental effect of hemispatially incongruent Auditory Cues is far greater than the benefit produced by hemispatially congruent cues. Our working hypothesis for future studies is that auditory-spatial attention cues suppress irrelevant visual locations in a global fashion, without modulating the local visual precision at relevant sites."
LUCIA M VAINA,Acoustic facilitation of object movement detection during self-motion,"In humans, as well as most animal species, perception of object motion is critical to successful interaction with the surrounding environment. Yet, as the observer also moves, the retinal projections of the various motion components add to each other and extracting accurate object motion becomes computationally challenging. Recent psychophysical studies have demonstrated that observers use a flow-parsing mechanism to estimate and subtract self-motion from the optic flow field. We investigated whether concurrent acoustic cues for motion can facilitate visual flow parsing, thereby enhancing the detection of moving objects during simulated self-motion. Participants identified an object (the target) that moved either forward or backward within a visual scene containing nine identical textured objects simulating forward observer translation. We found that spatially co-localized, directionally congruent, moving auditory stimuli enhanced object motion detection. Interestingly, subjects who performed poorly on the visual-only task benefited more from the addition of moving auditory stimuli. When auditory stimuli were not co-localized to the visual target, improvements in detection rates were weak. Taken together, these results suggest that parsing object motion from self-motion-induced optic flow can operate on multisensory object representations."
LUCIA M VAINA,Functional neuroanatomy of time-to-passage perception,"The time until an approaching object passes the observer is referred to as time-to-passage (TTP). Accurate judgment of TTP is critical for visually guided navigation, such as when walking, riding a bicycle, or driving a car. Previous research has shown that observers are able to make TTP judgments in the absence of information about local retinal object expansion. In this paper we combine psychophysics and functional MRI (fMRI) to investigate the neural substrate of TTP processing. In a previous psychophysical study, we demonstrated that when local retinal expansion cues are not available, observers take advantage of multiple sources of information to judge TTP, such as optic flow and object retinal velocities, and integrate these cues through a flexible and economic strategy. To induce strategy changes, we introduced trials with motion but without coherent optic flow (0% coherence of the background), and trials with coherent, but noisy, optic flow (75% coherence of the background). In a functional magnetic resonance imaging (fMRI) study we found that coherent optic flow cues resulted in better behavioral performance as well as higher and broader cortical activations across the visual motion processing pathway. Blood oxygen-level-dependent (BOLD) signal changes showed significant involvement of optic flow processing in the precentral sulcus (PreCS), postcentral sulcus (PostCS) and middle temporal gyrus (MTG) across all conditions. Not only highly activated during motion processing, bilateral hMT areas also showed a complex pattern in TTP judgment processing, which reflected a flexible TTP response strategy."
LUCIA M VAINA,A fast statistical significance test for baseline correction and comparative analysis in phase locking.,"Human perception, cognition, and action are supported by a complex network of interconnected brain regions. There is an increasing interest in measuring and characterizing these networks as a function of time and frequency, and inter-areal phase locking is often used to reveal these networks. This measure assesses the consistency of phase angles between the electrophysiological activity in two areas at a specific time and frequency. Non-invasively, the signals from which phase locking is computed can be measured with magnetoencephalography (MEG) and electroencephalography (EEG). However, due to the lack of spatial specificity of reconstructed source signals in MEG and EEG, inter-areal phase locking may be confounded by false positives resulting from crosstalk. Traditional phase locking estimates assume that no phase locking exists when the distribution of phase angles is uniform. However, this conjecture is not true when crosstalk is present. We propose a novel method to improve the reliability of the phase-locking measure by sampling phase angles from a baseline, such as from a prestimulus period or from resting-state data, and by contrasting this distribution against one observed during the time period of interest."
LUCIA M VAINA,The emotional valence of subliminal priming effects perception of facial expressions,"We investigated, in young healthy subjects, how the affective content of subliminally presented priming images and their specific visual attributes impacted conscious perception of facial expressions. The priming images were broadly categorised as aggressive, pleasant, or neutral and further subcategorised by the presence of a face and by the centricity (egocentric or allocentric vantage-point) of the image content. Subjects responded to the emotion portrayed in a pixelated target-face by indicating via key-press if the expression was angry or neutral. Priming images containing a face compared to those not containing a face significantly impaired performance on neutral or angry targetface evaluation. Recognition of angry target-face expressions was selectively impaired by pleasant prime images which contained a face. For egocentric primes, recognition of neutral target-face expressions was significantly better than of angry expressions. Our results suggest that, first, the affective primacy hypothesis which predicts that affective information can be accessed automatically, preceding conscious cognition, holds true in subliminal priming only when the priming image contains a face. Second, egocentric primes interfere with the perception of angry target-face expressions suggesting that this vantage-point, directly relevant to the viewer, perhaps engages processes involved in action preparation which may weaken the priority of affect processing."
LUCIA M VAINA,Maturation trajectories of cortical resting-state networks depend on the mediating frequency band,"The functional significance of resting state networks and their abnormal manifestations in psychiatric disorders are firmly established, as is the importance of the cortical rhythms in mediating these networks. Resting state networks are known to undergo substantial reorganization from childhood to adulthood, but whether distinct cortical rhythms, which are generated by separable neural mechanisms and are often manifested abnormally in psychiatric conditions, mediate maturation differentially, remains unknown. Using magnetoencephalography (MEG) to map frequency band specific maturation of resting state networks from age 7 to 29 in 162 participants (31 independent), we found significant changes with age in networks mediated by the beta (13–30 Hz) and gamma (31–80 Hz) bands. More specifically, gamma band mediated networks followed an expected asymptotic trajectory, but beta band mediated networks followed a linear trajectory. Network integration increased with age in gamma band mediated networks, while local segregation increased with age in beta band mediated networks. Spatially, the hubs that changed in importance with age in the beta band mediated networks had relatively little overlap with those that showed the greatest changes in the gamma band mediated networks. These findings are relevant for our understanding of the neural mechanisms of cortical maturation, in both typical and atypical development."
LUCIA M VAINA,Using action understanding to understand the left inferior parietal cortex in the human brain,"Humans have a sophisticated knowledge of the actions that can be performed with objects. In an fMRI study we tried to establish whether this depends on areas that are homologous with the inferior parietal cortex (area PFG) in macaque monkeys. Cells have been described in area PFG that discharge differentially depending upon whether the observer sees an object being brought to the mouth or put in a container. In our study the observers saw videos in which the use of different objects was demonstrated in pantomime; and after viewing the videos, the subject had to pick the object that was appropriate to the pantomime. We found a cluster of activated voxels in parietal areas PFop and PFt and this cluster was greater in the left hemisphere than in the right. We suggest a mechanism that could account for this asymmetry, relate our results to handedness and suggest that they shed light on the human syndrome of apraxia. Finally, we suggest that during the evolution of the hominids, this same pantomime mechanism could have been used to ‘name’ or request objects."
LUCIA M VAINA,Neural activity underlying the detection of an object movement by an observer during forward self-motion: Dynamic decoding and temporal evolution of directional cortical connectivity.,"Relatively little is known about how the human brain identifies movement of objects while the observer is also moving in the environment. This is, ecologically, one of the most fundamental motion processing problems, critical for survival. To study this problem, we used a task which involved nine textured spheres moving in depth, eight simulating the observer's forward motion while the ninth, the target, moved independently with a different speed towards or away from the observer. Capitalizing on the high temporal resolution of magnetoencephalography (MEG) we trained a Support Vector Classifier (SVC) using the sensor-level data to identify correct and incorrect responses. Using the same MEG data, we addressed the dynamics of cortical processes involved in the detection of the independently moving object and investigated whether we could obtain confirmatory evidence for the brain activity patterns used by the classifier. Our findings indicate that response correctness could be reliably predicted by the SVC, with the highest accuracy during the blank period after motion and preceding the response. The spatial distribution of the areas critical for the correct prediction was similar but not exclusive to areas underlying the evoked activity. Importantly, SVC identified frontal areas otherwise not detected with evoked activity that seem to be important for the successful performance in the task. Dynamic connectivity further supported the involvement of frontal and occipital-temporal areas during the task periods. This is the first study to dynamically map cortical areas using a fully data-driven approach in order to investigate the neural mechanisms involved in the detection of moving objects during observer's self-motion."
LUCIA M VAINA,Interaction of cortical networks mediating object motion detection by moving observers,"The task of parceling perceived visual motion into self- and object motion components is critical to safe and accurate visually guided navigation. In this paper, we used functional magnetic resonance imaging to determine the cortical areas functionally active in this task and the pattern connectivity among them to investigate the cortical regions of interest and networks that allow subjects to detect object motion separately from induced self-motion. Subjects were presented with nine textured objects during simulated forward self-motion and were asked to identify the target object, which had an additional, independent motion component toward or away from the observer. Cortical activation was distributed among occipital, intra-parietal and fronto-parietal areas. We performed a network analysis of connectivity data derived from partial correlation and multivariate Granger causality analyses among functionally active areas. This revealed four coarsely separated network clusters: bilateral V1 and V2; visually responsive occipito-temporal areas, including bilateral LO, V3A, KO (V3B) and hMT; bilateral VIP, DIPSM and right precuneus; and a cluster of higher, primarily left hemispheric regions, including the central sulcus, post-, pre- and sub-central sulci, pre-central gyrus, and FEF. We suggest that the visually responsive networks are involved in forming the representation of the visual stimulus, while the higher, left hemisphere cluster is involved in mediating the interpretation of the stimulus for action. Our main focus was on the relationships of activations during our task among the visually responsive areas. To determine the properties of the mechanism corresponding to the visual processing networks, we compared subjects’ psychophysical performance to a model of object motion detection based solely on relative motion among objects and found that it was inconsistent with observer performance. Our results support the use of scene context (e.g., eccentricity, depth) in the detection of object motion. We suggest that the cortical activation and visually responsive networks provide a potential substrate for this computation."
LUCIA M VAINA,Cross-modal cue effects in motion processing,"The everyday environment brings to our sensory systems competing inputs from different modalities. The ability to filter these multisensory inputs in order to identify and efficiently utilize useful spatial cues is necessary to detect and process the relevant information. In the present study, we investigate how feature-based attention affects the detection of motion across sensory modalities. We were interested to determine how subjects use intramodal, cross-modal auditory, and combined audiovisual motion cues to attend to specific visual motion signals. The results showed that in most cases, both the visual and the auditory cues enhance feature-based orienting to a transparent visual motion pattern presented among distractor motion patterns. Whereas previous studies have shown cross-modal effects of spatial attention, our results demonstrate a spread of cross-modal feature-based attention cues, which have been matched for the detection threshold of the visual target. These effects were very robust in comparisons of the effects of valid vs. invalid cues, as well as in comparisons between cued and uncued valid trials. The effect of intramodal visual, cross-modal auditory, and bimodal cues also increased as a function of motion-cue salience. Our results suggest that orienting to visual motion patterns among distracters can be facilitated not only by intramodal priors, but also by feature-based cross-modal information from the auditory system."
LUCIA M VAINA,Long-range coupling of prefrontal cortex and visual (MT) or polysensory (STP) cortical areas in motion perception,"To investigate how, where and when moving auditory cues interact with the perception of object-motion during self-motion, we conducted psychophysical, MEG, and fMRI experiments in which the subjects viewed nine textured objects during simulated forward self-motion. On each trial, one object was randomly assigned its own looming motion within the scene. Subjects reported which of four labeled objects had independent motion within the scene in two conditions: (1) visual information only and (2) with additional moving- auditory cue. In MEG, comparison of the two conditions showed: (i) MT activity is similar across conditions, (ii) late after the stimulus presentation there is additional activity in the auditory cue condition ventral to MT, (iii) with the auditory cue, the right auditory cortex (AC) shows early activity together with STS, (iv) these two activities have different time courses and the STS signals occur later in the epoch together with frontal activity in the right hemisphere, (v) for the visual-only condition activity in PPC (posterior parietal cortex) is stronger than in the auditory-cue condition. fMRI conducted for visual-only condition reveals activations in a network of parietal and frontal areas and in MT. In addition, Dynamic Granger Causality analysis showed for auditory cues a strong connection of the AC with STP but not with MT suggesting binding of visual and auditory information at STP. Also, while in the visual-only condition PFC is connected with MT, in the auditory-cue condition PFC is connected to STP (superior temporal polysensory) area. These results indicate that PFC allocates attention to the “object” as a whole, in STP to a moving visual-auditory object, and in MT to a moving visual object."
LUCIA M VAINA,Deficit of temporal dynamics of detection of a moving object during egomotion in a stroke patient: a psychophysical and MEG study,"To investigate the temporal dynamics underlying object motion detection during egomotion, we used psychophysics and MEG with a motion discrimination task. The display contained nine spheres moving for 1 second, eight moved consistent with forward observer translation, and one (the target) with independent motion within the scene (approaching or receding). Observers's task was to detect the target. Seven healthy subjects (7HS) and patient PF with an infarct involving the left occipital-temporal cortex participated in both the psychophysical and MEG study. Psychophysical results showed that PF was severely impaired on this task. He was also impaired on the discrimination of radial motion (with even poorer performance on contraction) and 2D direction as well as on detecting motion discontinuity. We used anatomically constrained MEG and dynamic Granger causality to investigate the direction and dynamics of connectivity between the functional areas involved in the object-motion task and compared the results of 7HS and PF. The dynamics of the causal connections among the motion responsive cortical areas (MT, STS, IPS) during the first 200 ms of the stimulus was similar in all subjects. However, in the later part of the stimulus (>200 ms) PF did not show significant causal connections among these areas. Also the 7HS had a strong, probably attention modulatory connection, between MPFC and MT, which was completely absent in PF. In PF and the 7HS, analysis of onset latencies revealed two stages of activations: early after motion onset (200–400 ms) bilateral activations in MT, IPS, and STS, followed (>500 ms) by activity in the postcentral sulcus and middle prefrontal cortex (MPFC). We suggest that the interaction of these early and late onset areas is critical to object motion detection during self-motion, and disrupted connections among late onset areas may have contributed to the perceptual deficits of patient PF."
LUCIA M VAINA,"Scale changes provide an alternative cue for the discrimination of heading, but not object motion","BACKGROUND Understanding the dynamics of our surrounding environments is a task usually attributed to the detection of motion based on changes in luminance across space. Yet a number of other cues, both dynamic and static, have been shown to provide useful information about how we are moving and how objects around us move. One such cue, based on changes in spatial frequency, or scale, over time has been shown to be useful in conveying motion in depth even in the absence of a coherent, motion-defined flow field (optic flow). MATERIAL AND METHODS 16 right handed healthy observers (ages 18-28) participated in the behavioral experiments described in this study. Using analytical behavioral methods we investigate the functional specificity of this cue by measuring the ability of observers to perform tasks of heading (direction of self-motion) and 3D trajectory discrimination on the basis of scale changes and optic flow. RESULTS Statistical analyses of performance on the test-experiments in comparison to the control experiments suggests that while scale changes may be involved in the detection of heading, they are not correctly integrated with translational motion and, thus, do not provide a correct discrimination of 3D object trajectories. CONCLUSIONS These results have the important implication for the type of visual guided navigation that can be done by an observer blind to optic flow. Scale change is an important alternative cue for self-motion."
LUCIA M VAINA,Effect of residual interatrial shunt on migraine burden after transcatheter closure of patent foramen ovale,"OBJECTIVES: This study sought to evaluate the long-term effect of transcatheter patent foramen ovale (PFO) closure on migraineurs with and without aura and examine the effect of residual right-to-left shunt. BACKGROUND: Many studies reported improvement in migraine symptoms after PFO closure, yet randomized trials failed to reach its clinical endpoints. METHODS: The study retrospectively analyzed data from 474 patients who underwent transcatheter PFO closure at Massachusetts General Hospital. Patients completed a migraine burden questionnaire at baseline and at follow-up. Migraine severity is reported as migraine frequency (days/month), average duration (min), and migraine burden (days × min/month). Improvement following closure was defined as complete abolishment of symptoms or >50% reduction in migraine burden. RESULTS: A total of 110 migraineurs who underwent PFO closure were included; 77.0% had aura and 23.0% were without aura, and 91.0% had a cryptogenic stroke. During long-term median follow-up of 3.2 (interquartile range: 2.1 to 4.9) years, there was a significant improvement in migraine symptoms in migraineurs with or without aura. Migraine burden was reduced by >50% in 87.0% of patients, and symptoms were completely abolished in 48%. Presence of aura was associated with abolishment of migraine (odds ratio: 4.30; 95% confidence interval: 1.50 to 12.30; p = 0.006). At 6 months after PFO closure, residual right-to-left shunt was present in 26% of patients. Absence of right-to-left shunt was associated with improvement in migraine burden by >50% (odds ratio: 4.60; 95% confidence interval: 1.30 to 16.10; p = 0.017). CONCLUSIONS: Long-term follow-up after transcatheter PFO closure was associated with significant improvement in migraine burden. Aura was a predictor of abolishing symptoms. Absence of residual right-to-left shunt was a predictor of significant reduction in migraine burden."
PAUL CARLILE,Making lemonade: dealing with analytics surveillance in the workplace,
PAUL CARLILE,The pragmatic cycle of knowledge work: unlocking cross domain collaboration in open innovation spaces,"Collaborating is increasingly characterized by working across domains and organizations. Teams rapidly form and dissolve, actors and settings frequently change, yet most academic research focuses on stable organizations and team configurations with familiar domains. This leads to the question: how do people successfully collaborate across domains and organizations in circumstances where there is little shared knowledge? We explored this question within the nascent digital health sector when Hacking Health—a non-profit organization—used an open innovation approach to bring together actors from different domains and organizations in temporary spaces to spur new collaborations. We found that actors faced many challenges and engaged in four interconnected types of knowledge work to address them: exploring, complementing, mapping, and modeling. This article reveals how Hacking Health’s open innovation approach used different kinds of temporary spaces to progressively orient actors in their knowledge work to develop sustainable collaborations to create digital health solutions."
PRAKASH ISHWAR,VGAN-based image representation learning for privacy-preserving facial expression recognition,"Reliable facial expression recognition plays a critical role in human-machine interactions. However, most of the facial expression analysis methodologies proposed to date pay little or no attention to the protection of a user's privacy. In this paper, we propose a Privacy-Preserving Representation-Learning Variational Generative Adversarial Network (PPRL-VGAN) to learn an image representation that is explicitly disentangled from the identity information. At the same time, this representation is discriminative from the standpoint of facial expression recognition and generative as it allows expression-equivalent face image synthesis. We evaluate the proposed model on two public datasets under various threat scenarios. Quantitative and qualitative results demonstrate that our approach strikes a balance between the preservation of privacy and data utility. We further demonstrate that our model can be effectively applied to other tasks such as expression morphing and image completion."
PRAKASH ISHWAR,BUOCA: Budget-Optimized Crowd Worker Allocation,"Due to concerns about human error in crowdsourcing, it is standard practice to collect labels for the same data point from multiple internet workers. We here show that the resulting budget can be used more effectively with a flexible worker assignment strategy that asks fewer workers to analyze easy-to-label data and more workers to analyze data that requires extra scrutiny. Our main contribution is to show how the allocations of the number of workers to a task can be computed optimally based on task features alone, without using worker profiles. Our target tasks are delineating cells in microscopy images and analyzing the sentiment toward the 2016 U.S. presidential candidates in tweets. We first propose an algorithm that computes budget-optimized crowd worker allocation (BUOCA). We next train a machine learning system (BUOCA-ML) that predicts an optimal number of crowd workers needed to maximize the accuracy of the labeling. We show that the computed allocation can yield large savings in the crowdsourcing budget (up to 49 percent points) while maintaining labeling accuracy. Finally, we envisage a human-machine system for performing budget-optimized data analysis at a scale beyond the feasibility of crowdsourcing."
PRAKASH ISHWAR,A fully-convolutional neural network for background subtraction of unseen videos,"Background subtraction is a basic task in computer vision and video processing often applied as a pre-processing step for object tracking, people recognition, etc. Recently, a number of successful background-subtraction algorithms have been proposed, however nearly all of the top-performing ones are supervised. Crucially, their success relies upon the availability of some annotated frames of the test video during training. Consequently, their performance on completely “unseen” videos is undocumented in the literature. In this work, we propose a new, supervised, backgroundsubtraction algorithm for unseen videos (BSUV-Net) based on a fully-convolutional neural network. The input to our network consists of the current frame and two background frames captured at different time scales along with their semantic segmentation maps. In order to reduce the chance of overfitting, we also introduce a new data-augmentation technique which mitigates the impact of illumination difference between the background frames and the current frame. On the CDNet-2014 dataset, BSUV-Net outperforms stateof-the-art algorithms evaluated on unseen videos in terms of several metrics including F-measure, recall and precision."
PRAKASH ISHWAR,Convolutional neural network denoising of focused ion beam micrographs,"Most research on deep learning algorithms for image denoising has focused on signal-independent additive noise. Focused ion beam (FIB) microscopy with direct secondary electron detection has an unusual Neyman Type A (compound Poisson) measurement model, and sample damage poses fundamental challenges in obtaining training data. Model-based estimation is difficult and ineffective because of the nonconvexity of the negative log likelihood. In this paper, we develop deep learning-based denoising methods for FIB micrographs using synthetic training data generated from natural images. To the best of our knowledge, this is the first attempt in the literature to solve this problem with deep learning. Our results show that the proposed methods slightly outperform a total variation-regularized model-based method that requires time-resolved measurements that are not conventionally available. Improvements over methods using conventional measurements and less accurate noise modeling are dramatic - around 10 dB in peak signal-to-noise ratio."
PRAKASH ISHWAR,Feature analysis and extraction for post aphasia recovery prediction,
PRAKASH ISHWAR,"Ergodic limits, relaxations, and geometric properties of random walk node embeddings","Random walk based node embedding algorithms learn vector representations of nodes by optimizing an objective function of node embedding vectors and skip-bigram statistics computed from random walks on the network. They have been applied to many supervised learning problems such as link prediction and node classification and have demonstrated state-of-the-art performance. Yet, their properties remain poorly understood. This paper studies properties of random walk based node embeddings in the unsupervised setting of discovering hidden block structure in the network, i.e., learning node representations whose cluster structure in Euclidean space reflects their adjacency structure within the network. We characterize the ergodic limits of the embedding objective, its generalization, and related convex relaxations to derive corresponding non-randomized versions of the node embedding objectives. We also characterize the optimal node embedding Grammians of the non-randomized objectives for the expected graph of a two-community Stochastic Block Model (SBM). We prove that the solution Grammian has rank 1 for a suitable nuclear norm relaxation of the non-randomized objective. Comprehensive experimental results on SBM random networks reveal that our non-randomized ergodic objectives yield node embeddings whose distribution is Gaussian-like, centered at the node embeddings of the expected network within each community, and concentrate in the linear degree-scaling regime as the number of nodes increases."
PRAKASH ISHWAR,BSUV-Net: a fully-convolutional neural network for background subtraction of unseen videos,"Background subtraction is a basic task in computer vision and video processing often applied as a pre-processing step for object tracking, people recognition, etc. Recently, a number of successful background-subtraction algorithms have been proposed, however nearly all of the top-performing ones are supervised. Crucially, their success relies upon the availability of some annotated frames of the test video during training. Consequently, their performance on completely “unseen” videos is undocumented in the literature. In this work, we propose a new, supervised, background subtraction algorithm for unseen videos (BSUV-Net) based on a fully-convolutional neural network. The input to our network consists of the current frame and two background frames captured at different time scales along with their semantic segmentation maps. In order to reduce the chance of overfitting, we also introduce a new data-augmentation technique which mitigates the impact of illumination difference between the background frames and the current frame. On the CDNet-2014 dataset, BSUV-Net outperforms stateof-the-art algorithms evaluated on unseen videos in terms of several metrics including F-measure, recall and precision."
PRAKASH ISHWAR,Semi-coupled two-stream fusion ConvNets for action recognition at extremely low resolutions,"Deep convolutional neural networks (ConvNets) have been recently shown to attain state-of-the-art performance for action recognition on standard-resolution videos. However, less attention has been paid to recognition performance at extremely low resolutions (eLR) (e.g., 16 12 pixels). Reliable action recognition using eLR cameras would address privacy concerns in various application environments such as private homes, hospitals, nursing/rehabilitation facilities, etc. In this paper, we propose a semi-coupled, filter-sharing network that leverages high-resolution (HR) videos during training in order to assist an eLR ConvNet. We also study methods for fusing spatial and temporal ConvNets customized for eLR videos in order to take advantage of appearance and motion information. Our method outperforms state-of-the-art methods at extremely low resolutions on IXMAS (93:7%) and HMDB (29:2%) datasets."
PRAKASH ISHWAR,Community detection of the framing element network: proposing and assessing a new computational framing analysis approach,"The evolving computational news framing detection has been a prominent yet contested field among mass communication scholars. This study explores a new approach to identifying frames as clusters of framing elements including actors (i.e., individual and organizational entities) and topics in news articles based on the community detection algorithm. Our approach highlights the fundamental importance of considering individual and organizational actors mentioned in news articles as components of frames, which is overlooked in previous research that uses a similar unsupervised approach. We evaluate the performance of our method by comparing it with one of the most popular unsupervised methods--LDA topic modeling--and a state-of-art deep learning method, BERT, based on 2,900 US gun violence news articles."
PRAKASH ISHWAR,OpenFraming: open-sourced tool for computational framing analysis of multilingual data,"When journalists cover a news story, they can cover the story from multiple angles or perspectives. These perspectives are called “frames,” and usage of one frame or another may influence public perception and opinion of the issue at hand. We develop a web-based system for analyzing frames in multilingual text documents. We propose and guide users through a five-step end-to-end computational framing analysis framework grounded in media framing theory in communication research. Users can use the framework to analyze multilingual text data, starting from the exploration of frames in user’s corpora and through review of previous framing literature (step 1-3) to frame classification (step 4) and prediction (step 5). The framework combines unsupervised and supervised machine learning and leverages a state-of-the-art (SoTA) multilingual language model, which can significantly enhance frame prediction performance while requiring a considerably small sample of manual annotations. Through the interactive website, anyone can perform the proposed computational framing analysis, making advanced computational analysis available to researchers without a programming background and bridging the digital divide within the communication research discipline in particular and the academic community in general. The system is available online at http://www.openframing.org, via an API http://www.openframing.org:5000/docs, or through our GitHub page https://github.com/vibss2397/openFraming."
PRAKASH ISHWAR,Multimodal neural and behavioral data predict response to rehabilitation in chronic post-stroke aphasia,"BACKGROUND: Poststroke recovery depends on multiple factors and varies greatly across individuals. Using machine learning models, this study investigated the independent and complementary prognostic role of different patient-related factors in predicting response to language rehabilitation after a stroke. METHODS: Fifty-five individuals with chronic poststroke aphasia underwent a battery of standardized assessments and structural and functional magnetic resonance imaging scans, and received 12 weeks of language treatment. Support vector machine and random forest models were constructed to predict responsiveness to treatment using pretreatment behavioral, demographic, and structural and functional neuroimaging data. RESULTS: The best prediction performance was achieved by a support vector machine model trained on aphasia severity, demographics, measures of anatomic integrity and resting-state functional connectivity (F1=0.94). This model resulted in a significantly superior prediction performance compared with support vector machine models trained on all feature sets (F1=0.82, P<0.001) or a single feature set (F1 range=0.68–0.84, P<0.001). Across random forest models, training on resting-state functional magnetic resonance imaging connectivity data yielded the best F1 score (F1=0.87). CONCLUSIONS: While behavioral, multimodal neuroimaging data and demographic information carry complementary information in predicting response to rehabilitation in chronic poststroke aphasia, functional connectivity of the brain at rest after stroke is a particularly important predictor of responsiveness to treatment, both alone and combined with other patient-related factors."
RICHARD C BROWER,Lattice setup for quantum field theory in AdS2,"Holographic conformal field theories (CFTs) are usually studied in a limit where the gravity description is weakly coupled. By contrast, lattice quantum field theory can be used as a tool for doing computations in a wider class of holographic CFTs where nongravitational interactions in AdS become strong, and gravity is decoupled. We take preliminary steps for studying such theories on the lattice by constructing the discretized theory of a scalar field in AdS 2 and investigating its approach to the continuum limit in the free and perturbative regimes. Our main focus is on finite sublattices of maximally symmetric tilings of hyperbolic space. Up to boundary effects, these tilings preserve the triangle group as a large discrete subgroup of AdS 2 , but have a minimum lattice spacing that is comparable to the radius of curvature of the underlying spacetime. We quantify the effects of the lattice spacing as well as the boundary effects, and find that they can be accurately modeled by modifications within the framework of the continuum limit description. We also show how to do refinements of the lattice that shrink the lattice spacing at the cost of breaking the triangle group symmetry of the maximally symmetric tilings."
THOMAS F WEBSTER,Bias magnification in ecologic studies: a methodological investigation,"BACKGROUND: As ecologic studies are often inexpensive to conduct, consideration of the magnitude and direction of ecologic biases may be useful in both study design and sensitivity analysis of results. This paper examines three types of ecologic bias: confounding by group, effect measure modification by group, and non-differential exposure misclassification. METHODS: Bias of the risk difference on the individual and ecologic levels are compared using two-by-two tables, simple equations, and risk diagrams. Risk diagrams provide a convenient way to simultaneously display information from both levels. RESULTS: Confounding by group and effect measure modification by group act in the same direction on the individual and group levels, but have larger impact on the latter. The reduction in exposure variance caused by aggregation magnifies the individual level bias due to ignoring groups. For some studies, the magnification factor can be calculated from the ecologic data alone. Small magnification factors indicate little bias beyond that occurring at the individual level. Aggregation is also responsible for the different impacts of non-differential exposure misclassification on individual and ecologic studies. CONCLUSION: The analytical tools developed here are useful in analyzing ecologic bias. The concept of bias magnification may be helpful in designing ecologic studies and performing sensitivity analysis of their results."
THOMAS F WEBSTER,Prenatal Exposure to Tetrachloroethylene-Contaminated Drinking Water and the Risk of Congenital Anomalies: A Retrospective Cohort Study,"BACKGROUND: Prior animal and human studies of prenatal exposure to solvents including tetrachloroethylene (PCE) have shown increases in the risk of certain congenital anomalies among exposed offspring. OBJECTIVES: This retrospective cohort study examined whether PCE contamination of public drinking water supplies in Massachusetts influenced the occurrence of congenital anomalies among children whose mothers were exposed around the time of conception. METHODS: The study included 1,658 children whose mothers were exposed to PCE-contaminated drinking water and a comparable group of 2,999 children of unexposed mothers. Mothers completed a self-administered questionnaire to gather information on all of their prior births, including the presence of anomalies, residential histories and confounding variables. PCE exposure was estimated using EPANET water distribution system modeling software that incorporated a fate and transport model. RESULTS: Children whose mothers had high exposure levels around the time of conception had an increased risk of congenital anomalies. The adjusted odds ratio of all anomalies combined among children with prenatal exposure in the uppermost quartile was 1.5 (95% CI: 0.9, 2.5). No meaningful increases in the risk were seen for lower exposure levels. Increases were also observed in the risk of neural tube defects (OR: 3.5, 95% CI: 0.8, 14.0) and oral clefts (OR 3.2, 95% CI: 0.7, 15.0) among offspring with any prenatal exposure. CONCLUSION: The results of this study suggest that the risk of certain congenital anomalies is increased among the offspring of women who were exposed to PCE-contaminated drinking water around the time of conception. Because these results are limited by the small number of children with congenital anomalies that were based on maternal reports, a follow-up investigation should be conducted with a larger number of affected children who are identified by independent records."
THOMAS F WEBSTER,"Association of urinary phthalate metabolite concentrations with body mass index and waist circumference: a cross-sectional study of NHANES data, 1999–2002","BACKGROUND: Although diet and activity are key factors in the obesity epidemic, laboratory studies suggest that endocrine disrupting chemicals may also affect obesity. METHODS: We analyzed associations between six phthalate metabolites measured in urine and body mass index (BMI) and waist circumference (WC) in National Health and Nutrition Examination Survey (NHANES) participants aged 6–80. We included 4369 participants from NHANES 1999–2002, with data on mono-ethyl (MEP), mono-2-ethylhexyl (MEHP), mono-n-butyl (MBP), and mono-benzyl (MBzP) phthalate; 2286 also had data on mono-2-ethyl-5-hydroxyhexyl (MEHHP) and mono-2-ethyl-5-oxohexyl (MEOHP) phthalate (2001–2002). Using multiple regression, we computed mean BMI and WC within phthalate quartiles in eight age/gender specific models. RESULTS: The most consistent associations were in males aged 20–59; BMI and WC increased across quartiles of MBzP (adjusted mean BMI = 26.7, 27.2, 28.4, 29.0, p-trend = 0.0002), and positive associations were also found for MEOHP, MEHHP, MEP, and MBP. In females, BMI and WC increased with MEP quartile in adolescent girls (adjusted mean BMI = 22.9, 23.8, 24.1, 24.7, p-trend = 0.03), and a similar but less strong pattern was seen in 20–59 year olds. In contrast, MEHP was inversely related to BMI in adolescent girls (adjusted mean BMI = 25.4, 23.8, 23.4, 22.9, p-trend = 0.02) and females aged 20–59 (adjusted mean BMI = 29.9, 29.9, 27.9, 27.6, p-trend = 0.02). There were no important associations among children, but several inverse associations among 60–80 year olds. CONCLUSION: This exploratory, cross-sectional analysis revealed a number of interesting associations with different phthalate metabolites and obesity outcomes, including notable differences by gender and age subgroups. Effects of endocrine disruptors, such as phthalates, may depend upon endogenous hormone levels, which vary dramatically by age and gender. Individual phthalates also have different biologic and hormonal effects. Although our study has limitations, both of these factors could explain some of the variation in the observed associations. These preliminary data support the need for prospective studies in populations at risk for obesity."
THOMAS F WEBSTER,"Exposure to Polyfluoroalkyl Chemicals and Cholesterol, Body Weight, and Insulin Resistance in the General U.S. Population","BACKGROUND. Polyfluoroalkyl chemicals (PFCs) are used commonly in commercial applications and are detected in humans and the environment worldwide. Concern has been raised that they may disrupt lipid and weight regulation. OBJECTIVES. We investigated the relationship between PFC serum concentrations and lipid and weight outcomes in a large publicly available data set. METHODS. We analyzed data from the 2003-2004 National Health and Nutrition Examination Survey (NHANES) for participants 12-80 years of age. Using linear regression to control for covariates, we studied the association between serum concentrations of perfluorooctanoic acid (PFOA), perfluorononanoic acid (PFNA), perfluorooctane sulfonic acid (PFOS), and perfluorohexane sulfonic acid (PFHxS) and measures of cholesterol, body size, and insulin resistance. RESULTS. We observed a positive association between concentrations of PFOS, PFOA, and PFNA and total and non-high-density cholesterol. We found the opposite for PFHxS. Those in the highest quartile of PFOS exposure had total cholesterol levels 13.4 mg/dL [95% confidence interval (CI), 3.8-23.0] higher than those in the lowest quartile. For PFOA, PFNA, and PFHxS, effect estimates were 9.8 (95% CI, -0.2 to 19.7), 13.9 (95% CI, 1.9-25.9), and -7.0 (95% CI, -13.2 to -0.8), respectively. A similar pattern emerged when exposures were modeled continuously. We saw little evidence of a consistent association with body size or insulin resistance. CONCLUSIONS. This exploratory cross-sectional study is consistent with other epidemiologic studies in finding a positive association between PFOS and PFOA and cholesterol, despite much lower exposures in NHANES. Results for PFNA and PFHxS are novel, emphasizing the need to study PFCs other than PFOS and PFOA."
THOMAS F WEBSTER,Using Residential History and Groundwater Modeling to Examine Drinking Water Exposure and Breast Cancer,"BACKGROUND. Spatial analyses of case-control data have suggested a possible link between breast cancer and groundwater plumes in upper Cape Cod, Massachusetts. OBJECTIVE. We integrated residential histories, public water distribution systems, and groundwater modeling within geographic information systems (GIS) to examine the association between exposure to drinking water that has been contaminated by wastewater effluent and breast cancer. METHODS. Exposure was assessed from 1947 to 1993 for 638 breast cancer cases who were diagnosed from 1983 to 1993 and 842 controls; we took into account residential mobility and drinking water source. To estimate the historical impact of effluent on drinking water wells, we modified a modular three-dimensional finite-difference groundwater model (MODFLOW) from the U.S. Geological Survey. The analyses included latency and exposure duration. RESULTS. Wastewater effluent impacted the drinking water wells of study participants as early as 1966. For > 0-5 years of exposure (versus no exposure), associations were generally null. Adjusted odds ratios (AORs) for > 10 years of exposure were slightly increased, assuming latency periods of 0 or 10 years [AOR = 1.3; 95% confidence interval (CI), 0.9-1.9 and AOR = 1.6; 95% CI, 0.8-3.2, respectively]. Statistically significant associations were estimated for ever-exposed versus never-exposed women when a 20-year latency period was assumed (AOR = 1.9; 95% CI, 1.0-3.4). A sensitivity analysis that classified exposures assuming lower well-pumping rates showed similar results. CONCLUSION. We investigated the hypothesis generated by earlier spatial analyses that exposure to drinking water contaminated by wastewater effluent may be associated with breast cancer. Using a detailed exposure assessment, we found an association with breast cancer that increased with longer latency and greater exposure duration."
THOMAS F WEBSTER,San Antonio Statement on Brominated and Chlorinated Flame Retardants,
THOMAS F WEBSTER,Diet Contributes Significantly to the Body Burden of PBDEs in the General U.S. Population,"BACKGROUND. Exposure of the U.S. population to polybrominated diphenyl ethers (PBDEs) is thought to be via exposure to dust and diet. However, little work has been done to empirically link body burdens of these compounds to either route of exposure. OBJECTIVES. The primary goal of this research was to evaluate the dietary contribution to PBDE body burdens in the United States by linking serum levels to food intake. METHODS. We used two dietary instruments-a 24-hr food recall (24FR) and a 1-year food frequency questionnaire (FFQ)-to examine food intake among participants of the 2003-2004 National Health and Nutrition Examination Survey. We regressed serum concentrations of five PBDEs (BDE congeners 28, 47, 99, 100, and 153) and their sum (ΣPBDE) against diet variables while adjusting for age, sex, race/ethnicity, income, and body mass index. RESULTS. ΣPBDE serum concentrations among vegetarians were 23% (p = 0.006) and 27% (p = 0.009) lower than among omnivores for 24FR and 1-year FFQ, respectively. Serum levels of five PBDE congeners were associated with consumption of poultry fat: Low, medium, and high intake corresponded to geometric mean ΣPBDE concentrations of 40.6, 41.9, and 48.3 ng/g lipid, respectively (p = 0.0005). We observed similar trends for red meat fat, which were statistically significant for BDE-100 and BDE-153. No association was observed between serum PBDEs and consumption of dairy or fish. Results were similar for both dietary instruments but were more robust using 24FR. CONCLUSIONS. Intake of contaminated poultry and red meat contributes significantly to PBDE body burdens in the United States."
THOMAS F WEBSTER,A New Spin on Research Translation: The Boston Consensus Conference on Human Biomonitoring,"BACKGROUND. Translating research to make it more understandable and effective (research translation) has been declared a priority in environmental health but does not always include communication to the public or residents of communities affected by environmental hazards. Their unique perspectives are also commonly missing from discussions about science and technology policy. The consensus conference process, developed in Denmark, offers a way to address this gap. OBJECTIVES. The Boston Consensus Conference on Human Biomonitoring, held in Boston, Massachusetts, in the fall of 2006, was designed to educate and elicit input from 15 Boston-area residents on the scientifically complex topic of human biomonitoring for environmental chemicals. This lay panel considered the many ethical, legal, and scientific issues surrounding biomonitoring and prepared a report expressing their views. DISCUSSION. The lay panel's findings provide a distinct and important voice on the expanding use of biomonitoring. In some cases, such as a call for opt-in reporting of biomonitoring results to study participants, they mirror recommendations raised elsewhere. Other conclusions have not been heard previously, including the recommendation that an individual's results should be statutorily exempted from the medical record unless permission is granted, and the opportunity to use biomonitoring data to stimulate green chemistry. CONCLUSION. The consensus conference model addresses both aspects of a broader conception of research translation: engaging the public in scientific questions, and bringing their unique perspectives to bear on public health research, practice, and policy. In this specific application, a lay panel's recommendations on biomonitoring surveillance, communication, and ethics have practical implications for the conduct of biomonitoring studies and surveillance programs."
THOMAS F WEBSTER,"Community- and Individual-Level Socioeconomic Status and Breast Cancer Risk: Multilevel Modeling on Cape Cod, Massachusetts","BACKGROUND. Previous research demonstrated increased risk of breast cancer associated with higher socioeconomic status (SES) measured at both the individual and community levels. However, little attention has been paid to simultaneously examining both measures. OBJECTIVES. We evaluated the independent influences of individual and community SES on the risk of breast cancer using case-control data. Because our previous work suggests that associations may be stronger after including a latency period, we also assessed the effect of community-level SES assuming a 10-year latency period. METHODS. We obtained individual education for cases and matched controls diagnosed between 1987 and 1993 on Cape Cod, Massachusetts (USA). We acquired community-level SES from census data for 1980 and 1990. Using SES data at diagnosis and 10 years earlier, we constructed models for breast cancer risk using individual-level SES only, community-level SES only, and a multilevel analysis including both. We adjusted models for other individual-level risk factors. RESULTS. Women with the highest education were at greater risk of developing breast cancer in both 1980 and 1990 [odds ratio (OR) = 1.17 and 1.19, respectively]. Similarly, women living in the highest-SES communities in 1990 had greater risk (OR = 1.30). Results were stronger in the analyses considering a latency period (OR = 1.69). Adjusting for intragroup correlation had little effect on the analyses. CONCLUSIONS. Models including individual- or community-level measures of SES produced associations similar to those observed in previous research. Results for models including both measures are consistent with a contextual effect of SES on risk of breast cancer independent of individual SES."
THOMAS F WEBSTER,Exposure to Polyfluoroalkyl Chemicals and Attention Deficit/Hyperactivity Disorder in U.S. Children 12-15 Years of Age,"BACKGROUND. Polyfluoroalkyl chemicals (PFCs) have been widely used in consumer products. Exposures in the United States and in world populations are widespread. PFC exposures have been linked to various health impacts, and data in animals suggest that PFCs may be potential developmental neurotoxicants. OBJECTIVES. We evaluated the associations between exposures to four PFCs and parental report of diagnosis of attention deficit/hyperactivity disorder (ADHD). METHODS. Data were obtained from the National Health and Nutrition Examination Survey (NHANES) 1999-2000 and 2003-2004 for children 12-15 years of age. Parental report of a previous diagnosis by a doctor or health care professional of ADHD in the child was the primary outcome measure. Perfluorooctane sulfonic acid (PFOS), perfluorooctanoic acid (PFOA), perfluorononanoic acid (PFNA), and perfluorohexane sulfonic acid (PFHxS) levels were measured in serum samples from each child. RESULTS. Parents reported that 48 of 571 children included in the analysis had been diagnosed with ADHD. The adjusted odds ratio (OR) for parentally reported ADHD in association with a 1-μg/L increase in serum PFOS (modeled as a continuous predictor) was 1.03 [95% confidence interval (CI), 1.01-1.05]. Adjusted ORs for 1-μg/L increases in PFOA and PFHxS were also statistically significant (PFOA: OR = 1.12; 95% CI, 1.01-1.23; PFHxS: OR = 1.06; 95% CI, 1.02-1.11), and we observed a nonsignificant positive association with PFNA (OR = 1.32; 95% CI, 0.86-2.02). CONCLUSIONS. Our results, using cross-sectional data, are consistent with increased odds of ADHD in children with higher serum PFC levels. Given the extremely prevalent exposure to PFCs, follow-up of these data with cohort studies is needed."
THOMAS F WEBSTER,Generalized concentration addition predicts joint effects of aryl hydrocarbon receptor agonists with partial agonists and competitive antagonists,"BACKGROUND. Predicting the expected outcome of a combination exposure is critical to risk assessment. The toxic equivalency factor (TEF) approach used for analyzing joint effects of dioxin-like chemicals is a special case of the method of concentration addition. However, the TEF method assumes that individual agents are full aryl hydrocarbon receptor (AhR) agonists with parallel dose-response curves, whereas many mixtures include partial agonists. OBJECTIVES. We assessed the ability of generalized concentration addition (GCA) to predict effects of combinations of full AhR agonists with partial agonists or competitive antagonists. METHODS. We measured activation of AhR-dependent gene expression in H1G1.1c3 cells after application of binary combinations of AhR ligands. A full agonist (2,3,7,8-tetrachlorodibenzo-p-dioxin or 2,3,7,8-tetrachlorodibenzofuran) was combined with either a full agonist (3,3',4,4',5-pentachlorobiphenyl), a partial agonist (2,3,3',4,4'-pentachlorobiphenyl or galangin), or an antagonist (3,3'-diindolylmethane). Combination effects were modeled by the TEF and GCA approaches, and goodness of fit of the modeled response surface to the experimental data was assessed using a nonparametric statistical test. RESULTS. The GCA and TEF models fit the experimental data equally well for a mixture of two full agonists. In all other cases, GCA fit the experimental data significantly better than the TEF model. CONCLUSIONS. The TEF model overpredicts effects of AhR ligands at the highest concentration combinations. At lower concentrations, the difference between GCA and TEF approaches depends on the efficacy of the partial agonist. GCA represents a more accurate definition of additivity for mixtures that include partial agonist or competitive antagonist ligands."
THOMAS F WEBSTER,Association between Residences in U.S. Northern Latitudes and Rheumatoid Arthritis: A Spatial Analysis of the Nurses' Health Study,"BACKGROUND. The etiology of rheumatoid arthritis (RA) remains largely unknown, although epidemiologic studies suggest genetic and environmental factors may play a role. Geographic variation in incident RA has been observed at the regional level. OBJECTIVE. Spatial analyses are a useful tool for confirming existing exposure hypotheses or generating new ones. To further explore the association between location and RA risk, we analyzed individual-level data from U.S. women in the Nurses' Health Study, a nationwide cohort study. METHODS. Participants included 461 incident RA cases and 9,220 controls with geocoded addresses; participants were followed from 1988 to 2002. We examined spatial variation using addresses at baseline in 1988 and at the time of case diagnosis or the censoring of controls. Generalized additive models (GAMs) were used to predict a continuous risk surface by smoothing on longitude and latitude while adjusting for known risk factors. Permutation tests were conducted to evaluate the overall importance of location and to identify, within the entire study area, those locations of statistically significant risk. RESULTS. A statistically significant area of increased RA risk was identified in the northeast United States (p-value = 0.034). Risk was generally higher at northern latitudes, and it increased slightly when we used the nurses' 1988 locations compared with those at the time of diagnosis or censoring. Crude and adjusted models produced similar results. CONCLUSIONS. Spatial analyses suggest women living in higher latitudes may be at greater risk for RA. Further, RA risk may be greater for locations that occur earlier in residential histories. These results illustrate the usefulness of GAM methods in generating hypotheses for future investigation and supporting existing hypotheses."
THOMAS F WEBSTER,Residential History and Groundwater Modeling: Gallagher et al. Respond,
THOMAS F WEBSTER,Evaluation of the Webler-Brown Model for Estimating Tetrachloroethylene Exposure from Vinyl-Lined Asbestos-Cement Pipes,"BACKGROUND: From May 1968 through March 1980, vinyl-lined asbestos-cement (VL/AC) water distribution pipes were installed in New England to avoid taste and odor problems associated with asbestos-cement pipes. The vinyl resin was applied to the inner pipe surface in a solution of tetrachloroethylene (perchloroethylene, PCE). Substantial amounts of PCE remained in the liner and subsequently leached into public drinking water supplies. METHODS: Once aware of the leaching problem and prior to remediation (April-November 1980), Massachusetts regulators collected drinking water samples from VL/AC pipes to determine the extent and severity of the PCE contamination. This study compares newly obtained historical records of PCE concentrations in water samples (n = 88) with concentrations estimated using an exposure model employed in epidemiologic studies on the cancer risk associated with PCE-contaminated drinking water. The exposure model was developed by Webler and Brown to estimate the mass of PCE delivered to subjects' residences. RESULTS: The mean and median measured PCE concentrations in the water samples were 66 and 0.5 μg/L, respectively, and the range extended from non-detectable to 2432 μg/L. The model-generated concentration estimates and water sample concentrations were moderately correlated (Spearman rank correlation coefficient = 0.48, p < 0.0001). Correlations were higher in samples taken at taps and spigots vs. hydrants (ρ = 0.84 vs. 0.34), in areas with simple vs. complex geometry (ρ = 0.51 vs. 0.38), and near pipes installed in 1973–1976 vs. other years (ρ = 0.56 vs. 0.42 for 1968–1972 and 0.37 for 1977–1980). Overall, 24% of the variance in measured PCE concentrations was explained by the model-generated concentration estimates (p < 0.0001). Almost half of the water samples had undetectable concentrations of PCE. Undetectable levels were more common in areas with the earliest installed VL/AC pipes, at the beginning and middle of VL/AC pipes, at hydrants, and in complex pipe configurations. CONCLUSION: PCE concentration estimates generated using the Webler-Brown model were moderately correlated with measured water concentrations. The present analysis suggests that the exposure assessment process used in prior epidemiological studies could be improved with more accurate characterization of water flow. This study illustrates one method of validating an exposure model in an epidemiological study when historical measurements are not available."
THOMAS F WEBSTER,Participant experiences in a breastmilk biomonitoring study: a qualitative assessment,"BACKGROUND: Biomonitoring studies can provide information about individual and population-wide exposure. However they must be designed in a way that protects the rights and welfare of participants. This descriptive qualitative study was conducted as a follow-up to a breastmilk biomonitoring study. The primary objectives were to assess participants' experiences in the study, including the report-back of individual body burden results, and to determine if participation in the study negatively affected breastfeeding rates or duration. METHODS: Participants of the Greater Boston PBDE Breastmilk Biomonitoring Study were contacted and asked about their experiences in the study: the impact of study recruitment materials on attitudes towards breastfeeding; if participants had wanted individual biomonitoring results; if the protocol by which individual results were distributed met participants' needs; and the impact of individual results on attitudes towards breastfeeding. RESULTS: No participants reported reducing the duration of breastfeeding because of the biomonitoring study, but some responses suggested that breastmilk biomonitoring studies have the potential to raise anxieties about breastfeeding. Almost all participants wished to obtain individual results. Although several reported some concern about individual body burden, none reported reducing the duration of breastfeeding because of biomonitoring results. The study literature and report-back method were found to mitigate potential negative impacts. CONCLUSION: Biomonitoring study design, including clear communication about the benefits of breastfeeding and the manner in which individual results are distributed, can prevent negative impacts of biomonitoring on breastfeeding. Adoption of more specific standards for biomonitoring studies and continued study of risk communication issues related to biomonitoring will help protect participants from harm."
THOMAS F WEBSTER,A multilevel non-hierarchical study of birth weight and socioeconomic status,"BACKGROUND. It is unclear whether the socioeconomic status (SES) of the community of residence has a substantial association with infant birth weight. We used multilevel models to examine associations of birth weight with family- and community-level SES in the Cape Cod Family Health Study. Data were collected retrospectively on births to women between 1969 and 1983 living on Cape Cod, Massachusetts. The sample included siblings born in different residences with differing community-level SES. METHODS. We used cross-classified models to account for multiple levels of correlation in a non-hierarchical data structure. We accounted for clustering at family- and community-levels. Models included extensive individual- and family-level covariates. SES variables of interest were maternal education; paternal occupation; percent adults living in poverty; percent adults with a four year college degree; community mean family income; and percent adult unemployment. RESULTS. Residual correlation was detected at the family- but not the community-level. Substantial effects sizes were observed for family-level SES while smaller magnitudes were observed for community-level SES. Overall, higher SES corresponded to increased birth weight though neither family- nor community-level variables had significant associations with the outcome. In a model applied to a reduced sample that included a single child per family, enforcing a hierarchical data structure, paternal occupation was found to have a significant association with birth weight (p = 0.033). Larger effect sizes for community SES appeared in models applied to the full sample that contained limited covariates, such as those typically found on birth certificates. CONCLUSIONS. Cross-classified models allowed us to include more than one child per family even when families moved between births. There was evidence of mild associations between family SES and birth weight. Stronger associations between paternal occupation and birth weight were observed in models applied to reduced samples with hierarchical data structures, illustrating consequences of excluding observations from the cross-classified analysis. Models with limited covariates showed associations of birth weight with community SES. In models adjusting for a complete set of individual- and family-level covariates, community SES was not as important."
THOMAS F WEBSTER,A power comparison of generalized additive models and the spatial scan statistic in a case-control setting,"BACKGROUND. A common, important problem in spatial epidemiology is measuring and identifying variation in disease risk across a study region. In application of statistical methods, the problem has two parts. First, spatial variation in risk must be detected across the study region and, second, areas of increased or decreased risk must be correctly identified. The location of such areas may give clues to environmental sources of exposure and disease etiology. One statistical method applicable in spatial epidemiologic settings is a generalized additive model (GAM) which can be applied with a bivariate LOESS smoother to account for geographic location as a possible predictor of disease status. A natural hypothesis when applying this method is whether residential location of subjects is associated with the outcome, i.e. is the smoothing term necessary? Permutation tests are a reasonable hypothesis testing method and provide adequate power under a simple alternative hypothesis. These tests have yet to be compared to other spatial statistics. RESULTS. This research uses simulated point data generated under three alternative hypotheses to evaluate the properties of the permutation methods and compare them to the popular spatial scan statistic in a case-control setting. Case 1 was a single circular cluster centered in a circular study region. The spatial scan statistic had the highest power though the GAM method estimates did not fall far behind. Case 2 was a single point source located at the center of a circular cluster and Case 3 was a line source at the center of the horizontal axis of a square study region. Each had linearly decreasing logodds with distance from the point. The GAM methods outperformed the scan statistic in Cases 2 and 3. Comparing sensitivity, measured as the proportion of the exposure source correctly identified as high or low risk, the GAM methods outperformed the scan statistic in all three Cases. CONCLUSIONS. The GAM permutation testing methods provide a regression-based alternative to the spatial scan statistic. Across all hypotheses examined in this research, the GAM methods had competing or greater power estimates and sensitivities exceeding that of the spatial scan statistic."
THOMAS F WEBSTER,"Spatial analysis of learning and developmental disorders in upper Cape Cod, Massachusetts using generalized additive models","The spatial variability of three indicators of learning and developmental disability (LDD) was assessed for Cape Cod, Massachusetts. Maternal reports of receiving special education services, attention deficit hyperactivity disorder, and educational attainment were available for a birth cohort from 1969-1983. Using generalized additive models and residential history, maps of the odds of LDD were produced that also controlled for known risk factors. While results were not statistically significant, they suggest that children living in certain parts of Cape Cod were more likely to have a LDD. The spatial variation may be due to variation in the physical and social environment."
THOMAS F WEBSTER,"Spatial-temporal analysis of breast cancer in upper Cape Cod, Massachusetts","INTRODUCTION. The reasons for elevated breast cancer rates in the upper Cape Cod area of Massachusetts remain unknown despite several epidemiological studies that investigated possible environmental risk factors. Data from two of these population-based case-control studies provide geocoded residential histories and information on confounders, creating an invaluable dataset for spatial-temporal analysis of participants' residency over five decades. METHODS. The combination of statistical modeling and mapping is a powerful tool for visualizing disease risk in a spatial-temporal analysis. Advances in geographic information systems (GIS) enable spatial analytic techniques in public health studies previously not feasible. Generalized additive models (GAMs) are an effective approach for modeling spatial and temporal distributions of data, combining a number of desirable features including smoothing of geographical location, residency duration, or calendar years; the ability to estimate odds ratios (ORs) while adjusting for confounders; selection of optimum degree of smoothing (span size); hypothesis testing; and use of standard software. We conducted a spatial-temporal analysis of breast cancer case-control data using GAMs and GIS to determine the association between participants' residential history during 1947–1993 and the risk of breast cancer diagnosis during 1983–1993. We considered geographic location alone in a two-dimensional space-only analysis. Calendar year, represented by the earliest year a participant lived in the study area, and residency duration in the study area were modeled individually in one-dimensional time-only analyses, and together in a two-dimensional time-only analysis. We also analyzed space and time together by applying a two-dimensional GAM for location to datasets of overlapping calendar years. The resulting series of maps created a movie which allowed us to visualize changes in magnitude, geographic size, and location of elevated breast cancer risk for the 40 years of residential history that was smoothed over space and time. RESULTS. The space-only analysis showed statistically significant increased areas of breast cancer risk in the northern part of upper Cape Cod and decreased areas of breast cancer risk in the southern part (p-value = 0.04; ORs: 0.90–1.40). There was also a significant association between breast cancer risk and calendar year (p-value = 0.05; ORs: 0.53–1.38), with earlier calendar years resulting in higher risk. The results of the one-dimensional analysis of residency duration and the two-dimensional analysis of calendar year and duration showed that the risk of breast cancer increased with increasing residency duration, but results were not statistically significant. When we considered space and time together, the maps showed a large area of statistically significant elevated risk for breast cancer near the Massachusetts Military Reservation (p-value range:0.02–0.05; ORs range: 0.25–2.5). This increased risk began with residences in the late 1940s and remained consistent in size and location through the late 1950s. CONCLUSION. Spatial-temporal analysis of the breast cancer data may help identify new exposure hypotheses that warrant future epidemiologic investigations with detailed exposure models. Our methods allow us to visualize breast cancer risk, adjust for known confounders including age at diagnosis or index year, family history of breast cancer, parity and age at first live- or stillbirth, and test for the statistical significance of location and time. Despite the advantages of GAMs, analyses are for exploratory purposes and there are still methodological issues that warrant further research. This paper illustrates that GAM methods are a suitable alternative to widely-used cluster detection methods and may be preferable when residential histories from existing epidemiological studies are available."
IRVING J BIGIO,Head & Neck Optical Diagnostics: Vision of the Future of Surgery,Review paper and Proceedings of the Inaugural Meeting of the Head and Neck Optical Diagnostics Society (HNODS) on March 14th 2009 at University College London. The aim of our research must be to provide breakthrough translational research which can be applied clinically in the immediate rather than the near future. We are fortunate that this is indeed a possibility and may fundamentally change current clinical and surgical practice to improve our patients' lives.
IRVING J BIGIO,"Towards minimally-invasive, quantitative assessment of chronic kidney disease using optical spectroscopy","The universal pathologic features implicated in the progression of chronic kidney disease (CKD) are interstitial fibrosis and tubular atrophy (IFTA). Current methods of estimating IFTA are slow, labor-intensive and fraught with variability and sampling error, and are not quantitative. As such, there is pressing clinical need for a less-invasive and faster method that can quantitatively assess the degree of IFTA. We propose a minimally-invasive optical method to assess the macro-architecture of kidney tissue, as an objective, quantitative assessment of IFTA, as an indicator of the degree of kidney disease. The method of elastic-scattering spectroscopy (ESS) measures backscattered light over the spectral range 320-900 nm and is highly sensitive to micromorphological changes in tissues. Using two discrete mouse models of CKD, we observed spectral trends of increased scattering intensity in the near-UV to short-visible region (350-450 nm), relative to longer wavelengths, for fibrotic kidneys compared to normal kidney, with a quasi-linear correlation between the ESS changes and the histopathology-determined degree of IFTA. These results suggest the potential of ESS as an objective, quantitative and faster assessment of IFTA for the management of CKD patients and in the allocation of organs for kidney transplantation."
MARK E CROVELLA,Measuring Bottleneck Link Speed in Package-Switched Networks,"The quality of available network connections can often have a large impact on the performance of distributed applications. For example, document transfer applications such as FTP, Gopher and the World Wide Web suffer increased response times as a result of network congestion. For these applications, the document transfer time is directly related to the available bandwidth of the connection. Available bandwidth depends on two things: 1) the underlying capacity of the path from client to server, which is limited by the bottleneck link; and 2) the amount of other traffic competing for links on the path. If measurements of these quantities were available to the application, the current utilization of connections could be calculated. Network utilization could then be used as a basis for selection from a set of alternative connections or servers, thus providing reduced response time. Such a dynamic server selection scheme would be especially important in a mobile computing environment in which the set of available servers is frequently changing. In order to provide these measurements at the application level, we introduce two tools: bprobe, which provides an estimate of the uncongested bandwidth of a path; and cprobe, which gives an estimate of the current congestion along a path. These two measures may be used in combination to provide the application with an estimate of available bandwidth between server and client thereby enabling application-level congestion avoidance. In this paper we discuss the design and implementation of our probe tools, specifically illustrating the techniques used to achieve accuracy and robustness. We present validation studies for both tools which demonstrate their reliability in the face of actual Internet conditions; and we give results of a survey of available bandwidth to a random set of WWW servers as a sample application of our probe technique. We conclude with descriptions of other applications of our measurement tools, several of which are currently under development."
MARK E CROVELLA,Connection Scheduling in Web Servers,"Under high loads, a Web server may be servicing many hundreds of connections concurrently. In traditional Web servers, the question of the order in which concurrent connections are serviced has been left to the operating system. In this paper we ask whether servers might provide better service by using non-traditional service ordering. In particular, for the case when a Web server is serving static files, we examine the costs and benefits of a policy that gives preferential service to short connections. We start by assessing the scheduling behavior of a commonly used server (Apache running on Linux) with respect to connection size and show that it does not appear to provide preferential service to short connections. We then examine the potential performance improvements of a policy that does favor short connections (shortest-connection-first). We show that mean response time can be improved by factors of four or five under shortest-connection-first, as compared to an (Apache-like) size-independent policy. Finally we assess the costs of shortest-connection-first scheduling in terms of unfairness (i.e., the degree to which long connections suffer). We show that under shortest-connection-first scheduling, long connections pay very little penalty. This surprising result can be understood as a consequence of heavy-tailed Web server workloads, in which most connections are small, but most server load is due to the few large connections. We support this explanation using analysis."
MARK E CROVELLA,Explaining World Wide Web Traffic Self-Similarity,"Recently the notion of self-similarity has been shown to apply to wide-area and local-area network traffic. In this paper we examine the mechanisms that give rise to self-similar network traffic. We present an explanation for traffic self-similarity by using a particular subset of wide area traffic: traffic due to the World Wide Web (WWW). Using an extensive set of traces of actual user executions of NCSA Mosaic, reflecting over half a million requests for WWW documents, we show evidence that WWW traffic is self-similar. Then we show that the self-similarity in such traffic can be explained based on the underlying distributions of WWW document sizes, the effects of caching and user preference in file transfer, the effect of user ""think time"", and the superimposition of many such transfers in a local area network. To do this we rely on empirically measured distributions both from our traces and from data independently collected at over thirty WWW sites."
MARK E CROVELLA,Characteristics of WWW Client-based Traces,"The explosion of WWW traffic necessitates an accurate picture of WWW use, and in particular requires a good understanding of client requests for WWW documents. To address this need, we have collected traces of actual executions of NCSA Mosaic, reflecting over half a million user requests for WWW documents. In this paper we describe the methods we used to collect our traces, and the formats of the collected data. Next, we present a descriptive statistical summary of the traces we collected, which identifies a number of trends and reference patterns in WWW use. In particular, we show that many characteristics of WWW use can be modelled using power-law distributions, including the distribution of document sizes, the popularity of documents as a function of size, the distribution of user requests for documents, and the number of references to documents as a function of their overall rank in popularity (Zipf's law). Finally, we show how the power-law distributions derived from our traces can be used to guide system designers interested in caching WWW documents."
MARK E CROVELLA,Task Assignment in a Distributed System: Improving Performance by Unbalancing Load,"We consider the problem of task assignment in a distributed system (such as a distributed Web server) in which task sizes are drawn from a heavy-tailed distribution. Many task assignment algorithms are based on the heuristic that balancing the load at the server hosts will result in optimal performance. We show this conventional wisdom is less true when the task size distribution is heavy-tailed (as is the case for Web file sizes). We introduce a new task assignment policy, called Size Interval Task Assignment with Variable Load (SITA-V). SITA-V purposely operates the server hosts at different loads, and directs smaller tasks to the lighter-loaded hosts. The result is that SITA-V provably decreases the mean task slowdown by significant factors (up to 1000 or more) where the more heavy-tailed the workload, the greater the improvement factor. We evaluate the tradeoff between improvement in slowdown and increase in waiting time in a system using SITA-V, and show conditions under which SITA-V represents a particularly appealing policy. We conclude with a discussion of the use of SITA-V in a distributed Web server, and show that it is attractive because it has a simple implementation which requires no communication from the server hosts back to the task router."
MARK E CROVELLA,Dynamic Server Selection using Bandwidth Probing in Wide-Area Networks,"Replication is a commonly proposed solution to problems of scale associated with distributed services. However, when a service is replicated, each client must be assigned a server. Prior work has generally assumed that assignment to be static. In contrast, we propose dynamic server selection, and show that it enables application-level congestion avoidance. To make dynamic server selection practical, we demonstrate the use of three tools. In addition to direct measurements of round-trip latency, we introduce and validate two new tools: bprobe, which estimates the maximum possible bandwidth along a given path; and cprobe, which estimates the current congestion along a path. Using these tools we demonstrate dynamic server selection and compare it to previous static approaches. We show that dynamic server selection consistently outperforms static policies by as much as 50%. Furthermore, we demonstrate the importance of each of our tools in performing dynamic server selection."
MARK E CROVELLA,Dynamic Server Selection in the Internet,"As distributed information services like the World Wide Web become increasingly popular on the Internet, problems of scale are clearly evident. A promising technique that addresses many of these problems is service (or document) replication. However, when a service is replicated, clients then need the additional ability to find a ""good"" provider of that service. In this paper we report on techniques for finding good service providers without a priori knowledge of server location or network topology. We consider the use of two principal metrics for measuring distance in the Internet: hops, and round-trip latency. We show that these two metrics yield very different results in practice. Surprisingly, we show data indicating that the number of hops between two hosts in the Internet is not strongly correlated to round-trip latency. Thus, the distance in hops between two hosts is not necessarily a good predictor of the expected latency of a document transfer. Instead of using known or measured distances in hops, we show that the extra cost at runtime incurred by dynamic latency measurement is well justified based on the resulting improved performance. In addition we show that selection based on dynamic latency measurement performs much better in practice that any static selection scheme. Finally, the difference between the distribution of hops and latencies is fundamental enough to suggest differences in algorithms for server replication. We show that conclusions drawn about service replication based on the distribution of hops need to be revised when the distribution of latencies is considered instead."
MARK E CROVELLA,To Queue or Not to Queue: When Queueing is Better Than Timesharing in a Distributed System,"We examine the question of whether to employ the first-come-first-served (FCFS) discipline or the processor-sharing (PS) discipline at the hosts in a distributed server system. We are interested in the case in which service times are drawn from a heavy-tailed distribution, and so have very high variability. Traditional wisdom when task sizes are highly variable would prefer the PS discipline, because it allows small tasks to avoid being delayed behind large tasks in a queue. However, we show that system performance can actually be significantly better under FCFS queueing, if each task is assigned to a host based on the task's size. By task assignment, we mean an algorithm that inspects incoming tasks and assigns them to hosts for service. The particular task assignment policy we propose is called SITA-E: Size Interval Task Assignment with Equal Load. Surprisingly, under SITA-E, FCFS queueing typically outperforms the PS discipline by a factor of about two, as measured by mean waiting time and mean slowdown (waiting time of task divided by its service time). We compare the FCFS/SITA-E policy to the processor-sharing case analytically; in addition we compare it to a number of other policies in simulation. We show that the benefits of SITA-E are present even in small-scale distributed systems (four or more hosts). Furthermore, SITA-E is a static policy that does not incorporate feedback knowledge of the state of the hosts, which allows for a simple and scalable implementation."
MARK E CROVELLA,Application-Level Document Caching in the Internet,"With the increasing demand for document transfer services such as the World Wide Web comes a need for better resource management to reduce the latency of documents in these systems. To address this need, we analyze the potential for document caching at the application level in document transfer services. We have collected traces of actual executions of Mosaic, reflecting over half a million user requests for WWW documents. Using those traces, we study the tradeoffs between caching at three levels in the system, and the potential for use of application-level information in the caching system. Our traces show that while a high hit rate in terms of URLs is achievable, a much lower hit rate is possible in terms of bytes, because most profitably-cached documents are small. We consider the performance of caching when applied at the level of individual user sessions, at the level of individual hosts, and at the level of a collection of hosts on a single LAN. We show that the performance gain achievable by caching at the session level (which is straightforward to implement) is nearly all of that achievable at the LAN level (where caching is more difficult to implement). However, when resource requirements are considered, LAN level caching becomes much more desirable, since it can achieve a given level of caching performance using a much smaller amount of cache space. Finally, we consider the use of organizational boundary information as an example of the potential for use of application-level information in caching. Our results suggest that distinguishing between documents produced locally and those produced remotely can provide useful leverage in designing caching policies, because of differences in the potential for sharing these two document types among multiple users."
MARK E CROVELLA,Network kriging,"Network service providers and customers are often concerned with aggregate performance measures that span multiple network paths. Unfortunately, forming such network-wide measures can be difficult, due to the issues of scale involved. In particular, the number of paths grows too rapidly with the number of endpoints to make exhaustive measurement practical. As a result, it is of interest to explore the feasibility of methods that dramatically reduce the number of paths measured in such situations while maintaining acceptable accuracy. We cast the problem as one of statistical prediction—in the spirit of the so-called ‘kriging’ problem in spatial statistics—and show that end-to-end network properties may be accurately predicted in many cases using a surprisingly small set of carefully chosen paths. More precisely, we formulate a general framework for the prediction problem, propose a class of linear predictors for standard quantities of interest (e.g., averages, totals, differences) and show that linear algebraic methods of subset selection may be used to effectively choose which paths to measure. We characterize the performance of the resulting methods, both analytically and numerically. The success of our methods derives from the low effective rank of routing matrices as encountered in practice, which appears to be a new observation in its own right with potentially broad implications on network measurement generally."
MARK E CROVELLA,A statistical framework for efficient monitoring of end-to-end network properties,"Network service providers and customers are often concerned with aggregate performance measures that span multiple network paths. Unfortunately, forming such network-wide measures can be difficult, due to the issues of scale involved. In particular, the number of paths grows too rapidly with the number of endpoints to make exhaustive measurement practical. As a result, it is of interest to explore the feasibility of methods that dramatically reduce the number of paths measured in such situations while maintaining acceptable accuracy. In previous work we have proposed a statistical framework for efficiently addressing this problem, in the context of additive metrics such as delay and loss rate, for which the per-path metric is a sum of per-link measures (possibly under appropriate transformation). The key to our method lies in the observation and exploitation of the fact that network paths show significant redundancy (sharing of common links). In this paper we make three contributions: (1) we generalize the framework to make it more immediately applicable to network measurements encountered in practice; (2) we demonstrate that the observed path redundancy upon which our method is based is robust to variation in key network conditions and characteristics, including the presence of link failures; and (3) we show how the framework may be applied to address three practical problems of interest to network providers and customers, using data from an operating network. In particular, we show how appropriate selection of small sets of path measurements can be used to accurately estimate network-wide averages of path delays, to reliably detect network anomalies, and to effectively make a choice between alternative sub-networks, as a customer choosing between two providers or two ingress points into a provider network."
MARK E CROVELLA,Going the distance for protein function prediction: a new distance metric for protein interaction networks,"In protein-protein interaction (PPI) networks, functional similarity is often inferred based on the function of directly interacting proteins, or more generally, some notion of interaction network proximity among proteins in a local neighborhood. Prior methods typically measure proximity as the shortest-path distance in the network, but this has only a limited ability to capture fine-grained neighborhood distinctions, because most proteins are close to each other, and there are many ties in proximity. We introduce diffusion state distance (DSD), a new metric based on a graph diffusion property, designed to capture finer-grained distinctions in proximity for transfer of functional annotation in PPI networks. We present a tool that, when input a PPI network, will output the DSD distances between every pair of proteins. We show that replacing the shortest-path metric by DSD improves the performance of classical function prediction methods across the board."
JEFFREY H SAMET,A transitional opioid program to engage hospitalized drug users,"BACKGROUND: Many opioid-dependent patients do not receive care for addiction issues when hospitalized for other medical problems. Based on 3 years of clinical practice, we report the Transitional Opioid Program (TOP) experience using hospitalization as a ""reachable moment"" to identify and link opioid-dependent persons to addiction treatment from medical care. METHODS: A program nurse identified, assessed, and enrolled hospitalized, out-of-treatment opioid-dependent drug users based on their receipt of methadone during hospitalization. At discharge, patients transitioned to an outpatient interim opioid agonist program providing 30-day stabilization followed by 60-day taper. The nurse provided case management emphasizing HIV risk reduction, health education, counseling, and medical follow-up. Treatment outcomes included opioid agonist stabilization then taper or transfer to long-term opioid agonist treatment. RESULTS: From January 2002 to January 2005, 362 unique hospitalized, opioid-dependent drug users were screened; 56% (n = 203) met eligibility criteria and enrolled into the program. Subsequently, 82% (167/203) presented to the program clinic post-hospital discharge; for 59% (119/203) treatment was provided, for 26% (52/203) treatment was not provided, and for 16% (32/203) treatment was not possible (pursuit of TOP objectives precluded by medical problems, psychiatric issues, or incarceration). Program patients adhered to a spectrum of medical recommendations (e.g., obtaining prescription medications, medical follow-up). CONCLUSIONS: The Transitional Opioid Program (TOP) identified at-risk hospitalized, out-of-treatment opioid-dependent drug users and, by offering a range of treatment intensity options, engaged a majority into addiction treatment. Hospitalization can be a ""reachable moment"" to engage and link drug users into addiction treatment."
JEFFREY H SAMET,Alcohol and HIV Disease Progression: Weighing the Evidence,"Heavy alcohol use is commonplace among HIV-infected individuals; however, the extent that alcohol use adversely impacts HIV disease progression has not been fully elucidated. Fairly strong evidence suggests that heavy alcohol consumption results in behavioral and biological processes that likely increase HIV disease progression, and experimental evidence of the biological effect of heavy alcohol on simian immunodeficiency virus in macaques is quite suggestive. However, several observational studies of the effect of heavy alcohol consumption on HIV progression conducted in the 1990s found no association of heavy alcohol consumption with time to AIDS diagnosis, while some more recent studies showed associations of heavy alcohol consumption with declines of CD4 cell counts and nonsuppression of HIV viral load. We discuss several plausible biological and behavioral mechanisms by which alcohol may cause HIV disease progression, evidence from prospective observational human studies, and suggest future research to further illuminate this important issue."
JEFFREY H SAMET,Translational Methods in Biostatistics: Linear Mixed Effect Regression Models of Alcohol Consumption and HIV Disease Progression Over Time,"Longitudinal studies are helpful in understanding how subtle associations between factors of interest change over time. Our goal is to apply statistical methods which are appropriate for analyzing longitudinal data to a repeated measures epidemiological study as a tutorial in the appropriate use and interpretation of random effects models. To motivate their use, we study the association of alcohol consumption on markers of HIV disease progression in an observational cohort. To make valid inferences, the association among measurements correlated within a subject must be taken into account. We describe a linear mixed effects regression framework that accounts for the clustering of longitudinal data and that can be fit using standard statistical software. We apply the linear mixed effects model to a previously published dataset of HIV infected individuals with a history of alcohol problems who are receiving HAART (n = 197). The researchers were interested in determining the effect of alcohol use on HIV disease progression over time. Fitting a linear mixed effects multiple regression model with a random intercept and random slope for each subject accounts for the association of observations within subjects and yields parameters interpretable as in ordinary multiple regression. A significant interaction between alcohol use and adherence to HAART is found: subjects who use alcohol and are not fully adherent to their HIV medications had higher log RNA (ribonucleic acid) viral load levels than fully adherent non-drinkers, fully adherent alcohol users, and non-drinkers who were not fully adherent. Longitudinal studies are increasingly common in epidemiological research. Software routines that account for correlation between repeated measures using linear mixed effects methods are now generally available and straightforward to utilize. These models allow the relaxation of assumptions needed for approaches such as repeated measures ANOVA, and should be routinely incorporated into the analysis of cohort studies."
JEFFREY H SAMET,Implications of Cannabis Use and Heavy Alcohol Use on HIV Drug Risk Behaviors in Russian Heroin Users,"Cannabis and heavy alcohol use potentially increase HIV transmission by increasing risky drug behaviors. We studied 404 subjects entering treatment for heroin dependence, in St. Petersburg, Russia. We used the HIV Risk Assessment Battery (RAB) drug subscale to measure risky drug behavior. Although all heavy alcohol users had risky drug behaviors, their drug RAB scores did not differ from non-heavy alcohol users in unadjusted or adjusted analyses. Cannabis use was significantly associated with drug RAB scores in unadjusted analyses (mean difference 1.7 points) and analyses adjusted for age, sex, and employment (mean difference 1.3 points). When also adjusting for stimulant use, the impact of cannabis use was attenuated and no longer statistically significant (mean difference 1.1 points). Because of the central role of risky drug behaviors in the Russian HIV epidemic, it is important to understand how the use of multiple substances, including cannabis and alcohol, impacts risky drug behaviors."
JEFFREY H SAMET,Treating Homeless Opioid Dependent Patients with Buprenorphine in an Office-Based Setting,"CONTEXT Although office-based opioid treatment with buprenorphine (OBOT-B) has been successfully implemented in primary care settings in the US, its use has not been reported in homeless patients. OBJECTIVE To characterize the feasibility of OBOT-B in homeless relative to housed patients. DESIGN A retrospective record review examining treatment failure, drug use, utilization of substance abuse treatment services, and intensity of clinical support by a nurse care manager (NCM) among homeless and housed patients in an OBOT-B program between August 2003 and October 2004. Treatment failure was defined as elopement before completing medication induction, discharge after medication induction due to ongoing drug use with concurrent nonadherence with intensified treatment, or discharge due to disruptive behavior. RESULTS Of 44 homeless and 41 housed patients enrolled over 12 months, homeless patients were more likely to be older, nonwhite, unemployed, infected with HIV and hepatitis C, and report a psychiatric illness. Homeless patients had fewer social supports and more chronic substance abuse histories with a 3- to 6-fold greater number of years of drug use, number of detoxification attempts and percentage with a history of methadone maintenance treatment. The proportion of subjects with treatment failure for the homeless (21%) and housed (22%) did not differ (P=.94). At 12 months, both groups had similar proportions with illicit opioid use [Odds ratio (OR), 0.9 (95% CI, 0.5–1.7) P=.8], utilization of counseling (homeless, 46%; housed, 49%; P=.95), and participation in mutual-help groups (homeless, 25%; housed, 29%; P=.96). At 12 months, 36% of the homeless group was no longer homeless. During the first month of treatment, homeless patients required more clinical support from the NCM than housed patients. CONCLUSIONS Despite homeless opioid dependent patients' social instability, greater comorbidities, and more chronic drug use, office-based opioid treatment with buprenorphine was effectively implemented in this population comparable to outcomes in housed patients with respect to treatment failure, illicit opioid use, and utilization of substance abuse treatment."
JEFFREY H SAMET,HIV pre-exposure prophylaxis and buprenorphine at a drug detoxification center during the opioid epidemic: opportunities and challenges,"Human immunodeficiency virus (HIV) pre-exposure prophylaxis (PrEP) and buprenorphine decrease HIV acquisition. Between November, 2016 - July, 2017, we surveyed persons (N=200) at a drug detoxification center to assess their interest in PrEP and in buprenorphine, and to examine factors associated with such interests. Participants had a mean [SD] age of 39 [10] years. Over the previous 6 months, 58% (117/200) injected drugs and 50% (85/171) had condomless sex. Only 22% (26/117) of persons who injected drugs were aware of PrEP, yet 74% (86/116) and 72% (84/116) were interested in oral or injectable PrEP, respectively. Thirty-eight percent (47/125) of persons not receiving buprenorphine or methadone expressed interest in buprenorphine. After multivariable adjustment, Latinx ethnicity was associated with interest in PrEP (aOR: 3.80; 95% CI, 1.37-10.53), while male gender (aOR: 2.76; 95% CI, 1.21-6.34) was associated with interest in buprenorphine. Opportunities exist to implement PrEP and buprenorphine within drug detoxification centers."
JEFFREY H SAMET,Episodic homelessness and health care utilization in a prospective cohort of HIV-infected persons with alcohol problems,"BACKGROUND: Because individuals with HIV/AIDS often have complex medical and social needs, the impact of housing status on medical service utilization is difficult to isolate from the impact of conditions that may worsen during periods of homelessness such as depression and substance abuse. We examine whether episodes of homelessness are independently associated with suboptimal medical utilization even when accounting for concurrent addiction severity and depression. METHODS: We used data from a 30-month cohort of patients with HIV/AIDS and alcohol problems. Housing status, utilization (ambulatory visits, emergency department (ED) visits, and hospitalizations) and other features were assessed with standardized research interviews at 6-month intervals. Multivariable longitudinal regression models calculated incidence rate ratios (IRR) comparing utilization rates during 6-month intervals (homeless versus housed). Additional models assessed whether addiction severity and depressive symptoms could account for utilization differences. RESULTS: Of the 349 subjects, 139 (39%) reported homelessness at least once during the study period; among these subjects, the median number of nights homeless per 6-month interview period was 30. Homelessness was associated with higher ED utilization (IRR = 2.17; 95% CI = 1.72–2.74) and hospitalizations (IRR = 2.30; 1.70–3.12), despite no difference in ambulatory care utilization (IRR = 1.09; 0.89–1.33). These associations were attenuated but remained significant when adjusting for addiction severity and depressive symptoms. CONCLUSION: In patients with HIV/AIDS and alcohol problems, efforts to improve housing stability may help to mitigate intensive medical utilization patterns."
DAVID ROHRLICH,Dihedral Artin representations and CM fields,"For a fixed CM field K with maximal totally real subfield F, we consider dihedral Artin representations of F induced from K. We prove that a positive proportion of such representations have image D4."
DAVID ROHRLICH,Quaternionic Artin representations and nontraditional arithmetic statistics,"We classify and then attempt to count the real quadratic fields (ordered by the size of the totally positive fundamental unit, as in Sarnak [14], [15]) from which quaternionic Artin representations of minimal conductor can be induced. Some of our results can be interpreted as criteria for a real quadratic field to be contained in a Galois extension of Q with controlled ramification and Galois group isomorphic to a generalized quaternion group."
HAROLD S PARK,Forward and inverse design of kirigami via supervised autoencoder,"Machine learning (ML) methods have recently been used as forward solvers to predict the mechanical properties of composite materials. Here, we use a supervised autoencoder (SAE) to perform the inverse design of graphene kirigami, where predicting the ultimate stress or strain under tensile loading is known to be difficult due to nonlinear effects arising from the out-of-plane buckling. Unlike the standard autoencoder, our SAE is able not only to reconstruct cut configurations but also to predict the mechanical properties of graphene kirigami and classify the kirigami with either parallel or orthogonal cuts. By interpolating in the latent space of kirigami structures, the SAE is able to generate designs that mix parallel and orthogonal cuts, despite being trained independently on parallel or orthogonal cuts. Our method allows us to both identify alternate designs and predict, with reasonable accuracy, their mechanical properties, which is crucial for expanding the search space for materials design."
HAROLD S PARK,Towards out of distribution generalization for problems in mechanics,
HAROLD S PARK,Elastic instabilities govern the morphogenesis of the optic cup,"Because the normal operation of the eye depends on sensitive morphogenetic processes for its eventual shape, developmental flaws can lead to wide-ranging ocular defects. However, the physical processes and mechanisms governing ocular morphogenesis are not well understood. Here, using analytical theory and nonlinear shell finite-element simulations, we show, for optic vesicles experiencing matrix-constrained growth, that elastic instabilities govern the optic cup morphogenesis. By capturing the stress amplification owing to mass increase during growth, we show that the morphogenesis is driven by two elastic instabilities analogous to the snap through in spherical shells, where the second instability is sensitive to the optic cup geometry. In particular, if the optic vesicle is too slender, it will buckle and break axisymmetry, thus, preventing normal development. Our results shed light on the morphogenetic mechanisms governing the formation of a functional biological system and the role of elastic instabilities in the shape selection of soft biological structures."
HAROLD S PARK,Efficient snap-through of spherical caps by applying a localized curvature stimulus,"In bistable actuators and other engineered devices, a homogeneous stimulus (e.g., mechanical, chemical, thermal, or magnetic) is often applied to an entire shell to initiate a snap-through instability. In this work, we demonstrate that restricting the active area to the shell boundary allows for a large reduction in its size, thereby decreasing the energy input required to actuate the shell. To do so, we combine theory with 1D finite element simulations of spherical caps with a non-homogeneous distribution of stimulus-responsive material. We rely on the effective curvature stimulus, i.e., the natural curvature induced by the non-mechanical stimulus, which ensures that our results are entirely stimulus-agnostic. To validate our numerics and demonstrate this generality, we also perform two sets of experiments, wherein we use residual swelling of bilayer silicone elastomers-a process that mimics differential growth-as well as a magneto-elastomer to induce curvatures that cause snap-through. Our results elucidate the underlying mechanics, offering an intuitive route to optimal design for efficient snap-through."
HAROLD S PARK,Kirigami actuators,"Thin elastic sheets bend easily and, if they are patterned with cuts, can deform in sophisticated ways. Here we show that carefully tuning the location and arrangement of cuts within thin sheets enables the design of mechanical actuators that scale down to atomically-thin 2D materials. We first show that by understanding the mechanics of a single non-propagating crack in a sheet, we can generate four fundamental forms of linear actuation: roll, pitch, yaw, and lift. Our analytical model shows that these deformations are only weakly dependent on thickness, which we confirm with experiments on centimeter-scale objects and molecular dynamics simulations of graphene and MoS₂ nanoscale sheets. We show how the interactions between non-propagating cracks can enable either lift or rotation, and we use a combination of experiments, theory, continuum computational analysis, and molecular dynamics simulations to provide mechanistic insights into the geometric and topological design of kirigami actuators."
HAROLD S PARK,Atomistic simulations of tension-induced large deformation and stretchability in graphene kirigami,"Graphene's exceptional mechanical properties, including its highest-known stiffness (1 TPa) and strength (100 GPa), have been exploited for various structural applications. However, graphene is also known to be quite brittle, with experimentally measured tensile fracture strains that do not exceed a few percent. In this work, we introduce the notion of graphene kirigami, where concepts that have been used almost exclusively for macroscale structures are applied to dramatically enhance the stretchability of both zigzag and armchair graphene. Specifically, we show using classical molecular-dynamics simulations that the yield and fracture strains of graphene can be enhanced by about a factor of 3 using kirigami as compared to standard monolayer graphene. Finally, we demonstrate that this enhanced ductility in graphene may open up interesting opportunities in coupling to graphene's electronic behavior."
HAROLD S PARK,"Strain-induced gauge and Rashba fields in ferroelectric Rashba lead chalcogenide PbX monolayers (X = S, Se, Te)","One of the exciting features of two-dimensional (2D) materials is their electronic and optical tunability through strain engineering. Previously, we found a class of 2D ferroelectric Rashba semiconductors PbX (X=S, Se, Te) with tunable spin-orbital properties. In this work, based on our previous tight-binding (TB) results, we derive an effective low-energy Hamiltonian around the symmetry points that captures the effects of strain on the electronic properties of PbX. We find that strains induce gauge fields which shift the Rashba point and modify the Rashba parameter. This effect is equivalent to the application of in-plane magnetic fields. The out-of-plane strain, which is proportional to the electric polarization, is also shown to modify the Rashba parameter. Overall, our theory connects strain and spin splitting in ferroelectric Rashba materials, which will be important to understand the strain-induced variations in local Rashba parameters that will occur in practical applications."
HAROLD S PARK,Two-dimensional square buckled Rashba lead chalcogenides,"We propose the lead sulphide (PbS) monolayer as a two-dimensional semiconductor with a large Rashba-like spin-orbit effect controlled by the out-of-plane buckling. The buckled PbS conduction band is found to possess Rashba-like dispersion and spin texture at the M and Γ points, with large effective Rashba parameters of λ∼5 eV Å and λ∼1 eV Å, respectively. Using a tight-binding formalism, we show that the Rashba effect originates from the very large spin-orbit interaction and the hopping term that mixes the in-plane and out-of-plane p orbitals of Pb and S atoms. The latter, which depends on the buckling angle, can be controlled by applying strain to vary the spin texture as well as the Rashba parameter at Γ and M. Our density functional theory results together with tight-binding formalism provide a unifying framework for designing Rashba monolayers and for manipulating their spin properties."
HAROLD S PARK,Accelerated search and design of stretchable graphene kirigami using machine learning,"Making kirigami-inspired cuts into a sheet has been shown to be an effective way of designing stretchable materials with metamorphic properties where the 2D shape can transform into complex 3D shapes. However, finding the optimal solutions is not straightforward as the number of possible cutting patterns grows exponentially with system size. Here, we report on how machine learning (ML) can be used to approximate the target properties, such as yield stress and yield strain, as a function of cutting pattern. Our approach enables the rapid discovery of kirigami designs that yield extreme stretchability as verified by molecular dynamics (MD) simulations. We find that convolutional neural networks, commonly used for classification in vision tasks, can be applied for regression to achieve an accuracy close to the precision of the MD simulations. This approach can then be used to search for optimal designs that maximize elastic stretchability with only 1000 training samples in a large design space of ∼4×106 candidate designs. This example demonstrates the power and potential of ML in finding optimal kirigami designs at a fraction of iterations that would be required of a purely MD or experiment-based approach, where no prior knowledge of the governing physics is known or available."
HAROLD S PARK,Graphene kirigami as a platform for stretchable and tunable quantum dot arrays,"The quantum transport properties of a graphene kirigami similar to those studied in recent experiments are calculated in the regime of elastic, reversible deformations. Our results show that, at low electronic densities, the conductance profile of such structures replicates that of a system of coupled quantum dots, characterized by a sequence of minibands and stopgaps. The conductance and I-V curves have different characteristics in the distinct stages of deformation that characterize the elongation of these structures. Notably, the effective coupling between localized states is strongly reduced in the small elongation stage but revived at large elongations that allow the reestablishment of resonant tunneling across the kirigami. This provides an interesting example of interplay between geometry, strain, spatial confinement, and electronic transport. The alternating miniband and stopgap structure in the transmission leads to I-V characteristics with negative differential conductance in well defined energy/doping ranges. These effects should be stable in a realistic scenario that includes edge roughness and Coulomb interactions, as these are expected to further promote localization of states at low energies in narrow segments of graphene nanostructures."
HAROLD S PARK,Highly stretchable MoS2 kirigami,"We report the results of classical molecular dynamics simulations focused on studying the mechanical properties of MoS2 kirigami. Several different kirigami structures were studied based upon two simple non-dimensional parameters, which are related to the density of cuts, as well as the ratio of the overlapping cut length to the nanoribbon length. Our key findings are significant enhancements in tensile yield (by a factor of four) and fracture strains (by a factor of six) as compared to pristine MoS2 nanoribbons. These results, in conjunction with recent results on graphene, suggest that the kirigami approach may be generally useful for enhancing the ductility of two-dimensional nanomaterials."
HAROLD S PARK,Mechanical MNIST – Distribution Shift,"The Mechanical MNIST – Distribution Shift dataset contains the results of finite element simulation of heterogeneous material subject to large deformation due to equibiaxial extension at a fixed boundary displacement of d = 7.0. The result provided in this dataset is the change in strain energy after this equibiaxial extension. The Mechanical MNIST dataset is generated by converting the MNIST bitmap images (28x28 pixels) with range 0 - 255 to 2D heterogeneous blocks of material (28x28 unit square) with varying modulus in range 1- s. The original bitmap images are sourced from the MNIST Digits dataset, (http://www.pymvpa.org/datadb/mnist.html) which corresponds to Mechanical MNIST – MNIST, and the EMNIST Letters dataset (https://www.nist.gov/itl/products-and-services/emnist-dataset) which correspond to Mechanical MNIST – EMNIST Letters. The Mechanical MNIST – Distribution Shift dataset is specifically designed to demonstrate three types of data distribution shift: (1) covariate shift, (2) mechanism shift, and (3) sampling bias, for all of which the training and testing environments are drawn from different distributions. For each type of data distribution shift, we have one dataset generated from the Mechanical MNIST bitmaps and one from the Mechanical MNIST – EMNIST Letters bitmaps. For the covariate shift dataset, the training dataset is collected from two environments (2500 samples from s = 100, and 2500 samples from s = 90), and the test data is collected from two additional environments (2000 samples from s = 75, and 2000 samples from s = 50). For the mechanism shift dataset, the training data is identical to the training data in the covariate shift dataset (i.e., 2500 samples from s = 100, and 2500 samples from s = 90), and the test datasets are from two additional environments (2000 samples from s = 25, and 2000 samples from s = 10).  For the sampling bias dataset, datasets are collected such that each datapoint is selected from the broader MNIST and EMNIST inputs bitmap selection by a probability which is controlled by a parameter r. The training data is collected from two environments (9800 from r = 15, and 200 from r = -2), and the test data is collected from three different environments (2000 from r = -5, 2000 from r = -10, and 2000 from r = 1).  Thus, in the end we have 6 benchmark datasets with multiple training and testing environments in each. The enclosed document “folder_description.pdf'” shows the organization of each zipped folder provided on this page. The code to reproduce these simulations is available on GitHub (https://github.com/elejeune11/Mechanical-MNIST/blob/master/generate_dataset/Equibiaxial_Extension_FEA_test_FEniCS.py). "
HAROLD S PARK,"Erratum: “stellar diameters and temperatures. II. main-sequence K- and M-stars” (2012, ApJ, 757, 112)",
HAROLD S PARK,A review on mechanics and mechanical properties of 2D materials—graphene and beyond,"Since the first successful synthesis of graphene just over a decade ago, a variety of two-dimensional (2D) materials (e.g., transition metal-dichalcogenides, hexagonal boron-nitride, etc.) have been discovered. Among the many unique and attractive properties of 2D materials, mechanical properties play important roles in manufacturing, integration and performance for their potential applications. Mechanics is indispensable in the study of mechanical properties, both experimentally and theoretically. The coupling between the mechanical and other physical properties (thermal, electronic, optical) is also of great interest in exploring novel applications, where mechanics has to be combined with condensed matter physics to establish a scalable theoretical framework. Moreover, mechanical interactions between 2D materials and various substrate materials are essential for integrated device applications of 2D materials, for which the mechanics of interfaces (adhesion and friction) has to be developed for the 2D materials. Here we review recent theoretical and experimental works related to mechanics and mechanical properties of 2D materials. While graphene is the most studied 2D material to date, we expect continual growth of interest in the mechanics of other 2D materials beyond graphene."
HAROLD S PARK,Memory effects in monolayer group-IV monochalcoginides,"Group-IV monochalcogenides are a family of two-dimensional puckered materials with an orthorhombic structure that is comprised of polar layers. In this article, we use first principles calculations to show the multistability of monolayer SnS and GeSe, two prototype materials where the direction of the puckering can be switched by application of tensile stress or electric field. Furthermore, the two inequivalent valleys in momentum space, which are dictated by the puckering orientation, can be excited selectively using linearly polarized light, and this provides an additional tool to identify the polarization direction. Our findings suggest that SnS and GeSe monolayers may have observable ferroelectricity and multistability, with potential applications in information storage."
HAROLD S PARK,Stimuli-responsive shell theory,"Soft matter mechanics generally involve finite deformations and instabilities of structures in response to a wide range of mechanical and non-mechanical stimuli. Modeling plates and shells is generally a challenge due to their geometrically nonlinear response to loads; however, non-mechanical loads further complicate matters as it is often not clear how they modify the shell’s energy functional. In this work, we demonstrate how to form a mechanical interpretation of these non-mechanical stimuli, in which the standard shell strain energy can be augmented with potentials corresponding to how a non-mechanical stimulus acts to change the shell’s area and curvature via the natural stretch and curvature. As a result, the effect of non-mechanical stimuli to deform shells is transformed into effective external loadings, and this framework allows for the application of analytical and computational tools that are standard within the mechanics community. Furthermore, we generalize the effect of mass change during biological growth to account for its effect on the stress constitution. The theory is formulated based on a standard, stress-free reference configuration which is known a priori, meaning it can be physically observed, and only requires the solution of a single-field equation, the standard mechanical momentum or equilibrium equation, despite capturing the effects of non-mechanical stimuli. We validate the performance of this model by several benchmark problems, and finally, we apply it to complex examples, including the snapping of the Venus flytrap, leaf growth, and the buckling of electrically active polymer plates. Overall, we expect that mechanicians and non-mechanicians alike can use the approach presented here to quickly modify existing computational tools with effective external loadings calculated in this novel theory to study how various types of non-mechanical stimuli impact the mechanics and physics of thin shell structures."
TAREK HASSAN,Forward and spot exchange rates in a multi-currency world,"Separate literatures study violations of uncovered interest parity using regression-based and portfolio-based methods. We propose a decomposition of these violations into a cross-currency, a between-time-and-currency, and a cross-time component that allows us to analytically relate regression-based and portfolio-based anomalies, to test whether they are empirically distinct, and to estimate the joint restrictions they place on models of currency returns. We find that the forward premium puzzle (FPP) and the ""dollar trade"" anomaly are intimately linked. Both anomalies are almost exclusively driven by the cross-time component. By contrast, the ""carry trade"" anomaly is driven largely by the cross-currency component. The simplest model that the data do not reject features a highly persistent asymmetry that makes some currencies pay higher expected returns than others, and a more elastic expected return on the US dollar than on other currencies. In addition, we never reject the hypothesis that currencies with high interest rates are expected to depreciate rather than appreciate, so that none of our estimates require a systematic association between currency risk premia and predictable movements in exchange rates."
TAREK HASSAN,The power of the street: evidence from Egypt's Arab Spring,"Unprecedented street protests brought down Mubarak’s government and ushered in an era of competition between three rival political groups in Egypt. Using daily variation in the number of protesters, we document that more intense protests are associated with lower stock market valuations for firms connected to the group currently in power relative to non-connected firms, but have no impact on the relative valuations of firms connected to rival groups. These results suggest that street protests serve as a partial check on political rent-seeking. General discontent expressed on Twitter predicts protests but has no direct effect on valuations."
TAREK HASSAN,The geography of new technologies,"We identify novel technologies using textual analysis of earnings conference calls, newspapers, announcements, and patents. Our approach enables us to document the rollout of 20 new technologies across firms and labor markets in the U.S. Four stylized facts emerge from our data. First, as technologies develop, the number of new positions related to them grows, but the average education requirements and wage levels of the positions drop. Second, as technologies develop, their employment impact diffuses across the country: initially, technologies are concentrated in local hubs, but over time, their adoption diffuses geographically. Third, despite this diffusion, the initial hubs retain a disproportionate share employment in the technology, particularly at the high-skill end of the spectrum. Finally, technology hubs are more likely to arise in areas with universities and high skilled labor pools."
TAREK HASSAN,The economics of currency risk,"This article reviews the literature on currency and country risk with a focus on their macroeconomic origins and implications. A growing body of evidence shows that countries with safer currencies enjoy persistently lower interest rates and a lower required return to capital. As a result, they accumulate relatively more capital than countries with currencies that international investors perceive as risky. Whereas earlier research focused mainly on the role of currency risk in generating violations of uncovered interest parity and other financial anomalies, more recent evidence points to important implications for the allocation of capital across countries, the efficacy of exchange rate stabilization policies, the sustainability of trade deficits, and the spillovers of shocks across international borders."
TAREK HASSAN,"Firm-level exposure to epidemic diseases: COVID-19, SARS, and H₁N₁","We construct text-based measures of the primary concerns listed firms associated with the spread of COVID-19 and other epidemic diseases. We identify which firms perceive to lose or gain from a given epidemic and textually decompose the epidemic’s effect on the firm’s demand and supply. We find that the effects of COVID-19 manifest as a simultaneous shock to demand and supply, with both shocks affecting firms’ market valuations in equal measure on average. By contrast, demand-related impacts appear more important in accounting for the observed collapse in firm-level investment during the COVID-19 crisis."
JOHN A WHITE,Membrane properties and the balance between excitation and inhibition control gamma-frequency oscillations arising from feedback inhibition,"Computational studies as well as in vivo and in vitro results have shown that many cortical neurons fire in a highly irregular manner and at low average firing rates. These patterns seem to persist even when highly rhythmic signals are recorded by local field potential electrodes or other methods that quantify the summed behavior of a local population. Models of the 30–80 Hz gamma rhythm in which network oscillations arise through ‘stochastic synchrony’ capture the variability observed in the spike output of single cells while preserving network-level organization. We extend upon these results by constructing model networks constrained by experimental measurements and using them to probe the effect of biophysical parameters on network-level activity. We find in simulations that gamma-frequency oscillations are enabled by a high level of incoherent synaptic conductance input, similar to the barrage of noisy synaptic input that cortical neurons have been shown to receive in vivo. This incoherent synaptic input increases the emergent network frequency by shortening the time scale of the membrane in excitatory neurons and by reducing the temporal separation between excitation and inhibition due to decreased spike latency in inhibitory neurons. These mechanisms are demonstrated in simulations and in vitro current-clamp and dynamic-clamp experiments. Simulation results further indicate that the membrane potential noise amplitude has a large impact on network frequency and that the balance between excitatory and inhibitory currents controls network stability and sensitivity to external inputs."
JOHN A WHITE,Imaging activity in astrocytes and neurons with genetically encoded calcium indicators following in utero electroporation,"Complex interactions between networks of astrocytes and neurons are beginning to be appreciated, but remain poorly understood. Transgenic mice expressing fluorescent protein reporters of cellular activity, such as the GCaMP family of genetically encoded calcium indicators (GECIs), have been used to explore network behavior. However, in some cases, it may be desirable to use long-established rat models that closely mimic particular aspects of human conditions such as Parkinson's disease and the development of epilepsy following status epilepticus. Methods for expressing reporter proteins in the rat brain are relatively limited. Transgenic rat technologies exist but are fairly immature. Viral-mediated expression is robust but unstable, requires invasive injections, and only works well for fairly small genes (<5 kb). In utero electroporation (IUE) offers a valuable alternative. IUE is a proven method for transfecting populations of astrocytes and neurons in the rat brain without the strict limitations on transgene size. We built a toolset of IUE plasmids carrying GCaMP variants 3, 6s, or 6f driven by CAG and targeted to the cytosol or the plasma membrane. Because low baseline fluorescence of GCaMP can hinder identification of transfected cells, we included the option of co-expressing a cytosolic tdTomato protein. A binary system consisting of a plasmid carrying a piggyBac inverted terminal repeat (ITR)-flanked CAG-GCaMP-IRES-tdTomato cassette and a separate plasmid encoding for expression of piggyBac transposase was employed to stably express GCaMP and tdTomato. The plasmids were co-electroporated on embryonic days 13.5–14.5 and astrocytic and neuronal activity was subsequently imaged in acute or cultured brain slices prepared from the cortex or hippocampus. Large spontaneous transients were detected in slices obtained from rats of varying ages up to 127 days. In this report, we demonstrate the utility of this toolset for interrogating astrocytic and neuronal activity in the rat brain."
JOHN A WHITE,Nonlinear properties of medial entorhinal cortex neurons reveal frequency selectivity during multi-sinusoidal stimulation,"The neurons in layer II of the medial entorhinal cortex are part of the grid cell network involved in the representation of space. Many of these neurons are likely to be stellate cells with specific oscillatory and firing properties important for their function. A fundamental understanding of the nonlinear basis of these oscillatory properties is critical for the development of theories of grid cell firing. In order to evaluate the behavior of stellate neurons, measurements of their quadratic responses were used to estimate a second order Volterra kernel. This paper uses an operator theory, termed quadratic sinusoidal analysis (QSA), which quantitatively determines that the quadratic response accounts for a major part of the nonlinearity observed at membrane potential levels characteristic of normal synaptic events. Practically, neurons were probed with multi-sinusoidal stimulations to determine a Hermitian operator that captures the quadratic function in the frequency domain. We have shown that the frequency content of the stimulation plays an important role in the characteristics of the nonlinear response, which can distort the linear response as well. Stimulations with enhanced low frequency amplitudes evoked a different nonlinear response than broadband profiles. The nonlinear analysis was also applied to spike frequencies and it was shown that the nonlinear response of subthreshold membrane potential at resonance frequencies near the threshold is similar to the nonlinear response of spike trains."
JOHN A WHITE,The Event Horizon general relativistic magnetohydrodynamic code comparison project,"Recent developments in compact object astrophysics, especially the discovery of merging neutron stars by LIGO, the imaging of the black hole in M87 by the Event Horizon Telescope, and high- precision astrometry of the Galactic Center at close to the event horizon scale by the GRAVITY experiment motivate the development of numerical source models that solve the equations of general relativistic magnetohydrodynamics (GRMHD). Here we compare GRMHD solutions for the evolution of a magnetized accretion flow where turbulence is promoted by the magnetorotational instability from a set of nine GRMHD codes: Athena++, BHAC, Cosmos++, ECHO, H-AMR, iharm3D, HARM-Noble, IllinoisGRMHD, and KORAL. Agreement among the codes improves as resolution increases, as measured by a consistently applied, specially developed set of code performance metrics. We conclude that the community of GRMHD codes is mature, capable, and consistent on these test problems."
JOHN A WHITE,First Sagittarius A* Event Horizon Telescope results. V. Testing astrophysical models of the galactic center black hole,"In this paper we provide a first physical interpretation for the Event Horizon Telescope's (EHT) 2017 observations of Sgr A*. Our main approach is to compare resolved EHT data at 230 GHz and unresolved non-EHT observations from radio to X-ray wavelengths to predictions from a library of models based on time-dependent general relativistic magnetohydrodynamics simulations, including aligned, tilted, and stellar-wind-fed simulations; radiative transfer is performed assuming both thermal and nonthermal electron distribution functions. We test the models against 11 constraints drawn from EHT 230 GHz data and observations at 86 GHz, 2.2 μm, and in the X-ray. All models fail at least one constraint. Light-curve variability provides a particularly severe constraint, failing nearly all strongly magnetized (magnetically arrested disk (MAD)) models and a large fraction of weakly magnetized models. A number of models fail only the variability constraints. We identify a promising cluster of these models, which are MAD and have inclination i ≤ 30°. They have accretion rate (5.2–9.5) × 10−9 M ⊙ yr−1, bolometric luminosity (6.8–9.2) × 1035 erg s−1, and outflow power (1.3–4.8) × 1038 erg s−1. We also find that all models with i ≥ 70° fail at least two constraints, as do all models with equal ion and electron temperature; exploratory, nonthermal model sets tend to have higher 2.2 μm flux density; and the population of cold electrons is limited by X-ray constraints due to the risk of bremsstrahlung overproduction. Finally, we discuss physical and numerical limitations of the models, highlighting the possible importance of kinetic effects and duration of the simulations."
JOHN A WHITE,The Mechanism of Abrupt Transition between Theta and Hyper-Excitable Spiking Activity in Medial Entorhinal Cortex Layer II Stellate Cells,"Recent studies have shown that stellate cells (SCs) of the medial entorhinal cortex become hyper-excitable in animal models of temporal lobe epilepsy. These studies have also demonstrated the existence of recurrent connections among SCs, reduced levels of recurrent inhibition in epileptic networks as compared to control ones, and comparable levels of recurrent excitation among SCs in both network types. In this work, we investigate the biophysical and dynamic mechanism of generation of the fast time scale corresponding to hyper-excitable firing and the transition between theta and fast firing frequency activity in SCs. We show that recurrently connected minimal networks of SCs exhibit abrupt, threshold-like transition between theta and hyper-excitable firing frequencies as the result of small changes in the maximal synaptic (AMPAergic) conductance. The threshold required for this transition is modulated by synaptic inhibition. Similar abrupt transition between firing frequency regimes can be observed in single, self-coupled SCs, which represent a network of recurrently coupled neurons synchronized in phase, but not in synaptically isolated SCs as the result of changes in the levels of the tonic drive. Using dynamical systems tools (phase-space analysis), we explain the dynamic mechanism underlying the genesis of the fast time scale and the abrupt transition between firing frequency regimes, their dependence on the intrinsic SC's currents and synaptic excitation. This abrupt transition is mechanistically different from others observed in similar networks with different cell types. Most notably, there is no bistability involved. 'In vitro' experiments using single SCs self-coupled with dynamic clamp show the abrupt transition between firing frequency regimes, and demonstrate that our theoretical predictions are not an artifact of the model. In addition, these experiments show that high-frequency firing is burst-like with a duration modulated by an M-current."
JOHN A WHITE,GenNet: a platform for hybrid network experiments.,"We describe General Network (GenNet), a software plugin for the real time experimental interface (RTXI) dynamic clamp system that allows for straightforward and flexible implementation of hybrid network experiments. This extension to RTXI allows for hybrid networks that contain an arbitrary number of simulated and real neurons, significantly improving upon previous solutions that were limited, particularly by the number of cells supported. The benefits of this system include the ability to rapidly and easily set up and perform scalable experiments with hybrid networks and the ability to scan through ranges of parameters. We present instructions for installing, running and using GenNet for hybrid network experiments and provide several example uses of the system."
JOHN A WHITE,Hippocampal cells segregate positive and negative engrams,"The hippocampus is involved in processing a variety of mnemonic computations specifically the spatiotemporal components and emotional dimensions of contextual memory. Recent studies have demonstrated cellular heterogeneity along the hippocampal axis. The ventral hippocampus has been shown to be important in the processing of emotion and valence. Here, we combine transgenic and all-virus based activity-dependent tagging strategies to visualize multiple valence-specific engrams in the vHPC and demonstrate two partially segregated cell populations and projections that respond to appetitive and aversive experiences. Next, using RNA sequencing and DNA methylation sequencing approaches, we find that vHPC appetitive and aversive engram cells display different transcriptional programs and DNA methylation landscapes compared to a neutral engram population. Additionally, optogenetic manipulation of tagged cell bodies in vHPC is not sufficient to drive appetitive or aversive behavior in real-time place preference, stimulation of tagged vHPC terminals projecting to the amygdala and nucleus accumbens (NAc), but not the prefrontal cortex (PFC), showed the capacity drive preference and avoidance. These terminals also were able to change their capacity to drive behavior. We conclude that the vHPC contains genetically, cellularly, and behaviorally segregated populations of cells processing appetitive and aversive memory engrams."
JOHN A WHITE,3D printing of liquid crystal elastomeric actuators with spatially programed nematic order,"Liquid crystal elastomers (LCEs) are soft materials capable of large, reversible shape changes, which may find potential application as artificial muscles, soft robots, and dynamic functional architectures. Here, the design and additive manufacturing of LCE actuators (LCEAs) with spatially programed nematic order that exhibit large, reversible, and repeatable contraction with high specific work capacity are reported. First, a photopolymerizable, solvent-free, main-chain LCE ink is created via aza-Michael addition with the appropriate viscoelastic properties for 3D printing. Next, high operating temperature direct ink writing of LCE inks is used to align their mesogen domains along the direction of the print path. To demonstrate the power of this additive manufacturing approach, shape-morphing LCEA architectures are fabricated, which undergo reversible planar-to-3D and 3D-to-3D′ transformations on demand, that can lift significantly more weight than other LCEAs reported to date."
JOHN A WHITE,First Sagittarius A* Event Horizon Telescope results. I. The shadow of the supermassive black hole in the center of the Milky Way,"We present the first Event Horizon Telescope (EHT) observations of Sagittarius A* (Sgr A*), the Galactic center source associated with a supermassive black hole. These observations were conducted in 2017 using a global interferometric array of eight telescopes operating at a wavelength of λ = 1.3 mm. The EHT data resolve a compact emission region with intrahour variability. A variety of imaging and modeling analyses all support an image that is dominated by a bright, thick ring with a diameter of 51.8 ± 2.3 μas (68% credible interval). The ring has modest azimuthal brightness asymmetry and a comparatively dim interior. Using a large suite of numerical simulations, we demonstrate that the EHT images of Sgr A* are consistent with the expected appearance of a Kerr black hole with mass ∼4 × 106 M ⊙, which is inferred to exist at this location based on previous infrared observations of individual stellar orbits, as well as maser proper-motion studies. Our model comparisons disfavor scenarios where the black hole is viewed at high inclination (i &gt; 50°), as well as nonspinning black holes and those with retrograde accretion disks. Our results provide direct evidence for the presence of a supermassive black hole at the center of the Milky Way, and for the first time we connect the predictions from dynamical measurements of stellar orbits on scales of 103–105 gravitational radii to event-horizon-scale images and variability. Furthermore, a comparison with the EHT results for the supermassive black hole M87* shows consistency with the predictions of general relativity spanning over three orders of magnitude in central mass."
JOHN A WHITE,The long-baseline neutrino experiment: exploring fundamental symmetries of the universe,"The preponderance of matter over antimatter in the early Universe, the dynamics of the supernova bursts that produced the heavy elements necessary for life and whether protons eventually decay --- these mysteries at the forefront of particle physics and astrophysics are key to understanding the early evolution of our Universe, its current state and its eventual fate. The Long-Baseline Neutrino Experiment (LBNE) represents an extensively developed plan for a world-class experiment dedicated to addressing these questions. LBNE is conceived around three central components: (1) a new, high-intensity neutrino source generated from a megawatt-class proton accelerator at Fermi National Accelerator Laboratory, (2) a near neutrino detector just downstream of the source, and (3) a massive liquid argon time-projection chamber deployed as a far detector deep underground at the Sanford Underground Research Facility. This facility, located at the site of the former Homestake Mine in Lead, South Dakota, is approximately 1,300 km from the neutrino source at Fermilab -- a distance (baseline) that delivers optimal sensitivity to neutrino charge-parity symmetry violation and mass ordering effects. This ambitious yet cost-effective design incorporates scalability and flexibility and can accommodate a variety of upgrades and contributions. With its exceptional combination of experimental configuration, technical capabilities, and potential for transformative discoveries, LBNE promises to be a vital facility for the field of particle physics worldwide, providing physicists from around the globe with opportunities to collaborate in a twenty to thirty year program of exciting science. In this document we provide a comprehensive overview of LBNE's scientific objectives, its place in the landscape of neutrino physics worldwide, the technologies it will incorporate and the capabilities it will possess."
JOHN A WHITE,Broadband multi-wavelength properties of M87 during the 2017 Event Horizon Telescope campaign,"In 2017, the Event Horizon Telescope (EHT) Collaboration succeeded in capturing the first direct image of the center of the M87 galaxy. The asymmetric ring morphology and size are consistent with theoretical expectations for a weakly accreting supermassive black hole of mass ∼6.5 × 109 M ⊙. The EHTC also partnered with several international facilities in space and on the ground, to arrange an extensive, quasi-simultaneous multi-wavelength campaign. This Letter presents the results and analysis of this campaign, as well as the multi-wavelength data as a legacy data repository. We captured M87 in a historically low state, and the core flux dominates over HST-1 at high energies, making it possible to combine core flux constraints with the more spatially precise very long baseline interferometry data. We present the most complete simultaneous multi-wavelength spectrum of the active nucleus to date, and discuss the complexity and caveats of combining data from different spatial scales into one broadband spectrum. We apply two heuristic, isotropic leptonic single-zone models to provide insight into the basic source properties, but conclude that a structured jet is necessary to explain M87’s spectrum. We can exclude that the simultaneous γ-ray emission is produced via inverse Compton emission in the same region producing the EHT mm-band emission, and further conclude that the γ-rays can only be produced in the inner jets (inward of HST-1) if there are strongly particle-dominated regions. Direct synchrotron emission from accelerated protons and secondaries cannot yet be excluded."
JENNIFER G. GREEN,Multidisciplinary approaches to research on bullying in adolescence,"Bullying is a significant public health problem in the United States that affects youth functioning in multiple domains. Much of the research on bullying to date has focused on children, however, leaving gaps in the literature with respect to understanding bullying among adolescents. In particular, less is known about how adolescents conceptualize bullying, what predicts and is associated with bullying involvement among adolescents, and how prevention programs might address the unique needs of middle and high school students. This special issue proposes that a multidisciplinary perspective might be particularly useful in better understanding bullying among adolescents and determining how to design more effective interventions and prevention programs for this age-group. The current article introduces the special issue by briefly discussing what is known about bullying in adolescence and considering three disciplines (computer science, big data, and virtual communities; media studies; anthropology) that are particularly well situated to move the field forward. Next, this article reviews teen pregnancy prevention efforts, as an example of another adolescent public health concern that has been addressed successfully using a multidisciplinary approach. The article concludes with an overview of the three manuscripts that are part of the special issue."
YANHANG ZHANG,The effect of static stretch on elastin degradation in arteries,"Previously we have shown that gradual changes in the structure of elastin during an elastase treatment can lead to important transition stages in the mechanical behavior of arteries. However, in vivo arteries are constantly being loaded due to systolic and diastolic pressures and so understanding the effects of loading on the enzymatic degradation of elastin in arteries is important. With biaxial tensile testing, we measured the mechanical behavior of porcine thoracic aortas digested with a mild solution of purified elastase (5 U/mL) in the presence of a static stretch. Arterial mechanical properties and biochemical composition were analyzed to assess the effects of mechanical stretch on elastin degradation. As elastin is being removed, the dimensions of the artery increase by more than 20% in both the longitude and circumference directions. Elastin assays indicate a faster rate of degradation when stretch was present during the digestion. A simple exponential decay fitting confirms the time constant for digestion with stretch (0.11 ± 0.04 h(-1)) is almost twice that of digestion without stretch (0.069 ± 0.028 h(-1)). The transition from J-shaped to S-shaped stress vs. strain behavior in the longitudinal direction generally occurs when elastin content is reduced by about 60%. Multiphoton image analysis confirms the removal/fragmentation of elastin and also shows that the collagen fibers are closely intertwined with the elastin lamellae in the medial layer. After removal of elastin, the collagen fibers are no longer constrained and become disordered. Release of amorphous elastin during the fragmentation of the lamellae layers is observed and provides insights into the process of elastin degradation. Overall this study reveals several interesting microstructural changes in the extracellular matrix that could explain the resulting mechanical behavior of arteries with elastin degradation."
YANHANG ZHANG,Experimental and modeling study of collagen scaffolds with the effects of crosslinking and fiber alignment,"Collagen type I scaffolds are commonly used due to its abundance, biocompatibility, and ubiquity. Most applications require the scaffolds to operate under mechanical stresses. Therefore understanding and being able to control the structural-functional integrity of collagen scaffolds becomes crucial. Using a combined experimental and modeling approach, we studied the structure and function of Type I collagen gel with the effects of spatial fiber alignment and crosslinking. Aligned collagen scaffolds were created through the flow of magnetic particles enmeshed in collagen fibrils to mimic the anisotropy seen in native tissue. Inter- and intra- molecular crosslinking was modified chemically with Genipin to further improve the stiffness of collagen scaffolds. The anisotropic mechanical properties of collagen scaffolds were characterized using a planar biaxial tensile tester and parallel plate rheometer. The tangent stiffness from biaxial tensile test is two to three orders of magnitude higher than the storage moduli from rheological measurements. The biphasic nature of collagen gel was discussed and used to explain the mechanical behavior of collagen scaffolds under different types of mechanical tests. An anisotropic hyperelastic constitutive model was used to capture the characteristics of the stress-strain behavior exhibited by collagen scaffolds."
YANHANG ZHANG,Vascular smooth muscle Sirtuin-1 protects against aortic dissection during Angiotensin II-induced hypertension,"BACKGROUND: Sirtuin-1 (SirT1), a nicotinamide adenine dinucleotide(+)-dependent deacetylase, is a key enzyme in the cellular response to metabolic, inflammatory, and oxidative stresses; however, the role of endogenous SirT1 in the vasculature has not been fully elucidated. Our goal was to evaluate the role of vascular smooth muscle SirT1 in the physiological response of the aortic wall to angiotensin II, a potent hypertrophic, oxidant, and inflammatory stimulus. METHODS AND RESULTS: Mice lacking SirT1 in vascular smooth muscle (ie, smooth muscle SirT1 knockout) had drastically high mortality (70%) caused by aortic dissection after angiotensin II infusion (1 mg/kg per day) but not after an equipotent dose of norepinephrine, despite comparable blood pressure increases. Smooth muscle SirT1 knockout mice did not show any abnormal aortic morphology or blood pressure compared with wild-type littermates. Nonetheless, in response to angiotensin II, aortas from smooth muscle SirT1 knockout mice had severely disorganized elastic lamellae with frequent elastin breaks, increased oxidant production, and aortic stiffness compared with angiotensin II-treated wild-type mice. Matrix metalloproteinase expression and activity were increased in the aortas of angiotensin II-treated smooth muscle SirT1 knockout mice and were prevented in mice overexpressing SirT1 in vascular smooth muscle or with use of the oxidant scavenger tempol. CONCLUSIONS: Endogenous SirT1 in aortic smooth muscle is required to maintain the structural integrity of the aortic wall in response to oxidant and inflammatory stimuli, at least in part, by suppressing oxidant-induced matrix metalloproteinase activity. SirT1 activators could potentially be a novel therapeutic approach to prevent aortic dissection and rupture in patients at risk, such as those with hypertension or genetic disorders, such as Marfan's syndrome."
YANHANG ZHANG,Understanding the viscoelastic behavior of collagen matrices through relaxation time distribution spectrum,"This study aims to provide understanding of the macroscopic viscoelastic behavior of collagen matrices through studying the relaxation time distribution spectrum obtained from stress relaxation tests. Hydrated collagen gel and dehydrated collagen thin film was exploited as two different hydration levels of collagen matrices. Genipin solution was used to induce crosslinking in collagen matrices. Biaxial stress relaxation tests were performed to characterize the viscoelastic behavior of collagen matrices. The rate of stress relaxation of both hydrated and dehydrated collagen matrices shows a linear initial stress level dependency. Increased crosslinking reduces viscosity in collagen gel, but the effect is negligible for thin film. Relaxation time distribution spectrum was obtained from the stress relaxation data by inverse Laplace transform. For most of the collagen matrices, three peaks at the short (0.3s ~1 s), medium (3s ~90 s), and long relaxation time (> 200 s) were observed in the continuous spectrum, which likely corresponds to relaxation mechanisms involve fiber, inter-fibril, and fibril sliding. Splitting of the middle peak was observed at higher initial stress levels suggesting increased structural heterogeneity at the fibril level with mechanical loading. The intensity of the long-term peaks increases with higher initial stress levels indicating the engagement of collagen fibrils at higher levels of tissue strain."
YANHANG ZHANG,Predicting attitudinal and behavioral responses to COVID-19 pandemic using machine learning,"At the beginning of 2020, COVID-19 became a global problem. Despite all the efforts to emphasize the relevance of preventive measures, not everyone adhered to them. Thus, learning more about the characteristics determining attitudinal and behavioral responses to the pandemic is crucial to improving future interventions. In this study, we applied machine learning on the multinational data collected by the International Collaboration on the Social and Moral Psychology of COVID-19 (N = 51,404) to test the predictive efficacy of constructs from social, moral, cognitive, and personality psychology, as well as socio-demographic factors, in the attitudinal and behavioral responses to the pandemic. The results point to several valuable insights. Internalized moral identity provided the most consistent predictive contribution-individuals perceiving moral traits as central to their self-concept reported higher adherence to preventive measures. Similar results were found for morality as cooperation, symbolized moral identity, self-control, open-mindedness, and collective narcissism, while the inverse relationship was evident for the endorsement of conspiracy theories. However, we also found a non-neglible variability in the explained variance and predictive contributions with respect to macro-level factors such as the pandemic stage or cultural region. Overall, the results underscore the importance of morality-related and contextual factors in understanding adherence to public health recommendations during the pandemic."
YANHANG ZHANG,Avalanches and power law behavior in aortic dissection progression,Aortic dissection is a devastating cardiovascular disease known for its rapid propagation and high morbidity and mortality. The mechanisms underlying the propagation of aortic dissection are not well understood. Our study reports the discovery of avalanche-like failure of the aorta during dissection propagation that results from the local buildup of strain energy followed by a cascade failure of inhomogeneously distributed interlamellar collagen fibers. An innovative computational model was developed that successfully describes the failure mechanics of dissection propagation. Our study provides the first quantitative agreement between experiment and model prediction of the dissection propagation within the complex extracellular matrix (ECM). Our results may lead to the possibility of predicting such catastrophic events based on microscopic features of the ECM.
YANHANG ZHANG,Compressive remodeling alters fluid transport properties of collagen networks - implications for tumor growth,"Biomechanical alterations to the tumor microenvironment include accumulation of solid stresses, extracellular matrix (ECM) stiffening and increased fluid pressure in both interstitial and peri-tumoral spaces. The relationship between interstitial fluid pressurization and ECM remodeling in vascularized tumors is well characterized, while earlier biomechanical changes occurring during avascular tumor growth within the peri-tumoral ECM remain poorly understood. Type I collagen, the primary fibrous ECM constituent, bears load in tension while it buckles under compression. We hypothesized that tumor-generated compressive forces cause collagen remodeling via densification which in turn creates a barrier to convective fluid transport and may play a role in tumor progression and malignancy. To better understand this process, we characterized the structure-function relationship of collagen networks under compression both experimentally and computationally. Here we show that growth of epithelial cancers induces compressive remodeling of the ECM, documented in the literature as a TACS-2 phenotype, which represents a localized densification and tangential alignment of peri-tumoral collagen. Such compressive remodeling is caused by the unique features of collagen network mechanics, such as fiber buckling and cross-link rupture, and reduces the overall hydraulic permeability of the matrix."
YANHANG ZHANG,Contribution of elastin and collagen to the mechanical behavior of bovine nuchal ligament,"Ligamentum nuchae is a highly elastic tissue commonly used to study the structure and mechanics of elastin. This study combines imaging, mechanical testing, and constitutive modeling to examine the structural organization of elastic and collagen fibers and their contributions to the nonlinear stress-strain behavior of the tissue. Rectangular samples of bovine ligamentum nuchae cut in both longitudinal and transverse directions were tested in uniaxial tension. Purified elastin samples were also obtained and tested. It was observed that the stress-stretch response of purified elastin tissue follows a similar curve as the intact tissue initially, but the intact tissue shows a significant stiffening behavior for stretches above 1.29 with collagen engagement. Multiphoton and histology images confirm the elastin-dominated bulk of ligamentum nuchae interspersed with small bundles of collagen fibrils and sporadic collagen-rich regions with cellular components and ground substance. A transversely isotropic constitutive model that considers the longitudinal organization of elastic and collagen fibers was developed to describe the mechanical behavior of both intact and purified elastin tissue under uniaxial tension. These findings shed light on the unique structural and mechanical roles of elastic and collagen fibers in tissue mechanics and may aid in future use of ligamentum nuchae in tissue grafting."
JAMES MILLER,Are the average gait speeds during the 10 meter and 6 minute walk tests redundant in Parkinson disease?,"We investigated the relationships between average gait speed collected with the 10Meter Walk Test (Comfortable and Fast) and 6Minute Walk Test (6MWT) in 346 people with Parkinson disease (PD) and how the relationships change with increasing disease severity. Pearson correlation and linear regression analyses determined relationships between 10Meter Walk Test and 6MWT gait speed values for the entire sample and for sub-samples stratified by Hoehn & Yahr (H&Y) stage I (n=53), II (n=141), III (n=135) and IV (n=17). We hypothesized that redundant tests would be highly and significantly correlated (i.e. r>0.70, p<0.05) and would have a linear regression model slope of 1 and intercept of 0. For the entire sample, 6MWT gait speed was significantly (p<0.001) related to the Comfortable 10 Meter Walk Test (r=0.75) and Fast 10Meter Walk Test (r=0.79) gait speed, with 56% and 62% of the variance in 6MWT gait speed explained, respectively. The regression model of 6MWT gait speed predicted by Comfortable 10 Meter Walk gait speed produced slope and intercept values near 1 and 0, respectively, especially for participants in H&Y stages II-IV. In contrast, slope and intercept values were further from 1 and 0, respectively, for the Fast 10Meter Walk Test. Comfortable 10 Meter Walk Test and 6MWT gait speeds appeared to be redundant in people with moderate to severe PD, suggesting the Comfortable 10 Meter Walk Test can be used to estimate 6MWT distance in this population."
JAMES MILLER,Magnetic-field measurement and analysis for the Muon g−2 Experiment at Fermilab,"The Fermi National Accelerator Laboratory (FNAL) Muon g−2 Experiment has measured the anomalous precession frequency aμ≡(gμ−2)/2 of the muon to a combined precision of 0.46 parts per million with data collected during its first physics run in 2018. This paper documents the measurement of the magnetic field in the muon storage ring. The magnetic field is monitored by systems and calibrated in terms of the equivalent proton spin precession frequency in a spherical water sample at 34.7∘C. The measured field is weighted by the muon distribution resulting in ˜ω′p, the denominator in the ratio ωa/˜ω′p that together with known fundamental constants yields aμ. The reported uncertainty on ˜ω′p for the Run-1 data set is 114 ppb consisting of uncertainty contributions from frequency extraction, calibration, mapping, tracking, and averaging of 56 ppb, and contributions from fast transient fields of 99 ppb."
JAMES MILLER,"The ISCIP Analyst, Volume V, Issue 8",
JAMES MILLER,"The ISCIP Analyst, Volume V, Issue 9",
JAMES MILLER,"The ISCIP Analyst, Volume IV, Issue 15",
JAMES MILLER,"Boston University Pre-Law Review: Volume XXII, Issue 1, Fall 2012",
JAMES MILLER,"Journal of African Christian Biography: v. 6, no. 3",A publication of the Dictionary of African Christian Biography with U.S. offices located at the Center for Global Christianity and Mission at Boston University. This issue focuses on: The Project Luke scholarship program ran from 1999 to 2011 at the Overseas Ministries Study Center where DACB Founder and Director Emeritus Jonathan Bonk served as Executive Director from 2000 to 2013. This issue of the Journal retraces the history of Project Luke by recounting the stories provided by seventeen men and two women in these pages.
JAMES MILLER,Charged lepton flavour violation using intense muon beams at future facilities,"Charged-lepton flavour-violating (cLFV) processes o er deep probes for new physics with discovery sensitivity to a broad array of new physics models | SUSY, Higgs Doublets, Extra Dimensions, and, particularly, models explaining the neutrino mass hierarchy and the matter-antimatter asymmetry of the universe via leptogenesis. The most sensitive probes of cLFV utilize high-intensity muon beams to search for μ → e transitions. We summarize the status of muon-cLFV experiments currently under construction at PSI, Fermilab, and J-PARC. These experiments o er sensitivity to e ective new physics mass scales approaching 𝒪(10^4) TeV/c^2. Further improvements are possible and next-generation experiments, using upgraded accelerator facilities at PSI, Fermilab, and J-PARC, could begin data taking within the next decade. In the case of discoveries at the LHC, they could distinguish among alternative models; even in the absence of direct discoveries, they could establish new physics. These experiments both complement and extend the searches at the LHC."
JAMES MILLER,Expression of interest for evolution of the Mu2e experiment,"We propose an evolution of the Mu2e experiment, called Mu2e-II, that would leverage advances in detector technology and utilize the increased proton intensity provided by the Fermilab PIP-II upgrade to improve the sensitivity for neutrinoless muon-to-electron conversion by one order of magnitude beyond the Mu2e experiment, providing the deepest probe of charged lepton flavor violation in the foreseeable future. Mu2e-II will use as much of the Mu2e infrastructure as possible, providing, where required, improvements to the Mu2e apparatus to accommodate the increased beam intensity and cope with the accompanying increase in backgrounds."
JAMES MILLER,"The muon (g-2) spin equations, the magic γ, what’s small and what’s not","We review the spin equations for the muon in the 1.45 T muon (g — 2) storage ring, now relocated to Fermilab. Muons are stored in a uniform 1.45 T magnetic field, and vertical focusing is provided by four sets of electrostatic quadrupoles placed symmetrically around the storage ring. The storage ring is operated at a Lorentz factor centered on the ""magic 𝛄 = 29:3""; the effect of the electric field on the muon spin precession cancels for muons at the magic momentum. We point out the relative sizes of the various terms in the spin equations, and show that for experiments that use the magic 𝛄 and electric quadrupole focusing to store the muon beam, any proposed effect that multiplies either the motional magnetic field β × 𝐸 or the muon pitching motion β • 𝐵 term, will be smaller by three or more orders of magnitude, relative to the spin precession due to the storage ring magnetic field. We use a recently proposed General Relativity correction [1] as an example, to demonstrate the smallness of any such contribution, and point out that their revised preprint [7] still contains a conceptual error, that signi cantly overestimates the magnitude of their proposed correction. We have prepared this document in the hope that future authors will nd it useful, should they wish to propose corrections from some additional term added to the Thomas equation, Eq. 13, below. Our goal is to clarify how the experiment is done, and how the small corrections due to the presence of the radial electric field and the vertical pitching motion of the muons (betatron motion) in the storage ring are taken into account."
JAMES MILLER,Comment on arXiv:1612.01502 'Is the trineutron resonance lower in energy than a tetraneutron resonance?',"The quantum Monte Carlo study [S. Gandolfi, H.-W. Hammer, P. Klos, J. E. Lynn, and A. Schwenk, Phys. Rev. Lett. {\bf 118}, 232501 (2017), arXiv:1612.01502] of few-neutron resonant states provided results incompatible with rigorous few-body calculations. In this Comment we point out serious shortcomings in the work by Gandolfi et al, leading to misinterpretation of unbound few-body systems. Comment on the article by S. Gandolfi et al. Phys.Rev.Letters 118 (2017), 232501; arXiv:1612.01502"
JAMES MILLER,Beam dynamics corrections to the Run-1 measurement of the muon anomalous magnetic moment at Fermilab,"This paper presents the beam dynamics systematic corrections and their uncertainties for the Run-1 dataset of the Fermilab Muon g−2 Experiment. Two corrections to the measured muon precession frequency ωma are associated with well-known effects owing to the use of electrostatic quadrupole (ESQ) vertical focusing in the storage ring. An average vertically oriented motional magnetic field is felt by relativistic muons passing transversely through the radial electric field components created by the ESQ system. The correction depends on the stored momentum distribution and the tunes of the ring, which has relatively weak vertical focusing. Vertical betatron motions imply that the muons do not orbit the ring in a plane exactly orthogonal to the vertical magnetic field direction. A correction is necessary to account for an average pitch angle associated with their trajectories. A third small correction is necessary, because muons that escape the ring during the storage time are slightly biased in initial spin phase compared to the parent distribution. Finally, because two high-voltage resistors in the ESQ network had longer than designed RC time constants, the vertical and horizontal centroids and envelopes of the stored muon beam drifted slightly, but coherently, during each storage ring fill. This led to the discovery of an important phase-acceptance relationship that requires a correction. The sum of the corrections to ωma is 0.50±0.09  ppm; the uncertainty is small compared to the 0.43 ppm statistical precision of ωma."
JAMES MILLER,"The ISCIP Analyst, Volume V, Issue 13",
JAMES MILLER,"The ISCIP Analyst, Volume VI, Issue 5",
JAMES MILLER,An inquiry into Plato's treatment of wealth.,"Plato's moral philosophy has come under attack in recent years on the grounds that his works lay the basis for totalitarian theories such as Marxian Communism and its offspring, Stalinism. It is also argued that Plato is a reactionary anti-democrat who detested those who carry on work with the hands or who are engaged in commerce. In order to understand the place of wealth in Plato's thought certain points should not be overlooked. (1) Plato's treatment of the problems of economics form a part of his ethical and political theories which must not be construed as mere totalitarian camouflages of one is to be critically honest. Throughout the dialogues references to wealth are scattered as illustrative comment on some point in a psychological, ethical, or political discussion. He develops no separate science of economics. [TRUNCATED]"
JAMES MILLER,"The ISCIP Analyst, Volume V, Issue 1",
JAMES MILLER,"The ISCIP Analyst, Volume IV, Issue 19",
JAMES MILLER,"The ISCIP Analyst, Volume IV, Issue 16",
JAMES MILLER,"The ISCIP Analyst, Volume IV, Issue 13",
JAMES MILLER,"The ISCIP Analyst, Volume IV, Issue 20",
JAMES MILLER,"The ISCIP Analyst, Volume IV, Issue 17",
JAMES MILLER,"The ISCIP Analyst, Volume IV, Issue 18",
JAMES MILLER,"The ISCIP Analyst, Volume IV, Issue 14",
JAMES MILLER,"The ISCIP Analyst, Volume V, Issue 18",
JAMES MILLER,"The ISCIP Analyst, Volume V, Issue 15",
JAMES MILLER,"The ISCIP Analyst, Volume VI, Issue 6",
JAMES MILLER,"The ISCIP Analyst, Volume VI, Issue 9",
JAMES MILLER,"The ISCIP Analyst, Volume VI, Issue 2",
JAMES MILLER,"The ISCIP Analyst, Volume VI, Issue 8",
JAMES MILLER,"The ISCIP Analyst, Volume VI, Issue 7",
JAMES MILLER,"The ISCIP Analyst, Volume V, Issue 17",
JAMES MILLER,"Caribbean Corals in Crisis: Record Thermal Stress, Bleaching, and Mortality in 2005","BACKGROUND. The rising temperature of the world's oceans has become a major threat to coral reefs globally as the severity and frequency of mass coral bleaching and mortality events increase. In 2005, high ocean temperatures in the tropical Atlantic and Caribbean resulted in the most severe bleaching event ever recorded in the basin. METHODOLOGY/PRINCIPAL FINDINGS. Satellite-based tools provided warnings for coral reef managers and scientists, guiding both the timing and location of researchers' field observations as anomalously warm conditions developed and spread across the greater Caribbean region from June to October 2005. Field surveys of bleaching and mortality exceeded prior efforts in detail and extent, and provided a new standard for documenting the effects of bleaching and for testing nowcast and forecast products. Collaborators from 22 countries undertook the most comprehensive documentation of basin-scale bleaching to date and found that over 80% of corals bleached and over 40% died at many sites. The most severe bleaching coincided with waters nearest a western Atlantic warm pool that was centered off the northern end of the Lesser Antilles. CONCLUSIONS/SIGNIFICANCE. Thermal stress during the 2005 event exceeded any observed from the Caribbean in the prior 20 years, and regionally-averaged temperatures were the warmest in over 150 years. Comparison of satellite data against field surveys demonstrated a significant predictive relationship between accumulated heat stress (measured using NOAA Coral Reef Watch's Degree Heating Weeks) and bleaching intensity. This severe, widespread bleaching and mortality will undoubtedly have long-term consequences for reef ecosystems and suggests a troubled future for tropical marine ecosystems under a warming climate."
JAMES MILLER,"The ISCIP Analyst, Volume VI, Issue 4",
JAMES MILLER,"The ISCIP Analyst, Volume VI, Issue 3",
JAMES MILLER,"The ISCIP Analyst, Volume VI, Issue 10",
JAMES MILLER,"The ISCIP Analyst, Volume V, Issue 4",
JAMES MILLER,"The ISCIP Analyst, Volume V, Issue 14",
JAMES MILLER,"The ISCIP Analyst, Volume V, Issue 16",
JAMES MILLER,"The ISCIP Analyst, Volume VI, Issue 1",
JAMES MILLER,"The ISCIP Analyst, Volume V, Issue 2",
JAMES MILLER,"The ISCIP Analyst, Volume V, Issue 19",
JAMES MILLER,"The ISCIP Analyst, Volume V, Issue 5",
JAMES MILLER,"The ISCIP Analyst, Volume V, Issue 3",
JAMES MILLER,"The ISCIP Analyst, Volume V, Issue 6",
JAMES MILLER,"The ISCIP Analyst, Volume V, Issue 7",
JAMES MILLER,Measurements of the νμ and ¯νμ-induced coherent charged pion production cross sections on 12C by the T2K experiment,
JAMES MILLER,Updated T2K measurements of muon neutrino and antineutrino disappearance using 3.6×1021 protons on target,
JAMES MILLER,Measurement of the anomalous precession frequency of the muon in the Fermilab Muon g − 2 Experiment,"The Muon g−2 Experiment at Fermi National Accelerator Laboratory (FNAL) has measured the muon anomalous precession frequency ωma to an uncertainty of 434 parts per billion (ppb), statistical, and 56 ppb, systematic, with data collected in four storage ring configurations during its first physics run in 2018. When combined with a precision measurement of the magnetic field of the experiment’s muon storage ring, the precession frequency measurement determines a muon magnetic anomaly of aμ(FNAL)=116592040(54)×10−11 (0.46 ppm). This article describes the multiple techniques employed in the reconstruction, analysis, and fitting of the data to measure the precession frequency. It also presents the averaging of the results from the 11 separate determinations of ωma, and the systematic uncertainties on the result."
JAMES MILLER,Buildout and integration of an automated high-throughput CLIA laboratory for SARS-CoV-2 testing on a large urban campus,"In 2019, the first cases of SARS-CoV-2 were detected in Wuhan, China, and by early 2020 the first cases were identified in the United States. SARS-CoV-2 infections increased in the US causing many states to implement stay-at-home orders and additional safety precautions to mitigate potential outbreaks. As policies changed throughout the pandemic and restrictions lifted, there was an increase in demand for COVID-19 testing which was costly, difficult to obtain, or had long turn-around times. Some academic institutions, including Boston University (BU), created an on-campus COVID-19 screening protocol as part of a plan for the safe return of students, faculty, and staff to campus with the option for in-person classes. At BU, we put together an automated high-throughput clinical testing laboratory with the capacity to run 45,000 individual tests weekly by Fall of 2020, with a purpose-built clinical testing laboratory, a multiplexed reverse transcription PCR (RT-qPCR) test, robotic instrumentation, and trained staff. There were many challenges including supply chain issues for personal protective equipment and testing materials in addition to equipment that were in high demand. The BU Clinical Testing Laboratory (CTL) was operational at the start of Fall 2020 and performed over 1 million SARS-CoV-2 PCR tests during the 2020-2021 academic year."
JAMES MILLER,Prospects for beyond the standard model physics searches at the deep underground neutrino experiment: DUNE collaboration,"The Deep Underground Neutrino Experiment (DUNE) will be a powerful tool for a variety of physics topics. The high-intensity proton beams provide a large neutrino flux, sampled by a near detector system consisting of a combination of capable precision detectors, and by the massive far detector system located deep underground. This configuration sets up DUNE as a machine for discovery, as it enables opportunities not only to perform precision neutrino measurements that may uncover deviations from the present three-flavor mixing paradigm, but also to discover new particles and unveil new interactions and symmetries beyond those predicted in the Standard Model (SM). Of the many potential beyond the Standard Model (BSM) topics DUNE will probe, this paper presents a selection of studies quantifying DUNE's sensitivities to sterile neutrino mixing, heavy neutral leptons, non-standard interactions, CPT symmetry violation, Lorentz invariance violation, neutrino trident production, dark matter from both beam induced and cosmogenic sources, baryon number violation, and other new physics topics that complement those at high-energy colliders and significantly extend the present reach."
JAMES MILLER,Identification and reconstruction of low-energy electrons in the ProtoDUNE-SP detector,
JAMES MILLER,The long-baseline neutrino experiment: exploring fundamental symmetries of the universe,"The preponderance of matter over antimatter in the early Universe, the dynamics of the supernova bursts that produced the heavy elements necessary for life and whether protons eventually decay --- these mysteries at the forefront of particle physics and astrophysics are key to understanding the early evolution of our Universe, its current state and its eventual fate. The Long-Baseline Neutrino Experiment (LBNE) represents an extensively developed plan for a world-class experiment dedicated to addressing these questions. LBNE is conceived around three central components: (1) a new, high-intensity neutrino source generated from a megawatt-class proton accelerator at Fermi National Accelerator Laboratory, (2) a near neutrino detector just downstream of the source, and (3) a massive liquid argon time-projection chamber deployed as a far detector deep underground at the Sanford Underground Research Facility. This facility, located at the site of the former Homestake Mine in Lead, South Dakota, is approximately 1,300 km from the neutrino source at Fermilab -- a distance (baseline) that delivers optimal sensitivity to neutrino charge-parity symmetry violation and mass ordering effects. This ambitious yet cost-effective design incorporates scalability and flexibility and can accommodate a variety of upgrades and contributions. With its exceptional combination of experimental configuration, technical capabilities, and potential for transformative discoveries, LBNE promises to be a vital facility for the field of particle physics worldwide, providing physicists from around the globe with opportunities to collaborate in a twenty to thirty year program of exciting science. In this document we provide a comprehensive overview of LBNE's scientific objectives, its place in the landscape of neutrino physics worldwide, the technologies it will incorporate and the capabilities it will possess."
JAMES MILLER,Measurement of the positive muon anomalous magnetic moment to 0.46 ppm,"We present the first results of the Fermilab National Accelerator Laboratory (FNAL) Muon g-2 Experiment for the positive muon magnetic anomaly a_{μ}≡(g_{μ}-2)/2. The anomaly is determined from the precision measurements of two angular frequencies. Intensity variation of high-energy positrons from muon decays directly encodes the difference frequency ω_{a} between the spin-precession and cyclotron frequencies for polarized muons in a magnetic storage ring. The storage ring magnetic field is measured using nuclear magnetic resonance probes calibrated in terms of the equivalent proton spin precession frequency ω[over ˜]_{p}^{'} in a spherical water sample at 34.7 °C. The ratio ω_{a}/ω[over ˜]_{p}^{'}, together with known fundamental constants, determines a_{μ}(FNAL)=116 592 040(54)×10^{-11} (0.46 ppm). The result is 3.3 standard deviations greater than the standard model prediction and is in excellent agreement with the previous Brookhaven National Laboratory (BNL) E821 measurement. After combination with previous measurements of both μ^{+} and μ^{-}, the new experimental average of a_{μ}(Exp)=116 592 061(41)×10^{-11} (0.35 ppm) increases the tension between experiment and theory to 4.2 standard deviations."
JAMES MILLER,Scintillation light detection in the 6-m drift-length ProtoDUNE Dual Phase liquid argon TPC,"DUNE is a dual-site experiment for long-baseline neutrino oscillation studies, neutrino astrophysics and nucleon decay searches. ProtoDUNE Dual Phase (DP) is a 6  ×  6  ×  6 m 3 liquid argon time-projection-chamber (LArTPC) that recorded cosmic-muon data at the CERN Neutrino Platform in 2019-2020 as a prototype of the DUNE Far Detector. Charged particles propagating through the LArTPC produce ionization and scintillation light. The scintillation light signal in these detectors can provide the trigger for non-beam events. In addition, it adds precise timing capabilities and improves the calorimetry measurements. In ProtoDUNE-DP, scintillation and electroluminescence light produced by cosmic muons in the LArTPC is collected by photomultiplier tubes placed up to 7 m away from the ionizing track. In this paper, the ProtoDUNE-DP photon detection system performance is evaluated with a particular focus on the different wavelength shifters, such as PEN and TPB, and the use of Xe-doped LAr, considering its future use in giant LArTPCs. The scintillation light production and propagation processes are analyzed and a comparison of simulation to data is performed, improving understanding of the liquid argon properties."
JAMES MILLER,First measurement of muon neutrino charged-current interactions on hydrocarbon without pions in the final state using multiple detectors with correlated energy spectra at T2K,
JAMES MILLER,A new paradigm for pandemic preparedness,"PURPOSE OF REVIEW: Preparing for pandemics requires a degree of interdisciplinary work that is challenging under the current paradigm. This review summarizes the challenges faced by the field of pandemic science and proposes how to address them. RECENT FINDINGS: The structure of current siloed systems of research organizations hinders effective interdisciplinary pandemic research. Moreover, effective pandemic preparedness requires stakeholders in public policy and health to interact and integrate new findings rapidly, relying on a robust, responsive, and productive research domain. Neither of these requirements are well supported under the current system. SUMMARY: We propose a new paradigm for pandemic preparedness wherein interdisciplinary research and close collaboration with public policy and health practitioners can improve our ability to prevent, detect, and treat pandemics through tighter integration among domains, rapid and accurate integration, and translation of science to public policy, outreach and education, and improved venues and incentives for sustainable and robust interdisciplinary work."
KIMBERLY A MCCALL,Defective phagocytic corpse processing results in neurodegeneration and can be rescued by TORC1 activation,
KIMBERLY A MCCALL,I spy in the developing fly a multitude of ways to die,"Cell proliferation and cell death are two opposing, yet complementary fundamental processes in development. Cell proliferation provides new cells, while developmental programmed cell death adjusts cell numbers and refines structures as an organism grows. Apoptosis is the best-characterized form of programmed cell death; however, there are many other non-apoptotic forms of cell death that occur throughout development. Drosophila is an excellent model for studying these varied forms of cell death given the array of cellular, molecular, and genetic techniques available. In this review, we discuss select examples of apoptotic and non-apoptotic cell death that occur in different tissues and at different stages of Drosophila development. For example, apoptosis occurs throughout the nervous system to achieve an appropriate number of neurons. Elsewhere in the fly, non-apoptotic modes of developmental cell death are employed, such as in the elimination of larval salivary glands and midgut during metamorphosis. These and other examples discussed here demonstrate the versatility of Drosophila as a model organism for elucidating the diverse modes of programmed cell death."
ALEXA BEISER,Genetic Correlates of Brain Aging on MRI and Cognitive Test Measures: A Genome-Wide Association and Linkage Analysis in the Framingham Study,"BACKGROUND: Brain magnetic resonance imaging (MRI) and cognitive tests can identify heritable endophenotypes associated with an increased risk of developing stroke, dementia and Alzheimer's disease (AD). We conducted a genome-wide association (GWA) and linkage analysis exploring the genetic basis of these endophenotypes in a community-based sample. METHODS: A total of 705 stroke- and dementia-free Framingham participants (age 62 +9 yrs, 50% male) who underwent volumetric brain MRI and cognitive testing (1999–2002) were genotyped. We used linear models adjusting for first degree relationships via generalized estimating equations (GEE) and family based association tests (FBAT) in additive models to relate qualifying single nucleotide polymorphisms (SNPs, 70,987 autosomal on Affymetrix 100K Human Gene Chip with minor allele frequency ≥ 0.10, genotypic call rate ≥ 0.80, and Hardy-Weinberg equilibrium p-value ≥ 0.001) to multivariable-adjusted residuals of 9 MRI measures including total cerebral brain (TCBV), lobar, ventricular and white matter hyperintensity (WMH) volumes, and 6 cognitive factors/tests assessing verbal and visuospatial memory, visual scanning and motor speed, reading, abstract reasoning and naming. We determined multipoint identity-by-descent utilizing 10,592 informative SNPs and 613 short tandem repeats and used variance component analyses to compute LOD scores. RESULTS: The strongest gene-phenotype association in FBAT analyses was between SORL1 (rs1131497; p = 3.2 × 10-6) and abstract reasoning, and in GEE analyses between CDH4 (rs1970546; p = 3.7 × 10-8) and TCBV. SORL1 plays a role in amyloid precursor protein processing and has been associated with the risk of AD. Among the 50 strongest associations (25 each by GEE and FBAT) were other biologically interesting genes. Polymorphisms within 28 of 163 candidate genes for stroke, AD and memory impairment were associated with the endophenotypes studied at p < 0.001. We confirmed our previously reported linkage of WMH on chromosome 4 and describe linkage of reading performance to a marker on chromosome 18 (GATA11A06), previously linked to dyslexia (LOD scores = 2.2 and 5.1). CONCLUSION: Our results suggest that genes associated with clinical neurological disease also have detectable effects on subclinical phenotypes. These hypothesis generating data illustrate the use of an unbiased approach to discover novel pathways that may be involved in brain aging, and could be used to replicate observations made in other studies."
CLAUDIO C CHAMON,Quantum vertex model for reversible classical computing,"Mappings of classical computation onto statistical mechanics models have led to remarkable successes in addressing some complex computational problems. However, such mappings display thermodynamic phase transitions that may prevent reaching solution even for easy problems known to be solvable in polynomial time. Here we map universal reversible classical computations onto a planar vertex model that exhibits no bulk classical thermodynamic phase transition, independent of the computational circuit. Within our approach the solution of the computation is encoded in the ground state of the vertex model and its complexity is reflected in the dynamics of the relaxation of the system to its ground state. We use thermal annealing with and without ‘learning’ to explore typical computational problems. We also construct a mapping of the vertex model into the Chimera architecture of the D-Wave machine, initiating an approach to reversible classical computation based on state-of-the-art implementations of quantum annealing."
CAREY MOREWEDGE,Evolution of consumption: a psychological ownership framework,"Technological innovations are creating new products, services, and markets that satisfy enduring consumer needs. These technological innovations create value for consumers and firms in many ways, but they also disrupt psychological ownership––the feeling that a thing is “MINE.” The authors describe two key dimensions of this technology-driven evolution of consumption pertaining to psychological ownership: (1) replacing legal ownership of private goods with legal access rights to goods and services owned and used by others and (2) replacing “solid” material goods with “liquid” experiential goods. They propose that these consumption changes can have three effects on psychological ownership: they can threaten it, cause it to transfer to other targets, and create new opportunities to preserve it. These changes and their effects are organized in a framework and examined across three macro trends in marketing: (1) growth of the sharing economy, (2) digitization of goods and services, and (3) expansion of personal data. This psychological ownership framework generates future research opportunities and actionable marketing strategies for firms aiming to preserve the positive consequences of psychological ownership and navigate cases for which it is a liability."
CAREY MOREWEDGE,Which social comparisons influence happiness with unequal pay?,"We examine which social comparisons most affect happiness with pay that is unequally distributed (e.g., salaries and bonuses). We find that ensemble representation-attention to statistical properties of distributions such as their range and mean-makes the proximal extreme (i.e., the maximum or minimum) and distribution mean salient social comparison standards. Happiness with a salary or bonus is more affected by how it compares to the distribution mean and proximal extreme than by exemplar-based properties of the payment, like its comparison to the nearest payment or its distribution rank. This holds for randomly assigned and performance-based payments. Process studies demonstrate that ensemble representations lead people to spontaneously select these statistical properties of pay distributions as comparison standards. Exogenously increasing the salience of less extreme exemplars moderates the influence of the maximum on happiness with pay, but exogenously increasing the salience of the distribution maximum does not. As with other social comparison standards, top-down information moderates their selection. Happiness with a bonus payment is influenced by the largest payment made to others who solve the same math problems, for instance, but not by the largest payment made to others who solve different verbal problems. Our findings yield theoretical and practical insights about which members of groups are selected as social comparison standards, effects of relative income on happiness, and the attentional processes involved in ensemble representation. (PsycInfo Database Record (c) 2020 APA, all rights reserved)."
CAREY MOREWEDGE,Decision making can be improved through observational learning,"Observational learning can debias judgment and decision making. One-shot observational learning-based training interventions (akin to “hot seating”) can produce reductions in cognitive biases in the laboratory (i.e., anchoring, representativeness, and social projection), and successfully teach a decision rule that increases advice taking in a weight on advice paradigm (i.e., the averaging principle). These interventions improve judgment, rule learning, and advice taking more than practice. We find observational learning-based interventions can be as effective as information-based interventions. Their effects are additive for advice taking, and for accuracy when advice is algorithmically optimized. As found in the organizational learning literature, explicit knowledge transferred through information appears to reduce the stickiness of tacit knowledge transferred through observational learning. Moreover, observational learning appears to be a unique debiasing training strategy, an addition to the four proposed by Fischhoff (1982). We also report new scales measuring individual differences in anchoring, representativeness heuristics, and social projection."
CAREY MOREWEDGE,"Preference for human, not algorithm aversion","People sometimes exhibit a costly preference for humans relative to algorithms, which is often defined as a domain-general algorithm aversion. I propose it is instead driven by biased evaluations of self and other humans, which occurs more narrowly in domains where identity is threatened and when evaluative criteria are ambiguous."
CAREY MOREWEDGE,Resistance to medical artificial intelligence is an attribute in a compensatory decision process: response to Pezzo and Becksted (2020),"In Longoni et al. (2019), we examine how algorithm aversion influences utilization of healthcare delivered by human and artificial intelligence providers. Pezzo and Becksted’s (2020) commentary asks whether resistance to medical AI takes the form of a noncompensatory decision strategy, in which a single attribute determines provider choice, or whether resistance to medical AI is one of several attributes considered in a compensatory decision strategy. We clarify that our paper both claims and finds that, all else equal, resistance to medical AI is one of several attributes (e.g., cost and performance) influencing healthcare utilization decisions. In other words, resistance to medical AI is a consequential input to compensatory decisions regarding healthcare utilization and provider choice decisions, not a noncompensatory decision strategy. People do not always reject healthcare provided by AI, and our article makes no claim that they do."
CAREY MOREWEDGE,Debiasing training improves decision making in the field,"The primary objection to debiasing-training interventions is a lack of evidence that they improve decision making in field settings, where reminders of bias are absent. We gave graduate students in three professional programs (N = 290) a one-shot training intervention that reduces confirmation bias in laboratory experiments. Natural variance in the training schedule assigned participants to receive training before or after solving an unannounced business case modeled on the decision to launch the Space Shuttle Challenger. We used case solutions to surreptitiously measure participants' susceptibility to confirmation bias. Trained participants were 29% less likely to choose the inferior hypothesis-confirming solution than untrained participants. Analysis of case write-ups suggests that a reduction in confirmatory hypothesis testing accounts for their improved decision making in the case. The results provide promising evidence that debiasing-training effects transfer to field settings and can improve decision making in professional and private life."
CAREY MOREWEDGE,"Correction, uncertainty, and anchoring effects","We compare the predictions of two important proposals made by De Neys to findings in the anchoring effect literature. Evidence for an anchoring-and-adjustment heuristic supports his proposal that System 1 and System 2 are non-exclusive. The relationship between psychophysical noise and anchoring effects, however, challenges his proposal that epistemic uncertainty determines the involvement of System 2 corrective processes in judgment."
ANITA L DESTEFANO,Estrogen-related and other disease diagnoses preceding Parkinson's disease,"PURPOSE: Estrogen exposure has been associated with the occurrence of Parkinson's disease (PD), as well as many other disorders, and yet the mechanisms underlying these relations are often unknown. While it is likely that estrogen exposure modifies the risk of various diseases through many different mechanisms, some estrogen-related disease processes might work in similar manners and result in association between the diseases. Indeed, the association between diseases need not be due only to estrogen-related factors, but due to similar disease processes from a variety of mechanisms. PATIENTS AND METHODS: All female Parkinson's disease cases between 1982 and 2007 (n = 12,093) were identified from the Danish National Registry of Patients, along with 10 controls matched by years of birth and enrollment. Conditional logistic regressions (CLR) were used to calculate risk of PD after diagnosis of the estrogen-related diseases, endometriosis and osteoporosis, conditioning on years of birth and enrollment. To identify novel associations between PD and any other preceding conditions, CLR was also used to calculate the odds ratios (ORs) for risk of PD for 202 different categories of preceding disease diagnoses. Empirical Bayes methods were used to identify the robust associations from the over 200 associations produced by this analysis. RESULTS: We found a positive association between osteoporosis and osteoporotic fractures and PD (OR = 1.18, 95% confidence interval [CI] of 1.08–1.28), while a lack of association was observed between endometriosis and PD (OR = 1.37, 95% CI 0.99–1.90). Using empirical Bayes analyses, 24 additional categories of diseases, likely unrelated to estrogen exposure, were also identified as potentially associated with PD. CONCLUSION: We identified several novel associations, which may provide insight into common causal mechanisms between the diseases or greater understanding of potential early preclinical signs of PD. In particular, the associations with several categories of mental disorders suggest that these may be early warning signs of PD onset or these diseases (or the causes of these diseases) may predispose to PD."
ANITA L DESTEFANO,The Framingham Heart Study 100K SNP Genome-Wide Association Study Resource: Overview of 17 Phenotype Working Group Reports,"BACKGROUND: The Framingham Heart Study (FHS), founded in 1948 to examine the epidemiology of cardiovascular disease, is among the most comprehensively characterized multi-generational studies in the world. Many collected phenotypes have substantial genetic contributors; yet most genetic determinants remain to be identified. Using single nucleotide polymorphisms (SNPs) from a 100K genome-wide scan, we examine the associations of common polymorphisms with phenotypic variation in this community-based cohort and provide a full-disclosure, web-based resource of results for future replication studies. METHODS: Adult participants (n = 1345) of the largest 310 pedigrees in the FHS, many biologically related, were genotyped with the 100K Affymetrix GeneChip. These genotypes were used to assess their contribution to 987 phenotypes collected in FHS over 56 years of follow up, including: cardiovascular risk factors and biomarkers; subclinical and clinical cardiovascular disease; cancer and longevity traits; and traits in pulmonary, sleep, neurology, renal, and bone domains. We conducted genome-wide variance components linkage and population-based and family-based association tests. RESULTS: The participants were white of European descent and from the FHS Original and Offspring Cohorts (examination 1 Offspring mean age 32 ± 9 years, 54% women). This overview summarizes the methods, selected findings and limitations of the results presented in the accompanying series of 17 manuscripts. The presented association results are based on 70,897 autosomal SNPs meeting the following criteria: minor allele frequency ≥ 10%, genotype call rate ≥ 80%, Hardy-Weinberg equilibrium p-value ≥ 0.001, and satisfying Mendelian consistency. Linkage analyses are based on 11,200 SNPs and short-tandem repeats. Results of phenotype-genotype linkages and associations for all autosomal SNPs are posted on the NCBI dbGaP website at. CONCLUSION: We have created a full-disclosure resource of results, posted on the dbGaP website, from a genome-wide association study in the FHS. Because we used three analytical approaches to examine the association and linkage of 987 phenotypes with thousands of SNPs, our results must be considered hypothesis-generating and need to be replicated. Results from the FHS 100K project with NCBI web posting provides a resource for investigators to identify high priority findings for replication."
ANITA L DESTEFANO,Genomewide Association Study for Onset Age in Parkinson Disease,"BACKGROUND: Age at onset in Parkinson disease (PD) is a highly heritable quantitative trait for which a significant genetic influence is supported by multiple segregation analyses. Because genes associated with onset age may represent invaluable therapeutic targets to delay the disease, we sought to identify such genetic modifiers using a genomewide association study in familial PD. There have been previous genomewide association studies (GWAS) to identify genes influencing PD susceptibility, but this is the first to identify genes contributing to the variation in onset age. METHODS: Initial analyses were performed using genotypes generated with the Illumina HumanCNV370Duo array in a sample of 857 unrelated, familial PD cases. Subsequently, a meta-analysis of imputed SNPs was performed combining the familial PD data with that from a previous GWAS of 440 idiopathic PD cases. The SNPs from the meta-analysis with the lowest p-values and consistency in the direction of effect for onset age were then genotyped in a replication sample of 747 idiopathic PD cases from the Parkinson Institute Biobank of Milan, Italy. RESULTS: Meta-analysis across the three studies detected consistent association (p < 1 × 10-5) with five SNPs, none of which reached genomewide significance. On chromosome 11, the SNP with the lowest p-value (rs10767971; p = 5.4 × 10-7) lies between the genes QSER1 and PRRG4. Near the PARK3 linkage region on chromosome 2p13, association was observed with a SNP (rs7577851; p = 8.7 × 10-6) which lies in an intron of the AAK1 gene. This gene is closely related to GAK, identified as a possible PD susceptibility gene in the GWAS of the familial PD cases. CONCLUSION: Taken together, these results suggest an influence of genes involved in endocytosis and lysosomal sorting in PD pathogenesis."
ANITA L DESTEFANO,The Gly2019Ser Mutation in LRRK2 Is not Fully Penetrant in Familial Parkinson's Disease: The GenePD Study,"BACKGROUND: We report age-dependent penetrance estimates for leucine-rich repeat kinase 2 (LRRK2)-related Parkinson's disease (PD) in a large sample of familial PD. The most frequently seen LRRK2 mutation, Gly2019Ser (G2019S), is associated with approximately 5 to 6% of familial PD cases and 1 to 2% of idiopathic cases, making it the most common known genetic cause of PD. Studies of the penetrance of LRRK2 mutations have produced a wide range of estimates, possibly due to differences in study design and recruitment, including in particular differences between samples of familial PD versus sporadic PD. METHODS: A sample, including 903 affected and 58 unaffected members from 509 families ascertained for having two or more PD-affected members, 126 randomly ascertained PD patients and 197 controls, was screened for five different LRRK2 mutations. Penetrance was estimated in families of LRRK2 carriers with consideration of the inherent bias towards increased penetrance in a familial sample. RESULTS: Thirty-one out of 509 families with multiple cases of PD (6.1%) were found to have 58 LRRK2 mutation carriers (6.4%). Twenty-nine of the 31 families had G2019S mutations while two had R1441C mutations. No mutations were identified among controls or unaffected relatives of PD cases. Nine PD-affected relatives of G2019S carriers did not carry the LRRK2 mutation themselves. At the maximum observed age range of 90 to 94 years, the unbiased estimated penetrance was 67% for G2019S families, compared with a baseline PD risk of 17% seen in the non-LRRK2-related PD families. CONCLUSION: Lifetime penetrance of LRRK2 estimated in the unascertained relatives of multiplex PD families is greater than that reported in studies of sporadically ascertained LRRK2 cases, suggesting that inherited susceptibility factors may modify the penetrance of LRRK2 mutations. In addition, the presence of nine PD phenocopies in the LRRK2 families suggests that these susceptibility factors may also increase the risk of non-LRRK2-related PD. No differences in penetrance were found between men and women, suggesting that the factors that influence penetrance for LRRK2 carriers are independent of the factors which increase PD prevalence in men."
ANITA L DESTEFANO,Risk of Parkinson's Disease after Tamoxifen Treatment,"BACKGROUND: Women have a reduced risk of developing Parkinson's disease (PD) compared with age-matched men. Neuro-protective effects of estrogen potentially explain this difference. Tamoxifen, commonly used in breast cancer treatment, may interfere with the protective effects of estrogen and increase risk of PD. We compared the rate of PD in Danish breast cancer patients treated with tamoxifen to the rate among those not treated with tamoxifen. METHODS: A cohort of 15,419 breast cancer patients identified from the Danish Breast Cancer Collaborative Group database was linked to the National Registry of Patients to identify PD diagnoses. Overall risk and rate of PD following identification into the study was compared between patients treated with tamoxifen as adjuvant hormonal therapy and patients not receiving tamoxifen. Time-dependent effects of tamoxifen treatment on PD rate were examined to estimate the likely induction period for tamoxifen. RESULTS: In total, 35 cases of PD were identified among the 15,419 breast cancer patients. No overall effect of tamoxifen on rate of PD was observed (HR = 1.3, 95% CI: 0.64-2.5), but a PD hazard ratio of 5.1 (95% CI: 1.0-25) was seen four to six years following initiation of tamoxifen treatment. CONCLUSIONS: These results provide evidence that the neuro-protective properties of estrogen against PD occurrence may be disrupted by tamoxifen therapy. Tamoxifen treatments may be associated with an increased rate of PD; however these effects act after four years, are of limited duration, and the adverse effect is overwhelmed by the protection against breast recurrence conferred by tamoxifen therapy."
ANITA L DESTEFANO,Genetic Correlates of Brain Aging on MRI and Cognitive Test Measures: A Genome-Wide Association and Linkage Analysis in the Framingham Study,"BACKGROUND: Brain magnetic resonance imaging (MRI) and cognitive tests can identify heritable endophenotypes associated with an increased risk of developing stroke, dementia and Alzheimer's disease (AD). We conducted a genome-wide association (GWA) and linkage analysis exploring the genetic basis of these endophenotypes in a community-based sample. METHODS: A total of 705 stroke- and dementia-free Framingham participants (age 62 +9 yrs, 50% male) who underwent volumetric brain MRI and cognitive testing (1999–2002) were genotyped. We used linear models adjusting for first degree relationships via generalized estimating equations (GEE) and family based association tests (FBAT) in additive models to relate qualifying single nucleotide polymorphisms (SNPs, 70,987 autosomal on Affymetrix 100K Human Gene Chip with minor allele frequency ≥ 0.10, genotypic call rate ≥ 0.80, and Hardy-Weinberg equilibrium p-value ≥ 0.001) to multivariable-adjusted residuals of 9 MRI measures including total cerebral brain (TCBV), lobar, ventricular and white matter hyperintensity (WMH) volumes, and 6 cognitive factors/tests assessing verbal and visuospatial memory, visual scanning and motor speed, reading, abstract reasoning and naming. We determined multipoint identity-by-descent utilizing 10,592 informative SNPs and 613 short tandem repeats and used variance component analyses to compute LOD scores. RESULTS: The strongest gene-phenotype association in FBAT analyses was between SORL1 (rs1131497; p = 3.2 × 10-6) and abstract reasoning, and in GEE analyses between CDH4 (rs1970546; p = 3.7 × 10-8) and TCBV. SORL1 plays a role in amyloid precursor protein processing and has been associated with the risk of AD. Among the 50 strongest associations (25 each by GEE and FBAT) were other biologically interesting genes. Polymorphisms within 28 of 163 candidate genes for stroke, AD and memory impairment were associated with the endophenotypes studied at p < 0.001. We confirmed our previously reported linkage of WMH on chromosome 4 and describe linkage of reading performance to a marker on chromosome 18 (GATA11A06), previously linked to dyslexia (LOD scores = 2.2 and 5.1). CONCLUSION: Our results suggest that genes associated with clinical neurological disease also have detectable effects on subclinical phenotypes. These hypothesis generating data illustrate the use of an unbiased approach to discover novel pathways that may be involved in brain aging, and could be used to replicate observations made in other studies."
ANITA L DESTEFANO,The Framingham Heart Study 100K SNP Genome-Wide Association Study Resource: Overview of 17 Phenotype Working Group Reports,"BACKGROUND: The Framingham Heart Study (FHS), founded in 1948 to examine the epidemiology of cardiovascular disease, is among the most comprehensively characterized multi-generational studies in the world. Many collected phenotypes have substantial genetic contributors; yet most genetic determinants remain to be identified. Using single nucleotide polymorphisms (SNPs) from a 100K genome-wide scan, we examine the associations of common polymorphisms with phenotypic variation in this community-based cohort and provide a full-disclosure, web-based resource of results for future replication studies. METHODS: Adult participants (n = 1345) of the largest 310 pedigrees in the FHS, many biologically related, were genotyped with the 100K Affymetrix GeneChip. These genotypes were used to assess their contribution to 987 phenotypes collected in FHS over 56 years of follow up, including: cardiovascular risk factors and biomarkers; subclinical and clinical cardiovascular disease; cancer and longevity traits; and traits in pulmonary, sleep, neurology, renal, and bone domains. We conducted genome-wide variance components linkage and population-based and family-based association tests. RESULTS: The participants were white of European descent and from the FHS Original and Offspring Cohorts (examination 1 Offspring mean age 32 ± 9 years, 54% women). This overview summarizes the methods, selected findings and limitations of the results presented in the accompanying series of 17 manuscripts. The presented association results are based on 70,897 autosomal SNPs meeting the following criteria: minor allele frequency ≥ 10%, genotype call rate ≥ 80%, Hardy-Weinberg equilibrium p-value ≥ 0.001, and satisfying Mendelian consistency. Linkage analyses are based on 11,200 SNPs and short-tandem repeats. Results of phenotype-genotype linkages and associations for all autosomal SNPs are posted on the NCBI dbGaP website at. CONCLUSION: We have created a full-disclosure resource of results, posted on the dbGaP website, from a genome-wide association study in the FHS. Because we used three analytical approaches to examine the association and linkage of 987 phenotypes with thousands of SNPs, our results must be considered hypothesis-generating and need to be replicated. Results from the FHS 100K project with NCBI web posting provides a resource for investigators to identify high priority findings for replication."
ANITA L DESTEFANO,Two-Stage Approach for Identifying Single-Nucleotide Polymorphisms Associated with Rheumatoid Arthritis Using Random Forests and Bayesian Networks,"We used the simulated data set from Genetic Analysis Workshop 15 Problem 3 to assess a two-stage approach for identifying single-nucleotide polymorphisms (SNPs) associated with rheumatoid arthritis (RA). In the first stage, we used random forests (RF) to screen large amounts of genetic data using the variable importance measure, which takes into account SNP interaction effects as well as main effects without requiring model specification. We used the simulated 9187 SNPs mimicking a 10 K SNP chip, along with covariates DR (the simulated DRB1 gentoype), smoking, and sex as input to the RF analyses with a training set consisting of 750 unrelated RA cases and 750 controls. We used an iterative RF screening procedure to identify a smaller set of variables for further analysis. In the second stage, we used the software program CaMML for producing Bayesian networks, and developed complex etiologic models for RA risk using the variables identified by our RF screening procedure. We evaluated the performance of this method using independent test data sets for up to 100 replicates."
ANITA L DESTEFANO,Genetic Analyses of Longitudinal Phenotype Data: A Comparison of Univariate Methods and a Multivariate Approach,"BACKGROUND. We explored three approaches to heritability and linkage analyses of longitudinal total cholesterol levels (CHOL) in the Genetic Analysis Workshop 13 simulated data without knowing the answers. The first two were univariate approaches and used 1) baseline measure at exam one or 2) summary measures such as mean and slope from multiple exams. The third method was a multivariate approach that directly models multiple measurements on a subject. A variance components model (SOLAR) was employed in the univariate approaches. A mixed regression model with polynomials was employed in the multivariate approach and implemented in SAS/IML. RESULTS. Using the baseline measure at exam 1, we detected all baseline or slope genes contributing a substantial amount (0.08) of variance (LOD > 3). Compared to the baseline measure, the mean measures yielded slightly higher LOD at the slope genes, and a lower LOD at the baseline genes. The slope measure produced a somewhat lower LOD for the slope gene than did the mean measure. Descriptive information on the pattern of changes in gene effects with age was estimated for three linked loci by the third approach. CONCLUSION. We found simple univariate methods may be effective to detect genes affecting longitudinal phenotypes but may not fully reveal temporal trends in gene effects. The relative efficiency of the univariate methods to detect genes depends heavily on the underlying model. Compared with the univariate approaches, the multivariate approach provided more information on temporal trends in gene effects at the cost of more complicated modelling and more intense computations."
ANITA L DESTEFANO,The Framingham Heart Study 100K SNP genome-wide association study resource: overview of 17 phenotype working group reports,"BACKGROUND:The Framingham Heart Study (FHS), founded in 1948 to examine the epidemiology of cardiovascular disease, is among the most comprehensively characterized multi-generational studies in the world. Many collected phenotypes have substantial genetic contributors; yet most genetic determinants remain to be identified. Using single nucleotide polymorphisms (SNPs) from a 100K genome-wide scan, we examine the associations of common polymorphisms with phenotypic variation in this community-based cohort and provide a full-disclosure, web-based resource of results for future replication studies.METHODS:Adult participants (n = 1345) of the largest 310 pedigrees in the FHS, many biologically related, were genotyped with the 100K Affymetrix GeneChip. These genotypes were used to assess their contribution to 987 phenotypes collected in FHS over 56 years of follow up, including: cardiovascular risk factors and biomarkers; subclinical and clinical cardiovascular disease; cancer and longevity traits; and traits in pulmonary, sleep, neurology, renal, and bone domains. We conducted genome-wide variance components linkage and population-based and family-based association tests.RESULTS:The participants were white of European descent and from the FHS Original and Offspring Cohorts (examination 1 Offspring mean age 32 +/- 9 years, 54% women). This overview summarizes the methods, selected findings and limitations of the results presented in the accompanying series of 17 manuscripts. The presented association results are based on 70,897 autosomal SNPs meeting the following criteria: minor allele frequency [greater than or equal to] 10%, genotype call rate [greater than or equal to] 80%, Hardy-Weinberg equilibrium p-value [greater than or equal to] 0.001, and satisfying Mendelian consistency. Linkage analyses are based on 11,200 SNPs and short-tandem repeats. Results of phenotype-genotype linkages and associations for all autosomal SNPs are posted on the NCBI dbGaP website at http://www.ncbi.nlm.nih.gov/projects/gap/cgi-bin/study.cgi?id=phs000007.CONCLUSION:We have created a full-disclosure resource of results, posted on the dbGaP website, from a genome-wide association study in the FHS. Because we used three analytical approaches to examine the association and linkage of 987 phenotypes with thousands of SNPs, our results must be considered hypothesis-generating and need to be replicated. Results from the FHS 100K project with NCBI web posting provides a resource for investigators to identify high priority findings for replication."
STEPHEN SCULLY,A Homer for the twenty-first century,
STEPHEN SCULLY,Hephaistos’ shield and Achilles’ delight: a study of Iliad XVIII and XIX,
STEPHEN SCULLY,Studies towards the diazobenzofluorene natural products: total synthesis of the epoxykinamycin FL-120B',"Diazobenzofluorene natural products are a unique class of compounds that contain a diazo moiety--a functionality sparsely found in nature. This family of molecules is comprised of the monomeric kinamycins and dimeric lomaiviticins, both of which have demonstrated potent antibacterial and anticancer activity. Synthetic studies will allow larger access to better understand the properties of these important natural products. FL-120B' was targeted to establish a synthetic route to epoxide-containing diazobenzofluorenes, which may serve as precursors to the lomaiviticins. The general strategy towards FL-120B' follows our group's previous synthesis of kinamycin C. In addition to these prior efforts, an efficient and scalable synthesis of a chiral epoxide building block was established. Furthermore, Stille cross coupling and intramolecular Friedei-Crafts acylation provided the tetracyclic carbon framework of FL-120B'. The successful outcome of the Friedei-Crafts cyclization was found to be temperature and substrate dependent, in which five carboxylic acid intermediates - differing in protecting groups - were screened. In attempts to perform a photo-Friedel-Crafts cyclization, a novel photodecarbonylation reaction to form highly substituted benzofurans was discovered. To complete the synthesis, a benzofluorene resulting from intramolecular FriedelCrafts acylation of a Hoc-protected carboxylic acid was further functionalized. Bissilylation, mesylhydrazone formation, and an oxidation with spontaneous desulfination formed the diazo and resulting bis-silyl-protected FL-120B'. Desilylation gave FL-120B' and provided the first total synthesis of an epoxykinamycin. Furthermore, studies towards the synthesis of the lomaiviticins have been initiated and involve the synthesis of the requisite cyclohexene moiety."
CHERYL D KNOTT,Field collection and preservation of urine in orangutans and chimpanzees,"Disease is recognized as a critical factor that can affect primate behavior, yet few methods allow for the quantification of disease states in wild primates. This paper reports on the use of urinary test strips to detect the presence of disease and monitor physiological status in wild orangutans . Urine was collected from wild orangutans at Gunung Palung National Park, Indonesian Borneo, between August 1994 and August 1995. A total of 387 urine samples were obtained from over 43 orangutans by placing plastic sheets beneath individuals during urination. Boehringer Mannheim urinary test strips were used to evaluate specific gravity, leukocytes, nitrite, pH, ketones, protein, glucose, urobilinogen, bilirubin and blood . Objectives of using these test strips were (1) to evaluate the presence of disease (2) to detect signs of nutritional stress (3) to monitor the occurrence of menstruation and (4) to use specific gravity as a measure of urine concentration for hormonal analysis. [TRUNCATED]"
CHERYL D KNOTT,Are male orangutans a threat to infants? Evidence of mother-offspring counter strategies to infanticide in Bornean orangutans (Pongo pygmaeus wurmbii),"Sexually selected infanticide by males is widespread in primates. Female primates employ a variety of strategies to reduce infanticide risk. While infanticide has never been directly observed in wild orangutans (Pongo spp.), their slow life history makes infants vulnerable to infanticide. The mating strategies of female orangutans include polyandrous and postconceptive mating that may serve to increase paternity confusion, an infanticide avoidance strategy. Here, we investigate whether female orangutans alter their social interactions with males as another infanticide avoidance strategy. We hypothesize that females with younger offspring avoid males and that the distance between mother and offspring decreases in the presence of males. We use long-term behavioral data collected between 1994 and 2016 from Bornean orangutans (Pongo pygmaeus wurmbii) in Gunung Palung National Park, Indonesia, to test whether the sexual selection hypothesis for infanticide helps explain aspects of orangutan social behavior. We found that mothers with offspring <6 yr. old both encountered fewer males and spent less time with males during social interactions than did mothers with offspring >6 yr. old and females without offspring. In addition, the distance between a mother–offspring dyad showed a statistically significant decrease in the presence of males, but not females. Our results are consistent with the hypothesis that female orangutans employ strategies to reduce infanticide risk in their social interactions. Because orangutans have a high fission–fusion dynamic, they have flexibility in manipulating social interactions as a counter-infanticide strategy. Our results suggest that infanticide by males is a selective pressure shaping female orangutan social behavior."
CHERYL D KNOTT,Orangutans in perspective: Forced copulations and female mating resistance,
CHERYL D KNOTT,Wild Bornean orangutans (Pongo pygmaeus wurmbii) navigate to non-fruit foods,
CHERYL D KNOTT,Possible male infanticide in wild orangutans and a re-evaluation of infanticide risk,"Infanticide as a male reproductive tactic is widespread across mammals, and is particularly prevalent in catarrhine primates. While it has never been observed in wild orangutans, infanticide by non-sire males has been predicted to occur due to their extremely long inter-birth intervals, semi-solitary social structure, and the presence of female counter-tactics to infanticide. Here, we report on the disappearance of a healthy four-month-old infant, along with a serious foot injury suffered by the primiparous mother. No other cases of infant mortality have been observed at this site in 30 years of study. Using photographic measurements of the injury, and information on the behavior and bite size of potential predators, we evaluate the possible causes of this injury. The context, including the behavior of the female and the presence of a new male at the time of the injury, lead us to conclude that the most likely cause of the infant loss and maternal injury was male infanticide. We suggest that in orangutans, and other species where nulliparous females are not preferred mates, these females may be less successful at using paternity confusion as an infanticide avoidance tactic, thus increasing the likelihood of infanticide of their first-born infants."
CHERYL D KNOTT,Reducing the primate pet trade: actions for primatologists,"This commentary emerged from a panel presentation at the International Primatological Society Congress in Nairobi, Kenya, 2018. The goal was to provide regional updates on the status of primate removal from habitat countries, especially for the pet trade, and develop guidelines that could help primatologists address this critical problem. The trade in live primates includes those used as pets, in entertainment, and as subjects of biomedical experimentation, but here we focus on those primates destined for the pet trade. Such transactions are a hugely lucrative business, impacting hundreds of thousands of individuals annually and affecting the survival of wild populations. Being intimately familiar with primate social behavior, life history and biology, primatologists, whether they work with captive or wild primates, are in a unique position to understand the nature of the trade and attempt to counter its effects. In addition to updating the status of the primate pet trade, we provide recommendations that may help primatologists formulate a plan to deal, locally and regionally, with illegal trafficking in live primates. General guidelines include increasing awareness of local customs, policies and laws; developing collaborative research opportunities for local people; engaging in training/informational opportunities; and instructing on how to take action when encountering illegally-trafficked primates."
CHERYL D KNOTT,"Habituation, avoidance strategies, and social learning in wild Bornean Orangutans in Gunung Palung National Park, Indonesia",
CHERYL D KNOTT,Low testosterone correlates with delayed development in male orangutans,"Male orangutans (Pongo spp.) display an unusual characteristic for mammals in that some adult males advance quickly to full secondary sexual development while others can remain in an adolescent-like form for a decade or more past the age of sexual maturity. Remarkably little is understood about how and why differences in developmental timing occur. While fully-developed males are known to produce higher androgen levels than arrested males, the longer-term role of steroid hormones in male life history variation has not been examined. We examined variation in testosterone and cortisol production among 18 fully-developed (“flanged”) male orangutans in U.S. captive facilities. Our study revealed that while testosterone levels did not vary significantly according to current age, housing condition, and species origin, males that had undergone precocious development had higher testosterone levels than males that had experienced developmental arrest. While androgen variation had previously been viewed as a state-dependent characteristic of male developmental status, our study reveals that differences in the physiology of early and late developing males are detectable long past the developmental transition and may instead be trait-level characteristics associated with a male’s life history strategy. Further studies are needed to determine how early in life differences in testosterone levels emerge and what consequences this variation may have for male behavioral strategies."
CHERYL D KNOTT,Population-specific use of the same tool-assisted alarm call between two wild orangutan populations (Pongopygmaeus wurmbii) indicates functional arbitrariness,"Arbitrariness is an elementary feature of human language, yet seldom an object of comparative inquiry. While arbitrary signals for the same function are relatively frequent between animal populations across taxa, the same signal with arbitrary functions is rare and it remains unknown whether, in parallel with human speech, it may involve call production in animals. To investigate this question, we examined a particular orangutan alarm call – the kiss-squeak – and two variants – hand and leaf kiss-squeaks. In Tuanan (Central Kalimantan, Indonesia), the acoustic frequency of unaided kiss-squeaks is negatively related to body size. The modified variants are correlated with perceived threat and are hypothesized to increase the perceived body size of the sender, as the use of a hand or leaves lowers the kiss-squeak’s acoustic frequency. We examined the use of these variants in the same context in another orangutan population of the same sub-species and with partially similar habitat at Cabang Panti (West Kalimantan, Indonesia). Identical analyses of data from this site provided similar results for unaided kiss-squeaks but dissimilar results for hand and leaf kiss-squeaks. Unaided kiss-squeaks at Cabang Panti were emitted as commonly and showed the same relationship to body size as in Tuanan. However, at Cabang Panti, hand kiss-squeaks were extremely rare, while leaf-use neither conveyed larger body size nor was related to perceived threat. These findings indicate functional discontinuity between the two sites and therefore imply functional arbitrariness of leaf kiss-squeaks. These results show for the first time the existence of animal signals involving call production with arbitrary function. Our findings are consistent with previous studies arguing that these orangutan call variants are socially learned and reconcile the role of gestures and calls within evolutionary theories based on common ancestry for speech and music."
CHERYL D KNOTT,Sociality predicts orangutan vocal phenotype,"In humans, individuals' social setting determines which and how language is acquired. Social seclusion experiments show that sociality also guides vocal development in songbirds and marmoset monkeys, but absence of similar great ape data has been interpreted as support to saltational notions for language origin, even if such laboratorial protocols are unethical with great apes. Here we characterize the repertoire entropy of orangutan individuals and show that in the wild, different degrees of sociality across populations are associated with different 'vocal personalities' in the form of distinct regimes of alarm call variants. In high-density populations, individuals are vocally more original and acoustically unpredictable but new call variants are short lived, whereas individuals in low-density populations are more conformative and acoustically consistent but also exhibit more complex call repertoires. Findings provide non-invasive evidence that sociality predicts vocal phenotype in a wild great ape. They prove false hypotheses that discredit great apes as having hardwired vocal development programmes and non-plastic vocal behaviour. Social settings mould vocal output in hominids besides humans."
CHERYL D KNOTT,Dataset for: Female choice via facultative associations and mutual maintenance of consortships in Bornean orangutans,
BRETT T LITZ,Prolonged Grief Disorder: Psychometric Validation of Criteria Proposed for DSM-V and ICD-11,"Holly Prigerson and colleagues tested the psychometric validity of criteria for prolonged grief disorder (PGD) to enhance the detection and care of bereaved individuals at heightened risk of persistent distress and dysfunction. BACKGROUND. Bereavement is a universal experience, and its association with excess morbidity and mortality is well established. Nevertheless, grief becomes a serious health concern for a relative few. For such individuals, intense grief persists, is distressing and disabling, and may meet criteria as a distinct mental disorder. At present, grief is not recognized as a mental disorder in the DSM-IV or ICD-10. The goal of this study was to determine the psychometric validity of criteria for prolonged grief disorder (PGD) to enhance the detection and potential treatment of bereaved individuals at heightened risk of persistent distress and dysfunction. METHODS AND FINDINGS. A total of 291 bereaved respondents were interviewed three times, grouped as 0–6, 6–12, and 12–24 mo post-loss. Item response theory (IRT) analyses derived the most informative, unbiased PGD symptoms. Combinatoric analyses identified the most sensitive and specific PGD algorithm that was then tested to evaluate its psychometric validity. Criteria require reactions to a significant loss that involve the experience of yearning (e.g., physical or emotional suffering as a result of the desired, but unfulfilled, reunion with the deceased) and at least five of the following nine symptoms experienced at least daily or to a disabling degree: feeling emotionally numb, stunned, or that life is meaningless; experiencing mistrust; bitterness over the loss; difficulty accepting the loss; identity confusion; avoidance of the reality of the loss; or difficulty moving on with life. Symptoms must be present at sufficiently high levels at least six mo from the death and be associated with functional impairment. CONCLUSIONS. The criteria set for PGD appear able to identify bereaved persons at heightened risk for enduring distress and dysfunction. The results support the psychometric validity of the criteria for PGD that we propose for inclusion in DSM-V and ICD-11."
WILLIAM KAHN,Navigating space for personal agency: auxiliary routines as adaptations in toxic organizations,"Many workers experience organization dysfunction stemming from leaders. Yet organization members have limited responses; they can directly or indirectly confront senior leaders, engage individual stress coping strategies, or leave the organization. We offer another response by theorizing auxiliary routines as behavioral sequences through which multiple actors coordinate responses to complex and enduring socioemotional dynamics that threaten to undermine the enactment of standard operating task routines. Through a qualitative, inductive study of a consulting firm, we delimit three auxiliary routines—absorption, dissemination, and differentiation—through which people navigate between the destructiveness of organizational toxicity and the need to perform given roles and tasks. We illustrate how these routines emerged in response to role and psychological diminishment originating from senior leaders, how the routines helped manage and sometimes perpetuate diminishment, and the consequences for individuals’ personal agency and the organization-as-a-whole. In doing so, we contribute to knowledge about coping with toxic organizational conditions and on routines as a facet of emotional capability in organizations."
WILLIAM KAHN,Discoveries-through-Prose Nobody home: a parallel process investigation of a child welfare agency,"During action research related to the persistent burnout of social workers in a child welfare agency, I discovered that members regularly referred to how the agency was “just like” client families. I explore the possibility that the likeness between the two social systems was not simply coincidental but reflected a parallel process by which client family issues were unconsciously “absorbed” into the agency. Parallel processes related to individuals and groups have been noted anecdotally but have not been examined in organization-environment relations. I collected and analyzed data to uncover key parallels and infer the absorption processes by which they were created. I discuss those parallels as agency reenactments of key client family dynamics—of disconnection, abuse, neglect—and note individual and collective collusions that maintain those dynamics. I thus show how what happened in the agency to enable persistent social worker burnout was intimately related to what happened to the agency at its boundary with key aspects of its environment. This realization expands possibilities for scholars to take seriously the subterranean flow of emotions across organizational-environment boundaries that shape the absorptive capacity of organizations."
WILLIAM KAHN,Exploring gender bias in six key domains of academic science: an adversarial collaboration,"Claims of gender bias in academic science have been widely published, including general descriptions of systemic societal factors that limit women—such as their roles as primary parents and caregivers—and more specific statements asserting sexism at key evaluation points of academic careers. We comprehensively reviewed the evidence in published research regarding differential treatment by gender for six key evaluation domains in the tenure-track academy: hiring, grant funding, journal acceptances, teaching ratings, recommendation letters, and salary, over a 20-year period (2000 to 2020). We focused on these specific domains because they are readily operationalizable and they are represented across a vast literature available for quantitative analysis. Contrary to omnipresent claims in top journals and the media, we found that tenure-track women are at parity with men in three domains (U.S. grant funding, journal acceptances, and recommendation letters), and women are advantaged over men in the domain of hiring. However, for teaching ratings and salary, we found evidence of bias against women. In the four domains in which we failed to find evidence of bias against women, we nevertheless acknowledge that broad societal structural factors may still impede women’s advancement in academic science. We suggest that efforts and resources to combat bias be redirected and focused on domains in which empirically demonstrable bias actually persists."
WILLIAM KAHN,Survey-evaluation of clarinet methods used in the public schools,"The Problem The purpose and aim of this study has been to make a thorough investigation and analysis, through a national survey, of clarinet methods used in the public schools, and to arrive at some tenable conclusions regarding the efficacy of various types of methods in school situations. Procedures The steps in this study have been: 1. The selection and organization of a set of criteria for evaluating methods 2. The construction of a questionnaire and the use of it as a survey instrument 3. The tabulation of data resulting from the survey 4. The development of an evaluative chart for evaluating methods of study 5. The study and evaluation of nine leading elementary clarinet methods reported in the survey. Results Two hundred and fifty-one questionnaires were sent to various music supervisors and instrumental instructors throughout the country. One hundred and eight questionnaires were returned which was a 43.3 per cent response. The survey showed that 81.5 per cent of the schools offered first year instruction, 67.6 per cent offered second year instruction, and 50 per cent third year instruction, and 45.3 per cent offered private study. The methods or series of methods reported to be most successfully used for all levels of instruction were as follows: Rubank; Easy Steps to Band and Intermediate Steps to Band; Belwin; A Tune A Day; Smith-Yoder-Bachman; Boosey and Hawkes Band; Hetzel's Visual; Beginning Band Musicianship (3-way); First Semester Band; Modern Melody; Klose; Langenus; and Lazarus. [TRUNCATED]."
PIERRE PERRON,Continuous record Laplace-based inference about the break date in structural change models,"Building upon the continuous record asymptotic framework recently introduced by Casini and Perron (2018a) for inference in structural change models, we propose a Laplace-based (Quasi-Bayes) procedure for the construction of the estimate and confidence set for the date of a structural change. It is defined by an integration rather than an optimization-based method.A transformation of the least-squares criterion function is evaluated in order to derive a proper distribution, referred to as the Quasi-posterior. For a given choice of a loss function, the Laplace-type estimator is the minimizer of the expected risk with the expectation taken under the Quasi-posterior. Besides providing an alternative estimate that is more precise—lower mean absolute error (MAE) and lower root-mean squared error (RMSE)—than the usual least-squares one, the Quasi-posterior distribution can be used to construct asymptotically valid inference using the concept of Highest Density Region. The resulting Laplace-based inferential procedure is shown to have lower MAE and RMSE, and the confidence sets strike the best balance between empirical coverage rates and average lengths of the confidence sets relative to traditional long-span methods, whether the break size is small or large."
PIERRE PERRON,Statistically derived contributions of diverse human influences to twentieth-century temperature changes,"The warming of the climate system is unequivocal as evidenced by an increase in global temperatures by 0.8 °C over the past century. However, the attribution of the observed warming to human activities remains less clear, particularly because of the apparent slow-down in warming since the late 1990s. Here we analyse radiative forcing and temperature time series with state-of-the-art statistical methods to address this question without climate model simulations. We show that long-term trends in total radiative forcing and temperatures have largely been determined by atmospheric greenhouse gas concentrations, and modulated by other radiative factors. We identify a pronounced increase in the growth rates of both temperatures and radiative forcing around 1960, which marks the onset of sustained global warming. Our analyses also reveal a contribution of human interventions to two periods when global warming slowed down. Our statistical analysis suggests that the reduction in the emissions of ozone-depleting substances under the Montreal Protocol, as well as a reduction in methane emissions, contributed to the lower rate of warming since the 1990s. Furthermore, we identify a contribution from the two world wars and the Great Depression to the documented cooling in the mid-twentieth century, through lower carbon dioxide emissions. We conclude that reductions in greenhouse gas emissions are effective in slowing the rate of warming in the short term."
PIERRE PERRON,Combining long memory and level shifts in modeling and forecasting the volatility of asset returns,"We propose a parametric state space model of asset return volatility with an accompanying estimation and forecasting framework that allows for ARFIMA dynamics, random level shifts and measurement errors. The Kalman filter is used to construct the state-augmented likelihood function and subsequently to generate forecasts, which are mean- and path-corrected. We apply our model to eight daily volatility series constructed from both high-frequency and daily returns. Full sample parameter estimates reveal that random level shifts are present in all series. Genuine long memory is present in high-frequency measures of volatility whereas there is little remaining dynamics in the volatility measures constructed using daily returns. From extensive forecast evaluations, we find that our ARFIMA model with random level shifts consistently belongs to the 10% Model Confidence Set across a variety of forecast horizons, asset classes, and volatility measures. The gains in forecast accuracy can be very pronounced, especially at longer horizons."
PIERRE PERRON,Testing for common breaks in a multiple equations system,"The issue addressed in this paper is that of testing for common breaks across or within equations. Our framework is very general and allows integrated regressors and trends as well as stationary regressors. The null hypothesis is that some subsets of the parameters (either regression coe cients or elements of the covariance matrix of the errors) share one or more common break dates, with the break dates in the system asymptotically distinct so that each regime is separated by some positive fraction of the sample size. Under the alternative hypothesis, the break dates are not the same and also need not be separated by a positive fraction of the sample size. The test con- sidered is the quasi-likelihood ratio test assuming normal errors, though as usual the limit distribution of the test remains valid with non-normal errors. Also of indepen- dent interest, we provide results about the consistency and rate of convergence when searching over all possible partitions subject only to the requirement that each regime contains at least as many observations as the number of parameters in the model. Sim- ulation results show that the test has good nite sample properties. We also provide an application to various measures of in ation to illustrate its usefulness."
PIERRE PERRON,Inference on locally ordered breaks in multiple regressions,"We consider issues related to inference about locally ordered breaks in a system of equations, as originally proposed by Qu and Perron (2007 Qu, Z., Perron, P. (2007). Estimating and testing structural changes in multivariate regressions. Econometrica 75:459–502.[Crossref], [Web of Science ®], [Google Scholar]). These apply when break dates in different equations within the system are not separated by a positive fraction of the sample size. This allows constructing joint confidence intervals of all such locally ordered break dates. We extend the results of Qu and Perron (2007 Qu, Z., Perron, P. (2007). Estimating and testing structural changes in multivariate regressions. Econometrica 75:459–502.[Crossref], [Web of Science ®], [Google Scholar]) in several directions. First, we allow the covariates to be any mix of trends and stationary or integrated regressors. Second, we allow for breaks in the variance-covariance matrix of the errors. Third, we allow for multiple locally ordered breaks, each occurring in a different equation within a subset of equations in the system. Via some simulation experiments, we show first that the limit distributions derived provide good approximations to the finite sample distributions. Second, we show that forming confidence intervals in such a joint fashion allows more precision (tighter intervals) compared to the standard approach of forming confidence intervals using the method of Bai and Perron (1998 Bai, J., Perron, P. (1998). Estimating and testing linear models with multiple structural changes. Econometrica 66:47–78.[Crossref], [Web of Science ®], [Google Scholar]) applied to a single equation. Simulations also indicate that using the locally ordered break confidence intervals yields better coverage rates than using the framework for globally distinct breaks when the break dates are separated by roughly 10% of the total sample size."
PIERRE PERRON,Fractional unit root tests allowing for a structural change in trend under both the null and alternative hypotheses,"This paper considers testing procedures for the null hypothesis of a unit root process against the alternative of a fractional process, called a fractional unit root test. We extend the Lagrange Multiplier (LM) tests of Robinson (1994) and Tanaka (1999), which are locally best invariant and uniformly most powerful, to allow for a slope change in trend with or without a concurrent level shift under both the null and alternative hypotheses. We show that the limit distribution of the proposed LM tests is standard normal. Finite sample simulation experiments show that the tests have good size and power. As an empirical analysis, we apply the tests to the Consumer Price Indices of the G7 countries."
PIERRE PERRON,Continuous record asymptotics for structural change models,"For a partial structural change in a linear regression model with a single break, we develop a continuous record asymptotic framework to build inference methods for the break date. We have T observations with a sampling frequency h over a fixed time horizon [0 , N ], and let T →∞ with h ↓ 0 while keeping the time span N fixed. We impose very mild regularity conditions on an underlying continuous-time model assumed to generate the data. We consider the least-squares estimate of the break date and establish consistency and convergence rate. We provide a limit theory for shrinking magnitudes of shifts and locally increasing variances. The asymptotic distribution corresponds to the location of the extremum of a function of the quadratic variation of the regressors and of a Gaussian centered martingale process over a certain time interval. We can account for the asymmetric informational content provided by the pre- and post-break regimes and show how the location of the break and shift magnitude are key ingredients in shaping the distribution. We consider a feasible version based on plug-in estimates, which provides a very good approximation to the finite sample distribution. We use the concept of Highest Density Region to construct confidence sets. Overall, our method is reliable and delivers accurate coverage probabilities and relatively short average length of the confidence sets. Importantly, it does so irrespective of the size of the break."
PIERRE PERRON,"Comments on ""In-sample confidence bands and out-of-sample forecast bands for time-varying parameters in observation driven models""",
PIERRE PERRON,Measuring business cycles with structural breaks and outliers: Applications to international data,"This paper first generalizes the trend-cycle decomposition framework of Perron and Wada (2009) based on unobserved components models with innovations having a mixture of normals distribution, which is able to handle sudden level and slope changes to the trend function as well as outliers. We investigate how important are the differences in the implied trend and cycle compared to the popular decomposition based on the Hodrick and Prescott (HP) (1997) filter. Our results show important qualitative and quantitative differences in the implied cycles for both real GDP and consumption series for the G7 countries. Most of the differences can be ascribed to the fact that the HP filter does not handle well slope changes, level shifts and outliers, while our method does so. Then, we reassess how such different cycles affect some so-called “stylized facts” about the relative variability of consumption and output across countries."
PIERRE PERRON,Using OLS to estimate and test for structural changes in models with endogenous regressors,"We consider the problem of estimating and testing for multiple breaks in a single-equation framework with regressors that are endogenous, i.e. correlated with the errors. We show that even in the presence of endogenous regressors it is still preferable, in most cases, to simply estimate the break dates and test for structural change using the usual ordinary least squares (OLS) framework. Except for some knife-edge cases, it delivers estimates of the break dates with higher precision and tests with higher power compared to those obtained using an instrumental variable (IV) method. Also, the OLS method avoids potential weak identification problems caused by weak instruments. To illustrate the relevance of our theoretical results, we consider the stability of the New Keynesian hybrid Phillips curve. IV-based methods only provide weak evidence of instability. On the other hand, OLS-based ones strongly indicate a change in 1991:Q1 and that after this date the model loses all explanatory power."
PIERRE PERRON,A note on estimating and testing for multiple structural changes in models with endogenous regressors via 2SLS,"This note provides a simple proof for the problem of estimating and testing for multiple breaks in a single equation framework with regressors that are endogenous. We show based on standard assumptions about the regressors, instruments, and errors that the second-stage regression of the instrumental variable procedure involves regressors and errors that satisfy all the assumptions in Perron and Qu (2006, Journal of Econometrics 134, 373–399) so that the results about consistency, rate of convergence and limit distributions of the estimates of the break dates, in addition to the limit distributions of the tests, are obtained as simple consequences. The results are obtained within a unified framework for various cases about the nature of the reduced form: stable, no structural changes but time variations in the parameters, structural changes at dates that are common to those of the structural form, and structural changes occurring at arbitrary dates."
PIERRE PERRON,Modelling exchange rate volatility with random level shifts,"Recent literature has shown that the volatility of exchange rate returns displays long memory features. It has also been shown that if a short memory process is contaminated by level shifts, the estimate of the long memory parameter tends to be upward biased. In this article, we directly estimate a random level shift model to the logarithm of the absolute returns of five exchange rates series, in order to assess whether random level shifts (RLSs) can explain this long memory property. Our results show that there are few level shifts for the five series, but once they are taken into account the long memory property of the series disappears. We also provide out-of-sample forecasting comparisons, which show that, in most cases, the RLS model outperforms popular models in forecasting volatility. We further support our results using a variety of robustness checks."
PIERRE PERRON,Improved tests for forecast comparisons in the presence of instabilities,"Of interest is comparing the out-of-sample forecasting performance of two competing models in the presence of possible instabilities. To that effect, we suggest using simple structural change tests, sup-Wald and UDmax for changes in the mean of the loss differences. It is shown that Giacomini and Rossi (2010) tests have undesirable power properties, power that can be low and non-increasing as the alternative becomes further from the null hypothesis. On the contrary, our statistics are shown to have higher monotonic power, especially the UDmax version. We use their empirical examples to show the practical relevance of the issues raised."
PIERRE PERRON,Extracting and analyzing the warming trend in global and hemispheric temperatures,"This article offers an updated and extended attribution analysis based on recently published versions of temperature and forcing datasets. It shows that both temperature and radiative forcing variables can be best represented as trend stationary processes with structural changes occurring in the slope of their trend functions and that they share a common secular trend and common breaks, largely determined by the anthropogenic radiative forcing. The common nonlinear trend is isolated, and further evidence on the possible causes of the current slowdown in warming is presented. Our analysis offers interesting results in relation to the recent literature. Changes in the anthropogenic forcings are directly responsible for the hiatus, while natural variability modes such as the Atlantic Multidecadal Oscillation, as well as new temperature adjustments, contribute to weaken the signal. In other words, natural variability and data adjustments do not explain in any way the hiatus; they simply mask its presence."
PIERRE PERRON,A comparison of alternative methods to construct confidence intervals for the estimate of a break date in linear regression models,"This article considers constructing confidence intervals for the date of a structural break in linear regression models. Using extensive simulations, we compare the performance of various procedures in terms of exact coverage rates and lengths of the confidence intervals. These include the procedures of Bai (1997 Bai, J. (1997). Estimation of a change point in multiple regressions. Review of Economics and Statistics 79:551–563.) based on the asymptotic distribution under a shrinking shift framework, Elliott and Müller (2007 Elliott, G., Müller, U. (2007). Confidence sets for the date of a single break in linear time series regressions. Journal of Econometrics 141:1196–1218.) based on inverting a test locally invariant to the magnitude of break, Eo and Morley (2015 Eo, Y., Morley, J. (2015). Likelihood-ratio-based confidence sets for the timing of structural breaks. Quantitative Economics 6:463–497.) based on inverting a likelihood ratio test, and various bootstrap procedures. On the basis of achieving an exact coverage rate that is closest to the nominal level, Elliott and Müller's (2007 Elliott, G., Müller, U. (2007). Confidence sets for the date of a single break in linear time series regressions. Journal of Econometrics 141:1196–1218.) approach is by far the best one. However, this comes with a very high cost in terms of the length of the confidence intervals. When the errors are serially correlated and dealing with a change in intercept or a change in the coefficient of a stationary regressor with a high signal-to-noise ratio, the length of the confidence interval increases and approaches the whole sample as the magnitude of the change increases. The same problem occurs in models with a lagged dependent variable, a common case in practice. This drawback is not present for the other methods, which have similar properties. Theoretical results are provided to explain the drawbacks of Elliott and Müller's (2007 Elliott, G., Müller, U. (2007). Confidence sets for the date of a single break in linear time series regressions. Journal of Econometrics 141:1196–1218 method."
PIERRE PERRON,Testing for flexible nonlinear trends with an integrated or stationary noise component,"This paper proposes a new test for the presence of a nonlinear deterministic trend approximated by a Fourier expansion in a univariate time series for which there is no prior knowledge as to whether the noise component is stationary or contains an autoregressive unit root. Our approach builds on the work of Perron and Yabu (2009a) and is based on a Feasible Generalized Least Squares procedure that uses a super-efficient estimator of the sum of the autoregressive coefficients α when α = 1. The resulting Wald test statistic asymptotically follows a chi-square distribution in both the I(0) and I(1) cases. To improve the finite sample properties of the test, we use a bias-corrected version of the OLS estimator of α proposed by Roy and Fuller (2001). We show that our procedure is substantially more powerful than currently available alternatives. We illustrate the usefulness of our method via an application to modelling the trend of global and hemispheric temperatures."
PIERRE PERRON,Residuals-based tests for cointegration with generalized least-squares detrended data,"We provide generalized least-squares (GLS) detrended versions of single-equation static regression or residuals-based tests for testing whether or not non-stationary time series are cointegrated. Our approach is to consider nearly optimal tests for unit roots and to apply them in the cointegration context. We derive the local asymptotic power functions of all tests considered for a triangular data-generating process, imposing a directional restriction such that the regressors are pure integrated processes. Our GLS versions of the tests do indeed provide substantial power improvements over their ordinary least-squares counterparts. Simulations show that the gains in power are important and stable across various configurations."
PIERRE PERRON,Inference on a structural break in trend with fractionally integrated errors,"Perron and Zhu (2005) established the consistency, convergence rate and limiting distributions of parameter estimates in time trends with a change in slope with or without a concurrent level change for the cases with I(1) or I(0) errors. We extend their analysis to the general case of fractionally integrated errors with memory parameter d∗. Our results uncover interesting features; e.g., with a level shift allowed, the convergence rate for the break date estimate is the same for all d∗∈(−0.5,0.5). In other cases, it is decreasing as d∗ increases. We also provide results about the so-called spurious break issue."
PIERRE PERRON,Combining long memory and level shifts in modeling and forecasting the volatility of asset returns,"We propose a parametric state space model of asset return volatility with an accompanying estimation and forecasting framework that allows for ARFIMA dynamics, random level shifts and measurement errors. The Kalman filter is used to construct the state-augmented likelihood function and subsequently to generate forecasts, which are mean and path-corrected. We apply our model to eight daily volatility series constructed from both high-frequency and daily returns. Full sample parameter estimates reveal that random level shifts are present in all series. Genuine long memory is present in most high-frequency measures of volatility, whereas there is little remaining dynamics in the volatility measures constructed using daily returns. From extensive forecast evaluations, we find that our ARFIMA model with random level shifts consistently belongs to the 10% Model Confidence Set across a variety of forecast horizons, asset classes and volatility measures. The gains in forecast accuracy can be very pronounced, especially at longer horizons."
PIERRE PERRON,Characterizing and attributing the warming trend in sea and land surface temperatures,"Because of low-frequency internal variability, the observed and underlying warming trends in temperature series can be markedly different. Important differences in the observed nonlinear trends in hemispheric temperature series suggest that the northern and southern hemispheres have responded differently to the changes in the radiative forcing. Using recent econometric techniques, we can reconcile such differences and show that all sea and land temperatures share similar time series properties and a common underlying warming trend having a dominant anthropogenic origin. We also investigate the interhemispheric temperature asymmetry (ITA) and show that the differences in warming between hemispheres are in part driven by anthropogenic forcing but that most of the observed rapid changes is likely due to natural variability. The attribution of changes in ITA is relevant since increases in the temperature contrast between hemispheres could potentially produce a shift in the Intertropical Convergence Zone and alter rainfall patterns. The existence of a current slowdown in the warming and its causes are also investigated. The results suggest that the slowdown is a common feature in global and hemispheric sea and land temperatures that can, at least partly, be attributed to changes in anthropogenic forcing.
 Debido a la variabilidad interna de baja frecuencia, las tendencias del calentamiento observadas y subyacentes en series de temperatura pueden ser marcadamente diferentes. Las temperaturas hemisféricas están caracterizadas por importantes discrepancias en las tendencias no lineales observadas, sugiriendo que los hemisferios norte y sur han respondido de manera diferente a los cambios en el forzamiento radiativo. Mediante la utilización de técnicas econométricas recientes es posible reconciliar estas diferencias y mostrar que todas las temperaturas terrestres y oceánicas comparten propiedades de series de tiempo similares, así como una tendencia subyacente común de origen antrópico. También se investiga la asimetría inter-hemisférica de temperatura (ITA, por sus siglas en inglés) y se muestra que la diferencia en el calentamiento entre hemisferios se debe en parte al forzamiento antrópico, pero que la mayoría de los cambios rápidos observados son probablemente producto de la variabilidad natural. La atribución de cambios en la ITA es importante porque los aumentos en el contraste de temperaturas entre hemisferios podrían ocasionar un desplazamiento de la zona intertropical de convergencia y alterar los patrones de precipitación. También se investigan la existencia y causas de una reciente ralentización en el calentamiento. Los resultados sugieren que dicha lentificación es una característica común de las temperaturas hemisféricas globales tanto en tierra como en el océano, y que puede atribuirse al menos parcialmente a cambios en el forzamiento antrópico.
 "
PIERRE PERRON,Time series methods applied to climate change,
PIERRE PERRON,A comparison of alternative methods to construct confidence intervals for the estimate of a break date in linear regression models,"This article considers constructing confidence intervals for the date of a structural break in linear regression models. Using extensive simulations, we compare the performance of various procedures in terms of exact coverage rates and lengths of the confidence intervals. These include the procedures of Bai (1997 Bai, J. (1997). Estimation of a change point in multiple regressions. Review of Economics and Statistics 79:551–563.) based on the asymptotic distribution under a shrinking shift framework, Elliott and Müller (2007 Elliott, G., Müller, U. (2007). Confidence sets for the date of a single break in linear time series regressions. Journal of Econometrics 141:1196–1218.) based on inverting a test locally invariant to the magnitude of break, Eo and Morley (2015 Eo, Y., Morley, J. (2015). Likelihood-ratio-based confidence sets for the timing of structural breaks. Quantitative Economics 6:463–497.[Crossref], [Web of Science ®], [Google Scholar]) based on inverting a likelihood ratio test, and various bootstrap procedures. On the basis of achieving an exact coverage rate that is closest to the nominal level, Elliott and Müller's (2007 Elliott, G., Müller, U. (2007). Confidence sets for the date of a single break in linear time series regressions. Journal of Econometrics 141:1196–1218.) approach is by far the best one. However, this comes with a very high cost in terms of the length of the confidence intervals. When the errors are serially correlated and dealing with a change in intercept or a change in the coefficient of a stationary regressor with a high signal-to-noise ratio, the length of the confidence interval increases and approaches the whole sample as the magnitude of the change increases. The same problem occurs in models with a lagged dependent variable, a common case in practice. This drawback is not present for the other methods, which have similar properties. Theoretical results are provided to explain the drawbacks of Elliott and Müller's (2007 Elliott, G., Müller, U. (2007). Confidence sets for the date of a single break in linear time series regressions. Journal of Econometrics 141:1196–1218.) method."
PIERRE PERRON,Testing for changes in forecasting performance,"We consider the issue of forecast failure (or breakdown) and propose methods to assess retrospectively whether a given forecasting model provides forecasts which show evidence of changes with respect to some loss function. We adapt the classical structural change tests to the forecast failure context. First, we recommend that all tests should be carried with a fixed scheme to have best power. This ensures a maximum difference between the fitted in and out-of-sample means of the losses and avoids contamination issues under the rolling and recursive schemes. With a fixed scheme, Giacomini and Rossi’s (2009) (GR) test is simply a Wald test for a one-time change in the mean of the total (the in-sample plus out-of-sample) losses at a known break date, say m, the value that separates the in and out-of-sample periods. To alleviate this problem, we consider a variety of tests: maximizing the GR test over values of m within a pre-specified range; a Double sup-Wald (DSW) test which for each m performs a sup-Wald test for a change in the mean of the out-of-sample losses and takes the maximum of such tests over some range; we also propose to work directly with the total loss series to define the Total Loss sup-Wald (TLSW) and Total Loss UDmax (TLUD) tests. Using theoretical analyses and simulations, we show that with forecasting models potentially involving lagged dependent variables, the only tests having a monotonic power function for all data-generating processes considered are the DSW and TLUD tests, constructed with a fixed forecasting window scheme. Some explanations are provided and empirical applications illustrate the relevance of our findings in practice."
PIERRE PERRON,Pitfalls of two-step testing for changes in the error variance and coefficients of a linear regression model,"In empirical applications based on linear regression models, structural changes often occur in both the error variance and regression coefficients, possibly at different dates. A commonly applied method is to first test for changes in the coefficients (or in the error variance) and, conditional on the break dates found, test for changes in the variance (or in the coefficients). In this note, we provide evidence that such procedures have poor finite sample properties when the changes in the first step are not correctly accounted for. In doing so, we show that testing for changes in the coefficients (or in the variance) ignoring changes in the variance (or in the coefficients) induces size distortions and loss of power. Our results illustrate a need for a joint approach to test for structural changes in both the coefficients and the variance of the errors. We provide some evidence that the procedures suggested by Perron et al. (2019) provide tests with good size and power."
PIERRE PERRON,Inference related to common breaks in a multivariate system with joined segmented trends with applications to global and hemispheric temperatures,"What transpires from recent research is that temperatures and radiative forcing seem to be characterized by a linear trend with two changes in the rate of growth. The first occurs in the early 60s and indicates a very large increase in the rate of growth of both temperature and radiative forcing series. This was termed as the “onset of sustained global warming”. The second is related to the more recent so-called hiatus period, which suggests that temperatures and total radiative forcing have increased less rapidly since the mid-90s compared to the larger rate of increase from 1960 to 1990. There are two issues that remain unresolved. The first is whether the breaks in the slope of the trend functions of temperatures and radiative forcing are common. This is important because common breaks coupled with the basic science of climate change would strongly suggest a causal effect from anthropogenic factors to temperatures. The second issue relates to establishing formally via a proper testing procedure that takes into account the noise in the series, whether there was indeed a ‘hiatus period’ for temperatures since the mid 90s. This is important because such a test would counter the widely held view that the hiatus is the product of natural internal variability. Our paper provides tests related to both issues. The results show that the breaks in temperatures and radiative forcing are common and that the hiatus is characterized by a significant decrease in their rate of growth. The statistical results are of independent interest and applicable more generally."
PIERRE PERRON,"Breaks, trends and the attribution of climate change: a time-series analysis","Climate change detection and attribution have been the subject of intense research and debate over at least four decades. However, direct attribution of climate change to anthropogenic activities using observed climate and forcing variables through statistical methods has remained elusive, partly caused by difficulties to correctly identify the time-series properties of these variables and by the limited availability of methods to relate nonstationary variables. This paper provides strong evidence concerning the direct attribution of observed climate change to anthropogenic greenhouse gases emissions by first investigating the univariate time-series properties of observed global and hemispheric temperatures and forcing variables and then by proposing statistically adequate multivariate models. The results show that there is a clear anthropogenic fingerprint on both global and hemispheric temperatures. The signal of the well-mixed Greenhouse Gases (GHG) forcing in all temperature series is very clear and accounts for most of their secular movements since the beginning of observations. Both temperature and forcing variables are characterized by piecewise linear trends with abrupt changes in their slopes estimated to occur at different dates. Nevertheless, their long-term movements are so closely related that the observed temperature and forcing trends cancel out. The warming experimented during the last century was mainly due to the increase in GHG which was partially offset by the effect of tropospheric aerosols. Other forcing sources, such as solar, are shown to only contribute to (shorter-term) variations around the GHG forcing trend."
PIERRE PERRON,Simultaneous bandwidths determiniation for DK-HAC estimators and long-run variance estimation in nonparametric settings,
PIERRE PERRON,Causality from long-lived radiative forcings to the climate trend,"In our study, we present a purely statistical observations‐based model‐free analysis that provides evidence about Granger causality (GC) from long‐lived radiative forcings (LLRFs) to the climate trend (CT). This relies on having locally ordered breaks in the slopes of the trend functions of LLRF and the CT, with the break for LLRF occurring before that of the CT and with the slope changes being of the same sign. The empirical evidence indicates that these conditions are satisfied empirically using standard global surface temperature series and an aggregate measure of LLRF (carbon dioxide, nitrous oxide, and chlorofluorocarbons). We also discuss why the presence of broken trends can lead one to conclude in favor of GC when using standard methods even if the noise function in LLRF is negligible."
PIERRE PERRON,Statistical evidence about human influence on the climate system,"We use recent methods for the analysis of time series data, in particular related to breaks in trends, to establish that human factors are the main contributors to the secular movements in observed global and hemispheric temperatures series. The most important feature documented is a marked increase in the growth rates of temperatures (purged from the Atlantic Multidecadal Oscillation) and anthropogenic greenhouse gases occurring for all series around 1955, which marks the start of sustained global warming. Also evidence shows that human interventions effectively slowed global warming in two occasions. The Montreal Protocol and the technological change in agricultural production in Asia are major drivers behind the slowdown of the warming since 1994, providing evidence about the effectiveness of reducing emissions of greenhouse gases other than CO2 for mitigating climate change in the shorter term. The largest socioeconomic disruptions, the two World Wars and the Great Crash, are shown to have contributed to the cooling in the mid 20th century. While other radiative factors have modulated their effect, the greenhouse gases defined the secular movement in both the total radiative forcing and the global and hemispheric temperature series. Deviations from this anthropogenic trend are shown to have transitory effects."
PIERRE PERRON,Comments on “In-sample confidence bands and out-of-sample forecast bands for time-varying parameters in observation driven models”,
PIERRE PERRON,Single-equation tests for cointegration with GLS detrended data,"We provide GLS-based versions of two widely used approaches for testing whether or not non-stationary economic time series are cointegrated: single-equation static re- gression or residual-based tests and single-equation conditional error correction model (ECM) based tests. Our approach is to consider nearly optimal tests for unit roots and apply them in the cointegration context. Our GLS versions of the tests do in- deed provide substantial improvements over their OLS counterparts. We derive the local asymptotic power functions of all tests considered for a DGP with weakly ex- ogenous regressors. This allows obtaining the relevant non-centrality parameter to quasi-di§erence the data. We investigate the e§ect of non-weakly exogenous regressors via simulations. With weakly exogenous regressors strongly correlated with the depen- dent variable, the ECM tests are clearly superior. When the regressors are potentially non-weakly exogenous, the residuals-based tests are clearly preferred."
PIERRE PERRON,Estimating and testing multiple structural changes in models with endogenous regressors,"We consider the problem of estimating and testing for multiple breaks in a single equation framework with regressors that are endogenous, i.e., correlated with the errors. First, we show based on standard assumptions about the regressors, instruments and errors that the second stage regression of the instrumental variable (IV) procedure involves regressors and errors that satisfy all the assumptions in Perron and Qu (2006) so that the results about consistency, rate of convergence and limit distributions of the estimates of the break dates, as well as the limit distributions of the tests, are obtained as simple consequences. More importantly from a practical perspective, we show that even in the presence of endogenous regressors, it is still preferable to simply estimate the break dates and test for structural change using the usual ordinary least-squares (OLS) framework. It delivers estimates of the break dates with higher precision and tests with higher power compared to those obtained using an IV method. To illustrate the relevance of our theoretical results, we consider the stability of the New Keynesian hybrid Phillips curve. IV-based methods do not indicate any instability. On the other hand, OLS-based ones strongly indicate a change in 1991:1 and that after this date the model looses all explanatory power."
PIERRE PERRON,Inference on conditional quantile processes in partially linear models with applications to the impact of unemployment benefits,"We propose methods to estimate and conduct inference on conditional quantile processes for models with nonparametric and linear components. The estimation procedure uses local linear or quadratic regressions, with the bandwidth allowed to vary across quantiles to adapt to data sparsity. We establish a Bahadur representation that holds uniformly in the covariate value and the quantile index. Then,we show that the proposed estimator converges weakly to a Gaussian process and develop methods for constructing uniform confidence bands and hypothesis testing. Our results also cover locally partially linear models with boundary points, thereby allowing for Sharp Regression Discontinuity Designs (SRD). This allows us to study the effects of unemployment insurance (UI) benefits extensions using the dataset of Nekoei and Weber (2017) who found a statistically significant effect, though of minor economic importance using an SRD focusing on the average effect. Our model allows heterogeneity with respect to both the covariate and the quantile. We find economically strong significant effects in the tail of the distribution,say the 10% quantile of the outcome variable (e.g., the wage change distribution). Under a rank invariance assumption, this implies that individuals who benefited the most are those who would have experienced substantial wage cuts if there were no benefit extension. Since our setup allows for discrete covariates, we also find positive and statistically significant effects for white-collar and female workers and those with a college education, but not for blue-collar male workers without higher education. Hence, while UI benefits reduce the within-group inequality for some subgroups by covariates, they can be viewed as regressive and enhancing between-group inequality, although they also help to bridge the gender gap."
PIERRE PERRON,Unit roots and structural breaks,
PIERRE PERRON,Testing for common breaks in a multiple equations system,"The issue addressed in this paper is that of testing for common breaks across or within equations. Our framework is very general and allows integrated regressors and trends as well as stationary regressors. The null hypothesis is that some subsets of the parameters (either regression coe cients or elements of the covariance matrix of the errors) share one or more common break dates, with the break dates in the system asymptotically distinct so that each regime is separated by some positive fraction of the sample size. Under the alternative hypothesis, the break dates are not the same and also need not be separated by a positive fraction of the sample size. The test considered is the quasi-likelihood ratio test assuming normal errors, though as usual the limit distribution of the test remains valid with non-normal errors. Also of independent interest, we provide results about the consistency and rate of convergence when searching over all possible partitions subject only to the requirement that each regime contains at least as many observations as the number of parameters in the model. Simulation results show that the test has good nite sample properties. We also provide an application to various measures of in ation to illustrate its usefulness."
PIERRE PERRON,Structural change tests under heteroskedasticity: Joint estimation versus two‐steps methods,"There has been a recent upsurge of interest in testing for structural changes in heteroskedastic time series, as changes in the variance invalidate the asymptotic distribution of conventional structural change tests. Several tests have been proposed that are robust to general form of heteroskedastic errors. The most popular use a two-steps approach: first estimate the residuals assuming no changes in the regression coefficients; second, use the residuals to approximate the heteroskedastic asymptotic distribution or take an entire sample average to construct a test for which the variance process is averaged out. An alternative approach was proposed by Perron et al. (Perron et al. (2020). Quantitative Economics 11: 1019–1057) who provided a test for changes in the coefficients allowing for changes in the variance of the error term. We show that it transforms the variance profile into one that effectively has very little impact on the size of the test. With respect to the power properties, the two-steps procedures can suffer from non-monotonic power problems in dynamic models and in static models with a correction for serial correlation in the error. Most have power equals to size with zero-mean regressors. Even when the two-steps tests have power, it is generally lower than that of the latter test."
PIERRE PERRON,Testing for common breaks in a multiple equations system,"The issue addressed in this paper is that of testing for common breaks across or within equations of a multivariate system. Our framework is very general and allows integrated regressors and trends as well as stationary regressors. The null hypothesis is that breaks in different parameters occur at common locations and are separated by some positive fraction of the sample size unless they occur across different equations. Under the alternative hypothesis, the break dates across parameters are not the same and also need not be separated by a positive fraction of the sample size whether within or across equations. The test considered is the quasi-likelihood ratio test assuming normal errors, though as usual the limit distribution of the test remains valid with non-normal errors. Of independent interest, we provide results about the rate of convergence of the estimates when searching over all possible partitions subject only to the requirement that each regime contains at least as many observations as some positive fraction of the sample size, allowing break dates not separated by a positive fraction of the sample size across equations. Simulations show that the test has good finite sample properties. We also provide an application to issues related to level shifts and persistence for various measures of inflation to illustrate its usefulness."
PIERRE PERRON,A time-series analysis of the 20th century climate simulations produced for the IPCC's Fourth Assessment Report.,"In this paper evidence of anthropogenic influence over the warming of the 20th century is presented and the debate regarding the time-series properties of global temperatures is addressed in depth. The 20th century global temperature simulations produced for the Intergovernmental Panel on Climate Change's Fourth Assessment Report and a set of the radiative forcing series used to drive them are analyzed using modern econometric techniques. Results show that both temperatures and radiative forcing series share similar time-series properties and a common nonlinear secular movement. This long-term co-movement is characterized by the existence of time-ordered breaks in the slope of their trend functions. The evidence presented in this paper suggests that while natural forcing factors may help explain the warming of the first part of the century, anthropogenic forcing has been its main driver since the 1970's. In terms of Article 2 of the United Nations Framework Convention on Climate Change, significant anthropogenic interference with the climate system has already occurred and the current climate models are capable of accurately simulating the response of the climate system, even if it consists in a rapid or abrupt change, to changes in external forcing factors. This paper presents a new methodological approach for conducting time-series based attribution studies."
PIERRE PERRON,Continuous record asymptotics for structural change models,"For a partial structural change in a linear regression model with a single break, we develop a continuous record asymptotic framework to build inference methods for the break date. We have T observations with a sampling frequency h over a fixed time horizon [0, N] , and let T with h 0 while keeping the time span N fixed. We impose very mild regularity conditions on an underlying continuous-time model assumed to generate the data. We consider the least-squares estimate of the break date and establish consistency and convergence rate. We provide a limit theory for shrinking magnitudes of shifts and locally increasing variances. The asymptotic distribution corresponds to the location of the extremum of a function of the quadratic variation of the regressors and of a Gaussian centered martingale process over a certain time interval. We can account for the asymmetric informational content provided by the pre- and post-break regimes and show how the location of the break and shift magnitude are key ingredients in shaping the distribution. We consider a feasible version based on plug-in estimates, which provides a very good approximation to the finite sample distribution. We use the concept of Highest Density Region to construct confidence sets. Overall, our method is reliable and delivers accurate coverage probabilities and relatively short average length of the confidence sets. Importantly, it does so irrespective of the size of the break."
PIERRE PERRON,Forecasting in the presence of in and out of sample breaks,"We present a frequentist-based approach to forecast time series in the presence of in-sample and out-of-sample breaks in the parameters of the forecasting model. We first model the parameters as following a random level shift process, with the occurrence of a shift governed by a Bernoulli process. In order to have a structure so that changes in the parameters be forecastable, we introduce two modifications. The first models the probability of shifts according to some covariates that can be forecasted. The second incorporates a built-in mean reversion mechanism to the time path of the parameters. Similar modifications can also be made to model changes in the variance of the error process. Our full model can be cast into a conditional linear and Gaussian state space framework. To estimate it, we use the mixture Kalman filter and a Monte Carlo expectation maximization algorithm. Simulation results show that our proposed forecasting model provides improved forecasts over standard forecasting models that are robust to model misspecifications. We provide two empirical applications and compare the forecasting performance of our approach with a variety of alternative methods. These show that substantial gains in forecasting accuracy are obtained."
PIERRE PERRON,Structural breaks in time series,"This article covers methodological issues related to estimation, testing, and computation for models involving structural changes. Our aim is to review developments as they relate to econometric applications based on linear models. Substantial advances have been made to cover models at a level of generality that allow a host of interesting practical applications. These include models with general stationary regressors and errors that can exhibit temporal dependence and heteroskedasticity, models with trending variables and possible unit roots and cointegrated models, among others. Advances have been made pertaining to computational aspects of constructing estimates, their limit distributions, tests for structural changes, and methods to determine the number of changes present. A variety of topics are covered including recent developments: testing for common breaks, models with endogenous regressors (emphasizing that simply using least-squares is preferable over instrumental variables methods), quantile regressions, methods based on Lasso, panel data models, testing for changes in forecast accuracy, factors models, and methods of inference based on a continuous records asymptotic framework. Our focus is on the so-called off-line methods whereby one wants to retrospectively test for breaks in a given sample of data and form confidence intervals about the break dates. The aim is to provide the readers with an overview of methods that are of direct use in practice as opposed to issues mostly of theoretical interest."
PIERRE PERRON,The great moderation: updated evidence with joint tests for multiple structural changes in variance and persistence,"We assess the empirical evidence about the Great Moderation using a comprehensive framework to test for multiple structural changes in the coefficients and in the variance of the error term of a linear regression model provided by Perron, Yamamoto, and Zhou (2019). We apply it to U.S. real GDP and its major components for the period 1960:1 to 2018:4. A notable feature of our approach is that we adopt an unobserved component model, allowing for two breaks in the trend function in 1973:1 and 2008:1, in order to obtain a stationary or cyclical component modelled as an autoregressive process. First, we confirm evidence about the Great Moderation, i.e., a structural change in variance of the errors in the mid-80s for the various series. Second, additional breaks in variance are found in 1970:3 for GDP and production (goods), after which the sample standard deviation increased by three times. Hence, a part of the Great Moderation can be viewed as a reversion to the pre-70s level of volatility. Third, the evidence about systematic changes in the sum of the autoregressive coefficients (a measure of persistence) is weak over the whole sample period. Finally, we find little evidence of structural changes occurring in both the variance and the coefficients following the Great Recession (2007-2008). These results support views emphasizing the good luck hypothesis as a source of the Great Moderation, which continues even after the Great Recession."
PIERRE PERRON,Temporal aggregation and long memory for asset price volatility,"The effects of temporal aggregation and choice of sampling frequency are of great interest in modeling the dynamics of asset price volatility. We show how the squared low-frequency returns can be expressed in terms of the temporal aggregation of a high-frequency series. Based on the theory of temporal aggregation, we provide the link between the spectral density function of the squared low-frequency returns and that of the squared high-frequency returns. Furthermore, we analyze the properties of the spectral density function of realized volatility series, constructed from squared returns with different frequencies under temporal aggregation. Our theoretical results allow us to explain some findings reported recently and uncover new features of volatility in financial market indices. The theoretical findings are illustrated via the analysis of both low-frequency daily Standard and Poor’s 500 (S&P 500) returns from 1928 to 2011 and high-frequency 1-min S&P 500 returns from 1986 to 2007."
PIERRE PERRON,Spatial variations in the warming trend and the transition to more severe weather in midlatitudes,"Due to various feedback processes called Arctic amplification, the high-latitudes' response to increases in radiative forcing is much larger than elsewhere in the world, with a warming more than twice the global average. Since the 1990's, this rapid warming of the Arctic was accompanied by no-warming or cooling over midlatitudes in the Northern Hemisphere in winter (the hiatus). The decrease in the thermal contrast between Arctic and midlatitudes has been connected to extreme weather events in midlatitudes via, e.g., shifts in the jet stream towards the equator and increases in the probability of high-latitude atmospheric blocking. Here we present an observational attribution study showing the spatial structure of the response to changes in radiative forcing. The results also connect the hiatus with diminished contrast between temperatures over regions in the Arctic and midlatitudes. Recent changes in these regional warming trends are linked to international actions such as the Montreal Protocol, and illustrate how changes in radiative forcing can trigger unexpected responses from the climate system. The lesson for climate policy is that human intervention with the climate is already large enough that even if stabilization was attained, impacts from an adjusting climate are to be expected."
PIERRE PERRON,Anthropogenic influence in observed regional warming trends and the implied social time of emergence,"The attribution of climate change allows for the evaluation of the contribution of human drivers to observed warming. At the global and hemispheric scales, many physical and observation-based methods have shown a dominant anthropogenic signal, in contrast, regional attribution of climate change relies on physically based numerical climate models. Here we show, using state-of-the-art statistical tests, the existence of a common nonlinear trend in observed regional air surface temperatures largely imparted by anthropogenic forcing. All regions, continents and countries considered have experienced warming during the past century due to increasing anthropogenic radiative forcing. The results show that we now experience mean temperatures that would have been considered extreme values during the mid-20th century. The adaptation window has been getting shorter and is projected to markedly decrease in the next few decades. Our findings provide independent empirical evidence about the anthropogenic influence on the observed warming trend in different regions of the world."
PIERRE PERRON,Disentangling the trend in the warming of urban areas into global and local factors,"Large cities account for a significant share of national population and wealth, and exert high pressure on local and regional resources, exacerbating socioenvironmental risks. The replacement of natural landscapes with higher heat capacity materials because of urbanization and anthropogenic waste heat are some of the factors contributing to local climate change caused by the urban heat island (UHI) effect. Because of synergistic effects, local climate change can exacerbate the impacts of global warming in cities. Disentangling the contributions to warming in cities from global and local drivers can help to understand their relative importance and guide local adaptation policies. The canopy UHI intensity is commonly approximated by the difference between temperatures within cities and the surrounding areas. We present a complementary approach that applies the concept of common trends to extract the global contributions to observed warming in cities and to obtain a residual warming trend caused by local and regional factors. Once the effects of global drivers are removed, common features appear in cities' temperatures in the eastern part of the United States. Most cities experienced higher warming than that attributable to global climate change, and some shared a period of rapid warming during urban sprawl in the mid-20th century in the United States."
PIERRE PERRON,Bootstrap procedures for detecting multiple persistence shifts in heteroskedastic time series,"This article proposes new bootstrap procedures for detecting multiple persistence shifts in a time series driven by non‐stationary volatility. The assumed volatility process can accommodate discrete breaks, smooth transition variation as well as trending volatility. We develop wild bootstrap sup‐Wald tests of the null hypothesis that the process is either stationary [I(0)] or has a unit root [I(1)] throughout the sample. We also propose a sequential procedure to estimate the number of persistence breaks based on ordering the regime‐specific bootstrap p‐values. The asymptotic validity of the advocated procedures is established both under the null of stability and a variety of persistence change alternatives. A comparison with existing tests that assume homoskedasticity illustrates the finite sample improvements offered by our methods. An application to OECD inflation rates highlights the empirical relevance of the proposed approach and weakens the case for persistence change relative to existing procedures."
PIERRE PERRON,Testing jointly for structural changes in the error variance and coefficients of a linear regression model,"We provide a comprehensive treatment for the problem of testing jointly for structural changes in both the regression coefficients and the variance of the errors in a single equation system involving stationary regressors. Our framework is quite general in that we allow for general mixing-type regressors and the assumptions on the errors are quite mild. Their distribution can be non-normal and conditional heteroskedasticity is permitted. Extensions to the case with serially correlated errors are also treated. We provide the required tools to address the following testing problems, among others: a) testing for given numbers of changes in regression coefficients and variance of the errors; b) testing for some unknown number of changes within some pre-specified maximum; c) testing for changes in variance (regression coefficients) allowing for a given number of changes in the regression coefficients (variance); d) a sequential procedure to estimate the number of changes present. These testing problems are important for practical applications as witnessed by interests in macroeconomics and finance where documenting structural changes in the variability of shocks to simple autoregressions or Vector Autoregressive Models has been a concern."
PIERRE PERRON,A two step procedure for testing partial parameter stability in cointegrated regression models,"Kejriwal and Perron (2010, KP) provided a comprehensive treatment for the problem of testing multiple structural changes in cointegrated regression models. A variety of models were considered depending on whether all regression coefficients are allowed to change (pure structural change) or a subset of the coefficients is held fixed (partial structural change). In this note, we first show that the limit distributions of the test statistics in the latter case are not invariant to changes in the coefficients not being tested; in fact, they diverge as the sample size increases. To address this issue, we propose a simple two step procedure to test for partial parameter stability. The first entails the application of a joint test of stability for all coefficients as in KP. Upon a rejection, the second conducts a stability test on the subset of coefficients of interest while allowing the other coefficients to change at the estimated breakpoints. Its limit distribution is standard chi-square. The relevant asymptotic theory is provided along with simulations that illustrates the usefulness of the procedure in finite samples."
PIERRE PERRON,Generalized Laplace inference in multiple change-points models,"Under the classical long-span asymptotic framework we develop a class of Generalized Laplace (GL) inference methods for the change-point dates in a linear time series regression model with multiple structural changes analyzed in, e.g., Bai and Perron (1998). The GL estimator is defined by an integration rather than optimization-based method and relies on the least-squares criterion function. It is interpreted as a classical (non-Bayesian) estimator and the inference methods proposed retain a frequentist interpretation. This approach provides a better approximation about the uncertainty in the data of the change-points relative to existing methods. On the theoretical side, depending on some input (smoothing) parameter, the class of GL estimators exhibits a dual limiting distribution; namely, the classical shrinkage asymptotic distribution, or a Bayes-type asymptotic distribution. We propose an inference method based on Highest Density Regions using the latter distribution. We show that it has attractive theoretical properties not shared by the other popular alternatives, i.e., it is bet-proof. Simulations confirm that these theoretical properties translate to better finite-sample performance."
PIERRE PERRON,Anthropogenic influence on extremes and risk hotspots,"Study of the frequency and magnitude of climate extremes as the world warms is of utmost importance, especially separating the influence of natural and anthropogenic forcing factors. Record-breaking temperature and precipitation events have been studied using event-attribution techniques. Here, we provide spatial and temporal observation-based analyses of the role of natural and anthropogenic factors, using state-of-the-art time series methods. We show that the risk from extreme temperature and rainfall events has severely increased for most regions worldwide. In some areas the probabilities of occurrence of extreme temperatures and precipitation have increased at least fivefold and twofold, respectively. Anthropogenic forcing has been the main driver of such increases and its effects amplify those of natural forcing. We also identify risk hotspots defined as regions for which increased risk of extreme events and high exposure in terms of either high Gross Domestic Product (GDP) or large population are both present. For the year 2018, increased anthropogenic forcings are mostly responsible for increased risk to extreme temperature/precipitation affecting 94%/72% of global population and 97%/76% of global GDP relative to the baseline period 1961-1990."
PIERRE PERRON,Theory of low frequency contamination from nonstationarity and misspecification: consequences for HAR inference,
PIERRE PERRON,Change-point analysis of time series with evolutionary spectra,
PIERRE PERRON,Estimation in the presence of heteroskedasticty of unknown form: A Lasso-based approach,"We study the Feasible Generalized Least-Squares (FGLS) estimation of the parameters of a linear regression model in the presence of heteroskedasticity of unknown form in the errors. We suggest a Lasso based procedure to estimate the skedastic function of the residuals. The advantage of using Lasso is that it can handle a large number of potential covariates, yet still yields a parsimonious specification. Using extensive simulation experiments, we show that our suggested procedure always provide some improvements in the precision of the parameter of interest (lower Mean- Squared Errors) when heteroskedasticity is present and is equivalent to OLS when there is none. It also performs better than previously suggested procedures. Since the fitted value of the skedastic function falls short of the true specification, we form confidence intervals using a bias-corrected version of the usual heteroskedasticity-robust covariance matrix estimator. These have the correct size and substantially shorter length than when using OLS. Our method is applicable to both cross-section (with a random sample) and time series models, though here we concentrate on the former."
PIERRE PERRON,Prewhitened long-run variance estimation robust to nonstationarity,"We introduce a nonparametric nonlinear VAR prewhitened long-run variance (LRV) estimator for the construction of standard errors robust to autocorrelation and heteroskedasticity that can be used for hypothesis testing in a variety of contexts including the linear regression model. Existing methods either are theoretically valid only under stationarity and have poor finite-sample properties under nonstationarity (i.e., fixed-b methods), or are theoretically valid under the null hypothesis but lead to tests that are not consistent under nonstationary alternative hypothesis (i.e., both fixed-b and traditional HAC estimators). The proposed estimator accounts explicitly for nonstationarity, unlike previous prewhitened procedures which are known to be unreliable, and leads to tests with accurate null rejection rates and good monotonic power. We also establish MSE bounds for LRV estimation that are sharper than previously established and use them to determine the data-dependent bandwidths."
PIERRE PERRON,Robust testing of time trend and mean with unknown integration order errors,"We provide tests to perform inference on the coe cient of a linear trend assuming the noise to be a fractionally integrated process with memory parameter 𝑑 ∈ (−0.5; 1.5) excluding the boundary case 0.5 by applying a quasi-generalized least squares procedure using 𝑑-differences of the data. Doing so, the asymptotic distribution of the ordinary least squares estimators applied to quasi-differenced data and their t-statistics are unaffected by the value of d and have a normal limiting distribution. To have feasible tests, we use the exact local whittle estimator, valid for processes with a linear trend. The small sample properties of the tests are investigated via simulations and we provide comparisons with existing tests valid for a short-memory stationary, 𝐼 (0), or an autoregressive unit root, 𝐼 (1), noise. The results are encouraging in that our test is valid under more general conditions, yet has power approaching to the Perron and Yabu [Estimating deterministic trends with an integrated or stationary noise component. J. of Econometrics. 2009;151;56-69] tests that apply to the dichotomous cases with d either being 0 or 1. We also use our method of proof to show that the main result of Iacone, Leybourne and Taylor [Testing for a break in trend when the order of integration is unknown. J. of Econometrics. 2013;176:30-45] dealing with a test for a break in the slope of a trend function with a fractionally integrated noise is valid for 𝑑 ∈ (—0.5) ∪ (0:5; 1:5)."
PIERRE PERRON,On the persistence of near-surface temperature dynamics in a warming world,"We consider issues related to the effect of climate change on the persistence of (trend-corrected) temperatures using global gridded data for both land and oceans. We first discuss how the presence of trends and additive noise affects inference about persistence. Ignoring a trend induces an upward bias, while not accounting for noise induces a downward bias. We show that the increase in persistence in the commonly used Warm Spell Duration Index is simply an artifact of increasing temperatures. To purge the impact of both trends and noise, we adopt a simple state-space model. Of separate interest, we document a much larger noise component for land than for oceans. The estimates of the persistence are much larger for oceans than for land. Inspection of the estimates across various subsamples and the application of tests for structural changes suggest the same pattern of persistence for both land and oceans across time, with few minor exceptions. Hence, our results show that surface temperature persistence has remained constant during the observed period."
PIERRE PERRON,L’estimation de modèles avec changements structurels multiples,
KEVIN E SMITH,A systematic search of Zwicky Transient Facility data for ultracompact binary LISA-detectable gravitational-wave sources,"Using photometry collected with the Zwicky Transient Facility, we are conducting an ongoing survey for binary systems with short orbital periods (P_b < 1 hr) with the goal of identifying new gravitational-wave sources detectable by the upcoming Laser Interferometer Space Antenna (LISA). We present a sample of 15 binary systems discovered thus far, with orbital periods ranging from 6.91 to 56.35 minutes. Of the 15 systems, seven are eclipsing systems that do not show signs of significant mass transfer. Additionally, we have discovered two AM Canum Venaticorum systems and six systems exhibiting primarily ellipsoidal variations in their lightcurves. We present follow-up spectroscopy and high-speed photometry confirming the nature of these systems, estimates of their LISA signal-to-noise ratios, and a discussion of their physical characteristics."
KEVIN E SMITH,Nano-engineering of electron correlation in oxide superlattices,"Oxide heterostructures and superlattices have attracted a great deal of attention in recent years owing to the rich exotic properties encountered at their interfaces. We focus on the potential of tunable correlated oxides by investigating the spectral function of the prototypical correlated metal SrVO&lt;sub&gt;3&lt;/sub&gt;, using soft x-ray absorption spectroscopy (XAS) and resonant inelastic soft x-ray scattering (RIXS) to access both unoccupied and occupied electronic states, respectively. We demonstrate a remarkable level of tunability in the spectral function of SrVO&lt;sub&gt;3&lt;/sub&gt; by varying its thickness within the SrVO&lt;sub&gt;3&lt;/sub&gt;/SrTiO&lt;sub&gt;3&lt;/sub&gt; superlattice, showing that the effects of electron correlation can be tuned from dominating the energy spectrum in a strongly correlated Mott-Hubbard insulator, towards a correlated metal. We show that the effects of dimensionality on the correlated properties of SrVO&lt;sub&gt;3&lt;/sub&gt; are augmented by interlayer coupling, yielding a highly flexible correlated oxide that may be readily married with other oxide systems."
KEVIN E SMITH,Effect of lattice mismatch on film morphology of the quasi-one dimensional conductor K0.3MoO3,"High quality epitaxial thin films of the quasi-one dimensional conductor K0.3MoO3 have been successfully grown on SrTiO3(100), SrTiO3(110), and SrTiO3(510) substrates via pulsed laser deposition. Scanning electron microscopy revealed quasi-one dimensional rod-shaped structures parallel to the substrate surface, and the crystal structure was verified by using X-ray diffraction. The temperature dependence of the resistivity for the K0.3MoO3 thin films demonstrates a metal-to-semiconductor transition at about 180 K. Highly anisotropic resistivity was also observed for films grown on SrTiO3(510)."
KEVIN E SMITH,Deep-sea microbes as tools to refine the rules of innate immune pattern recognition.,"The assumption of near-universal bacterial detection by pattern recognition receptors is a foundation of immunology. The limits of this pattern recognition concept, however, remain undefined. As a test of this hypothesis, we determined whether mammalian cells can recognize bacteria that they have never had the natural opportunity to encounter. These bacteria were cultivated from the deep Pacific Ocean, where the genus Moritella was identified as a common constituent of the culturable microbiota. Most deep-sea bacteria contained cell wall lipopolysaccharide (LPS) structures that were expected to be immunostimulatory, and some deep-sea bacteria activated inflammatory responses from mammalian LPS receptors. However, LPS receptors were unable to detect 80% of deep-sea bacteria examined, with LPS acyl chain length being identified as a potential determinant of immunosilence. The inability of immune receptors to detect most bacteria from a different ecosystem suggests that pattern recognition strategies may be defined locally, not globally."
KEVIN E SMITH,The long-baseline neutrino experiment: exploring fundamental symmetries of the universe,"The preponderance of matter over antimatter in the early Universe, the dynamics of the supernova bursts that produced the heavy elements necessary for life and whether protons eventually decay --- these mysteries at the forefront of particle physics and astrophysics are key to understanding the early evolution of our Universe, its current state and its eventual fate. The Long-Baseline Neutrino Experiment (LBNE) represents an extensively developed plan for a world-class experiment dedicated to addressing these questions. LBNE is conceived around three central components: (1) a new, high-intensity neutrino source generated from a megawatt-class proton accelerator at Fermi National Accelerator Laboratory, (2) a near neutrino detector just downstream of the source, and (3) a massive liquid argon time-projection chamber deployed as a far detector deep underground at the Sanford Underground Research Facility. This facility, located at the site of the former Homestake Mine in Lead, South Dakota, is approximately 1,300 km from the neutrino source at Fermilab -- a distance (baseline) that delivers optimal sensitivity to neutrino charge-parity symmetry violation and mass ordering effects. This ambitious yet cost-effective design incorporates scalability and flexibility and can accommodate a variety of upgrades and contributions. With its exceptional combination of experimental configuration, technical capabilities, and potential for transformative discoveries, LBNE promises to be a vital facility for the field of particle physics worldwide, providing physicists from around the globe with opportunities to collaborate in a twenty to thirty year program of exciting science. In this document we provide a comprehensive overview of LBNE's scientific objectives, its place in the landscape of neutrino physics worldwide, the technologies it will incorporate and the capabilities it will possess."
KEVIN E SMITH,New Physics at the LHC. A Les Houches report: Physics at TeV Colliders 2009 - New Physics working group,"We present a collection of signatures for physics beyond the standard model that need to be explored at the LHC. First, are presented various tools developed to measure new particle masses in scenarios where all decays include an unobservable particle. Second, various aspects of supersymmetric models are discussed. Third, some signatures of models of strong electroweak symmetry are discussed. In the fourth part, a special attention is devoted to high mass resonances, as the ones appearing in models with warped extra dimensions. Finally, prospects for models with a hidden sector/valley are presented. Our report, which includes brief experimental and theoretical reviews as well as original results, summarizes the activities of the ""New Physics"" working group for the ""Physics at TeV Colliders"" workshop (Les Houches, France, 8-26 June, 2009)."
KEVIN E SMITH,Event Horizon Telescope imaging of the archetypal blazar 3C 279 at an extreme 20 microarcsecond resolution,"3C 279 is an archetypal blazar with a prominent radio jet that show broadband flux density variability across the entire electromagnetic spectrum. We use an ultra-high angular resolution technique – global Very Long Baseline Interferometry (VLBI) at 1.3 mm (230 GHz) – to resolve the innermost jet of 3C 279 in order to study its fine-scale morphology close to the jet base where highly variable γ-ray emission is thought to originate, according to various models. The source was observed during four days in April 2017 with the Event Horizon Telescope at 230 GHz, including the phased Atacama Large Millimeter/submillimeter Array (ALMA), at an angular resolution of ∼20 μas (at a redshift of z = 0.536 this corresponds to ∼0.13 pc  ∼ 1700 Schwarzschild radii with a black hole mass M<jats:sub>BH</jats:sub> = 8 × 10^8 M_⊙). Imaging and model-fitting techniques were applied to the data to parameterize the fine-scale source structure and its variation. We find a multicomponent inner jet morphology with the northernmost component elongated perpendicular to the direction of the jet, as imaged at longer wavelengths. The elongated nuclear structure is consistent on all four observing days and across different imaging methods and model-fitting techniques, and therefore appears robust. Owing to its compactness and brightness, we associate the northern nuclear structure as the VLBI “core”. This morphology can be interpreted as either a broad resolved jet base or a spatially bent jet. We also find significant day-to-day variations in the closure phases, which appear most pronounced on the triangles with the longest baselines. Our analysis shows that this variation is related to a systematic change of the source structure. Two inner jet components move non-radially at apparent speeds of ∼15 c and ∼20 c (∼1.3 and ∼1.7 μas day^−1, respectively), which more strongly supports the scenario of traveling shocks or instabilities in a bent, possibly rotating jet. The observed apparent speeds are also coincident with the 3C 279 large-scale jet kinematics observed at longer (cm) wavelengths, suggesting no significant jet acceleration between the 1.3 mm core and the outer jet. The intrinsic brightness temperature of the jet components are ≲10^10 K, a magnitude or more lower than typical values seen at ≥7 mm wavelengths. The low brightness temperature and morphological complexity suggest that the core region of 3C 279 becomes optically thin at short (mm) wavelengths."
KEVIN E SMITH,First M87 Event Horizon Telescope results. II. Array and instrumentation,"The Event Horizon Telescope (EHT) is a very long baseline interferometry (VLBI) array that comprises millimeter- and submillimeter-wavelength telescopes separated by distances comparable to the diameter of the Earth. At a nominal operating wavelength of ~1.3 mm, EHT angular resolution (λ/D) is ~25 μas, which is sufficient to resolve nearby supermassive black hole candidates on spatial and temporal scales that correspond to their event horizons. With this capability, the EHT scientific goals are to probe general relativistic effects in the strong-field regime and to study accretion and relativistic jet formation near the black hole boundary. In this Letter we describe the system design of the EHT, detail the technology and instrumentation that enable observations, and provide measures of its performance. Meeting the EHT science objectives has required several key developments that have facilitated the robust extension of the VLBI technique to EHT observing wavelengths and the production of instrumentation that can be deployed on a heterogeneous array of existing telescopes and facilities. To meet sensitivity requirements, high-bandwidth digital systems were developed that process data at rates of 64 gigabit s−1, exceeding those of currently operating cm-wavelength VLBI arrays by more than an order of magnitude. Associated improvements include the development of phasing systems at array facilities, new receiver installation at several sites, and the deployment of hydrogen maser frequency standards to ensure coherent data capture across the array. These efforts led to the coordination and execution of the first Global EHT observations in 2017 April, and to event-horizon-scale imaging of the supermassive black hole candidate in M87."
KEVIN E SMITH,First M87 Event Horizon Telescope results. I. The shadow of the supermassive black hole,"When surrounded by a transparent emission region, black holes are expected to reveal a dark shadow caused by gravitational light bending and photon capture at the event horizon. To image and study this phenomenon, we have assembled the Event Horizon Telescope, a global very long baseline interferometry array observing at a wavelength of 1.3 mm. This allows us to reconstruct event-horizon-scale images of the supermassive black hole candidate in the center of the giant elliptical galaxy M87. We have resolved the central compact radio source as an asymmetric bright emission ring with a diameter of 42 ± 3 μas, which is circular and encompasses a central depression in brightness with a flux ratio gsim10:1. The emission ring is recovered using different calibration and imaging schemes, with its diameter and width remaining stable over four different observations carried out in different days. Overall, the observed image is consistent with expectations for the shadow of a Kerr black hole as predicted by general relativity. The asymmetry in brightness in the ring can be explained in terms of relativistic beaming of the emission from a plasma rotating close to the speed of light around a black hole. We compare our images to an extensive library of ray-traced general-relativistic magnetohydrodynamic simulations of black holes and derive a central mass of M = (6.5 ± 0.7) × 10^9 M ⊙. Our radio-wave observations thus provide powerful evidence for the presence of supermassive black holes in centers of galaxies and as the central engines of active galactic nuclei. They also present a new tool to explore gravity in its most extreme limit and on a mass scale that was so far not accessible."
STEVEN HOMER,Determining Acceptance Possibility for a Quantum Computation is Hard for PH,"It is shown that determining whether a quantum computation has a non-zero probability of accepting is at least as hard as the polynomial time hierarchy. This hardness result also applies to determining in general whether a given quantum basis state appears with nonzero amplitude in a superposition, or whether a given quantum bit has positive expectation value at the end of a quantum computation."
STEVEN HOMER,Small Depth Quantum Circuits,Small depth quantum circuits have proved to be unexpectedly powerful in comparison to their classical counterparts. We survey some of the recent work on this and present some open problems.
STEVEN HOMER,Non-Uniform Reductions,"We study properties of non-uniform reductions and related completeness notions. We strengthen several results of Hitchcock and Pavan and give a trade-off between the amount of advice needed for a reduction and its honesty on NEXP. We construct an oracle relative to which this trade-off is optimal. We show, in a more systematic study of non-uniform reductions, that among other things non-uniformity can be removed at the cost of more queries. In line with Post's program for complexity theory we connect such 'uniformization' properties to the separation of complexity classes."
STEVEN HOMER,On the Complexity of Quantum ACC,"For any q > 1, let MOD_q be a quantum gate that determines if the number of 1's in the input is divisible by q. We show that for any q,t > 1, MOD_q is equivalent to MOD_t (up to constant depth). Based on the case q=2, Moore has shown that quantum analogs of AC^(0), ACC[q], and ACC, denoted QAC^(0)_wf, QACC[2], QACC respectively, define the same class of operators, leaving q > 2 as an open question. Our result resolves this question, implying that QAC^(0)_wf = QACC[q] = QACC for all q. We also prove the first upper bounds for QACC in terms of related language classes. We define classes of languages EQACC, NQACC (both for arbitrary complex amplitudes) and BQACC (for rational number amplitudes) and show that they are all contained in TC^(0). To do this, we show that a TC^(0) circuit can keep track of the amplitudes of the state resulting from the application of a QACC operator using a constant width polynomial size tensor sum. In order to accomplish this, we also show that TC^(0) can perform iterated addition and multiplication in certain field extensions."
STEVEN HOMER,Determining Acceptance Possibility for a Quantum Computation is Hard for the Polynomial Hierarchy,"It is shown that determining whether a quantum computation has a non-zero probability of accepting is at least as hard as the polynomial time hierarchy. This hardness result also applies to determining in general whether a given quantum basis state appears with nonzero amplitude in a superposition, or whether a given quantum bit has positive expectation value at the end of a quantum computation. This result is achieved by showing that the complexity class NQP of Adleman, Demarrais, and Huang, a quantum analog of NP, is equal to the counting class coC=P."
STEVEN HOMER,On the Performance of Polynomial-time CLIQUE Approximation Algorithms on Very Large Graphs,"The performance of a randomized version of the subgraph-exclusion algorithm (called Ramsey) for CLIQUE by Boppana and Halldorsson is studied on very large graphs. We compare the performance of this algorithm with the performance of two common heuristic algorithms, the greedy heuristic and a version of simulated annealing. These algorithms are tested on graphs with up to 10,000 vertices on a workstation and graphs as large as 70,000 vertices on a Connection Machine. Our implementations establish the ability to run clique approximation algorithms on very large graphs. We test our implementations on a variety of different graphs. Our conclusions indicate that on randomly generated graphs minor changes to the distribution can cause dramatic changes in the performance of the heuristic algorithms. The Ramsey algorithm, while not as good as the others for the most common distributions, seems more robust and provides a more even overall performance. In general, and especially on deterministically generated graphs, a combination of simulated annealing with either the Ramsey algorithm or the greedy heuristic seems to perform best. This combined algorithm works particularly well on large Keller and Hamming graphs and has a competitive overall performance on the DIMACS benchmark graphs."
STEVEN HOMER,Learning Unions of Rectangles with Queries,"We investigate the efficient learnability of unions of k rectangles in the discrete plane (1,...,n)[2] with equivalence and membership queries. We exhibit a learning algorithm that learns any union of k rectangles with O(k^3log n) queries, while the time complexity of this algorithm is bounded by O(k^5log n). We design our learning algorithm by finding ""corners"" and ""edges"" for rectangles contained in the target concept and then constructing the target concept from those ""corners"" and ""edges"". Our result provides a first approach to on-line learning of nontrivial subclasses of unions of intersections of halfspaces with equivalence and membership queries."
STEVEN HOMER,On Learning Counting Functions With Queries,"We investigate the problem of learning disjunctions of counting functions, which are general cases of parity and modulo functions, with equivalence and membership queries. We prove that, for any prime number p, the class of disjunctions of integer-weighted counting functions with modulus p over the domain Znq (or Zn) for any given integer q ≥ 2 is polynomial time learnable using at most n + 1 equivalence queries, where the hypotheses issued by the learner are disjunctions of at most n counting functions with weights from Zp. The result is obtained through learning linear systems over an arbitrary field. In general a counting function may have a composite modulus. We prove that, for any given integer q ≥ 2, over the domain Zn2, the class of read-once disjunctions of Boolean-weighted counting functions with modulus q is polynomial time learnable with only one equivalence query, and the class of disjunctions of log log n Boolean-weighted counting functions with modulus q is polynomial time learnable. Finally, we present an algorithm for learning graph-based counting functions."
STEVEN HOMER,Some properties of sets in the plane closed under linear extrapolation by a fixed parameter,"Fix any 𝛌 ⊆ ℂ. We say that a set S ⊆ ℂ is 𝛌-convex if, whenever a and b are in S, the point (1- 𝛌)a + 𝛌b is also in S. If S is also (topologically) closed, then we say that S is 𝛌-clonvex. We investigate the properties of 𝛌-convex and 𝛌-clonvex sets and prove a number of facts about them. Letting R_𝛌 ⊆ ℂ be the least 𝛌-clonvex superset of {0,1}, we show that if R_𝛌 is convex in the usual sense, then R_𝛌 must be either [0,1] or ℝor ℂ, depending on 𝛌. We investigate which 𝛌 make R_𝛌 convex, derive a number of conditions equivalent to R_𝛌 being convex, give several conditions sufficient for R_𝛌 to be convex or not convex in particular, R_𝛌 is either convex or discrete, and investigate the properties of some particular discrete R_𝛌, as well as other 𝛌-convex sets. Our work combines elementary concepts and techniques from algebra and plane geometry."
STEVEN HOMER,"Bostonia: 1993-1994, no. 2-3",
SALLY SEDGWICK,Sensitivity of Super-Kamiokande with Gadolinium to Low Energy Antineutrinos from Pre-supernova Emission,"Supernova detection is a major objective of the Super-Kamiokande (SK) experiment. In the next stage of SK (SK-Gd), gadolinium (Gd) sulfate will be added to the detector, which will improve the ability of the detector to identify neutrons. A core-collapse supernova (CCSN) will be preceded by an increasing flux of neutrinos and antineutrinos, from thermal and weak nuclear processes in the star, over a timescale of hours; some of which may be detected at SK-Gd. This could provide an early warning of an imminent CCSN, hours earlier than the detection of the neutrinos from core collapse. Electron antineutrino detection will rely on inverse beta decay events below the usual analysis energy threshold of SK, so Gd loading is vital to reduce backgrounds while maximizing detection efficiency. Assuming normal neutrino mass ordering, more than 200 events could be detected in the final 12 hr before core collapse for a 15–25 solar mass star at around 200 pc, which is representative of the nearest red supergiant to Earth, α-Ori (Betelgeuse). At a statistical false alarm rate of 1 per century, detection could be up to 10 hr before core collapse, and a pre-supernova star could be detected by SK-Gd up to 600 pc away. A pre-supernova alert could be provided to the astrophysics community following gadolinium loading."
SALLY SEDGWICK,Search for astronomical neutrinos from blazar TXS 0506+056 in Super-Kamiokande,"We report a search for astronomical neutrinos in the energy region from several GeV to TeV in the direction of the blazar TXS 0506+056 using the Super-Kamiokande detector following the detection of a 100 TeV neutrinos from the same location by the IceCube collaboration. Using Super-Kamiokande neutrino data across several data samples observed from 1996 April to 2018 February we have searched for both a total excess above known backgrounds across the entire period as well as localized excesses on smaller timescales in that interval. No significant excess nor significant variation in the observed event rate are found in the blazar direction. Upper limits are placed on the electron- and muon-neutrino fluxes at the 90% confidence level as 6.0 × 10^−7 and 4.5 × 10^−7–9.3 × 10^−10 [erg cm^−2 s^−1], respectively."
SOFYA RASKHODNIKOVA,Erasure-resilient sublinear-time graph algorithms,"We investigate sublinear-time algorithms that take partially erased graphs represented by adjacency lists as input. Our algorithms make degree and neighbor queries to the input graph and work with a specified fraction of adversarial erasures in adjacency entries. We focus on two computational tasks: testing if a graph is connected or ε-far from connected and estimating the average degree. For testing connectedness, we discover a threshold phenomenon: when the fraction of erasures is less than ε, this property can be tested efficiently (in time independent of the size of the graph); when the fraction of erasures is at least ε, then a number of queries linear in the size of the graph representation is required. Our erasure-resilient algorithm (for the special case with no erasures) is an improvement over the previously known algorithm for connectedness in the standard property testing model and has optimal dependence on the proximity parameter ε. For estimating the average degree, our results provide an “interpolation” between the query complexity for this computational task in the model with no erasures in two different settings: with only degree queries, investigated by Feige (SIAM J. Comput. ‘06), and with degree queries and neighbor queries, investigated by Goldreich and Ron (Random Struct. Algorithms ‘08) and Eden et al. (ICALP ‘17). We conclude with a discussion of our model and open questions raised by our work."
SOFYA RASKHODNIKOVA,The price of differential privacy under continual observation,"We study the accuracy of differentially private mechanisms in the continual release model. A continual release mechanism receives a sensitive dataset as a stream of T inputs and produces, after receiving each input, an accurate output on the obtained inputs. In contrast, a batch algorithm receives the data as one batch and produces a single output. We provide the first strong lower bounds on the error of continual release mechanisms. In particular, for two fundamental problems that are widely studied and used in the batch model, we show that the worst case error of every continual release algorithm is ~Ω (T^1/3) times larger than that of the best batch algorithm. Previous work shows only a polylogarithimic (in T) gap between the worst case error achievable in these two models; further, for many problems, including the summation of binary attributes, the polylogarithmic gap is tight (Dwork et al., 2010; Chan et al., 2010). Our results show that problems closely related to summation-specifically, those that require selecting the largest of a set of sums|are fundamentally harder in the continual release model than in the batch model. Our lower bounds assume only that privacy holds for streams fixed in advance (the ""nonadaptive"" setting). However, we provide matching upper bounds that hold in a model where privacy is required even for adaptively selected streams. This model may be of independent interest."
SOFYA RASKHODNIKOVA,Differentially private sampling from distributions,"We initiate an investigation of private sampling from distributions. Given a dataset with n independent observations from an unknown distribution P, a sampling algorithm must output a single observation from a distribution that is close in total variation distance to P while satisfying differential privacy. Sampling abstracts the goal of generating small amounts of realistic-looking data. We provide tight upper and lower bounds for the dataset size needed for this task for three natural families of distributions: arbitrary distributions on {1,…,k}, arbitrary product distributions on {0,1}d, and product distributions on on {0,1}d with bias in each coordinate bounded away from 0 and 1. We demonstrate that, in some parameter regimes, private sampling requires asymptotically fewer observations than learning a description of P nonprivately; in other regimes, however, private sampling proves to be as difficult as private learning. Notably, for some classes of distributions, the overhead in the number of observations needed for private learning compared to non-private learning is completely captured by the number of observations needed for private sampling."
SOFYA RASKHODNIKOVA,Sublinear-time computation in the presence of online erasures,"We initiate the study of sublinear-time algorithms that access their input via an online adversarial erasure oracle. After answering each query to the input object, such an oracle can erase t input values. Our goal is to understand the complexity of basic computational tasks in extremely adversarial situations, where the algorithm’s access to data is blocked during the execution of the algorithm in response to its actions. Specifically, we focus on property testing in the model with online erasures. We show that two fundamental properties of functions, linearity and quadraticity, can be tested for constant t with asymptotically the same complexity as in the standard property testing model. For linearity testing, we prove tight bounds in terms of t, showing that the query complexity is Θ(log t). In contrast to linearity and quadraticity, some other properties, including sortedness and the Lipschitz property of sequences, cannot be tested at all, even for t = 1. Our investigation leads to a deeper understanding of the structure of violations of linearity and other widely studied properties."
SOFYA RASKHODNIKOVA,Node-differentially private estimation of the number of connected components,
SOFYA RASKHODNIKOVA,Testing connectedness of images,
ARI TRACHTENBERG,Priority-Based Synchronization of Distributed Data,"We consider the general problem of synchronizing the data on two devices using a minimum amount of communication, a core infrastructural requirement for a large variety of distributed systems. Our approach considers the interactive synchronization of prioritized data, where, for example, certain information is more time-sensitive than other information. We propose and analyze a new scheme for efficient priority-based synchronization, which promises benefits over conventional synchronization."
ARI TRACHTENBERG,Anonymous collocation discovery: taming the coronavirus while preserving privacy,"Successful containment of the Coronavirus pandemic rests on the ability to quickly and reliably identify those who have been in close proximity to a contagious individual. Existing tools for doing so rely on the collection of exact location information of individuals over lengthy time periods, and combining this information with other personal information. This unprecedented encroachment on individual privacy at national scales has created an outcry and risks rejection of these tools. We propose an alternative: an extremely simple scheme for providing fine-grained and timely alerts to users who have been in the close vicinity of an infected individual. Crucially, this is done while preserving the anonymity of all individuals, and without collecting or storing any personal information or location history. Our approach is based on using short-range communication mechanisms, like Bluetooth, that are available in all modern cell phones. It can be deployed with very little infrastructure, and incurs a relatively low false-positive rate compared to other collocation methods. We also describe a number of extensions and tradeoffs. We believe that the privacy guarantees provided by the scheme will encourage quick and broad voluntary adoption. When combined with sufficient testing capacity and existing best practices from healthcare professionals, we hope that this may significantly reduce the infection rate."
ARI TRACHTENBERG,Automated exposure notification for COVID-19,"In the current COVID-19 pandemic, various Automated Exposure Notification (AEN) systems have been proposed to help quickly identify potential contacts of infected individuals. All these systems try to leverage the current understanding of the following factors: transmission risk, technology to address risk modeling, system policies and privacy considerations. While AEN holds promise for mitigating the spread of COVID-19, using short-range communication channels (Bluetooth) in smartphones to detect close individual contacts may be inaccurate for modeling and informing transmission risk. This work finds that the current close contact definitions may be inadequate to reduce viral spread using AEN technology. Consequently, relying on distance measurements from Bluetooth Low-Energy may not be optimal for determining risks of exposure and protecting privacy. This paper's literature analysis suggests that AEN may perform better by using broadly accessible technologies to sense the respiratory activity, mask status, or environment of participants. Moreover, the paper remains cognizant that smartphone sensors can leak private information and thus recommends additional objectives for maintaining user privacy without compromising utility for population health. This literature review and analysis will simultaneously interest (i) health professionals who desire a fundamental understanding of the design and utility of AEN systems and (ii) technologists interested in understanding their epidemiological basis in the light of recent research. Ultimately, the two disparate communities need to understand each other to assess the value of AEN systems in mitigating viral spread, whether for the COVID-19 pandemic or for future ones."
ARI TRACHTENBERG,Empirical comparison of block relay protocols,"Block relay protocols play a key role in the performance and security of public blockchains. As a result, several such protocols have been deployed in the context of Bitcoin and its variants (e.g., legacy, compact block relay and Graphene) in an attempt to reduce bandwidth utilization. However, the relative performance of these protocols in realistic networking conditions (e.g., with nodes churning - joining and leaving the network) is still not known. This paper aims to fill this key knowledge gap using an experimental testbed of twelve full nodes connected to the Bitcoin Cash blockchain. With the aid of novel logging tools, we contrast the performance of these three protocols, in realistic scenarios, with respect to communication, delay, and block decoding success. Our main findings are that Graphene generally performs the best when nodes remain connected, boasting an average propagation delay of 190 ms (i.e., 29% lower than compact block and 80% lower than the legacy protocol). However, when nodes churn at a high rate, compact blocks may perform better. Through a careful temporal analysis, we identify some root causes of the protocol inefficiencies, together with potential mitigation. We have made our measurement framework and experimental logs publicly available to the broader research community."
ARI TRACHTENBERG,Case study: disclosure of indirect device fingerprinting in privacy policies,"Recent developments in online tracking make it harder for individuals to detect and block trackers. This is especially true for de- vice fingerprinting techniques that websites use to identify and track individual devices. Direct trackers { those that directly ask the device for identifying information { can often be blocked with browser configu- rations or other simple techniques. However, some sites have shifted to indirect tracking methods, which attempt to uniquely identify a device by asking the browser to perform a seemingly-unrelated task. One type of indirect tracking known as Canvas fingerprinting causes the browser to render a graphic recording rendering statistics as a unique identifier. Even experts find it challenging to discern some indirect fingerprinting methods. In this work, we aim to observe how indirect device fingerprint- ing methods are disclosed in privacy policies, and consider whether the disclosures are sufficient to enable website visitors to block the track- ing methods. We compare these disclosures to the disclosure of direct fingerprinting methods on the same websites. Our case study analyzes one indirect ngerprinting technique, Canvas fingerprinting. We use an existing automated detector of this fingerprint- ing technique to conservatively detect its use on Alexa Top 500 websites that cater to United States consumers, and we examine the privacy poli- cies of the resulting 28 websites. Disclosures of indirect fingerprinting vary in specificity. None described the specific methods with enough granularity to know the website used Canvas fingerprinting. Conversely, many sites did provide enough detail about usage of direct fingerprint- ing methods to allow a website visitor to reliably detect and block those techniques. We conclude that indirect fingerprinting methods are often technically difficult to detect, and are not identified with specificity in legal privacy notices. This makes indirect fingerprinting more difficult to block, and therefore risks disturbing the tentative armistice between individuals and websites currently in place for direct fingerprinting. This paper illustrates differences in fingerprinting approaches, and explains why technologists, technology lawyers, and policymakers need to appreciate the challenges of indirect fingerprinting."
DANIEL G REMICK,Allergens Induce Enhanced Bronchoconstriction and Leukotriene Production in C5 Deficient Mice,"BACKGROUND. Previous genetic analysis has shown that a deletion in the complement component 5 gene-coding region renders mice more susceptible to allergen-induced airway hyperresponsiveness (AHR) due to reduced IL-12 production. We investigated the role of complement in a murine model of asthma-like pulmonary inflammation. METHODS. In order to evaluate the role of complement B10 mice either sufficient or deficient in C5 were studied. Both groups of mice immunized and challenged with a house dust extract (HDE) containing high levels of cockroach allergens. Airways hyper-reactivity was determined with whole-body plesthysmography. Bronchoalveolar lavage (BAL) was performed to determine pulmonary cellular recruitment and measure inflammatory mediators. Lung homogenates were assayed for mediators and plasma levels of IgE determined. Pulmonary histology was also evaluated. RESULTS. C5-deficient mice showed enhanced AHR to methylcholine challenge, 474% and 91% increase above baseline Penh in C5-deficient and C5-sufficient mice respectively, p < 0.001. IL-12 levels in the lung homogenate (LH) were only slightly reduced and BAL IL-12 was comparable in C5-sufficient and C5-deficient mice. However, C5-deficient mice had significantly higher cysteinyl-leukotriene levels in the BAL fluid, 1913 +/- 246 pg/ml in C5d and 756 +/- 232 pg/ml in C5-sufficient, p = 0.003. CONCLUSION. These data demonstrate that C5-deficient mice show enhanced AHR due to increased production of cysteinyl-leukotrienes."
DANIEL G REMICK,Oral tolerance inhibits pulmonary eosinophilia in a cockroach allergen induced model of asthma: a randomized laboratory study,"BACKGROUND. Antigen desensitization through oral tolerance is becoming an increasingly attractive treatment option for allergic diseases. However, the mechanism(s) by which tolerization is achieved remain poorly defined. In this study we endeavored to induce oral tolerance to cockroach allergen (CRA: a complex mixture of insect components) in order to ameliorate asthma-like, allergic pulmonary inflammation. METHODS. We compared the pulmonary inflammation of mice which had received four CRA feedings prior to intratracheal allergen sensitization and challenge to mice fed PBS on the same time course. Respiratory parameters were assessed by whole body unrestrained plethysmography and mechanical ventilation with forced oscillation. Bronchoalveolar lavage fluid (BAL) and lung homogenate (LH) were assessed for cytokines and chemokines by ELISA. BAL inflammatory cells were also collected and examined by light microscopy. RESULTS. CRA feeding prior to allergen sensitization and challenge led to a significant improvement in respiratory health. Airways hyperreactivity measured indirectly via enhanced pause (Penh) was meaningfully reduced in the CRA-fed mice compared to the PBS fed mice (2.3 ± 0.4 vs 3.9 ± 0.6; p = 0.03). Directly measured airways resistance confirmed this trend when comparing the CRA-fed to the PBS-fed animals (2.97 ± 0.98 vs 4.95 ± 1.41). This effect was not due to reduced traditional inflammatory cell chemotactic factors, Th2 or other cytokines and chemokines. The mechanism of improved respiratory health in the tolerized mice was due to significantly reduced eosinophil numbers in the bronchoalveolar lavage fluid (43300 ± 11445 vs 158786 ± 38908; p = 0.007) and eosinophil specific peroxidase activity in the lung homogenate (0.59 ± 0.13 vs 1.19 ± 0.19; p = 0.017). The decreased eosinophilia was likely the result of increased IL-10 in the lung homogenate of the tolerized mice (6320 ± 354 ng/mL vs 5190 ± 404 ng/mL, p = 0.02). CONCLUSION. Our results show that oral tolerization to CRA can improve the respiratory health of experimental mice in a CRA-induced model of asthma-like pulmonary inflammation by reducing pulmonary eosinophilia."
THOMAS T PERLS,Imputation of Missing Genotypes: An Empirical Evaluation of IMPUTE,"BACKGROUND: Imputation of missing genotypes is becoming a very popular solution for synchronizing genotype data collected with different microarray platforms but the effect of ethnic background, subject ascertainment, and amount of missing data on the accuracy of imputation are not well understood. RESULTS: We evaluated the accuracy of the program IMPUTE to generate the genotype data of partially or fully untyped single nucleotide polymorphisms (SNPs). The program uses a model-based approach to imputation that reconstructs the genotype distribution given a set of referent haplotypes and the observed data, and uses this distribution to compute the marginal probability of each missing genotype for each individual subject that is used to impute the missing data. We assembled genome-wide data from five different studies and three different ethnic groups comprising Caucasians, African Americans and Asians. We randomly removed genotype data and then compared the observed genotypes with those generated by IMPUTE. Our analysis shows 97% median accuracy in Caucasian subjects when less than 10% of the SNPs are untyped and missing genotypes are accepted regardless of their posterior probability. The median accuracy increases to 99% when we require 0.95 minimum posterior probability for an imputed genotype to be acceptable. The accuracy decreases to 86% or 94% when subjects are African Americans or Asians. We propose a strategy to improve the accuracy by leveraging the level of admixture in African Americans. CONCLUSION: Our analysis suggests that IMPUTE is very accurate in samples of Caucasians origin, it is slightly less accurate in samples of Asians background, but substantially less accurate in samples of admixed background such as African Americans. Sample size and ascertainment do not seem to affect the accuracy of imputation."
THOMAS T PERLS,A Hierarchical and Modular Approach to the Discovery of Robust Associations in Genome-Wide Association Studies from Pooled DNA Samples,"BACKGROUND: One of the challenges of the analysis of pooling-based genome wide association studies is to identify authentic associations among potentially thousands of false positive associations. RESULTS. We present a hierarchical and modular approach to the analysis of genome wide genotype data that incorporates quality control, linkage disequilibrium, physical distance and gene ontology to identify authentic associations among those found by statistical association tests. The method is developed for the allelic association analysis of pooled DNA samples, but it can be easily generalized to the analysis of individually genotyped samples. We evaluate the approach using data sets from diverse genome wide association studies including fetal hemoglobin levels in sickle cell anemia and a sample of centenarians and show that the approach is highly reproducible and allows for discovery at different levels of synthesis. CONCLUSION: Results from the integration of Bayesian tests and other machine learning techniques with linkage disequilibrium data suggest that we do not need to use too stringent thresholds to reduce the number of false positive associations. This method yields increased power even with relatively small samples. In fact, our evaluation shows that the method can reach almost 70% sensitivity with samples of only 100 subjects."
THOMAS T PERLS,RNA Editing Genes Associated with Extreme Old Age in Humans and with Lifespan in C. elegans,"BACKGROUND. The strong familiality of living to extreme ages suggests that human longevity is genetically regulated. The majority of genes found thus far to be associated with longevity primarily function in lipoprotein metabolism and insulin/IGF-1 signaling. There are likely many more genetic modifiers of human longevity that remain to be discovered. METHODOLOGY/PRINCIPAL FINDINGS. Here, we first show that 18 single nucleotide polymorphisms (SNPs) in the RNA editing genes ADARB1 and ADARB2 are associated with extreme old age in a U.S. based study of centenarians, the New England Centenarian Study. We describe replications of these findings in three independently conducted centenarian studies with different genetic backgrounds (Italian, Ashkenazi Jewish and Japanese) that collectively support an association of ADARB1 and ADARB2 with longevity. Some SNPs in ADARB2 replicate consistently in the four populations and suggest a strong effect that is independent of the different genetic backgrounds and environments. To evaluate the functional association of these genes with lifespan, we demonstrate that inactivation of their orthologues adr-1 and adr-2 in C. elegans reduces median survival by 50%. We further demonstrate that inactivation of the argonaute gene, rde-1, a critical regulator of RNA interference, completely restores lifespan to normal levels in the context of adr-1 and adr-2 loss of function. CONCLUSIONS/SIGNIFICANCE. Our results suggest that RNA editors may be an important regulator of aging in humans and that, when evaluated in C. elegans, this pathway may interact with the RNA interference machinery to regulate lifespan."
THOMAS T PERLS,"Whole genome sequences of a male and female supercentenarian, ages greater than 114 years","Supercentenarians (age 110+ years old) generally delay or escape age-related diseases and disability well beyond the age of 100 and this exceptional survival is likely to be influenced by a genetic predisposition that includes both common and rare genetic variants. In this report, we describe the complete genomic sequences of male and female supercentenarians, both age >114 years old. We show that: (1) the sequence variant spectrum of these two individuals' DNA sequences is largely comparable to existing non-supercentenarian genomes; (2) the two individuals do not appear to carry most of the well-established human longevity enabling variants already reported in the literature; (3) they have a comparable number of known disease-associated variants relative to most human genomes sequenced to-date; (4) approximately 1% of the variants these individuals possess are novel and may point to new genes involved in exceptional longevity; and (5) both individuals are enriched for coding variants near longevity-associated variants that we discovered through a large genome-wide association study. These analyses suggest that there are both common and rare longevity-associated variants that may counter the effects of disease-predisposing variants and extend lifespan. The continued analysis of the genomes of these and other rare individuals who have survived to extremely old ages should provide insight into the processes that contribute to the maintenance of health during extreme aging."
JAMES B. REBITZER,Team formation and performance: evidence from healthcare referral networks,"We examine the teams that emerge when a primary care physician (PCP) refers patients to specialists. When PCPs concentrate their specialist referrals — for instance, sending their cardiology patients to fewer distinct cardiologists — this encourages repeat interactions between PCPs and specialists. Repeated interactions provide more opportunities and incentives to develop productive team relationships. Using data from the Massachusetts All Payer Claims Database, we construct a new measure of PCP team referral concentration and document that it varies widely across PCPs, even among PCPs in the same organization. Chronically ill patients treated by PCPs with 1 standard deviation higher team referral concentration have 4% lower health care utilization on average, with no discernible reduction in quality. We corroborate this finding using a national sample of Medicare claims, and show that it holds under various identification strategies that account for observed and unobserved patient and physician characteristics. The results suggest that repeated PCP-specialist interactions improve team performance."
LEONID REYZIN,Verifiable Random Functions (VRFs),"A Verifiable Random Function (VRF) is the public-key version of a keyed cryptographic hash. Only the holder of the private key can compute the hash, but anyone with public key can verify the correctness of the hash. VRFs are useful for preventing enumeration of hash-based data structures. This document specifies several VRF constructions that are secure in the cryptographic random oracle model. One VRF uses RSA and the other VRF uses Eliptic Curves (EC)."
LEONID REYZIN,Entropy Loss is Maximal for Uniform Inputs,"A secure sketch (deﬁned by Dodis et al.) is an algorithm that on an input w produces an output s such that w can be reconstructed given its noisy version w' and s. Security is deﬁned in terms of two parameters m and m˜ : if w comes from a distribution of entropy m, then a secure sketch guarantees that the distribution of w conditioned on s has entropy m˜ , where λ = m−m˜ is called the entropy loss. In this note we show that the entropy loss of any secure sketch (or, more generally, any randomized algorithm) on any distribution is no more than it is on the uniform distribution."
LEONID REYZIN,Authenticated Index Structures for Aggregation Queries in Outsourced Databases,"In an outsourced database system the data owner publishes information through a number of remote, untrusted servers with the goal of enabling clients to access and query the data more efficiently. As clients cannot trust servers, query authentication is an essential component in any outsourced database system. Clients should be given the capability to verify that the answers provided by the servers are correct with respect to the actual data published by the owner. While existing work provides authentication techniques for selection and projection queries, there is a lack of techniques for authenticating aggregation queries. This article introduces the first known authenticated index structures for aggregation queries. First, we design an index that features good performance characteristics for static environments, where few or no updates occur to the data. Then, we extend these ideas and propose more involved structures for the dynamic case, where the database owner is allowed to update the data arbitrarily. Our structures feature excellent average case performance for authenticating queries with multiple aggregate attributes and multiple selection predicates. We also implement working prototypes of the proposed techniques and experimentally validate the correctness of our ideas."
LEONID REYZIN,Authenticated Index Structures for Outsourced Database Systems,"In outsourced database (ODB) systems the database owner publishes its data through a number of remote servers, with the goal of enabling clients at the edge of the network to access and query the data more efficiently. As servers might be untrusted or can be compromised, query authentication becomes an essential component of ODB systems. Existing solutions for this problem concentrate mostly on static scenarios and are based on idealistic properties for certain cryptographic primitives. In this work, first we define a variety of essential and practical cost metrics associated with ODB systems. Then, we analytically evaluate a number of different approaches, in search for a solution that best leverages all metrics. Most importantly, we look at solutions that can handle dynamic scenarios, where owners periodically update the data residing at the servers. Finally, we discuss query freshness, a new dimension in data authentication that has not been explored before. A comprehensive experimental evaluation of the proposed and existing approaches is used to validate the analytical models and verify our claims. Our findings exhibit that the proposed solutions improve performance substantially over existing approaches, both for static and dynamic environments."
LEONID REYZIN,A Note On the Statistical Difference of Small Direct Products,"We demonstrate that if two probability distributions D and E of sufficiently small min-entropy have statistical difference ε, then the direct-product distributions D^l and E^l have statistical difference at least roughly ε\s√l, provided that l is sufficiently small, smaller than roughly ε^{4/3}. Previously known bounds did not work for few repetitions l, requiring l>ε^2."
LEONID REYZIN,Efficient noninteractive certification of RSA moduli and beyond,"In many applications, it is important to verify that an RSA public key (N, e) specifies a permutation over the entire space ℤ𝑁 , in order to prevent attacks due to adversarially-generated public keys. We design and implement a simple and efficient noninteractive zero-knowledge protocol (in the random oracle model) for this task. Applications concerned about adversarial key generation can just append our proof to the RSA public key without any other modifications to existing code or cryptographic libraries. Users need only perform a one-time verification of the proof to ensure that raising to the power e is a permutation of the integers modulo N. For typical parameter settings, the proof consists of nine integers modulo N; generating the proof and verifying it both require about nine modular exponentiations. We extend our results beyond RSA keys and also provide efficient noninteractive zero-knowledge proofs for other properties of N, which can be used to certify that N is suitable for the Paillier cryptosystem, is a product of two primes, or is a Blum integer. As compared to the recent work of Auerbach and Poettering (PKC 2018), who provide two-message protocols for similar languages, our protocols are more efficient and do not require interaction, which enables a broader class of applications."
LEONID REYZIN,Impacting IP prefix reachability via RPKI manipulations,"The RPKI is an infrastructure that will provide digitally signed attestations for the hierarchical allocation and suballocation of IP addresses. Its goal is to improve security of interdomain routing by providing reliable data showing which autonomous system (AS) is authorized to originate which IP prefix. We discuss how the hierarchical nature of the RPKI makes it technically possible for any party above a target IP prefix in the RPKI hierarchy to revoke that target IP prefix. We show that such revocation can be ``surgical''---i.e., impacting only the desired IP address or prefix---and difficult to detect. We also discuss the impact such revocation has on routing. This note focuses only on the issues of technical feasibility (rather than legal or operational issues), and should not be taken as recommendation for or against the use of the RPKI."
LEONID REYZIN,Secure pairing of mobile devices (MA thesis),"As mobile devices become increasingly popular, the necessity for both user-friendly and secure pairing methods for these devices also rises. One natural approach to pairing devices is to match them based on a shared experience. In this work, we define a shared experience as the act of physically holding two devices together and shaking them for a short period. The common movement data collected during the shaking process can subsequently be used to verify the authenticity of a secret key established via a key exchange protocol. This paper explores the process of key verification using two different measures: a coherence measure derived through time series analysis and a measure based on Hamming distance. Using ROC curves, we show that both of these measures robustly distinguish between the case where two devices have been shaken together and the case where two devices have been shaken separately."
LEONID REYZIN,Can NSEC5 be practical for DNSSEC deployments?,"NSEC5 is proposed modification to DNSSEC that simultaneously guarantees two security properties: (1) privacy against offline zone enumeration, and (2) integrity of zone contents, even if an adversary compromises the authoritative nameserver responsible for responding to DNS queries for the zone. This paper redesigns NSEC5 to make it both practical and performant. Our NSEC5 redesign features a new fast verifiable random function (VRF) based on elliptic curve cryptography (ECC), along with a cryptographic proof of its security. This VRF is also of independent interest, as it is being standardized by the IETF and being used by several other projects. We show how to integrate NSEC5 using our ECC-based VRF into the DNSSEC protocol, leveraging precomputation to improve performance and DNS protocol-level optimizations to shorten responses. Next, we present the first full-fledged implementation of NSEC5—extending widely-used DNS software to present a nameserver and recursive resolver that support NSEC5—and evaluate their performance under aggressive DNS query loads. Our performance results indicate that our redesigned NSEC5 can be viable even for high-throughput scenarios"
LEONID REYZIN,Computational entropy and information leakage (MA Thesis),"We investigate how information leakage reduces computational entropy of a random variable X. Recall that HILL and metric computational entropy are parameterized by quality (how distinguishable is X from a variable Z that has true entropy) and quantity (how much true entropy is there in Z). We prove an intuitively natural result: conditioning on an event of probability p reduces the quality of metric entropy by a factor of p and the quantity of metric entropy by log 1/p (note that this means that the reduction in quantity and quality is the same, because the quantity of entropy is measured on logarithmic scale). Our result improves previous bounds of Dziembowski and Pietrzak (FOCS 2008), where the loss in the quantity of entropy was related to its original quality. The use of metric entropy tightens the result of Reingold et. al. (FOCS 2008) and makes it easy to measure entropy even after conditioning on several events. Further, we simplify dealing with information leakage by investigating conditional metric entropy. We show that, conditioned on leakage of λ bits, metric entropy gets reduced by a factor 2^λ in quality and λ in quantity. Our formulation allow us to formulate a ""chain rule"" for leakage on computational entropy. We show that conditioning on λ bits of leakage reduces conditional metric entropy by λ bits. This is the same loss as leaking from unconditional metric entropy. This result makes it easy to measure entropy even after several rounds of information leakage."
LEONID REYZIN,Fuzzy password-authenticated key exchange,
LEONID REYZIN,A unified framework for trapdoor-permutation-based sequential aggregate signatures,"We give a framework for trapdoor-permutation-based sequential aggregate signatures (SAS) that unifies and simplifies prior work and leads to new results. The framework is based on ideal ciphers over large domains, which have recently been shown to be realizable in the random oracle model. The basic idea is to replace the random oracle in the full-domain-hash signature scheme with an ideal cipher. Each signer in sequence applies the ideal cipher, keyed by the message, to the output of the previous signer, and then inverts the trapdoor permutation on the result. We obtain different variants of the scheme by varying additional keying material in the ideal cipher and making different assumptions on the trapdoor permutation. In particular, we obtain the first scheme with lazy verification and signature size independent of the number of signers that does not rely on bilinear pairings. Since existing proofs that ideal ciphers over large domains can be realized in the random oracle model are lossy, our schemes do not currently permit practical instantiation parameters at a reasonable security level, and thus we view our contribution as mainly conceptual. However, we are optimistic tighter proofs will be found, at least in our specific application."
LEONID REYZIN,Turning HATE into LOVE: compact homomorphic ad hoc threshold encryption for scalable MPC,"In a public-key threshold encryption scheme, the sender produces a single ciphertext, and any 𝑡+1 out of 𝑛 intended recipients can combine their partial decryptions to obtain the plaintext. Ad hoc threshold encryption (ATE) schemes require no correlated setup, enabling each party to simply generate its own key pair. In this paper, we initiate a systematic study of the possibilities and limitations of ad-hoc threshold encryption, and introduce a key application to scalable multiparty computation (MPC). Assuming indistinguishability obfuscation (iO), we construct the first ATE that is sender-compact—that is, with ciphertext length independent of 𝑛. This allows for succinct communication once public keys have been shared. We also show a basic lower bound on the extent of key sharing: every sender-compact scheme requires that recipients of a message know the public keys of other recipients in order to decrypt. We then demonstrate that threshold encryption that is ad hoc and homomorphic can be used to build efficient large-scale fault-tolerant multiparty computation (MPC) on a minimal (star) communication graph. We explore several homomorphic schemes, in particular obtaining one iO-based ATE scheme that is both sender-compact and homomorphic: each recipient can derive what they need for evaluation from a single short ciphertext. In the resulting MPC protocol, once the public keys have been distributed, all parties in the graph except for the central server send and receive only short messages, whose size is independent of the number of participants. Taken together, our results chart new possibilities for threshold encryption and raise intriguing open questions."
LEONID REYZIN,Efficient noninteractive certification of RSA moduli and beyond,"In many applications, it is important to verify that an RSA public key (N; e) speci es a permutation over the entire space ZN, in order to prevent attacks due to adversarially-generated public keys. We design and implement a simple and e cient noninteractive zero-knowledge protocol (in the random oracle model) for this task. Applications concerned about adversarial key generation can just append our proof to the RSA public key without any other modi cations to existing code or cryptographic libraries. Users need only perform a one-time veri cation of the proof to ensure that raising to the power e is a permutation of the integers modulo N. For typical parameter settings, the proof consists of nine integers modulo N; generating the proof and verifying it both require about nine modular exponentiations. We extend our results beyond RSA keys and also provide e cient noninteractive zero- knowledge proofs for other properties of N, which can be used to certify that N is suitable for the Paillier cryptosystem, is a product of two primes, or is a Blum integer. As compared to the recent work of Auerbach and Poettering (PKC 2018), who provide two-message protocols for similar languages, our protocols are more e cient and do not require interaction, which enables a broader class of applications."
LEONID REYZIN,Certifying RSA public keys with an efficient NIZK,"In many applications, it is important to verify that an RSA public key ( N,e ) specifies a permutation, in order to prevent attacks due to adversarially-generated public keys. We design and implement a simple and efficient noninteractive zero-knowledge protocol (in the random oracle model) for this task. The key feature of our protocol is compatibility with existing RSA implementations and standards. The protocol works for any choice of e. Applications concerned about adversarial key generation can just append our proof to the RSA public key without any other modifications to existing code or cryptographic libraries. Users need only perform a one- time verification of the proof to ensure that raising to the power e is a permutation of the integers modulo N . For typical parameter settings, the proof consists of nine integers modulo N; generating the proof and verifying it both require about nine modular exponentiations."
MICHAEL FISCHER,"Practice of streaming processing of dynamic graphs: concepts, models, and systems",
MICHAEL FISCHER,"Twenty questions about design behavior for sustainability, report of the International Expert Panel on behavioral science for design","How behavioral scientists, engineers, and architects can work together to advance how we all understand and practice design—in order to enhance sustainability in the built environment, and beyond."
JAMES UDEN,Reassessing the Gothic/Classical relationship,"This chapter examines in three stages the surprisingly vital place of the Classical literatures of Greece and Rome in the development of the Gothic. First, Horace Walpole and his contemporaries Edward Young and Richard Hurd irreverently reimagined Classical antiquity not as a model of propriety and decorum, but as a grotesque realm of monsters and ghosts. Second, Clara Reeve challenged the social prejudice that accorded prestige to the masculine zone of Classical texts but not to popular literature; The Old English Baron blends a Gothic narrative with motifs from Classical historiography in order to challenge the artificial hierarchy separating the two modes. Third, writers of the Romantic age presented Rome as a haunted city, recasting the influence of Greece and Rome in spectral terms. The Gothic, it shows, is no simple departure from the Classical. Rather, the tension between the two is sustained throughout the history of the genre as one of its basic elements, and we need to restore a sense of that tension in order to understand the full force of the Gothic in the literary and aesthetic consciousness of the long eighteenth century."
JAMES UDEN,"Horace Walpole, gothic classicism, and the aesthetics of collection","Scholars of eighteenth-century literature have long seen the development of the Gothic as a break from neoclassical aesthetics, but this article posits a more complex engagement with classical imitation at the origins of the genre. In Horace Walpole’s formative Gothic novel The Castle of Otranto, his Gothic drama The Mysterious Mother, and in the curiosities in his villa, classical elements are detached from their contexts and placed in startling and strange juxtapositions. His tendency towards the fragmentation of ancient culture, frequently expressed through the imagery of dismemberment, suggests an aesthetic not of imitation, but of collection. Moreover, rather than abandoning or ignoring the classical, Walpole reconfigures literary history to demonstrate elements of monstrosity and hybridity already present in Greek and Roman texts."
JAMES UDEN,Response: expert or intellectual? other views of legal and medical expertise,
JAMES UDEN,A crowd of Gods: atheism and superstition in Juvenal Satire 13,
JAMES UDEN,Quintilian and Juvenal's Fourteenth Satire,"This working paper explores Juvenal's fourteenth satire (published c. 127 CE), and its connections with Quintilian's monumental, first-century treatise on education, the 'Institutes of Oratory'."
JAMES UDEN,"The margins of satire: Suetonius, satura, and scholarly outsiders in ancient Rome","Scholars have long been interested in Suetonius' De Grammaticis et Rhetoribus for the evidence it preserves of the history of education and philology at Rome. This article focuses on a different aspect of the work: its repeated links with satire. Suetonius' grammatici are presented both as authors and targets of satirical attacks, and fragments of their work preserved in the De Grammaticis et Rhetoribus reveal a wider, sub-elite field of satirical writing occluded in the polished, literary genre of Roman satura. Through analysis of Suetonius' biographical vignettes and related passages in Juvenal's Satire 7, this article sheds light on a vision of grammatici as outsiders who critique Rome—and each other—from the social and literary margins."
JAMES UDEN,How we write plagues,
JAMES UDEN,Complex inferiorities: the poetics of the weaker voice in Latin literature,
JAMES UDEN,Gothic travel in Northanger Abbey,"This book chapter explores the relevance of travel literature for understanding Jane Austen's posthumously published novel 'Northanger Abbey'. The novel was shaped by the fashion for 'Gothic travel', a mode of travel discourse in the late 1790s that sought to rediscover historical locations in Britain as sources of exciting gloom and terror. The experiences of the central character in the novel have also been shaped by Austen's reading of contemporary travel literature in the period. Travel, it turns out, is central to understanding both the plot of the novel and its cultural context."
JAMES UDEN,Impersonating Priapus,"Whenever Catullus is sexually aggressive or brutally frank in his poetry, modern commentators will often call him ""Priapic,"" an adjective that tends to obscure rather than elucidate the various ways in which Catullus uses the figure of Priapus in crafting his poetic persona. This article attempts to read poems 47, 56, and, in particular, 16, as Catullus' experiments in the Greek and Roman subgenre of Priapic poetry. Once we see that these poems are focalized through the generic perspective of Priapus, Catullus' impersonation of Priapus becomes less an assumption of hyperphallic masculinity and more a witty way in which to lampoon a world-view dominated by an obsessive focus on penetration. Impersonating Priapus meant, in fact, exposing the garden god and his hopeless rusticity to urbane critique."
JAMES UDEN,The Vanishing Gardens of Priapus,
JAMES UDEN,The Elegiac Puella as Virgin Martyr,"This paper explores the ideological currents running through Maximianus's subversive revival of the genre of Augustan love elegy in the beleaguered Rome of the mid-sixth century. The third elegy narrates an apparent childhood reminiscence of the poet, a failed romance with a young girl, Aquilina. But it soon becomes clear that, in the character of Aquilina, Maximianus has deliberately blurred the literary archetypes of the elegiac puella and the virgin martyr from Christian hagiography. This bizarre configuration allows the elegist simultaneously to provoke questions about the representation of female figures in both genres. By likening the elegiac puella to the martyr, Maximianus highlights the latent violence of elegiac topoi. By likening the martyr to the elegiac puella, Maximianus highlights the eroticism that often has a prominent place in accounts of virgin martyrdom. Not merely a formal experiment or the product of Augustan nostalgia, Maximianus's elegies represent a real attempt to reinvent elegy's questioning stance in a new social and religious context."
JAMES UDEN,The Contest of Homer and Hesiod and the ambitions of Hadrian,"This article examines the compilation known as the Contest of Homer and Hesiod. More usually mined for the material it preserves from the sophist Alcidamas, here I advance a reading that seeks to make sense of the compi- lation as a whole and situates the work ideologically in its Imperial context. An anecdote early in the compilation depicts the emperor Hadrian enquiring about Homer’s birthplace and parents from the Delphic Oracle; he is told that Telemachus was Homer’s father and Ithaca his homeland. When the text says that we must believe this self-evidently absurd response on account of the status of the emperor, its author is satirizing Hadrian’s ambitions to participate in the Greek intellectual world and the pressures on scholars to accept Hadrian’s authority in their field. Moreover, the compiler has linked this anecdote to the long account of the poetic contest between Homer and Hesiod in order to draw an unflattering parallel between Hadrian and King Panedes, who, as writers such as Lucian and Dio Chrysostom suggested, exposed his ineptitude in choosing Hesiod over Homer as the victor of the contest."
JAMES UDEN,"Conserving the classical past: Elizabeth Carter, “On his Design of Cutting Down a Shady Walk” (1745)","Elizabeth Carter (1717-1806) was the most famous female classicist of the eighteenth century. This short essay focuses on a poem in which Carter protests the cutting down of a grove of trees. She inserts herself in a tradition of male classical figures whose thoughts were inspired by the environment, casting the natural grove as a gendered space for scholarly thought."
JAMES UDEN,Egnatius the Epicurean: the banalization of philosophy in Catullus,"This article offers a new examination of the place of philosophy in Catullus’ Carmina. It focuses on Egnatius, the ‘smiling Spaniard’ of poems 37 and 39, and argues that Catullus’ attacks on this character make use of many standard invective tropes against Epicureans in the late Republic. More than merely an opportunity to show off his whitened teeth, Egnatius’ smile may well have been proof of his philosophical detachment and ataraxia. Yet Catullus maliciously misrepresents this mark of Epicurean virtue as a social gaffe, and an unflattering reminder of Egnatius’ provincial origins. I then reinterpret poems 37, 38, and 39 as a poetic series unified by the ‘banalization’ of philosophical ideas. Ultimately, Catullus creates his own singular voice – the arbiter of style and taste – by representing aspects of other people's behaviour as trite and ordinary. To banalize is an act of power, and it is a weapon that Catullus wields to articulate a sense of difference from other poets and thinkers in his intellectual world."
JAMES UDEN,The noise-lovers: cultures of speech and sound in second-century Rome,"This chapter provides an examination of an ideal of the ‘deliberate speaker’, who aims to reflect time, thought, and study in his speech. In the Roman Empire, words became a vital tool for creating and defending in-groups, and orators and authors in both Latin and Greek alleged, by contrast, that their enemies produced babbling noise rather than articulate speech. In this chapter, the ideal of the deliberate speaker is explored through the works of two very different contemporaries: the African-born Roman orator Fronto and the Syrian Christian apologist Tatian. Despite moving in very different circles, Fronto and Tatian both express their identity and authority through an expertise in words, in strikingly similar ways. The chapter ends with a call for scholars of the Roman Empire to create categories of analysis that move across different cultural and linguistic groups. If we do not, we risk merely replicating the parochialism and insularity of our sources."
JAMES UDEN,Introduction: pluralized voices in women's travel writing,"This chapter surveys recent approaches to travel and mobility, women's writing, and the study of travel literature in languages beyond English."
JAMES UDEN,Reception,
HOWARD CABRAL,Associations between Maternal Thyroid Function in Pregnancy and Obstetric and Perinatal Outcomes - Supplemental Table 1,
HOWARD CABRAL,Automated MRI Measures Identify Individuals with Mild Cognitive Impairment and Alzheimer's Disease*,"Mild cognitive impairment can represent a transitional state between normal ageing and Alzheimer's disease. Non-invasive diagnostic methods are needed to identify mild cognitive impairment individuals for early therapeutic interventions. Our objective was to determine whether automated magnetic resonance imaging-based measures could identify mild cognitive impairment individuals with a high degree of accuracy. Baseline volumetric T1-weighted magnetic resonance imaging scans of 313 individuals from two independent cohorts were examined using automated software tools to identify the volume and mean thickness of 34 neuroanatomic regions. The first cohort included 49 older controls and 48 individuals with mild cognitive impairment, while the second cohort included 94 older controls and 57 mild cognitive impairment individuals. Sixty-five patients with probable Alzheimer's disease were also included for comparison. For the discrimination of mild cognitive impairment, entorhinal cortex thickness, hippocampal volume and supramarginal gyrus thickness demonstrated an area under the curve of 0.91 (specificity 94%, sensitivity 74%, positive likelihood ratio 12.12, negative likelihood ratio 0.29) for the first cohort and an area under the curve of 0.95 (specificity 91%, sensitivity 90%, positive likelihood ratio 10.0, negative likelihood ratio 0.11) for the second cohort. For the discrimination of Alzheimer's disease, these three measures demonstrated an area under the curve of 1.0. The three magnetic resonance imaging measures demonstrated significant correlations with clinical and neuropsychological assessments as well as with cerebrospinal fluid levels of tau, hyperphosphorylated tau and abeta 42 proteins. These results demonstrate that automated magnetic resonance imaging measures can serve as an in vivo surrogate for disease severity, underlying neuropathology and as a non-invasive diagnostic method for mild cognitive impairment and Alzheimer's disease."
HOWARD CABRAL,Selective Disruption of the Cerebral Neocortex in Alzheimer's Disease,"BACKGROUND. Alzheimer's disease (AD) and its transitional state mild cognitive impairment (MCI) are characterized by amyloid plaque and tau neurofibrillary tangle (NFT) deposition within the cerebral neocortex and neuronal loss within the hippocampal formation. However, the precise relationship between pathologic changes in neocortical regions and hippocampal atrophy is largely unknown. METHODOLOGY/PRINCIPAL FINDINGS. In this study, combining structural MRI scans and automated image analysis tools with reduced cerebrospinal fluid (CSF) Aß levels, a surrogate for intra-cranial amyloid plaques and elevated CSF phosphorylated tau (p-tau) levels, a surrogate for neocortical NFTs, we examined the relationship between the presence of Alzheimer's pathology, gray matter thickness of select neocortical regions, and hippocampal volume in cognitively normal older participants and individuals with MCI and AD (n=724). Amongst all 3 groups, only select heteromodal cortical regions significantly correlated with hippocampal volume. Amongst MCI and AD individuals, gray matter thickness of the entorhinal cortex and inferior temporal gyrus significantly predicted longitudinal hippocampal volume loss in both amyloid positive and p-tau positive individuals. Amongst cognitively normal older adults, thinning only within the medial portion of the orbital frontal cortex significantly differentiated amyloid positive from amyloid negative individuals whereas thinning only within the entorhinal cortex significantly discriminated p-tau positive from p-tau negative individuals. CONCLUSIONS/SIGNIFICANCE. Cortical Aß and tau pathology affects gray matter thinning within select neocortical regions and potentially contributes to downstream hippocampal degeneration. Neocortical Alzheimer's pathology is evident even amongst older asymptomatic individuals suggesting the existence of a preclinical phase of dementia."
HOWARD CABRAL,"Migraine, Fibromyalgia, and Depression among People with IBS: A Prevalence Study","BACKGROUND. Case descriptions suggest IBS patients are more likely to have other disorders, including migraine, fibromyalgia, and depression. We sought to examine the prevalence of these conditions in cohorts of people with and without IBS. METHODS. The source of data was a large U.S. health plan from January 1, 1996 though June 30, 2002. We identified all people with a medical claim associated with an ICD-9 code for IBS. A non-IBS cohort was a random sample of people with an ICD-9 code for routine medical care. In the cohorts, we identified all claims for migraine, depression, and fibromyalgia. We estimated the prevalence odds ratios (PORs) of each of the three conditions using the Mantel-Haenszel method. We conducted quantitative sensitivity analyses to quantify the impact of residual confounding and in differential outcome identification. RESULTS. We identified 97,593 people in the IBS cohort, and a random sample of 27,402 people to compose the non-IBS comparison cohort. With adjustment, there was a 60% higher odds in the IBS cohort of having any one of the three disorders relative to the comparison cohort (POR 1.6, 95% CI 1.5 – 1.7). There was a 40% higher odds of depression in the IBS cohort (POR 1.4, 95% CI 1.3 – 1.4). The PORs for fibromyalgia and migraine were similar (POR for fibromyalgia 1.8, 95% CI 1.7 – 1.9; POR for migraine 1.6, 95% CI 1.4 – 1.7). Differential prevalence of an unmeasured confounder, or imperfect sensitivity or specificity of outcome detection would have impacted the observed results. CONCLUSION. People in the IBS cohort had a 40% to 80% higher prevalence odds of migraine, fibromyalgia, and depression."
HOWARD CABRAL,Long-term yogurt consumption and risk of incident hypertension in adults,
HOWARD CABRAL,A web-based nutrition program reduces health care costs in employees with cardiac risk factors: before and after cost analysis,"BACKGROUND: Rising health insurance premiums represent a rapidly increasing burden on employer-sponsors of health insurance and their employees. Some employers have become proactive in managing health care costs by providing tools to encourage employees to directly manage their health and prevent disease. One example of such a tool is DASH for Health, an Internet-based nutrition and exercise behavior modification program. This program was offered as a free, opt-in benefit to US-based employees of the EMC Corporation. OBJECTIVE: The aim was to determine whether an employer-sponsored, Internet-based diet and exercise program has an effect on health care costs. METHODS. There were 15,237 total employees and spouses who were included in our analyses, of whom 1967 enrolled in the DASH for Health program (DASH participants). Using a retrospective, quasi-experimental design, study year health care costs among DASH participants and non-participants were compared, controlling for baseline year costs, risk, and demographic variables. The relationship between how often a subject visited the DASH website and health care costs also was examined. These relationships were examined among all study subjects and among a subgroup of 735 subjects with cardiovascular conditions (diabetes, hypertension, hyperlipidemia). Multiple linear regression analysis examined the relationship of program use to health care costs, comparing study year costs among DASH participants and non-participants and then examining the effects of increased website use on health care costs. Analyses were repeated among the cardiovascular condition subgroups. RESULTS: Overall, program use was not associated with changes in health care costs. However, among the cardiovascular risk study subjects, health care costs were US$827 lower, on average, during the study year (P = .05; t 729 = 1.95). Among 1028 program users, increased website use was significantly associated with lower health care costs among those who visited the website at least nine times during the study year (US$14 decrease per visit; P = .04; t 1022 = 2.05), with annual savings highest among 80 program users with targeted conditions (US$55 decrease per visit; P< .001; t 74 = 2.71). CONCLUSIONS: An employer-sponsored, Internet-based diet and exercise program shows promise as a low-cost benefit that contributes to lower health care costs among persons at higher risk for above-average health care costs and utilization."
HOWARD CABRAL,"Weight, Blood Pressure, and Dietary Benefits After 12 Months of a Web-based Nutrition Education Program (DASH for Health): Longitudinal Observational Study","BACKGROUND The dietary habits of Americans are creating serious health concerns, including obesity, hypertension, diabetes, cardiovascular disease, and even some types of cancer. While considerable attention has been focused on calorie reduction and weight loss, approaches are needed that will not only help the population reduce calorie intake but also consume the type of healthy, well-balanced diet that would prevent this array of medical complications. OBJECTIVE To design an Internet-based nutrition education program and to explore its effect on weight, blood pressure, and eating habits after 12 months of participation. METHODS. We designed the DASH for Health program to provide weekly articles about healthy nutrition via the Internet. Dietary advice was based on the DASH diet (Dietary Approaches to Stop Hypertension). The program was offered as a free benefit to the employees of EMC Corporation, and 2834 employees and spouses enrolled. Enrollees voluntarily entered information about themselves on the website (food intake), and we used these self-entered data to determine if the program had any effect. Analyses were based upon the change in weight, blood pressure, and food intake between the baseline period (before the DASH program began) and the 12th month. To be included in an outcome, a subject had to have provided both a baseline and 12th-month entry. RESULTS After 12 months, 735 of 2834 original enrollees (26%) were still actively using the program. For subjects who were overweight/obese (body mass index >25; n = 151), weight change at 12 months was -4.2 lbs (95% CI: -2.2, -6.2; P< .001). For subjects with hypertension or prehypertension at baseline (n = 62), systolic blood pressure fell 6.8 mmHg at 12 months (CI: -2.6, -11.0; P<.001; n = 62). Diastolic pressure fell 2.1 mmHg (P = .16). Based upon self-entered food surveys, enrollees (n = 181) at 12 months were eating significantly more fruits, more vegetables, and fewer grain products. They also reduced consumption of carbonated beverages. Enrollees who had visited the website more often tended to have greater blood pressure and weight loss effect, suggesting that use of the DASH for Health program was at least partially responsible for the benefits we observed. CONCLUSIONS We have found that continued use of a nutrition education program delivered totally via the Internet, with no person-to-person contact with health professionals, is associated with significant weight loss, blood pressure lowering, and dietary improvements after 12 months. Effective programs like DASH for Health, delivered via the Internet, can provide benefit to large numbers of subjects at low cost and may help address the nutritional public health crisis."
HOWARD CABRAL,Performance of mixed effects models in the analysis of mediated longitudinal data,"BACKGROUND: Linear mixed effects models (LMMs) are a common approach for analyzing longitudinal data in a variety of settings. Although LMMs may be applied to complex data structures, such as settings where mediators are present, it is unclear whether they perform well relative to methods for mediational analyses such as structural equation models (SEMs), which have obvious appeal in such settings. For some researchers, SEMs may be more difficult than LMMs to implement, e.g. due to lack of training in the methodology or the need for specialized SEM software. It therefore is of interest to evaluate whether the LMM performs sufficiently in a scenario particularly suitable for SEMs. We focus on evaluation of the total effect (i.e. direct and indirect) of an exposure on an outcome of interest when a mediating factor is present. Our aim is to explore whether the LMM performs as well as the SEM in a setting that is conducive to using the SEM. METHODS: We simulated mediated longitudinal data from an SEM where a binary, main independent variable has both direct and indirect effects on a continuous outcome. We conducted analyses with both the LMM and SEM to evaluate the performance of the LMM in a setting where the SEM is expected to be preferable. Models were evaluated with respect to bias, coverage probability and power. Sample size, effect size and error distribution of the simulated data were varied. RESULTS: Both models performed well in a range of settings. Marginal increases in power estimates were observed for the SEM, although generally there were no major differences in performance. Power for both models was good with a sample of size of 250 and a small to medium effect size. Bias did not substantially increase for either model when data were generated from distributions that were both skewed and kurtotic. CONCLUSIONS: In settings where the goal is to evaluate the overall effects, the LMM excluding mediating variables appears to have good performance with respect to power, bias and coverage probability relative to the SEM. The major benefit of SEMs is that it simultaneously and efficiently models both the direct and indirect effects of the mediation process."
AVRUM SPIRA,SIEGE: Smoking Induced Epithelial Gene Expression Database,"The SIEGE (Smoking Induced Epithelial Gene Expression) database is a clinical resource for compiling and analyzing gene expression data from epithelial cells of the human intra-thoracic airway. This database supports a translational research study whose goal is to profile the changes in airway gene expression that are induced by cigarette smoke. RNA is isolated from airway epithelium obtained at bronchoscopy from current-, former- and never-smoker subjects, and hybridized to Affymetrix HG-U133A Genechips, which measure the level of expression of ~22 500 human transcripts. The microarray data generated along with relevant patient information is uploaded to SIEGE by study administrators using the database's web interface, found at http://pulm.bumc.bu.edu/siegeDB. PERL-coded scripts integrated with SIEGE perform various quality control functions including the processing, filtering and formatting of stored data. The R statistical package is used to import database expression values and execute a number of statistical analyses including t-tests, correlation coefficients and hierarchical clustering. Values from all statistical analyses can be queried through CGI-based tools and web forms found on the �Search� section of the database website. Query results are embedded with graphical capabilities as well as with links to other databases containing valuable gene resources, including Entrez Gene, GO, Biocarta, GeneCards, dbSNP and the NCBI Map Viewer."
AVRUM SPIRA,Smoking-Induced Gene Expression Changes in the Bronchial Airway Are Reflected in Nasal and Buccal Epithelium,"BACKGROUND: Cigarette smoking is a leading cause of preventable death and a significant cause of lung cancer and chronic obstructive pulmonary disease. Prior studies have demonstrated that smoking creates a field of molecular injury throughout the airway epithelium exposed to cigarette smoke. We have previously characterized gene expression in the bronchial epithelium of never smokers and identified the gene expression changes that occur in the mainstem bronchus in response to smoking. In this study, we explored relationships in whole-genome gene expression between extrathorcic (buccal and nasal) and intrathoracic (bronchial) epithelium in healthy current and never smokers. RESULTS: Using genes that have been previously defined as being expressed in the bronchial airway of never smokers (the ""normal airway transcriptome""), we found that bronchial and nasal epithelium from non-smokers were most similar in gene expression whencompared to other epithelial and nonepithelial tissues, with several antioxidant, detoxification, and structural genes being highly expressed in both the bronchus and nose. Principle component analysis of previously defined smoking-induced genes from the bronchus suggested that smoking had a similar effect on gene expression in nasal epithelium. Gene set enrichment analysis demonstrated that this set of genes was also highly enriched among the genes most altered by smoking in both nasal and buccal epithelial samples. The expression of several detoxification genes was commonly altered by smoking in all three respiratory epithelial tissues, suggesting a common airway-wide response to tobacco exposure. CONCLUSION: Our findings support a relationship between gene expression in extra- and intrathoracic airway epithelial cells and extend the concept of a smoking-induced field of injury to epithelial cells that line the mouth and nose. This relationship could potentially be utilized to develop a non-invasive biomarker for tobacco exposure as well as a non-invasive screening or diagnostic tool providing information about individual susceptibility to smoking-induced lung diseases."
AVRUM SPIRA,Reversible and Permanent Effects of Tobacco Smoke Exposure on Airway Epithelial Gene Expression,"Oligonucleotide microarray analysis revealed 175 genes that are differentially expressed in large airway epithelial cells of people who currently smoke compared with those who never smoked, with 28 classified as irreversible, 6 as slowly reversible, and 139 as rapidly reversible. BACKGROUND. Tobacco use remains the leading preventable cause of death in the US. The risk of dying from smoking-related diseases remains elevated for former smokers years after quitting. The identification of irreversible effects of tobacco smoke on airway gene expression may provide insights into the causes of this elevated risk. RESULTS. Using oligonucleotide microarrays, we measured gene expression in large airway epithelial cells obtained via bronchoscopy from never, current, and former smokers (n = 104). Linear models identified 175 genes differentially expressed between current and never smokers, and classified these as irreversible (n = 28), slowly reversible (n = 6), or rapidly reversible (n = 139) based on their expression in former smokers. A greater percentage of irreversible and slowly reversible genes were down-regulated by smoking, suggesting possible mechanisms for persistent changes, such as allelic loss at 16q13. Similarities with airway epithelium gene expression changes caused by other environmental exposures suggest that common mechanisms are involved in the response to tobacco smoke. Finally, using irreversible genes, we built a biomarker of ever exposure to tobacco smoke capable of classifying an independent set of former and current smokers with 81% and 100% accuracy, respectively. CONCLUSION. We have categorized smoking-related changes in airway gene expression by their degree of reversibility upon smoking cessation. Our findings provide insights into the mechanisms leading to reversible and persistent effects of tobacco smoke that may explain former smokers increased risk for developing tobacco-induced lung disease and provide novel targets for chemoprophylaxis. Airway gene expression may also serve as a sensitive biomarker to identify individuals with past exposure to tobacco smoke."
AVRUM SPIRA,Comparison of Proteomic and Transcriptomic Profiles in the Bronchial Airway Epithelium of Current and Never Smokers,"BACKGROUND. Although prior studies have demonstrated a smoking-induced field of molecular injury throughout the lung and airway, the impact of smoking on the airway epithelial proteome and its relationship to smoking-related changes in the airway transcriptome are unclear. METHODOLOGY/PRINCIPAL FINDINGS. Airway epithelial cells were obtained from never (n=5) and current (n=5) smokers by brushing the mainstem bronchus. Proteins were separated by one dimensional polyacrylamide gel electrophoresis (1D-PAGE). After in-gel digestion, tryptic peptides were processed via liquid chromatography/ tandem mass spectrometry (LC-MS/MS) and proteins identified. RNA from the same samples was hybridized to HG-U133A microarrays. Protein detection was compared to RNA expression in the current study and a previously published airway dataset. The functional properties of many of the 197 proteins detected in a majority of never smokers were similar to those observed in the never smoker airway transcriptome. LC-MS/MS identified 23 proteins that differed between never and current smokers. Western blotting confirmed the smoking-related changes of PLUNC, P4HB1, and uteroglobin protein levels. Many of the proteins differentially detected between never and current smokers were also altered at the level of gene expression in this cohort and the prior airway transcriptome study. There was a strong association between protein detection and expression of its corresponding transcript within the same sample, with 86% of the proteins detected by LC-MS/MS having a detectable corresponding probeset by microarray in the same sample. Forty-one proteins identified by LC-MS/MS lacked detectable expression of a corresponding transcript and were detected in =5% of airway samples from a previously published dataset. CONCLUSIONS/SIGNIFICANCE. 1D-PAGE coupled with LC-MS/MS effectively profiled the airway epithelium proteome and identified proteins expressed at different levels as a result of cigarette smoke exposure. While there was a strong correlation between protein and transcript detection within the same sample, we also identified proteins whose corresponding transcripts were not detected by microarray. This noninvasive approach to proteomic profiling of airway epithelium may provide additional insights into the field of injury induced by tobacco exposure."
AVRUM SPIRA,Genetic Variation and Antioxidant Response Gene Expression in the Bronchial Airway Epithelium of Smokers at Risk for Lung Cancer,"Prior microarray studies of smokers at high risk for lung cancer have demonstrated that heterogeneity in bronchial airway epithelial cell gene expression response to smoking can serve as an early diagnostic biomarker for lung cancer. As a first step in applying functional genomic analysis to population studies, we have examined the relationship between gene expression variation and genetic variation in a central molecular pathway (NRF2-mediated antioxidant response) associated with smoking exposure and lung cancer. We assessed global gene expression in histologically normal airway epithelial cells obtained at bronchoscopy from smokers who developed lung cancer (SC, n=20), smokers without lung cancer (SNC, n=24), and never smokers (NS, n=8). Functional enrichment analysis showed that the NRF2-mediated, antioxidant response element (ARE)-regulated genes, were significantly lower in SC, when compared with expression levels in SNC. Importantly, we found that the expression of MAFG (a binding partner of NRF2) was correlated with the expression of ARE genes, suggesting MAFG levels may limit target gene induction. Bioinformatically we identified single nucleotide polymorphisms (SNPs) in putative ARE genes and to test the impact of genetic variation, we genotyped these putative regulatory SNPs and other tag SNPs in selected NRF2 pathway genes. Sequencing MAFG locus, we identified 30 novel SNPs and two were associated with either gene expression or lung cancer status among smokers. This work demonstrates an analysis approach that integrates bioinformatics pathway and transcription factor binding site analysis with genotype, gene expression and disease status to identify SNPs that may be associated with individual differences in gene expression and/or cancer status in smokers. These polymorphisms might ultimately contribute to lung cancer risk via their effect on the airway gene expression response to tobacco-smoke exposure."
AVRUM SPIRA,SIEGE: Smoking Induced Epithelial Gene Expression Database,"The SIEGE (Smoking Induced Epithelial Gene Expression) database is a clinical resource for compiling and analyzing gene expression data from epithelial cells of the human intra-thoracic airway. This database supports a translational research study whose goal is to profile the changes in airway gene expression that are induced by cigarette smoke. RNA is isolated from airway epithelium obtained at bronchoscopy from current-, former- and never-smoker subjects, and hybridized to Affymetrix HG-U133A Genechips, which measure the level of expression of ^∼22500 human transcripts. The microarray data generated along with relevant patient information is uploaded to SIEGE by study administrators using the database's web interface, found at http://pulm.bumc.bu.edu/siegeDB. PERL-coded scripts integrated with SIEGE perform various quality control functions including the processing, filtering and formatting of stored data. The R statistical package is used to import database expression values and execute a number of statistical analyses including t-tests, correlation coefficients and hierarchical clustering. Values from all statistical analyses can be queried through CGI-based tools and web forms found on the 'Search' section of the database website. Query results are embedded with graphical capabilities as well as with links to other databases containing valuable gene resources, including Entrez Gene, GO, Biocarta, GeneCards, dbSNP and the NCBI Map Viewer."
AVRUM SPIRA,Alterations in gene expression in T1α null lung: a model of deficient alveolar sac development,"BACKGROUND. Development of lung alveolar sacs of normal structure and size at late gestation is necessary for the gas exchange process that sustains respiration at birth. Mice lacking the lung differentiation gene T1α [T1α(-/-)] fail to form expanded alveolar sacs, resulting in respiratory failure at birth. Since little is known about the molecular pathways driving alveolar sacculation, we used expression microarrays to identify genes altered in the abnormal lungs and, by inference, may play roles in normal lung morphogenesis. RESULTS. Altered expression of genes related to cell-cell interaction, such as ephrinA3, are observed in T1α(-/-) at E18.5. At term, FosB, Egr1, MPK-1 and Nur77, which can function as negative regulators of the cell-cycle, are down-regulated. This is consistent with the hyperproliferation of peripheral lung cells in term T1α (-/-) lungs reported earlier. Biochemical assays show that neither PCNA nor p21 are altered at E18.5. At term in contrast, PCNA is increased, and p21 is decreased. CONCLUSION. This global analysis has identified a number of candidate genes that are significantly altered in lungs in which sacculation is abnormal. Many genes identified were not previously associated with lung development and may participate in formation of alveolar sacs prenatally."
EDWARD KEARNS,Calorimetry for low-energy electrons using charge and light in liquid argon,"Precise calorimetric reconstruction of 5–50 MeV electrons in liquid argon time projection chambers (LArTPCs) will enable the study of astrophysical neutrinos in DUNE and could enhance the physics reach of oscillation analyses. Liquid argon scintillation light has the potential to improve energy reconstruction for low-energy electrons over charge-based measurements alone. Here we demonstrate light-augmented calorimetry for low-energy electrons in a single-phase LArTPC using a sample of Michel electrons from decays of stopping cosmic muons in the LArIAT experiment at Fermilab. Michel electron energy spectra are reconstructed using both a traditional charge-based approach as well as a more holistic approach that incorporates both charge and light. A maximum-likelihood fitter, using LArIAT’s well-tuned simulation, is developed for combining these quantities to achieve optimal energy resolution. A sample of isolated electrons is simulated to better determine the energy resolution expected for astrophysical electron-neutrino charged-current interaction final states. In LArIAT, which has very low wire noise and an average light yield of 18  pe/MeV, an energy resolution of σ/E≃9.3%/√E⊕1.3% is achieved. Samples are then generated with varying wire noise levels and light yields to gauge the impact of light-augmented calorimetry in larger LArTPCs. At a charge-readout signal-to-noise of S/N≃30, for example, the energy resolution for electrons below 40 MeV is improved by ≈10%, ≈20%, and ≈40% over charge-only calorimetry for average light yields of 10  pe/MeV, 20  pe/MeV, and 100  pe/MeV, respectively."
EDWARD KEARNS,Search for astrophysical electron antineutrinos in Super-Kamiokande with 0.01% gadolinium-loaded water,"We report the first search result for the flux of astrophysical electron antineutrinos for energies 𝒪(10) MeV in the gadolinium-loaded Super-Kamiokande (SK) detector. In 2020 June, gadolinium was introduced to the ultrapure water of the SK detector in order to detect neutrons more efficiently. In this new experimental phase, SK-Gd, we can search for electron antineutrinos via inverse beta decay with efficient background rejection thanks to the high efficiency of the neutron tagging technique. In this paper, we report the result for the initial stage of SK-Gd, during 2020 August 26, and 2022 June 1 with a 22.5 × 552 kton · day exposure at 0.01% Gd mass concentration. No significant excess over the expected background in the observed events is found for the neutrino energies below 31.3 MeV. Thus, the flux upper limits are placed at the 90% confidence level. The limits and sensitivities are already comparable with the previous SK result with pure water (22.5 × 2970 kton · day) owing to the enhanced neutron tagging. Operation with Gd increased to 0.03% started in 2022 June."
EDWARD KEARNS,Sensitivity of Super-Kamiokande with Gadolinium to Low Energy Antineutrinos from Pre-supernova Emission,"Supernova detection is a major objective of the Super-Kamiokande (SK) experiment. In the next stage of SK (SK-Gd), gadolinium (Gd) sulfate will be added to the detector, which will improve the ability of the detector to identify neutrons. A core-collapse supernova (CCSN) will be preceded by an increasing flux of neutrinos and antineutrinos, from thermal and weak nuclear processes in the star, over a timescale of hours; some of which may be detected at SK-Gd. This could provide an early warning of an imminent CCSN, hours earlier than the detection of the neutrinos from core collapse. Electron antineutrino detection will rely on inverse beta decay events below the usual analysis energy threshold of SK, so Gd loading is vital to reduce backgrounds while maximizing detection efficiency. Assuming normal neutrino mass ordering, more than 200 events could be detected in the final 12 hr before core collapse for a 15–25 solar mass star at around 200 pc, which is representative of the nearest red supergiant to Earth, α-Ori (Betelgeuse). At a statistical false alarm rate of 1 per century, detection could be up to 10 hr before core collapse, and a pre-supernova star could be detected by SK-Gd up to 600 pc away. A pre-supernova alert could be provided to the astrophysics community following gadolinium loading."
EDWARD KEARNS,Search for astronomical neutrinos from blazar TXS 0506+056 in Super-Kamiokande,"We report a search for astronomical neutrinos in the energy region from several GeV to TeV in the direction of the blazar TXS 0506+056 using the Super-Kamiokande detector following the detection of a 100 TeV neutrinos from the same location by the IceCube collaboration. Using Super-Kamiokande neutrino data across several data samples observed from 1996 April to 2018 February we have searched for both a total excess above known backgrounds across the entire period as well as localized excesses on smaller timescales in that interval. No significant excess nor significant variation in the observed event rate are found in the blazar direction. Upper limits are placed on the electron- and muon-neutrino fluxes at the 90% confidence level as 6.0 × 10^−7 and 4.5 × 10^−7–9.3 × 10^−10 [erg cm^−2 s^−1], respectively."
EDWARD KEARNS,Search for heavy neutrinos with the T2K near detector ND280,"This paper reports on the search for heavy neutrinos with masses in the range 140<MN<493  MeV/c^2 using the off-axis near detector ND280 of the T2K experiment. These particles can be produced from kaon decays in the standard neutrino beam and then subsequently decay in ND280. The decay modes under consideration are N→ℓ±απ∓ and N→ℓ+αℓ−β(−)ν(α,β=e,μ). A search for such events has been made using the Time Projection Chambers of ND280, where the background has been reduced to less than two events in the current dataset in all channels. No excess has been observed in the signal region. A combined Bayesian statistical approach has been applied to extract upper limits on the mixing elements of heavy neutrinos to electron-, muon- and tau- flavored currents (U^2e, U^2μ, U^2τ) as a function of the heavy neutrino mass, e.g., U^2e<10−9 at 90% C.L. for a mass of 390  MeV/c^2. These constraints are competitive with previous experiments."
EDWARD KEARNS,Measurement of the cosmogenic neutron yield in Super-Kamiokande with gadolinium loaded water,
EDWARD KEARNS,Prospects for beyond the standard model physics searches at the deep underground neutrino experiment: DUNE collaboration,"The Deep Underground Neutrino Experiment (DUNE) will be a powerful tool for a variety of physics topics. The high-intensity proton beams provide a large neutrino flux, sampled by a near detector system consisting of a combination of capable precision detectors, and by the massive far detector system located deep underground. This configuration sets up DUNE as a machine for discovery, as it enables opportunities not only to perform precision neutrino measurements that may uncover deviations from the present three-flavor mixing paradigm, but also to discover new particles and unveil new interactions and symmetries beyond those predicted in the Standard Model (SM). Of the many potential beyond the Standard Model (BSM) topics DUNE will probe, this paper presents a selection of studies quantifying DUNE's sensitivities to sterile neutrino mixing, heavy neutral leptons, non-standard interactions, CPT symmetry violation, Lorentz invariance violation, neutrino trident production, dark matter from both beam induced and cosmogenic sources, baryon number violation, and other new physics topics that complement those at high-energy colliders and significantly extend the present reach."
EDWARD KEARNS,Identification and reconstruction of low-energy electrons in the ProtoDUNE-SP detector,
EDWARD KEARNS,The long-baseline neutrino experiment: exploring fundamental symmetries of the universe,"The preponderance of matter over antimatter in the early Universe, the dynamics of the supernova bursts that produced the heavy elements necessary for life and whether protons eventually decay --- these mysteries at the forefront of particle physics and astrophysics are key to understanding the early evolution of our Universe, its current state and its eventual fate. The Long-Baseline Neutrino Experiment (LBNE) represents an extensively developed plan for a world-class experiment dedicated to addressing these questions. LBNE is conceived around three central components: (1) a new, high-intensity neutrino source generated from a megawatt-class proton accelerator at Fermi National Accelerator Laboratory, (2) a near neutrino detector just downstream of the source, and (3) a massive liquid argon time-projection chamber deployed as a far detector deep underground at the Sanford Underground Research Facility. This facility, located at the site of the former Homestake Mine in Lead, South Dakota, is approximately 1,300 km from the neutrino source at Fermilab -- a distance (baseline) that delivers optimal sensitivity to neutrino charge-parity symmetry violation and mass ordering effects. This ambitious yet cost-effective design incorporates scalability and flexibility and can accommodate a variety of upgrades and contributions. With its exceptional combination of experimental configuration, technical capabilities, and potential for transformative discoveries, LBNE promises to be a vital facility for the field of particle physics worldwide, providing physicists from around the globe with opportunities to collaborate in a twenty to thirty year program of exciting science. In this document we provide a comprehensive overview of LBNE's scientific objectives, its place in the landscape of neutrino physics worldwide, the technologies it will incorporate and the capabilities it will possess."
EDWARD KEARNS,First measurement of muon neutrino charged-current interactions on hydrocarbon without pions in the final state using multiple detectors with correlated energy spectra at T2K,
EDWARD KEARNS,Multiple muon measurements with MACRO,"The MACRO experiment at Gran Sasso provides means for detailed studies of multiple coincident penetrating cosmic ray muons. In this paper we concentrate on the studies of the ultrahigh energy primary cosmic ray composition using muon bundle multiplicities, muon pair lateral and angular separation distributions."
EDWARD KEARNS,Calibration of Super-Kamiokande using an electron LINAC,"In order to calibrate the Super-Kamiokande experiment for solar neutrino measurements, a linear accelerator (LINAC) for electrons was installed at the detector. LINAC data were taken at various positions in the detector volume, tracking the detector response in the variables relevant to solar neutrino analysis. In particular, the absolute energy scale is now known with less than 1% uncertainty."
LESLIE BODEN,"Workers' compensation in the United States: high costs, low benefits","Studies suggest that income replacement is low for many workers with serious occupational injuries and illnesses. This review discusses three areas that hold promise for raising benefits to workers while reducing workers' compensation costs to employers: improving safety, containing medical costs, and reducing litigation. In theory, workers' compensation increases the costs to employers of injuries and so provides incentives to improve safety. Yet, taken as a whole, research does not provide convincing evidence that workers' compensation reduces injury rates. Moreover, unlike safety and health regulation, workers' compensation focuses the attention of employers on individual workers. High costs may lead employers to discourage claims and litigate when claims are filed. Controlling medical costs can reduce workers' compensation costs. Most studies, however, have focused on costs and have not addressed the effectiveness of medical care or patient satisfaction. Research also has shown that workers' compensation systems can reduce the need for litigation. Without litigation, benefits can be delivered more quickly and at lower costs."
LESLIE BODEN,Litigation-Generated Science: Why Should We Care?,"BACKGROUND: In a 1994 Ninth Circuit decision on the remand of Daubert v. Merrell Dow Pharmaceuticals, Inc., Judge Alex Kosinski wrote that science done for the purpose of litigation should be subject to more stringent standards of admissibility than other science. OBJECTIVES: We analyze this proposition by considering litigation-generated science as a subset of science involving conflict of interest. DISCUSSION: Judge Kosinski's formulation suggests there may be reasons to treat science involving conflict of interest differently but raises questions about whether litigation-generated science should be singled out. In particular we discuss the similar problems raised by strategically motivated science done in anticipation of possible future litigation or otherwise designed to benefit the sponsor and ask what special treatment, if any, should be given to science undertaken to support existing or potential future litigation. CONCLUSION: The problems with litigation-generated science are not special. On the contrary, they are very general and apply to much or most science that is relevant and reliable in the courtroom setting."
LESLIE BODEN,Understanding the hospital sharps injury reporting pathway,"INTRODUCTION: Patient-care workers are frequently exposed to sharps injuries, which can involve the risk of serious illness. Underreporting of these injuries can compromise prevention efforts. MATERIALS AND METHODS: We linked survey responses of 1572 non-physician patient-care workers with the Occupational Health Services (OHS) database at two academic hospitals. We determined whether survey respondents who said they had sharps injuries indicated that they had reported them and whether reported injuries were recorded in the OHS database. RESULTS: Respondents said that they reported 62 of 78 sharps injuries occurring over a 12-month period. Only 28 appeared in the OHS data. Safety practices were positively associated with respondents’ saying they reported sharps injuries but not with whether reported injuries appeared in the OHS data. CONCLUSIONS: Administrators should consider creating reporting mechanisms that are simpler and more direct. Administrators and researchers should attempt to understand how incidents might be lost before they are recorded."
LESLIE BODEN,The Impact of Contract Operations on Safety in Underground Coal Mines,"OBJECTIVE: The aim of this study was to test for differences in injury rates for contractor-operated underground coal mines relative to owner-operated mines in Kentucky, controlling for other covariates. METHODS: We used disparities between MSHA contractor data and surface reclamation permit data to identify mines operated by contractors. We then used negative binomial regression to estimate injury rates from 1999 to 2013, controlling for mine and controller characteristics available from MSHA and the Energy Information Administration (EIA). RESULTS: Contractor-operated mines with 15 or fewer full-time equivalent workers (FTEs) had a statistically significant 57% higher covariate-adjusted reported traumatic injury rate than similar mines without contract operators. Larger contractor-operated mines did not have a statistically significant elevated rate. CONCLUSIONS: We detected a significant elevation of traumatic injury rates only among the smallest contractor-operated mines. This increase appears substantial enough to warrant attention."
LESLIE BODEN,Litigation-Generated Science: Why Should We Care?,"BACKGROUND. In a 1994 Ninth Circuit decision on the remand of Daubert v. Merrell Dow Pharmaceuticals, Inc., Judge Alex Kosinski wrote that science done for the purpose of litigation should be subject to more stringent standards of admissibility than other science. OBJECTIVES. We analyze this proposition by considering litigation-generated science as a subset of science involving conflict of interest. DISCUSSION. Judge Kosinski's formulation suggests there may be reasons to treat science involving conflict of interest differently but raises questions about whether litigation-generated science should be singled out. In particular we discuss the similar problems raised by strategically motivated science done in anticipation of possible future litigation or otherwise designed to benefit the sponsor and ask what special treatment, if any, should be given to science undertaken to support existing or potential future litigation. CONCLUSION. The problems with litigation-generated science are not special. On the contrary, they are very general and apply to much or most science that is relevant and reliable in the courtroom setting."
ROBERT PINSKY,Poetry and politics: poetry and empire,
ROBERT PINSKY,"Bostonia: 2000-2001, no. 1-4",
WEIMING XIA,Small molecule amyloid-beta protein precursor processing modulators lower amyloid-beta peptide levels via cKit signaling,"Alzheimer’s disease (AD) is characterized by the accumulation of neurotoxic amyloid-β (Aβ) peptides consisting of 39-43 amino acids, proteolytically derived fragments of the amyloid-β protein precursor (AβPP), and the accumulation of the hyperphosphorylated microtubule-associated protein tau. Inhibiting Aβ production may reduce neurodegeneration and cognitive dysfunction associated with AD. We have previously used an AβPP-firefly luciferase enzyme complementation assay to conduct a high throughput screen of a compound library for inhibitors of AβPP dimerization, and identified a compound that reduces Aβ levels. In the present study, we have identified an analog, compound Y10, which also reduced Aβ. Initial kinase profiling assays identified the receptor tyrosine kinase cKit as a putative Y10 target. To elucidate the precise mechanism involved, AβPP phosphorylation was examined by IP-western blotting. We found that Y10 inhibits cKit phosphorylation and increases AβPP phosphorylation mainly on tyrosine residue Y743, according to AβPP751 numbering. A known cKit inhibitor and siRNA specific to cKit were also found to increase AβPP phosphorylation and lower Aβ levels. We also investigated a cKit downstream signaling molecule, the Shp2 phosphatase, and found that known Shp2 inhibitors and siRNA specific to Shp2 also increase AβPP phosphorylation, suggesting that the cKit signaling pathway is also involved in AβPP phosphorylation and Aβ production. We further found that inhibitors of both cKit and Shp2 enhance AβPP surface localization. Thus, regulation of AβPP phosphorylation by small molecules should be considered as a novel therapeutic intervention for AD."
DAVID COKER,Asymmetric synthesis of griffipavixanthone employing a chiral phosphoric acid-catalyzed cycloaddition,"Asymmetric synthesis of the biologically active xanthone dimer griffipavixanthone is reported along with its absolute stereochemistry determination. Synthesis of the natural product is accomplished via dimerization of a p-quinone methide using a chiral phosphoric acid catalyst to afford a protected precursor in excellent diastereo- and enantioselectivity. Mechanistic studies, including an unbiased computational investigation of chiral ion-pairs using parallel tempering, were performed in order to probe the mode of asymmetric induction."
DAVID COKER,Transport properties of pristine few-layer black phosphorus by van der Waals passivation in an inert atmosphere,"Ultrathin black phosphorus is a two-dimensional semiconductor with a sizeable band gap. Its excellent electronic properties make it attractive for applications in transistor, logic and optoelectronic devices. However, it is also the first widely investigated two-dimensional material to undergo degradation upon exposure to ambient air. Therefore a passivation method is required to study the intrinsic material properties, understand how oxidation affects the physical properties and enable applications of phosphorene. Here we demonstrate that atomically thin graphene and hexagonal boron nitride can be used for passivation of ultrathin black phosphorus. We report that few-layer pristine black phosphorus channels passivated in an inert gas environment, without any prior exposure to air, exhibit greatly improved n-type charge transport resulting in symmetric electron and hole transconductance characteristics."
DAVID COKER,Quantum biology revisited,"Photosynthesis is a highly optimized process from which valuable lessons can be learned about the operating principles in nature. Its primary steps involve energy transport operating near theoretical quantum limits in efficiency. Recently, extensive research was motivated by the hypothesis that nature used quantum coherences to direct energy transfer. This body of work, a cornerstone for the field of quantum biology, rests on the interpretation of small-amplitude oscillations in two-dimensional electronic spectra of photosynthetic complexes. This Review discusses recent work reexamining these claims and demonstrates that interexciton coherences are too short lived to have any functional significance in photosynthetic energy transfer. Instead, the observed long-lived coherences originate from impulsively excited vibrations, generally observed in femtosecond spectroscopy. These efforts, collectively, lead to a more detailed understanding of the quantum aspects of dissipation. Nature, rather than trying to avoid dissipation, exploits it via engineering of exciton-bath interaction to create efficient energy flow."
DAVID COKER,The eighteenth data release of the Sloan Digital Sky Surveys: targeting and first spectra from SDSS-V,"The eighteenth data release (DR18) of the Sloan Digital Sky Survey (SDSS) is the first one for SDSS-V, the fifth generation of the survey. SDSS-V comprises three primary scientific programs or “Mappers”: the Milky Way Mapper (MWM), the Black Hole Mapper (BHM), and the Local Volume Mapper. This data release contains extensive targeting information for the two multiobject spectroscopy programs (MWM and BHM), including input catalogs and selection functions for their numerous scientific objectives. We describe the production of the targeting databases and their calibration and scientifically focused components. DR18 also includes ∼25,000 new SDSS spectra and supplemental information for X-ray sources identified by eROSITA in its eFEDS field. We present updates to some of the SDSS software pipelines and preview changes anticipated for DR19. We also describe three value-added catalogs (VACs) based on SDSS-IV data that have been published since DR17, and one VAC based on the SDSS-V data in the eFEDS field."
DAVID COKER,Iterative Linearized Density Matrix Propagation for Modeling Coherent Energy Transfer in Photosynthetic Light Harvesting,"We present results of calculations [1] that employ a new mixed quantum classical iterative density matrix propagation approach (ILDM , or so called Is‐Landmap) [2] to explore the survival of coherence in different photo synthetic models. Our model studies confirm the long lived quantum coherence , while conventional theoretical tools (such as Redfield equation) fail to describe these phenomenon [3,4]. Our ILDM method is a numerical exactly propagation scheme and can be served as a bench mark calculation tools[2]. Result get from ILDM and from other recent methods have been compared and show agreement with each other[4,5]. Long lived coherence plateau has been attribute to the shift of harmonic potential due to the system bath interaction, and the harvesting efficiency is a balance between the coherence and dissipation[1]. We use this approach to investigate the excitation energy transfer dynamics in various light harvesting complex include Fenna‐Matthews‐Olsen light harvesting complex[1] and Cryptophyte Phycocyanin 645 [6]. [1] P.Huo and D.F.Coker ,J. Chem. Phys. 133, 184108 (2010) . [2] E.R. Dunkel, S. Bonella, and D.F. Coker, J. Chem. Phys. 129, 114106 (2008). [3] A. Ishizaki and G.R. Fleming, J. Chem. Phys. 130, 234111 (2009). [4] A. Ishizaki and G.R. Fleming, Proc. Natl. Acad. Sci. 106, 17255 (2009). [5] G. Tao and W.H. Miller, J. Phys. Chem. Lett. 1, 891 (2010). [6] P.Huo and D.F.Coker in preparation"
DAVID COKER,XXVI IUPAP Conference on Computational Physics (CCP2014),"The 26th IUPAP Conference on Computational Physics, CCP2014, was held in Boston, Massachusetts, during August 11-14, 2014. Almost 400 participants from 38 countries convened at the George Sherman Union at Boston University for four days of plenary and parallel sessions spanning a broad range of topics in computational physics and related areas. The first meeting in the series that developed into the annual Conference on Computational Physics (CCP) was held in 1989, also on the campus of Boston University and chaired by our colleague Claudio Rebbi. The express purpose of that meeting was to discuss the progress, opportunities and challenges of common interest to physicists engaged in computational research. The conference having returned to the site of its inception, it is interesting to recect on the development of the field during the intervening years. Though 25 years is a short time for mankind, computational physics has taken giant leaps during these years, not only because of the enormous increases in computer power but especially because of the development of new methods and algorithms, and the growing awareness of the opportunities the new technologies and methods can offer. Computational physics now represents a ''third leg'' of research alongside analytical theory and experiments in almost all subfields of physics, and because of this there is also increasing specialization within the community of computational physicists. It is therefore a challenge to organize a meeting such as CCP, which must have suffcient depth in different areas to hold the interest of experts while at the same time being broad and accessible. Still, at a time when computational research continues to gain in importance, the CCP series is critical in the way it fosters cross-fertilization among fields, with many participants specifically attending in order to get exposure to new methods in fields outside their own. As organizers and editors of these Proceedings, we are very pleased with the high quality of the papers provided by the participants. These articles represent a good cross-section of what was presented at the meeting, and it is our hope that they will not only be useful individually for their specific scientific content but will also represent a historical snapshot of the state of computational physics that they represent collectively. The remainder of this Preface contains lists detailing the organizational structure of CCP2014, endorsers and sponsors of the meeting, plenary and invited talks, and a presentation of the 2014 IUPAP C20 Young Scientist Prize. We would like to take the opportunity to again thank all those who contributed to the success of CCP214, as organizers, sponsors, presenters, exhibitors, and participants. Anders Sandvik, David Campbell, David Coker, Ying Tang"
JAMES KATZ,"The role of religion in the longer-range future, April 6, 7, and 8, 2006","The conference brought together some 40 experts from various disciplines to ponder upon the “great dilemma” of how science, religion, and the human future interact. In particular, different panels looked at trends in what is happening to religion around the world, questions about how religion is impacting the current political and economic order, and how the social dynamics unleashed by science and by religion can be reconciled."
JAMES KATZ,Personal power and agency when dealing with interactive voice response systems and alternative modalities,"In summer 2015, we conducted an exploratory study of how people in the U.S. use and respond to robot-like systems in order to achieve their needs through mediated customer service interfaces. To understand this process, we carried out three focus groups sessions along with 50 in-depth interviews. Strikingly we found that people perceive (correctly or not) that interactive voice response customer service technology is set up to deter them from pursuing further contact. And yet, for the most part, people were unwilling to simply give up on the goals that motivated their initial contact. Consequently, they had to innovate ways to communicate with the automated systems that essentially serve as gatekeepers to their desired ends. These results have implications for communication theory and system design, especially since these systems will be increasingly presented to consumers as social media affordances evolve."
JAMES KATZ,Commentary on news and participation through and beyond proprietary platforms in an age of social media,"The far-seeing collection in this issue is arrayed across the terrain of journalism infused with social media. The authors take deep dives into the material and in the process contribute significantly to the research community’s corpus on social media and proprietary platforms in journalism. In their wake, they leave an ambitious albeit hazy roster of research topics. My aim is to offer a brief critique of the articles and conclude with a few hortatory words."
JAMES KATZ,The Physician Clinical Support System-Buprenorphine (PCSS-B): A Novel Project to Expand/Improve Buprenorphine Treatment,"Opioid dependence is largely an undertreated medical condition in the United States. The introduction of buprenorphine has created the potential to expand access to and use of opioid agonist treatment in generalist settings. Physicians, however, often have limited training and experience providing this type of care. Some physicians believe having a mentoring relationship with an experienced provider during their initial introduction to the use of buprenorphine would ease implementation. Our goal was to describe the development, implementation, resources, and evaluation of the Physician Clinical Support System-Buprenorphine (PCSS-B), a federally funded program to improve access to and quality of treatment with buprenorphine. We provide a description of the PCSS-B, a national network of 88 trained physician mentors with expertise in buprenorphine treatment and skills in clinical education. We provide information regarding the use the PCSS-B core services including telephone, email and in-person support, a website, clinical guidances, a warmline and outreach to primary care and specialty organizations. Between July 2005 and July 2009, 67 mentors and 4 clinical experts reported providing mentoring services to 632 participants in 48 states, Washington DC and Puerto Rico. A total of 1,455 contacts were provided through email (45%), telephone (34%) and in-person visits (20%). Seventy-six percent of contacts addressed a clinical issue. Eighteen percent of contacts addressed a logistical issue. The number of contacts per participant ranged from 1–125. Between August 2005 and April 2009 there were 72,822 visits to the PCSS-B website with 179,678 pages viewed. Seven guidances were downloaded more than 1000 times. The warmline averaged more than 100 calls per month. The PCSS-B model provides support for a mentorship program to assist non-specialty physicians in the provision of buprenorphine and may serve as a model for dissemination of other types of care."
JAMES KATZ,Opening education through emerging technology: what are the prospects? Public perceptions of artificial intelligence and virtual reality in the classroom,"Education technology (Edtech) is a booming industry based on its potential to transform education and learning outcomes. With concern over remote learning, there is renewed excitement about the visual component of Edtech, namely VR, along with artificial intelligence (AI), resulting in more significant investments and innovations. Despite industrial-scale investment in Edtech's diffusion, less is known about the public's view. The public's reception of these technologies, though, maybe necessary in determining the contours of their eventual utilization. Therefore, we conducted a mixed-methods analysis based on a survey of a representative sample of the US population (N=2,254) that explores perceptions of Edtech in two instantiations: AI and VR in education. Respondents were more accepting of VR as a teaching tool than AI taking on educational roles. Assistive AI was born over AI with decision-making responsibilities. Personality and experiential traits had an influence on respondents' openness to education technologies. The results suggest support for a blended model of AI and VR use in the classroom."
JAMES KATZ,Mediated communication and customer service experiences,"People around the world who seek to interact with large organisations increasingly find they must do so via mediated and automated communication. Organisations often deploy both mediated and automated platforms, such as instant messaging and interactive voice response systems (IVRs), for efficiency and cost-savings. Customer and client responses to these systems range from delight to frustration. To better understand the factors affecting people's satisfaction with these systems, we conducted a representative U.S. national survey (N = 1321). We found that people overwhelmingly like and trust in-person customer service compared to mediated and automated modalities. As to demographic attitude predictors, age was important (older respondents liked mediated systems less), but income and education were not strong attitude predictors. For personality variables, innovativeness was positively associated with mediated system satisfaction. However, communication apprehensiveness, which we expected to be related to satisfaction, was not. We conclude by discussing implications for the burgeoning field of human-machine communication, as well as social policy, equity, and the pullulating digital services divide."
JAMES KATZ,Broadband multi-wavelength properties of M87 during the 2017 Event Horizon Telescope campaign,"In 2017, the Event Horizon Telescope (EHT) Collaboration succeeded in capturing the first direct image of the center of the M87 galaxy. The asymmetric ring morphology and size are consistent with theoretical expectations for a weakly accreting supermassive black hole of mass ∼6.5 × 109 M ⊙. The EHTC also partnered with several international facilities in space and on the ground, to arrange an extensive, quasi-simultaneous multi-wavelength campaign. This Letter presents the results and analysis of this campaign, as well as the multi-wavelength data as a legacy data repository. We captured M87 in a historically low state, and the core flux dominates over HST-1 at high energies, making it possible to combine core flux constraints with the more spatially precise very long baseline interferometry data. We present the most complete simultaneous multi-wavelength spectrum of the active nucleus to date, and discuss the complexity and caveats of combining data from different spatial scales into one broadband spectrum. We apply two heuristic, isotropic leptonic single-zone models to provide insight into the basic source properties, but conclude that a structured jet is necessary to explain M87’s spectrum. We can exclude that the simultaneous γ-ray emission is produced via inverse Compton emission in the same region producing the EHT mm-band emission, and further conclude that the γ-rays can only be produced in the inner jets (inward of HST-1) if there are strongly particle-dominated regions. Direct synchrotron emission from accelerated protons and secondaries cannot yet be excluded."
JI-XIN CHENG,Highly sensitive transient absorption imaging of graphene and graphene oxide in living cells and circulating blood,We report a transient absorption (TA) imaging method for fast visualization and quantitative layer analysis of graphene and GO. Forward and backward imaging of graphene on various substrates under ambient condition was imaged with a speed of 2 μs per pixel. The TA intensity linearly increased with the layer number of graphene. Real-time TA imaging of GO in vitro with capability of quantitative analysis of intracellular concentration and ex vivo in circulating blood were demonstrated. These results suggest that TA microscopy is a valid tool for the study of graphene based materials.
JI-XIN CHENG,Far-field imaging of non-fluorescent species with sub-diffraction resolution,"Super-resolution optical microscopy is opening a new window to unveil the unseen details on the nanoscopic scale. Current far-field super-resolution techniques rely on fluorescence as the read-out1-5. Here, we demonstrate a scheme for breaking the diffraction limit in far-field imaging of non-fluorescent species by using spatially controlled saturation of electronic absorption. Our method is based on a pump-probe process where a modulated pump field perturbs the charge-carrier density in a sample, thus modulating the transmission of a probe field. A doughnut shape laser beam is then added to transiently saturate the electronic transition in the periphery of the focal volume, thus the induced modulation in the sequential probe pulse only occurs at the focal center. By raster scanning the three collinearly aligned beams, high-speed sub-diffraction-limited imaging of graphite nano-platelets was performed. This technique potentially enables super-resolution imaging of nano-materials and non-fluorescent chromophores, which may remain out of reach for fluorescence-based methods."
JI-XIN CHENG,Bond-selective full-field optical coherence tomography,
JI-XIN CHENG,Bond-selective interferometric scattering microscopy,"Interferometric scattering microscopy has been a very promising technology for highly sensitive label-free imaging of a broad spectrum of biological nanoparticles from proteins to viruses in a high-throughput manner. Although it can reveal the specimen's size and shape information, the chemical composition is inaccessible in interferometric measurements. Infrared spectroscopic imaging provides chemical specificity based on inherent chemical bond vibrations of specimens but lacks the ability to image and resolve individual nanoparticles due to long infrared wavelengths. Here, we describe a bond-selective interferometric scattering microscope where the mid-infrared induced photothermal signal is detected by a visible beam in a wide-field common-path interferometry configuration. A thin film layered substrate is utilized to reduce the reflected light and provide a reference field for the interferometric detection of the weakly scattered field. A pulsed mid-IR laser is employed to modulate the interferometric signal. Subsequent demodulation via a virtual lock-in camera offers simultaneous chemical information about tens of micro- or nano-particles. The chemical contrast arises from a minute change in the particle's scattered field in consequence of the vibrational absorption at the target molecule. We characterize the system with sub-wavelength polymer beads and highlight biological applications by chemically imaging several microorganisms including Staphylococcus aureus, Escherichia coli, and Candida albicans. A theoretical framework is established to extend bond-selective interferometric scattering microscopy to a broad range of biological micro- and nano-particles."
JI-XIN CHENG,Plasmon-enhanced stimulated Raman scattering microscopy with single-molecule detection sensitivity,"Stimulated Raman scattering (SRS) microscopy allows for high-speed label-free chemical imaging of biomedical systems. The imaging sensitivity of SRS microscopy is limited to ~10 mM for endogenous biomolecules. Electronic pre-resonant SRS allows detection of sub-micromolar chromophores. However, label-free SRS detection of single biomolecules having extremely small Raman cross-sections (~10-30 cm2 sr-1) remains unreachable. Here, we demonstrate plasmon-enhanced stimulated Raman scattering (PESRS) microscopy with single-molecule detection sensitivity. Incorporating pico-Joule laser excitation, background subtraction, and a denoising algorithm, we obtain robust single-pixel SRS spectra exhibiting single-molecule events, verified by using two isotopologues of adenine and further confirmed by digital blinking and bleaching in the temporal domain. To demonstrate the capability of PESRS for biological applications, we utilize PESRS to map adenine released from bacteria due to starvation stress. PESRS microscopy holds the promise for ultrasensitive detection and rapid mapping of molecular events in chemical and biomedical systems."
JI-XIN CHENG,Volumetric chemical imaging in vivo by a remote-focusing stimulated Raman scattering microscope,"Operable under ambient light and providing chemical selectivity, stimulated Raman scattering (SRS) microscopy opens a new window for imaging molecular events on a human subject, such as filtration of topical drugs through the skin. A typical approach for volumetric SRS imaging is through piezo scanning of an objective lens, which often disturbs the sample and offers a low axial scan rate. To address these challenges, we have developed a deformable mirror-based remote-focusing SRS microscope, which not only enables high-quality volumetric chemical imaging without mechanical scanning of the objective but also corrects the system aberrations simultaneously. Using the remote-focusing SRS microscope, we performed volumetric chemical imaging of living cells and captured in real time the dynamic diffusion of topical chemicals into human sweat pores."
JI-XIN CHENG,Background-suppressed high-throughput mid-infrared photothermal microscopy via pupil engineering,
JI-XIN CHENG,Nrg4 promotes fuel oxidation and a healthy adipokine profile to ameliorate diet-induced metabolic disorders,"OBJECTIVE: Brown and white adipose tissue exerts pleiotropic effects on systemic energy metabolism in part by releasing endocrine factors. Neuregulin 4 (Nrg4) was recently identified as a brown fat-enriched secreted factor that ameliorates diet-induced metabolic disorders, including insulin resistance and hepatic steatosis. However, the physiological mechanisms through which Nrg4 regulates energy balance and glucose and lipid metabolism remain incompletely understood. The aims of the current study were: i) to investigate the regulation of adipose Nrg4 expression during obesity and the physiological signals involved, ii) to elucidate the mechanisms underlying Nrg4 regulation of energy balance and glucose and lipid metabolism, and iii) to explore whether Nrg4 regulates adipose tissue secretome gene expression and adipokine secretion. METHODS: We examined the correlation of adipose Nrg4 expression with obesity in a cohort of diet-induced obese mice and investigated the upstream signals that regulate Nrg4 expression. We performed metabolic cage and hyperinsulinemic-euglycemic clamp studies in Nrg4 transgenic mice to dissect the metabolic pathways regulated by Nrg4. We investigated how Nrg4 regulates hepatic lipid metabolism in the fasting state and explored the effects of Nrg4 on adipose tissue gene expression, particularly those encoding secreted factors. RESULTS: Adipose Nrg4 expression is inversely correlated with adiposity and regulated by pro-inflammatory and anti-inflammatory signaling. Transgenic expression of Nrg4 increases energy expenditure and augments whole body glucose metabolism. Nrg4 protects mice from diet-induced hepatic steatosis in part through activation of hepatic fatty acid oxidation and ketogenesis. Finally, Nrg4 promotes a healthy adipokine profile during obesity. CONCLUSIONS: Nrg4 exerts pleiotropic beneficial effects on energy balance and glucose and lipid metabolism to ameliorate obesity-associated metabolic disorders. Biologic therapeutics based on Nrg4 may improve both type 2 diabetes and non-alcoholic fatty liver disease (NAFLD) in patients."
JI-XIN CHENG,Nanosecond-resolution photothermal dynamic imaging via MHZ digitization and match filtering,"Photothermal microscopy has enabled highly sensitive label-free imaging of absorbers, from metallic nanoparticles to chemical bonds. Photothermal signals are conventionally detected via modulation of excitation beam and demodulation of probe beam using lock-in amplifier. While convenient, the wealth of thermal dynamics is not revealed. Here, we present a lock-in free, mid-infrared photothermal dynamic imaging (PDI) system by MHz digitization and match filtering at harmonics of modulation frequency. Thermal-dynamic information is acquired at nanosecond resolution within single pulse excitation. Our method not only increases the imaging speed by two orders of magnitude but also obtains four-fold enhancement of signal-to-noise ratio over lock-in counterpart, enabling high-throughput metabolism analysis at single-cell level. Moreover, by harnessing the thermal decay difference between water and biomolecules, water background is effectively separated in mid-infrared PDI of living cells. This ability to nondestructively probe chemically specific photothermal dynamics offers a valuable tool to characterize biological and material specimens."
JI-XIN CHENG,Multiwindow SRS imaging using a rapid widely tunable fiber laser,"Spectroscopic stimulated Raman scattering (SRS) imaging has become a useful tool finding a broad range of applications. Yet, wider adoption is hindered by the bulky and environmentally sensitive solid-state optical parametric oscillator (OPO) in a current SRS microscope. Moreover, chemically informative multiwindow SRS imaging across C-H, C-D, and fingerprint Raman regions is challenging due to the slow wavelength tuning speed of the solid-state OPO. In this work, we present a multiwindow SRS imaging system based on a compact and robust fiber laser with rapid and wide tuning capability. To address the relative intensity noise intrinsic to a fiber laser, we implemented autobalanced detection, which enhances the signal-to-noise ratio of stimulated Raman loss imaging by 23 times. We demonstrate high-quality SRS metabolic imaging of fungi, cancer cells, and Caenorhabditis elegans across the C-H, C-D, and fingerprint Raman windows. Our results showcase the potential of the compact multiwindow SRS system for a broad range of applications."
JI-XIN CHENG,Single virus fingerprinting by widefield interferometric defocus-enhanced mid-infrared photothermal microscopy,"Clinical identification and fundamental study of viruses rely on the detection of viral proteins or viral nucleic acids. Yet, amplification-based and antigen-based methods are not able to provide precise compositional information of individual virions due to small particle size and low-abundance chemical contents (e.g., ~ 5000 proteins in a vesicular stomatitis virus). Here, we report a widefield interferometric defocus-enhanced mid-infrared photothermal (WIDE-MIP) microscope for high-throughput fingerprinting of single viruses. With the identification of feature absorption peaks, WIDE-MIP reveals the contents of viral proteins and nucleic acids in single DNA vaccinia viruses and RNA vesicular stomatitis viruses. Different nucleic acid signatures of thymine and uracil residue vibrations are obtained to differentiate DNA and RNA viruses. WIDE-MIP imaging further reveals an enriched β sheet components in DNA varicella-zoster virus proteins. Together, these advances open a new avenue for compositional analysis of viral vectors and elucidating protein function in an assembled virion."
XINNING LI,Highly sensitive transient absorption imaging of graphene and graphene oxide in living cells and circulating blood,We report a transient absorption (TA) imaging method for fast visualization and quantitative layer analysis of graphene and GO. Forward and backward imaging of graphene on various substrates under ambient condition was imaged with a speed of 2 μs per pixel. The TA intensity linearly increased with the layer number of graphene. Real-time TA imaging of GO in vitro with capability of quantitative analysis of intracellular concentration and ex vivo in circulating blood were demonstrated. These results suggest that TA microscopy is a valid tool for the study of graphene based materials.
XINNING LI,Global quasi-daily fractional vegetation cover estimated from the DSCOVR EPIC directional hotspot dataset,
XINNING LI,Towards verification-aware knowledge distillation for neural-network controlled systems: invited paper,"Neural networks are widely used in many applications ranging from classification to control. While these networks are composed of simple arithmetic operations, they are challenging to formally verify for properties such as reachability due to the presence of nonlinear activation functions. In this paper, we make the observation that Lipschitz continuity of a neural network not only can play a major role in the construction of reachable sets for neural-network controlled systems but also can be systematically controlled during training of the neural network. We build on this observation to develop a novel verification-aware knowledge distillation framework that transfers the knowledge of a trained network to a new and easier-to-verify network. Experimental results show that our method can substantially improve reachability analysis of neural-network controlled systems for several state-of-the-art tools"
XINNING LI,Fractional and composite excitations of antiferromagnetic quantum spin trimer chains,"Using quantum Monte Carlo, exact diagonalization, and perturbation theory, we study the spectrum of the S = 1/2 antiferromagnetic Heisenberg trimer chain by varying the ratio g = J2/J1 of the intertrimer and intratrimer coupling strengths. The doublet ground states of trimers form effective interacting S = 1/2 degrees of freedom described by a Heisenberg chain. Therefore, the conventional two-spinon continuum of width ∝ J1 when g = 1 evolves into to a similar continuum of width ∝ J2 when g → 0. The intermediate-energy and high-energy modes are termed doublons and quartons which fractionalize with increasing g to form the conventional spinon continuum. In particular, at g ≈ 0.716, the gap between the low-energy spinon branch and the high-energy band with mixed doublons, quartons, and spinons closes. These features should be observable in inelastic neutron scattering experiments if a quasi-one-dimensional quantum magnet with the linear trimer structure and J2 < J1 can be identified. Our results may open a window for exploring the high-energy fractional excitations."
XINNING LI,Fractional and composite excitations of antiferromagnetic quantum spin trimer chains,"Using Lanczos exact diagonalization, stochastic analytic continuation of quantum Monte Carlo data, and perturbation theory, we investigate the dynamic spin structure factor S(q, 𝜔) of the S=1/2 antiferromagnetic Heisenberg trimer chain. We systematically study the evolution of the spectrum by varying the ratio g=J_2/J_1 of the intertrimer and intratrimer coupling strengths and interpret the observed features using analytical and numerical calculations with the trimer eigenstates. The doublet ground states of the trimers form effective interacting S=1/2 degrees of freedom described by a Heisenberg chain with coupling J_eff=(4/9)J_2. Therefore, the conventional two-spinon continuum of width ∝ J_1 when g =1 evolves into to a similar continuum of width ∝ J_2 in the reduced Brillouin zone when g ⟶ 0. The high-energy modes (at 𝜔 ∝ J_1) for g ≈ 0.5 can be understood as weakly dispersing propagating internal trimer excitations (which we term doublons and quartons), and these fractionalize with increasing g to form the conventional spinon continuum when g is increased toward 1. The coexistence of two kinds of emergent spinon branches for intermediate values of g give rise to interesting spectral signatures, especially at g ≈ 0.7 where the gap between the low-energy spinon branch and the high energy band of mixed doublons, quartons, and spinons closes. These features should be observable in inelastic neutron scattering experiments if a quasi-one-dimensional quantum magnet with the linear trimer structure and J_2 < J_1 can be identified. We suggest that finding such materials would be useful, enabling detailed studies of coexisting exotic excitations and their interplay within a relatively simple theoretical framework."
XINNING LI,Extreme suppression of antiferromagnetic order and critical scaling in a two-dimensional random quantum magnet,"Sr_2CuTeO_6 is a square-lattice Néel antiferromagnet with superexchange between first-neighbor S=1/2 Cu spins mediated by plaquette centered Te ions. Substituting Te by W, the affected impurity plaquettes have predominantly second-neighbor interactions, thus causing local magnetic frustration. Here we report a study of Sr_2CuTe_1-xW_xO_6 using neutron diffraction and μSR techniques, showing that the Néel order vanishes already at x=0.025±0.005. We explain this extreme order suppression using a two-dimensional Heisenberg spin model, demonstrating that a W-type impurity induces a deformation of the order parameter that decays with distance as 1/r^2 at temperature T=0. The associated logarithmic singularity leads to loss of order for any x>0. Order for small x>0 and T>0 is induced by weak interplane couplings. In the nonmagnetic phase of Sr_2CuTe_1-x W_x O_6, the μSR relaxation rate exhibits quantum critical scaling with a large dynamic exponent, z≈3, consistent with a random-singlet state."
XINNING LI,Volumetric chemical imaging in vivo by a remote-focusing stimulated Raman scattering microscope,"Operable under ambient light and providing chemical selectivity, stimulated Raman scattering (SRS) microscopy opens a new window for imaging molecular events on a human subject, such as filtration of topical drugs through the skin. A typical approach for volumetric SRS imaging is through piezo scanning of an objective lens, which often disturbs the sample and offers a low axial scan rate. To address these challenges, we have developed a deformable mirror-based remote-focusing SRS microscope, which not only enables high-quality volumetric chemical imaging without mechanical scanning of the objective but also corrects the system aberrations simultaneously. Using the remote-focusing SRS microscope, we performed volumetric chemical imaging of living cells and captured in real time the dynamic diffusion of topical chemicals into human sweat pores."
XINNING LI,Non-line-of-sight imaging over 1.43 km,"Non-line-of-sight (NLOS) imaging has the ability to reconstruct hidden objects from indirect light paths that scatter multiple times in the surrounding environment, which is of considerable interest in a wide range of applications. Whereas conventional imaging involves direct line-of-sight light transport to recover the visible objects, NLOS imaging aims to reconstruct the hidden objects from the indirect light paths that scatter multiple times, typically using the information encoded in the time-of-flight of scattered photons. Despite recent advances, NLOS imaging has remained at short-range realizations, limited by the heavy loss and the spatial mixing due to the multiple diffuse reflections. Here, both experimental and conceptual innovations yield hardware and software solutions to increase the standoff distance of NLOS imaging from meter to kilometer range, which is about three orders of magnitude longer than previous experiments. In hardware, we develop a high-efficiency, low-noise NLOS imaging system at near-infrared wavelength based on a dual-telescope confocal optical design. In software, we adopt a convex optimizer, equipped with a tailored spatial-temporal kernel expressed using three-dimensional matrix, to mitigate the effect of the spatial-temporal broadening over long standoffs. Together, these enable our demonstration of NLOS imaging and real-time tracking of hidden objects over a distance of 1.43 km. The results will open venues for the development of NLOS imaging techniques and relevant applications to real-world conditions."
XINNING LI,The long-baseline neutrino experiment: exploring fundamental symmetries of the universe,"The preponderance of matter over antimatter in the early Universe, the dynamics of the supernova bursts that produced the heavy elements necessary for life and whether protons eventually decay --- these mysteries at the forefront of particle physics and astrophysics are key to understanding the early evolution of our Universe, its current state and its eventual fate. The Long-Baseline Neutrino Experiment (LBNE) represents an extensively developed plan for a world-class experiment dedicated to addressing these questions. LBNE is conceived around three central components: (1) a new, high-intensity neutrino source generated from a megawatt-class proton accelerator at Fermi National Accelerator Laboratory, (2) a near neutrino detector just downstream of the source, and (3) a massive liquid argon time-projection chamber deployed as a far detector deep underground at the Sanford Underground Research Facility. This facility, located at the site of the former Homestake Mine in Lead, South Dakota, is approximately 1,300 km from the neutrino source at Fermilab -- a distance (baseline) that delivers optimal sensitivity to neutrino charge-parity symmetry violation and mass ordering effects. This ambitious yet cost-effective design incorporates scalability and flexibility and can accommodate a variety of upgrades and contributions. With its exceptional combination of experimental configuration, technical capabilities, and potential for transformative discoveries, LBNE promises to be a vital facility for the field of particle physics worldwide, providing physicists from around the globe with opportunities to collaborate in a twenty to thirty year program of exciting science. In this document we provide a comprehensive overview of LBNE's scientific objectives, its place in the landscape of neutrino physics worldwide, the technologies it will incorporate and the capabilities it will possess."
DARRELL KOTTON,Transcriptional Analysis of Fracture Healing and the Induction of Embryonic Stem Cell-Related Genes,"Fractures are among the most common human traumas. Fracture healing represents a unique temporarily definable post-natal process in which to study the complex interactions of multiple molecular events that regulate endochondral skeletal tissue formation. Because of the regenerative nature of fracture healing, it is hypothesized that large numbers of post-natal stem cells are recruited and contribute to formation of the multiple cell lineages that contribute to this process. Bayesian modeling was used to generate the temporal profiles of the transcriptome during fracture healing. The temporal relationships between ontologies that are associated with various biologic, metabolic, and regulatory pathways were identified and related to developmental processes associated with skeletogenesis, vasculogenesis, and neurogenesis. The complement of all the expressed BMPs, Wnts, FGFs, and their receptors were related to the subsets of transcription factors that were concurrently expressed during fracture healing. We further defined during fracture healing the temporal patterns of expression for 174 of the 193 genes known to be associated with human genetic skeletal disorders. In order to identify the common regulatory features that might be present in stem cells that are recruited during fracture healing to other types of stem cells, we queried the transcriptome of fracture healing against that seen in embryonic stem cells (ESCs) and mesenchymal stem cells (MSCs). Approximately 300 known genes that are preferentially expressed in ESCs and ~350 of the known genes that are preferentially expressed in MSCs showed induction during fracture healing. Nanog, one of the central epigenetic regulators associated with ESC stem cell maintenance, was shown to be associated in multiple forms or bone repair as well as MSC differentiation. In summary, these data present the first temporal analysis of the transcriptome of an endochondral bone formation process that takes place during fracture healing. They show that neurogenesis as well as vasculogenesis are predominant components of skeletal tissue formation and suggest common pathways are shared between post-natal stem cells and those seen in ESCs."
DARRELL KOTTON,Generation of a purified iPSC-derived smooth muscle-like population for cell sheet engineering,"Induced pluripotent stem cells (iPSCs) provide a potential source for the derivation of smooth muscle cells (SMCs); however, current approaches are limited by the production of heterogeneous cell types and a paucity of tools or markers for tracking and purifying candidate SMCs. Here, we develop murine and human iPSC lines carrying fluorochrome reporters (Acta2hrGFP and ACTA2eGFP, respectively) that identify Acta2+/ACTA2+ cells as they emerge in vitro in real time during iPSC-directed differentiation. We find that Acta2hrGFP+ and ACTA2eGFP+ cells can be sorted to purity and are enriched in markers characteristic of an immature or synthetic SMC. We characterize the resulting GFP+ populations through global transcriptomic profiling and functional studies, including the capacity to form engineered cell sheets. We conclude that these reporter lines allow for generation of sortable, live iPSC-derived Acta2+/ACTA2+ cells highly enriched in smooth muscle lineages for basic developmental studies, tissue engineering, or future clinical regenerative applications."
JOHN MARSTON,Best practices for digitizing a wood slide collection: The Bailey-Wetmore Wood Collection of the Harvard University Herbaria,"As herbaria move to digitize their collections, the question remains of how to efficiently digitize collections other than standard herbarium sheets, such as wood slide collections. Beginning in September 2018, the Harvard University Herbaria began a project to image and digitize the wood slides contained in the Bailey-Wetmore Wood Collection. The primary goal of this project was to produce images of the wood tissue that could be used for specimen-level research and to make them available on the internet for remote scholarship. A secondary goal was to establish best practices for digitizing and imaging a microscope slide collection of tissue sections. Due to the size of the wood slide collection (approximately 30,000 slides), a medical histology scanner and virtual microscopy software were used to image these slides. This article outlines the workflow used to create these images and compares the results with digital resources currently available for wood anatomy research. Prior to this project, the very little of the Bailey-Wetmore Wood Collection was cataloged digitally and none of it was imaged, which made access to this unique collection difficult. By imaging and digitizing 6605 slides in the collection, this project has demonstrated how other institutions can make similar slide collections available to the broader scientific community."
JOHN MARSTON,"Environmental reconstruction and wood use at late Chalcolithic Çamlıbel Tarlası, Turkey","Çamlıbel Tarlası is a short-lived, mid 4th millennium BCE Chalcolithic archaeological site in northern central Anatolia, modern Turkey, with evidence for both intensive metallurgy and permanent occupation. Analysis of a wood charcoal assemblage from the site, totaling 2815 charcoal fragments, is the first from this period and region. Anthracological analysis indicates that the primary fuel wood used was deciduous oak, which comprised nearly 90% of identifiable fragments. We find little evidence of differences in wood species used for different functions or over time; however, a significant trend towards the increased use of large-diameter branch or trunk wood over time is noted both for oak and other minor taxa. We reconstruct a dense oak-dominated woodland in the vicinity of the site at the time of first use, with increased forest clearance over time, due to either diminished fuel availability or agricultural expansion, or a combination of the two. An intensification in metallurgical activity in later periods of occupation may have increased demand specifically for large-diameter wood."
JOHN MARSTON,"Early and Middle Holocene wood exploitation in the Fayum basin, Egypt","The early and middle Holocene of North Africa was a time of dramatic climatic and social change, including rapid shifts in vegetation communities and the introduction of domesticated plants and animals. Recent research from the Fayum basin of Egypt, which holds archaeological evidence for early use of domesticates, aims to place inhabitants of that region within their contemporary environmental setting. We present here results of wood charcoal analysis from three early- and middle-Holocene deposits on the north shore of the Fayum and reconstruct both contemporary woodland ecology and patterns of anthropogenic wood use. In total, three woodland communities likely existed in the area, but inhabitants of this region made heavy use of only the local lakeshore woodland, emphasizing tamarisk (Tamarix sp.) for fuel. While seasonally watered wadi woodlands were not harvested for fuel, more arid locations on the landscape were, evidencing regional mobility between ecological zones. Results indicate that wood was locally abundant and that inhabitants were able to select only preferred species for fuel. This study provides further evidence for low-level food production in the Fayum that preserved critical ecosystem services, rather than dramatic niche construction to promote agriculture as seen elsewhere in middle-Holocene Southwest Asia."
JOHN MARSTON,Publishing in Ethnobiology Letters in 2018,"Ethnobiology Letters was launched in 2010 with the goal of providing a free-to-publish, open-access, online venue for short peer-reviewed articles in ethnobiology (Wolverton et al. 2010). Over the course of nine volumes, which comprise 12 issues, published since that date, Ethnobiology Letters has grown and changed, with new editors, authors, and submission categories. We write this editorial to highlight those changes, as well as to report submission and review metrics for the journal since the inception of our online journal management system. We describe the current status of Ethnobiology Letters and plans for the future of the journal."
JOHN MARSTON,Rural agricultural economies and military provisioning at Roman Gordion (Central Turkey),"Roman Gordion, on the Anatolian plateau, is the only excavated rural military settlement in a pacified territory in the Roman East, providing a unique opportunity to investigate the agricultural economy of a permanent Roman garrison. We present combined results of archaeobotanical and zooarchaeological analyses, assessing several hypotheses regarding Roman military provisioning. The garrison adapted its dietary preferences to local agricultural systems, but maintained its traditional meat supply of pork, beef, and chickens as well. There is evidence for economic interdependence with local farmers and cattle herders, self- sufficiency in pork and chicken production, and complex relationships with autonomous sheep and goat herders who pursued their own economic goals. If the Roman military in Gordion exercised a command economy, they were able to implement that control only on specific components of the agricultural sector, especially cereal farming. The sheep and goat herding system remained unaltered, targeting secondary products for a market economy and/or broader provincial taxation authorities. The garrison introduced new elements to the animal economy of the Gordion region, including a new pig husbandry system. Comparison with contemporary non-military settlements suggests both similarities and differences with urban meat economies of Roman Anatolia."
JOHN MARSTON,Modeling resilience and sustainability in ancient agricultural systems,"The reasons why people adopt unsustainable agricultural practices, and the ultimate environmental implications of those practices, remain incompletely understood in the present world. Archaeology, however, offers unique datasets on coincident cultural and ecological change, and their social and environmental effects. This article applies concepts derived from ecological resilience thinking to assess the sustainability of agricultural practices as a result of long-term interactions between political, economic, and environmental systems. Using the urban center of Gordion, in central Turkey, as a case study, it is possible to identify mismatched social and ecological processes on temporal, spatial, and organizational scales, which help to resolve thresholds of resilience. Results of this analysis implicate temporal and spatial mismatches as a cause for local environmental degradation, and increasing extralocal economic pressures as an ultimate cause for the adoption of unsustainable land-use practices. This analysis suggests that a research approach that integrates environmental archaeology with a resilience perspective has considerable potential for explicating regional patterns of agricultural change and environmental degradation in the past."
JOHN MARSTON,Agricultural adaptation to highland climate in Iron Age Anatolia,"As polities grow and expand into environments distinct from their homeland, settlers moving to new landscapes may need to adapt familiar agricultural strategies to a new climate. This article explores one such case through the site of Kerkenes, a fortified, mountaintop urban center of Iron Age Central Anatolia evidently founded by Phrygian settlers from further west. New archaeobotanical data from Kerkenes indicate a set of agricultural practices broadly similar to that of other contemporary sites in Anatolia. Farmers at Kerkenes, however, appear to have prioritized bread wheat cultivation over that of barley, in stark contrast to agricultural strategies at Gordion, capital of the Phrygian kingdom. Placing Kerkenes in its environmental and economic landscape suggests that farmers took advantage of favorable rainfall patterns to emphasize a preferred cereal crop, deploying new strategies to minimize local subsistence risk. These results highlight the potential of regional syntheses of agricultural practices within large territorial states to illuminate the environmental footprints and agricultural signatures of individual polities."
JOHN MARSTON,"Kara-tepe, Karakalpakstan: agropastoralism in a Central Eurasian oasis in the 4th/5th century A.D. transition","This paper reports on the results of archaeological field excavations at the site of Kara-tepe, in the semi-autonomous region of Karakalpakstan in northwestern Uzbekistan. Investigations at the site in 2008–2009 turned up an unusually rich assemblage of remains from a household context. Combined analysis of the household botanical and faunal remains has allowed us to reconstruct the agropastoral practices of local inhabitants in this oasis region during a critical period of social and environmental change in the Early Medieval transition (4th–5th centuries A.D.). The results of the study raise important new questions about agropastoralism in the oases of Central Eurasia, highlighting continuities of practice between oasis and steppe populations, and revealing dynamic changes in these systems over time."
JOHN MARSTON,Scholarly motivations to conduct interdisciplinary climate change research,"Understanding and responding to today’s complex environmental problems requires collaboration that bridges disciplinary boundaries. As the barriers to interdisciplinary research are formidable, promoting interdisciplinary environmental research requires understanding what motivates researchers to embark upon such challenging research. This article draws upon research on problem choice and interdisciplinary research practice to investigate motivators and barriers to interdisciplinary climate change (IDCC) research. Results from a survey on the motivations of 526 Ph.D.-holding, early- to mid-career, self-identified IDCC scholars indicate how those scholars make decisions regarding their research choices including the role of intrinsic and extrinsic motivations and the barriers arising from the nature of interdisciplinary research and institutional structures. Climate change was not the main motivation for most respondents to become scholars, yet the majority began to study the issue because they could not ignore the problem. Respondents’ decisions to conduct IDCC research are driven by personal motivations, including personal interest, the importance of IDCC research to society, and enjoyment of interdisciplinary collaborations. Two thirds of respondents reported having encountered challenges in communication across disciplines, longer timelines while conducting interdisciplinary work, and a lack of peer support. Nonetheless, most respondents plan to conduct IDCC research in the future and will choose their next research project based on its societal benefits and the opportunity to work with specific collaborators. We conclude that focused attention to supporting intrinsic motivations, as well as removing institutional barriers, can facilitate future IDCC research."
JOHN MARSTON,"Neanderthal plant use and pyrotechnology: phytolith analysis from Roc de Marsal, France","The plant component of Neanderthal subsistence and technology is not well documented, partially due to the preservation constraints of macrobotanical components. Phytoliths, however, are preserved even when other plant remains have decayed and so provide evidence for Neanderthal plant use and the environmental context of archaeological sites. Phytolith assemblages from Roc de Marsal, a Middle Paleolithic cave site in SW France, provide new insight into the relationship between Neanderthals and plant resources. Ninety-seven samples from all archaeological units and 18 control samples are analyzed. Phytoliths from the wood and bark of dicotyledonous plants are the most prevalent, but there is also a significant proportion of grass phytoliths in many samples. Phytolith densities are much greater in earlier layers, which is likely related to the presence of combustion features in those layers. These phytoliths indicate a warmer, wetter climate, whereas phytoliths from upper layers indicate a cooler, drier environment. Phytoliths recovered from combustion features indicate that wood was the primary plant fuel source, while grasses may have been used as surface preparations."
JOHN MARSTON,"Raw data for Chapter 22 ""Phytolith results from Tomb 16/H/50""",
JOHN MARSTON,Mentoring is an intellectual pillar of ethnobiology,"Ethnobiology relies on community partnerships and relationships between elders or other knowledge keepers and students. Our Society of Ethnobiology, like all academic organizations, has its own issues with discrimination and abuses of power. But more than other academic disciplines, contemporary ethnobiology is practiced with and strengthened by close, respectful working relationships. As such, we offer our thoughts on the lessons ethnobiology brings to mentorship and accountability while outlining some of the specific steps we are taking as an academic and practicing community."
JOHN MARSTON,"Exploring space, economy, and interregional Interaction at a second-millennium B.C.E. citadel in central western Anatolia: 2014-2017 research at Kaymakçı","Current understandings of the archaeology of second-millennium B.C.E. central western Anatolia are enriched by ongoing research at Kaymakçı, located in the Marmara Lake basin of the middle Gediz River valley in western Turkey. Discovered during regional survey in 2001, the site offers a critical node of exploration for understanding a previously unexamined period in a well-traversed geography thought to be the core of the Late Bronze Age Seha River Land known from Hittite texts. Here we present results from the first three seasons of excavation on the citadel of Kaymakçı plus a study season (2014–2017), introducing the site’s chronology, historical and regional context, and significance through presentation of excavation areas as well as material and subsistence economies. With reference to such evidence, we discuss the site’s development, organization, and interregional interactions, demonstrating its place in local and regional networks that connected Aegean and central Anatolian spheres of interest."
JOHN MARSTON,Archaeologies of empire and environment,"This paper promotes an explicit study of archaeologies of empire and environment, and advances theories and methods in environmental archaeology that demonstrate that environmental practices articulate people's relationships to imperial authority. While many studies of empire take for granted that centralized organization and surplus production lead to political control and social inequity, in the papers assembled for this special issue, the very relationship between human-environment interactions and political power becomes the object of study. In this introduction, we review established archaeological approaches to empire, explain how environmental frameworks productively recast our understandings of imperialism, and proffer a number of avenues for continued research on the subject, including those provided by the articles in this issue. We present three over-arching themes for the study of empire and environment—scale, legacy, and resilience and resistance—and discuss their implementation with the papers that follow. Ultimately, we argue that imperialism entails the management of heterogeneous peoples and environments, and therefore, archaeologies of empire require the integrated study of humans, landscapes, and biota."
JOHN MARSTON,Agropastoral economies and land use in Bronze Age Western Anatolia,
JOHN MARSTON,"Assessing the potential of phytolith analysis to investigate local environment and prehistoric plant resource use in temperate regions: A case study from Williamson’s Moss, Cumbria, Britain",
JOHN MARSTON,Ethnobiology after four years of socioecological violence,
JOHN MARSTON,The relationship of readiness factors to Jan. first grade reading achievement,
JOHN MARSTON,Ancient DNA (aDNA) extraction and amplification from 3500-year-old charred economic crop seeds from Kaymakçı in Western Turkey: comparative sequence analysis using the 26S rDNA gene,"Ancient DNA (aDNA) from 3500–4000 year old seeds of Triticum aestivum or T. durum, Vicia ervillia, Cicer arietinum and Vitis vinifera excavated from the archaeological site of Kaymakçı was successfully isolated using various isolation methods. The genomic DNA of each species was amplified further using the aDNA of the seeds. Using the amplified ancient genomic DNA from the ancient seeds and their contemporaries, the sequence analysis of the 26S ribosomal DNA (rDNA) gene was carried out comparatively. The results indicated that all seeds were identified correctly by the DNA sequence data from the 26S rDNA gene. The wheat seed from Kaymakcı was characterized as bread wheat (Triticum aestivum). Comparative sequence analysis also revealed that specific base locations in the ancient 26S rDNA gene were either lost or substituted with different DNA bases due to continued domestication and breeding activities. This was more evident in sexually reproducing species than it was asexually reproducing species such as Vitis vinifera. Attaining of high amount and good quality of amplified genomic DNA from ancient seeds will further allow the investigation of the extent of genetic change (allelic loss, bottlenecks, etc.) between ancient seeds and their contemporary species in genetic diversity studies."
JOHN MARSTON,Applied archaeobotany of southwest Asia: a tribute to Naomi F. Miller,
CARL A RUCK,"D-cycloserine augmentation of exposure-based cognitive behavior therapy for anxiety, obsessive-compulsive, and posttraumatic stress disorders: a systematic review and meta-analysis of individual participant data","Importance: Whether and under which conditions D-cycloserine (DCS) augments the effects of exposure-based cognitive behavior therapy for anxiety, obsessive-compulsive, and posttraumatic stress disorders is unclear. Objective: To clarify whether DCS is superior to placebo in augmenting the effects of cognitive behavior therapy for anxiety, obsessive-compulsive, and posttraumatic stress disorders and to evaluate whether antidepressants interact with DCS and the effect of potential moderating variables. Data Sources: PubMed, EMBASE, and PsycINFO were searched from inception to February 10, 2016. Reference lists of previous reviews and meta-analyses and reports of randomized clinical trials were also checked. Study Selection: Studies were eligible for inclusion if they were (1) double-blind randomized clinical trials of DCS as an augmentation strategy for exposure-based cognitive behavior therapy and (2) conducted in humans diagnosed as having specific phobia, social anxiety disorder, panic disorder with or without agoraphobia, obsessive-compulsive disorder, or posttraumatic stress disorder. Data Extraction and Synthesis: Raw data were obtained from the authors and quality controlled. Data were ranked to ensure a consistent metric across studies (score range, 0-100). We used a 3-level multilevel model nesting repeated measures of outcomes within participants, who were nested within studies. Results: Individual participant data were obtained for 21 of 22 eligible trials, representing 1047 of 1073 eligible participants. When controlling for antidepressant use, participants receiving DCS showed greater improvement from pretreatment to posttreatment (mean difference, -3.62; 95% CI, -0.81 to -6.43; P = .01; d = -0.25) but not from pretreatment to midtreatment (mean difference, -1.66; 95% CI, -4.92 to 1.60; P = .32; d = -0.14) or from pretreatment to follow-up (mean difference, -2.98, 95% CI, -5.99 to 0.03; P = .05; d = -0.19). Additional analyses showed that participants assigned to DCS were associated with lower symptom severity than those assigned to placebo at posttreatment and at follow-up. Antidepressants did not moderate the effects of DCS. None of the prespecified patient-level or study-level moderators was associated with outcomes. Conclusions and Relevance: D-cycloserine is associated with a small augmentation effect on exposure-based therapy. This effect is not moderated by the concurrent use of antidepressants. Further research is needed to identify patient and/or therapy characteristics associated with DCS response."
CARL A RUCK,The new Aphrodite,"The tale of Eros and Psyche is known from its Latin version as Cupid and Psyche, encapsulated in the novel titled the Metamorphoses or Golden Ass (Asinus Aureus) of the second-century CE Apuleius from a Roman colony in nort-hern Africa. It survived antiquity perhaps in a single manuscript and excited great interest in Florence of the Medici Renaissance because of its Neoplatonic motif of the transcendent mystical escape from the Cave of delusionary appearance. Apuleius was an initiate into the Egyptian Mystery religion of Isis, and probably also the great Mys-tery of Greek Eleusis. The tale had been told as well by his Syrian Greek contemporary Lucian and was already sacred in fourth century BCE Magna Graecia at certain cave sanctuaries where the promiscuously sexual love goddess Aphrodite was jointly worshipped with Demeter, the goddess of fertility, and her daughter Persephone, as patrons of the union between husbands and wives. The tale employs the peculiar monogamous mating of the butterfly and its metamorphosis within the cave-like excreted exoskeleton of its golden chrysalis. It is an allegory of the incarnation of spirit in physicality that produces a new version of sexuality as love, rewarded with immortality, and a superior version of beauty endowed with mortality."
CARL A RUCK,The Lady who served the potion: the Eleusinian sacrament personified,
CARL A RUCK,Sunshine and Matricide: Dionysus and the Electra plays,"Ancient Greek drama is often discussed in isolation from the fifth-century Athenian Theater of Dionysus, where for the most part it was first produced, and commonly without regard for the religion and rites of Dionysus, the patron deity of the playwrights who composed its dramas, and without consideration for the nature of the festival experience afforded their audiences in the daylong sequences of enactments. Primarily lacking in the centuries of scholarship that have attempted to understand and evaluate the corpus of surviving dramas is an understanding of the nature of the ancient intoxication accessed by the god’s drink of wine, its relationship to the plant-gathering rituals of the bacchanalian revel and the herbal psychoactive fortifying agents added to the drink, making it a sacramental Eucharist, an entheogen, whose symbolism mediated the dichotomous antithesis of the wild and the cultivated, both botanical and social. Three playwrights, each the prominent exponent in each of three succeeding generations, dominate the history of Greek tragedy—Aeschylus, Sophocles, and Euripides. Each composed at least one play on the mythical figure of Electra, who was pivotal in urging her brother Orestes to commit matricide, to kill their mother Clytemnestra, the sister of Helen, for whom the Trojan War was fought. The two later playwrights were aware of the work of their great predecessor, and commented on it, as well as each other in their dramas. The story of Orestes’ matricide, moreover, has a quasi-historical referent in the dynastic succession for the kingship of Mycenae, whose disputed token of sovereignty was a golden lamb, a zoomorphism for an intoxicant that accessed shamanic empowerment. The controversy over possession of it involves the societal transition of the royal house from a queendom to a kingdom, from matriarchal to patriarchal dominance—the dawning of a new day, hence the coincidence of matricide with the rising sun. The murder of Clytemnestra and her chosen mate or paramour the goat-man Aegisthus, symbolically sometimes presented as a decapitation, has its analogue, in the foundational myth for the citadel of Mycenae, with the heroic task of Perseus, who first imposed masculine control over the city when he harvested the head of the Gorgon Medusa at the site of the fortress—as a mushroom."
CARL A RUCK,Mushrooms and the wine of Maron,"Although the excavators of the sanctuary of the Great Gods on the island of Samothrace recognize that drinking to the point of intoxication was practiced at the Mystery, naively this has not been seen as an element in the initiation scenario. Numerous drinking cups have been found, inscribed as the property of the gods, and the ancient village of Keramidaria (‘Ceramics’) was devoted to the manufacture of amphorae, officially stamped as genuine provenance of Samothrace for the export of the wine distinctive of the Mystery, probably a version of Homer’s potent Maronian wine of the Cyclops. That wine still existed in the Roman Period, and on the testimony of the proconsul assigned to the province, it even required dilution with eight parts water to be drunk safely. At the time of Odysseus, the rate of dilution was twentyfold. Such potent wines achieved their high intoxicating potential from the substances added to the ferment, a fact that has now been confirmed by the discovery of an intact wine cellar from Canaan, dated to the beginning of the second millennium BCE. The myth of the establishment of the Mystery, dated to the generations before the Trojan War, narrates the tale of its founder sailing like a drunken loon upon a wineskin, and similar establishments of the Mystery of the Great Gods depict a Kabeiric dwarfish Odysseus sailing upon an amphora filled with the special potion of the great sorceress Circe. This wine was fortified with a sacred psychoactive mushroom, whose antiquity can be traced back to the wolf sacrament of the Achaemenid Persians, and documented as well in Celtic lore and among the Nordic berserkers, recorded as early as the Emperor Trajan as a rite of the Dacians of Thrace, who are named as the ‘People of the Wolf’ and who carried the banner of Draco into battle, a serpent with the head of a wolf. The serpent is an indication of the wolf’s toxicity, and the fondness of wolves for eating the mushrooms was the basis for the rituals of lycanthropy and the initiated fraternal packs of warriors. In Athens of the Classical Age, the fungal identity of this initiatory sacrament was common knowledge, obscenely parodied on the comic stage. The Etruscans carried this sacrament to the Italian peninsula and it was incorporated into the mythologized history of Rome’s founding by the Trojan Aeneas as the fulfillment of the prophecy of the edible tables that would signal the site for the future city. The cult of the Great Gods involved the widespread phenomenon of the little people that materialized from the sacramental fungus as fairy creatures, using the mushroom as their tables set with dainty morsels that inspired visionary experience and of which it was taboo for the uninitiated to partake."
MARGRIT BETKE,3D multimodal dataset and token-based pose optimization,
MARGRIT BETKE,Personalizing gesture recognition using hierarchical bayesian neural networks,"Building robust classifiers trained on data susceptible to group or subject-specific variations is a challenging pattern recognition problem. We develop hierarchical Bayesian neural networks to capture subject-specific variations and share statistical strength across subjects. Leveraging recent work on learning Bayesian neural networks, we build fast, scalable algorithms for inferring the posterior distribution over all network weights in the hierarchy. We also develop methods for adapting our model to new subjects when a small number of subject-specific personalization data is available. Finally, we investigate active learning algorithms for interactively labeling personalization data in resource-constrained scenarios. Focusing on the problem of gesture recognition where inter-subject variations are commonplace, we demonstrate the effectiveness of our proposed techniques. We test our framework on three widely used gesture recognition datasets, achieving personalization performance competitive with the state-of-the-art."
MARGRIT BETKE,Optical flow sensing and the inverse perception problem for flying bats,"The movements of birds, bats, and other flying species are governed by complex sensorimotor systems that allow the animals to react to stationary environmental features as well as to wind disturbances, other animals in nearby airspace, and a wide variety of unexpected challenges. The paper and talk will describe research that analyzes the three-dimensional trajectories of bats flying in a habitat in Texas. The trajectories are computed with stereoscopic methods using data from synchronous thermal videos that were recorded with high temporal and spatial resolution from three viewpoints. Following our previously reported work, we examine the possibility that bat trajectories in this habitat are governed by optical flow sensing that interpolates periodic distance measurements from echolocation. Using an idealized geometry of bat eyes, we introduce the concept of time-to-transit, and recall some research that suggests that this quantity is computed by the animals' visual cortex. Several steering control laws based on time-to-transit are proposed for an idealized flight model, and it is shown that these can be used to replicate the observed flight of what we identify as typical bats. Although the vision-based motion control laws we propose and the protocols for switching between them are quite simple, some of the trajectories that have been synthesized are qualitatively bat-like. Examination of the control protocols that generate these trajectories suggests that bat motions are governed both by their reactions to a subset of key feature points as well by their memories of where these feature points are located."
MARGRIT BETKE,A Self-initializing Eyebrow Tracker for Binary Switch Emulation,"We designed the Eyebrow-Clicker, a camera-based human computer interface system that implements a new form of binary switch. When the user raises his or her eyebrows, the binary switch is activated and a selection command is issued. The Eyebrow-Clicker thus replaces the ""click"" functionality of a mouse. The system initializes itself by detecting the user's eyes and eyebrows, tracks these features at frame rate, and recovers in the event of errors. The initialization uses the natural blinking of the human eye to select suitable templates for tracking. Once execution has begun, a user therefore never has to restart the program or even touch the computer. In our experiments with human-computer interaction software, the system successfully determined 93% of the time when a user raised his eyebrows."
MARGRIT BETKE,Active Hidden Models for Tracking with Kernel Projections,"We introduce Active Hidden Models (AHM) that utilize kernel methods traditionally associated with classification. We use AHMs to track deformable objects in video sequences by leveraging kernel projections. We introduce the ""subset projection"" method which improves the efficiency of our tracking approach by a factor of ten. We successfully tested our method on facial tracking with extreme head movements (including full 180-degree head rotation), facial expressions, and deformable objects. Given a kernel and a set of training observations, we derive unbiased estimates of the accuracy of the AHM tracker. Kernels are generally used in classification methods to make training data linearly separable. We prove that the optimal (minimum variance) tracking kernels are those that make the training observations linearly dependent."
MARGRIT BETKE,Web Mediators for Accessible Browsing,"We present a highly accurate method for classifying web pages based on link percentage, which is the percentage of text characters that are parts of links normalized by the number of all text characters on a web page. K-means clustering is used to create unique thresholds to differentiate index pages and article pages on individual web sites. Index pages contain mostly links to articles and other indices, while article pages contain mostly text. We also present a novel link grouping algorithm using agglomerative hierarchical clustering that groups links in the same spatial neighborhood together while preserving link structure. Grouping allows users with severe disabilities to use a scan-based mechanism to tab through a web page and select items. In experiments, we saw up to a 40-fold reduction in the number of commands needed to click on a link with a scan-based interface, which shows that we can vastly improve the rate of communication for users with disabilities. We used web page classification and link grouping to alter web page display on an accessible web browser that we developed to make a usable browsing interface for users with disabilities. Our classification method consistently outperformed a baseline classifier even when using minimal data to generate article and index clusters, and achieved classification accuracy of 94.0% on web sites with well-formed or slightly malformed HTML, compared with 80.1% accuracy for the baseline classifier."
MARGRIT BETKE,A Customizable Camera-based Human Computer Interaction System Allowing People With Disabilities Autonomous Hands Free Navigation of Multiple Computing Task,"Many people suffer from conditions that lead to deterioration of motor control and makes access to the computer using traditional input devices difficult. In particular, they may loose control of hand movement to the extent that the standard mouse cannot be used as a pointing device. Most current alternatives use markers or specialized hardware to track and translate a user's movement to pointer movement. These approaches may be perceived as intrusive, for example, wearable devices. Camera-based assistive systems that use visual tracking of features on the user's body often require cumbersome manual adjustment. This paper introduces an enhanced computer vision based strategy where features, for example on a user's face, viewed through an inexpensive USB camera, are tracked and translated to pointer movement. The main contributions of this paper are (1) enhancing a video based interface with a mechanism for mapping feature movement to pointer movement, which allows users to navigate to all areas of the screen even with very limited physical movement, and (2) providing a customizable, hierarchical navigation framework for human computer interaction (HCI). This framework provides effective use of the vision-based interface system for accessing multiple applications in an autonomous setting. Experiments with several users show the effectiveness of the mapping strategy and its usage within the application framework as a practical tool for desktop users with disabilities."
MARGRIT BETKE,Object Detection at the Optimal Scale with Hidden State Shape Models,"Hidden State Shape Models (HSSMs) [2], a variant of Hidden Markov Models (HMMs) [9], were proposed to detect shape classes of variable structure in cluttered images. In this paper, we formulate a probabilistic framework for HSSMs which provides two major improvements in comparison to the previous method [2]. First, while the method in [2] required the scale of the object to be passed as an input, the method proposed here estimates the scale of the object automatically. This is achieved by introducing a new term for the observation probability that is based on a object-clutter feature model. Second, a segmental HMM [6, 8] is applied to model the ""duration probability"" of each HMM state, which is learned from the shape statistics in a training set and helps obtain meaningful registration results. Using a segmental HMM provides a principled way to model dependencies between the scales of different parts of the object. In object localization experiments on a dataset of real hand images, the proposed method significantly outperforms the method of [2], reducing the incorrect localization rate from 40% to 15%. The improvement in accuracy becomes more significant if we consider that the method proposed here is scale-independent, whereas the method of [2] takes as input the scale of the object we want to localize."
MARGRIT BETKE,Computing a Uniform Scaling Parameter for 3D Registration of Lung Surfaces,A difficulty in lung image registration is accounting for changes in the size of the lungs due to inspiration. We propose two methods for computing a uniform scale parameter for use in lung image registration that account for size change. A scaled rigid-body transformation allows analysis of corresponding lung CT scans taken at different times and can serve as a good low-order transformation to initialize non-rigid registration approaches. Two different features are used to compute the scale parameter. The first method uses lung surfaces. The second uses lung volumes. Both approaches are computationally inexpensive and improve the alignment of lung images over rigid registration. The two methods produce different scale parameters and may highlight different functional information about the lungs.
MARGRIT BETKE,Music Maker – A Camera-based Music Making Tool for Physical Rehabilitation,"The therapeutic effects of playing music are being recognized increasingly in the field of rehabilitation medicine. People with physical disabilities, however, often do not have the motor dexterity needed to play an instrument. We developed a camera-based human-computer interface called ""Music Maker"" to provide such people with a means to make music by performing therapeutic exercises. Music Maker uses computer vision techniques to convert the movements of a patient's body part, for example, a finger, hand, or foot, into musical and visual feedback using the open software platform EyesWeb. It can be adjusted to a patient's particular therapeutic needs and provides quantitative tools for monitoring the recovery process and assessing therapeutic outcomes. We tested the potential of Music Maker as a rehabilitation tool with six subjects who responded to or created music in various movement exercises. In these proof-of-concept experiments, Music Maker has performed reliably and shown its promise as a therapeutic device."
MARGRIT BETKE,Detecting Instances of Shape Classes That Exhibit Variable Structure,"This paper proposes a method for detecting shapes of variable structure in images with clutter. The term ""variable structure"" means that some shape parts can be repeated an arbitrary number of times, some parts can be optional, and some parts can have several alternative appearances. The particular variation of the shape structure that occurs in a given image is not known a priori. Existing computer vision methods, including deformable model methods, were not designed to detect shapes of variable structure; they may only be used to detect shapes that can be decomposed into a fixed, a priori known, number of parts. The proposed method can handle both variations in shape structure and variations in the appearance of individual shape parts. A new class of shape models is introduced, called Hidden State Shape Models, that can naturally represent shapes of variable structure. A detection algorithm is described that finds instances of such shapes in images with large amounts of clutter by finding globally optimal correspondences between image features and shape models. Experiments with real images demonstrate that our method can localize plant branches that consist of an a priori unknown number of leaves and can detect hands more accurately than a hand detector based on the chamfer distance."
MARGRIT BETKE,Real Time Eye Tracking and Blink Detection with USB Cameras,"A human-computer interface (HCI) system designed for use by people with severe disabilities is presented. People that are severely paralyzed or afflicted with diseases such as ALS (Lou Gehrig's disease) or multiple sclerosis are unable to move or control any parts of their bodies except for their eyes. The system presented here detects the user's eye blinks and analyzes the pattern and duration of the blinks, using them to provide input to the computer in the form of a mouse click. After the automatic initialization of the system occurs from the processing of the user's involuntary eye blinks in the first few seconds of use, the eye is tracked in real time using correlation with an online template. If the user's depth changes significantly or rapid head movement occurs, the system is automatically reinitialized. There are no lighting requirements nor offline templates needed for the proper functioning of the system. The system works with inexpensive USB cameras and runs at a frame rate of 30 frames per second. Extensive experiments were conducted to determine both the system's accuracy in classifying voluntary and involuntary blinks, as well as the system's fitness in varying environment conditions, such as alternative camera placements and different lighting conditions. These experiments on eight test subjects yielded an overall detection accuracy of 95.3%."
MARGRIT BETKE,MosaicShape: Stochastic Region Grouping with Shape Prior,"A novel method that combines shape-based object recognition and image segmentation is proposed for shape retrieval from images. Given a shape prior represented in a multi-scale curvature form, the proposed method identifies the target objects in images by grouping oversegmented image regions. The problem is formulated in a unified probabilistic framework and solved by a stochastic Markov Chain Monte Carlo (MCMC) mechanism. By this means, object segmentation and recognition are accomplished simultaneously. Within each sampling move during the simulation process,probabilistic region grouping operations are influenced by both the image information and the shape similarity constraint. The latter constraint is measured by a partial shape matching process. A generalized parallel algorithm by Barbu and Zhu,combined with a large sampling jump and other implementation improvements, greatly speeds up the overall stochastic process. The proposed method supports the segmentation and recognition of multiple occluded objects in images. Experimental results are provided for both synthetic and real images."
MARGRIT BETKE,Facial Feature Tracking and Occlusion Recovery in American Sign Language,"Facial features play an important role in expressing grammatical information in signed languages, including American Sign Language(ASL). Gestures such as raising or furrowing the eyebrows are key indicators of constructions such as yes-no questions. Periodic head movements (nods and shakes) are also an essential part of the expression of syntactic information, such as negation (associated with a side-to-side headshake). Therefore, identification of these facial gestures is essential to sign language recognition. One problem with detection of such grammatical indicators is occlusion recovery. If the signer's hand blocks his/her eyebrows during production of a sign, it becomes difficult to track the eyebrows. We have developed a system to detect such grammatical markers in ASL that recovers promptly from occlusion. Our system detects and tracks evolving templates of facial features, which are based on an anthropometric face model, and interprets the geometric relationships of these templates to identify grammatical markers. It was tested on a variety of ASL sentences signed by various Deaf native signers and detected facial gestures used to express grammatical information, such as raised and furrowed eyebrows as well as headshakes."
MARGRIT BETKE,SymbolDesign: A User-centered Method to Design Pen-based Interfaces and Extend the Functionality of Pointer Input Devices,"A method called ""SymbolDesign"" is proposed that can be used to design user-centered interfaces for pen-based input devices. It can also extend the functionality of pointer input devices such as the traditional computer mouse or the Camera Mouse, a camera-based computer interface. Users can create their own interfaces by choosing single-stroke movement patterns that are convenient to draw with the selected input device and by mapping them to a desired set of commands. A pattern could be the trace of a moving finger detected with the Camera Mouse or a symbol drawn with an optical pen. The core of the SymbolDesign system is a dynamically created classifier, in the current implementation an artificial neural network. The architecture of the neural network automatically adjusts according to the complexity of the classification task. In experiments, subjects used the SymbolDesign method to design and test the interfaces they created, for example, to browse the web. The experiments demonstrated good recognition accuracy and responsiveness of the user interfaces. The method provided an easily-designed and easily-used computer input mechanism for people without physical limitations, and, with some modifications, has the potential to become a computer access tool for people with severe paralysis."
MARGRIT BETKE,Fast Head Tilt Detection for Human-Computer Interaction,"Accurate head tilt detection has a large potential to aid people with disabilities in the use of human-computer interfaces and provide universal access to communication software. We show how it can be utilized to tab through links on a web page or control a video game with head motions. It may also be useful as a correction method for currently available video-based assistive technology that requires upright facial poses. Few of the existing computer vision methods that detect head rotations in and out of the image plane with reasonable accuracy can operate within the context of a real-time communication interface because the computational expense that they incur is too great. Our method uses a variety of metrics to obtain a robust head tilt estimate without incurring the computational cost of previous methods. Our system runs in real time on a computer with a 2.53 GHz processor, 256 MB of RAM and an inexpensive webcam, using only 55% of the processor cycles."
MARGRIT BETKE,Example-Based Image Restoration via Boosted Classifiers,We propose a novel image registration framework which uses classifiers trained from examples of aligned images to achieve registration. Our approach is designed to register images of medical data where the physical condition of the patient has changed significantly and image intensities are drastically different. We use two boosted classifiers for each degree of freedom of image transformation. These two classifiers can both identify when two images are correctly aligned and provide an efficient means of moving towards correct registration for misaligned images. The classifiers capture local alignment information using multi-pixel comparisons and can therefore achieve correct alignments where approaches like correlation and mutual-information which rely on only pixel-to-pixel comparisons fail. We test our approach using images from CT scans acquired in a study of acute respiratory distress syndrome. We show significant increase in registration accuracy in comparison to an approach using mutual information.
MARGRIT BETKE,Camera Canvas: Image Editor for People with Severe Disabilities,"Camera Canvas is an image editing software package for users with severe disabilities that limit their mobility. It is specially designed for Camera Mouse, a camera-based mouse-substitute input system. Users can manipulate images through various head movements, tracked by Camera Mouse. The system is also fully usable with traditional mouse or touch-pad input. Designing the system, we studied the requirements and solutions for image editing and content creation using Camera Mouse. Experiments with 20 subjects, each testing Camera Canvas with Camera Mouse as the input mechanism, showed that users found the software easy to understand and operate. User feedback was taken into account to make the software more usable and the interface more intuitive. We suggest that the Camera Canvas software makes important progress in providing a new medium of utility and creativity in computing for users with severe disabilities."
MARGRIT BETKE,Camera-Based Interfaces and Assistive Software for People with Sever Motion Impairments,"Intelligent assistive technology can greatly improve the daily lives of people with severe paralysis, who have limited communication abilities. People with motion impairments often prefer camera-based communication interfaces, because these are customizable, comfortable, and do not require user-borne accessories that could draw attention to their disability. We present an overview of assistive software that we specifically designed for camera-based interfaces such as the Camera Mouse, which serves as a mouse-replacement input system. The applications include software for text-entry, web browsing, image editing, animation, and music therapy. Using this software, people with severe motion impairments can communicate with friends and family and have a medium to explore their creativity."
MARGRIT BETKE,Using artificial intelligence to interpret pneumonia CXR (chest X ray) findings in children with a phone application platform,
MARGRIT BETKE,Automatic 3D Registration of Lung Surfaces in Computed Tomography Scans,"We developed an automated system that registers chest CT scans temporally. Our registration method matches corresponding anatomical landmarks to obtain initial registration parameters. The initial point-to-point registration is then generalized to an iterative surface-to-surface registration method. Our ""goodness-of-ﬁt"" measure is evaluated at each step in the iterative scheme until the registration performance is sufficient. We applied our method to register the 3D lung surfaces of 11 pairs of chest CT scans and report promising registration performance."
MARGRIT BETKE,"Integrated Chest Image Analysis System ""BU-MIA""","We introduce ""BU-MIA,"" a Medical Image Analysis system that integrates various advanced chest image analysis methods for detection, estimation, segmentation, and registration. BU-MIA evaluates repeated computed tomography (CT) scans of the same patient to facilitate identification and evaluation of pulmonary nodules for interval growth. It provides a user-friendly graphical user interface with a number of interaction tools for development, evaluation, and validation of chest image analysis methods. The structures that BU-MIA processes include the thorax, lungs, and trachea, pulmonary structures, such as lobes, fissures, nodules, and vessels, and bones, such as sternum, vertebrae, and ribs."
MARGRIT BETKE,Salient object subitizing,"We study the problem of salient object subitizing, i.e. predicting the existence and the number of salient objects in an image using holistic cues. This task is inspired by the ability of people to quickly and accurately identify the number of items within the subitizing range (1–4). To this end, we present a salient object subitizing image dataset of about 14 K everyday images which are annotated using an online crowdsourcing marketplace. We show that using an end-to-end trained convolutional neural network (CNN) model, we achieve prediction accuracy comparable to human performance in identifying images with zero or one salient object. For images with multiple salient objects, our model also provides significantly better than chance performance without requiring any localization process. Moreover, we propose a method to improve the training of the CNN subitizing model by leveraging synthetic images. In experiments, we demonstrate the accuracy and generalizability of our CNN subitizing model and its applications in salient object detection and image retrieval."
MARGRIT BETKE,"SAVOIAS: a diverse, multi-category visual complexity dataset","Visual complexity identifies the level of intricacy and details in an image or the level of difficulty to describe the image. It is an important concept in a variety of areas such as cognitive psychology, computer vision and visualization, and advertisement. Yet, efforts to create large, downloadable image datasets with diverse content and unbiased groundtruthing are lacking. In this work, we introduce Savoias, a visual complexity dataset that compromises of more than 1,400 images from seven image categories relevant to the above research areas, namely Scenes, Advertisements, Visualization and infographics, Objects, Interior design, Art, and Suprematism. The images in each category portray diverse characteristics including various low-level and high-level features, objects, backgrounds, textures and patterns, text, and graphics. The ground truth for Savoias is obtained by crowdsourcing more than 37,000 pairwise comparisons of images using the forced-choice methodology and with more than 1,600 contributors. The resulting relative scores are then converted to absolute visual complexity scores using the Bradley-Terry method and matrix completion. When applying five state-of-the-art algorithms to analyze the visual complexity of the images in the Savoias dataset, we found that the scores obtained from these baseline tools only correlate well with crowdsourced labels for abstract patterns in the Suprematism category (Pearson correlation r=0.84). For the other categories, in particular, the objects and advertisement categories, low correlation coefficients were revealed (r=0.3 and 0.56, respectively). These findings suggest that (1) state-of-the-art approaches are mostly insufficient and (2) Savoias enables category-specific method development, which is likely to improve the impact of visual complexity analysis on specific application areas, including computer vision."
MARGRIT BETKE,Perceptual modalities guiding bat flight in a native habitat,"Flying animals accomplish high-speed navigation through fields of obstacles using a suite of sensory modalities that blend spatial memory with input from vision, tactile sensing, and, in the case of most bats and some other animals, echolocation. Although a good deal of previous research has been focused on the role of individual modes of sensing in animal locomotion, our understanding of sensory integration and the interplay among modalities is still meager. To understand how bats integrate sensory input from echolocation, vision, and spatial memory, we conducted an experiment in which bats flying in their natural habitat were challenged over the course of several evening emergences with a novel obstacle placed in their flight path. Our analysis of reconstructed flight data suggests that vision, echolocation, and spatial memory together with the possible exercise of an ability in using predictive navigation are mutually reinforcing aspects of a composite perceptual system that guides flight. Together with the recent development in robotics, our paper points to the possible interpretation that while each stream of sensory information plays an important role in bat navigation, it is the emergent effects of combining modalities that enable bats to fly through complex spaces."
MARGRIT BETKE,How to collect high quality segmentations: use human or computer drawn object boundaries?,"High quality segmentations must be captured consistently for applications such as biomedical image analysis. While human drawn segmentations are often collected because they provide a consistent level of quality, computer drawn segmentations can be collected efficiently and inexpensively. In this paper, we examine how to leverage available human and computer resources to consistently create high quality segmentations. We propose a quality control methodology. We demonstrate how to apply this approach using crowdsourced and domain expert votes for the ""best"" segmentation from a collection of human and computer drawn segmentations for 70 objects from a public dataset and 274 objects from biomedical images. We publicly share the library of biomedical images which includes 1,879 manual annotations of the boundaries of 274 objects. We found for the 344 objects that no single segmentation source was preferred and that human annotations are not always preferred over computer annotations. These results motivated us to examine the traditional approach to evaluate segmentation algorithms, which involves comparing the segmentations produced by the algorithms to manual annotations on benchmark datasets. We found that algorithm benchmarking results change when the comparison is made to consensus-voted segmentations. Our results led us to suggest a new segmentation approach that uses machine learning to predict the optimal segmentation source and a modified segmentation evaluation approach."
MARGRIT BETKE,Camera Canvas: Image Editing Software for People with Disabilities,"We developed Camera Canvas, photo editing and picture drawing software for individuals who cannot use their hands to operate a computer mouse. Camera Canvas is designed for use with camera- based mouse-replacement interfaces that allow a user with severe motion impairments to control the mouse pointer by moving his or her head in front of a web camera. To make Camera Canvas accessible to as wide of a range of movement abilities as possible, we designed its user interface so that it can be extensively tailored to meet individual user needs. We conducted studies with users without disabilities, who used Camera Canvas with the mouse-replacement input system Camera Mouse. The studies showed that Camera Canvas is easy to understand and use, even for participants without prior experience with the Camera Mouse. An experiment with a participant with severe cerebral palsy and quadriplegia showed that he was able to use some but not all of the functionality of Camera Canvas. Ongoing work includes conducting additional user studies and improving the software based on feedback."
MARGRIT BETKE,Detecting frames in news headlines and its application to analyzing news framing trends surrounding U.S. gun violence,"Different news articles about the same topic often offer a variety of perspectives: an article written about gun violence might emphasize gun control, while another might promote 2nd Amendment rights, and yet a third might focus on mental health issues. In communication research, these different perspectives are known as “frames”, which, when used in news media will influence the opinion of their readers in multiple ways. In this paper, we present a method for effectively detecting frames in news headlines. Our training and performance evaluation is based on a new dataset of news headlines related to the issue of gun violence in the United States. This Gun Violence Frame Corpus (GVFC) was curated and annotated by journalism and communication experts. Our proposed approach sets a new state-of-the-art performance for multiclass news frame detection, significantly outperforming a recent baseline by 35.9% absolute difference in accuracy. We apply our frame detection approach in a large scale study of 88k news headlines about the coverage of gun violence in the U.S. between 2016 and 2018."
MARGRIT BETKE,Home-based physical therapy with an interactive computer vision system,"In this paper, we present ExerciseCheck. ExerciseCheck is an interactive computer vision system that is sufficiently modular to work with different sources of human pose estimates, i.e., estimates from deep or traditional models that interpret RGB or RGB-D camera input. In a pilot study, we first compare the pose estimates produced by four deep models based on RGB input with those of the MS Kinect based on RGB-D data. The results indicate a performance gap that required us to choose the MS Kinect when we tested ExerciseCheck with Parkinson’s disease patients in their homes. ExerciseCheck is capable of customizing exercises, capturing exercise information, evaluating patient performance, providing therapeutic feedback to the patient and the therapist, checking the progress of the user over the course of the physical therapy, and supporting the patient throughout this period. We conclude that ExerciseCheck is a user-friendly computer vision application that can assist patients by providing motivation and guidance to ensure correct execution of the required exercises. Our results also suggest that while there has been considerable progress in the field of pose estimation using deep learning, current deep learning models are not fully ready to replace RGB-D sensors, especially when the exercises involved are complex, and the patient population being accounted for has to be carefully tracked for its “active range of motion.”"
MARGRIT BETKE,Deep-learning-driven quantification of interstitial fibrosis in digitized kidney biopsies,"Interstitial fibrosis and tubular atrophy (IFTA) on a renal biopsy are strong indicators of disease chronicity and prognosis. Techniques that are typically used for IFTA grading remain manual, leading to variability among pathologists. Accurate IFTA estimation using computational techniques can reduce this variability and provide quantitative assessment. Using trichrome-stained whole-slide images (WSIs) processed from human renal biopsies, we developed a deep-learning framework that captured finer pathologic structures at high resolution and overall context at the WSI level to predict IFTA grade. WSIs (n = 67) were obtained from The Ohio State University Wexner Medical Center. Five nephropathologists independently reviewed them and provided fibrosis scores that were converted to IFTA grades: ≤10% (none or minimal), 11% to 25% (mild), 26% to 50% (moderate), and >50% (severe). The model was developed by associating the WSIs with the IFTA grade determined by majority voting (reference estimate). Model performance was evaluated on WSIs (n = 28) obtained from the Kidney Precision Medicine Project. There was good agreement on the IFTA grading between the pathologists and the reference estimate (κ = 0.622 ± 0.071). The accuracy of the deep-learning model was 71.8% ± 5.3% on The Ohio State University Wexner Medical Center and 65.0% ± 4.2% on Kidney Precision Medicine Project data sets. Our approach to analyzing microscopic- and WSI-level changes in renal biopsies attempts to mimic the pathologist and provides a regional and contextual estimation of IFTA. Such methods can assist clinicopathologic diagnosis."
MARGRIT BETKE,Perception and steering control in paired bat flight,"Animals within groups need to coordinate their reactions to perceived environmental features and to each other in order to safely move from one point to another. This paper extends our previously published work on the flight patterns of Myotis velifer that have been observed in a habitat near Johnson City, Texas. Each evening, these bats emerge from a cave in sequences of small groups that typically contain no more than three or four individuals, and they thus provide ideal subjects for studying leader-follower behaviors. By analyzing the flight paths of a group of M. velifer, the data show that the flight behavior of a follower bat is influenced by the flight behavior of a leader bat in a way that is not well explained by existing pursuit laws, such as classical pursuit, constant bearing and motion camouflage. Thus we propose an alternative steering law based on virtual loom, a concept we introduce to capture the geometrical configuration of the leader-follower pair. It is shown that this law may be integrated with our previously proposed vision-enabled steering laws to synthesize trajectories, the statistics of which fit with those of the bats in our data set. The results suggest that bats use perceived information of both the environment and their neighbors for navigation."
MARGRIT BETKE,Age-constrained ear recognition: the EICZA dataset and SASE baseline model,
MARGRIT BETKE,Menu controller: making existing software more accessible for people with motor impairments,"Menu Controller was developed to make existing software more accessible for people with severe motor impairments, especially individuals who use mouse-replacement input systems. Windows applications have menus that are difficult to access by users with limited muscle control, due to the size and placement of the menu entries. The goal of Menu Controller is to take these entries and generate customizable user interfaces that can be catered to the individual user. Menu Controller accomplishes this by harvesting existing menu items without needing to change any existing code in these applications and then by displaying them to the user in an external toolbar that is more easily accessible to people with impairments. The initial challenge in developing Menu Controller was to find a method for harvesting and re-displaying menu items by using the Windows API. The rest of the work involved exploring an appropriate way for displaying the harvested menu entries. We ultimately chose an approach based on a two-level sliding toolbar. Experiments with a user with severe motor impairments, who used the Camera Mouse as a mouse-replacement input system, showed that this approach was indeed promising. The experiments also exposed areas that need further research and development. We suggest that Menu Controller provides a valuable contribution towards making everyday software more accessible to people with disabilities."
MARGRIT BETKE,Camera canvas: image editing software for people with disabilities,"We developed Camera Canvas, photo editing and picture drawing software for individuals who cannot use their hands to operate a computer mouse. Camera Canvas is designed for use with camera-based mouse-replacement interfaces that allow a user with severe motion impairments to control the mouse pointer by moving his or her head in front of a web camera. To make Camera Canvas accessible to as wide of a range of movement abilities as possible, we designed its user interface so that it can be extensively tailored to meet individual user needs. We conducted studies with users without disabilities, who used Camera Canvas with the mouse-replacement input system Camera Mouse. The studies showed that Camera Canvas is easy to understand and use, even for participants without prior experience with the Camera Mouse. An experiment with a participant with severe cerebral palsy and quadriplegia showed that he was able to use some but not all of the functionality of Camera Canvas. Ongoing work includes conducting additional user studies and improving the software based on feedback."
MARGRIT BETKE,Dynamic allocation of crowd contributions for sentiment analysis during the 2016 U.S. presidential election,"Opinions about the 2016 U.S. Presidential Candidates have been expressed in millions of tweets that are challenging to analyze automatically. Crowdsourcing the analysis of political tweets effectively is also difficult, due to large inter-rater disagreements when sarcasm is involved. Each tweet is typically analyzed by a fixed number of workers and majority voting. We here propose a crowdsourcing framework that instead uses a dynamic allocation of the number of workers. We explore two dynamic-allocation methods: (1) The number of workers queried to label a tweet is computed offline based on the predicted difficulty of discerning the sentiment of a particular tweet. (2) The number of crowd workers is determined online, during an iterative crowd sourcing process, based on inter-rater agreements between labels.We applied our approach to 1,000 twitter messages about the four U.S. presidential candidates Clinton, Cruz, Sanders, and Trump, collected during February 2016. We implemented the two proposed methods using decision trees that allocate more crowd efforts to tweets predicted to be sarcastic. We show that our framework outperforms the traditional static allocation scheme. It collects opinion labels from the crowd at a much lower cost while maintaining labeling accuracy."
MARGRIT BETKE,A protocol and calibration method for accurate multi-camera field videography,"Stereo videography is a powerful technique for quantifying the kinematics and behavior of animals, but it can be challenging to use in an outdoor field setting. We here present a workflow and associated software for performing calibration of cameras placed in a field setting and estimating the accuracy of the resulting stereoscopic reconstructions. We demonstrate the workflow through example stereoscopic reconstructions of bat and bird flight. We provide software tools for planning experiments and processing the resulting calibrations that other researchers may use to calibrate their own cameras. Our field protocol can be deployed in a single afternoon, requiring only short video clips of light, portable calibration objects."
MARGRIT BETKE,Prediction of people’s emotional response towards multi-modal news,"We aim to develop methods for understanding how multimedia news exposure can affect people’s emotional responses, and we especially focus on news content related to gun violence, a very important yet polarizing issue in the U.S. We created the dataset NEmo+ by significantly extending the U.S. gun violence news-to-emotions dataset, BU-NEmo, from 320 to 1,297 news headline and lead image pairings and collecting 38,910 annotations in a large crowdsourcing experiment. In curating the NEmo+ dataset, we developed methods to identify news items that will trigger similar versus divergent emotional responses. For news items that trigger similar emotional responses, we compiled them into the NEmo+-Consensus dataset. We benchmark models on this dataset that predict a person’s dominant emotional response toward the target news item (single-label prediction). On the full NEmo+ dataset, containing news items that would lead to both differing and similar emotional responses, we also benchmark models for the novel task of predicting the distribution of evoked emotional responses in humans when presented with multi-modal news content. Our single-label and multi-label prediction models outperform baselines by large margins across several metrics."
MARGRIT BETKE,BU-NEmo: news and emotions dataset,"BU-NEmo is a multimodal affective dataset of gun violence news content. BU-NEmo extends the Gun Violence Framing Corpus (GVFC) proposed by Liu et. al (2019) and Tourni et. al (2021), which contains pairs of news headlines and lead images and their ""frames"" (view points) from gun violence-related articles. The extension concerns the results of an annotation experiment that evaluates the effect of the news content on the emotions of news consumers. The data in BU-NEmo are annotated with three types of affective annotations: (1) The emotion the annotator feels from looking at the content, out of the following 8 classes: Amusement, Awe, Contentment, Excitement, Fear, Sadness, Anger, Disgust. (2) The intensity of the annotator's emotional response, on a scale from 1-5 (5 being the most intense). (3) A free-text written response explaining their emotional response, structured as ""I feel because."" These annotations were collected in three experimental conditions: only the headline text was presented, only the image, and text and image together. By comparing the annotations across these three conditions, the relationship between news modality, frames, and emotional response can be studied."
MARGRIT BETKE,BUOCA: Budget-Optimized Crowd Worker Allocation,"Due to concerns about human error in crowdsourcing, it is standard practice to collect labels for the same data point from multiple internet workers. We here show that the resulting budget can be used more effectively with a flexible worker assignment strategy that asks fewer workers to analyze easy-to-label data and more workers to analyze data that requires extra scrutiny. Our main contribution is to show how the allocations of the number of workers to a task can be computed optimally based on task features alone, without using worker profiles. Our target tasks are delineating cells in microscopy images and analyzing the sentiment toward the 2016 U.S. presidential candidates in tweets. We first propose an algorithm that computes budget-optimized crowd worker allocation (BUOCA). We next train a machine learning system (BUOCA-ML) that predicts an optimal number of crowd workers needed to maximize the accuracy of the labeling. We show that the computed allocation can yield large savings in the crowdsourcing budget (up to 49 percent points) while maintaining labeling accuracy. Finally, we envisage a human-machine system for performing budget-optimized data analysis at a scale beyond the feasibility of crowdsourcing."
MARGRIT BETKE,Tracking a Large Number of Objects from Multiple Views,"We propose a multi-object multi-camera framework for tracking large numbers of tightly-spaced objects that rapidly move in three dimensions. We formulate the problem of finding correspondences across multiple views as a multidimensional assignment problem and use a greedy randomized adaptive search procedure to solve this NP-hard problem efficiently. To account for occlusions, we relax the one-to-one constraint that one measurement corresponds to one object and iteratively solve the relaxed assignment problem. After correspondences are established, object trajectories are estimated by stereoscopic reconstruction using an epipolar-neighborhood search. We embedded our method into a tracker-to-tracker multi-view fusion system that not only obtains the three-dimensional trajectories of closely-moving objects but also accurately settles track uncertainties that could not be resolved from single views due to occlusion. We conducted experiments to validate our greedy assignment procedure and our technique to recover from occlusions. We successfully track hundreds of flying bats and provide an analysis of their group behavior based on 150 reconstructed 3D trajectories."
MARGRIT BETKE,HAIL,"We present a framework to adapt software to the needs of individuals with severe motion disabilities who use mouse substitution interfaces. Typically, users are required to adapt to the interfaces that they wish to use. We propose interfaces that change and adapt to the user and their individual abilities. The Hierarchical Adaptive Interface Layout (HAIL) model is a set of specifications for the design of user interface applications that adapt to the user. In HAIL applications, all of the interactive components take place on configurable toolbars along the edge of the screen. We show two HAIL-based applications: a general purpose web browser and a Twitter client."
MARGRIT BETKE,Adaptive mappings for mouse-replacement interfaces,Users of mouse-replacement interfaces may have difficulty conforming to the motion requirements of their interfacesystem. We have observed users with severe motor disabilities who controlled the mouse pointer with a head tracking interface. Our analysis shows that some users may be able to move in some directions easier than other directions. We propose several mouse pointer mappings that adapt to the user's movement abilities. These mappings will take into account the user's motions in two-or three-dimensions to move the mouse pointer in the intended direction.
MARGRIT BETKE,Customizable Keyboard,Customizable Keyboard is an on-screen keyboard designed to be flexible and expandable. Instead of giving the user a keyboard layout Customizable Keyboard allows the user to create a layout that is accommodating to the user's needs. Customizable Keyboard also allows the user to select from a variety of ways to interact with the keyboard including but not limited to using the mouse pointer to select keys and different types of scan based systems. Customizable Keyboard provides more functionality than a typical onscreen keyboard including the ability to control infrared devices such as TVs and send Twitter Tweets.
MARGRIT BETKE,Tracking-Reconstruction or Reconstruction-Tracking?,"We developed two methods for tracking multiple objects using several camera views. The methods use the Multiple Hypothesis Tracking (MHT) framework to solve both the across-view data association problem (i.e., finding object correspondences across several views) and the across-time data association problem (i.e., the assignment of current object measurements to previously established object tracks). The ""tracking-reconstruction method"" establishes two-dimensional (2D) objects tracks for each view and then reconstructs their three-dimensional (3D) motion trajectories. The ""reconstruction-tracking method"" assembles 2D object measurements from all views, reconstructs 3D object positions, and then matches these 3D positions to previously established 3D object tracks to compute 3D motion trajectories. For both methods, we propose techniques for pruning the number of association hypotheses and for gathering track fragments. We tested and compared the performance of our methods on thermal infrared video of bats using several performance measures. Our analysis of video sequences with different levels of densities of flying bats reveals that the reconstruction-tracking method produces fewer track fragments than the tracking-reconstruction method but creates more false positive 3D tracks."
MARGRIT BETKE,Reconstruction and analysis of 3D trajectories of Brazilian free-tailed bats in flight,"The Brazilian free-tailed bat, Tadarida brasiliensis, roosts in very large colonies, consisting of hundreds of thousands of individuals. Each night, bats emerge from their day roosts in dense columns in a highly coordinated manner. We recorded short segments of an emergence using three spatially-calibrated and temporally-synchronized thermal infrared cameras. We applied stereoscopic methods to reconstruct the three-dimensional positions of these flying bats. We applied a multiple hypothesis tracking algorithm to obtain 7,016 reconstructed trajectories. Our analysis includes estimates of the velocities of bats in flight, the distances between animals within the emergence column, and the angles subtended by the bats and their nearest neighbors."
MARGRIT BETKE,RefLink: An Interface that Enables People with Motion Impairments to Analyze Web Content and Dynamically Link to References,"We present RefLink, an interface that allows users to analyze the content of webpages by dynamically linking to an online encyclopedia such as Wikipedia. Upon opening a webpage, RefLink instantly provides a list of terms extracted from the webpage and annotates each term by the number of its occurrences in the page. RefLink uses the text-to-speech interface to read out the list of terms. The user can select a term of interest and follow its link to the encyclopedia. RefLink thus helps the users to perform an informed and efficient contextual analysis. Initial user testing suggests that RefLink is a valuable web browsing tool, in particular for people with motion impairments, because it simplifies the process of obtaining reference material and performing contextual analysis."
MARGRIT BETKE,Automating image analysis by annotating landmarks with deep neural networks,"Image and video analysis is often a crucial step in the study of animal behavior and kinematics. Often these analyses require that the position of one or more animal landmarks are annotated (marked) in numerous images. The process of annotating landmarks can require a significant amount of time and tedious labor, which motivates the need for algorithms that can automatically annotate landmarks. In the community of scientists that use image and video analysis to study the 3D flight of animals, there has been a trend of developing more automated approaches for annotating landmarks, yet they fall short of being generally applicable. Inspired by the success of Deep Neural Networks (DNNs) on many problems in the field of computer vision, we investigate how suitable DNNs are for accurate and automatic annotation of landmarks in video datasets representative of those collected by scientists studying animals. Our work shows, through extensive experimentation on videos of hawkmoths, that DNNs are suitable for automatic and accurate landmark localization. In particular, we show that one of our proposed DNNs is more accurate than the current best algorithm for automatic localization of landmarks on hawkmoth videos. Moreover, we demonstrate how these annotations can be used to quantitatively analyze the 3D flight of a hawkmoth. To facilitate the use of DNNs by scientists from many different fields, we provide a self contained explanation of what DNNs are, how they work, and how to apply them to other datasets using the freely available library Caffe and supplemental code that we provide."
MARGRIT BETKE,Scraping social media photos posted in Kenya and elsewhere to detect and analyze food types,"Monitoring population-level changes in diet could be useful for education and for implementing interventions to improve health. Research has shown that data from social media sources can be used for monitoring dietary behavior. We propose a scrape-by-location methodology to create food image datasets from Instagram posts. We used it to collect 3.56 million images over a period of 20 days in March 2019. We also propose a scrape-by-keywords methodology and used it to scrape ∼30,000 images and their captions of 38 Kenyan food types. We publish two datasets of 104,000 and 8,174 image/caption pairs, respectively. With the first dataset, Kenya104K, we train a Kenyan Food Classifier, called KenyanFC, to distinguish Kenyan food from non-food images posted in Kenya. We used the second dataset, KenyanFood13, to train a classifier KenyanFTR, short for Kenyan Food Type Recognizer, to recognize 13 popular food types in Kenya. The KenyanFTR is a multimodal deep neural network that can identify 13 types of Kenyan foods using both images and their corresponding captions. Experiments show that the average top-1 accuracy of KenyanFC is 99% over 10,400 tested Instagram images and of KenyanFTR is 81% over 8,174 tested data points. Ablation studies show that three of the 13 food types are particularly difficult to categorize based on image content only and that adding analysis of captions to the image analysis yields a classifier that is 9 percent points more accurate than a classifier that relies only on images. Our food trend analysis revealed that cakes and roasted meats were the most popular foods in photographs on Instagram in Kenya in March 2019."
MARGRIT BETKE,The impact of ear growth on identification rates using an ear biometric system in young infants,"BACKGROUND: Accurate patient identification is essential for delivering longitudinal care. Our team developed an ear biometric system (SEARCH) to improve patient identification. To address how ear growth affects matching rates longitudinally, we constructed an infant cohort, obtaining ear image sets monthly to map a 9-month span of observations. This analysis had three main objectives: 1) map trajectory of ear growth during the first 9 months of life; 2) determine the impact of ear growth on matching accuracy; and 3) explore computer vision techniques to counter a loss of accuracy.   METHODOLOGY: Infants were enrolled from an urban clinic in Lusaka, Zambia. Roughly half were enrolled at their first vaccination visit and ~half at their last vaccination. Follow-up visits for each patient occurred monthly for 6 months. At each visit, we collected four images of the infant’s ears, and the child’s weight. We analyze ear area versus age and change in ear area versus age. We conduct pair-wise comparisons for all age intervals. RESULTS: From 227 enrolled infants we acquired age-specific datasets for 6 days through 9 months. Maximal ear growth occurred between 6 days and 14 weeks. Growth was significant until 6 months of age, after which further growth appeared minimal. Examining look-back performance to the 6-month visit, baseline pair-wise comparisons yielded identification rates that ranged 46.9–75%. Concatenating left and right ears per participant improved identification rates to 61.5–100%. Concatenating images captured on adjacent visits further improved identification rates to 90.3–100%. Lastly, combining these two approaches improved identification to 100%. All matching strategies showed the weakest matching rates during periods of maximal growth (i.e., <6 months). CONCLUSION: By quantifying the effect that ear growth has on performance of the SEARCH platform, we show that ear identification is a feasible solution for patient identification in an infant population 6 months and above."
MARGRIT BETKE,Discovering useful parts for pose estimation in sparsely annotated datasets,"Our work introduces a novel way to increase pose estimation accuracy by discovering parts from unannotated regions of training images. Discovered parts are used to generate more accurate appearance likelihoods for traditional part-based models like Pictorial Structures and its derivatives. Our experiments on images of a hawkmoth in flight show that our proposed approach significantly improves over existing work for this application, while also being more generally applicable. Our proposed approach localizes landmarks at least twice as accurately as a baseline based on a Mixture of Pictorial Structures (MPS) model. Our unique High-Resolution Moth Flight (HRMF) dataset is made publicly available with annotations."
MARGRIT BETKE,Adaptive mouse-replacement interface control functions for users with disabilities,"We discuss experiences employing a video-based mouse-replacement interface system, the Camera Mouse, at care facilities for individuals with severe motion impairments and propose adaptations of the system. Traditional approaches to assistive technology are often inflexible, requiring users to adapt their limited motions to the requirements of the system. Such systems may have static or difficult-to-change configurations that make it challenging for multiple users to share the same system or for users whose motion abilities slowly degenerate. As users fatigue, they may experience more limited motion ability or additional unintended motions. To address these challenges, we propose adaptive mouse-control functions to be used in our mouse-replacement system. These functions can be changed to adapt the technology to the needs of the user, rather than making the user adapt to the technology. We present observations of an individual with severe cerebral palsy using our system."
MARGRIT BETKE,A unified framework for domain adaptive pose estimation,"While pose estimation is an important computer vision task, it requires expensive annotation and suffers from domain shift. In this paper, we investigate the problem of domain adaptive 2D pose estimation that transfers knowledge learned on a synthetic source domain to a target domain without supervision. While several domain adaptive pose estimation models have been proposed recently, they are not generic but only focus on either human pose or animal pose estimation, and thus their effectiveness is somewhat limited to specific scenarios. In this work, we propose a unified framework that generalizes well on various domain adaptive pose estimation problems. We propose to align representations using both input-level and output-level cues (pixels and pose labels, respectively), which facilitates the knowledge transfer from the source domain to the unlabeled target domain. Our experiments show that our method achieves state-of-the-art performance under various domain shifts. Our method outperforms existing baselines on human pose estimation by up to 4.5 percent points (pp), hand pose estimation by up to 7.4 pp, and animal pose estimation by up to 4.8 pp for dogs and 3.3 pp for sheep. These results suggest that our method is able to mitigate domain shift on diverse tasks and even unseen domains and objects (e.g., trained on horse and tested on dog). Our code will be publicly available at: https://github.com/VisionLearningGroup/UDA_PoseEstimation."
MARGRIT BETKE,HGaze Typing: head-gesture assisted gaze typing,"This paper introduces a bi-modal typing interface, HGaze Typing, which combines the simplicity of head gestures with the speed of gaze inputs to provide efficient and comfortable dwell-free text entry. HGaze Typing uses gaze path information to compute candidate words and allows explicit activation of common text entry commands, such as selection, deletion, and revision, by using head gestures (nodding, shaking, and tilting). By adding a head-based input channel, HGaze Typing reduces the size of the screen regions for cancel/deletion buttons and the word candidate list, which are required by most eye-typing interfaces. A user study finds HGaze Typing outperforms a dwell-time-based keyboard in efficacy and user satisfaction. The results demonstrate that the proposed method of integrating gaze and head-movement inputs can serve as an effective interface for text entry and is robust to unintended selections."
MARGRIT BETKE,LAL: linguistically aware learning for scene text recognition,"Scene text recognition is the task of recognizing character sequences in images of natural scenes. The considerable diversity in the appearance of text in a scene image and potentially highly complex backgrounds make text recognition challenging. Previous approaches employ character sequence generators to analyze text regions and, subsequently, compare the candidate character sequences against a language model. In this work, we propose a bimodal framework that simultaneously utilizes visual and linguistic information to enhance recognition performance. Our linguistically aware learning (LAL) method effectively learns visual embeddings using a rectifier, encoder, and attention decoder approach, and linguistic embeddings, using a deep next-character prediction model. We present an innovative way of combining these two embeddings effectively. Our experiments on eight standard benchmarks show that our method outperforms previous methods by large margins, particularly on rotated, foreshortened, and curved text. We show that the bimodal approach has a statistically significant impact. We also contribute a new dataset, and show robust performance when LAL is combined with a text detector in a pipelined text spotting framework."
MARGRIT BETKE,Learning to separate: detecting heavily-occluded objects in urban scenes,"While visual object detection with deep learning has received much attention in the past decade, cases when heavy intra-class occlusions occur have not been studied thoroughly. In this work, we propose a Non-Maximum-Suppression (NMS) algorithm that dramatically improves the detection recall while maintaining high precision in scenes with heavy occlusions. Our NMS algorithm is derived from a novel embedding mechanism, in which the semantic and geometric features of the detected boxes are jointly exploited. The embedding makes it possible to determine whether two heavily-overlapping boxes belong to the same object in the physical world. Our approach is particularly useful for car detection and pedestrian detection in urban scenes where occlusions often happen. We show the effectiveness of our approach by creating a model called SG-Det (short for Semantics and Geometry Detection) and testing SG-Det on two widely-adopted datasets, KITTI and CityPersons for which it achieves state-of-the-art performance. Our code is available at https://github.com/ChenhongyiYang/SG-NMS."
MARGRIT BETKE,Feature analysis and extraction for post aphasia recovery prediction,
MARGRIT BETKE,An unsupervised approach to discover media frames,"Media framing refers to highlighting certain aspect of an issue in the news to promote a particular interpretation to the audience. Supervised learning has often been used to recognize frames in news articles, requiring a known pool of frames for a particular issue, which must be identified by communication researchers through thorough manual content analysis. In this work, we devise an unsupervised learning approach to discover the frames in news articles automatically. Given a set of news articles for a given issue, e.g., gun violence, our method first extracts frame elements from these articles using related Wikipedia articles and the Wikipedia category system. It then uses a community detection approach to identify frames from these frame elements. We discuss the effectiveness of our approach by comparing the frames it generates in an unsupervised manner to the domain-expert-derived frames for the issue of gun violence, for which a supervised learning model for frame recognition exists."
MARGRIT BETKE,Community detection of the framing element network: proposing and assessing a new computational framing analysis approach,"The evolving computational news framing detection has been a prominent yet contested field among mass communication scholars. This study explores a new approach to identifying frames as clusters of framing elements including actors (i.e., individual and organizational entities) and topics in news articles based on the community detection algorithm. Our approach highlights the fundamental importance of considering individual and organizational actors mentioned in news articles as components of frames, which is overlooked in previous research that uses a similar unsupervised approach. We evaluate the performance of our method by comparing it with one of the most popular unsupervised methods--LDA topic modeling--and a state-of-art deep learning method, BERT, based on 2,900 US gun violence news articles."
MARGRIT BETKE,"NEmo – news that triggers emotions, an affectively-annotated dataset of gun violence news","Given our society’s increased exposure to multimedia formats on social media platforms, efforts to understand how digital content impacts people’s emotions are burgeoning. As such, we introduce a U.S. gun violence news dataset that contains news headline and image pairings from 840 news articles with 15K high-quality, crowdsourced annotations on emotional responses to the news pairings. We created three experimental conditions for the annotation process: two with a single modality (headline or image only), and one multimodal (headline and image together). In contrast to prior works on affectively-annotated data, our dataset includes annotations on the dominant emotion experienced with the content, the intensity of the selected emotion and an open-ended, written component. By collecting annotations on different modalities of the same news content pairings, we explore the relationship between image and text influence on human emotional response. We offer initial analysis on our dataset, showing the nuanced affective differences that appear due to modality and individual factors such as political leaning and media consumption habits. Our dataset is made publicly available to facilitate future research in affective computing."
MARGRIT BETKE,SIDOD: a synthetic image dataset for 3D object pose recognition with distractors,"We present a new, publicly-available image dataset generated by the NVIDIA Deep Learning Data Synthesizer intended for use in object detection, pose estimation, and tracking applications. This dataset contains 144k stereo image pairs that synthetically combine 18 camera viewpoints of three photorealistic virtual environments with up to 10 objects (chosen randomly from the 21 object models of the YCB dataset ) and flying distractors. Object and camera pose, scene lighting, and quantity of objects and distractors were randomized. Each provided view includes RGB, depth, segmentation, and surface normal images, all pixel level. We describe our approach for domain randomization and provide insight into the decisions that produced the dataset."
MARGRIT BETKE,OpenFraming: open-sourced tool for computational framing analysis of multilingual data,"When journalists cover a news story, they can cover the story from multiple angles or perspectives. These perspectives are called “frames,” and usage of one frame or another may influence public perception and opinion of the issue at hand. We develop a web-based system for analyzing frames in multilingual text documents. We propose and guide users through a five-step end-to-end computational framing analysis framework grounded in media framing theory in communication research. Users can use the framework to analyze multilingual text data, starting from the exploration of frames in user’s corpora and through review of previous framing literature (step 1-3) to frame classification (step 4) and prediction (step 5). The framework combines unsupervised and supervised machine learning and leverages a state-of-the-art (SoTA) multilingual language model, which can significantly enhance frame prediction performance while requiring a considerably small sample of manual annotations. Through the interactive website, anyone can perform the proposed computational framing analysis, making advanced computational analysis available to researchers without a programming background and bridging the digital divide within the communication research discipline in particular and the academic community in general. The system is available online at http://www.openframing.org, via an API http://www.openframing.org:5000/docs, or through our GitHub page https://github.com/vibss2397/openFraming."
MARGRIT BETKE,OpenFraming: we brought the ML; you bring the data. Interact with your data and discover its frames,"When journalists cover a news story, they can cover the story from multiple angles or perspectives. A news article written about COVID-19 for example, might focus on personal preventative actions such as mask-wearing, while another might focus on COVID-19's impact on the economy. These perspectives are called ""frames,"" which when used may influence public perception and opinion of the issue. We introduce a Web-based system for analyzing and classifying frames in text documents. Our goal is to make effective tools for automatic frame discovery and labeling based on topic modeling and deep learning widely accessible to researchers from a diverse array of disciplines. To this end, we provide both state-of-the-art pre-trained frame classification models on various issues as well as a user-friendly pipeline for training novel classification models on user-provided corpora. Researchers can submit their documents and obtain frames of the documents. The degree of user involvement is flexible: they can run models that have been pre-trained on select issues; submit labeled documents and train a new model for frame classification; or submit unlabeled documents and obtain potential frames of the documents. The code making up our system is also open-sourced and well-documented, making the system transparent and expandable. The system is available on-line at http://www.openframing.org and via our GitHub page https://github.com/davidatbu/openFraming ."
MARGRIT BETKE,Prediction of student engagement,"A major challenge for online learning is the inability of systems to support student emotion and to maintain student engagement. In response to this challenge, computer vision has become an embedded feature in some instructional applications. In this paper, we propose a video dataset of college students solving math problems on the educational platform MathSpring.org with a front facing camera collecting visual feedback of student gestures. The video dataset is annotated to indicate whether students’ attention at specific frames is engaged or wandering. In addition, we train baselines for a computer vision module that determines the extent of student engagement during remote learning. Baselines include state-of-the-art deep learning image classifiers and traditional conditional and logistic regression for head pose estimation. We then incorporate a gaze baseline into the MathSpring learning platform, and we are evaluating its performance with the currently implemented approach."
MARGRIT BETKE,BU-CVKit: extendable computer vision framework for species independent tracking and analysis,
MARGRIT BETKE,Concordance between chest X ray (CXR) and point of care ultrasound (POCUS) findings in children diagnosed with RSV infection by nasopharyngeal RT-PCR: the Zambia experience,
MARGRIT BETKE,ATL-BP: a student engagement dataset and model for affect transfer learning for behavior prediction,
MARGRIT BETKE,COVES: a cognitive-affective deep model that personalizes math problem difficulty in real time and improves student engagement with an online tutor,
MARGRIT BETKE,Multimodal neural and behavioral data predict response to rehabilitation in chronic post-stroke aphasia,"BACKGROUND: Poststroke recovery depends on multiple factors and varies greatly across individuals. Using machine learning models, this study investigated the independent and complementary prognostic role of different patient-related factors in predicting response to language rehabilitation after a stroke. METHODS: Fifty-five individuals with chronic poststroke aphasia underwent a battery of standardized assessments and structural and functional magnetic resonance imaging scans, and received 12 weeks of language treatment. Support vector machine and random forest models were constructed to predict responsiveness to treatment using pretreatment behavioral, demographic, and structural and functional neuroimaging data. RESULTS: The best prediction performance was achieved by a support vector machine model trained on aphasia severity, demographics, measures of anatomic integrity and resting-state functional connectivity (F1=0.94). This model resulted in a significantly superior prediction performance compared with support vector machine models trained on all feature sets (F1=0.82, P<0.001) or a single feature set (F1 range=0.68–0.84, P<0.001). Across random forest models, training on resting-state functional magnetic resonance imaging connectivity data yielded the best F1 score (F1=0.87). CONCLUSIONS: While behavioral, multimodal neuroimaging data and demographic information carry complementary information in predicting response to rehabilitation in chronic poststroke aphasia, functional connectivity of the brain at rest after stroke is a particularly important predictor of responsiveness to treatment, both alone and combined with other patient-related factors."
JORGE DELVA,The role of social relationships in the association between adolescents' depressive symptoms and academic achievement,"While research has established that depression interferes with academic achievement, less is understood about the processes by which social relationships may buffer the relationship between depression and academic outcomes. In this study we examined the role of positive relationships in the school, family and peer contexts in the association between depressive symptoms and academic achievement among 894 adolescents aged 12-17 years living in Santiago, Chile. Depressive symptoms were associated with lower levels of academic achievement; parental monitoring, school belonging, positive mother relationships, and having academically inclined peers moderated this relationship, though some interactions differed by sex and age. Implications for promoting the academic success of adolescents experiencing depressive symptoms are discussed"
JORGE DELVA,Estimating the heterogeneous relationship between peer drinking and youth alcohol consumption in Chile using propensity score stratification,"When estimating the association between peer and youth alcohol consumption, it is critical to account for possible differential levels of response to peer socialization processes across youth, in addition to variability in individual, family, and social factors. Failure to account for intrinsic differences in youth's response to peers may pose a threat of selection bias. To address this issue, we used a propensity score stratification method to examine whether the size of the association between peer and youth drinking is contingent upon differential predicted probabilities of associating with alcohol-consuming friends. Analyzing a Chilean youth sample (N = 914) of substance use, we found that youths are susceptible to the detrimental role of peer drinkers, but the harmful relationship with one's own drinking behavior may be exacerbated among youth who already have a high probability of socializing with peers who drink. In other words, computing a single weighted-average estimate for peer drinking would have underestimated the detrimental role of peers, particularly among at-risk youths, and overestimated the role of drinking peers among youths who are less susceptible to peer socialization processes. Heterogeneous patterns in the association between peer and youth drinking may shed light on social policies that target at-risk youths."
JORGE DELVA,Behavior problems among adolescents exposed to family and community violence in Chile,"Research that simultaneously examines the relationship of multiple types of family and community violence with adolescent outcomes is limited in the previous research literature, particularly in Latin America. This study examines the relationship of adolescent exposure to family and community violence—parental use of corporal punishment, violence in the community, intimate partner physical aggression—with eight subscales of the Youth Self Report among a Chilean sample of 593 adolescent–mother pairs. Results from multilevel models indicated a positive association between adolescent exposure to violence in the family and community, and a wide range of behavior problem outcomes, in particular, aggression. With growing evidence concerning the detrimental effect of violence on adolescent well‐being, these findings emphasize the need for a more comprehensive understanding of the various kinds of violence adolescents are exposed to within the family and community and the concomitant need to reduce multiple forms of violence."
JORGE DELVA,Associations of maternal and adolescent religiosity and spirituality with adolescent alcohol use in Chile: implications for social work practice,"To inform social work practice with adolescents who may consume alcohol, we examined if alcohol use among Chilean adolescents varied as a function of their mothers’ and their own religiosity and spirituality. Data were from 787 Chilean adolescents and their mothers. Adolescent spirituality was a protective factor against more deleterious alcohol use. Parental monitoring and alcohol using opportunities mediated the associations. The practice of religious behaviors by themselves without meaningful faith were not associated with alcohol use among adolescents. Implications for social work practice are discussed."
JORGE DELVA,"Parental perceptions of neighborhood effects in Latino comunas: the script of ""the delinquent"" in understanding drug use, violence, and social disorganization","OBJECTIVES: To obtain rich information about how adult Latinos living in high-poverty/high-drug use neighborhoods perceive and negotiate their environment. METHODS: In 2008, thirteen adult caregivers in Santiago, Chile were interviewed with open-ended questions to ascertain beliefs about neighborhood effects and drug use. ANALYSIS: Inductive analysis was used to develop the codebook/identify trends. DISCUSSION: Residents externalized their understanding of drug use and misuse by invoking the concept of delinquent youth. A typology of their perceptions is offered. Learning more about residents’ circumstances may help focus on needs-based interventions. More research with Latino neighborhoods is needed for culturally-competent models of interventions."
JORGE DELVA,Volatile substance misuse among high school students in South America,"This article summarizes data from a 2004 study of over 300,000 high school students (aged 13–18 years) in nine South American countries. A probabilistic sample targeted urban secondary schools, utilizing a self-administered questionnaire on prevalence and frequency of substance use. Multivariate analysis showed that volatile substances were the first or second most commonly reported substances used after alcohol and cigarettes in all countries (lifetime prevalence range: 2.67% [Paraguay] to 16.55% [Brazil]). Previous studies have highlighted volatile substance misuse among street children, whereas this study demonstrates that it is common among South American high school students."
JORGE DELVA,"Differences in service utilization and barriers among Blacks, Hispanics, and Whites with drug use disorders","BACKGROUND: Treatment for drug use disorders (DUD) can be effective, but only a small proportion of people with DUD seek or receive treatment. Research on racial and ethnic treatment differences and disparities remains unclear. Understanding racial and ethnic differences and disparities in drug treatment is necessary in order to develop a more effective referral system and to improve the accessibility of treatment. The purpose of the current study was to explore the role of race and ethnicity in service utilization. METHODS: Using data from the National Epidemiologic Survey on Alcohol and Related Conditions (NESARC), this study examined racial and ethnic differences in use of 14 types of treatment services for DUD and 27 different treatment barriers among persons who met lifetime criteria for a DUD. Multivariate logistic regression analyses were used to examine service utilization and barriers among the racial and ethnic groups, while adjusting for other sociodemographic and clinical variables. RESULTS AND DISCUSSION: Among Blacks, Hispanics and Whites in the overall NESARC sample, approximately 10.5% met criteria for at least one lifetime drug use disorder. Approximately 16.2% of persons with a lifetime DUD received at least one type of service. Overall, this study indicated that Whites were less likely to report receiving help for drug-related problems than Blacks, Blacks used a greater number of different types of services, and no racial and ethnic differences were observed with respect to perceived barriers to drug treatment. However, by examining types of services separately, a complex picture of racial and ethnic differences emerges. Most notably, Whites were most likely to use professional services, whereas Blacks were most likely to use 12-step and clergy. The service use pattern of Hispanics most resembled that of Whites. CONCLUSION: While structural barriers to accessing treatment were observed, broad-based educational programs and interventions that are appropriately targeted to racial and ethnic groups remains an important area for prevention and treatment."
JORGE DELVA,Perception of neighborhood crime and drugs Increases cardiometabolic risk in Chilean adolescents,"PURPOSE: Studies report an association between neighborhood risk and both obesity and cardiometabolic risk factors (CMR) among adolescents. Here we describe the effect of perceived neighborhood risk on adiposity and CMR among Chilean adolescents. METHODS: Participants were 523 low- to middle-income Chilean adolescents. We assessed neighborhood risk in early adolescence, adiposity in childhood and in early and later adolescence, and blood pressure and fasting glucose in later adolescence. Neighborhood risk profiles were estimated using latent profile analysis (LPA) and based on reported perceptions of crime and drug sales/use. Using linear and logistic regression, we examined the effect of neighborhood risk on adiposity and CMR. RESULTS: Mean age in early and later adolescence was 14 and 17 years, respectively. Participants were 52% male, with a mean BMI z-score of .67, and 8% met criteria for metabolic syndrome. LPA identified two neighborhood profiles: 61% low risk and 39% high risk. In later adolescence, being in the high risk profile predicted a higher BMI z-score, waist-to-height ratio, and fat mass index (p < .05). Adolescents in the high risk profile had three times greater odds of meeting criteria for metabolic syndrome (OR = 3.1, 95% CI: 1.5, 5.8) compared with those in the low risk profile. CONCLUSIONS: Our findings suggest that there are physiological responses to living in a neighborhood perceived as ""risky,"" which may contribute to obesity and CMR even in adolescence. For Chilean neighborhoods with high crime and drugs, targeted public health interventions and policies for youth could be beneficial."
JORGE DELVA,Eating disorders among a community-based sample of Chilean female adolescents,"The purpose of this study was to explore the prevalence and correlates of eating disorders among a community-based sample of female Chilean adolescents. Data were collected through structured interviews with 420 female adolescents residing in Santiago, Chile. Approximately 4% of the sample reported ever being diagnosed with an eating disorder. Multivariate logistic regression analyses revealed that those with higher symptoms of anxiety and who had tried cigarettes were significantly more likely to have been diagnosed with an eating disorder. Findings indicate that Chilean female adolescents are at risk of eating disorders and that eating disorders, albeit maladaptive, may be a means to cope with negative affect, specifically anxiety."
JORGE DELVA,The role of religiousness on substance-use disorder treatment outcomes: a comparison of black and white adolescents,"This study compares 41 Black and 124 White adolescents at intake and discharge from a residential treatment program for substance-use disorders. Study data were obtained as part of a larger study (N = 195) that sought to assess the relationship of helping behavior and addiction recovery. This post-hoc analysis aims to identify cultural strengths that may be associated with recovery from substance-use disorders among Black adolescents. Using regression analyses and controlling for the severity of substance use and background variables that distinguish racial groups, religious practices and behaviors at intake were examined. Specifically, Black youth and White youth were compared on treatment outcomes, including alcohol or drug use during treatment, drug craving, 12-Step work, and 12-Step helping. The burden of health and socioeconomic disparities at intake did not disproportionately disfavor Black adolescents. Outcomes related to 12-Step measures were similar between Black and White youth. White adolescents reported higher craving scores at discharge, and Black adolescents were more likely to use drugs during treatment. High levels of religiousness at treatment intake were linked to greater 12-Step work and greater 12-Step helping at discharge. High levels of religiousness at intake were not related to drug use during treatment or to craving scores at discharge. The relationship between intake levels of religiousness and treatment-related outcomes did not differ by race."
JORGE DELVA,Restrictive ID policies: implications for health equity,
JORGE DELVA,“I Put a Mask on” the human side of deportation effects on Latino youth,"Recent research on immigration has looked at forced deportation issues and specifically on the mental health issues of immigrant parents separated from their children rather than from the child’s experience. Hispanic adolescents residing in the United States who live with the fear of being separated from their parents either through forced parental deportation or as a result of being detained themselves may face serious health and mental health problems during the crucial developmental stage of adolescence and pre-adolescence. This study looks at twenty children ages 11-18 (males and females). Qualitative methods were used including focus groups and individual in-depth interviews to examine issues among youth who were at risk of being deported and/or whose parents had been deported or were at risk of deportation. Evidence from the study demonstrated that the youth have complex understandings of the stress of living in undocumented families that can be categorized in individual, social, and structural levels."
JORGE DELVA,Health implications of an immigration raid: findings from a Latino community in the midwestern United States.,"Immigration raids exemplify the reach of immigration law enforcement into the lives of Latino community members, yet little research characterizes the health effects of these raids. We examined the health implications of an immigration raid that resulted in multiple arrests and deportations and occurred midway through a community survey of a Latino population. We used linear regression following principal axis factoring to examine the influence of raid timing on immigration enforcement stress and self-rated health. We controlled for age, sex, relationship status, years in the county in which the raid occurred, children in the home, and nativity. 325 participants completed the survey before the raid and 151 after. Completing the survey after the raid was associated with higher levels of immigration enforcement stress and lower self-rated health scores. Findings indicate the negative impact of immigration raids on Latino communities. Immigration discussions should include holistic assessments of health."
JORGE DELVA,"Estimating the heterogeneous relationship between peer drinking and youth alcohol consumption in Chile using propensity score stratification (2014, v. 11 p. 11879-11897) [Addendum]","The authors wish to update the Acknowledgments in their paper published in International Journal of Environmental Research and Public Health [1], doi:10.3390/ijerph111111879, website: http://www.mdpi.com/1660-4601/11/11/11879."
JORGE DELVA,The effect of computer usage in Internet cafe on cigarette smoking and alcohol use among Chinese adolescents and youth: a longitudinal study,We used longitudinal data to investigate the relationship between computer use in internet cafés and smoking/drinking behavior among Chinese adolescents and young adults. Data are from two waves of the China Health and Nutrition Survey (2004 and 2006). Fixed effects models were used to examine if changes in internet café use were associated with changes in cigarette smoking and drinking of alcohol. Male café users spent on average 17.3 hours in front of the computer/week. This was associated with an increase in the probability of being a current smoker by 13.3% and with smoking 1.7 more cigarettes. Female café users spent on average 11 hours on the computer/week. This was associated with an increase in the probability of drinking wine and/or liquor by 14.74% and was not associated with smoking. Internet cafés are an important venue by which adolescent and young adults in China are exposed to smoking and drinking. Multi-component interventions are needed ranging from policies regulating cigarette and alcohol availability in these venues to anti-tobacco campaigns aimed at the general population but also at individuals who frequent these establishments.
JORGE DELVA,Race/ethnicity and gender differences in drug use and abuse among college students,"This study examines race/ethnicity and gender differences in drug use and abuse for substances other than alcohol among undergraduate college students. A probability-based sample of 4,580 undergraduate students at a Midwestern research university completed a cross-sectional Web-based questionnaire that included demographic information and several substance use measures. Male students were generally more likely to report drug use and abuse than female students. Hispanic and White students were more likely to report drug use and abuse than Asian and African American students prior to coming to college and during college. The findings of the present study reveal several important racial/ethnic differences in drug use and abuse that need to be considered when developing collegiate drug prevention and intervention efforts."
JORGE DELVA,Examining the quality of adolescent–parent relationships among Chilean families,"The purpose of this study was to examine if adolescents reports of warm and harsh parenting practices by their mothers and fathers varied as a function of demographic, youth and their mothers or mother figures' individual and family characteristics. Data are from 707 community-dwelling adolescents (mean age=14, SD=1.4) and their mothers or mother figures in Santiago, Chile. Having a warmer relationship with both parents was inversely associated with the adolescents' age and positively associated with adolescents' family involvement and parental monitoring. Both mothers' and fathers' harsh parenting were positively associated with adolescent externalizing behaviors and being male and inversely associated with youth autonomy and family involvement. These findings suggest that net of adolescent developmental emancipation and adolescent behavioral problems, positive relationships with parents, especially fathers, may be nurtured through parental monitoring and creation of an interactive family environment, and can help to foster positive developmental outcomes."
JORGE DELVA,Examining racial/ethnic minority treatment experiences with specialty behavioral health service providers,"This study investigated whether satisfaction and helpfulness of treatment by mental health service provider is related to race/ethnicity and psychosocial factors. Data from the National Co morbidity Survey-Replication study, which administered mental health service use questions for the past 12-months (1332), was analyzed. Data were stratified by service provider and analyzed with multiple logistic regressions. Racial/ethnic minorities were generally more likely to be satisfied with services provided by specialty mental health providers compared to white respondents. Racial/ethnic minorities generally perceived the services provided by specialty mental health providers as more helpful than did other racial/ethnic groups. Those who reported high cultural identity were more likely to find their treatment experience less satisfying and less helpful. Greater attention to specialty referrals for racial/ethnic minority groups may fruitfully contribute to improve help-seeking for these groups. The role culture plays in shaping the mental health treatment experience needs to be further investigated."
JORGE DELVA,Social capital and cigarette smoking among Latinos in the United States,"This paper presents the results of analyses conducted to examine if social capital indicators were associated with current cigarette smoking and with quitting smoking among a national representative sample of Latinos living in the United States. Data are from 2540 Mexican Americans, Puerto Ricans, Cuban Americans, and Other Latinos who participated in the National Latino and Asian American Survey. A significant inverse association between neighborhood cohesion and current smoking, and a positive association with quitting smoking, were found only among Mexican Americans. No other significant associations were found except for family conflict being associated with higher odds of current smoking with Cuban Americans. Implications of these findings are discussed to unravel the differences in social capital and smoking behaviors among Latino populations."
JORGE DELVA,"Ecological factors and adolescent marijuana use: results of a prospective study in Santiago, Chile","PURPOSE: Despite the growing evidence that ecological factors contribute to substance use, the relationship of ecological factors and illicit drugs such as marijuana use is not well understood, particularly among adolescents in Latin America. Guided by social disorganization and social stress theories, we prospectively examined the association of disaggregated neighborhood characteristics with marijuana use among adolescents in Santiago, Chile, and tested if these relationships varied by sex. METHODS: Data for this study are from 725 community-dwelling adolescents participating in the Santiago Longitudinal Study, a study of substance using behaviors among urban adolescents in Santiago, Chile. Adolescents completed a two-hour interviewer administered questionnaire with questions about drug use and factors related to drug using behaviors. RESULTS: As the neighborhood levels of drug availability at baseline increased, but not crime or noxious environment, adolescents had higher odds of occasions of marijuana use at follow up, approximately 2 years later (odds ratio [OR] = 1.39; 95% CI = 1.16–1.66), even after controlling for the study’s covariates. No interactions by sex were significant. DISCUSSION: The findings suggest that “poverty”, “crime”, and “drug problems” may not be synonyms and thus can be understood discretely. As Latin American countries re-examine their drug policies, especially those concerning decriminalizing marijuana use, the findings suggest that attempts to reduce adolescent marijuana use in disadvantaged neighborhoods may do best if efforts are concentrated on specific features of the “substance abuse environment”."
JORGE DELVA,"Corporal punishment and youth externalizing behavior in Santiago, Chile","OBJECTIVES: Corporal punishment is still widely practiced around the globe, despite the large body of child development research that substantiates its short- and long-term consequences. Within this context, this paper examined the relationship between parental use of corporal punishment and youth externalizing behavior with a Chilean sample to add to the growing empirical evidence concerning the potential relationship between increased corporal punishment and undesirable youth outcomes across cultures. METHODS: Analysis was based on 919 adolescents in Santiago, Chile. Descriptive and multivariate analyses were conducted to examine the extent to which parents' use of corporal punishment and positive family measures were associated with youth externalizing behavior. Furthermore, the associations between self-reported externalizing behavior and infrequent, as well as frequent, use of corporal punishment were investigated to understand how varying levels of parental use of corporal punishment were differently related to youth outcomes. RESULTS: Both mothers' and fathers' use of corporal punishment were associated with greater youth externalizing behavior. Additionally, increases in positive parenting practices, such as parental warmth and family involvement, were met with decreases in youth externalizing behavior when controlling for youth demographics, family socioeconomic status, and parents' use of corporal punishment. Finally, both infrequent and frequent use of corporal punishment were positively associated with higher youth problem behaviors, though frequent corporal punishment had a stronger relationship with externalizing behavior than did infrequent corporal punishment. CONCLUSIONS: Parental use of corporal punishment, even on an occasional basis, is associated with greater externalizing behavior for youth while a warm and involving family environment may protect youth from serious problem behaviors. Therefore, findings of this study add to the growing evidence concerning the negative consequences of corporal punishment for youth outcomes."
JORGE DELVA,"Gender differences in predictors of self-reported physical aggression: exploring theoretically relevant dimensions among adolescents from Santiago, Chile","Research findings remain unclear on whether different factors predict aggression for adolescent men and women. Given that aggression research is rarely conducted with Latin American populations, the current study used multiple imputation and linear regression to assess gender differences in levels and predictors of self-reported physical aggression among a community sample of young (ages 11 through 17) men (n=504) and women (n = 471) from Santiago, Chile. Results revealed that adolescent women reported engaging in higher levels of physical aggression than men. The variables found to be significantly associated with higher levels of reported aggression—younger age, less family involvement, less parental control, less positive relationships with caregivers, having more friends who act out and use substances, having fewer friends committed to learning, presence of dating violence, and more exposure to neighborhood crime—were not moderated by gender, implying that similar factors are related to aggression in adolescent men and women from Chile. Implications for prevention and intervention efforts to address high-risk adolescents and reduce aggression among Chilean youth are discussed."
JORGE DELVA,"Understanding neighbourhoods, communities and environments: new approaches for social work research","This article discusses some new ways in which social work research can explore the interaction between neighbourhoods and child and adult wellbeing. The authors note that social work practices are often criticised for taking an individualistic approach and paying too little attention to the service user’s environment. The article uses examples of research projects from Chile, the United States of America and Wales, to discuss the use of spatially oriented research methods for understanding neighbourhood factors. Quantitative, qualitative and mixed methods approaches that are particularly appropriate for investigating social work relevant topics are discussed in turn, including quantitative and qualitative uses for geographical information systems (GIS), hierarchical linear modelling (HLM) for analysing spatially clustered data and qualitative mobile interviews. The article continues with a discussion of the strengths and limitations of using spatially orientated research designs in social work research settings and concludes optimistically with suggestions for future directions in this area."
JORGE DELVA,"The association of at-risk, problem, and pathological gambling with substance use, depression, and arrest history","We examined at-risk, problem, or pathological gambling co-occurrence with frequency of past-year alcohol, tobacco, and marijuana use; depressive symptoms; and arrest history. Data included the responses of over 3,000 individuals who participated in a 2006 telephone survey designed to understand the extent of at-risk, problem, and pathological gambling; comorbidity levels with substance use; mental health; and social problems among Southwestern U.S. residents. Data were analyzed with multinomial and bivariate logistic regression. Respondents at risk for problem gambling were more likely to use alcohol, tobacco, and marijuana than those respondents not at risk. Pathological gamblers were no more or less likely to consume alcohol or tobacco than were non-gamblers or those not at risk. A dose-response relationship existed between degree of gambling problems and depressive symptoms and arrest history. Interventions for at-risk or problem gamblers need to include substance use treatment, and the phenomenon of low levels of substance use among pathological gamblers needs further exploration."
JORGE DELVA,Pervasive exposure to violence and posttraumatic stress disorder in a predominantly African American urban community: the Detroit neighborhood health study,"Exposure to traumatic events is common, particularly among economically disadvantaged, urban African Americans. There is, however, scant data on the psychological consequences of exposure to traumatic events in this group. We assessed experience with traumatic events and posttraumatic stress disorder (PTSD) among 1,306 randomly selected, African American residents of Detroit. Lifetime prevalence of exposure to at least 1 traumatic event was 87.2% (assault = 51.0%). African Americans from Detroit have a relatively high burden of PTSD; 17.1% of those who experienced a traumatic event met criteria for probable lifetime PTSD. Assaultive violence is pervasive and is more likely to be associated with subsequent PTSD than other types of events. Further efforts to prevent violence and increase access to mental health treatment could reduce the mental health burden in economically disadvantaged urban areas."
JORGE DELVA,"Sexual intercourse among adolescents in Santiago, Chile: a study of individual and parenting factors","OBJECTIVE: To examine a range of individual, parenting, and family factors associated with sexual intercourse among a community sample of youth and their families in Santiago, Chile. METHODS: Data were taken from the first wave of the Santiago Longitudinal Study conducted in January 2008–November 2009. Participants were 766 youth (mean age = 14.03 years, 51% male) from municipalities of low-to mid-socioeconomic status. Variables included emotional and behavioral subscales from the Child Behavior Checklist’s Youth Self Report, parental monitoring, family involvement, parental control and autonomy, relationship with each parent, and sexual activity. Bivariate and multivariate logistic regression models were used to examine the odds of sexual intercourse initiation. RESULTS: Seventy (9.14%) youth reported having had sex in their lifetime; the average age of first sexual intercourse among this group was 13.5 years (Standard Deviation [SD] = 1.74) for males and 14.08 (SD = 1.40) for females. Having sex was inversely associated with withdrawn-depressed symptoms (Odds Ratio [OR] = 0.84, Confidence Interval [CI] = 0.72–0.97), but positively associated with somatic complaints (OR = 1.20, CI = 1.04–1.38) and rule breaking behavior (OR = 1.21, CI = 1.08–1.36), after adjusting for demographic and other individual and parenting variables. The majority (80%) of the youth who had had sex reported using protection at the time of last intercourse. CONCLUSIONS: Findings highlight the role that mental health problems—some of them not commonly associated with onset of sexual activity—may play in a youth’s decision to have sex. The potential protective effects of several parenting and family characteristics disappeared with youth age and youth behavioral problems."
JORGE DELVA,"Alcohol consumption among Chilean adolescents: examining individual, peer, parenting and environmental factors","AIMS: This study examined whether adolescents from Santiago, Chile who had never drunk alcohol differed from those who had drunk alcohol but who had never experienced an alcohol-related problem, as well as from those who had drunk and who had experienced at least one alcohol-related problem on a number of variables from four domains - individual, peers, parenting, and environmental. DESIGN: Cross-sectional. SETTING: Community based sample. PARTICIPANTS: 909 adolescents from Santiago, Chile. MEASUREMENTS: Data were analyzed with multinomial logistic regression to compare adolescents who had never drunk alcohol (non-drinkers) with i) those that had drunk but who had experienced no alcohol-related problems (non-problematic drinkers) and ii) those who had drunk alcohol and had experienced at least one alcohol-related problem (problematic drinkers). The analyses included individual, peer, parenting, and environmental factors while controlling for age, sex, and socioeconomic status. FINDINGS: Compared to non-drinkers, both non-problematic and problematic drinkers were older, reported having more friends who drank alcohol, greater exposure to alcohol ads, lower levels of parental monitoring, and more risk-taking behaviors. In addition, problematic drinkers placed less importance on religious faith to make daily life decisions and had higher perceptions of neighborhood crime than non-drinkers. CONCLUSIONS: Prevention programs aimed at decreasing problematic drinking could benefit from drawing upon adolescents' spiritual sources of strength, reinforcing parental tools to monitor their adolescents, and improving environmental and neighborhood conditions."
JORGE DELVA,Family and parenting characteristics associated with marijuana use by Chilean adolescents,"OBJECTIVE: Family involvement and several characteristics of parenting have been suggested to be protective factors for adolescent substance use. Some parenting behaviors may have stronger relationships with adolescent behavior while others may have associations with undesirable behavior among youth. Although it is generally acknowledged that families play an important role in the lives of Chilean adolescents, scant research exists on how different family and parenting factors may be associated with marijuana use and related problems in this population which has one of the highest rates of drug use in Latin America. METHODS: Using logistic regression and negative binomial regression, we examined whether a large number of family and parenting variables were associated with the possibility of Chilean adolescents ever using marijuana, and with marijuana-related problems. Analyses controlled for a number of demographic and peer-related variables. RESULTS: Controlling for other parenting and family variables, adolescent reports of parental marijuana use showed a significant and positive association with adolescent marijuana use. The multivariate models also revealed that harsh parenting by fathers was the only family variable associated with the number of marijuana-related problems youth experienced. CONCLUSION: Of all the family and parenting variables studied, perceptions of parental use of marijuana and harsh parenting by fathers were predictors for marijuana use, and the experience of marijuana-related problems. Prevention interventions need to continue emphasizing the critical socializing role that parental behavior plays in their children's development and potential use of marijuana."
JORGE DELVA,Relationship between discordance in parental monitoring and behavioral problems among Chilean adolescents,"This study investigated the role of discrepancies between parent and youth reports of perceived parental monitoring in adolescent problem behaviors with a Chilean sample (N= 850). Higher levels of discordance concerning parental monitoring predicted greater levels of maladaptive youth behaviors. A positive association between parent-youth discordance and externalizing problems indicated that large adult-youth disagreement in parental monitoring may impose a great risk, despite protective efforts of parental monitoring. Although the direct relationship between parental monitoring and youth internalizing behaviors was not significant, parent-youth incongruence in monitoring was associated with greater levels of internalizing behaviors. Therefore, differing assessments of parental behaviors, as an indicator of less optimal family functioning, may provide important information about youth maladjustment and may potentially provide a beginning point for family-focused intervention."
JORGE DELVA,"The association of family and peer factors with tobacco, alcohol, and marijuana use among Chilean adolescents in neighborhood context","Research on adolescent use of substances has long sought to understand the family factors that may be associated with use of different substances such as alcohol, tobacco and marijuana. However, scant attention has been focused on these questions in Latin American contexts, despite growing concerns about substance use among Latin American youth. Using data from a sample of 866 Chilean youth, we examined the relationship of family and neighborhood factors with youth substance abuse. We found that in a Latin American context access to substances is an important predictor of use, but that neighborhood effects differ for marijuana use as opposed to cigarettes or alcohol. Age of youth, family and peer relationships, and gender all play significant roles of substance use.The study findings provide additional evidence that the use of substances is complex whereby individual, family, and community influences must be considered jointly to prevent or reduce substance use among adolescents."
JORGE DELVA,Differential item functioning due to gender between depression and anxiety items among Chilean adolescents,"Although much is known about the higher prevalence of anxiety and depressive disorders among adolescent females, less is known about the differential item endorsement due to gender in items of commonly scales used to measure anxiety and depression. We conducted a study to examine if adolescent males and females from Chile differed on how they endorsed the items of the Youth Self-Report (YSR) anxious/depressed problem scale. We used data from a cross-sectional sample consisting of 925 participants (Mean age = 14, SD=1.3, 49% females) of low to lower-middle socioeconomic status. A two-parameter logistic (2PL) IRT DIF model was fit. Results revealed differential item endorsement (DIF) by gender for six of the 13 items with adolescent females being more likely to endorse a depression item while males were found more likely to endorse anxiety items. Findings suggest that items found in commonly utilized measures of anxiety and depression symptoms may not equally capture true levels of these behavioral problems among adolescent males and females. Given the high levels of mental disorders in Chile and surrounding countries, further attention should be focused on increasing the number of empirical studies examining potential gender differences in the assessment of mental health problems among Latin American populations to better aid our understanding of the phenomenology and determinants of these problems in the region."
JORGE DELVA,"Personality and parenting processes associated with problem behaviors: a study of adolescents in Santiago, Chile","Considerable research in the U.S. has established that adolescent antisocial, aggressive, and attention problems have a negative influence on adolescents' ability to become productive members of society. However, although these behaviors appear in other cultures, little is known about the development of these problems among adolescents in countries other than the U.S.. This study contributes to our understanding of personality and parenting factors associated with adolescent problem behaviors using an international sample. Data are from a NIDA-funded study of 884 community-dwelling adolescents in Santiago, Chile (Mean age=14, SD=1.4, 48% females) of mid-to-low socioeconomic status. Results revealed that rule-breaking and aggressive behaviors were both associated with greater levels of adolescent drive but lower levels of parental monitoring and positive parenting by both parents. Adolescents who reported more attention problems were more likely to exhibit driven behavior, more behavioral inhibition, to report lower levels of parental monitoring, and positive parenting by mother and father. Results of interactions revealed that the influences of positive parenting and parental monitoring on adolescent aggressive behaviors varied as a function of the gender of the adolescent. Helping parents build on their parenting skills may result in important reductions in adolescent problem behaviors among U.S. and international adolescents."
JORGE DELVA,"Examining the factor structure of anxiety and depression symptom items among adolescents in Santiago, Chile","The co-occurrence of emotional disorders among adolescents has received considerable empirical attention. This study aims to contribute to the understanding of co-occurring anxiety and depression by examining the factor structure of the Youth Self-Report used with a sample of low-income adolescents from Santiago, Chile. Data from two independent, randomly selected subsamples were analyzed using exploratory and confirmatory factor analyses. Results indicate the best fit for the data is a two-factor model of anxiety and depression symptoms, which factors anxiety and depression into separate latent constructs. Because the findings show that anxiety and depression are not measured by the same factor in this international sample, the results imply that a valid and useful distinction exists between these constructs. That these constructs are found to be separate factors suggests that anxiety and depression may have separate etiologies and consequences, which might be best addressed by separate intervention components. These findings are consistent with the viewpoint that anxiety and depression constructs have similar emotional features and, despite sharing a common underlying internalizing disorder, distinct items capture aspects of each construct."
JORGE DELVA,"The association of recreational space with youth smoking in low-socioeconomic status neighborhoods in Santiago, Chile","OBJECTIVES: This study examines the relationship of neighborhood recreational space with youth smoking in mid- to low-income areas in the capital of Chile, Santiago. METHODS: A unique data set of adolescents (n = 779, mean age = 14, 51 % male) provided home addresses of study participants which were geocoded and mapped. Satellite maps of neighborhoods were used to identify open spaces for recreational use (e.g., soccer fields and plazas). Thiessen polygons were generated to associate study participants with the nearest available open space using ArcGIS. Regression models, with smoking as a dependent variable, were estimated in which age, sex, family socioeconomic status, peer substance usage, neighborhood crime, and accessibility of open space were covariates. RESULTS: The results show that residential proximity to recreational space was significantly and inversely associated with tobacco consumption among female, but not male, adolescents. Age and neighborhood crime were both positively associated with tobacco consumption among both male and female adolescents. CONCLUSIONS: This study suggests that recreational spaces in proximity to residences may have a positive impact on reducing adolescents' inclination to consume tobacco. The relationship of the accessibility to such spaces with smoking appears to vary by adolescents' sex."
JORGE DELVA,Gender differences in psychological factors shaping smoking decisions of Chilean adolescents,"This study examined gender differences in how internalizing and externalizing symptoms affect adolescents’ decisions about smoking in Chile, where girls smoke at some of the highest rates in the world. In multivariate logistic regression analyses with 607 adolescents, internalizing symptoms, such as depressed mood and anxiety, predicted smoking among girls more than boys, with girls who were low in internalizing symptoms being more likely to smoke than those who were high in internalizing symptoms. In Chile’s high-risk context, internalizing symptoms may be indirectly protective for girls by decreasing their exposure to peer pressure and related influences that encourage cigarette use."
JORGE DELVA,La observación sistemática de vecindarios: el caso de Chile y sus perspectivas para trabajo social,"El estudio acerca de las características de los vecindarios y sus efectos sobre las personas ha llegado a ser un área de creciente atención por parte de investigadores de diversas disciplinas en países desarrollados. Aunque actualmente existen diversas metodologías para estudiar efectos del vecindario, una de las más utilizadas es la Observación Sistemática de Vecindarios –Systematic Social Observation SSO, en inglés—porque permite recolectar información acerca de diversas características del entorno físico, social, ambiental y económico de los vecindarios donde se aplica. El objetivo de este artículo es (i) dar a conocer sumariamente algunas investigaciones influyentes sobre efectos del vecindario en Estados Unidos, ii) describir cómo se diseñó e implementó la Observación Sistemática de Vecindarios en la ciudad de Santiago de Chile, iii) señalar algunos facilitadores y obstaculizadores de la implementación del proyecto y, finalmente iv) enunciar posibles contribuciones y limitaciones que esta metodología ofrecería al trabajo social en Chile.
 The study of neighborhood characteristics and their effects on individuals has become an area of increasing attention by scholars from various disciplines in developed countries. Although there are various methods to study neighborhoods and their impact on human populations, one of the most used is the Systematic Social Observation –Observación Sistemática de Vecindarios (OSV), in Spanish—because it allows the collection of information about various features of the physical, social, environmental and economic characteristics of neighborhoods. The purpose of this article is to (i) briefly present some research on neighborhood effects influential in the U.S., ii) describe how they Systematic Social Observation was designed and implemented in the city of Santiago, Chile, iii) discuss some facilitators and obstacles of the implementation process and, finally iv) list possible contributions and limitations this approach would offer the profession of social work in Chile.
 "
JORGE DELVA,Does gender moderate associations between social capital and smoking? An Asian American study,"Growing research finds that social capital is associated with smoking. However, most studies focus on white populations and do not take into account potential differences between genders. The present study examines the associations between social capital and self-report smoking status and assesses the moderating role of gender among a national representative sample of Asian American adults. Social capital consisted of measures of individual social connectedness (i.e. social ties with relatives and friends) and subjective evaluation of family and neighborhood environment (i.e. family and neighborhood cohesion, family conflict). Asian men were almost three times more likely to be current smokers than women (20.1% vs. 7.0%). Results of multivariate logistic regression analyses showed that family conflicts or higher levels of connectedness with family members were associated with increased odds of being a current smoker among Asian Americans as a whole. Further stratified analysis revealed significant gender differences in several aspects of social capital: there were stronger effects of social connectedness with family members on increasing the odds of smoking for women than for men. In addition, women who had closer connections to friends had greater odds of being current smokers, whereas the opposite was true for men. The findings of this study provide new evidence for the differential effects of social capital by gender, suggesting that more studies are needed to understand social capital’s effects in different racial/ethnic populations and the mechanisms by which the effects vary with gender."
JORGE DELVA,Cigarette smoking among low-income African Americans: a serious public health problem,"BACKGROUND: This study examines the current prevalence of cigarette smoking and the number of cigarettes smoked in a community-based sample of 1021 low-income African-American men and women. METHODS: Participants were selected using a two-stage, area probability sample design. Data were collected in 2002-2003 in face-to-face interviews and analyzed in 2005. All data and analyses were weighted to account for the complex sampling design. RESULTS: Fifty-nine percent of men and 41% of women were current smokers, with younger individuals apparently initiating smoking at an earlier age than older individuals. CONCLUSIONS: The high prevalence of cigarette use provides further evidence that the excess burden of tobacco-related disease among low-income African-American families may be on the rise. This is of great concern, and if confirmed by further research, indicates an urgent need for preventive intervention."
JORGE DELVA,"Adolescent internalizing, externalizing, and social problems following iron deficiency at 12-18 months: the role of maternal responsiveness","This study tested whether maternal responsiveness moderated or mediated pathways from iron deficiency (ID) at 12-18 months to adolescent behavior problems. Participants were part of a large Chilean cohort (N = 933). Iron status was assessed at 12 and 18 months. Maternal responsiveness was assessed at 9 months and 5 years. Parents reported their child's symptomology at 5 years, 10 years, and adolescence (11-17 years; M = 14.4). Structural equation modeling identified a previously unrecognized pathway by which child externalizing problems and negative maternal responsiveness at 5 years mediated associations between ID at 12-18 months and adolescent internalizing, externalizing, and social problems. Positive maternal responsiveness in infancy did not buffer those with ID anemia from developing 5-year internalizing problems."
JORGE DELVA,"Associations among mothers' depression, emotional and learning-material support to their child, and children's cognitive functioning: a 16-year longitudinal study","This study examined the associations among maternal depression, mothers' emotional and material investment in their child, and children's cognitive functioning. Middle-class Chilean mothers and children (N = 875; 52% males) were studied when children were 1, 5, 10, and 16 years (1991-2007). Results indicated that highly depressed mothers provided less emotional and material support to their child across all ages, which related to children's lower IQ. Children with lower mental abilities at age 1 received less learning-material support at age 5, which led to mothers' higher depression at child age 10. Mothers' low support was more strongly linked to maternal depression as children got older. Findings elucidate the dynamic and enduring effects of depression on mothers' parenting and children's development."
JORGE DELVA,"Risk taking, sensation seeking and personality as related to changes in substance use from adolescence to young adulthood","INTRODUCTION: This study examined changes in substance use from adolescence to young adulthood as related to adolescents' risk taking, sensation seeking, antisocial activities, and personality traits. METHODS: Chilean youth (N = 890, 52% female) were studied in adolescence (14.5 and 16.2 years) and young adulthood (M age 21.3 years). Risk taking was assessed via a laboratory-based performance task (Balloon Analogue Risk Task), and self-administered questionnaires assessed sensation seeking, antisocial behaviors, personality and substance use. RESULTS: Frequent involvement in sensation seeking and antisocial activities were associated with increased odds of continued marijuana use from adolescence to young adulthood and of illicit substance use at young adulthood. High risk taking was associated with a reduced likelihood of discontinuing marijuana use at young adulthood, and high agreeableness and conscientiousness were associated with reduced likelihood of new onset marijuana use and illicit substance use at young adulthood. CONCLUSIONS: Results highlight specific risk-taking tendencies and personality characteristics that relate to initiating, continuing, or discontinuing substance use at entry into adulthood. Sensation seeking and involvement in antisocial activities were the two foremost risk factors for continued use, which is a forecaster of drug dependence. Findings suggest potential prevention and intervention targets for abstaining from or discontinuing substance use as youth transition to adulthood."
JORGE DELVA,Overeating and binge eating among immigrants in the United States: new terrain for the healthy immigrant hypothesis,"BACKGROUND: Prior research indicates that, compared to individuals born in the United States (US), immigrants are less likely to experience mental health and inhibitory control problems. However, our understanding of overeating and binge eating-both related to mental health and inhibitory control-among immigrants in the US remains limited. Drawing from a large national study, we report the prevalence of overeating and binge eating among immigrants vis-à-vis the US-born. METHODS: The data source used for the present study is the National Epidemiologic Survey on Alcohol and Related Conditions (NESARC-III, 2012-2013), a nationally representative survey of 36,309 civilian, non-institutionalized adults ages 18 and older in the US. Logistic regression was employed to examine the relationship between immigrant status and key outcomes. RESULTS: The prevalence of any (immigrants = 7.8%, US-born = 17.0%) and recurrent overeating (immigrants = 2.9%, US-born = 5.3%) was lower among immigrants than US-born individuals. Among those reporting recurrent overeating, the prevalence of binge eating with loss of control was comparable among immigrant (37.2%) and US-born participants (39.9%), in general. However, stratified analyses revealed that risk of binge eating with loss of control was lower among immigrant women compared to US-born women (AOR 0.54, 95% CI 0.29-0.98). CONCLUSIONS: Findings from the present study provide clear results that immigrants are substantially less likely to overeat as compared to US-born individuals and that, among women but not men, immigrant status is associated with lower risk of binge eating with loss of control."
JORGE DELVA,Maternal depression trajectories relate to youths' psychosocial and cognitive functioning at adolescence and young adulthood,"This study evaluated how patterns of mothers' depressive symptoms across their child's childhood relate to children's psychosocial adjustment at adolescence and young adulthood and to cognitive functioning at adolescence. Depressive symptoms were measured in 1,273 mothers when their children were 1, 5, 10, and 14.6 years of age. Children (53.5% male; n = 1,024) completed the Youth Self-Report at adolescence (M = 14.6y), and the Adult Self-Report in young adulthood (M = 20.5y; n = 817) to assess internalizing and externalizing symptoms. Adolescents also completed standardized cognitive tests to assess verbal and mathematical skills. Growth mixture modeling analyses identified four patterns of maternal depressive symptom trajectories: infrequent (55%), increasing at adolescence (20%), decreasing at adolescence (14%), and chronic severe (11%). Results indicated that exposure to maternal depression of any duration, severity or time period during childhood portended higher levels of externalizing and attention problems at both adolescence and adulthood and higher levels of internalizing problems at adulthood. Adolescents whose mothers had chronic severe depressive symptoms had lower language, vocabulary, reading comprehension and mathematical test scores than youth whose mothers had stable infrequent depressive symptoms. Findings illustrate the significance and long-term ramifications of mothers' depressed mood for their children's mental and psychosocial health into adulthood. Findings also demonstrate that the lower cognitive abilities among children of severely depressed mothers persist beyond childhood and pertain to a broad range of cognitive abilities."
JORGE DELVA,Culturally adapted motivational interviewing's effects on drinking in response to immigration and acculturation stressors among Latinx adults with heavy drinking problems,"INTRODUCTION: Culturally adapted motivational interviewing (CAMI) is a form of motivational interviewing that was adapted to address immigration- and acculturation-related stressors among Latinx adults who met criteria for hazardous drinking. This study hypothesized that (1) receiving CAMI was associated with reduced immigration/acculturation stress and related drinking and that (2) these associations differed by participants' acculturation and perceived discrimination levels. METHODS: This study employed a single group pre-post study design using data from a randomized controlled trial. Participants were Latinx adults who received CAMI (N = 149). The study assessed immigration/acculturation stress with the Measure of Immigration and Acculturation Stressors (MIAS) and measured related drinking with the Measure of Drinking Related to Immigration and Acculturation Stressors (MDRIAS). The study team conducted linear mixed modeling for repeated measures to examine outcome changes between the baseline and the 6-month and 12-month follow-ups and moderation effects. RESULTS: Compared to baseline, the study found significant decreases in the total MIAS and MDRIAS scores and subscale scores at 6- and 12-month follow-ups. Moderation analysis results showed that lower acculturation levels and higher levels of perceived discrimination were significantly associated with larger decreases at follow-up in total MIAS and MDRIAS scores and several subscale scores. CONCLUSIONS: Findings provide preliminary support for CAMI's efficacy in reducing immigration and acculturation stress and related drinking among Latinx adults with heavy drinking problems. The study observed more improvements among the less acculturated and more discriminated participants. Larger studies with more rigorous designs are needed."
JORGE DELVA,"Associations among household chaos, school belonging and risk behaviors in adolescents","We examined the associations between adolescent risk behaviors and household chaos, and whether associations varied by adolescents' sense of school belonging. We collected data from 801 Chilean adolescents from working-class families (M age 16.2 years). Approximately, one-quarter of participants reported past-month cigarette use, and 8% and 9% reported past-month binge drinking and marijuana use, respectively. More than half of youth reported having sex (52%), 23% of youth reported having unprotected sex at their last encounter, and 14% reported having multiple sex partners. Within the past year, 16%, 36%, and 23% reported carrying a weapon, physically attacking someone, and threatening to physically hurt someone, respectively. Greater household chaos was related to increased odds of each risk behavior except unprotected sex. These associations did not vary significantly by level of school belonging. Results are compared to previously reported results using a U.S., largely Latinx sample in attempts to replicate findings across culture and context."
JORGE DELVA,"Children's inattention and hyperactivity, mother's parenting, and risk behaviors in adolescence: a 10-year longitudinal study of Chilean children","OBJECTIVE: To examine how mothers' nurturant and, separately, hostile parenting mediate the effects of young children's inattentiveness and hyperactivity on risk behaviors in adolescence. METHOD: Data were analyzed from 920 healthy Chilean children, studied at 5.5, 10 years, and adolescence. Children's hyperactivity and inattentiveness at 5.5 years were assessed by mother ratings on the Children's Adaptive Behavior Inventory. Mothers' nurturance and hostility toward the child at 10 years were assessed by maternal interview on the Home Observation for Measurement of the Environment. Youth's delinquent and aggressive behaviors in adolescence were compiled from the Youth Self-Report questionnaire, and youth's substance use in adolescence was assessed by an extensive self-report substance use inventory. Structural equation modeling was used to identify direct and indirect effects. RESULTS: Findings supported a mediating effect, with more severe child hyperactivity at 5.5 years leading to mothers' greater hostility toward her child at age 10, which, in turn, led to greater delinquency and aggression in adolescence. Marginal mediating effects were also found from child hyperactivity and inattention at 5.5 years to mothers' lower nurturance at 10 years to youth substance use in adolescence. CONCLUSION: Mothers' hostile treatment of children with hyperactive or inattentive behaviors contributed to adolescent risk behaviors and is an area of intervention."
JORGE DELVA,"Infant iron deficiency and iron supplementation predict adolescent internalizing, externalizing, and social problems","Objective: To evaluate associations between iron supplementation and iron deficiency in infancy and internalizing, externalizing, and social problems in adolescence. Study design: The study is a follow-up of infants as adolescents from working-class communities around Santiago, Chile who participated in a preventive trial of iron supplementation at 6 months of age. Inclusionary criteria included birth weight ≥3.0 kg, healthy singleton term birth, vaginal delivery, and a stable caregiver. Iron status was assessed at 12 and 18 months of age. At 11-17 years of age, internalizing, externalizing, and social problems were reported by 1018 adolescents with the Youth Self Report and by parents with the Child Behavior Checklist. Results: Adolescents who received iron supplementation in infancy had greater self-reported attention-deficit/hyperactivity disorder but lower parent-reported conduct disorder symptoms than those who did not (Ps < .05). Iron deficiency with or without anemia at 12 or 18 months of age predicted greater adolescent behavior problems compared with iron sufficiency: more adolescent-reported anxiety and social problems, and parent-reported social, post-traumatic stress disorder, attention-deficit/hyperactivity disorder, oppositional defiant, conduct, aggression, and rule breaking problems (Ps < .05). The threshold was iron deficiency with or without anemia for each of these outcomes. Conclusions: Iron deficiency with or without anemia in infancy was associated with increased internalizing, externalizing, and social problems in adolescence."
JORGE DELVA,"Associations among infant iron deficiency, childhood emotion and attention regulation, and adolescent problem behaviors","This study examined whether iron deficiency (ID) in infancy contributes to problem behaviors in adolescence through its influence on poor regulatory abilities in childhood. Chilean infants (N = 1,116) were studied when there was no national program for iron fortification (1991-1996), resulting in high rates of ID (28%) and iron-deficiency anemia (IDA, 17%). Infants (54% male) were studied at childhood (Mage  = 10 years) and adolescence (Mage  = 14 years). IDA in infancy was related to excessive alcohol use and risky sexual behavior in adolescence through its effect on poor emotion regulation in childhood. Attentional control deficits at age 10 were also related to both infant IDA and heightened risk taking in adolescence. Findings elucidate how poor childhood regulatory abilities associated with infant IDA compromise adjustment in adolescence."
JORGE DELVA,"Infant iron deficiency, child affect, and maternal unresponsiveness: testing the long-term effects of functional isolation","Children who are iron deficient (ID) or iron-deficient anemic (IDA) have been shown to seek and receive less stimulation from their caregivers, contributing to functional isolation. Over time, the reduced interactions between child and caregiver are thought to interfere with the acquisition of normative social competencies and adversely affect the child's development. The current study examined functional isolation in children who were ID or IDA in infancy in relation to social difficulties in middle childhood and problem behaviors in adolescence. Using a sample of 873 Chilean children, 45% of whom were ID or IDA in infancy, structural equation modeling results indicated that infant IDA was associated with children's dull affect and social reticence at age 5, which were related to mothers' unresponsiveness and understimulation. Mothers' limited responsiveness and stimulation were, in turn, related to children's peer rejection at age 10, which further linked to problem behaviors and associating with deviant peers at adolescence. Findings support the functional isolation hypothesis and suggest that early limited caregiver responsiveness and stimulation contribute to long-term social difficulties in adolescents who were iron-deficient anemic in infancy. (PsycINFO Database Record"
JORGE DELVA,Family factors and parenting in Ukraine,"The present study aimed to estimate the use of positive and negative parenting practices in Ukraine and explore relationships between parenting practices, intimate partner violence (IPV), alcohol use, and sociodemographics. Parents of children (N=320) ages 9-16 from three Ukrainian regions answered questions from the Alabama Parenting Questionnaire (APQ), the Revised Conflict Tactics Scales (CTS-R), Family Adaptability and Cohesion Evaluation Scale (FACES), and the Alcohol Use Section of the Drinking and Drug History and Current Use Patterns questionnaire. Ukrainian parents who reported lower use of alcohol, balanced family functioning and lower intimate partner violence were more likely to use positive parenting and less likely to use negative parenting practices. Parents with lower education were more likely to use negative parenting practices. Furthermore, alcohol use, IPV, parent education and higher family income were significantly and indirectly related with positive and negative parenting scores. The model explained 61% of variance in the positive parenting, 67% in the negative parenting."
JORGE DELVA,The usability and acceptability of an adolescent mHealth HIV/STI and drug abuse preventive intervention in primary care,"Human Immunodeficiency Virus (HIV)/sexually transmitted infection (STI) risk behaviors among adolescents remain significant public health concerns. Shifts in policy and advances in technology provide opportunities for researchers and clinicians to deliver and evaluate mobile-health (mHealth) prevention programs in primary care, however, research is limited. This study assessed the usability and acceptability of Storytelling 4 Empowerment-a mHealth HIV/STI and drug abuse preventive intervention app-among adolescents in primary care. Informed by principles of community-based participatory research, we recruited a purposive sample of 30 adolescents from a youth-centered community health care clinic in Southeast Michigan. The study sample is primarily African American and female. Adolescents who participated in the Storytelling 4 Empowerment intervention assessed its usability and acceptability, and self-reported their HIV/STI risk behaviors. We used a multiple-methods approach. Adolescents reported high acceptability of the content, process, and format of Storytelling 4 Empowerment, as evidenced by qualitative data and mean scores from the Session Evaluation Form for the HIV/STI and Alcohol/Drug content, overall Storytelling 4 Empowerment intervention, and Client Satisfaction Questionnaire-8. Findings indicate that Storytelling 4 Empowerment is acceptable among adolescents in primary care. A next step is to examine the effect of Storytelling 4 Empowerment on adolescent sexual risk and drug use behaviors and HIV/STI testing."
JORGE DELVA,"It works, but for whom? examining racial bias in carding experiences and acceptance of a county identification card","Purpose: Policies that restrict access to U.S. government-issued photo identification (ID) cards adversely affect multiple marginalized communities. This context impedes access to health-promoting resources that increasingly require government-issued IDs and exacerbates health inequities. In 2015, Washtenaw County, Michigan, implemented the Washtenaw ID to improve access to resources contingent upon having an ID. We employed an audit study to examine whether Washtenaw ID users experienced racially biased treatment in carding experiences and acceptance of the Washtenaw ID. Methods: Seven 25- to 32-year-old mystery shoppers (two Latina, three black, and two white women) attempted to purchase a standardized basket of goods, including an age-restricted item in Washtenaw County stores (n=130 shopping experiences). We examined whether experiences of being asked for ID and acceptance of the Washtenaw ID varied by race/ethnicity. Results: Each shopper visited 9-22 stores. Shoppers were asked for ID in 63.1% of shopping experiences. Of these, the Washtenaw ID was accepted 91.5% of the time. Among those who were asked for ID, a higher percentage of Latina (16.0%) shoppers had their Washtenaw IDs rejected than black (6.3%) and white (4.0%) shoppers, although differences were not statistically significant (p=0.27). Latina shoppers had 2.9 times the odds of receiving a comment about their Washtenaw ID relative to white shoppers (OR=2.92, p=0.08), comments that were nonpositive. Conclusion: Local IDs may improve access to resources contingent upon having an ID. However, racialization processes, including anti-immigrant sentiments, may inhibit the mitigating goal of local IDs. Continued attention to the health equity impacts of equity-driven interventions is warranted."
JORGE DELVA,A systematic review of mammography educational interventions for low-income women,"OBJECTIVE: We conducted a systematic review to examine the effectiveness of educational interventions in increasing mammography screening among low-income women. DATA SOURCES: Bibliographic databases, including MEDLINE, The Cochrane Central Register of Controlled Trials, The Cochrane Database of Systematic Reviews, and the ISI Web of Science, were searched for relevant articles. STUDY INCLUSION AND EXCLUSION CRITERIA: Randomized, community-based trials targeting low-income women and published between January 1980 and March 2003 were included. DATA EXTRACTION: The search yielded 242 studies; 24 met all inclusion criteria. DATA SYNTHESIS: Three studies used mammography vans, three used low-cost vouchers or provided free mammograms, three used home visits, one used community education alone, one provided referrals, five incorporated multiple intervention strategies, two used phone calls, one used videos and print material, and five used primarily print material. RESULTS: Of nine studies that reduced barriers to care via mammography vans, cost vouchers, or home visits, eight showed statistically significant increases in mammography screening. Seven of the eight studies that used peer educators had significant increases in screening, as did four of the five studies that used multiple (intervention) components. CONCLUSIONS: Interventions that used peer educators, incorporated multiple intervention strategies, or provided easy access via vans, cost vouchers, or home visits were effective in increasing screenings. Mailed letter or telephone reminders were not effective in trials involving low-income women, which is contrary to findings from middle/upper-income studies."
JORGE DELVA,Relationship of birth order and gender with academic standing and substance use among youth in Latin America,"Alfred Adler attempted to understand how family affects youth outcomes by considering the order of when a child enters a family (Adler, 1964). Adler's theory posits that birth order formation impacts individuals. We tested Adler's birth order theory using data from a cross-sectional survey of 946 Chilean youths. We examined how birth order and gender are associated with drug use and educational outcomes using three different birth order research models including: (1) Expedient Research, (2) Adler's birth order position, and (3) Family Size theoretical models. Analyses were conducted with structural equation modeling (SEM). We conclude that birth order has an important relationship with substance use outcomes for youth but has differing effects for educational achievement across both birth order status and gender."
JORGE DELVA,Predictors of discordance among Chilean families,"Parent-youth agreement on parental behaviors can characterize effective parenting. Although discordance in families may be developmentally salient and harmful to youth outcomes, predictors of discordance have been understudied, and existing research in this field has been mostly limited to North American samples. This paper addressed this literature gap by using data from a community-based study of Chilean adolescents. Analysis was based on 1,068 adolescents in Santiago, Chile. The dependent variable was discordance which was measured by the difference between parent and youth’s assessment of parental monitoring. Major independent variables for this study were selected based on previous research findings that underscore youth’s developmental factors, positive parental and familial factors and demographic factors. Descriptive and multivariate analyses were conducted to examine the prevalence and associations between youth, parental and familial measures with parent-youth discordance. There was a sizable level of discordance between parent and youth’s report of parental monitoring. Youth’s gender and externalizing behavior were significant predictors of discordance. Warm parenting and family involvement were met with decreases in discordance. The negative interaction coefficients between parental warmth and youth’s gender indicated that positive parental and familial measures have a greater effect on reducing parent-youth discordance among male youths. Results support the significance of positive family interactions in healthy family dynamics. Findings from this study inform the importance of services and interventions for families that aim to reduce youth’s problem behavior and to create a warm and interactive family environment."
JORGE DELVA,A study of the relationship between parental alcohol problems and alcohol Use among adolescent females in Republic of Korea,"OBJECTIVES: The study was designed to test if alcohol use and alcohol-related problems among adolescent females are related to their parents' level of alcohol problems. METHODS: In 2001, a stratified sample of 2077 adolescent females, grades 10-11, from twelve female-only high schools located in a large metropolitan city in the Republic of Korea completed a questionnaire about alcohol use, parental attention, and parental alcohol consumption, and other risk and protective factors. Data were analyzed with chi-square and regression analyses. RESULTS: Nearly 63% of the student drinkers had experienced at least one to two alcohol-related problems in their lives. Two-thirds of all 2077 students indicated that at least one of their parents had an alcohol-related problem and that approximately 29% had experienced several problems. Results of random effects ordinal logistic regression analyses suggest a dose-response relationship between parental and youth alcohol-related problems. Youth who report having parents with some and many alcohol problems were 30% (Odds Ratios [OR] = 1.30; 95% Confidence Interval [CI] = 1.10 - 1.53) and 55% (OR = 1.55; 95%CI = 1.23 - 1.95) more likely to experience alcoholrelated problems than youth whose parents do not have alcohol problems, respectively, after statistically adjusting for important covariates. CONCLUSIONS: This study presents evidence that alcoholrelated problems among adolescent female students is highly prevalent. Also, the study findings reveal a high percentage of parents with alcohol problems, as reported by students. This study presents evidence of what might be a hidden problem among adults and youths in the Republic of Korea that merits serious attention."
JORGE DELVA,The role of peers and parents in predicting alcohol consumption among Chilean youth,"This study estimated marginal associations of parent- and peer-related measures to examine the different patterns of lifetime ever-use and frequency of alcohol consumption among adolescents in Santiago, Chile (N=918). Probit and negative binomial models were applied to predict the probability of ever-use and the average number of drinks consumed in the past 30 days. Results supported the profound role of peer-relationships in the development of youth drinking behavior. Particularly, peer pressure seemed more important in predicting alcohol ever-use than the frequency of drinking. Simultaneously, parents, especially fathers, played a crucial protective role. Policies aimed at preventing various drinking patterns may be more effective if they not only focus on the targeted adolescents, but also reach out to peers and parents."
JORGE DELVA,"Trends in cannabis use among immigrants in the United States, 2002-2017: evidence from two national surveys","BACKGROUND AND AIMS: Findings from recent studies suggest that, among the general population of adults, the prevalence of cannabis use has increased over the last decade in the United States (US). And yet, there is much we do not know regarding the trends in cannabis use among immigrants. We address this important shortcoming by examining data on immigrants vis-à-vis US-born individuals using two national surveys. METHODS: We examine trend data from the National Epidemiologic Study on Alcohol and Related Conditions (NESARC, 2001-2013) and the National Survey on Drug Use and Health's Restricted Data Analysis System (NSDUH, 2002-2017). Main outcomes were past year cannabis use and cannabis use disorder with survey adjusted prevalence estimates generated for immigrants and US-born individuals. RESULTS: In the NESARC, significant increases in the past year prevalence of cannabis use were observed both among US-born (2001-2002: 4.53%, 2012-2013: 10.74%) and immigrant participants (2001-2002: 1.67%, 2012-2013: 3.32%). We also found significant increases among immigrants arriving before age 12 and among immigrants from Latin America and Europe. In the NSDUH, we observed a significantly higher prevalence of cannabis use in 2016-2017 (6.3%) when compared to 2002-2003 (4.4%). CONCLUSIONS: Findings make clear that cannabis use among US-born individuals has consistently been higher than that of immigrants since the early 2000s. However, while rates of cannabis use have declined among US-born adolescents in recent years, the prevalence of cannabis use has remained stable among immigrant adolescents. At the same time, cannabis use increased two-fold among both US-born and immigrant adults."
JORGE DELVA,Trends in substance use prevention program participation among adolescents in the U.S.,"PURPOSE: The aim of the article was to examine national trends in adolescent participation in substance use prevention programs (SUPP). METHODS: We examine 15 years of cross-sectional data (2002-2016) from the National Survey on Drug Use and Health. Main outcomes were participation in past-year school and community-based SUPP (no/yes). Logistic regression was used to examine trends in the prevalence of participation. RESULTS: Participation in school-based SUPP decreased significantly from 48% among adolescents in 2002-2003 to 40% in 2015-2016, a 16.5% proportional decline. Significant declines for school-based participation were observed in all demographic and drug involvement subgroups examined. Youth participation in community-based SUPP also decreased significantly. However, this downward trend was significant only among younger teens, females, youth in very low (<$20,000) and moderate ($40,000-$74,999) income households and in rural areas. CONCLUSIONS: Participation in SUPP has decreased since the early 2000s, with noteworthy declines among Latino youth and youth from rural areas and socioeconomically disadvantaged backgrounds."
JORGE DELVA,The school food environment and student body mass index and food consumption: 2004 to 2007 national data,"PURPOSE: This study identifies trends in the availability of various food choices in United States' middle and high schools from 2004 to 2007, and examines the potential associations between such food availability and students' self-reported eating habits and body mass index (BMI)-related outcomes. METHODS: Data are based on nationally representative samples of 78,442 students in 684 secondary schools surveyed from 2004 to 2007 as part of the Youth, Education, and Society (YES) study and the Monitoring the Future (MTF) study. In the YES study, school administrators and food service managers completed self-administered questionnaires on their school's food environment. In the MTF study, students in the same schools completed self-administered questionnaires, providing data used to construct BMI and food consumption measures. RESULTS: Overall, there was a decrease in the availability of regular-sugar/fat food items in both middle and high schools, and some indication of an increase in high school availability of reduced-fat food items through school lunch or a la carte. Some minimal evidence was found for relationships between the school food environment and student BMI-related outcomes and food consumption measures. CONCLUSIONS: United States secondary schools are making progress in the types of foods offered to students, with food items of lower nutritional value becoming less prevalent in recent years. Continued monitoring of food environment trends may help clarify whether and how such factors relate to youth health outcomes."
JORGE DELVA,Marijuana use associated with worse verbal learning and delayed recall in a sample of young adults,"BACKGROUND: There is concern about the cognitive consequences of marijuana consumption. AIM: To assess the influence of current and past marijuana use and frequency on verbal learning and memory in a sample of adults aged 21 years old. MATERIAL AND METHODS: Marijuana use was assessed using a clinician administered interview in 654 participants (56% females), who reported frequency of use, age of first use and whether its use led to problems in their lives. The CogState International Shopping List was administered to assess learning and memory. RESULTS: Seventy percent reported ever using marijuana, 46% consuming during the past year and 27% during the past 30 days. The latter scored significantly lower on delayed recall. Current and frequent use were significantly associated with lower accuracy in verbal learning and memory. CONCLUSIONS: In this cohort of adults aged 21 years old, marijuana use was prevalent and related to worse verbal memory."
DEBORAH CARR,How and why weight stigma drives the obesity 'epidemic' and harms health,"BACKGROUND: In an era when obesity prevalence is high throughout much of the world, there is a correspondingly pervasive and strong culture of weight stigma. For example, representative studies show that some forms of weight discrimination are more prevalent even than discrimination based on race or ethnicity. DISCUSSION: In this Opinion article, we review compelling evidence that weight stigma is harmful to health, over and above objective body mass index. Weight stigma is prospectively related to heightened mortality and other chronic diseases and conditions. Most ironically, it actually begets heightened risk of obesity through multiple obesogenic pathways. Weight stigma is particularly prevalent and detrimental in healthcare settings, with documented high levels of ‘anti-fat’ bias in healthcare providers, patients with obesity receiving poorer care and having worse outcomes, and medical students with obesity reporting high levels of alcohol and substance use to cope with internalized weight stigma. In terms of solutions, the most effective and ethical approaches should be aimed at changing the behaviors and attitudes of those who stigmatize, rather than towards the targets of weight stigma. Medical training must address weight bias, training healthcare professionals about how it is perpetuated and on its potentially harmful effects on their patients. CONCLUSION: Weight stigma is likely to drive weight gain and poor health and thus should be eradicated. This effort can begin by training compassionate and knowledgeable healthcare providers who will deliver better care and ultimately lessen the negative effects of weight stigma."
DEBORAH CARR,COVID-19 and the long-standing vulnerabilities of older adults,
DEBORAH CARR,Do family relationships buffer the impact of disability on older adults' daily mood? An exploration of gender and marital status differences,"OBJECTIVE: We evaluate whether non-spousal family support and strain moderate the effect of disability on two daily emotions (happiness and frustration) among older adults, and whether these patterns differ by gender among married persons, and by marital status among women. BACKGROUND: Stress buffering perspectives predict that harmful effects of stress on well-being are buffered by family support, whereas stress proliferation models suggest these effects are intensified by family strain. The extent to which family relationships moderate associations between stress and well-being may vary on the basis of gender and marital status, as non-spousal family ties are considered especially salient for women and those without a romantic partner. METHOD: Daily diary data are from the 2013 Disability and Use of Time supplement to the Panel Study of Income Dynamics (n=1,474), a national sample of adults ages 60+. Multivariate regression models are estimated for married/partnered men and women, and formerly married women. RESULTS: Neither family support nor strain moderated the effect of severe impairment on married men's daily emotions. Family support buffered the effect of severe impairment on frustration among divorced and widowed women, but not their married counterparts. Counterintuitively, family arguments mitigated against frustration and increased happiness among married women with severe impairment. CONCLUSION: Consistent with stress buffering perspectives, family support was most protective for the vulnerable population of formerly married older women with severe impairment. IMPLICATIONS: This study underscores the importance of family support for the large and growing population of formerly married women managing health-related challenges in later life."
DEBORAH CARR,Families in later life: a consequence and engine of social inequalities,"The implications of economic inequality for American families are profound, giving rise to widening race and socioeconomic disparities in key family transitions including marriage, divorce, cohabitation, childrearing, and family bereavement. However, little scholarly attention focuses on how these divergences in family structure shape the health and well-being of older adults, and especially older women. In this chapter, I propose that family relationships are an important although overlooked mechanism linking economic inequality to persistent race, socioeconomic, and gender disparities in late-life well-being. I provide a statistical snapshot of older adults’ families, showing how rates of marriage, divorce, widowhood, and remarriage differ markedly on the basis of gender, socioeconomic status, and race, with these disparities widening against the backdrop of rising economic inequality in the late 20th and early 21th centuries. I then describe how these patterns perpetuate disparities in late-life economic well-being, due in part to the structure of Social Security benefits which advantage those whose family lives conformed to the mid-20th century White middle-class ”ideal” of a lifelong marriage between a male breadwinner and female homemaker. I further show how three stressful aspects of family lives — family bereavement, custodial grandparenting, and caregiving –– disproportionately befall women, and especially low-income and women of color. As such, these family-related stressors exacerbate race, class, and gender-based disparities in health and well-being. I conclude by highlighting social policies that may help to mitigate against these disparities, and provide resources so that Americans of all backgrounds have an opportunity to grow old with dignity"
DEBORAH CARR,"Review of values at the end of life: the logic of palliative care, by Roi Livne",
DEBORAH CARR,"Estimating prevalence of bereavement, its contribution to risk for binge drinking, and other high-risk health states in a state population survey, 2019 Georgia behavioral risk factor surveillance survey","BACKGROUND: Binge drinking is a pattern of alcohol abuse. Its prevalence and associated risk factors are not well documented. Heavy drinking, on the other hand, has a well-documented association with bereavement. This report uses a cross-sectional, population-based survey to estimate prevalence of bingeing and its association with new bereavement. Bingeing is defined as 4 or more drinks (women) or 5 or more drinks (men) in a 2-4-h setting. For the first time in 2019, the Georgia Behavioral Risk Factor Surveillance Survey (BRFSS) included a bereavement item: 'Have you experienced the death of a family member or close friend in the years 2018 or 2019?' METHODS: Georgia BRFSS is a complex sampling survey administered annually. It is designed to represent the 8.1 million people aged 18 years and older in the U.S. state of Georgia. Alcohol consumption patterns are routinely measured in the common core. In 2019, the state added a new item probing for bereavement in the prior 24 months predating the COVID-19 pandemic. Imputation and weighting techniques were applied to yield the population prevalence rates of new bereavement, bingeing, and their co-occurrence with other high-risk health behaviors and outcomes. Multivariate models, adjusted for age, gender, and race, were used to estimate the risk for other unhealthy behaviors posed by the co-occurrence of bereavement and bingeing. RESULTS: In Georgia, bereavement (45.8%), and alcohol consumption (48.8%) are common. Bereavement and alcohol use co-occurred among 1,796,817 people (45% of all drinkers) with a subset of 608,282 persons reporting bereavement combined with bingeing. The most common types of bereavement were death of a friend/neighbor (30.7%) or three plus deaths (31.8%). CONCLUSIONS: While bingeing is a known risk to public health, its co-occurrence with recent bereavement is a new observation. Public health surveillance systems need to monitor this co-occurrence to protect both individual and societal health. In a time of global bereavement, documenting its influence on binge drinking can support the work towards Sustainable Development Goal #3-Good health and Well-Being."
DEBORAH CARR,Perceived disrespectful treatment in low-income healthcare settings through the lens of intersectionality,"Race and gender differences in the quality of one’s health care encounters are widely documented, but few studies explore the multiplicative impacts of race and gender. Drawing on intersectionality frameworks, we contrast White, Black, Hispanic, Asian, and Native American men’s and women’s perceptions of disrespectful treatment from health care providers and staff (e.g., receptionists, clerks) in low-income health settings. Data are from the 2014 Health Center Patient Survey (N = 5385), a nationally representative survey of community-dwelling patients receiving care at health centers funded through Section 330 of the Public Health Service Act. Our results show race and sex differences in the study outcomes, yet weak support for intersectional effects. Asian and Native American patients report more disrespectful treatment from providers, relative to other ethnic groups. Women are more likely than men to report disrespectful treatment from staff. Asians also report disrespectful treatment from staff. Health care providers and staff may require training focused on the distinctive needs of Asian patients, for whom “saving face” may be a salient goal. Structural changes to reduce wait times and employ medical interpreters to ensure clear communication, and staff training focused on verbal and nonverbal communication may mitigate against actions perceived as disrespectful in low-income health settings."
DEBORAH CARR,Occupational differences in advance care planning: are medical professionals more likely to plan?,"Advance care planning (ACP) helps ensure that treatment preferences are met at the end of life. Medical professionals typically are responsible for facilitating patients' ACP, and may be especially effective in doing so if they have first-hand insights from their own planning. However, no large-scale U.S. studies examine whether persons working on the front lines of health care are more likely than other workers to have done ACP. We contrast the use of three ACP components (living wills, durable power of attorney for health care, and informal discussions) among persons working in medical, legal, social/health support services, other professional, and other non-professional occupations. Data are from the Health and Retirement Study (n = 7668) and Wisconsin Longitudinal Study (n = 5464). Multivariable logistic regression analyses are adjusted for socioeconomic, demographic, health, and psychosocial factors that may confound associations between occupational group and ACP. Medical professionals in both samples are more likely than other professional workers to discuss their own treatment preferences, net of all controls. Medical professionals in the WLS are more likely to execute living wills and DPAHC designations, whereas legal professionals in the HRS are more likely to name a DPAHC. Non-professional workers are significantly less likely to do all three types of planning, although these differences are accounted for by socioeconomic factors. Social and health services professionals are no more likely than other professionals to do ACP. The on-the-job experiences and expertise of medical professionals may motivate them to discuss their own end-of-life preferences, which may render them more trustworthy sources of information for patients and clients. The Affordable Care Act provides reimbursement for medical professionals' end-of-life consultations with Medicare beneficiary patients, yet practitioners uncomfortable with such conversations may fail to initiate them. Programs to increase medical professionals' own ACP may have the secondary benefit of increasing ACP among their patients."
DEBORAH CARR,Physical disability at work: how functional limitation affects perceived discrimination and interpersonal relationships in the workplace,"Adults with disability have significantly lower rates of labor force participation relative to persons without disability, although it is unclear whether this disparity extends to subjective workplace experiences. Using data from the 2004 to 2006 wave of the National Survey of Midlife Development in the United States (n =2,030), we evaluate: (1) whether U.S. workers with physical disability report higher levels of perceived job discrimination and unequal workplace opportunities and lower levels of supervisor and coworker support and (2) whether these patterns differ by sex, age, and occupation group. We find that workers with physical disability fare significantly worse on all four outcomes net of covariates. Disability takes a particularly large toll on men's perceived workplace opportunities and white-collar employees' relationships with coworkers. Young adult workers (ages 30-39) with disability report significantly more support from their supervisor relative to their counterparts without disability. We discuss implications for research and policy."
DEBORAH CARR,Paternal occupation and delirium risk in older adults: a potential marker of early-life exposures,"BACKGROUND AND OBJECTIVES: Delirium is a common disorder among older adults following hospitalization or major surgery. Whereas many studies examine the risk of proximate exposures and comorbidities, little is known about pathways linking childhood exposures to later-life delirium. In this study, we explored the association between paternal occupation and delirium risk. RESEARCH DESIGN AND METHODS: A prospective observational cohort study of 528 older adults undergoing elective surgery at two academic medical centers. Paternal occupation group (white collar vs. blue collar) served as our independent variable. Delirium incidence was assessed using the Confusion Assessment Method (CAM) supplemented by medical chart review. Delirium severity was measured using the peak CAM-Severity score (CAM-S Peak), the highest value of CAM-S observed throughout the hospital stay. RESULTS: Blue-collar paternal occupation was significantly associated with a higher rate of incident delirium (91/234, 39%) compared with white-collar paternal occupation (84/294, 29%), adjusted odds ratio OR (95% confidence interval [CI]) = 1.6 (1.1, 2.3). All analyses were adjusted for participant age, race, gender, and Charlson Comorbidity Index. Blue-collar paternal occupation was also associated with greater delirium severity, with a mean score (SD) of 4.4 (3.3), compared with white-collar paternal occupation with a mean score (SD) of 3.5 (2.8). Among participants reporting blue-collar paternal occupation, we observed an adjusted mean difference of 0.86 (95% CI = 0.4, 1.4) additional severity units. DISCUSSION AND IMPLICATIONS: Blue-collar paternal occupation is associated with greater delirium incidence and severity, after adjustment for covariates. These findings support the application of a life-course framework to evaluate the risk of later-life delirium and delirium severity. Our results also demonstrate the importance of considering childhood exposures, which may be consequential even decades later."
DEBORAH CARR,Older Americans with disability are vulnerable to economic and food insecurity during COVID-19,"This study investigated whether older Americans with physical disability were vulnerable to three types of economic insecurity (difficulty paying regular bills, difficulty paying medical bills, income loss) and two types of food insecurity (economic obstacles, logistical obstacles) during the early months of the COVID-19 pandemic. We evaluated the extent to which associations are moderated by three personal characteristics (age, sex, race/ethnicity) and two pandemic-specific risk factors (job loss, COVID-19 diagnosis). Data were from a random 25 percent subsample of the Health and Retirement Study participants who completed a COVID-19 module administered in 2020. Our analytic sample included 3,166 adults aged 51 and older. We estimated logistic regression models to document the odds of experiencing each hardship. Persons with three or more functional limitations reported significantly higher odds of both types of food insecurity, and difficulty paying regular and medical bills, relative to those with no limitations. After controlling for health conditions, effects were no longer significant for paying medical bills, and attenuated yet remained statistically significant for other outcomes. Patterns did not differ significantly on the basis of the moderator variables. Older adults with more functional limitations are vulnerable to economic and food insecurity during the pandemic, potentially exacerbating the physical and emotional health threats imposed by the pandemic. Our findings reveal an urgent need to promote policies and procedures to protect older adults with disability from economic and food insecurity. Supports for older adults with disability should focus on logistical as well as financial support for ensuring food security."
DEBORAH CARR,Racial differences in the impact of subjective life expectancy on advance care planning,"The need for advance care planning (ACP) is heightened during the COVID-19 pandemic, especially for older Blacks and Latinx persons who are at a disproportionate risk of death from both infectious and chronic disease. A potentially important yet underexplored explanation for well-documented racial disparities in ACP is subjective life expectancy (SLE), which may impel or impede ACP. Using Health and Retirement Study data (n=7484), we examined the extent to which perceived chances of living another 10 years (100, 51-99, 50, 1-49, or 0 percent) predict three aspects of ACP (living will (LW), durable power of attorney for health care designations (DPAHC), and discussions). We use logistic regression models to predict the odds of each ACP behavior, adjusted for sociodemographic, health, and depressive symptoms. We found modest evidence that SLE predicts ACP behaviors. Persons who are 100% certain they will be alive in ten years are less likely (OR = .68 and .71, respectively) whereas those with pessimistic survival prospects are more likely (OR = 1.23 and 1.15, respectively) to have a LW and a DPAHC, relative to those with modest perceived survival. However, upon closer inspection, these patterns hold only for those whose LW specify aggressive measures versus no LW. We found no race differences for formal aspects of planning (LW, DPAHC) although we did detect differences for informal discussions. Blacks with pessimistic survival expectations are more likely to have discussions, whereas Latinos are less likely relative to whites. We discuss implications for policies and practices to increase ACP rates."
DEBORAH CARR,Global population aging and heat exposure in the 21st century: implications for late-life well-being and policy,"Climate change has profound consequences for older adults’ well-being. Global increases in the frequency, intensity and duration of extreme heat spells pose the most direct threats, given older adults’ underlying health conditions and reduced capacity to thermoregulate. Individual-level consequences of heat exposure are well-documented, yet gerontological research has paid less attention to older adults’ heat exposure at the population level. World regions with both increasing concentrations of older adults and high temperature extremes are potential hotspots where growing aged populations are at risk of heat-related health threats. We estimate older adults’ current and future heat exposure in six global regions (Africa, Asia, Europe, North America, Oceania, South America). Analyses use NASA NEX Global Daily Downscaled Product climate data and country-level projections for the size and distribution of the age 65+ population. We focus on measures of cumulative heat exposure (cooling degree days) and acute exposure to heat extremes (95th percentile of daily maximum temperatures). We project that by 2050, populations aged 65+ globally exposed to &amp;gt;1200 cooling degree days will double. We also project that one-quarter of the aged population worldwide (roughly 340 million) will be living in climates with 95th percentile TMax95 &amp;gt; 99.5 F◦, a critical exposure threshold for health. The drivers of this population-level exposure vary by region. Population aging will drive exposure in historically hot regions in Africa and Asia, whereas climate change will drive heat exposure in historically older regions in Europe and North America. These results can inform public policy and local decision-making."
DEBORAH CARR,Population aging and heat exposure in the 21st century: which U.S. regions are at greatest risk and why?,"BACKGROUND AND OBJECTIVES: The co-occurring trends of population aging and climate change mean that rising numbers of U.S. older adults are at risk of intensifying heat exposure. We estimate county-level variations in older populations' heat exposure in the early (1995-2014) and mid (2050) 21st century. We identify the extent to which rising exposures are attributable to climate change versus population aging. RESEARCH DESIGN AND METHODS: We estimate older adults' heat exposure in 3,109 counties in the 48 contiguous U.S. states. Analyses use NASA NEX Global Daily Downscaled Product (NEX-GDDP-CMIP6) climate data and county-level projections for the size and distribution of the U.S. age 69+ population. RESULTS: Population aging and rising temperatures are documented throughout the United States, with particular ""hotspots"" in the Deep South, Florida, and parts of the rural Midwest. Increases in heat exposure by 2050 will be especially steep in historically colder regions with large older populations in New England, the upper Midwest, and rural Mountain regions. Rising temperatures are driving exposure in historically colder regions, whereas population aging is driving exposure in historically warm southern regions. DISCUSSION AND IMPLICATIONS: Interventions to address the impacts of temperature extremes on older adult well-being should consider the geographic distribution and drivers of this exposure. In historically cooler areas where climate change is driving exposures, investments in warning systems may be productive, whereas investments in health care and social services infrastructures are essential in historically hot regions where exposures are driven by population aging."
LISA SULLIVAN,Effect of Aging on A1C Levels in Individuals Without Diabetes,"OBJECTIVE: Although glycemic levels are known to rise with normal aging, the nondiabetic A1C range is not age specific. We examined whether A1C was associated with age in nondiabetic subjects and in subjects with normal glucose tolerance (NGT) in two population-based cohorts. RESEARCH DESIGN AND METHODS: We performed cross-sectional analyses of A1C across age categories in 2,473 nondiabetic participants of the Framingham Offspring Study (FOS) and in 3,270 nondiabetic participants from the National Health and Nutrition Examination Survey (NHANES) 2001–2004. In FOS, we examined A1C by age in a subset with NGT, i.e., after excluding those with impaired fasting glucose (IFG) and/or impaired glucose tolerance (IGT). Multivariate analyses were performed, adjusting for sex, BMI, fasting glucose, and 2-h postload glucose values. RESULTS: In the FOS and NHANES cohorts, A1C levels were positively associated with age in nondiabetic subjects. Linear regression revealed 0.014- and 0.010-unit increases in A1C per year in the nondiabetic FOS and NHANES populations, respectively. The 97.5th percentiles for A1C were 6.0% and 5.6% for nondiabetic individuals aged <40 years in FOS and NHANES, respectively, compared with 6.6% and 6.2% for individuals aged ≥70 years (Ptrend < 0.001). The association of A1C with age was similar when restricted to the subset of FOS subjects with NGT and after adjustments for sex, BMI, fasting glucose, and 2-h postload glucose values. CONCLUSIONS: A1C levels are positively associated with age in nondiabetic populations even after exclusion of subjects with IFG and/or IGT. Further studies are needed to determine whether age-specific diagnostic and treatment criteria would be appropriate."
LISA SULLIVAN,"Plasma Leptin Levels and Incidence of Heart Failure, Cardiovascular Disease, and Total Mortality in Elderly Individuals","OBJECTIVE: Obesity predisposes individuals to congestive heart failure (CHF) and cardiovascular disease (CVD). Leptin regulates energy homeostasis, is elevated in obesity, and influences ventricular and vascular remodeling. We tested the hypothesis that leptin levels are associated with greater risk of CHF, CVD, and mortality in elderly individuals. RESEARCH DESIGN AND METHODS: We evaluated 818 elderly (mean age 79 years, 62% women) Framingham Study participants attending a routine examination at which plasma leptin was assayed. RESULTS: Leptin levels were higher in women and strongly correlated with BMI (P < 0.0001). On follow-up (mean 8.0 years), 129 (of 775 free of CHF) participants developed CHF, 187 (of 532 free of CVD) experienced a first CVD event, and 391 individuals died. In multivariable Cox regression models adjusting for established risk factors, log-leptin was positively associated with incidence of CHF and CVD (hazard ratio [HR] per SD increment 1.26 [95% CI 1.03–1.55] and 1.28 [1.09–1.50], respectively). Additional adjustment for BMI nullified the association with CHF (0.97 [0.75–1.24]) but only modestly attenuated the relation to CVD incidence (1.23 [1.00–1.51], P = 0.052). We observed a nonlinear, U-shaped relation between log-leptin and mortality (P = 0.005 for quadratic term) with greater risk of death evident at both low and high leptin levels. CONCLUSIONS: In our moderate-sized community-based elderly sample, higher circulating leptin levels were associated with a greater risk of CHF and CVD, but leptin did not provide incremental prognostic information beyond BMI. Additional investigations are warranted to elucidate the U-shaped relation of leptin to mortality."
LISA SULLIVAN,The long-baseline neutrino experiment: exploring fundamental symmetries of the universe,"The preponderance of matter over antimatter in the early Universe, the dynamics of the supernova bursts that produced the heavy elements necessary for life and whether protons eventually decay --- these mysteries at the forefront of particle physics and astrophysics are key to understanding the early evolution of our Universe, its current state and its eventual fate. The Long-Baseline Neutrino Experiment (LBNE) represents an extensively developed plan for a world-class experiment dedicated to addressing these questions. LBNE is conceived around three central components: (1) a new, high-intensity neutrino source generated from a megawatt-class proton accelerator at Fermi National Accelerator Laboratory, (2) a near neutrino detector just downstream of the source, and (3) a massive liquid argon time-projection chamber deployed as a far detector deep underground at the Sanford Underground Research Facility. This facility, located at the site of the former Homestake Mine in Lead, South Dakota, is approximately 1,300 km from the neutrino source at Fermilab -- a distance (baseline) that delivers optimal sensitivity to neutrino charge-parity symmetry violation and mass ordering effects. This ambitious yet cost-effective design incorporates scalability and flexibility and can accommodate a variety of upgrades and contributions. With its exceptional combination of experimental configuration, technical capabilities, and potential for transformative discoveries, LBNE promises to be a vital facility for the field of particle physics worldwide, providing physicists from around the globe with opportunities to collaborate in a twenty to thirty year program of exciting science. In this document we provide a comprehensive overview of LBNE's scientific objectives, its place in the landscape of neutrino physics worldwide, the technologies it will incorporate and the capabilities it will possess."
HENRIK SELIN,The Minamata Convention and the future of mercury abatement,"Pardee Faculty Fellow Henrik Selin examines the new Minamata Convention on Mercury, a global agreement intended to “protect human health and the environment from anthropogenic emissions and releases of mercury and mercury compounds.” Selin argues that the new convention is “more legally and politically important than environmentally significant.” To achieve truly meaningful reductions in mercury releases to the environment and threats to human health, he says collaborative measures must be enacted across global, regional, national, and local scales of governance, with support from inter-governmental organizations, non-governmental organizations and industry associations."
HENRIK SELIN,"Trade in the balance: reconciling trade and climate policy: report of the Working Group on Trade, Investment, and Climate Policy","This report outlines the general tensions between the trade and investment regime and climate policy, and outlines a framework toward making trade and investment rules more climate friendly. Members of the working group have contributed short pieces addressing a range of issues related to the intersection of trade and climate policy. The first two are by natural scientists. Anthony Janetos discusses the need to address the effects of international trade on efforts to limit the increase in global annual temperature to no more than 2oC over preindustrial levels. James J. Corbett examines the failure of the Trans Pacific Partnership (TPP) and the Transatlantic Trade and Investment Partnership (TTIP) to adequately address the environmental implications of shipping and maritime transport. The next two pieces are by economists who examine economic aspects of the trade-climate linkage. Irene Monasterolo and Marco Raberto discuss the potential impacts of including fossil fuel subsidies reduction under the TTIP. Frank Ackerman explores the economic costs of efforts to promote convergence of regulatory standards between the United States and the European Union under the TTIP. The following two contributions are by legal scholars. Brooke Güven and Lise Johnson explore the potential for international investment treaties to redirect investment flows to support climate change mitigation and adaptation, particularly with regard to China and India. Matt Porterfield provides an overview of the ways in which both existing and proposed trade and investment agreements could have either “climate positive” or “climate negative” effects on mitigation policies. The final article is by Tao Hu, a former WTO trade and environment expert advisor for China and currently at the World Wildlife Fund, arguing that the definition of environmental goods and services’ under the WTO negotiations needs to be expanded to better incorporate climate change."
HENRIK SELIN,Beyond Rio+20: governance for a green economy,"As an intellectual contribution to the preparations for the 2012 United Nations Conference on Sustainable Development (UNCSD, a.k.a. Rio +20), the Boston University Frederick S. Pardee Center for the Study of the Longer-Range Future convened a task force of experts to discuss the role of institutions in the actualization of a green economy in the context of sustainable development. A stellar group of experts from academia, government and civil society convened at the Pardee Center and were asked to outline ideas about what the world has learned about institutions for sustainable development from the past, and what we can propose about the governance challenges and opportunities for the continuous development of a green economy in the future. The Task Force members were encouraged to think big and think bold. They were asked to be innovative in their ideas, and maybe even a little irreverent and provocative. They were charged specifically NOT to come to consensus about specific recommendations, but to present a variety and diversity of views. This report presents their thoughts and ideas."
HENRIK SELIN,Climate finance and developing countries: the need for regime development,"The Paris Agreement in Article 2 calls for “making finance flows consistent with a pathway towards low greenhouse gas emissions and climate-resilient development” in support of a global transition towards sustainability. Climate finance include all forms of financial support from public, private and alternative sources that target low-carbon and climate-resilient development in all countries of the world. As a critical equity and justice issue to help the most vulnerable that have contributed very little to the problem, Article 9 of the Paris Agreement stipulates that developed countries shall provide financial resources to assist developing countries to help them reduce GHG emissions and adapt to a changing climate. The stated climate finance goal is for developed countries to mobilize jointly at least USD 100 billion a year from public and private sources by 2020, to be scaled up over time as part of the implementation of the Paris Agreement. The international climate finance regime set up to support developing countries towards this target involves a large number of actors, funds and mechanisms as financial support comes in many different forms. Existing estimates of current climate finance flows from multilateral, bilateral and private sources, however, are highly uncertain and subject to much controversy based on unavailability of data, methodological variations, and disagreements over what should be counted as climate finance. [TRUNCATED]"
HENRIK SELIN,Managing hazardous chemicals: longer-range challenges,"Improving global chemicals management is a significant sustainable development issue, involving many longer-range challenges. This paper examines some of these challenges. It begins by describing hazardous chemicals as a longer-range problem. This is followed by an outline of the global policy framework for managing hazardous chemicals. Next, the paper discusses four sets of management challenges for better environmental and human health protection: 1) Enhancing ratification and implementation of existing regulations; 2) Expanding risk assessments and controls; 3) Improving management capacity and raising awareness; and 4) Minimizing generation of hazardous chemicals and wastes. Furthermore, the paper argues that the adoption of more proactive and precautionary policies and management approaches is ultimately needed to achieve necessary environmental and human health protection standards. While some such policy and regulatory changes are under way in the European Union and other regions, they are not yet sufficiently reflected in international law."
MICHAEL D SORENSON,Ancient origin and maternal inheritance of blue cuckoo eggs,"Maternal inheritance via the female-specific W chromosome was long ago proposed as a potential solution to the evolutionary enigma of co-existing host-specific races (or 'gentes') in avian brood parasites. Here we report the first unambiguous evidence for maternal inheritance of egg colouration in the brood-parasitic common cuckoo Cuculus canorus. Females laying blue eggs belong to an ancient (∼2.6 Myr) maternal lineage, as evidenced by both mitochondrial and W-linked DNA, but are indistinguishable at nuclear DNA from other common cuckoos. Hence, cuckoo host races with blue eggs are distinguished only by maternally inherited components of the genome, which maintain host-specific adaptation despite interbreeding among males and females reared by different hosts. A mitochondrial phylogeny suggests that blue eggs originated in Asia and then expanded westwards as female cuckoos laying blue eggs interbred with the existing European population, introducing an adaptive trait that expanded the range of potential hosts."
MICHAEL D SORENSON,Strong mitochondrial DNA support for a Cretaceous origin of modern avian lineages,"Determining an absolute timescale for avian evolutionary history has proven contentious. The two sources of information available, paleontological data and inference from extant molecular genetic sequences (colloquially, 'rocks' and 'clocks'), have appeared irreconcilable; the fossil record supports a Cenozoic origin for most modern lineages, whereas molecular genetic estimates suggest that these same lineages originated deep within the Cretaceous and survived the K-Pg (Cretaceous-Paleogene; formerly Cretaceous-Tertiary or K-T) mass-extinction event. These two sources of data therefore appear to support fundamentally different models of avian evolution. The paradox has been speculated to reflect deficiencies in the fossil record, unrecognized biases in the treatment of genetic data or both. Here we attempt to explore uncertainty and limit bias entering into molecular divergence time estimates through: (i) improved taxon (n = 135) and character (n = 4594 bp mtDNA) sampling; (ii) inclusion of multiple cladistically tested internal fossil calibration points (n = 18); (iii) correction for lineage-specific rate heterogeneity using a variety of methods (n = 5); (iv) accommodation of uncertainty in tree topology; and (v) testing for possible effects of episodic evolution."
MICHAEL D SORENSON,"Multimodal signalling in estrildid finches: song, dance and colour are associated with different ecological and life-history traits","Sexual traits (e.g. visual ornaments, acoustic signals, courtship behaviour) are often displayed together as multimodal signals. Some hypotheses predict joint evolution of different sexual signals (e.g. to increase the efficiency of communication) or that different signals trade off with each other (e.g. due to limited resources). Alternatively, multiple signals may evolve independently for different functions, or to communicate different information (multiple message hypothesis). We evaluated these hypotheses with a comparative study in the family Estrildidae, one of the largest songbird radiations, and one that includes many model species for research in sexual selection and communication. We found little evidence for either joint evolution or trade-offs between song and colour ornamentation. Some negative correlations between dance repertoire and song traits may suggest a functional compromise, but generally courtship dance also evolved independently from other signals. Instead of correlated evolution, we found that song, dance and colour are each related to different socio-ecological traits. Song complexity evolved together with ecological generalism, song performance with investment in reproduction, dance with commonness and habitat type, whereas colour ornamentation was shown previously to correlate mostly with gregariousness. We conclude that multimodal signals evolve in response to various socio-ecological traits, suggesting the accumulation of distinct signalling functions."
MICHAEL D SORENSON,"Population differentiation and historical demography of the threatened snowy plover Charadrius nivosus (Cassin, 1858)","Delineating conservation units is a complex and often controversial process that is particularly challenging for highly vagile species. Here, we reassess population genetic structure and identify those populations of highest conservation value in the threatened snowy plover (Charadrius nivosus, Cassin, 1858), a partial migrant shorebird endemic to the Americas. We use four categories of genetic data—mitochondrial DNA (mtDNA), microsatellites, Z-linked and autosomal single nucleotide polymorphisms (SNPs)—to: (1) assess subspecies delineation and examine population structure (2) compare the sensitivity of the different types of genetic data to detect spatial genetic patterns, and (3) reconstruct demographic history of the populations analysed. Delineation of two traditionally recognised subspecies was broadly supported by all data. In addition, microsatellite and SNPs but not mtDNA supported the recognition of Caribbean snowy plovers (C. n. tenuirostris) and Floridian populations (eastern C. n. nivosus) as distinct genetic lineage and deme, respectively. Low migration rates estimated from autosomal SNPs (m < 0.03) reflect a general paucity of exchange between genetic lineages. In contrast, we detected strong unidirectional migration (m = 0.26) from the western into the eastern nivosus deme. Within western nivosus, we found no genetic differentiation between coastal Pacific and inland populations. The correlation between geographic and genetic distances was weak but significant for all genetic data sets. All demes showed signatures of bottlenecks occurring during the past 1000 years. We conclude that at least four snowy plover conservation units are warranted: in addition to subspecies nivosus and occidentalis, a third unit comprises the Caribbean tenuirostris lineage and a fourth unit the distinct eastern nivosus deme."
MICHAEL D SORENSON,Coevolution of male and female genital morphology in waterfowl,"Most birds have simple genitalia; males lack external genitalia and females have simple vaginas. However, male waterfowl have a phallus whose length (1.5->40 cm) and morphological elaborations vary among species and are positively correlated with the frequency of forced extra-pair copulations among waterfowl species. Here we report morphological complexity in female genital morphology in waterfowl and describe variation vaginal morphology that is unprecedented in birds. This variation comprises two anatomical novelties: (i) dead end sacs, and (ii) clockwise coils. These vaginal structures appear to function to exclude the intromission of the counter-clockwise spiralling male phallus without female cooperation. A phylogenetically controlled comparative analysis of 16 waterfowl species shows that the degree of vaginal elaboration is positively correlated with phallus length, demonstrating that female morphological complexityhas co-evolved with male phallus length. Intersexual selection is most likely responsible for the observed coevolution, although identifying the specific mechanism is difficult. Our results suggest that females have evolved a cryptic anatomical mechanism of choice in response to forced extra-pair copulations."
MICHAEL D SORENSON,Leveraging genomics to understand threats in a migratory waterbird,"Understanding how risk factors affect populations across their annual cycle is a major challenge for conserving migratory birds. For example, disease outbreaks may happen on the breeding grounds, the wintering grounds, or during migration and are expected to accelerate under climate change. The ability to identify the geographic origins of impacted individuals, especially outside of breeding areas, might make it possible to predict demographic trends and inform conservation decision-making. However, such an effort is made more challenging by the degraded state of carcasses and resulting low quality of DNA available. Here, we describe a rapid and low-cost approach for identifying the origins of birds sampled across their annual cycle that is robust even when DNA quality is poor. We illustrate the approach in the common loon (Gavia immer), an iconic migratory aquatic bird that is under increasing threat on both its breeding and wintering areas. Using 300 samples collected from across the breeding range, we develop a panel of 158 single-nucleotide polymorphisms (SNP) loci with divergent allele frequencies across six genetic subpopulations. We use this SNP panel to identify the breeding grounds for 142 live nonbreeding individuals and carcasses. For example, genetic assignment of loons sampled during botulism outbreaks in parts of the Great Lakes provides evidence for the significant role the lakes play as migratory stopover areas for loons that breed across wide swaths of Canada, and highlights the vulnerability of a large segment of the breeding population to botulism outbreaks that are occurring in the Great Lakes with increasing frequency. Our results illustrate that the use of SNP panels to identify breeding origins of carcasses collected during the nonbreeding season can improve our understanding of the population-specific impacts of mortality from disease and anthropogenic stressors, ultimately allowing more effective management."
MICHAEL D SORENSON,Dopamine Receptor Genetic Polymorphisms and Body Composition in Undernourished Pastoralists: An Exploration of Nutrition Indices among Nomadic and Recently Settled Ariaal Men of Northern Kenya,"BACKGROUND. Minor alleles of the human dopamine receptor polymorphisms, DRD2/TaqI A and DRD4/48 bp, are related to decreased functioning and/or numbers of their respective receptors and have been shown to be correlated with body mass, height and food craving. In addition, the 7R minor allele of the DRD4 gene is at a higher frequency in nomadic compared to sedentary populations. Here we examine polymorphisms in the DRD2 and DRD4 genes with respect to body mass index (BMI) and height among men in two populations of Ariaal pastoralists, one recently settled (n = 87) and the other still nomadic (n = 65). The Ariaal live in northern Kenya, are chronically undernourished and are divided socially among age-sets. RESULTS. Frequencies of the DRD4/7R and DRD2/A1 alleles were 19.4% and 28.2%, respectively and did not differ between the nomadic and settled populations. BMI was higher in those with one or two DRD4/7R alleles in the nomadic population, but lower among the settled. Post-hoc analysis suggests that the DRD4 differences in BMI were due primarily to differences in fat free body mass. Height was unrelated to either DRD2/TaqI A or DRD4/48 bp genotypes. CONCLUSION. Our results indicate that the DRD4/7R allele may be more advantageous among nomadic than settled Ariaal men. This result suggests that a selective advantage mediated through behaviour may be responsible for the higher frequency of the 7R alleles in nomadic relative to sedentary populations around the world. In contrast to previous work, we did not find an association between DRD2 genotypes and height. Our results support the idea that human phenotypic expression of genotypes should be rigorously evaluated in diverse environments and genetic backgrounds."
MICHAEL D SORENSON,Amplification biases and consistent recovery of loci in a double-digest RAD-seq Protocol,"A growing variety of “genotype-by-sequencing” (GBS) methods use restriction enzymes and high throughput DNA sequencing to generate data for a subset of genomic loci, allowing the simultaneous discovery and genotyping of thousands of polymorphisms in a set of multiplexed samples. We evaluated a “double-digest” restriction-site associated DNA sequencing (ddRAD-seq) protocol by 1) comparing results for a zebra finch (Taeniopygia guttata) sample with in silico predictions from the zebra finch reference genome; 2) assessing data quality for a population sample of indigobirds (Vidua spp.); and 3) testing for consistent recovery of loci across multiple samples and sequencing runs. Comparison with in silico predictions revealed that 1) over 90% of predicted, single-copy loci in our targeted size range (178–328 bp) were recovered; 2) short restriction fragments (38–178 bp) were carried through the size selection step and sequenced at appreciable depth, generating unexpected but nonetheless useful data; 3) amplification bias favored shorter, GC-rich fragments, contributing to among locus variation in sequencing depth that was strongly correlated across samples; 4) our use of restriction enzymes with a GC-rich recognition sequence resulted in an up to four-fold overrepresentation of GC-rich portions of the genome; and 5) star activity (i.e., non-specific cutting) resulted in thousands of “extra” loci sequenced at low depth. Results for three species of indigobirds show that a common set of thousands of loci can be consistently recovered across both individual samples and sequencing runs. In a run with 46 samples, we genotyped 5,996 loci in all individuals and 9,833 loci in 42 or more individuals, resulting in <1% missing data for the larger data set. We compare our approach to similar methods and discuss the range of factors (fragment library preparation, natural genetic variation, bioinformatics) influencing the recovery of a consistent set of loci among samples."
MICHAEL D SORENSON,Does coevolution promote species richness in parasitic cuckoos?,"Why some lineages have diversified into larger numbers of species than others is a fundamental but still relatively poorly understood aspect of the evolutionary process. Coevolution has been recognized as a potentially important engine of speciation, but has rarely been tested in a comparative framework. We use a comparative approach based on a complete phylogeny of all living cuckoos to test whether parasite–host coevolution is associated with patterns of cuckoo species richness. There are no clear differences between parental and parasitic cuckoos in the number of species per genus. However, a cladogenesis test shows that brood parasitism is associated with both significantly higher speciation and extinction rates. Furthermore, subspecies diversification rate estimates were over twice as high in parasitic cuckoos as in parental cuckoos. Among parasitic cuckoos, there is marked variation in the severity of the detrimental effects on host fitness; chicks of some cuckoo species are raised alongside the young of the host and others are more virulent, with the cuckoo chick ejecting or killing the eggs/young of the host. We show that cuckoos with a more virulent parasitic strategy have more recognized subspecies. In addition, cuckoo species with more recognized subspecies have more hosts. These results hold after controlling for confounding geographical effects such as range size and isolation in archipelagos. Although the power of our analyses is limited by the fact that brood parasitism evolved independently only three times in cuckoos, our results suggest that coevolutionary arms races with hosts have contributed to higher speciation and extinction rates in parasitic cuckoos."
MICHAEL D SORENSON,Coevolution of male and female genital morphology in Waterfowl,"Most birds have simple genitalia; males lack external genitalia and females have simple vaginas. However, male waterfowl have a phallus whose length (1.5–>40 cm) and morphological elaborations vary among species and are positively correlated with the frequency of forced extra-pair copulations among waterfowl species. Here we report morphological complexity in female genital morphology in waterfowl and describe variation vaginal morphology that is unprecedented in birds. This variation comprises two anatomical novelties: (i) dead end sacs, and (ii) clockwise coils. These vaginal structures appear to function to exclude the intromission of the counter-clockwise spiralling male phallus without female cooperation. A phylogenetically controlled comparative analysis of 16 waterfowl species shows that the degree of vaginal elaboration is positively correlated with phallus length, demonstrating that female morphological complexity has co-evolved with male phallus length. Intersexual selection is most likely responsible for the observed coevolution, although identifying the specific mechanism is difficult. Our results suggest that females have evolved a cryptic anatomical mechanism of choice in response to forced extra-pair copulations."
MICHAEL D SORENSON,Dopamine receptor genetic polymorphisms and body composition in undernourished pastoralists: an exploration of nutrition indices among nomadic and recently settled Ariaal men of northern Kenya,"Minor alleles of the human dopamine receptor polymorphisms, DRD2/TaqI A and DRD4/48 bp, are related to decreased functioning and/or numbers of their respective receptors and have been shown to be correlated with body mass, height and food craving. In addition, the 7R minor allele of the DRD4 gene is at a higher frequency in nomadic compared to sedentary populations. Here we examine polymorphisms in the DRD2 and DRD4 genes with respect to body mass index (BMI) and height among men in two populations of Ariaal pastoralists, one recently settled (n = 87) and the other still nomadic (n = 65). The Ariaal live in northern Kenya, are chronically undernourished and are divided socially among age-sets."
MICHAEL D SORENSON,De novo assembly of the dual transcriptomes of a polymorphic raptor species and its malarial parasite,"BACKGROUND: Studies of non-model species are important for understanding the molecular processes underpinning phenotypic variation under natural ecological conditions. The common buzzard (Buteo buteo; Aves: Accipitriformes) is a widespread and common Eurasian raptor with three distinct plumage morphs that differ in several fitness-related traits, including parasite infestation. To provide a genomic resource for plumage polymorphic birds in general and to search for candidate genes relating to fitness, we generated a transcriptome from a single dead buzzard specimen plus easily accessible, minimally invasive samples from live chicks. RESULTS: We not only de novo assembled a near-complete buzzard transcriptome, but also obtained a significant fraction of the transcriptome of its malaria-like parasite, Leucocytozoon buteonis. By identifying melanogenesis-related transcripts that are differentially expressed in light ventral and dark dorsal feathers, but which are also expressed in other regions of the body, we also identified a suite of candidate genes that could be associated with fitness differences among the morphs. These include several immune-related genes, providing a plausible link between melanisation and parasite load. qPCR analysis of a subset of these genes revealed significant differences between ventral and dorsal feathers and an additional effect of morph. CONCLUSION: This new resource provides preliminary insights into genes that could be involved in fitness differences between the buzzard colour morphs, and should facilitate future studies of raptors and their malaria-like parasites."
MICHAEL D SORENSON,Does Coevolution Promote Species Richness in Parasitic Cuckoos?,"Why some lineages have diversified into larger numbers of species than others is a fundamental but still relatively poorly understood aspect of the evolutionary process. Coevolution has been recognized as a potentially important engine of speciation, but has rarely been tested in a comparative framework. We use a comparative approach based on a complete phylogeny of all living cuckoos to test whether parasite-host coevolution is associated with patterns of cuckoo species richness. There are no clear differences between parental and parasitic cuckoos in the number of species per genus. However, a cladogenesis test shows that brood parasitism is associated with both significantly higher speciation and extinction rates. Furthermore, subspecies diversification rate estimates were over twice as high in parasitic cuckoos as in parental cuckoos. Among parasitic cuckoos, there is marked variation in the severity of the detrimental effects on host fitness; chicks of some cuckoo species are raised alongside the young of the host and others are more virulent, with the cuckoo chick ejecting or killing the eggs/young of the host. We show that cuckoos with a more virulent parasitic strategy have more recognized subspecies. In addition, cuckoo species with more recognized subspecies have more hosts. These results hold after controlling for confounding geographical effects such as range size and isolation in archipelagos. Although the power of our analyses is limited by the fact that brood parasitism evolved independently only three times in cuckoos, our results suggest that coevolutionary arms races with hosts have contributed to higher speciation and extinction rates in parasitic cuckoos."
MICHAEL D SORENSON,Strong Mitochondrial DNA support for a Cretaceous Origin of Modern Avian Lineages,"BACKGROUND. Determining an absolute timescale for avian evolutionary history has proven contentious. The two sources of information available, paleontological data and inference from extant molecular genetic sequences (colloquially, 'rocks' and 'clocks'), have appeared irreconcilable; the fossil record supports a Cenozoic origin for most modern lineages, whereas molecular genetic estimates suggest that these same lineages originated deep within the Cretaceous and survived the K-Pg (Cretaceous-Paleogene; formerly Cretaceous-Tertiary or K-T) mass-extinction event. These two sources of data therefore appear to support fundamentally different models of avian evolution. The paradox has been speculated to reflect deficiencies in the fossil record, unrecognized biases in the treatment of genetic data or both. Here we attempt to explore uncertainty and limit bias entering into molecular divergence time estimates through: (i) improved taxon (n = 135) and character (n = 4594 bp mtDNA) sampling; (ii) inclusion of multiple cladistically tested internal fossil calibration points (n = 18); (iii) correction for lineage-specific rate heterogeneity using a variety of methods (n = 5); (iv) accommodation of uncertainty in tree topology; and (v) testing for possible effects of episodic evolution. RESULTS. The various 'relaxed clock' methods all indicate that the major (basal) lineages of modern birds originated deep within the Cretaceous, although temporal intraordinal diversification patterns differ across methods. We find that topological uncertainty had a systematic but minor influence on date estimates for the origins of major clades, and Bayesian analyses assuming fixed topologies deliver similar results to analyses with unconstrained topologies. We also find that, contrary to expectation, rates of substitution are not autocorrelated across the tree in an ancestor-descendent fashion. Finally, we find no signature of episodic molecular evolution related to either speciation events or the K-Pg boundary that could systematically mislead inferences from genetic data. CONCLUSION. The 'rock-clock' gap has been interpreted by some to be a result of the vagaries of molecular genetic divergence time estimates. However, despite measures to explore different forms of uncertainty in several key parameters, we fail to reconcile molecular genetic divergence time estimates with dates taken from the fossil record; instead, we find strong support for an ancient origin of modern bird lineages, with many extant orders and families arising in the mid-Cretaceous, consistent with previous molecular estimates. Although there is ample room for improvement on both sides of the 'rock-clock' divide (e.g. accounting for 'ghost' lineages in the fossil record and developing more realistic models of rate evolution for molecular genetic sequences), the consistent and conspicuous disagreement between these two sources of data more likely reflects a genuine difference between estimated ages of (i) stem-group origins and (ii) crown-group morphological diversifications, respectively. Further progress on this problem will benefit from greater communication between paleontologists and molecular phylogeneticists in accounting for error in avian lineage age estimates."
MICHAEL D SORENSON,Genetic architecture facilitates then constrains adaptation in a host-parasite coevolutionary arms race,"In coevolutionary arms races, interacting species impose selection on each other, generating reciprocal adaptations and counter adaptations. This process is typically enhanced by genetic recombination and heterozygosity, but these sources of evolutionary novelty may be secondarily lost when uniparental inheritance evolves to ensure the integrity of sex-linked adaptations. We demonstrate that host-specific egg mimicry in the African cuckoo finch Anomalospiza imberbis is maternally inherited, confirming the validity of an almost century-old hypothesis. We further show that maternal inheritance not only underpins the mimicry of different host species but also additional mimetic diversification that approximates the range of polymorphic egg “signatures” that have evolved within host species as an escalated defense against parasitism. Thus, maternal inheritance has enabled the evolution and maintenance of nested levels of mimetic specialization in a single parasitic species. However, maternal inheritance and the lack of sexual recombination likely disadvantage cuckoo finches by stifling further adaptation in the ongoing arms races with their individual hosts, which we show have retained biparental inheritance of egg phenotypes. The inability to generate novel genetic combinations likely prevents cuckoo finches from mimicking certain host phenotypes that are currently favored by selection (e.g., the olive-green colored eggs laid by some tawny-flanked prinia, Prinia subflava, females). This illustrates an important cost of coding coevolved adaptations on the nonrecombining sex chromosome, which may impede further coevolutionary change by effectively reversing the advantages of sexual reproduction in antagonistic coevolution proposed by the Red Queen hypothesis."
MICHAEL D SORENSON,As the goose flies: migration routes and timing influence patterns of genetic diversity in a circumpolar migratory herbivore,"Migration schedules and the timing of other annual events (e.g., pair formation and molt) can affect the distribution of genetic diversity as much as where these events occur. The greater white-fronted goose (Anser albifrons) is a circumpolar goose species, exhibiting temporal and spatial variation of events among populations during the annual cycle. Previous range-wide genetic assessments of the nuclear genome based on eight microsatellite loci suggest a single, largely panmictic population despite up to five subspecies currently recognized based on phenotypic differences. We used double digest restriction-site associated DNA (ddRAD-seq) and mitochondrial DNA (mtDNA) sequence data to re-evaluate estimates of spatial genomic structure and to characterize how past and present processes have shaped the patterns of genetic diversity and connectivity across the Arctic and subarctic. We uncovered previously undetected inter-population differentiation with genetic clusters corresponding to sampling locales associated with current management groups. We further observed subtle genetic clustering within each management unit that can be at least partially explained by the timing and directionality of migration events along with other behaviors during the annual cycle. The Tule Goose (A. a. elgasi) and Greenland subspecies (A. a. flavirostris) showed the highest level of divergence among all sampling locales investigated. The recovery of previously undetected broad and fine-scale spatial structure suggests that the strong cultural transmission of migratory behavior restricts gene flow across portions of the species’ range. Our data further highlight the importance of re-evaluating previous assessments conducted based on a small number of highly variable genetic markers in phenotypically diverse species."
SIMON KASIF,Towards the Identification of Essential Genes Using Targeted Genome Sequencing and Comparative Analysis,"BACKGROUND: The identification of genes essential for survival is of theoretical importance in the understanding of the minimal requirements for cellular life, and of practical importance in the identification of potential drug targets in novel pathogens. With the great time and expense required for experimental studies aimed at constructing a catalog of essential genes in a given organism, a computational approach which could identify essential genes with high accuracy would be of great value. RESULTS: We gathered numerous features which could be generated automatically from genome sequence data and assessed their relationship to essentiality, and subsequently utilized machine learning to construct an integrated classifier of essential genes in both S. cerevisiae and E. coli. When looking at single features, phyletic retention, a measure of the number of organisms an ortholog is present in, was the most predictive of essentiality. Furthermore, during construction of our phyletic retention feature we for the first time explored the evolutionary relationship among the set of organisms in which the presence of a gene is most predictive of essentiality. We found that in both E. coli and S. cerevisiae the optimal sets always contain host-associated organisms with small genomes which are closely related to the reference. Using five optimally selected organisms, we were able to improve predictive accuracy as compared to using all available sequenced organisms. We hypothesize the predictive power of these genomes is a consequence of the process of reductive evolution, by which many parasites and symbionts evolved their gene content. In addition, essentiality is measured in rich media, a condition which resembles the environments of these organisms in their hosts where many nutrients are provided. Finally, we demonstrate that integration of our most highly predictive features using a probabilistic classifier resulted in accuracies surpassing any individual feature. CONCLUSION: Using features obtainable directly from sequence data, we were able to construct a classifier which can predict essential genes with high accuracy. Furthermore, our analysis of the set of genomes in which the presence of a gene is most predictive of essentiality may suggest ways in which targeted sequencing can be used in the identification of essential genes. In summary, the methods presented here can aid in the reduction of time and money invested in essential gene identification by targeting those genes for experimentation which are predicted as being essential with a high probability."
SIMON KASIF,Genes Involved in Complex Adaptive Processes Tend to Have Highly Conserved Upstream Regions in Mammalian Genomes,"BACKGROUND: Recent advances in genome sequencing suggest a remarkable conservation in gene content of mammalian organisms. The similarity in gene repertoire present in different organisms has increased interest in studying regulatory mechanisms of gene expression aimed at elucidating the differences in phenotypes. In particular, a proximal promoter region contains a large number of regulatory elements that control the expression of its downstream gene. Although many studies have focused on identification of these elements, a broader picture on the complexity of transcriptional regulation of different biological processes has not been addressed in mammals. The regulatory complexity may strongly correlate with gene function, as different evolutionary forces must act on the regulatory systems under different biological conditions. We investigate this hypothesis by comparing the conservation of promoters upstream of genes classified in different functional categories. RESULTS: By conducting a rank correlation analysis between functional annotation and upstream sequence alignment scores obtained by human-mouse and human-dog comparison, we found a significantly greater conservation of the upstream sequence of genes involved in development, cell communication, neural functions and signaling processes than those involved in more basic processes shared with unicellular organisms such as metabolism and ribosomal function. This observation persists after controlling for G+C content. Considering conservation as a functional signature, we hypothesize a higher density of cis-regulatory elements upstream of genes participating in complex and adaptive processes. CONCLUSION: We identified a class of functions that are associated with either high or low promoter conservation in mammals. We detected a significant tendency that points to complex and adaptive processes were associated with higher promoter conservation, despite the fact that they have emerged relatively recently during evolution. We described and contrasted several hypotheses that provide a deeper insight into how transcriptional complexity might have been emerged during evolution."
SIMON KASIF,Genomic Functional Annotation Using Co-Evolution Profiles of Gene Clusters,"The current speed of sequencing already exceeds the capability of annotation. A new method for functional annotation is proposed using the conservation patterns of gene clusters and has been applied to the genome of Escherichia coli and established functional relationships among 176 gene clusters, comprising 738 genes. BACKGROUND. The current speed of sequencing already exceeds the capability of annotation, creating a potential bottleneck. A large proportion of the genes in microbial genomes remains uncharacterized. Here we propose a new method for functional annotation using the conservation patterns of gene clusters. If several gene clusters show the same coevolution pattern across different genomes it is reasonable to infer they are functionally related. The gene cluster phylogenetic profile integrates chromosomal proximity information and phylogenetic profile information and allows us to infer functional dependences between the gene clusters even at great distance on the chromosome. RESULTS. As a proof of concept, we applied our method to the genome of Escherichia coli K12 strain. Our method establishes functional relationships among 176 gene clusters, comprising 738 E. coli genes. The accuracy of pair phylogenetic profiles was compared with the single-gene phylogenetic profile and was shown to be higher. As a result, we are able to suggest functional roles for several previously unknown genes or unknown genomic regions in E. coli. We also examined the robustness of coevolution signals across a larger set of genomes and suggest a possible upper limit of accuracy for the phylogenetic profile methods. CONCLUSIONS. The higher-order phylogenetic profiles, such as the gene-pair phylogenetic profiles, can detect functional dependences that are missed by using conventional single-gene phylogenetic profile or the chromosomal proximity method only. We show that the gene-pair phylogenetic profile is more accurate than the single-gene phylogenetic profiles."
SIMON KASIF,Analysis of Gene Expression in a Developmental Context Emphasizes Distinct Biological Leitmotifs in Human Cancers,"A systematic analysis of the relationship between the neoplastic and developmental transcriptome provides an outline of global trends in cancer gene expression. BACKGROUND. In recent years, the molecular underpinnings of the long-observed resemblance between neoplastic and immature tissue have begun to emerge. Genome-wide transcriptional profiling has revealed similar gene expression signatures in several tumor types and early developmental stages of their tissue of origin. However, it remains unclear whether such a relationship is a universal feature of malignancy, whether heterogeneities exist in the developmental component of different tumor types and to which degree the resemblance between cancer and development is a tissue-specific phenomenon. RESULTS. We defined a developmental landscape by summarizing the main features of ten developmental time courses and projected gene expression from a variety of human tumor types onto this landscape. This comparison demonstrates a clear imprint of developmental gene expression in a wide range of tumors and with respect to different, even non-cognate developmental backgrounds. Our analysis reveals three classes of cancers with developmentally distinct transcriptional patterns. We characterize the biological processes dominating these classes and validate the class distinction with respect to a new time series of murine embryonic lung development. Finally, we identify a set of genes that are upregulated in most cancers and we show that this signature is active in early development. CONCLUSION. This systematic and quantitative overview of the relationship between the neoplastic and developmental transcriptome spanning dozens of tissues provides a reliable outline of global trends in cancer gene expression, reveals potentially clinically relevant differences in the gene expression of different cancer types and represents a reference framework for interpretation of smaller-scale functional studies."
SIMON KASIF,Accelerated Postnatal Growth Increases Lipogenic Gene Expression and Adipocyte Size in Low–Birth Weight Mice,"OBJECTIVE: To characterize the hormonal milieu and adipose gene expression in response to catch-up growth (CUG), a growth pattern associated with obesity and diabetes risk, in a mouse model of low birth weight (LBW). RESEARCH DESIGN AND METHODS: ICR mice were food restricted by 50% from gestational days 12.5–18.5, reducing offspring birth weight by 25%. During the suckling period, dams were either fed ad libitum, permitting CUG in offspring, or food restricted, preventing CUG. Offspring were killed at age 3 weeks, and gonadal fat was removed for RNA extraction, array analysis, RT-PCR, and evaluation of cell size and number. Serum insulin, thyroxine (T4), corticosterone, and adipokines were measured. RESULTS: At age 3 weeks, LBW mice with CUG (designated U-C) had body weight comparable with controls (designated C-C); weight was reduced by 49% in LBW mice without CUG (designated U-U). Adiposity was altered by postnatal nutrition, with gonadal fat increased by 50% in U-C and decreased by 58% in U-U mice (P < 0.05 vs. C-C mice). Adipose expression of the lipogenic genes Fasn, AccI, Lpin1, and Srebf1 was significantly increased in U-C compared with both C-C and U-U mice (P < 0.05). Mitochondrial DNA copy number was reduced by >50% in U-C versus U-U mice (P = 0.014). Although cell numbers did not differ, mean adipocyte diameter was increased in U-C and reduced in U-U mice (P < 0.01). CONCLUSIONS: CUG results in increased adipose tissue lipogenic gene expression and adipocyte diameter but not increased cellularity, suggesting that catch-up fat is primarily associated with lipogenesis rather than adipogenesis in this murine model."
SIMON KASIF,Quantitative Analysis of Single Nucleotide Polymorphisms within Copy Number Variation,"Background Single nucleotide polymorphisms (SNPs) have been used extensively in genetics and epidemiology studies. Traditionally, SNPs that did not pass the Hardy-Weinberg equilibrium (HWE) test were excluded from these analyses. Many investigators have addressed possible causes for departure from HWE, including genotyping errors, population admixture and segmental duplication. Recent large-scale surveys have revealed abundant structural variations in the human genome, including copy number variations (CNVs). This suggests that a significant number of SNPs must be within these regions, which may cause deviation from HWE. Results We performed a Bayesian analysis on the potential effect of copy number variation, segmental duplication and genotyping errors on the behavior of SNPs. Our results suggest that copy number variation is a major factor of HWE violation for SNPs with a small minor allele frequency, when the sample size is large and the genotyping error rate is 0~1%. Conclusions Our study provides the posterior probability that a SNP falls in a CNV or a segmental duplication, given the observed allele frequency of the SNP, sample size and the significance level of HWE testing."
SIMON KASIF,Genes involved in complex adaptive processes tend to have highly conserved upstream regions in mammalian genomes,"BACKGROUND:Recent advances in genome sequencing suggest a remarkable conservation in gene content of mammalian organisms. The similarity in gene repertoire present in different organisms has increased interest in studying regulatory mechanisms of gene expression aimed at elucidating the differences in phenotypes. In particular, a proximal promoter region contains a large number of regulatory elements that control the expression of its downstream gene. Although many studies have focused on identification of these elements, a broader picture on the complexity of transcriptional regulation of different biological processes has not been addressed in mammals. The regulatory complexity may strongly correlate with gene function, as different evolutionary forces must act on the regulatory systems under different biological conditions. We investigate this hypothesis by comparing the conservation of promoters upstream of genes classified in different functional categories.RESULTS:By conducting a rank correlation analysis between functional annotation and upstream sequence alignment scores obtained by human-mouse and human-dog comparison, we found a significantly greater conservation of the upstream sequence of genes involved in development, cell communication, neural functions and signaling processes than those involved in more basic processes shared with unicellular organisms such as metabolism and ribosomal function. This observation persists after controlling for G+C content. Considering conservation as a functional signature, we hypothesize a higher density of cis-regulatory elements upstream of genes participating in complex and adaptive processes.CONCLUSION:We identified a class of functions that are associated with either high or low promoter conservation in mammals. We detected a significant tendency that points to complex and adaptive processes were associated with higher promoter conservation, despite the fact that they have emerged relatively recently during evolution. We described and contrasted several hypotheses that provide a deeper insight into how transcriptional complexity might have been emerged during evolution."
SIMON KASIF,Integration of relational and hierarchical network information for protein function prediction,"BACKGROUND:In the current climate of high-throughput computational biology, the inference of a protein's function from related measurements, such as protein-protein interaction relations, has become a canonical task. Most existing technologies pursue this task as a classification problem, on a term-by-term basis, for each term in a database, such as the Gene Ontology (GO) database, a popular rigorous vocabulary for biological functions. However, ontology structures are essentially hierarchies, with certain top to bottom annotation rules which protein function predictions should in principle follow. Currently, the most common approach to imposing these hierarchical constraints on network-based classifiers is through the use of transitive closure to predictions.RESULTS:We propose a probabilistic framework to integrate information in relational data, in the form of a protein-protein interaction network, and a hierarchically structured database of terms, in the form of the GO database, for the purpose of protein function prediction. At the heart of our framework is a factorization of local neighborhood information in the protein-protein interaction network across successive ancestral terms in the GO hierarchy. We introduce a classifier within this framework, with computationally efficient implementation, that produces GO-term predictions that naturally obey a hierarchical 'true-path' consistency from root to leaves, without the need for further post-processing.CONCLUSION:A cross-validation study, using data from the yeast Saccharomyces cerevisiae, shows our method offers substantial improvements over both standard 'guilt-by-association' (i.e., Nearest-Neighbor) and more refined Markov random field methods, whether in their original form or when post-processed to artificially impose 'true-path' consistency. Further analysis of the results indicates that these improvements are associated with increased predictive capabilities (i.e., increased positive predictive value), and that this increase is consistent uniformly with GO-term depth. Additional in silico validation on a collection of new annotations recently added to GO confirms the advantages suggested by the cross-validation study. Taken as a whole, our results show that a hierarchical approach to network-based protein function prediction, that exploits the ontological structure of protein annotation databases in a principled manner, can offer substantial advantages over the successive application of 'flat' network-based methods."
SIMON KASIF,Analysis of brain region-specific co-expression networks reveals clustering of established and novel genes associated with Alzheimer disease,"BACKGROUND: Identifying and understanding the functional role of genetic risk factors for Alzheimer disease (AD) has been complicated by the variability of genetic influences across brain regions and confounding with age-related neurodegeneration. METHODS: A gene co-expression network was constructed using data obtained from the Allen Brain Atlas for multiple brain regions (cerebral cortex, cerebellum, and brain stem) in six individuals. Gene network analyses were seeded with 52 reproducible (i.e., established) AD (RAD) genes. Genome-wide association study summary data were integrated with the gene co-expression results and phenotypic information (i.e., memory and aging-related outcomes) from gene knockout studies in Drosophila to generate rankings for other genes that may have a role in AD. RESULTS: We found that co-expression of the RAD genes is strongest in the cortical regions where neurodegeneration due to AD is most severe. There was significant evidence for two novel AD-related genes including EPS8 (FDR p = 8.77 × 10−3) and HSPA2 (FDR p = 0.245). CONCLUSIONS: Our findings indicate that AD-related risk factors are potentially associated with brain region-specific effects on gene expression that can be detected using a gene network approach."
SIMON KASIF,Seeing the Forest for the Trees: Using the Gene Ontology to Restructure Hierarchical Clustering,"Motivation: There is a growing interest in improving the cluster analysis of expression data by incorporating into it prior knowledge, such as the Gene Ontology (GO) annotations of genes, in order to improve the biological relevance of the clusters that are subjected to subsequent scrutiny. The structure of the GO is another source of background knowledge that can be exploited through the use of semantic similarity. Results: We propose here a novel algorithm that integrates semantic similarities (derived from the ontology structure) into the procedure of deriving clusters from the dendrogram constructed during expression-based hierarchical clustering. Our approach can handle the multiple annotations, from different levels of the GO hierarchy, which most genes have. Moreover, it treats annotated and unannotated genes in a uniform manner. Consequently, the clusters obtained by our algorithm are characterized by significantly enriched annotations. In both cross-validation tests and when using an external index such as protein–protein interactions, our algorithm performs better than previous approaches. When applied to human cancer expression data, our algorithm identifies, among others, clusters of genes related to immune response and glucose metabolism. These clusters are also supported by protein–protein interaction data. Contact: dotna@cs.bgu.ac.il Supplementary information: Supplementary data are available at Bioinformatics online."
SIMON KASIF,Probabilistic Protein Function Prediction from Heterogeneous Genome-Wide Data,"Dramatic improvements in high throughput sequencing technologies have led to a staggering growth in the number of predicted genes. However, a large fraction of these newly discovered genes do not have a functional assignment. Fortunately, a variety of novel high-throughput genome-wide functional screening technologies provide important clues that shed light on gene function. The integration of heterogeneous data to predict protein function has been shown to improve the accuracy of automated gene annotation systems. In this paper, we propose and evaluate a probabilistic approach for protein function prediction that integrates protein-protein interaction (PPI) data, gene expression data, protein motif information, mutant phenotype data, and protein localization data. First, functional linkage graphs are constructed from PPI data and gene expression data, in which an edge between nodes (proteins) represents evidence for functional similarity. The assumption here is that graph neighbors are more likely to share protein function, compared to proteins that are not neighbors. The functional linkage graph model is then used in concert with protein domain, mutant phenotype and protein localization data to produce a functional prediction. Our method is applied to the functional prediction of Saccharomyces cerevisiae genes, using Gene Ontology (GO) terms as the basis of our annotation. In a cross validation study we show that the integrated model increases recall by 18%, compared to using PPI data alone at the 50% precision. We also show that the integrated predictor is significantly better than each individual predictor. However, the observed improvement vs. PPI depends on both the new source of data and the functional category to be predicted. Surprisingly, in some contexts integration hurts overall prediction accuracy. Lastly, we provide a comprehensive assignment of putative GO terms to 463 proteins that currently have no assigned function."
SIMON KASIF,COMBREX: A Project to Accelerate the Functional Annotation of Prokaryotic Genomes,COMBREX (http://combrex.bu.edu) is a project to increase the speed of the functional annotation of new bacterial and archaeal genomes. It consists of a database of functional predictions produced by computational biologists and a mechanism for experimental biochemists to bid for the validation of those predictions. Small grants are available to support successful bids.
SIMON KASIF,Single-cell transcriptional networks in differentiating preadipocytes suggest drivers associated with tissue heterogeneity,"White adipose tissue plays an important role in physiological homeostasis and metabolic disease. Different fat depots have distinct metabolic and inflammatory profiles and are differentially associated with disease risk. It is unclear whether these differences are intrinsic to the pre-differentiated stage. Using single-cell RNA sequencing, a unique network methodology and a data integration technique, we predict metabolic phenotypes in differentiating cells. Single-cell RNA-seq profiles of human preadipocytes during adipogenesis in vitro identifies at least two distinct classes of subcutaneous white adipocytes. These differences in gene expression are separate from the process of browning and beiging. Using a systems biology approach, we identify a new network of zinc-finger proteins that are expressed in one class of preadipocytes and is potentially involved in regulating adipogenesis. Our findings gain a deeper understanding of both the heterogeneity of white adipocytes and their link to normal metabolism and disease."
SIMON KASIF,One for all and all for one: improving replication of genetic studies through network diffusion,"Improving accuracy in genetic studies would greatly accelerate understanding the genetic basis of complex diseases. One approach to achieve such an improvement for risk variants identified by the genome wide association study (GWAS) approach is to incorporate previously known biology when screening variants across the genome. We developed a simple approach for improving the prioritization of candidate disease genes that incorporates a network diffusion of scores from known disease genes using a protein network and a novel integration with GWAS risk scores, and tested this approach on a large Alzheimer disease (AD) GWAS dataset. Using a statistical bootstrap approach, we cross-validated the method and for the first time showed that a network approach improves the expected replication rates in GWAS studies. Several novel AD genes were predicted including CR2, SHARPIN, and PTPN2. Our re-prioritized results are enriched for established known AD-associated biological pathways including inflammation, immune response, and metabolism, whereas standard non-prioritized results were not. Our findings support a strategy of considering network information when investigating genetic risk factors."
SIMON KASIF,GEMS: A Web Server for Biclustering Analysis of Expression Data,"The advent of microarray technology has revolutionized the search for genes that are differentially expressed across a range of cell types or experimental conditions. Traditional clustering methods, such as hierarchical clustering, are often difficult to deploy effectively since genes rarely exhibit similar expression pattern across a wide range of conditions. Biclustering of gene expression data (also called co-clustering or two-way clustering) is a non-trivial but promising methodology for the identification of gene groups that show a coherent expression profile across a subset of conditions. Thus, biclustering is a natural methodology as a screen for genes that are functionally related, participate in the same pathways, affected by the same drug or pathological condition, or genes that form modules that are potentially co-regulated by a small group of transcription factors. We have developed a web-enabled service called GEMS (Gene Expression Mining Server) for biclustering microarray data. Users may upload expression data and specify a set of criteria. GEMS then performs bicluster mining based on a Gibbs sampling paradigm. The web server provides a flexible and an useful platform for the discovery of co-expressed and potentially co-regulated gene modules. GEMS is an open source software and is available at http://genomics10.bu.edu/terrence/gems."
SIMON KASIF,Functional Characterization of the YmcB and YqeV tRNA Methylthiotransferases of Bacillus Subtilis,"Methylthiotransferases (MTTases) are a closely related family of proteins that perform both radical-S-adenosylmethionine (SAM) mediated sulfur insertion and SAM-dependent methylation to modify nucleic acid or protein targets with a methyl thioether group (–SCH3). Members of two of the four known subgroups of MTTases have been characterized, typified by MiaB, which modifies N6-isopentenyladenosine (i6A) to 2-methylthio-N6-isopentenyladenosine (ms2i6A) in tRNA, and RimO, which modifies a specific aspartate residue in ribosomal protein S12. In this work, we have characterized the two MTTases encoded by Bacillus subtilis 168 and find that, consistent with bioinformatic predictions, ymcB is required for ms2i6A formation (MiaB activity), and yqeV is required for modification of N6-threonylcarbamoyladenosine (t6A) to 2-methylthio-N6-threonylcarbamoyladenosine (ms2t6A) in tRNA. The enzyme responsible for the latter activity belongs to a third MTTase subgroup, no member of which has previously been characterized. We performed domain-swapping experiments between YmcB and YqeV to narrow down the protein domain(s) responsible for distinguishing i6A from t6A and found that the C-terminal TRAM domain, putatively involved with RNA binding, is likely not involved with this discrimination. Finally, we performed a computational analysis to identify candidate residues outside the TRAM domain that may be involved with substrate recognition. These residues represent interesting targets for further analysis."
SIMON KASIF,Phylogenetic Detection of Conserved Gene Clusters in Microbial Genomes,"BACKGROUND. Microbial genomes contain an abundance of genes with conserved proximity forming clusters on the chromosome. However, the conservation can be a result of many factors such as vertical inheritance, or functional selection. Thus, identification of conserved gene clusters that are under functional selection provides an effective channel for gene annotation, microarray screening, and pathway reconstruction. The problem of devising a robust method to identify these conserved gene clusters and to evaluate the significance of the conservation in multiple genomes has a number of implications for comparative, evolutionary and functional genomics as well as synthetic biology. RESULTS. In this paper we describe a new method for detecting conserved gene clusters that incorporates the information captured by a genome phylogenetic tree. We show that our method can overcome the common problem of overestimation of significance due to the bias in the genome database and thereby achieve better accuracy when detecting functionally connected gene clusters. Our results can be accessed at database GeneChords . CONCLUSION. The methodology described in this paper gives a scalable framework for discovering conserved gene clusters in microbial genomes. It serves as a platform for many other functional genomic analyses in microorganisms, such as operon prediction, regulatory site prediction, functional annotation of genes, evolutionary origin and development of gene clusters."
SIMON KASIF,Segmentally Variable Genes: A New Perspective on Adaptation,"Genomic sequence variation is the hallmark of life and is key to understanding diversity and adaptation among the numerous microorganisms on earth. Analysis of the sequenced microbial genomes suggests that genes are evolving at many different rates. We have attempted to derive a new classification of genes into three broad categories: lineage-specific genes that evolve rapidly and appear unique to individual species or strains; highly conserved genes that frequently perform housekeeping functions; and partially variable genes that contain highly variable regions, at least 70 amino acids long, interspersed among well-conserved regions. The latter we term segmentally variable genes (SVGs), and we suggest that they are especially interesting targets for biochemical studies. Among these genes are ones necessary to deal with the environment, including genes involved in host–pathogen interactions, defense mechanisms, and intracellular responses to internal and environmental changes. For the most part, the detailed function of these variable regions remains unknown. We propose that they are likely to perform important binding functions responsible for protein–protein, protein–nucleic acid, or protein–small molecule interactions. Discerning their function and identifying their binding partners may offer biologists new insights into the basic mechanisms of adaptation, context-dependent evolution, and the interaction between microbes and their environment. Segmentally variable genes show a mosaic pattern of one or more rapidly evolving, variable regions. Discerning their function may provide new insights into the forces that shape genome diversity and adaptation"
SIMON KASIF,Network-Based Analysis of Affected Biological Processes in Type 2 Diabetes Models,"Type 2 diabetes mellitus is a complex disorder associated with multiple genetic, epigenetic, developmental, and environmental factors. Animal models of type 2 diabetes differ based on diet, drug treatment, and gene knockouts, and yet all display the clinical hallmarks of hyperglycemia and insulin resistance in peripheral tissue. The recent advances in gene-expression microarray technologies present an unprecedented opportunity to study type 2 diabetes mellitus at a genome-wide scale and across different models. To date, a key challenge has been to identify the biological processes or signaling pathways that play significant roles in the disorder. Here, using a network-based analysis methodology, we identified two sets of genes, associated with insulin signaling and a network of nuclear receptors, which are recurrent in a statistically significant number of diabetes and insulin resistance models and transcriptionally altered across diverse tissue types. We additionally identified a network of protein–protein interactions between members from the two gene sets that may facilitate signaling between them. Taken together, the results illustrate the benefits of integrating high-throughput microarray studies, together with protein–protein interaction networks, in elucidating the underlying biological processes associated with a complex disorder. Author Summary Type 2 diabetes mellitus currently affects millions of people. It is clinically characterized by insulin resistance in addition to an impaired glucose response and associated with numerous complications including heart disease, stroke, neuropathy, and kidney failure, among others. Accurate identification of the underlying molecular mechanisms of the disease or its complications is an important research problem that could lead to novel diagnostics and therapy. The main challenge stems from the fact that insulin resistance is a complex disorder and affects a multitude of biological processes, metabolic networks, and signaling pathways. In this report, the authors develop a network-based methodology that appears to be more sensitive than previous approaches in detecting deregulated molecular processes in a disease state. The methodology revealed that both insulin signaling and nuclear receptor networks are consistently and differentially expressed in many models of insulin resistance. The positive results suggest such network-based diagnostic technologies hold promise as potentially useful clinical and research tools in the future."
SIMON KASIF,Large-Scale Mapping and Validation of Escherichia coli Transcriptional Regulation from a Compendium of Expression Profiles,"Machine learning approaches offer the potential to systematically identify transcriptional regulatory interactions from a compendium of microarray expression profiles. However, experimental validation of the performance of these methods at the genome scale has remained elusive. Here we assess the global performance of four existing classes of inference algorithms using 445 Escherichia coli Affymetrix arrays and 3,216 known E. coli regulatory interactions from RegulonDB. We also developed and applied the context likelihood of relatedness (CLR) algorithm, a novel extension of the relevance networks class of algorithms. CLR demonstrates an average precision gain of 36% relative to the next-best performing algorithm. At a 60% true positive rate, CLR identifies 1,079 regulatory interactions, of which 338 were in the previously known network and 741 were novel predictions. We tested the predicted interactions for three transcription factors with chromatin immunoprecipitation, confirming 21 novel interactions and verifying our RegulonDB-based performance estimates. CLR also identified a regulatory link providing central metabolic control of iron transport, which we confirmed with real-time quantitative PCR. The compendium of expression data compiled in this study, coupled with RegulonDB, provides a valuable model system for further improvement of network inference algorithms using experimental data. Author SummaryOrganisms can adapt to changing environments—becoming more virulent, for example, or activating stress responses—thanks to a flexible gene expression program controlled by the dynamic interactions of hundreds of transcriptional regulators. To unravel this regulatory complexity, multiple computational algorithms have been developed to analyze gene expression profiles and detect dependencies among genes over different conditions. It has been difficult to judge whether these algorithms can generate accurate global maps of regulatory interactions, however, because of the absence of a model organism with both a compendium of gene expression data and a corresponding network of experimentally determined regulatory interactions. To address this issue, we assembled 445 Escherichia coli microarrays, applied four classes of inference algorithms to the dataset, and validated the predictions against 3,216 experimentally determined E. coli interactions. The top-performing algorithm identifies 1,079 regulatory interactions at a confidence level of 60% or higher. Of these predicted interactions, 741 are novel and illuminate the regulation of amino acid biosynthesis, flagella biosynthesis, osmotic stress response, antibiotic resistance, and iron regulation. By defining the capabilities and limitations of network inference algorithms for large-scale mapping of prokaryotic regulatory networks, our work should facilitate their application to the mapping of novel microbes. A novel, machine-learning method is developed to predict transcriptional regulatory interactions, making use of microarray data. One interaction identified appears to be important for the control of iron transport."
SIMON KASIF,Integration of Relational and Hierarchical Network Information for Protein Function Prediction,"BACKGROUND. In the current climate of high-throughput computational biology, the inference of a protein's function from related measurements, such as protein-protein interaction relations, has become a canonical task. Most existing technologies pursue this task as a classification problem, on a term-by-term basis, for each term in a database, such as the Gene Ontology (GO) database, a popular rigorous vocabulary for biological functions. However, ontology structures are essentially hierarchies, with certain top to bottom annotation rules which protein function predictions should in principle follow. Currently, the most common approach to imposing these hierarchical constraints on network-based classifiers is through the use of transitive closure to predictions. RESULTS. We propose a probabilistic framework to integrate information in relational data, in the form of a protein-protein interaction network, and a hierarchically structured database of terms, in the form of the GO database, for the purpose of protein function prediction. At the heart of our framework is a factorization of local neighborhood information in the protein-protein interaction network across successive ancestral terms in the GO hierarchy. We introduce a classifier within this framework, with computationally efficient implementation, that produces GO-term predictions that naturally obey a hierarchical 'true-path' consistency from root to leaves, without the need for further post-processing. CONCLUSION. A cross-validation study, using data from the yeast Saccharomyces cerevisiae, shows our method offers substantial improvements over both standard 'guilt-by-association' (i.e., Nearest-Neighbor) and more refined Markov random field methods, whether in their original form or when post-processed to artificially impose 'true-path' consistency. Further analysis of the results indicates that these improvements are associated with increased predictive capabilities (i.e., increased positive predictive value), and that this increase is consistent uniformly with GO-term depth. Additional in silico validation on a collection of new annotations recently added to GO confirms the advantages suggested by the cross-validation study. Taken as a whole, our results show that a hierarchical approach to network-based protein function prediction, that exploits the ontological structure of protein annotation databases in a principled manner, can offer substantial advantages over the successive application of 'flat' network-based methods."
SIMON KASIF,Integration of Heterogeneous Expression Data Sets Extends the Role of the Retinol Pathway in Diabetes and Insulin Resistance,"Motivation: Type 2 diabetes is a chronic metabolic disease that involves both environmental and genetic factors. To understand the genetics of type 2 diabetes and insulin resistance, the DIabetes Genome Anatomy Project (DGAP) was launched to profile gene expression in a variety of related animal models and human subjects. We asked whether these heterogeneous models can be integrated to provide consistent and robust biological insights into the biology of insulin resistance. Results: We perform integrative analysis of the 16 DGAP data sets that span multiple tissues, conditions, array types, laboratories, species, genetic backgrounds and study designs. For each data set, we identify differentially expressed genes compared with control. Then, for the combined data, we rank genes according to the frequency with which they were found to be statistically significant across data sets. This analysis reveals RetSat as a widely shared component of mechanisms involved in insulin resistance and sensitivity and adds to the growing importance of the retinol pathway in diabetes, adipogenesis and insulin resistance. Top candidates obtained from our analysis have been confirmed in recent laboratory studies. Contact: Isaac_kohane@harvard.edu"
SIMON KASIF,Genomewide Analysis of PRC1 and PRC2 Occupancy Identifies Two Classes of Bivalent Domains,"In embryonic stem (ES) cells, bivalent chromatin domains with overlapping repressive (H3 lysine 27 tri-methylation) and activating (H3 lysine 4 tri-methylation) histone modifications mark the promoters of more than 2,000 genes. To gain insight into the structure and function of bivalent domains, we mapped key histone modifications and subunits of Polycomb-repressive complexes 1 and 2 (PRC1 and PRC2) genomewide in human and mouse ES cells by chromatin immunoprecipitation, followed by ultra high-throughput sequencing. We find that bivalent domains can be segregated into two classes—the first occupied by both PRC2 and PRC1 (PRC1-positive) and the second specifically bound by PRC2 (PRC2-only). PRC1-positive bivalent domains appear functionally distinct as they more efficiently retain lysine 27 tri-methylation upon differentiation, show stringent conservation of chromatin state, and associate with an overwhelming number of developmental regulator gene promoters. We also used computational genomics to search for sequence determinants of Polycomb binding. This analysis revealed that the genomewide locations of PRC2 and PRC1 can be largely predicted from the locations, sizes, and underlying motif contents of CpG islands. We propose that large CpG islands depleted of activating motifs confer epigenetic memory by recruiting the full repertoire of Polycomb complexes in pluripotent cells. Author Summary Polycomb-group (PcG) proteins play essential roles in the epigenetic regulation of gene expression during development. PcG proteins are repressors that catalyze lysine 27 tri-methylation on histone H3. They are antagonized by trithorax-group proteins that catalyze lysine 4 tri-methylation. Recent studies of ES cells revealed a novel chromatin pattern consisting of overlapping lysine 27 and lysine 4 tri-methylation. Genomic regions with these opposing modifications were termed ""bivalent domains"" and proposed to silence developmental regulators while keeping them ""poised"" for alternate fates. However, our understanding of PcG regulation and bivalent domains remains limited. For instance, bivalent domains affect over 2,000 promoters with diverse functions, which suggests that they may function in diverse cellular processes. Moreover, the mechanisms that underlie the targeting of PcG complexes to specific genomic regions remain completely unknown. To gain insight into these issues, we used ultra high-throughput sequencing to map PcG complexes and related modifications genomewide in human and mouse ES cells. The data identify two classes of bivalent domains with distinct regulatory properties. They also reveal striking relationships between genome sequence and chromatin state that suggest a prominent role for the DNA sequence in dictating the genomewide localization of PcG complexes and, consequently, bivalent domains in ES cells."
SIMON KASIF,A Predictive Phosphorylation Signature of Lung Cancer,"BACKGROUND. Aberrant activation of signaling pathways drives many of the fundamental biological processes that accompany tumor initiation and progression. Inappropriate phosphorylation of intermediates in these signaling pathways are a frequently observed molecular lesion that accompanies the undesirable activation or repression of pro- and anti-oncogenic pathways. Therefore, methods which directly query signaling pathway activation via phosphorylation assays in individual cancer biopsies are expected to provide important insights into the molecular ""logic"" that distinguishes cancer and normal tissue on one hand, and enables personalized intervention strategies on the other. RESULTS. We first document the largest available set of tyrosine phosphorylation sites that are, individually, differentially phosphorylated in lung cancer, thus providing an immediate set of drug targets. Next, we develop a novel computational methodology to identify pathways whose phosphorylation activity is strongly correlated with the lung cancer phenotype. Finally, we demonstrate the feasibility of classifying lung cancers based on multi-variate phosphorylation signatures. CONCLUSIONS. Highly predictive and biologically transparent phosphorylation signatures of lung cancer provide evidence for the existence of a robust set of phosphorylation mechanisms (captured by the signatures) present in the majority of lung cancers, and that reliably distinguish each lung cancer from normal. This approach should improve our understanding of cancer and help guide its treatment, since the phosphorylation signatures highlight proteins and pathways whose phosphorylation should be inhibited in order to prevent unregulated proliferation."
SIMON KASIF,Biological Process Linkage Networks,"BACKGROUND. The traditional approach to studying complex biological networks is based on the identification of interactions between internal components of signaling or metabolic pathways. By comparison, little is known about interactions between higher order biological systems, such as biological pathways and processes. We propose a methodology for gleaning patterns of interactions between biological processes by analyzing protein-protein interactions, transcriptional co-expression and genetic interactions. At the heart of the methodology are the concept of Linked Processes and the resultant network of biological processes, the Process Linkage Network (PLN). RESULTS. We construct, catalogue, and analyze different types of PLNs derived from different data sources and different species. When applied to the Gene Ontology, many of the resulting links connect processes that are distant from each other in the hierarchy, even though the connection makes eminent sense biologically. Some others, however, carry an element of surprise and may reflect mechanisms that are unique to the organism under investigation. In this aspect our method complements the link structure between processes inherent in the Gene Ontology, which by its very nature is species-independent. As a practical application of the linkage of processes we demonstrate that it can be effectively used in protein function prediction, having the power to increase both the coverage and the accuracy of predictions, when carefully integrated into prediction methods. CONCLUSIONS. Our approach constitutes a promising new direction towards understanding the higher levels of organization of the cell as a system which should help current efforts to re-engineer ontologies and improve our ability to predict which proteins are involved in specific biological processes."
SIMON KASIF,Quantitative Analysis of Single Nucleotide Polymorphisms within Copy Number Variation,"BACKGROUND. Single nucleotide polymorphisms (SNPs) have been used extensively in genetics and epidemiology studies. Traditionally, SNPs that did not pass the Hardy-Weinberg equilibrium (HWE) test were excluded from these analyses. Many investigators have addressed possible causes for departure from HWE, including genotyping errors, population admixture and segmental duplication. Recent large-scale surveys have revealed abundant structural variations in the human genome, including copy number variations (CNVs). This suggests that a significant number of SNPs must be within these regions, which may cause deviation from HWE. RESULTS. We performed a Bayesian analysis on the potential effect of copy number variation, segmental duplication and genotyping errors on the behavior of SNPs. Our results suggest that copy number variation is a major factor of HWE violation for SNPs with a small minor allele frequency, when the sample size is large and the genotyping error rate is 0~1%. CONCLUSIONS. Our study provides the posterior probability that a SNP falls in a CNV or a segmental duplication, given the observed allele frequency of the SNP, sample size and the significance level of HWE testing."
SIMON KASIF,Identifying human interactors of SARS-CoV-2 proteins and drug targets for COVID-19 using network-based label propagation,"Motivated by the critical need to identify new treatments for COVID-19, we present a genome-scale, systems-level computational approach to prioritize drug targets based on their potential to regulate host- virus interactions or their downstream signaling targets. We adapt and specialize network label propagation methods to this end. We demonstrate that these techniques can predict human-SARS-CoV-2 protein interactors with high accuracy. The top-ranked proteins that we identify are enriched in host biological processes that are potentially coopted by the virus. We present cases where our methodology generates promising insights such as the potential role of HSPA5 in viral entry. We highlight the connection between endoplasmic reticulum stress, HSPA5, and anti-clotting agents. We identify tubulin proteins involved in ciliary assembly that are targeted by anti-mitotic drugs. Drugs that we discuss are already undergoing clinical trials to test their efficacy against COVID-19. Our prioritized list of human proteins and drug targets is available as a general resource for biological and clinical researchers who are repositioning existing and approved drugs or developing novel therapeutics as anti-COVID-19 agents."
SIMON KASIF,COMBREX: a project to accelerate the functional annotation of prokaryotic genomes,COMBREX (http://combrex.bu.edu) is a project to increase the speed of the functional annotation of new bacterial and archaeal genomes. It consists of a database of functional predictions produced by computational biologists and a mechanism for experimental biochemists to bid for the validation of those predictions. Small grants are available to support successful bids.
SIMON KASIF,Biological Context Networks: A Mosaic View of the Interactome,"Network models are a fundamental tool for the visualization and analysis of molecular interactions occurring in biological systems. While broadly illuminating the molecular machinery of the cell, graphical representations of protein interaction networks mask complex patterns of interaction that depend on temporal, spatial, or condition-specific contexts. In this paper, we introduce a novel graph construct called a biological context network that explicitly captures these changing patterns of interaction from one biological context to another. We consider known gene ontology biological process and cellular component annotations as a proxy for context, and show that aggregating small process-specific protein interaction sub-networks leads to the emergence of observed scale-free properties. The biological context model also provides the basis for characterizing proteins in terms of several context-specific measures, including 'interactive promiscuity,' which identifies proteins whose interacting partners vary from one context to another. We show that such context-sensitive measures are significantly better predictors of knockout lethality than node degree, reaching better than 70% accuracy among the top scoring proteins."
SIMON KASIF,Computational Tradeoffs in Multiplex PCR Assay Design for SNP Genotyping,"BACKGROUND: Multiplex PCR is a key technology for detecting infectious microorganisms, whole-genome sequencing, forensic analysis, and for enabling flexible yet low-cost genotyping. However, the design of a multiplex PCR assays requires the consideration of multiple competing objectives and physical constraints, and extensive computational analysis must be performed in order to identify the possible formation of primer-dimers that can negatively impact product yield. RESULTS: This paper examines the computational design limits of multiplex PCR in the context of SNP genotyping and examines tradeoffs associated with several key design factors including multiplexing level (the number of primer pairs per tube), coverage (the % of SNP whose associated primers are actually assigned to one of several available tube), and tube-size uniformity. We also examine how design performance depends on the total number of available SNPs from which to choose, and primer stringency criterial. We show that finding high-multiplexing/high-coverage designs is subject to a computational phase transition, becoming dramatically more difficult when the probability of primer pair interaction exceeds a critical threshold. The precise location of this critical transition point depends on the number of available SNPs and the level of multiplexing required. We also demonstrate how coverage performance is impacted by the number of available snps, primer selection criteria, and target multiplexing levels. CONCLUSION: The presence of a phase transition suggests limits to scaling Multiplex PCR performance for high-throughput genomics applications. Achieving broad SNP coverage rapidly transitions from being very easy to very hard as the target multiplexing level (# of primer pairs per tube) increases. The onset of a phase transition can be ""delayed"" by having a larger pool of SNPs, or loosening primer selection constraints so as to increase the number of candidate primer pairs per SNP, though the latter may produce other adverse effects. The resulting design performance tradeoffs define a benchmark that can serve as the basis for comparing competing multiplex PCR design optimization algorithms and can also provide general rules-of-thumb to experimentalists seeking to understand the performance limits of standard multiplex PCR."
SIMON KASIF,MuPlex: Multi-Objective Multiplex PCR Assay Design,"We have developed a web-enabled system called MuPlex that aids researchers in the design of multiplex PCR assays. Multiplex PCR is a key technology for an endless list of applications, including detecting infectious microorganisms, whole-genome sequencing and closure, forensic analysis and for enabling flexible yet low-cost genotyping. However, the design of a multiplex PCR assays is computationally challenging because it involves tradeoffs among competing objectives, and extensive computational analysis is required in order to screen out primer-pair cross interactions. With MuPlex, users specify a set of DNA sequences along with primer selection criteria, interaction parameters and the target multiplexing level. MuPlex designs a set of multiplex PCR assays designed to cover as many of the input sequences as possible. MuPlex provides multiple solution alternatives that reveal tradeoffs among competing objectives. MuPlex is uniquely designed for large-scale multiplex PCR assay design in an automated high-throughput environment, where high coverage of potentially thousands of single nucleotide polymorphisms is required. The server is available at http://genomics14.bu.edu:8080/MuPlex/MuPlex.html."
MICHAEL F HOLICK,"Products of Vitamin D3 or 7-Dehydrocholesterol Metabolism by Cytochrome P450scc Show Anti-Leukemia Effects, Having Low or Absent Calcemic Activity","BACKGROUND. Cytochrome P450scc metabolizes vitamin D3 to 20-hydroxyvitamin D3 (20(OH)D3) and 20,23(OH)2D3, as well as 1-hydroxyvitamin D3 to 1a,20-dihydroxyvitamin D3 (1,20(OH)2D3). It also cleaves the side chain of 7-dehydrocholesterol producing 7-dehydropregnenolone (7DHP), which can be transformed to 20(OH)7DHP. UVB induces transformation of the steroidal 5,7-dienes to pregnacalciferol (pD) and a lumisterol-like compounds (pL). METHODS AND FINDINGS. To define the biological significance of these P450scc-initiated pathways, we tested the effects of their 5,7-diene precursors and secosteroidal products on leukemia cell differentiation and proliferation in comparison to 1a,25-dihydroxyvitamin D3 (1,25(OH)2D3). These secosteroids inhibited proliferation and induced erythroid differentiation of K562 human chronic myeloid and MEL mouse leukemia cells with 20(OH)D3 and 20,23(OH)2D3 being either equipotent or slightly less potent than 1,25(OH)2D3, while 1,20(OH)2D3, pD and pL compounds were slightly or moderately less potent. The compounds also inhibited proliferation and induced monocytic differentiation of HL-60 promyelocytic and U937 promonocytic human leukemia cells. Among them 1,25(OH)2D3 was the most potent, 20(OH)D3, 20,23(OH)2D3 and 1,20(OH)2D3 were less active, and pD and pL compounds were the least potent. Since it had been previously proven that secosteroids without the side chain (pD) have no effect on systemic calcium levels we performed additional testing in rats and found that 20(OH)D3 had no calcemic activity at concentration as high as 1 µg/kg, whereas, 1,20(OH)2D3 was slightly to moderately calcemic and 1,25(OH)2D3 had strong calcemic activity. CONCLUSIONS. We identified novel secosteroids that are excellent candidates for anti-leukemia therapy with 20(OH)D3 deserving special attention because of its relatively high potency and lack of calcemic activity."
LAUREN A WISE,Urogenital Abnormalities in Men Exposed to Diethylstilbestrol in Utero: A Cohort Study,"BACKGROUND: Diethylstilbestrol (DES), a synthetic estrogen widely prescribed to pregnant women during the 1940s-70s, has been shown to cause reproductive problems in the daughters. Studies of prenatally-exposed males have yielded conflicting results. METHODS: In data from a collaborative follow-up of three U.S. cohorts of DES-exposed sons, we examined the relation of prenatal DES exposure to occurrence of male urogenital abnormalities. Exposure status was determined through review of prenatal records. Mailed questionnaires (1994, 1997, 2001) asked about specified abnormalities of the urogenital tract. Risk ratios (RR) were estimated by Cox regression with constant time at risk and control for year of birth. RESULTS: Prenatal DES exposure was not associated with varicocele, structural abnormalities of the penis, urethral stenosis, benign prostatic hypertrophy, or inflammation/infection of the prostate, urethra, or epididymus. However, RRs were 1.9 (95% confidence interval 1.1-3.4) for cryptorchidism, 2.5 (1.5-4.3) for epididymal cyst, and 2.4 (1.5-4.4) for testicular inflammation/infection. Stronger associations were observed for DES exposure that began before the 11th week of pregnancy: RRs were 2.9 (1.6-5.2) for cryptorchidism, 3.5 (2.0-6.0) for epididymal cyst, and 3.0 (1.7-5.4) for inflammation/infection of testes. CONCLUSION: These results indicate that prenatal exposure to DES increases risk of male urogenital abnormalities and that the association is strongest for exposure that occurs early in gestation. The findings support the hypothesis that endocrine disrupting chemicals may be a cause of the increased prevalence of cryptorchidism that has been seen in recent years."
LAUREN A WISE,Secondary Sex Ratio among Women Exposed to Diethylstilbestrol in Utero,"BACKGROUND. Diethylstilbestrol (DES), a synthetic estrogen widely prescribed to pregnant women during the mid-1900s, is a potent endocrine disruptor. Previous studies have suggested an association between endocrine-disrupting compounds and secondary sex ratio. METHODS. Data were provided by women participating in the National Cancer Institute (NCI) DES Combined Cohort Study. We used generalized estimating equations to estimate odds ratios (ORs) and 95% confidence intervals (CIs) for the relation of in utero DES exposure to sex ratio (proportion of male births). Models were adjusted for maternal age, child's birth year, parity, and cohort, and accounted for clustering among women with multiple pregnancies. RESULTS. The OR for having a male birth comparing DES-exposed to unexposed women was 1.05 (95% CI, 0.95-1.17). For exposed women with complete data on cumulative DES dose and timing (33%), those first exposed to DES earlier in gestation and to higher doses had the highest odds of having a male birth. The ORs were 0.91 (95% C, 0.65-1.27) for first exposure at ≥ 13 weeks gestation to < 5 g DES; 0.95 (95% CI, 0.71-1.27) for first exposure at ≥ 13 weeks to ≥ 5 g; 1.16 (95% CI, 0.96-1.41) for first exposure at < 13 weeks to < 5 g; and 1.24 (95% CI, 1.04-1.48) for first exposure at < 13 weeks to ≥ 5 g compared with no exposure. Results did not vary appreciably by maternal age, parity, cohort, or infertility history. CONCLUSIONS. Overall, no association was observed between in utero DES exposure and secondary sex ratio, but a significant increase in the proportion of male births was found among women first exposed to DES earlier in gestation and to a higher cumulative dose."
LAUREN A WISE,"Association of Exposure to Phthalates with Endometriosis and Uterine Leiomyomata: Findings from NHANES, 1999-2004","BACKGROUND. Phthalates are ubiquitous chemicals used in consumer products. Some phthalates are reproductive toxicants in experimental animals, but human data are limited. OBJECTIVE. We conducted a cross-sectional study of urinary phthalate metabolite concentrations in relation to self-reported history of endometriosis and uterine leiomyomata among 1,227 women 20-54 years of age from three cycles of the National Health and Nutrition Examination Survey (NHANES), 1999-2004. METHODS. We examined four phthalate metabolites: mono(2-ethylhexyl) phthalate (MEHP), monobutyl phthalate (MBP), monoethyl phthalate (MEP), and monobenzyl phthalate (MBzP). From the last two NHANES cycles, we also examined mono(2-ethyl-5-hydroxyhexyl) phthalate (MEHHP) and mono(2-ethyl-5-oxohexyl) phthalate (MEOHP). We used logistic regression to estimate odds ratios (ORs) and 95% confidence intervals (CIs), adjusting for potential confounders. RESULTS. Eighty-seven (7%) and 151 (12%) women reported diagnoses of endometriosis and leiomyomata, respectively. The ORs comparing the highest versus lowest three quartiles of urinary MBP were 1.36 (95% CI, 0.77-2.41) for endometriosis, 1.56 (95% CI, 0.93-2.61) for leiomyomata, and 1.71 (95% CI, 1.07-2.75) for both conditions combined. The corresponding ORs for MEHP were 0.44 (95% CI, 0.19-1.02) for endometriosis, 0.63 (95% CI, 0.35-1.12) for leiomyomata, and 0.59 (95% CI, 0.37-0.95) for both conditions combined. Findings for MEHHP and MEOHP agreed with findings for MEHP with respect to endometriosis only. We observed null associations for MEP and MBzP. Associations were similar when we excluded women diagnosed > 7 years before their NHANES evaluation. CONCLUSION. The positive associations for MBP and inverse associations for MEHP in relation to endometriosis and leiomyomata warrant investigation in prospective studies."
BRIAN JACK,Continuous release of tumor-derived factors improves the modeling of cachexia in muscle cell culture,"Cachexia is strongly associated with a poor prognosis in cancer patients but the biological trigger is unknown and therefore no therapeutics exist. The loss of skeletal muscle is the most deleterious aspect of cachexia and it appears to depend on secretions from tumor cells. Models for studying wasting in cell culture consist of experiments where skeletal muscle cells are incubated with medium conditioned by tumor cells. This has led to candidates for cachectic factors but some of the features of cachexiain vivoare not yet well-modeled in cell culture experiments. Mouse myotube atrophy measured by myotube diameter in response to medium conditioned by mouse colon carcinoma cells (C26) is consistently less than what is seen in muscles of mice bearing C26 tumors with moderate to severe cachexia. One possible reason for this discrepancy is thatin vivothe C26 tumor and skeletal muscle share a circulatory system exposing the muscle to tumor factors in a constant and increasing way. We have applied Transwell®-adapted cell culture conditions to more closely simulate conditions foundin vivowhere muscle is exposed to the ongoing kinetics of constant tumor secretion of active factors. C26 cells were incubated on a microporous membrane (a Transwell® insert) that constitutes the upper compartment of wells containing plated myotubes. In this model, myotubes are exposed to a constant supply of cancer cell secretions in the medium but without direct contact with the cancer cells, analogous to a shared circulation of muscle and cancer cells in tumor-bearing animals. The results for myotube diameter support the idea that the use of Transwell® inserts serves as a more physiological model of the muscle wasting associated with cancer cachexia than the bolus addition of cancer cell conditioned medium. The Transwell® model supports the notion that the dose and kinetics of cachectic factor delivery to muscle play a significant role in the extent of pathology."
BRIAN JACK,"Bostonia: 1999-2000, no. 1-4",
BRIAN JACK,"Bostonia: 1997-1998, no. 1-4",
BRIAN JACK,"Study protocol for the implementation of the Gabby Preconception Care System - an evidence-based, health information technology intervention for Black and African American women","BACKGROUND: Improving the health of women before pregnancy and throughout a woman’s lifespan could mitigate disparities and improve the health and wellbeing of women, infants and children. The preconception period is important for reducing health risks associated with poor maternal, perinatal and neonatal outcomes, and eliminating racial and ethnic disparities in maternal and child health. Low cost health information technology interventions provided in community-based settings have the potential to reach and reduce disparities in health outcomes for socially disadvantaged, underserved and health disparity populations. These interventions are particularly important for Black and African American women who have a disproportionate burden of pregnancy-related complications and infant mortality rates compared to any other racial and ethnic group in the U.S. METHODS: This is a hybrid type II implementation-effectiveness cohort study aimed at evaluating appropriateness, acceptability and feasibility implementation outcomes, while also systematically examining the clinical effectiveness of a preconception care (PCC) intervention, the Gabby System, for Black and African American women receiving health services in community-based sites. The intervention will be implemented in six Community Health Centers and six Healthy Start programs across the U.S. Each study site will recruit and enroll 25–50 young Black and African American women who will participate in the intervention for a 6-month period. Appropriateness, acceptability and feasibility of implementing the PCC intervention will be assessed using: 1) Qualitative data derived from individual interviews with Gabby System end-users (clients and patients) and site staff; and, 2) Quantitative data from staff surveys, Gabby System usage and uptake. Aggregate health risk and utilization measures collected directly from the Gabby server will be used to examine the effectiveness of the Gabby System on self-reported behavior change. DISCUSSION: This study will examine implementation outcomes and clinical effectiveness of an evidence-based PCC intervention for Black and African American women receiving services in Healthy Start programs and Community Health Centers. Contextual factors that influence uptake and appropriate implementation strategies will be identified to inform future scalability of the intervention. TRIAL REGISTRATION: ClinicalTrials.gov NCT04514224. DATE OF REGISTRATION: August 14, 2020. Retrospectively Registered."
BRIAN JACK,Proceedings of the Sixth International Workshop on Web Caching and Content Distribution,"OVERVIEW: The International Web Content Caching and Distribution Workshop (WCW) is a premiere technical meeting for researchers and practitioners interested in all aspects of content caching, distribution and delivery on the Internet. The 2001 WCW meeting was held on the Boston University Campus. Building on the successes of the five previous WCW meetings, WCW01 featured a strong technical program and record participation from leading researchers and practitioners in the field. This report includes all the technical papers presented at WCW'01. NOTE: Proceedings of WCW'01 are published by Elsevier. Hard copies of these proceedings can be purchased through the workshop organizers. As a service to the community, electronic copies of all WCW'01 papers are accessible through Technical Report BUCS‐TR‐2001‐017, available from the Boston University Computer Science Technical Report Archives at http://www.cs.bu.edu/techreps. [Ed.note: URL outdated. Use http://www.bu.edu/cs/research/technical-reports or http://hdl.handle.net/2144/1455 in this repository to access the reports.]"
BRIAN JACK,Dataset: observational study of the clinical performance of a Public-Private Partnership national referral hospital network in Lesotho: Do improvements last over time?,"Public-private partnerships (PPP) may increase healthcare quality but lack longitudinal evidence for success. The Queen ‘Mamohato Memorial Hospital (QMMH) in Lesotho is one of Africa's first healthcare PPPs. We compare data from 2012 and 2018 on capacity, utilization, quality, and outcomes to understand if early documented successes have been sustained using the same measures over time. In this observational study using administrative and clinical data, we assessed beds, admissions, average length of stay (ALOS), outpatient visits, and patient outcomes. We measured triage time and crash cart stock through direct observation in 2013 and 2020. Operational hospital beds increased from 390 to 410. Admissions decreased (-5.3%) while outpatient visits increased (3.8%). ALOS increased from 5.1 to 6.5 days. Occupancy increased from 82% to 99%; half of the wards had occupancy rates ≥90%, and Neonatal ward occupancy was 209%. The proportion of crash cart stock present (82.9% to 73.8%) and timely triage (84.0% to 27.6%) decreased. While overall mortality decreased (8.0% to 6.5%) and neonatal mortality overall decreased (18.0% to 16.3%), mortality among very low birth weight neonates increased (30.2% to 36.8%). Declines in overall hospital mortality are promising. Yet, continued high occupancy could compromise infection control and impede response to infections, such as COVID-19. High occupancy in the Neonatal ward suggests that the population need for neonatal care outpaces QMMH capacity; improvements should be addressed at the hospital and systemic levels. The increase in ALOS is acceptable for a hospital meant to take the most critical cases. The decline in crash cart stock completeness and timely triage may affect access to emergency treatment. While the partnership itself ended earlier than anticipated, our evaluation suggests that generally the hospital under the PPP was operational, providing high-level, critically needed services, and continued to improve patient outcomes. Quality at QMMH remained substantially higher than at the former Queen Elizabeth II hospital."
BRIAN JACK,"Minerva-Australis. I. design, commissioning, and first photometric results","The Minerva-Australis telescope array is a facility dedicated to the follow-up, confirmation, characterization, and mass measurement of planets orbiting bright stars discovered by the Transiting Exoplanet Survey Satellite (TESS)—a category in which it is almost unique in the Southern Hemisphere. It is located at the University of Southern Queensland's Mount Kent Observatory near Toowoomba, Australia. Its flexible design enables multiple 0.7 m robotic telescopes to be used both in combination, and independently, for high-resolution spectroscopy and precision photometry of TESS transit planet candidates. Minerva-Australis also enables complementary studies of exoplanet spin–orbit alignments via Doppler observations of the Rossiter–McLaughlin effect, radial velocity searches for nontransiting planets, planet searches using transit timing variations, and ephemeris refinement for TESS planets. In this first paper, we describe the design, photometric instrumentation, software, and science goals of Minerva-Australis, and note key differences from its Northern Hemisphere counterpart, the Minerva array. We use recent transit observations of four planets, WASP-2b, WASP-44b, WASP-45b, and HD 189733b, to demonstrate the photometric capabilities of Minerva-Australis."
BRIAN JACK,"Bostonia: 2001-2002, no. 1-4",
BRIAN JACK,The revised TESS Input Catalog and candidate target list,"We describe the catalogs assembled and the algorithms used to populate the revised TESS Input Catalog (TIC), based on the incorporation of the Gaia second data release. We also describe a revised ranking system for prioritizing stars for 2 minute cadence observations, and we assemble a revised Candidate Target List (CTL) using that ranking. The TIC is available on the Mikulski Archive for Space Telescopes server, and an enhanced CTL is available through the Filtergraph data visualization portal system at http://filtergraph.vanderbilt.edu/tess_ctl."
BRIAN JACK,The long-baseline neutrino experiment: exploring fundamental symmetries of the universe,"The preponderance of matter over antimatter in the early Universe, the dynamics of the supernova bursts that produced the heavy elements necessary for life and whether protons eventually decay --- these mysteries at the forefront of particle physics and astrophysics are key to understanding the early evolution of our Universe, its current state and its eventual fate. The Long-Baseline Neutrino Experiment (LBNE) represents an extensively developed plan for a world-class experiment dedicated to addressing these questions. LBNE is conceived around three central components: (1) a new, high-intensity neutrino source generated from a megawatt-class proton accelerator at Fermi National Accelerator Laboratory, (2) a near neutrino detector just downstream of the source, and (3) a massive liquid argon time-projection chamber deployed as a far detector deep underground at the Sanford Underground Research Facility. This facility, located at the site of the former Homestake Mine in Lead, South Dakota, is approximately 1,300 km from the neutrino source at Fermilab -- a distance (baseline) that delivers optimal sensitivity to neutrino charge-parity symmetry violation and mass ordering effects. This ambitious yet cost-effective design incorporates scalability and flexibility and can accommodate a variety of upgrades and contributions. With its exceptional combination of experimental configuration, technical capabilities, and potential for transformative discoveries, LBNE promises to be a vital facility for the field of particle physics worldwide, providing physicists from around the globe with opportunities to collaborate in a twenty to thirty year program of exciting science. In this document we provide a comprehensive overview of LBNE's scientific objectives, its place in the landscape of neutrino physics worldwide, the technologies it will incorporate and the capabilities it will possess."
STEPHEN WILSON,Butterfly genome reveals promiscuous exchange of mimicry adaptations among species,"The evolutionary importance of hybridization and introgression has long been debated1. Hybrids are usually rare and unfit, but even infrequent hybridization can aid adaptation by transferring beneficial traits between species. Here we use genomic tools to investigate introgression in Heliconius, a rapidly radiating genus of neotropical butterflies widely used in studies of ecology, behaviour, mimicry and speciation2,3,4,5. We sequenced the genome of Heliconius melpomene and compared it with other taxa to investigate chromosomal evolution in Lepidoptera and gene flow among multiple Heliconius species and races. Among 12,669 predicted genes, biologically important expansions of families of chemosensory and Hox genes are particularly noteworthy. Chromosomal organization has remained broadly conserved since the Cretaceous period, when butterflies split from the Bombyx (silkmoth) lineage. Using genomic resequencing, we show hybrid exchange of genes between three co-mimics, Heliconius melpomene, Heliconius timareta and Heliconius elevatus, especially at two genomic regions that control mimicry pattern. We infer that closely related Heliconius species exchange protective colour-pattern genes promiscuously, implying that hybridization has an important role in adaptive radiation."
STEPHEN WILSON,"Bostonia: v. 7, no. 1-10",
STEPHEN WILSON,"Minerva-Australis. I. design, commissioning, and first photometric results","The Minerva-Australis telescope array is a facility dedicated to the follow-up, confirmation, characterization, and mass measurement of planets orbiting bright stars discovered by the Transiting Exoplanet Survey Satellite (TESS)—a category in which it is almost unique in the Southern Hemisphere. It is located at the University of Southern Queensland's Mount Kent Observatory near Toowoomba, Australia. Its flexible design enables multiple 0.7 m robotic telescopes to be used both in combination, and independently, for high-resolution spectroscopy and precision photometry of TESS transit planet candidates. Minerva-Australis also enables complementary studies of exoplanet spin–orbit alignments via Doppler observations of the Rossiter–McLaughlin effect, radial velocity searches for nontransiting planets, planet searches using transit timing variations, and ephemeris refinement for TESS planets. In this first paper, we describe the design, photometric instrumentation, software, and science goals of Minerva-Australis, and note key differences from its Northern Hemisphere counterpart, the Minerva array. We use recent transit observations of four planets, WASP-2b, WASP-44b, WASP-45b, and HD 189733b, to demonstrate the photometric capabilities of Minerva-Australis."
STEPHEN WILSON,The long-baseline neutrino experiment: exploring fundamental symmetries of the universe,"The preponderance of matter over antimatter in the early Universe, the dynamics of the supernova bursts that produced the heavy elements necessary for life and whether protons eventually decay --- these mysteries at the forefront of particle physics and astrophysics are key to understanding the early evolution of our Universe, its current state and its eventual fate. The Long-Baseline Neutrino Experiment (LBNE) represents an extensively developed plan for a world-class experiment dedicated to addressing these questions. LBNE is conceived around three central components: (1) a new, high-intensity neutrino source generated from a megawatt-class proton accelerator at Fermi National Accelerator Laboratory, (2) a near neutrino detector just downstream of the source, and (3) a massive liquid argon time-projection chamber deployed as a far detector deep underground at the Sanford Underground Research Facility. This facility, located at the site of the former Homestake Mine in Lead, South Dakota, is approximately 1,300 km from the neutrino source at Fermilab -- a distance (baseline) that delivers optimal sensitivity to neutrino charge-parity symmetry violation and mass ordering effects. This ambitious yet cost-effective design incorporates scalability and flexibility and can accommodate a variety of upgrades and contributions. With its exceptional combination of experimental configuration, technical capabilities, and potential for transformative discoveries, LBNE promises to be a vital facility for the field of particle physics worldwide, providing physicists from around the globe with opportunities to collaborate in a twenty to thirty year program of exciting science. In this document we provide a comprehensive overview of LBNE's scientific objectives, its place in the landscape of neutrino physics worldwide, the technologies it will incorporate and the capabilities it will possess."
NANCY KRESSIN,Identifying the Tuskegee Syphilis Study: implications of results from recall and recognition questions,"BACKGROUND. This analysis assessed whether Blacks, Whites and Puerto-Rican (PR) Hispanics differed in their ability to identify the Tuskegee Syphilis Study (TSS) via open-ended questions following lead-in recognition and recall questions. METHODS. The Tuskegee Legacy Project (TLP) Questionnaire was administered via a Random-Digit Dial (RDD) telephone survey to a stratified random sample of Black, White and PR Hispanic adults in three U.S. cities. RESULTS. The TLP Questionnaire was administered to 1,162 adults (356 African-Americans, 313 PR Hispanics, and 493 non-Hispanic Whites) in San Juan, PR, Baltimore, MD and New York City, NY. Recall question data revealed: 1) that 89% or more of Blacks, Whites, and PR Hispanics were not able to name or definitely identify the Tuskegee Syphilis Study by giving study attributes; and, 2) that Blacks were the most likely to provide an open-ended answer that identified the Tuskegee Syphilis Study as compared to Whites and PR Hispanics (11.5% vs 6.3% vs 2.9%, respectively) (p ≤ 0.002). Even when probed by a recognition question, only a minority of each racial/ethnic group (37.1%, 26.9%, and 8.6%, for Blacks, Whites and PR Hispanics, respectively) was able to clearly identify the TSS (p < 0.001). CONCLUSIONS. The two major implications of these findings for health disparity researchers are 1) that it is unlikely that detailed knowledge of the Tuskegee Syphilis Study has any current widespread influence on the willingness of minorities to participate in biomedical research, and 2) that caution should be applied before assuming that what community leaders 'know and are aware of' is equally 'well known' within their community constituencies."
NANCY KRESSIN,A new brief measure of oral quality of life,"INTRODUCTION. We developed a brief measure of the impact of oral conditions on individual functioning and well-being, known as oral quality of life. METHODS. Among older male veterans (N = 827) and community dental patients (N = 113), we administered surveys consisting of extant oral quality of life items, using clinical dental data from the veteran samples. We assigned each oral quality of life item to a theoretical dimension, conducted an iterative series of multitrait scaling analyses to examine the item-fit with the dimensions, reduced the number of items, and examined the psychometric characteristics of new scales and their association with clinical indices. RESULTS. We developed two brief oral quality of life scales, one consisting of 12 items and the other of 6, the latter a subset of the former. Each demonstrated sound psychometric properties and was sensitive to clinical indices. CONCLUSION. The two brief oral quality of life scales can be used to assess the population-based impact of oral conditions as well as outcomes of dental care."
RHODA AU,Genetic Correlates of Brain Aging on MRI and Cognitive Test Measures: A Genome-Wide Association and Linkage Analysis in the Framingham Study,"BACKGROUND: Brain magnetic resonance imaging (MRI) and cognitive tests can identify heritable endophenotypes associated with an increased risk of developing stroke, dementia and Alzheimer's disease (AD). We conducted a genome-wide association (GWA) and linkage analysis exploring the genetic basis of these endophenotypes in a community-based sample. METHODS: A total of 705 stroke- and dementia-free Framingham participants (age 62 +9 yrs, 50% male) who underwent volumetric brain MRI and cognitive testing (1999–2002) were genotyped. We used linear models adjusting for first degree relationships via generalized estimating equations (GEE) and family based association tests (FBAT) in additive models to relate qualifying single nucleotide polymorphisms (SNPs, 70,987 autosomal on Affymetrix 100K Human Gene Chip with minor allele frequency ≥ 0.10, genotypic call rate ≥ 0.80, and Hardy-Weinberg equilibrium p-value ≥ 0.001) to multivariable-adjusted residuals of 9 MRI measures including total cerebral brain (TCBV), lobar, ventricular and white matter hyperintensity (WMH) volumes, and 6 cognitive factors/tests assessing verbal and visuospatial memory, visual scanning and motor speed, reading, abstract reasoning and naming. We determined multipoint identity-by-descent utilizing 10,592 informative SNPs and 613 short tandem repeats and used variance component analyses to compute LOD scores. RESULTS: The strongest gene-phenotype association in FBAT analyses was between SORL1 (rs1131497; p = 3.2 × 10-6) and abstract reasoning, and in GEE analyses between CDH4 (rs1970546; p = 3.7 × 10-8) and TCBV. SORL1 plays a role in amyloid precursor protein processing and has been associated with the risk of AD. Among the 50 strongest associations (25 each by GEE and FBAT) were other biologically interesting genes. Polymorphisms within 28 of 163 candidate genes for stroke, AD and memory impairment were associated with the endophenotypes studied at p < 0.001. We confirmed our previously reported linkage of WMH on chromosome 4 and describe linkage of reading performance to a marker on chromosome 18 (GATA11A06), previously linked to dyslexia (LOD scores = 2.2 and 5.1). CONCLUSION: Our results suggest that genes associated with clinical neurological disease also have detectable effects on subclinical phenotypes. These hypothesis generating data illustrate the use of an unbiased approach to discover novel pathways that may be involved in brain aging, and could be used to replicate observations made in other studies."
RHODA AU,Sleep Patterns in Elementary School Children (Grades 2-5) and Adolescents (Grade 10),"Background: To date, there is limited research examining sleep patterns in elementary school children. Previous researchers focused on parental responses rather than student responses to determine factors that affect sleep. The presented study surveyed sleep patterns and examined external factors affecting total sleep time among elementary school children and adolescents. Methods: Students in grades 2-5 (n=885) and grade 10 (n=190) enrolled in a public school system in the Northeast, completed a district administered survey that included questions on sleep duration and hygiene. Results. Average reported sleep duration decreased with increasing grade level. Children in grades 2-5 woke up earlier (31.7-72.4%) and on their own in comparison to adolescents in grade 10 (6.8%). Significantly shorter sleep durations were associated with having a television (grades 2, 4, 5, p< 0.01) or a cell phone in the room (grades 3, 4; p < 0.05), playing on the computer or video games (grades 3, 4, p<.001) before going to bed. In contrast, students in grade 2, 3, & 4 who reported reading a book before going to bed slept on average 21 minutes more per night (p=.029, .007, .009, respectively). For tenth graders, only consumption of energy drinks led to significant reduction in sleep duration (p<.0001). Conclusion. Sleep is a fundamental aspect in maintaining a healthy and adequate life style. Understanding sleep patterns will assist parents, health care providers, and educators in promoting quality sleep hygiene in school-aged children."
RHODA AU,Automated detection of mild cognitive impairment and dementia from voice recordings: A natural language processing approach,"INTRODUCTION: Automated computational assessment of neuropsychological tests would enable widespread, cost-effective screening for dementia. METHODS: A novel natural language processing approach is developed and validated to identify different stages of dementia based on automated transcription of digital voice recordings of subjects' neuropsychological tests conducted by the Framingham Heart Study (n = 1084). Transcribed sentences from the test were encoded into quantitative data and several models were trained and tested using these data and the participants' demographic characteristics. RESULTS: Average area under the curve (AUC) on the held-out test data reached 92.6%, 88.0%, and 74.4% for differentiating Normal cognition from Dementia, Normal or Mild Cognitive Impairment (MCI) from Dementia, and Normal from MCI, respectively. DISCUSSION: The proposed approach offers a fully automated identification of MCI and dementia based on a recorded neuropsychological test, providing an opportunity to develop a remote screening tool that could be adapted easily to any language."
RHODA AU,Large language models in neurology research and future practice,"Recent advancements in generative artificial intelligence, particularly using large language models (LLMs), are gaining increased public attention. We provide a perspective on the potential of LLMs to analyze enormous amounts of data from medical records and gain insights on specific topics in neurology. In addition, we explore use cases for LLMs, such as early diagnosis, supporting patient and caregivers, and acting as an assistant for clinicians. We point to the potential ethical and technical challenges raised by LLMs, such as concerns about privacy and data security, potential biases in the data for model training, and the need for careful validation of results. Researchers must consider these challenges and take steps to address them to ensure that their work is conducted in a safe and responsible manner. Despite these challenges, LLMs offer promising opportunities for improving care and treatment of various neurologic disorders."
ZHONGJUN QU,Uniform inference on quantile effects under sharp regression discontinuity designs,"This study develops methods for conducting uniform inference on quantile treatment effects for sharp regression discontinuity designs. We develop a score test for the treatment significance hypothesis and Wald-type tests for the hypotheses related to treatment significance, homogeneity, and unambiguity. The bias from the nonparametric estimation is studied in detail. In particular, we show that under some conditions, the asymptotic distribution of the score test is unaffected by the bias, without under-smoothing. For situations where the conditions can be restrictive, we incorporate a bias correction into the Wald tests and account for the estimation uncertainty. We also provide a procedure for constructing uniform confidence bands for quantile treatment effects. As an empirical application, we use the proposed methods to study the effect of cash-on-hand on unemployment duration. The results reveal pronounced treatment heterogeneity and also emphasize the importance of considering the long-term unemployed."
ZHONGJUN QU,Inference on conditional quantile processes in partially linear models with applications to the impact of unemployment benefits,"We propose methods to estimate and conduct inference on conditional quantile processes for models with nonparametric and linear components. The estimation procedure uses local linear or quadratic regressions, with the bandwidth allowed to vary across quantiles to adapt to data sparsity. We establish a Bahadur representation that holds uniformly in the covariate value and the quantile index. Then,we show that the proposed estimator converges weakly to a Gaussian process and develop methods for constructing uniform confidence bands and hypothesis testing. Our results also cover locally partially linear models with boundary points, thereby allowing for Sharp Regression Discontinuity Designs (SRD). This allows us to study the effects of unemployment insurance (UI) benefits extensions using the dataset of Nekoei and Weber (2017) who found a statistically significant effect, though of minor economic importance using an SRD focusing on the average effect. Our model allows heterogeneity with respect to both the covariate and the quantile. We find economically strong significant effects in the tail of the distribution,say the 10% quantile of the outcome variable (e.g., the wage change distribution). Under a rank invariance assumption, this implies that individuals who benefited the most are those who would have experienced substantial wage cuts if there were no benefit extension. Since our setup allows for discrete covariates, we also find positive and statistically significant effects for white-collar and female workers and those with a college education, but not for blue-collar male workers without higher education. Hence, while UI benefits reduce the within-group inequality for some subgroups by covariates, they can be viewed as regressive and enhancing between-group inequality, although they also help to bridge the gender gap."
ZHONGJUN QU,Using arbitrary precision arithmetic to sharpen identification analysis for DSGE models,"This paper is at the intersection of macroeconomics and modern computer arithmetic. It seeks to apply arbitrary precision arithmetic to resolve practical di¢ culties arising in the iden- ti cation analysis of log linearized DSGE models. The main focus is on methods in Qu and Tkachenko (2012, 2017) since the framework appears to be the most comprehensive to date. Working with this arithmetic, we develop the following three-step procedure for analyzing local and global identi cation. (1) The DSGE model solution algorithm is modi ed so that all the relevant objects are computed as multiprecision entities allowing for indeterminacy. (2) The rank condition and the Kullback-Leibler distance are computed using arbitrary precision Gauss- Legendre quadrature. (3) Minimization is carried out by combining double precision global and arbitrary precision local search algorithms, where the criterion for convergence is set based on the chosen precision level, so that it can be e¤ectively examined whether the minimized value equals zero. In an application to a model featuring monetary and scal policy interactions (Leeper, 1991 and Tan and Walker, 2015), we nd that the arithmetic removes all ambiguity in the analysis. As a result, we reach clear conclusions showing observational equivalence both within the same policy regime and across di¤erent policy regimes under generic parameter val- ues. We further illustrate the application of the method to medium scale DSGE models by considering the model of Schmitt-Grohé and Uribe (2012), where the use of extended precision again helps remove ambiguity in cases where near observational equivalence is detected."
ZHONGJUN QU,A composite likelihood framework for analyzing singular DSGE models,"This paper builds upon the composite likelihood concept of Lindsay (1988) to develop a framework for parameter identification, estimation, inference, and forecasting in DSGE models allowing for stochastic singularity. The framework consists of the following four components. First, it provides a necessary and sufficient condition for parameter identification, where the identifying information is provided by the first and second order properties of nonsingular submodels. Second, it provides an MCMC based procedure for parameter estimation. Third, it delivers confidence sets for structural parameters and impulse responses that allow for model misspecification. Fourth, it gen- erates forecasts for all the observed endogenous variables, irrespective of the number of shocks in the model. The framework encompasses the conventional likelihood analysis as a special case when the model is nonsingular. It enables the researcher to start with a basic model and then gradually incorporate more shocks and other features, meanwhile confronting all the models with the data to assess their implications. The methodology is illustrated using both small and medium scale DSGE models. These models have numbers of shocks ranging between one and seven."
ZHONGJUN QU,Sieve estimation of option implied state price density,"The state price density, as a central concept in asset pricing, embodies rich information about market expectations and risk attitudes. The paper develops a nonparametric estimator for this density using a single cross section of European option prices. The estimator has two features that di erentiate it from other methods in the literature. First, it uses information from both call and put option prices. Second, it does not require estimating any second order derivative. The estimator is characterized by the solution to a constrained and penalized linear regression. The technical analysis faces two challenges because the density is de ned by the Fredholm integral equation of the rst kind with an unbounded support, and the kernel functions are unbounded and non-di erentiable. We address these challenges by exploiting the structure of the option pricing problem. After establishing the consistency and the convergence rate of the estimator, we apply it to estimate the state price densities implied by S&P500 index options and by the VIX options. The sample periods include the recent nancial crisis and the Great Recession, during which the market turbulence imposes substantial challenges for adaptive estimation. We show that the procedure can work with both daily and high frequency observations. We also study whether quantiles of this density have predictive power for future return distributions."
FLORIAN EDERER,A tale of two networks: common ownership and product market rivalry,
FLORIAN EDERER,Innovation: the bright side of common ownership?,
TEREASA G BRAINERD,High-resolution simulations of cluster formation,"The formation history of rich clusters is investigated using a hybrid N-body simulation in which high spatial and mass resolution can be achieved self-consistently within a small region of a very large volume. The evolution of three massive clusters is studied via mass accretion, spherically averaged density profiles, three-dimensional and projected shapes, and degree of substructure. Each cluster is resolved well and consists of ~4 × 105 particles at the present epoch. Although the clusters have similar masses, M(r = 1.5 h-1 Mpc) ~ 2 × 1015 h-1 M☉, and similar spherically averaged density profiles at the end of the simulations, markedly different formation histories are observed. No single, dominant pattern is apparent in the time variation of the mass accretion rate, the cluster shape, or the degree of substructure. Individually, the density profiles of the clusters are fitted well by Navarro, Frenk, & White (NFW) profiles over the course of the simulations. The values of the NFW concentration parameter that best reproduce the cluster profiles are, however, lower than the values predicted for halos with masses identical to those of the simulated clusters. The largest discrepancy between the observed and predicted concentration parameters is of order a factor of 2.5 and is most likely caused by substructure in the clusters on scales less than the ""virial radius,"" r200."
TEREASA G BRAINERD,Mass-to-light ratios of 2dF galaxies,"We compute Mimg1.gif, the dynamical mass interior to a radius of 260 h-1 kpc, for a set of 809 isolated host galaxies in the 100k data release of the 2dF Galaxy Redshift Survey. The hosts are surrounded by 1556 fainter satellite galaxies. Our mass estimator and host/satellite selection criteria are taken from those used by the Sloan Digital Sky Survey collaboration for a similar analysis of Mimg1.gif, and overall our results compare well with theirs. In particular, for L gsim 2L*, we find (Mimg1.gif/L)img2.gif = 193 ± 14 h M☉/L☉, with a weak tendency for hosts with L < 2L* to have a somewhat higher M/L. Additionally, we investigate M/L for bright (bJ lesssim 18) galaxies with elliptical, S0, and spiral morphologies. There are 159 hosts in the elliptical/S0 sample; similar to the full sample, (Mimg1.gif/L)img2.gif = 271 ± 26 h M☉/L☉ for galaxies with L gsim 2L*. There is a weak tendency for less luminous galaxies to have a somewhat higher M/L. In stark contrast to this, the line-of-sight velocity dispersion for the 243 spiral hosts is independent of the host luminosity, with a value of σv = 189 ± 19 km s-1. Thus, for spiral hosts, we find (Mimg1.gif/L)img2.gif ∝ L-1.0±0.2, where (Mimg1.gif/L)img2.gif for a 2L* spiral galaxy is of order 200 h M☉/L☉."
TEREASA G BRAINERD,Satellite galaxies in the Illustris-1 simulation: anisotropic locations around relatively isolated hosts,"We investigate the locations of satellite galaxies in the z = 0 redshift slice of the hydrodynamical Illustris-1 simulation. As expected from previous work, the satellites are distributed anisotropically in the plane of the sky, with a preference for being located near the major axes of their hosts. Due to misalignment of mass and light within the hosts, the degree of anisotropy is considerably less when satellite locations are measured with respect to the hosts’ stellar surface mass density than when they are measured with respect to the hosts’ dark matter surface mass density. When measured with respect to the hosts’ dark matter surface mass density, the mean satellite location depends strongly on host stellar mass and luminosity, with the satellites of the faintest, least massive hosts showing the greatest anisotropy. When measured with respect to the hosts’ stellar surface mass density, the mean satellite location is essentially independent of host stellar mass and luminosity. In addition, the satellite locations are largely insensitive to the amount of stellar mass used to define the hosts’ stellar surface mass density, as long as at least 50–70 per cent of the hosts’ total stellar mass is used. The satellite locations are dependent upon the stellar masses of the satellites, with the most massive satellites having the most anisotropic distributions."
TEREASA G BRAINERD,Satellite galaxies in the Illustris-1 simulation: poor tracers of the mass distribution,"Number density profiles are computed for the satellites of relatively isolated host galaxies in the Illustris-1 simulation. The mean total mass density of the hosts is well fitted by a Navarro–Frenk–White (NFW) profile. The number density profile for the complete satellite sample is inconsistent with NFW, and on scales ≲0.5 r200, the satellites do not trace the hosts' mass. This differs substantially from previous results from semianalytic galaxy formation models. The shape of the satellite number density profile depends on the luminosities of the hosts and the satellites, and on the host virial mass. The number density profile for the faintest satellites is well fitted by an NFW profile, but the concentration is much less than the mean host mass density. The number density profile for the brightest satellites exhibits a steep increase in slope for host-satellite distances ≲0.1 r200, in qualitative agreement with recent observational studies that find a steep increase in the satellite number density at small host-satellite distances. On scales ≳0.1 r200 the satellites of the faintest hosts trace the host mass reasonably well. On scales ≲0.4 r200, the satellites of the brightest hosts do not trace the host mass, and the satellite number density increases steeply for host-satellite distances ≲0.1 r200. The discrepancy between the satellite number density profile and the host mass density is most pronounced for the most massive systems, with the satellite number density falling far below that of the mass density on scales ≲0.5 r200."
TEREASA G BRAINERD,The spatial distribution of satellite galaxies selected from redshift space,"We investigate the spatial distribution of satellite galaxies using a mock redshift survey of the first Millennium Run simulation. The satellites were identified using common redshift space criteria, therefore the sample includes a large percentage of interlopers. The satellite locations are well fitted by a combination of a Navarro, Frenk and White (NFW) density profile and a power law. At fixed stellar mass, the NFW scale parameter, r s , for the satellite distribution of red hosts exceeds r s for the satellite distribution of blue hosts. In both cases, the dependence of r s on host stellar mass is well fitted by a power law. For the satellites of red hosts, {r}sred}\propto {({M}* /{M}⊙ )}0.71+/- 0.05, while for the satellites of blue hosts {r}sblue}\propto {({M}* /{M}⊙ )}0.48+/- 0.07. For hosts with stellar masses {M}* ≳ 4× {10}10{M}⊙ , the satellite distribution around blue hosts is more concentrated than is the satellite distribution around red hosts. The spatial distribution of the satellites of red hosts traces that of the hosts’ halos; however, the spatial distribution of the satellites of blue hosts is more concentrated than that of the hosts’ halos by a factor of ˜2. Our methodology is general, and applies to any analysis of satellites in a mock redshift survey. However, our conclusions necessarily depend upon the semi-analytic galaxy formation model that was adopted, and different galaxy formation models may yield different results."
TEREASA G BRAINERD,Lopsided satellite distributions around isolated host galaxies,"We investigate the spatial distribution of the satellites of bright, isolated host galaxies. In agreement with previous studies, we find that, on average, the satellites of red hosts are found preferentially close to their hosts' major axes, while the satellites of blue hosts are distributed isotropically. We compute the pairwise clustering of the satellites and find a strong tendency for pairs of satellites to be located on the same side of their host, resulting in lopsided spatial distributions. The signal is most pronounced for the satellites of blue hosts, where the number of pairs on the same side of their host exceeds the number of pairs on opposite sides of their host by a factor of 1.8 ± 0.1. For the satellites of red hosts, the number of pairs on the same side of their host exceeds the number of pairs on opposite sides of their host by a factor of 1.08 ± 0.03. Satellites that are far from their hosts (rp ≳ 300 kpc) show a strong preference for being located on the same side of their hosts; satellites that are near to their hosts (rp ≲ 100 kpc) show a weak preference for being located on opposite sides of their hosts. While lopsided distributions have been found previously for the satellites of bright pairs of galaxies, ours is the first study to find lopsided distributions for the satellites of isolated bright galaxies."
TEREASA G BRAINERD,Fast generation of large-scale structure density maps via generative adversarial networks,"Generative Adversarial Networks (GANs) are a recent advancement in unsupervised machine learning. They are a cat-and-mouse game between two neural networks: (1) a discriminator network which learns to validate whether a sample is real or fake compared to a training set and (2) a generator network which learns to generate data that appear to belong to the training set. Both networks learn from each other until training is complete and the generator network is able to produce samples that are indistinguishable from the training set. We find that GANs are well-suited for fast generation of novel 3D density maps that are indistinguishable from those obtained from N-body simulations. In a matter of seconds, a fully trained GAN can generate thousands of density maps at different epochs in the history of the universe. These GAN-generated maps can then be used to study the evolution of large-scale structure over time."
CHRISTOPHER E BEAUDOIN,Developing an evidence-based fall prevention curriculum for community health workers,"This perspective paper describes processes in the development of an evidence-based fall prevention curriculum for community health workers/promotores (CHW/P) that highlights the development of the curriculum and addresses: (1) the need and rationale for involving CHW/P in fall prevention; (2) involvement of CHW/P and content experts in the curriculum development; (3) best practices utilized in the curriculum development and training implementation; and (4) next steps for dissemination and utilization of the CHW/P fall prevention curriculum. The project team of CHW/P and content experts developed, pilot tested, and revised bilingual in-person training modules about fall prevention among older adults. The curriculum incorporated the following major themes: (1) fall risk factors and strategies to reduce/prevent falls; (2) communication strategies to reduce risk of falling and strategies for developing fall prevention plans; and (3) health behavior change theories utilized to prevent and reduce falls. Three separate fall prevention modules were developed for CHW/P and CHW/P Instructors to be used during in-person trainings. Module development incorporated a five-step process: (1) conduct informal focus groups with CHW/P to inform content development; (2) develop three in-person modules in English and Spanish with input from content experts; (3) pilot-test the modules with CHW/P; (4) refine and finalize modules based on pilot-test feedback; and (5) submit modules for approval of continuing education units. This project contributes to the existing evidence-based literature by examining the role of CHW/P in fall prevention among older adults. By including evidence-based communication strategies such as message tailoring, the curriculum design allows CHW/P to personalize the information for individuals, which can result in an effective dissemination of a curriculum that is evidence-based and culturally appropriate."
CHRISTOPHER E BEAUDOIN,Disentangling real-world and virtual-world social norms: the persuasive elements and social psychological effects of a serious game,"In the immersive environment of an online serious video game designed to depict social norms about excessive alcohol consumption, this study examines the effects of real-world and virtual-world social norms on alcohol use outcomes, centering on the persuasive elements of the One Shot game and its social psychological effects. This study implements a one-group, pretest-posttest, quasi-experimental design. Using a national sample of young adults aged 21–25 (N = 550) who reported recent binge drinking, OLS regression analyses documented that virtual-world norms were more predictive of alcohol-related outcomes than real-world norms. Specifically, virtual-world descriptive norms paired with severe consequences of hazardous drinking predicted improvements from pre-game to post-game in drinking refusal self-efficacy and attitudes toward drinking. Likewise, virtual-world social disapproval had a similar effect on drinking refusal self-efficacy, but not on attitudes toward drinking. Neither measure of virtual-world social norms had a significant effect on changes in intention to drink less from pre-game to post-game. Real-world descriptive norms and social disapproval had no significant effects on any of the alcohol use outcomes. Each of the three alcohol use outcomes improved significantly from pre-game to post-game. Implications for norms-based research and online serious video games are discussed."
CHRISTOPHER E BEAUDOIN,The SNO+ experiment,
CHRISTOPHER E BEAUDOIN,Event Horizon Telescope imaging of the archetypal blazar 3C 279 at an extreme 20 microarcsecond resolution,"3C 279 is an archetypal blazar with a prominent radio jet that show broadband flux density variability across the entire electromagnetic spectrum. We use an ultra-high angular resolution technique – global Very Long Baseline Interferometry (VLBI) at 1.3 mm (230 GHz) – to resolve the innermost jet of 3C 279 in order to study its fine-scale morphology close to the jet base where highly variable γ-ray emission is thought to originate, according to various models. The source was observed during four days in April 2017 with the Event Horizon Telescope at 230 GHz, including the phased Atacama Large Millimeter/submillimeter Array (ALMA), at an angular resolution of ∼20 μas (at a redshift of z = 0.536 this corresponds to ∼0.13 pc  ∼ 1700 Schwarzschild radii with a black hole mass M<jats:sub>BH</jats:sub> = 8 × 10^8 M_⊙). Imaging and model-fitting techniques were applied to the data to parameterize the fine-scale source structure and its variation. We find a multicomponent inner jet morphology with the northernmost component elongated perpendicular to the direction of the jet, as imaged at longer wavelengths. The elongated nuclear structure is consistent on all four observing days and across different imaging methods and model-fitting techniques, and therefore appears robust. Owing to its compactness and brightness, we associate the northern nuclear structure as the VLBI “core”. This morphology can be interpreted as either a broad resolved jet base or a spatially bent jet. We also find significant day-to-day variations in the closure phases, which appear most pronounced on the triangles with the longest baselines. Our analysis shows that this variation is related to a systematic change of the source structure. Two inner jet components move non-radially at apparent speeds of ∼15 c and ∼20 c (∼1.3 and ∼1.7 μas day^−1, respectively), which more strongly supports the scenario of traveling shocks or instabilities in a bent, possibly rotating jet. The observed apparent speeds are also coincident with the 3C 279 large-scale jet kinematics observed at longer (cm) wavelengths, suggesting no significant jet acceleration between the 1.3 mm core and the outer jet. The intrinsic brightness temperature of the jet components are ≲10^10 K, a magnitude or more lower than typical values seen at ≥7 mm wavelengths. The low brightness temperature and morphological complexity suggest that the core region of 3C 279 becomes optically thin at short (mm) wavelengths."
CHRISTOPHER E BEAUDOIN,First M87 Event Horizon Telescope results. II. Array and instrumentation,"The Event Horizon Telescope (EHT) is a very long baseline interferometry (VLBI) array that comprises millimeter- and submillimeter-wavelength telescopes separated by distances comparable to the diameter of the Earth. At a nominal operating wavelength of ~1.3 mm, EHT angular resolution (λ/D) is ~25 μas, which is sufficient to resolve nearby supermassive black hole candidates on spatial and temporal scales that correspond to their event horizons. With this capability, the EHT scientific goals are to probe general relativistic effects in the strong-field regime and to study accretion and relativistic jet formation near the black hole boundary. In this Letter we describe the system design of the EHT, detail the technology and instrumentation that enable observations, and provide measures of its performance. Meeting the EHT science objectives has required several key developments that have facilitated the robust extension of the VLBI technique to EHT observing wavelengths and the production of instrumentation that can be deployed on a heterogeneous array of existing telescopes and facilities. To meet sensitivity requirements, high-bandwidth digital systems were developed that process data at rates of 64 gigabit s−1, exceeding those of currently operating cm-wavelength VLBI arrays by more than an order of magnitude. Associated improvements include the development of phasing systems at array facilities, new receiver installation at several sites, and the deployment of hydrogen maser frequency standards to ensure coherent data capture across the array. These efforts led to the coordination and execution of the first Global EHT observations in 2017 April, and to event-horizon-scale imaging of the supermassive black hole candidate in M87."
CHRISTOPHER E BEAUDOIN,First M87 Event Horizon Telescope results. I. The shadow of the supermassive black hole,"When surrounded by a transparent emission region, black holes are expected to reveal a dark shadow caused by gravitational light bending and photon capture at the event horizon. To image and study this phenomenon, we have assembled the Event Horizon Telescope, a global very long baseline interferometry array observing at a wavelength of 1.3 mm. This allows us to reconstruct event-horizon-scale images of the supermassive black hole candidate in the center of the giant elliptical galaxy M87. We have resolved the central compact radio source as an asymmetric bright emission ring with a diameter of 42 ± 3 μas, which is circular and encompasses a central depression in brightness with a flux ratio gsim10:1. The emission ring is recovered using different calibration and imaging schemes, with its diameter and width remaining stable over four different observations carried out in different days. Overall, the observed image is consistent with expectations for the shadow of a Kerr black hole as predicted by general relativity. The asymmetry in brightness in the ring can be explained in terms of relativistic beaming of the emission from a plasma rotating close to the speed of light around a black hole. We compare our images to an extensive library of ray-traced general-relativistic magnetohydrodynamic simulations of black holes and derive a central mass of M = (6.5 ± 0.7) × 10^9 M ⊙. Our radio-wave observations thus provide powerful evidence for the presence of supermassive black holes in centers of galaxies and as the central engines of active galactic nuclei. They also present a new tool to explore gravity in its most extreme limit and on a mass scale that was so far not accessible."
DILIP MOOKHERJEE,Financing smallholder agriculture: an experiment with agent-intermediated microloans in India,"We explore the hypothesis that traditional joint-liability microfinance programs fail to increase borrower incomes in part because they cannot screen out unproductive borrowers. In randomly selected villages in West Bengal, India, we implemented trader-agent-intermediated lending (TRAIL), in which local trader-lender agents were incentivized through repayment-based commissions to select borrowers for individual liability loans. In other randomly selected villages, we organized a group-based lending (GBL) program in which individuals formed 5-member groups and received joint liability loans. TRAIL loans increased the production of the leading cash crop by 27% and farm incomes by 22%. GBL loans had insignificant effects. We develop and test a theoretical model of borrower selection and incentives. Farmers selected by the TRAIL agents were more able than those who self-selected into the GBL scheme; this pattern of selection explains at least 30–40% of the observed difference in income impacts."
DILIP MOOKHERJEE,"Development that works, March 31, 2011","The theme and the title of the conference—”Development That Works”—stemmed from the conference organizers’ desire to explore, from a groundlevel perspective, what programs, policies, and practices have been shown—or appear to have the potential—to achieve sustained, long-term advances in development in various parts of the world. The intent was not to simply showcase “success stories,” but rather to explore the larger concepts and opportunities that have resulted in development that is meaningful and sustainable over time. The presentations and discussions focused on critical assessments of why and how some programs take hold, and what can be learned from them. From the influence of global economic structures to innovative private sector programs and the need to evaluate development programs at the “granular” level, the expert panelists provided well-informed and often provocative perspectives on what is and isn’t working in development programs today, and what could work better in the future."
DILIP MOOKHERJEE,Evaluating the distributive impacts of a micro-credit intervention,"Most analyses of randomized controlled trials of development interventions estimate an average treatment effect. However, the aggregate impact on welfare also depends on distributional effects. We propose a simple approach to evaluate efficiency-equity trade-offs, that follow in the utilitarian tradition of Atkinson (1970). The method does not impose additional assumptions or data requirements beyond those needed to estimate the average treatment effect. We illustrate the approach using data from a credit delivery experiment we implemented in West Bengal, India."
DILIP MOOKHERJEE,Regulatory mechanism design with strong collusion,
DILIP MOOKHERJEE,Credit rationing and pass-through in supply chains: theory and evidence from Bangladesh,"Traders are often blamed for high prices, prompting government regulation. We study the effects of a government ban of a layer of financing intermediaries in edible oil supply chain in Bangladesh during 2011-12. Contrary to the predictions of a standard model of an oligopolistic supply chain, the ban caused downstream wholesale and retail prices to rise, and pass-through of the changes in imported crude oil price to fall. These results can be explained by an extension of the standard model to incorporate trade credit frictions, where intermediaries expand credit access of downstream traders."
DILIP MOOKHERJEE,Clientelistic politics and pro-poor targeting: rules versus discretionary budgets,
DILIP MOOKHERJEE,A theory of progressive lending,
DILIP MOOKHERJEE,"Forest degradation and economic growth in Nepal, 2003–2010","We investigate the relation between economic growth, household firewood collection, and forest conditions in Nepal between 2003 and 2010. Comovements in these are examined at the household and village levels, combining satellite imagery and household (Nepal Living Standard Measurement Survey) data. Projections of the impact of economic growth based on Engel curves turn out to be highly inaccurate: forest conditions remained stable despite considerable growth in household consumption and income. Firewood collections at the village level remained stable, as effects of demographic growth were offset by substantial reductions in per household collections. Households substituted firewood by alternative energy sources, particularly when livestock and farm-based occupations declined in importance. Engel curve specifications which include household productive assets (a proxy for occupational patterns) provide more accurate predictions. Hence structural changes accompanying economic growth play an important role in offsetting adverse environmental consequences of growth."
DILIP MOOKHERJEE,A general equilibrium analysis of personal bankruptcy law,We analyse an economy where principals and agents match and contract subject to moral hazard. Bankruptcy law defines the limited liability constraint in these contracts. We analyse Walrasian allocations to generate the following predictions: (i) weakening bankruptcy law causes redistribution of debt and welfare from poor agents and principals to rich agents; (ii) exemption limits Pareto‐dominate other bankruptcy laws if project size is fixed; (iii) means‐testing (as in recent US personal bankruptcy law) that is ex post pro‐poor in intent makes the poor worse off ex ante.
DILIP MOOKHERJEE,Towards a comprehensive social security system: an assessment of recent UBI proposals,"Few people would disagree that welfare programmes in India are characterized by corruption, mistargeting and poor coordination, and impose high financial and environmental costs. A more pernicious consequence is the political culture of clientelistic vote buying they help create. Parties woo swing voters with the delivery of private short-term benefits such as Mahatma Gandhi National Rural Employment Guarantee Act (MNREGA) work, cheap food, liquor, sarees, domestic appliances, cash, subsidized loans, loan waivers and access to government services. Public goods or systematic programmes for lowering poverty, dependence, ignorance and disease (such as land reform, education, sanitation and public health) are neglected. Clientelism thrives on the provision of short-term benefits to a subsection of the poor, keeping them poor and perpetuating dependence on their patrons. Voting decisions of the poor are driven by self-interested considerations of securing political patronage, rather than expressing judgement on governance. It is no wonder then that public policy disasters and corruption scandals scarcely dent vote margins of popular chief ministers. Democracy thereby fails to deliver government accountability or long-term development."
DILIP MOOKHERJEE,Asymmetric information and middleman margins: an experiment with indian potato farmers,"West Bengal potato farmers cannot directly access wholesale markets and do not knowwholesale prices. Local middlemen earn large margins; pass-through from wholesale to farmgate prices is negligible. When we informed farmers in randomly chosen villages about wholesale prices, average farmgate sales and prices were unaffected, but pass-through to farmgate prices increased. These results can be explained by a model where farmers bargain ex post with village middlemen, with the outside option of selling to middlemen outside the village. They are inconsistent with standard oligopolistic models of pass-through, search frictions, or risk-sharing contracts."
DILIP MOOKHERJEE,Property rights and gender bias: evidence from land reform in West Bengal,"While land reforms are typically pursued in order to raise productivity and reduce inequality across households, an unintended consequence may be increased within-household gender inequality. We analyse a tenancy registration programme in West Bengal, and find that it increased child survival and reduced fertility. However, we also find that it intensified son preference in families without a first-born son to inherit the land title. These families exhibit no reduction in fertility, an increase in the probability that a subsequent birth is male, and a substantial increase in the survival advantage of subsequent sons over daughters."
DILIP MOOKHERJEE,Middleman margins and asymmetric information: an experiment with potato farmers in West Bengal,"West Bengal potato farmers cannot directly access wholesale markets and do not know wholesale prices. Local middlemen earn large margins; pass-through from wholesale to farm-gate prices is negligible. When we informed farmers in randomly chosen villages about wholesale prices, average farm-gate sales and priceswere unaffected, but pass-through to farm-gate prices increased. These results can be explained by a model where farmers bargain ex post with village middlemen, with the outside option of selling to middlemen outside the village. They are inconsistent with standard oligopolistic models of pass-through, search frictions or risk-sharing contracts."
DILIP MOOKHERJEE,"Growth, automation, and the long run share of labor","We study the long run implications of workplace automation induced by capital accumulation. We describe a minimal set of sufficient conditions for sustained growth, along with a declining labor share of income in the long run: (i) a basic asymmetry between physical and human capital; (ii) the technical possibility of automation in each sector; (ii) a self-replication condition on the production function for robot services; (iv) asymptotic homotheticity (more generally neutrality) of demand, and (v) a minimal degree of patience or intergenerational altruism among a fraction of households. However, the displacement of human labor is gradual, and absolute real wages could rise indefinitely. The results obtain in the absence of any technical progress; they extend to endogenous technical progress even if such progress is not biased ex ante in favor of automation."
DILIP MOOKHERJEE,Community origins of industrial entrepreneurship in pre-independence India,
DILIP MOOKHERJEE,Decentralised targeting of transfer programmes: a reassessment,"Decentralised governance has been widely adopted in developing countries in the hope of incorporating local information into policymaking, enhancing accountability and encouraging democratic participation in the delivery of public services to the poor and needy. However, evaluations of experience with this change have highlighted problems of corruption, elite capture, and clientelism that have undermined the success of decentralisation in improving targeting of transfer programmes. Given recent advances in information technology, this chapter suggests the need to consider suitable reforms, including enhanced monitoring and recentralisation initiatives that reduce local officials’ scope for discretion. It provides an overview of recent research on these topics, and discusses key questions raised by their findings."
DILIP MOOKHERJEE,"Acquiring land from traditional communities: bottlenecks, misallocation and second-best considerations","We discuss reasons why traditional rural communities may be reluctant to voluntarily relinquish their access to land despite being compensated at market prices, thereby limiting the scope for reallocating land to more productive uses in agriculture or urban development. Owing to financial market imperfections, insurance and collateral benefits of land ownership imply that welfare-optimal land allocations may not maximize productive efficiency, even if distributive or environmental considerations are ignored. We provide some suggestive evidence and discuss implications for land acquisition policy."
CARA STEPP,Surface electromyographic control of a novel phonemic interface for speech synthesis,"Many individuals with minimal movement capabilities use AAC to communicate. These individuals require both an interface with which to construct a message (e.g., a grid of letters) and an input modality with which to select targets. This study evaluated the interaction of two such systems: (a) an input modality using surface electromyography (sEMG) of spared facial musculature, and (b) an onscreen interface from which users select phonemic targets. These systems were evaluated in two experiments: (a) participants without motor impairments used the systems during a series of eight training sessions, and (b) one individual who uses AAC used the systems for two sessions. Both the phonemic interface and the electromyographic cursor show promise for future AAC applications."
CARA STEPP,Sensorimotor adaptation of voice fundamental frequency in Parkinson's disease,"OBJECTIVE: This study examined adaptive responses to auditory perturbation of fundamental frequency (fo) in speakers with Parkinson's disease (PD) and control speakers. METHOD: Sixteen speakers with PD and nineteen control speakers produced sustained vowels while they received perturbed auditory feedback (i.e., fo shifted upward or downward). Speakers' pitch acuity was quantified using a just-noticeable-difference (JND) paradigm. Twelve listeners provided estimates of the speech intelligibility for speakers with PD. RESULTS: Fifteen responses from each speaker group for each shift direction were included in analyses. While control speakers generally showed consistent adaptive responses opposing the perturbation, speakers with PD showed no compensation on average, with individual PD speakers showing highly variable responses. In the PD group, the degree of compensation was not significantly correlated with age, disease progression, pitch acuity, or intelligibility. CONCLUSIONS: These findings indicate reduced adaptation to sustained fo perturbation and higher variability in PD compared to control participants. No significant differences were seen in pitch acuity between groups, suggesting that the fo adaptation deficit in PD is not the result of purely perceptual mechanisms. SIGNIFICANCE: These results suggest there is an impairment in vocal motor control in PD. Building on these results, contributions can be made to developing targeted voice treatments for PD."
CARA STEPP,Contributions of auditory and somatosensory feedback to vocal motor control,"PURPOSE: To better define the contributions of somatosensory and auditory feedback in vocal motor control, a laryngeal perturbation experiment was conducted with and without masking of auditory feedback. METHOD: Eighteen native speakers of English produced a sustained vowel while their larynx was physically and externally displaced on a subset of trials. For the condition with auditory masking, speech-shaped noise was played via earphones at 90 dB SPL. Responses to the laryngeal perturbation were compared to responses by the same participants to an auditory perturbation experiment that involved a 100-cent downward shift in fundamental frequency (fo). Responses were also examined in relation to a measure of auditory acuity. RESULTS: Compensatory responses to the laryngeal perturbation were observed with and without auditory masking. The level of compensation was greatest in the laryngeal perturbation condition without auditory masking, followed by the condition with auditory masking; the level of compensation was smallest in the auditory perturbation experiment. No relationship was found between the degree of compensation to auditory versus laryngeal perturbations, and the variation in responses in both perturbation experiments was not related to auditory acuity. CONCLUSIONS: The findings indicate that somatosensory and auditory feedback control mechanisms work together to compensate for laryngeal perturbations, resulting in the greatest degree of compensation when both sources of feedback are available. In contrast, these two control mechanisms work in competition in response to auditory perturbations, resulting in an overall smaller degree of compensation. Supplemental Material https://doi.org/10.23641/asha.12559628."
DAVID H FARB,"Pharmacological Properties of DOV 315,090, an Ocinaplon Metabolite","BACKGROUND. Compounds targeting the benzodiazepine binding site of the GABAA-R are widely prescribed for the treatment of anxiety disorders, epilepsy, and insomnia as well as for pre-anesthetic sedation and muscle relaxation. It has been hypothesized that these various pharmacological effects are mediated by different GABAA-R subtypes. If this hypothesis is correct, then it may be possible to develop compounds targeting particular GABAA-R subtypes as, for example, selective anxiolytics with a diminished side effect profile. The pyrazolo[1,5-a]-pyrimidine ocinaplon is anxioselective in both preclinical studies and in patients with generalized anxiety disorder, but does not exhibit the selectivity between α1/α2-containing receptors for an anxioselective that is predicted by studies using transgenic mice. RESULTS. We hypothesized that the pharmacological properties of ocinaplon in vivo might be influenced by an active biotransformation product with greater selectivity for the α2 subunit relative to α1. One hour after administration of ocinaplon, the plasma concentration of its primary biotransformation product, DOV 315,090, is 38% of the parent compound. The pharmacological properties of DOV 315,090 were assessed using radioligand binding studies and two-electrode voltage clamp electrophysiology. We report that DOV 315,090 possesses modulatory activity at GABAA-Rs, but that its selectivity profile is similar to that of ocinaplon. CONCLUSION. These findings imply that DOV 315,090 could contribute to the action of ocinaplon in vivo, but that the anxioselective properties of ocinaplon cannot be readily explained by a subtype selective effect/action of DOV 315,090. Further inquiry is required to identify the extent to which different subtypes are involved in the anxiolytic and other pharmacological effects of GABAA-R modulators."
EDWARD RIEDL,The impact of online display advertising and paid search advertising relative to offline advertising on firm performance and firm value,"This research examines the impact of online display advertising and paid search advertising relative to offline advertising on firm performance and firm value. Using proprietary data on annualized advertising expenditures for 1651 firms spanning seven years, we document that both display advertising and paid search advertising exhibit positive effects on firm performance (measured by sales) and firm value (measured by Tobin's q). Paid search advertising has a more positive effect on sales than offline advertising, consistent with paid search being closest to the actual purchase decision and having enhanced targeting abilities. Display advertising exhibits a relatively more positive effect on Tobin's q than offline advertising, consistent with its long-term effects. The findings suggest heterogeneous economic benefits across different types of advertising, with direct implications for managers in analyzing advertising effectiveness and external stakeholders in assessing firm performance."
MARK A KON,Empirical normalization for quadratic discriminant analysis and classifying cancer subtypes,"We introduce a new discriminant analysis method (Empirical Discriminant Analysis or EDA) for binary classification in machine learning. Given a dataset of feature vectors, this method defines an empirical feature map transforming the training and test data into new data with components having Gaussian empirical distributions. This map is an empirical version of the Gaussian copula used in probability and mathematical finance. The purpose is to form a feature mapped dataset as close as possible to Gaussian, after which standard quadratic discriminants can be used for classification. We discuss this method in general, and apply it to some datasets in computational biology."
MARK A KON,Top scoring pairs for feature selection in machine learning and applications to cancer outcome prediction,"BACKGROUND: The widely used k top scoring pair (k-TSP) algorithm is a simple yet powerful parameter-free classifier. It owes its success in many cancer microarray datasets to an effective feature selection algorithm that is based on relative expression ordering of gene pairs. However, its general robustness does not extend to some difficult datasets, such as those involving cancer outcome prediction, which may be due to the relatively simple voting scheme used by the classifier. We believe that the performance can be enhanced by separating its effective feature selection component and combining it with a powerful classifier such as the support vector machine (SVM). More generally the top scoring pairs generated by the k-TSP ranking algorithm can be used as a dimensionally reduced subspace for other machine learning classifiers. RESULTS: We developed an approach integrating the k-TSP ranking algorithm (TSP) with other machine learning methods, allowing combination of the computationally efficient, multivariate feature ranking of k-TSP with multivariate classifiers such as SVM. We evaluated this hybrid scheme (k-TSP+SVM) in a range of simulated datasets with known data structures. As compared with other feature selection methods, such as a univariate method similar to Fisher's discriminant criterion (Fisher), or a recursive feature elimination embedded in SVM (RFE), TSP is increasingly more effective than the other two methods as the informative genes become progressively more correlated, which is demonstrated both in terms of the classification performance and the ability to recover true informative genes. We also applied this hybrid scheme to four cancer prognosis datasets, in which k-TSP+SVM outperforms k-TSP classifier in all datasets, and achieves either comparable or superior performance to that using SVM alone. In concurrence with what is observed in simulation, TSP appears to be a better feature selector than Fisher and RFE in some of the cancer datasets CONCLUSIONS: The k-TSP ranking algorithm can be used as a computationally efficient, multivariate filter method for feature selection in machine learning. SVM in combination with k-TSP ranking algorithm outperforms k-TSP and SVM alone in simulated datasets and in some cancer prognosis datasets. Simulation studies suggest that as a feature selector, it is better tuned to certain data characteristics, i.e. correlations among informative genes, which is potentially interesting as an alternative feature ranking method in pathway analysis."
MARK A KON,Use of the geometric mean as a statistic for the scale of the coupled Gaussian distributions,"The geometric mean is shown to be an appropriate statistic for the scale of a heavy-tailed coupled Gaussian distribution or equivalently the Student’s t distribution. The coupled Gaussian is a member of a family of distributions parameterized by the nonlinear statistical coupling which is the reciprocal of the degree of freedom and is proportional to fluctuations in the inverse scale of the Gaussian. Existing estimators of the scale of the coupled Gaussian have relied on estimates of the full distribution, and they suffer from problems related to outliers in heavy-tailed distributions. In this paper, the scale of a coupled Gaussian is proven to be equal to the product of the generalized mean and the square root of the coupling. From our numerical computations of the scales of coupled Gaussians using the generalized mean of random samples, it is indicated that only samples from a Cauchy distribution (with coupling parameter one) form an unbiased estimate with diminishing variance for large samples. Nevertheless, we also prove that the scale is a function of the geometric mean, the coupling term and a harmonic number. Numerical experiments show that this estimator is unbiased with diminishing variance for large samples for a broad range of coupling values."
MARK A KON,Relationships among interpolation bases of wavelet spaces and approximation spaces,"A multiresolution analysis is a nested chain of related approximation spaces.This nesting in turn implies relationships among interpolation bases in the approximation spaces and their derived wavelet spaces. Using these relationships, a necessary and sufficient condition is given for existence of interpolation wavelets, via analysis of the corresponding scaling functions. It is also shown that any interpolation function for an approximation space plays the role of a special type of scaling function (an interpolation scaling function) when the corresponding family of approximation spaces forms a multiresolution analysis. Based on these interpolation scaling functions, a new algorithm is proposed for constructing corresponding interpolation wavelets (when they exist in a multiresolution analysis). In simulations, our theorems are tested for several typical wavelet spaces, demonstrating our theorems for existence of interpolation wavelets and for constructing them in a general multiresolution analysis."
MARK A KON,A complexity analysis of statistical learning algorithms,"We apply information-based complexity analysis to support vector machine (SVM) algorithms, with the goal of a comprehensive continuous algorithmic analysis of such algorithms. This involves complexity measures in which some higher order operations (e.g., certain optimizations) are considered primitive for the purposes of measuring complexity. We consider classes of information operators and algorithms made up of scaled families, and investigate the utility of scaling the complexities to minimize error. We look at the division of statistical learning into information and algorithmic components, at the complexities of each, and at applications to support vector machine (SVM) and more general machine learning algorithms. We give applications to SVM algorithms graded into linear and higher order components, and give an example in biomedical informatics."
MARK A KON,On some integrated approaches to inference,"We present arguments for the formulation of unified approach to different standard continuous inference methods from partial information. It is claimed that an explicit partition of information into a priori (prior knowledge) and a posteriori information (data) is an important way of standardizing inference approaches so that they can be compared on a normative scale, and so that notions of optimal algorithms become farther-reaching. The inference methods considered include neural network approaches, information-based complexity, and Monte Carlo, spline, and regularization methods. The model is an extension of currently used continuous complexity models, with a class of algorithms in the form of optimization methods, in which an optimization functional (involving the data) is minimized. This extends the family of current approaches in continuous complexity theory, which include the use of interpolatory algorithms in worst and average case settings."
MARK A KON,On the probabilistic continuous complexity conjecture,"In this paper we prove the probabilistic continuous complexity conjecture. In continuous complexity theory, this states that the complexity of solving a continuous problem with probability approaching 1 converges (in this limit) to the complexity of solving the same problem in its worst case. We prove the conjecture holds if and only if space of problem elements is uniformly convex. The non-uniformly convex case has a striking counterexample in the problem of identifying a Brownian path in Wiener space, where it is shown that probabilistic complexity converges to only half of the worst case complexity in this limit."
MARK A KON,Biomedical informatics for computer-aided decision support systems: a survey,"The volumes of current patient data as well as their complexity make clinical decision making more challenging than ever for physicians and other care givers. This situation calls for the use of biomedical informatics methods to process data and form recommendations and/or predictions to assist such decision makers. The design, implementation, and use of biomedical informatics systems in the form of computer-aided decision support have become essential and widely used over the last two decades. This paper provides a brief review of such systems, their application protocols and methodologies, and the future challenges and directions they suggest."
MARK A KON,Bioinformatics and biomedical informatics,
MARK A KON,On the average uncertainty for systems with nonlinear coupling,"The increased uncertainty and complexity of nonlinear systems have motivated investigators to consider generalized approaches to defining an entropy function. New insights are achieved by defining the average uncertainty in the probability domain as a transformation of entropy functions. The Shannon entropy when transformed to the probability domain is the weighted geometric mean of the probabilities. For the exponential and Gaussian distributions, we show that the weighted geometric mean of the distribution is equal to the density of the distribution at the location plus the scale (i.e. at the width of the distribution). The average uncertainty is generalized via the weighted generalized mean, in which the moment is a function of the nonlinear source. Both the Rényi and Tsallis entropies transform to this definition of the generalized average uncertainty in the probability domain. For the generalized Pareto and Student’s t-distributions, which are the maximum entropy distributions for these generalized entropies, the appropriate weighted generalized mean also equals the density of the distribution at the location plus scale. A coupled entropy function is proposed, which is equal to the normalized Tsallis entropy divided by one plus the coupling."
MARK A KON,Transcription factor-DNA binding via machine learning ensembles,"The network of interactions between transcription factors (TFs) and their regulatory gene targets governs many of the behaviors and responses of cells. Construction of a transcriptional regulatory network involves three interrelated problems, defined for any regulator: finding (1) its target genes, (2) its binding motif and (3) its DNA binding sites. Many tools have been developed in the last decade to solve these problems. However, performance of algorithms for these has not been consistent for all transcription factors. Because machine learning algorithms have shown advantages in integrating information of different types, we investigate a machine-based approach to integrating predictions from an ensemble of commonly used motif exploration algorithms."
JOHN W BYERS,Geometric Generalizations of the Power of Two Choices,"A well-known paradigm for load balancing in distributed systems is the``power of two choices,''whereby an item is stored at the less loaded of two (or more) random alternative servers. We investigate the power of two choices in natural settings for distributed computing where items and servers reside in a geometric space and each item is associated with the server that is its nearest neighbor. This is in fact the backdrop for distributed hash tables such as Chord, where the geometric space is determined by clockwise distance on a one-dimensional ring. Theoretically, we consider the following load balancing problem. Suppose that servers are initially hashed uniformly at random to points in the space. Sequentially, each item then considers d candidate insertion points also chosen uniformly at random from the space,and selects the insertion point whose associated server has the least load. For the one-dimensional ring, and for Euclidean distance on the two-dimensional torus, we demonstrate that when n data items are hashed to n servers,the maximum load at any server is log log n / log d + O(1) with high probability. While our results match the well-known bounds in the standard setting in which each server is selected equiprobably, our applications do not have this feature, since the sizes of the nearest-neighbor regions around servers are non-uniform. Therefore, the novelty in our methods lies in developing appropriate tail bounds on the distribution of nearest-neighbor region sizes and in adapting previous arguments to this more general setting. In addition, we provide simulation results demonstrating the load balance that results as the system size scales into the millions."
JOHN W BYERS,Implications of Selfish Neighbor Selection in Overlay Networks,"In a typical overlay network for routing or content sharing, each node must select a fixed number of immediate overlay neighbors for routing traffic or content queries. A selfish node entering such a network would select neighbors so as to minimize the weighted sum of expected access costs to all its destinations. Previous work on selfish neighbor selection has built intuition with simple models where edges are undirected, access costs are modeled by hop-counts, and nodes have potentially unbounded degrees. However, in practice, important constraints not captured by these models lead to richer games with substantively and fundamentally different outcomes. Our work models neighbor selection as a game involving directed links, constraints on the number of allowed neighbors, and costs reflecting both network latency and node preference. We express a node's ""best response"" wiring strategy as a k-median problem on asymmetric distance, and use this formulation to obtain pure Nash equilibria. We experimentally examine the properties of such stable wirings on synthetic topologies, as well as on real topologies and maps constructed from PlanetLab and AS-level Internet measurements. Our results indicate that selfish nodes can reap substantial performance benefits when connecting to overlay networks composed of non-selfish nodes. On the other hand, in overlays that are dominated by selfish nodes, the resulting stable wirings are optimized to such great extent that even non-selfish newcomers can extract near-optimal performance through naive wiring strategies."
JOHN W BYERS,Robust Sketching and Aggregation of Distributed Data Streams,"The data streaming model provides an attractive framework for one-pass summarization of massive data sets at a single observation point. However, in an environment where multiple data streams arrive at a set of distributed observation points, sketches must be computed remotely and then must be aggregated through a hierarchy before queries may be conducted. As a result, many sketch-based methods for the single stream case do not apply directly, as either the error introduced becomes large, or because the methods assume that the streams are non-overlapping. These limitations hinder the application of these techniques to practical problems in network traffic monitoring and aggregation in sensor networks. To address this, we develop a general framework for evaluating and enabling robust computation of duplicate-sensitive aggregate functions (e.g., SUM and QUANTILE), over data produced by distributed sources. We instantiate our approach by augmenting the Count-Min and Quantile-Digest sketches to apply in this distributed setting, and analyze their performance. We conclude with experimental evaluation to validate our analysis."
JOHN W BYERS,EGOIST: Overlay Routing Using Selfish Neighbor Selection,"A foundational issue underlying many overlay network applications ranging from routing to P2P file sharing is that of connectivity management, i.e., folding new arrivals into an existing overlay, and re-wiring to cope with changing network conditions. Previous work has considered the problem from two perspectives: devising practical heuristics for specific applications designed to work well in real deployments, and providing abstractions for the underlying problem that are analytically tractable, especially via game-theoretic analysis. In this paper, we unify these two thrusts by using insights gleaned from novel, realistic theoretic models in the design of Egoist – a prototype overlay routing system that we implemented, deployed, and evaluated on PlanetLab. Using measurements on PlanetLab and trace-based simulations, we demonstrate that Egoist's neighbor selection primitives significantly outperform existing heuristics on a variety of performance metrics, including delay, available bandwidth, and node utilization. Moreover, we demonstrate that Egoist is competitive with an optimal, but unscalable full-mesh approach, remains highly effective under significant churn, is robust to cheating, and incurs minimal overhead. Finally, we discuss some of the potential benefits Egoist may offer to applications."
JOHN W BYERS,On the Geographic Location of Internet Resources,"One relatively unexplored question about the Internet's physical structure concerns the geographical location of its components: routers, links and autonomous systems (ASes). We study this question using two large inventories of Internet routers and links, collected by different methods and about two years apart. We first map each router to its geographical location using two different state-of-the-art tools. We then study the relationship between router location and population density; between geographic distance and link density; and between the size and geographic extent of ASes. Our findings are consistent across the two datasets and both mapping methods. First, as expected, router density per person varies widely over different economic regions; however, in economically homogeneous regions, router density shows a strong superlinear relationship to population density. Second, the probability that two routers are directly connected is strongly dependent on distance; our data is consistent with a model in which a majority (up to 75-95%) of link formation is based on geographical distance (as in the Waxman topology generation method). Finally, we find that ASes show high variability in geographic size, which is correlated with other measures of AS size (degree and number of interfaces). Among small to medium ASes, ASes show wide variability in their geographic dispersal; however, all ASes exceeding a certain threshold in size are maximally dispersed geographically. These findings have many implications for the next generation of topology generators, which we envisage as producing router-level graphs annotated with attributes such as link latencies, AS identifiers and geographical locations."
JOHN W BYERS,Smooth Multirate Multicast Congestion Control,"A significant impediment to deployment of multicast services is the daunting technical complexity of developing, testing and validating congestion control protocols ﬁt for wide-area deployment. Protocols such as pgmcc and TFMCC have recently made considerable progress on the single rate case, i.e. where one dynamic reception rate is maintained for all receivers in the session. However, these protocols have limited applicability, since scaling to session sizes beyond tens of participants necessitates the use of multiple rate protocols. Unfortunately, while existing multiple rate protocols exhibit better scalability, they are both less mature than single rate protocols and suffer from high complexity. We propose a new approach to multiple rate congestion control that leverages proven single rate congestion control methods by orchestrating an ensemble of independently controlled single rate sessions. We describe SMCC, a new multiple rate equation-based congestion control algorithm for layered multicast sessions that employs TFMCC as the primary underlying control mechanism for each layer. SMCC combines the benefits of TFMCC (smooth rate control, equation-based TCP friendliness) with the scalability and flexibility of multiple rates to provide a sound multiple rate multicast congestion control policy."
JOHN W BYERS,Generating Representative ISP Technologies From First-Principles,"Understanding and modeling the factors that underlie the growth and evolution of network topologies are basic questions that impact capacity planning, forecasting, and protocol research. Early topology generation work focused on generating network-wide connectivity maps, either at the AS-level or the router-level, typically with an eye towards reproducing abstract properties of observed topologies. But recently, advocates of an alternative ""first-principles"" approach question the feasibility of realizing representative topologies with simple generative models that do not explicitly incorporate real-world constraints, such as the relative costs of router configurations, into the model. Our work synthesizes these two lines by designing a topology generation mechanism that incorporates first-principles constraints. Our goal is more modest than that of constructing an Internet-wide topology: we aim to generate representative topologies for single ISPs. However, our methods also go well beyond previous work, as we annotate these topologies with representative capacity and latency information. Taking only demand for network services over a given region as input, we propose a natural cost model for building and interconnecting PoPs and formulate the resulting optimization problem faced by an ISP. We devise hill-climbing heuristics for this problem and demonstrate that the solutions we obtain are quantitatively similar to those in measured router-level ISP topologies, with respect to both topological properties and fault-tolerance."
JOHN W BYERS,Adaptive Weighing Designs for Keyword Value Computation,"Attributing a dollar value to a keyword is an essential part of running any profitable search engine advertising campaign. When an advertiser has complete control over the interaction with and monetization of each user arriving on a given keyword, the value of that term can be accurately tracked. However, in many instances, the advertiser may monetize arrivals indirectly through one or more third parties. In such cases, it is typical for the third party to provide only coarse-grained reporting: rather than report each monetization event, users are aggregated into larger channels and the third party reports aggregate information such as total daily revenue for each channel. Examples of third parties that use channels include Amazon and Google AdSense. In such scenarios, the number of channels is generally much smaller than the number of keywords whose value per click (VPC) we wish to learn. However, the advertiser has flexibility as to how to assign keywords to channels over time. We introduce the channelization problem: how do we adaptively assign keywords to channels over the course of multiple days to quickly obtain accurate VPC estimates of all keywords? We relate this problem to classical results in weighing design, devise new adaptive algorithms for this problem, and quantify the performance of these algorithms experimentally. Our results demonstrate that adaptive weighing designs that exploit statistics of term frequency, variability in VPCs across keywords, and flexible channel assignments over time provide the best estimators of keyword VPCs."
JOHN W BYERS,Sampling Biases in IP Topology Measurements,"Considerable attention has been focused on the properties of graphs derived from Internet measurements. Router-level topologies collected via traceroute studies have led some authors to conclude that the router graph of the Internet is a scale-free graph, or more generally a power-law random graph. In such a graph, the degree distribution of nodes follows a distribution with a power-law tail. In this paper we argue that the evidence to date for this conclusion is at best insufficient. We show that graphs appearing to have power-law degree distributions can arise surprisingly easily, when sampling graphs whose true degree distribution is not at all like a power-law. For example, given a classical Erdös-Rényi sparse, random graph, the subgraph formed by a collection of shortest paths from a small set of random sources to a larger set of random destinations can easily appear to show a degree distribution remarkably like a power-law. We explore the reasons for how this effect arises, and show that in such a setting, edges are sampled in a highly biased manner. This insight allows us to distinguish measurements taken from the Erdös-Rényi graphs from those taken from power-law random graphs. When we apply this distinction to a number of well-known datasets, we find that the evidence for sampling bias in these datasets is strong."
JOHN W BYERS,The EGOIST Overlay Routing System,"A foundational issue underlying many overlay network applications ranging from routing to peer-to-peer file sharing is that of connectivity management, i.e., folding new arrivals into an existing overlay, and rewiring to cope with changing network conditions. Previous work has considered the problem from two perspectives: devising practical heuristics for specific applications designed to work well in real deployments, and providing abstractions for the underlying problem that are analytically tractable, especially via game-theoretic analysis. In this paper, we unify these two thrusts by using insights gleaned from novel, realistic theoretic models in the design of Egoist – a distributed overlay routing system that we implemented, deployed, and evaluated on PlanetLab. Using extensive measurements of paths between nodes, we demonstrate that Egoist’s neighbor selection primitives significantly outperform existing heuristics on a variety of performance metrics, including delay, available bandwidth, and node utilization. Moreover, we demonstrate that Egoist is competitive with an optimal, but unscalable full-mesh approach, remains highly effective under significant churn, is robust to cheating, and incurs minimal overhead. Finally, we use a multiplayer peer-to-peer game to demonstrate the value of Egoist to end-user applications. This technical report supersedes BUCS-TR-2007-013."
JOHN W BYERS,ROMA: Reliable Overlay Multicast with Loosely Coupled TCP Connections,"We consider the problem of architecting a reliable content delivery system across an overlay network using TCP connections as the transport primitive. We first argue that natural designs based on store-and-forward principles that tightly couple TCP connections at intermediate end-systems impose fundamental performance limitations, such as dragging down all transfer rates in the system to the rate of the slowest receiver. In contrast, the ROMA architecture we propose incorporates the use of loosely coupled TCP connections together with fast forward error correction techniques to deliver a scalable solution that better accommodates a set of heterogeneous receivers. The methods we develop establish chains of TCP connections, whose expected performance we analyze through equation-based methods. We validate our analytical findings and evaluate the performance of our ROMA architecture using a prototype implementation via extensive Internet experimentation across the PlanetLab distributed testbed."
JOHN W BYERS,Approximately Uniform Random Sampling in Sensor Networks,"Recent work in sensor databases has focused extensively on distributed query problems, notably distributed computation of aggregates. Existing methods for computing aggregates broadcast queries to all sensors and use in-network aggregation of responses to minimize messaging costs. In this work, we focus on uniform random sampling across nodes, which can serve both as an alternative building block for aggregation and as an integral component of many other useful randomized algorithms. Prior to our work, the best existing proposals for uniform random sampling of sensors involve contacting all nodes in the network. We propose a practical method which is only approximately uniform, but contacts a number of sensors proportional to the diameter of the network instead of its size. The approximation achieved is tunably close to exact uniform sampling, and only relies on well-known existing primitives, namely geographic routing, distributed computation of Voronoi regions and von Neumann's rejection method. Ultimately, our sampling algorithm has the same worst-case asymptotic cost as routing a point-to-point message, and thus it is asymptotically optimal among request/reply-based sampling methods. We provide experimental results demonstrating the effectiveness of our algorithm on both synthetic and real sensor topologies."
JOHN W BYERS,Liquid data networking,
JOHN W BYERS,A digital fountain retrospective,"We introduced the concept of a digital fountain as a scalable approach to reliable multicast, realized with fast and practical erasure codes, in a paper published in ACM SIGCOMM '98. This invited editorial, on the occasion of the 50th anniversary of the SIG, reflects on the trajectory of work leading up to our approach, and the numerous developments in the field in the subsequent 21 years. We discuss advances in rateless codes, efficient implementations, applications of digital fountains in distributed storage systems, and connections to invertible Bloom lookup tables."
JOHN W BYERS,CASPR: judiciously using the cloud for wide-area packet recovery,"We revisit a classic networking problem -- how to recover from lost packets in the best-effort Internet. We propose CASPR, a system that judiciously leverages the cloud to recover from lost or delayed packets. CASPR supplements and protects best-effort connections by sending a small number of coded packets along the highly reliable but expensive cloud paths. When receivers detect packet loss, they recover packets with the help of the nearby data center, not the sender, thus providing quick and reliable packet recovery for latency-sensitive applications. Using a prototype implementation and its deployment on the public cloud and the PlanetLab testbed, we quantify the benefits of CASPR in providing fast, cost effective packet recovery. Using controlled experiments, we also explore how these benefits translate into improvements up and down the network stack."
JOHN W BYERS,Judicious QoS using cloud overlays,"We revisit the long-standing problem of providing network QoS to applications, and propose the concept of judicious QoS -- combining the cheaper, best effort IP service with the cloud, which offers a highly reliable infrastructure and the ability to add in-network services, albeit at higher cost. Our proposed J-QoS framework offers a range of reliability services with different cost vs. delay trade-offs, including: i) a forwarding service that forwards packets over the cloud overlay, ii) a caching service, which stores packets inside the cloud and allows them to be pulled in case of packet loss or disruption on the Internet, and iii) a novel coding service that provides the least expensive packet recovery option by combining packets of multiple application streams and sending a small number of coded packets across the more expensive cloud paths. We demonstrate the feasibility of these services using measurements from RIPE Atlas and a live deployment on PlanetLab. We also consider case studies on how J-QoS works with services up and down the network stack, including Skype video conferencing, TCP-based web transfers, and cellular access networks."
JOHN W BYERS,TwitterMancer: predicting user interactions on Twitter,"This paper investigates the interplay between different types of user interactions on Twitter, with respect to predicting missing or unseen interactions. For example, given a set of retweet interactions between Twitter users, how accurately can we predict reply interactions? Is it more difficult to predict retweet or quote interactions between a pair of accounts? Also, how important is time locality, and which features of interaction patterns are most important to enable accurate prediction of specific Twitter interactions? Our empirical study of Twitter interactions contributes initial answers to these questions.We have crawled an extensive data set of Greek-speaking Twitter accounts and their follow, quote, retweet, reply interactions over a period of a month. We find we can accurately predict many interactions of Twitter users. Interestingly, the most predictive features vary with the user profiles, and are not the same across all users. For example, for a pair of users that interact with a large number of other Twitter users, we find that certain “higher-dimensional” triads, i.e., triads that involve multiple types of interactions, are very informative, whereas for less active Twitter users, certain in-degrees and out-degrees play a major role. Finally, we provide various other insights on Twitter user behavior. Our code and data are available at https://github.com/twittermancer."
JOHN W BYERS,The rise of the sharing economy: estimating the impact of Airbnb on the hotel industry,"Peer-to-peer markets, collectively known as the sharing economy, have emerged as alternative suppliers of goods and services traditionally provided by long-established industries. We explore the economic impact of the sharing economy on incumbent firms by studying the case of Airbnb, a prominent platform for short-term accommodations. We analyze Airbnb's entry into the state of Texas, and quantify its impact on the Texas hotel industry over the subsequent decade. We estimate that in Austin, where Airbnb supply is highest, the causal impact on hotel revenue is in the 8-10% range; moreover, the impact is non-uniform, with lower-priced hotels and those hotels not catering to business travelers being the most affected. The impact manifests itself primarily through less aggressive hotel room pricing, an impact that benefits all consumers, not just participants in the sharing economy. The price response is especially pronounced during periods of peak demand, such as SXSW, and is due to a differentiating feature of peer-to-peer platforms -- enabling instantaneous supply to scale to meet demand."
JOHN W BYERS,Proceedings of the Sixth International Workshop on Web Caching and Content Distribution,"OVERVIEW: The International Web Content Caching and Distribution Workshop (WCW) is a premiere technical meeting for researchers and practitioners interested in all aspects of content caching, distribution and delivery on the Internet. The 2001 WCW meeting was held on the Boston University Campus. Building on the successes of the five previous WCW meetings, WCW01 featured a strong technical program and record participation from leading researchers and practitioners in the field. This report includes all the technical papers presented at WCW'01. NOTE: Proceedings of WCW'01 are published by Elsevier. Hard copies of these proceedings can be purchased through the workshop organizers. As a service to the community, electronic copies of all WCW'01 papers are accessible through Technical Report BUCS‐TR‐2001‐017, available from the Boston University Computer Science Technical Report Archives at http://www.cs.bu.edu/techreps. [Ed.note: URL outdated. Use http://www.bu.edu/cs/research/technical-reports or http://hdl.handle.net/2144/1455 in this repository to access the reports.]"
JOHN W BYERS,Learn to earn: enabling coordination within a ride-hailing fleet,
HARDIN L K COLEMAN,Listening and learning about civic education from the community,"From the United States to Myanmar, we see significant evidence that there are important and sometimes conflictive conversations about how we should be a civic community. Building on Coleman’s [1] argument that effective civic, character, or social-emotional education programs need to be embedded within a community’s values, this paper will give an example as to how community based listening tours can be used to facilitate this process. It will summarize how these listening tours gave voice to community beliefs about the why and the how of effective civic education. These listening tours found that effective civics education programs should a) elevate student voice, b) engage parents in the program development and implementation, c) engage community-based youth serving agencies in program development and implementation, d) include communities that have a prior interest or exposure to equity in K-12 civic education, e) demonstrate a commitment to centering the lived experience of students in the programming, f) have a school district-level commitment to civic education (staff, resources, stated mission, etc.), and g) have state-level commitment to civic education (education department policy, state standards and curricular frameworks, relevant civics legislation, etc) [2].One core takeaway from these listening tours are that successful educational programming is no longer a function of that great teacher, principal, or program. It is a function of the system working together to create opportunities for our children to thrive. Another is that a lived civics education program can serve to bring communities together around student learning and agency."
HARDIN L K COLEMAN,Developmental guidance and student acquisition of social competence.,"In a changing world, it is increasingly important to articulate what are the social emotional competencies that students leaving secondary schools need to acquire in order to be effective learners and citizens and how schools can facilitate the acquisition of such competencies. There is an emerging consensus that CASEL [1] has identified five of those core competencies. They are a) self-awareness, b) self-management, c) social awareness, d) relationship skills and e) responsible decision-making. There is not, however, a consensus as to how schools can facilitate the acquisition of these competencies. This paper will argue that each community needs to articulate the competencies they expect from their children and ensure that their schools implement a program of developmental guidance that helps them acquire these competencies. Developmental guidance is a combination of curriculum that teaches these competencies, experiences through which students can put them into practice, a systematic approach to developing and implementing a post-secondary plan for each student, and a way to assess the success of such an approach. In the same way that competence in literacy and numeracy is developed over a child’s career in school through a series of increasingly complex coursework, we need to implement systematic developmental guidance in all schools so that we more effectively prepare our children to take their place in a world that is changing as a result of technological and social developments. There are several barriers to implementing effective developmental guidance programs. One is the lack of consensus as to the role of schools in providing such training. Another is the lack of consensus as to what are those desired competencies. A third is the lack of resources made available to support such implementation."
HARDIN L K COLEMAN,"Caring, character, and community: leadership in times of crisis, lessons learned","To prepare the next generation to become caring and effectively engaged citizens requires a multi-faceted approach. It includes a focus on personal development, professional development, program development and implementation, and systems change [1]. To deepen our understanding of how leadership integrates these values into practice, this paper presents a thematic analysis of how leaders of PK-12 schools, higher education institutions, and leaders of community-based organizations integrate an ethic of caring, a focus on their own and others’ character development, and a commitment to serving the needs of others in order to guide and inform their leadership in times of crisis. We interviewed thirteen leaders who responded to the question, “How have you integrated caring, character, and commitment into your leadership style while managing crises?” Four of the leaders work with PK-12 schools, four work in higher education, and five work in youth-serving community based organizations. Some of the leaders have explicit commitments to a focus on character education and/or have a spiritual grounding in their work. Others were more focused on social justice, equity, and system change. All were deeply committed to creating conditions in which youth can flourish. This paper will summarize their thinking about how to use an ethic of caring, a focus on one’s own and others’ character development, and a commitment to community to create high-quality learning experiences and opportunities for all youth."
HARDIN L K COLEMAN,"Character, civic, and social emotional learning education","Increasingly we are hearing calls to move beyond test scores and to enhance our focus on the education of the whole child. OECD [1] has laid out a framework for what type of education we want to have by 2030 in which they centered their vision of whole child development. Research in child human development and the neuroscience of learning also suggest that we need to expand our focus from academics outcomes (e.g., math and language knowledge) to pro-social outcomes such as personal agency, persistence, and civic engagement [2]). When, however, you look at the structures of schools, there is no clarity as to who owns the policies, procedures, or curriculum associated with educating the whole child. There is confusion as to what we mean by whole child and how we measure of our success in educating the whole child. The purpose of this essay is to describe three areas of practice that effectively address the development of the whole child, a) character education, b) civic education, and c) social emotional learning and discuss the ways in which they are different and similar. It will end with a discussion on the challenges faced in implementing such programs and demonstrating their value."
HARDIN L K COLEMAN,Educator diversity and student accomplishment,"There are many factors that contribute to a student’s academic and personal performance within the context of their PK-12 Education. Independent of atypical developmental abilities, family wealth is a significant factor in student performance. Other factors, endogenous to what happens within school, include community resources, with a particular focus on sustained financial support for schools. Although the overall performance of a particular school is driven by the relative wealth of the students in the school, the level of language proficiency among students within the school, and the percentage of special needs students in the classroom and building, the variation of performance between schools with similar populations is driven by the effectiveness of the educators in the building (Opper, 2019). If we are going to close the various achievement gaps that plague our educational systems, a focus on how to prepare, recruit, and retain highly effective educators, particularly within our high needs and culturally diverse schools, is a national imperative."
HARDIN L K COLEMAN,Research practice partnerships and school improvement,"As the recent PISA scores have indicated [1], student performance around the world has hit an asymptote. Some countries’ performance are going down and some countries failing to make any progress, but overall, there is not significant improvement. After 30 years of education reform in the United States, this pattern repeats itself in urban, suburban, and rural settings. Coleman (2018) made the argument that in order to improve outcomes for all of our children, we need to take a more systemic approach to school improvement that includes cooperation among the adult community across the various ecologies in which a child grows. The purpose of this essay is to is to build on the work of the scholars in the field of research practice partnerships [2,3,4] to advocate for the use of cooperative approach between scholars and practitioners as an important factor in developing cultures of continual improvement within school, produce scholarship that improves the practice of education, and will lead to sustainable growth in student performance."
HARDIN L K COLEMAN,Networked improvement community for equity,"PK-12 education is at a fascinating moment in our work to improve the overall performance of school districts that have high percentages of children who are living in poverty, come from historically disadvantaged backgrounds, do not have English as their native language and/or are atypical learners. On the one hand, we have growing evidence of strategies that serve to improve the academic performance of all students in economically, culturally, and linguistically diverse school districts [1]. On the other, there is significant variance within and across these districts as to how effectively these strategies are implemented [2]. This suggests that we know what to do in order to improve the ability of schools to best serve all children, but we face continued challenges in effectively scaling effective strategies across a district to achieve desired equity outcomes within schools that are sustained over time. The focus of this paper is to describe a planned research-practice partnership [3] that is designed to use a networked improvement community of school leaders and regional superintendents to a) build the capacity of a district to develop and implement a policy that is designed to improve equitable access to high quality learning opportunities and b) indicate ways in the implementation of this policy can serve as a driver of closing achievement and opportunity gaps for the district’s most marginalized students, by identifying and codifying measures proximal measures that capture school-based needs and practices. The first goal of this project is to increase equitable access to high quality learning for all children, the second is to demonstrate the efficacy of network improvement science technigues to implement sustainable policies with fidelity."
ELIZABETH N PEARCE,A Genome-Wide Association for Kidney Function and Endocrine-Related Traits in the NHLBI's Framingham Heart Study,"BACKGROUND: Glomerular filtration rate (GFR) and urinary albumin excretion (UAE) are markers of kidney function that are known to be heritable. Many endocrine conditions have strong familial components. We tested for association between the Affymetrix GeneChip Human Mapping 100K single nucleotide polymorphism (SNP) set and measures of kidney function and endocrine traits. METHODS: Genotype information on the Affymetrix GeneChip Human Mapping 100K SNP set was available on 1345 participants. Serum creatinine and cystatin-C (cysC; n = 981) were measured at the seventh examination cycle (1998–2001); GFR (n = 1010) was estimated via the Modification of Diet in Renal Disease (MDRD) equation; UAE was measured on spot urine samples during the sixth examination cycle (1995–1998) and was indexed to urinary creatinine (n = 822). Thyroid stimulating hormone (TSH) was measured at the third and fourth examination cycles (1981–1984; 1984–1987) and mean value of the measurements were used (n = 810). Age-sex-adjusted and multivariable-adjusted residuals for these measurements were used in association with genotype data using generalized estimating equations (GEE) and family-based association tests (FBAT) models. We presented the results for association tests using additive allele model. We evaluated associations with 70,987 SNPs on autosomes with minor allele frequencies of at least 0.10, Hardy-Weinberg Equilibrium p-value ≥ 0.001, and call rates of at least 80%. RESULTS: The top SNPs associated with these traits using the GEE method were rs2839235 with GFR (p-value 1.6*10-05), rs1158167 with cysC (p-value 8.5*10-09), rs1712790 with UAE (p-value 1.9*10-06), and rs6977660 with TSH (p-value 3.7*10-06), respectively. The top SNPs associated with these traits using the FBAT method were rs6434804 with GFR(p-value 2.4*10-5), rs563754 with cysC (p-value 4.7*10-5), rs1243400 with UAE (p-value 4.8*10-6), and rs4128956 with TSH (p-value 3.6*10-5), respectively. Detailed association test results can be found at . Four SNPs in or near the CST3 gene were highly associated with cysC levels (p-value 8.5*10-09 to 0.007). CONCLUSION: Kidney function traits and TSH are associated with SNPs on the Affymetrix GeneChip Human Mapping 100K SNP set. These data will serve as a valuable resource for replication as more SNPs associated with kidney function and endocrine traits are identified."
ELIZABETH N PEARCE,Associations between Maternal Thyroid Function in Pregnancy and Obstetric and Perinatal Outcomes - Supplemental Table 1,
CALIN A BELTA,Modeling genetic circuit behavior in transiently transfected mammalian cells,"Binning cells by plasmid copy number is a common practice for analyzing transient transfection data. In many kinetic models of transfected cells, protein production rates are assumed to be proportional to plasmid copy number. The validity of this assumption in transiently transfected mammalian cells is not clear; models based on this assumption appear unable to reproduce experimental flow cytometry data robustly. We hypothesize that protein saturation at high plasmid copy number is a reason previous models break down and validate our hypothesis by comparing experimental data and a stochastic chemical kinetics model. The model demonstrates that there are multiple distinct physical mechanisms that can cause saturation. On the basis of these observations, we develop a novel minimal bin-dependent ODE model that assumes different parameters for protein production in cells with low versus high numbers of plasmids. Compared to a traditional Hill-function-based model, the bin-dependent model requires only one additional parameter, but fits flow cytometry input-output data for individual modules up to twice as accurately. By composing together models of individually fit modules, we use the bin-dependent model to predict the behavior of six cascades and three feed-forward circuits. The bin-dependent models are shown to provide more accurate predictions on average than corresponding (composed) Hill-function-based models and predictions of comparable accuracy to EQuIP, while still providing a minimal ODE-based model that should be easy to integrate as a subcomponent within larger differential equation circuit models. Our analysis also demonstrates that accounting for batch effects is important in developing accurate composed models."
CALIN A BELTA,Controlling the outcome of the Toll-like receptor signaling pathways,"The Toll-Like Receptors (TLRs) are proteins involved in the immune system that increase cytokine levels when triggered. While cytokines coordinate the response to infection, they appear to be detrimental to the host when reaching too high levels. Several studies have shown that the deletion of specific TLRs was beneficial for the host, as cytokine levels were decreased consequently. It is not clear, however, how targeting other components of the TLR pathways can improve the responses to infections. We applied the concept of Minimal Cut Sets (MCS) to the ihsTLR v1.0 model of the TLR pathways to determine sets of reactions whose knockouts disrupt these pathways. We decomposed the TLR network into 34 modules and determined signatures for each MCS, i.e. the list of targeted modules. We uncovered 2,669 MCS organized in 68 signatures. Very few MCS targeted directly the TLRs, indicating that they may not be efficient targets for controlling these pathways. We mapped the species of the TLR network to genes in human and mouse, and determined more than 10,000 Essential Gene Sets (EGS). Each EGS provides genes whose deletion suppresses the network's outputs."
CALIN A BELTA,Metrics for signal temporal logic formulae,"Signal Temporal Logic (STL) is a formal language for describing a broad range of real-valued, temporal properties in cyber-physical systems. While there has been extensive research on verification and control synthesis from STL requirements, there is no formal framework for comparing two STL formulae. In this paper, we show that under mild assumptions, STL formulae admit a metric space. We propose two metrics over this space based on i) the Pompeiu-Hausdorff distance and ii) the symmetric difference measure, and present algorithms to compute them. Alongside illustrative examples, we present applications of these metrics for two fundamental problems: a) design quality measures: to compare all the temporal behaviors of a designed system, such as a synthetic genetic circuit, with the “desired” specification, and b) loss functions: to quantify errors in Temporal Logic Inference (TLI) as a first step to establish formal performance guarantees of TLI algorithms."
CALIN A BELTA,How retroactivity affects the behavior of incoherent feedforward loops,"An incoherent feedforward loop (IFFL) is a network motif known for its ability to accelerate responses and generate pulses. It remains an open question to understand the behavior of IFFLs in contexts with high levels of retroactivity, where an upstream transcription factor binds to numerous downstream binding sites. Here we study the behavior of IFFLs by simulating and comparing ODE models with different levels of retroactivity. We find that increasing retroactivity in an IFFL can increase, decrease, or keep the network's response time and pulse amplitude constant. This suggests that increasing retroactivity, traditionally considered an impediment to designing robust synthetic systems, could be exploited to improve the performance of IFFLs. In contrast, we find that increasing retroactivity in a negative autoregulated circuit can only slow the response. The ability of an IFFL to flexibly handle retroactivity may have contributed to its significant abundance in both bacterial and eukaryotic regulatory networks."
KATHLEEN G. MORGAN,Vascular smooth muscle Sirtuin-1 protects against aortic dissection during Angiotensin II-induced hypertension,"BACKGROUND: Sirtuin-1 (SirT1), a nicotinamide adenine dinucleotide(+)-dependent deacetylase, is a key enzyme in the cellular response to metabolic, inflammatory, and oxidative stresses; however, the role of endogenous SirT1 in the vasculature has not been fully elucidated. Our goal was to evaluate the role of vascular smooth muscle SirT1 in the physiological response of the aortic wall to angiotensin II, a potent hypertrophic, oxidant, and inflammatory stimulus. METHODS AND RESULTS: Mice lacking SirT1 in vascular smooth muscle (ie, smooth muscle SirT1 knockout) had drastically high mortality (70%) caused by aortic dissection after angiotensin II infusion (1 mg/kg per day) but not after an equipotent dose of norepinephrine, despite comparable blood pressure increases. Smooth muscle SirT1 knockout mice did not show any abnormal aortic morphology or blood pressure compared with wild-type littermates. Nonetheless, in response to angiotensin II, aortas from smooth muscle SirT1 knockout mice had severely disorganized elastic lamellae with frequent elastin breaks, increased oxidant production, and aortic stiffness compared with angiotensin II-treated wild-type mice. Matrix metalloproteinase expression and activity were increased in the aortas of angiotensin II-treated smooth muscle SirT1 knockout mice and were prevented in mice overexpressing SirT1 in vascular smooth muscle or with use of the oxidant scavenger tempol. CONCLUSIONS: Endogenous SirT1 in aortic smooth muscle is required to maintain the structural integrity of the aortic wall in response to oxidant and inflammatory stimuli, at least in part, by suppressing oxidant-induced matrix metalloproteinase activity. SirT1 activators could potentially be a novel therapeutic approach to prevent aortic dissection and rupture in patients at risk, such as those with hypertension or genetic disorders, such as Marfan's syndrome."
KATHLEEN G. MORGAN,Myometrial mechanoadaptation during pregnancy: implications for smooth muscle plasticity and remodelling,"The smooth muscle of the uterus during pregnancy presents a unique circumstance of physiological mechanotransduction as the tissue remodels in response to stretches imposed by the growing foetus(es), yet the nature of the molecular and functional adaptations remain unresolved. We studied, in myometrium isolated from non‐pregnant (NP) and pregnant mice, the active and passive length–tension curves by myography and the expression and activation by immunoblotting of focal adhesion‐related proteins known in other systems to participate in mechanosensing and mechanotransduction. In situ uterine mass correlated with pup number and weight throughout pregnancy. In vitro myometrial active, and passive, length‐tension curves shifted significantly to the right during pregnancy indicative of altered mechanosensitivity; at term, maximum active tension was generated following 3.94 ± 0.33‐fold stretch beyond slack length compared to 1.91 ± 0.12‐fold for NP mice. Moreover, mechanotransduction was altered during pregnancy as evidenced by the progressive increase in absolute force production at each optimal stretch. Pregnancy was concomitantly associated with an increased expression of the dense plaque‐associated proteins FAK and paxillin, and elevated activation of FAK, paxillin, c‐Src and extracellular signal‐regulated kinase (ERK1/2) which reversed 1 day post‐partum. Electron microscopy revealed close appositioning of neighbouring myometrial cells across a narrow extracellular cleft adjoining plasmalemmal dense plaques. Collectively, these results suggest a physiological basis of myometrial length adaptation, long known to be a property of many smooth muscles, whereupon plasmalemmal dense plaque proteins serve as molecular signalling and structural platforms contributing to functional (contractile) remodelling in response to chronic stretch."
KATHLEEN G. MORGAN,"Vascular aging, the vascular cytoskeleton and aortic stiffness","Vascular aging, aortic stiffness and hypertension are mechanistically interrelated. The perspective presented here will focus mainly on the molecular mechanisms of age-associated increases in the stiffness of the vascular smooth muscle cell (VSMC). This review will highlight the mechanisms by which the VSMC contributes to disorders of vascular aging. Distinct functional sub-components of the vascular cell and the molecular mechanisms of the protein-protein interactions, signaling mechanisms and intracellular trafficking processes in the setting of the aging aorta will be detailed."
KATHLEEN G. MORGAN,Reversal of aging-induced increases in aortic stiffness by targeting cytoskeletal protein-protein interfaces,"BACKGROUND: The proximal aorta normally functions as a critical shock absorber that protects small downstream vessels from damage by pressure and flow pulsatility generated by the heart during systole. This shock absorber function is impaired with age because of aortic stiffening. METHODS AND RESULTS: We examined the contribution of common genetic variation to aortic stiffness in humans by interrogating results from the AortaGen Consortium genome‐wide association study of carotid‐femoral pulse wave velocity. Common genetic variation in the N‐WASP (WASL) locus is associated with carotid‐femoral pulse wave velocity (rs600420, P=0.0051). Thus, we tested the hypothesis that decoy proteins designed to disrupt the interaction of cytoskeletal proteins such as N‐WASP with its binding partners in the vascular smooth muscle cytoskeleton could decrease ex vivo stiffness of aortas from a mouse model of aging. A synthetic decoy peptide construct of N‐WASP significantly reduced activated stiffness in ex vivo aortas of aged mice. Two other cytoskeletal constructs targeted to VASP and talin‐vinculin interfaces similarly decreased aging‐induced ex vivo active stiffness by on‐target specific actions. Furthermore, packaging these decoy peptides into microbubbles enables the peptides to be ultrasound‐targeted to the wall of the proximal aorta to attenuate ex vivo active stiffness. CONCLUSIONS: We conclude that decoy peptides targeted to vascular smooth muscle cytoskeletal protein‐protein interfaces and microbubble packaged can decrease aortic stiffness ex vivo. Our results provide proof of concept at the ex vivo level that decoy peptides targeted to cytoskeletal protein‐protein interfaces may lead to substantive dynamic modulation of aortic stiffness."
KATHLEEN G. MORGAN,"Stretch Activates Human Myometrium via ERK, Caldesmon and Focal Adhesion Signaling","An incomplete understanding of the molecular mechanisms responsible for myometrial activation from the quiescent pregnant state to the active contractile state during labor has hindered the development of effective therapies for preterm labor. Myometrial stretch has been implicated clinically in the initiation of labor and the etiology of preterm labor, but the molecular mechanisms involved in the human have not been determined. We investigated the mechanisms by which gestation-dependent stretch contributes to myometrial activation, by using human uterine samples from gynecologic hysterectomies and Cesarean sections. Here we demonstrate that the Ca requirement for activation of the contractile filaments in human myometrium increases with caldesmon protein content during gestation and that an increase in caldesmon phosphorylation can reverse this inhibitory effect during labor. By using phosphotyrosine screening and mass spectrometry of stretched human myometrial samples, we identify 3 stretch-activated focal adhesion proteins, FAK, p130Cas, and alpha actinin. FAK-Y397, which signals integrin engagement, is constitutively phosphorylated in term human myometrium whereas FAK-Y925, which signals downstream ERK activation, is phosphorylated during stretch. We have recently identified smooth muscle Archvillin (SmAV) as an ERK regulator. A newly produced SmAV-specific antibody demonstrates gestation-specific increases in SmAV protein levels and stretch-specific increases in SmAV association with focal adhesion proteins. Thus, whereas increases in caldesmon levels suppress human myometrium contractility during pregnancy, stretch-dependent focal adhesion signaling, facilitated by the ERK activator SmAV, can contribute to myometrial activation. These results suggest that focal adhesion proteins may present new targets for drug discovery programs aimed at regulation of uterine contractility."
KATHLEEN G. MORGAN,"Functional remodeling of the contractile smooth muscle cell cortex, a provocative concept, supported by direct visualization of cortical remodeling","Considerable controversy has surrounded the functional anatomy of the cytoskeleton of the contractile vascular smooth muscle cell. Recent studies have suggested a dynamic nature of the cortical cytoskeleton of these cells, but direct proof has been lacking. Here, we review past studies in this area suggesting a plasticity of smooth muscle cells. We also present images testing these suggestions by using the technique of immunoelectron microscopy of metal replicas to directly visualize the cortical actin cytoskeleton of the contractile smooth muscle cell along with interactions by representative cytoskeletal binding proteins. We find the cortical cytoskeletal matrix to be a branched, interconnected network of linear actin bundles. Here, the focal adhesion proteins talin and zyxin were localized with nanometer accuracy. Talin is reported in past studies to span the integrin-cytoplasm distance in fibroblasts and zyxin is known to be an adaptor protein between alpha-actinin and VASP. In response to activation of signal transduction with the alpha-agonist phenylephrine, we found that no movement of talin was detectable but that the zyxin-zyxin spacing was statistically significantly decreased in the smooth muscle cells examined. Contractile smooth muscle is often assumed to have a fixed cytoskeletal structure. Thus, the results included here are important in that they directly support the concept at the electron microscopic level that the focal adhesion of the contractile smooth muscle cell has a dynamic nature and that the protein-protein interfaces showing plasticity are protein-specific."
KATHLEEN G. MORGAN,Ageing causes an aortic contractile dysfunction phenotype by targeting the expression of members of the extracellular signal-regulated kinase pathway,"The extracellular signal-regulated kinase (ERK) pathway is a well-known regulator of vascular smooth muscle cell proliferation, but it also serves as a regulator of caldesmon, which negatively regulates vascular contractility. This study examined whether aortic contractile function requires ERK activation and if this activation is regulated by ageing. Biomechanical experiments revealed that contractile responses to the alpha1-adrenergic agonist phenylephrine are attenuated specifically in aged mice, which is associated with downregulation of ERK phosphorylation. ERK inhibition attenuates phenylephrine-induced contractility, indicating that the contractile tone is at least partially ERK-dependent. To explore the mechanisms of this age-related downregulation of ERK phosphorylation, we transfected microRNAs, miR-34a and miR-137 we have previously shown to increase with ageing and demonstrated that in A7r5 cells, both miRs downregulate the expression of Src and paxillin, known regulators of ERK signalling, as well as ERK phosphorylation. Further studies in aortic tissues transfected with miRs show that miR-34a but not miR-137 has a negative effect on mRNA levels of Src and paxillin. Furthermore, ERK phosphorylation is decreased in aortic tissue treated with the Src inhibitor PP2. Increases in miR-34a and miR-137 with ageing downregulate the expression of Src and paxillin, leading to impaired ERK signalling and aortic contractile dysfunction."
KATHLEEN G. MORGAN,BCL11B regulates arterial stiffness and related target organ damage,"RATIONALE: BCL11B (B-cell leukemia 11b) is a transcription factor known as an essential regulator of T lymphocytes and neuronal development during embryogenesis. A genome-wide association study showed that a gene desert region downstream of BCL11B, known to function as a BCL11B enhancer, harbors single nucleotide polymorphisms associated with increased arterial stiffness. However, a role for BCL11B in the adult cardiovascular system is unknown. OBJECTIVE: Based on these human findings, we sought to examine the relation between BCL11B and arterial function. METHODS AND RESULTS: Here we report that BCL11B is expressed in the vascular smooth muscle where it regulates vascular stiffness. RNA sequencing of aortas from wild-type and Bcl11b null mice (BSMKO) identified the cGMP (cyclic guanosine monophosphate)-cGMP-dependent protein kinase G (PKG) as the most significant differentially regulated signaling pathway in BSMKO compared with wild-type mice. BSMKO aortas showed decreased levels of PKG1, increased levels of Ca++-calmodulin-dependent serine/threonine phosphatase calcineurin (PP2B) and decreased levels of their common phosphorylation target, phosphorylated vasodilator-stimulated phosphoprotein (pVASPS239), a regulator of cytoskelatal actin rearrangements. Decreased pVASPS239 in BSMKO aortas was associated with increased actin polymerization (filamentous/globular actin ratio). Functionally, aortic force, stress, wall tension, and stiffness, measured ex vivo in organ baths, were increased in BSMKO aortas, and BSMKO mice had increased pulse wave velocity, the in vivo index of arterial stiffness. Despite having no effect on blood pressure or microalbuminuria, increased arterial stiffness in BSMKO mice was associated with increased incidence of cerebral microbleeds compared with age-matched wild-type littermates. CONCLUSIONS: We have identified vascular smooth muscle BCL11B as a crucial regulator of aortic smooth muscle function and a potential therapeutic target for vascular stiffness."
KATHLEEN G. MORGAN,The importance of dystrophin and the dystrophin associated proteins in vascular smooth muscle,"This review details the role of dystrophin and the dystrophin associated proteins (DAPs) in the vascular smooth muscle. Dystrophin is most comprehensively studied in the skeletal muscle due to serious symptoms found related to the skeletal muscle of patients with muscular dystrophy. Mutations in the dystrophin gene, or DAPs genes, result in a wide range of muscular dystrophies. In skeletal muscle, dystrophin is known to act to as a cytoskeletal stabilization protein and protects cells against contraction-induced damage. In skeletal muscle, dystrophin stabilizes the plasma membrane by transmitting forces generated by sarcomeric contraction to the extracellular matrix (ECM). Dystrophin is a scaffold that binds the dystroglycan complex (DGC) and has many associated proteins (DAPs). These DAPs include sarcoglycans, syntrophins, dystroglycans, dystrobrevin, neuronal nitric oxide synthase, and caveolins. The DAPs provide biomechanical support to the skeletal or cardiac plasma membrane during contraction, and loss of one or several of these DAPs leads to plasma membrane fragility. Dystrophin is expressed near the plasma membrane of all muscles, including cardiac and vascular smooth muscle, and some neurons. Dystrophic mice have noted biomechanical irregularities in the carotid arteries and spontaneous motor activity in portal vein altered when compared to wild type mice. Additionally, some studies suggest the vasculature of patients and animal models with muscular dystrophy is abnormal. Although the function of dystrophin and the DAPs in vascular smooth muscle is not thoroughly established in the field, this review makes the point that these proteins are expressed, and important and further study is warranted."
KATHLEEN G. MORGAN,MicroRNA-203 mimics age-related aortic smooth muscle dysfunction of cytoskeletal pathways,"Increased aortic stiffness is a biomarker for subsequent adverse cardiovascular events. We have previously reported that vascular smooth muscle Src-dependent cytoskeletal remodelling, which contributes to aortic plasticity, is impaired with ageing. Here, we use a multi-scale approach to determine the molecular mechanisms behind defective Src-dependent signalling in an aged C57BL/6 male mouse model. Increased aortic stiffness, as measured in vivo by pulse wave velocity, was found to have a comparable time course to that in humans. Bioinformatic analyses predicted several miRs to regulate Src-dependent cytoskeletal remodelling. qRT-PCR was used to determine the relative levels of predicted miRs in aortas and, notably, the expression of miR-203 increased almost twofold in aged aorta. Increased miR-203 expression was associated with a decrease in both mRNA and protein expression of Src, caveolin-1 and paxillin in aged aorta. Probing with phospho-specific antibodies confirmed that overexpression of miR-203 significantly attenuated Src and extracellular signal regulated kinase (ERK) signalling, which we have previously found to regulate vascular smooth muscle stiffness. In addition, transfection of miR-203 into aortic tissue from young mice increased phenylephrine-induced aortic stiffness ex vivo, mimicking the aged phenotype. Upstream of miR-203, we found that DNA methyltransferases (DNMT) 1, 3a, and 3b are also significantly decreased in the aged mouse aorta and that DNMT inhibition significantly increases miR-203 expression. Thus, the age-induced increase in miR-203 may be caused by epigenetic promoter hypomethylation in the aorta. These findings indicate that miR-203 promotes a re-programming of Src/ERK signalling pathways in vascular smooth muscle, impairing the regulation of stiffness in aged aorta."
KATHLEEN G. MORGAN,Calponin 1 inhibits agonist-induced ERK activation and decreases calcium sensitization in vascular smooth muscle,"Smooth muscle cell (SMC) contraction and vascular tone are modulated by phosphorylation and multiple modifications of the thick filament, and thin filament regulation of SMC contraction has been reported to involve extracellular regulated kinase (ERK). Previous studies in ferrets suggest that the actin-binding protein, calponin 1 (CNN1), acts as a scaffold linking protein kinase C (PKC), Raf, MEK and ERK, promoting PKC-dependent ERK activation. To gain further insight into this function of CNN1 in ERK activation and the regulation of SMC contractility in mice, we generated a novel Calponin 1 knockout mouse (Cnn1 KO) by a single base substitution in an intronic CArG box that preferentially abolishes expression of CNN1 in vascular SMCs. Using this new Cnn1 KO mouse, we show that ablation of CNN1 has two effects, depending on the cytosolic free calcium level: (1) in the presence of elevated intracellular calcium caused by agonist stimulation, Cnn1 KO mice display a reduced amplitude of stress and stiffness but an increase in agonist-induced ERK activation; and (2) during intracellular calcium depletion, in the presence of an agonist, Cnn1 KO mice exhibit increased duration of SM tone maintenance. Together, these results suggest that CNN1 plays an important and complex modulatory role in SMC contractile tone amplitude and maintenance."
KATHLEEN G. MORGAN,A novel mechanism of ERK1/2 regulation in smooth muscle involving acetylation of the ERK1/2 scaffold IQGAP1,"Ceramide, a bioactive lipid and signaling molecule associated with cardiovascular disease, is known to activate extracellular signal regulated kinases 1 and 2 (ERK1/2). Here, we determined that the effect of ceramide on ERK1/2 is mediated by ceramide signaling on an ERK scaffold protein, IQ motif containing GTPase activating protein 1 (IQGAP1). Experiments were performed with aortic smooth muscle cells using inhibitor screening, small interfering RNA (siRNA), immunoprecipitation (IP), immunoblots and bioinformatics. We report here that C6 ceramide increases serum-stimulated ERK1/2 activation in a manner dependent on the ERK1/2 scaffold IQGAP1. C6 ceramide increases IQGAP1 protein levels by preventing its cleavage. Bioinformatic analysis of the IQGAP1 amino acid sequence revealed potential cleavage sites for proteases of the proprotein convertase family that match the cleavage products. These potential cleavage sites overlap with known motifs for lysine acetylation. Deacetylase inhibitor treatment increased IQGAP1 acetylation and reduced IQGAP1 cleavage. These data are consistent with a model in which IQGAP1 cleavage is regulated by acetylation of the cleavage sites. Activation of ERK1/2 by ceramide, known to increase lysine acetylation, appears to be mediated by acetylation-dependent stabilization of IQGAP1. This novel mechanism could open new possibilities for therapeutic intervention in cardiovascular diseases."
PAMELA H TEMPLER,"Tree transpiration and urban temperatures: current understanding, implications, and future research directions","The expansion of an urban tree canopy is a commonly proposed nature-based solution to combat excess urban heat. The influence trees have on urban climates via shading is driven by the morphological characteristics of trees, whereas tree transpiration is predominantly a physiological process dependent on environmental conditions and the built environment. The heterogeneous nature of urban landscapes, unique tree species assemblages, and land management decisions make it difficult to predict the magnitude and direction of cooling by transpiration. In the present article, we synthesize the emerging literature on the mechanistic controls on urban tree transpiration. We present a case study that illustrates the relationship between transpiration (using sap flow data) and urban temperatures. We examine the potential feed backs among urban canopy, the built environment, and climate with a focus on extreme heat events. Finally, we present modeled data demonstrating the influence of transpiration on temperatures with shifts in canopy extent and irrigation during a heat wave."
PAMELA H TEMPLER,"Roots mediate the effects of snowpack decline on soil bacteria, fungi, and nitrogen cycling in a northern hardwood forest","Rising winter air temperature will reduce snow depth and duration over the next century in northern hardwood forests. Reductions in snow depth may affect soil bacteria and fungi directly, but also affect soil microbes indirectly through effects of snowpack loss on plant roots. We incubated root exclusion and root ingrowth cores across a winter climate-elevation gradient in a northern hardwood forest for 29 months to identify direct (i.e., winter snow-mediated) and indirect (i.e., root-mediated) effects of winter snowpack decline on soil bacterial and fungal communities, as well as on potential nitrification and net N mineralization rates. Both winter snowpack decline and root exclusion increased bacterial richness and phylogenetic diversity. Variation in bacterial community composition was best explained by differences in winter snow depth or soil frost across elevation. Root ingrowth had a positive effect on the relative abundance of several bacterial taxonomic orders (e.g., Acidobacterales and Actinomycetales). Nominally saprotrophic (e.g., Saccharomycetales and Mucorales) or mycorrhizal (e.g., Helotiales, Russalales, Thelephorales) fungal taxonomic orders were also affected by both root ingrowth and snow depth variation. However, when grouped together, the relative abundance of saprotrophic fungi, arbuscular mycorrhizal fungi, and ectomycorrhizal fungi were not affected by root ingrowth or snow depth, suggesting that traits in addition to trophic mode will mediate fungal community responses to snowpack decline in northern hardwood forests. Potential soil nitrification rates were positively related to ammonia-oxidizing bacteria and archaea abundance (e.g., Nitrospirales, Nitrosomondales, Nitrosphaerales). Rates of N mineralization were positively and negatively correlated with ectomycorrhizal and saprotrophic fungi, respectively, and these relationships were mediated by root exclusion. The results from this study suggest that a declining winter snowpack and its effect on plant roots each have direct effects on the diversity and abundance of soil bacteria and fungal communities that interact to determine rates of soil N cycling in northern hardwood forests."
PAMELA H TEMPLER,"Carbon, indoor air, energy and financial benefits of coupled ventilation upgrade and enhanced rooftop garden installation: an interdisciplinary climate mitigation approach",
ELIZABETH S KLINGS,Role of Free Radicals in the Pathogenesis of Acute Chest Syndrome in Sickle Cell Disease,"Acute chest syndrome (ACS) of sickle cell disease (SCD) is characterized pathologically by vaso-occlusive processes that result from abnormal interactions between sickle red blood cells (RBCs), white blood cells (WBCs) and/or platelets, and the vascular endothelium. One potential mechanism of vascular damage in ACS is by generation of oxygen-related molecules, such as superoxide (O2-), hydrogen peroxide (H2O2), peroxynitrite (ONOO-), and the hydroxyl (•OH) radical. The present review summarizes the evidence for alterations in oxidant stress during ACS of SCD, and the potential contributions of RBCs, WBCs and the vascular endothelium to this process."
KIM MUESER,"The relationship between cognitive functioning, age and employment in people with severe mental illnesses in an urban area in India: a longitudinal study.","Although there is substantial evidence of the association between cognitive impairment and work in people with severe mental illnesses (SMI) in developed countries, less is known about this relationship in developing countries such as India. Studies showing higher rates of employment in people with SMI in developing countries than developed ones raise the question of whether cognitive functioning is related to work status and characteristics of work (e.g., wages earned). We conducted a one-year follow-up study to investigate the relationship between employment and cognitive functioning, assessed with the Montreal Cognitive Assessment (MoCA), in 150 participants with SMI (92% schizophrenia) living in an urban area and receiving psychiatric outpatient treatment at a public hospital in India. The MoCA had good internal reliability and test-retest reliability over the one-year period. Better cognitive functioning was associated with younger age, shorter duration of illness, higher education, and male gender. Both younger and older participants with higher cognitive functioning at baseline were more likely to be employed at baseline and one year later. Work status at baseline and one year follow-up was consistently related to executive functions among younger participants, and to attention among older participants, suggesting changes over the course of illness in the importance of specific cognitive domains for achieving satisfactory work performance. The findings suggest that cognitive functioning is associated with employment in people with SMI in India. Attention to impaired cognitive functioning may be critical to improving employment outcomes in this population."
KIM MUESER,Do cognitive impairments limit treatment gains in a standalone digital intervention for psychosis? A test of the digital divide,"Digital mental health interventions, such as those provided by smartphone applications (apps), show promise as cost-effective approaches to increasing access to evidence-based psychosocial interventions for psychosis. Although it is well known that limited financial resources can reduce the benefits of digital approaches to mental healthcare, the extent to which cognitive functioning in this population could impact capacity to engage in and benefit from these interventions is less studied. In the current study we examined the extent to which cognitive functioning (premorbid cognitive abilities and social cognition) were related to treatment engagement and outcome in a standalone digital intervention for social functioning. Premorbid cognitive abilities generally showed no association with aggregated treatment engagement markers, including proportion of notifications responded to and degree of interest in working on app content, though there was a small positive association with improvements in social functioning. Social cognition, as measured using facial affect recognition ability, was unrelated to treatment engagement or outcome. These preliminary findings suggest that cognitive functioning is generally not associated with engagement or outcomes in a standalone digital intervention designed for and with people with schizophrenia spectrum disorders."
KIM MUESER,Cognitive remediation and psychosocial rehabilitation for individuals with severe mental illness,
KIM MUESER,Cognitive and metacognitive factors predict engagement in employment in individuals with first episode psychosis,"BACKGROUND: Research has demonstrated that cognitive abilities predict work outcomes in people with psychosis. Cognitive Remediation Programs go some way in improving work outcomes, but individuals still experience difficulty maintaining employment. Metacognition has been demonstrated to predict work performance in individuals with schizophrenia, but this has not yet been applied to First Episode Psychosis (FEP). This study assessed whether metacognition, intellectual aptitude and functional capacity can predict engagement in work and number of hours of work within FEP. METHODS: Fifty-two individuals with psychosis, from an Early Intervention in Psychosis service, completed measures of IQ, metacognition (Metacognitive Assessment Interview), functional capacity (UPSA), and functional outcome (hours spent in structured activity per week, including employment). RESULTS: Twenty-six participants (22 males, 4 females) were employed and twenty-six (22 males, 4 females) were not employed. IQ and metacognition were significantly associated with whether the individual was engaged in employment [IQ (p = .02) and metacognition (p = 006)]. When controlling for IQ, metacognition (differentiation subscale) remained significant (p = .04). Next, including only those employed, no cognitive nor metacognitive factors predicted number of hours in employment. DISCUSSION: This is the first study to directly assess metacognition as a predictor of work hours for individuals with FEP. This study highlights the importance of enhancing metacognitive ability in order to improve likelihood of, and engagement in, employment for those with FEP."
KIM MUESER,Psychometric properties of the mock interview rating scale for schizophrenia and other serious mental illnesses,"BACKGROUND: Over the past 10 years, job interview training has emerged as an area of study among adults with schizophrenia and other serious mental illnesses who face significant challenges when navigating job interviews. The field of mental health services research has limited access to assessments of job interview skills with rigorously evaluated psychometric properties. OBJECTIVE: We sought to evaluate the initial psychometric properties of a measure assessing job interview skills via role-play performance. METHODS: As part of a randomized controlled trial, 90 adults with schizophrenia or other serious mental illnesses completed a job interview role-play assessment with eight items (and scored using anchors) called the mock interview rating scale (MIRS). A classical test theory analysis was conducted including confirmatory factor analyses, Rasch model analysis and calibration, and differential item functioning; along with inter-rater, internal consistency, and test-retest reliabilities. Pearson correlations were used to evaluate construct, convergent, divergent, criterion, and predictive validity by correlating the MIRS with demographic, clinical, cognitive, work history measures, and employment outcomes. RESULTS: Our analyses resulted in the removal of a single item (sounding honest) and yielded a unidimensional total score measurement with support for its inter-rater reliability, internal consistency, and test-retest reliability. There was initial support for the construct, convergent, criterion, and predictive validities of the MIRS, as it correlated with measures of social competence, neurocognition, valuing job interview training, and employment outcomes. Meanwhile, the lack of correlations with race, physical health, and substance abuse lent support for divergent validity. CONCLUSION: This study presents initial evidence that the seven-item version of the MIRS has acceptable psychometric properties supporting its use to assess job interview skills reliably and validly among adults with schizophrenia and other serious mental illnesses. CLINICAL TRIAL REGISTRATION: NCT03049813."
MARIANNE BAXTER,Robust determinants of bilateral trade,"What are the policies and country-level conditions which best explain bilateral trade flows between countries? As databases expand, an increasing number of possible explanatory variables are proposed that influence bilateral trade without a clear indication of which variables are robustly important across contexts, time periods, and which are not sensitive to inclusion of other control variables. To shed light on this problem, we apply three model selection methods – Lasso reguarlized regression, Bayesian Model Averaging, and Extreme Bound Analysis -- to candidate variables in a gravity models of trade. Using a panel of 198 countries covering the years 1970 to 2000, we find model selection methods suggest many fewer variables are robust that those suggested by the null hypothesis rejection methodology from ordinary least squares."
MARIANNE BAXTER,Detecting household production,"The economics of household choice has occupied an important position in both microeconomics and macroeconomics since Becker’s (1965) pioneering work. Yet our empirical understanding of the household sector is hampered by the absence of measurement of this sector’s output. Our paper contributes to our knowledge of the extent of home/market substitutions by utilizing detailed expenditure data provided by the Consumer Expenditure Survey (CEX). We compare expenditures by married, 1-earner households to expenditures by married, 2-earner households. Single-earner households have more non-market time that maybe be spent in home production or leisure. We detect the presence of several home/market substitutions across family types that are consistent with the theory."
MARIANNE BAXTER,New low price! An analysis of IKEA pricing,"IKEA is a multinational retailer of home furnishings. The dominant feature of IKEA advertising and market image is a reputation for low prices. IKEA's iconic catalog is the company's primary advertising device, representing 70% of IKEA's annual advertising budget. The catalog contains photos, prices, and written descriptions of items, with prices denominated in the customer's currency. IKEA prices are famous for remaining unchanged for the life of the catalog year. The IKEA data are free of two major problems that arise in other micro-pricing studies: product substitution and temporary sales. This paper analyzes IKEA catalog price data for Germany, France, Italy, Sweden, UK, US and Canada. We nd a substantial proportion of small price changes, as is common in micro price datasets, but we nd that small price changes are overwhelmingly associated with the practice of \charm pricing""{adjusting ending digits to end in the numeral \9"", with price changes less than one currency unit. Both intensive and extensive margins are important contributors to overall IKEA catalog price changes, in contrast to the prior literature which nds that only the intensive margin is important. We nd near-zero pass-through of exchange rates to IKEA prices but approximately 100% pass-through of local prices, which is consistent with the illustrative model that we present that predicts this pattern for goods with a high component of local distribution costs. Our most novel results concern price coordination. IKEA price changes show low coordination across countries, even among countries in the Euro Zone. For example, prices for identical goods are no more likely to move together between Germany and France than between Sweden and Canada. We nd evidence of signi cant price coordination with goods belonging to a product family such as the \BILLY"" bookcases. However, the price coordination exists only within countries and not across countries. These facts suggest that there may be economies of scope in price setting as suggested by Midrigan (2011), but that these economies operate at the family level of goods and do not operate across countries."
MARIANNE BAXTER,"IKEA: product, pricing, and pass-through","With over 300 stores in 40 countries, IKEA is a major international presence in retail housewares and furnishings. IKEA publishes country-specific catalogs with local-currency prices guaranteed to hold for 1 year. This paper explores a new dataset of IKEA products and catalog prices covering six countries for the time period 1994-2010. The dataset, with over 140,000 observations, is uniquely poised to shed light on the way in which a large multinational retailer operates in a setting characterized by a very large number of goods, distributed and priced in many countries. Thus, the goal of this paper is to document the choices made by IKEA in several related decision areas. In doing so, this paper provides evidence against which existing theories can be evaluated and revised in the light of this new information."
AARON V GARRETT,First Sagittarius A* Event Horizon Telescope results. I. The shadow of the supermassive black hole in the center of the Milky Way,"We present the first Event Horizon Telescope (EHT) observations of Sagittarius A* (Sgr A*), the Galactic center source associated with a supermassive black hole. These observations were conducted in 2017 using a global interferometric array of eight telescopes operating at a wavelength of λ = 1.3 mm. The EHT data resolve a compact emission region with intrahour variability. A variety of imaging and modeling analyses all support an image that is dominated by a bright, thick ring with a diameter of 51.8 ± 2.3 μas (68% credible interval). The ring has modest azimuthal brightness asymmetry and a comparatively dim interior. Using a large suite of numerical simulations, we demonstrate that the EHT images of Sgr A* are consistent with the expected appearance of a Kerr black hole with mass ∼4 × 106 M ⊙, which is inferred to exist at this location based on previous infrared observations of individual stellar orbits, as well as maser proper-motion studies. Our model comparisons disfavor scenarios where the black hole is viewed at high inclination (i &gt; 50°), as well as nonspinning black holes and those with retrograde accretion disks. Our results provide direct evidence for the presence of a supermassive black hole at the center of the Milky Way, and for the first time we connect the predictions from dynamical measurements of stellar orbits on scales of 103–105 gravitational radii to event-horizon-scale images and variability. Furthermore, a comparison with the EHT results for the supermassive black hole M87* shows consistency with the predictions of general relativity spanning over three orders of magnitude in central mass."
ANDERS W SANDVIK,Field-driven quantum phase transitions in S=1/2 spin chains,"We study the magnetization process of a one-dimensional extended Heisenberg model, the J−Q model, as a function of an external magnetic field h. In this model, J represents the traditional antiferromagnetic Heisenberg exchange and Q is the strength of a competing four-spin interaction. Without external field, this system hosts a twofold-degenerate dimerized (valence-bond solid) state above a critical value qc≈0.85 where q≡Q/J. The dimer order is destroyed and replaced by a partially polarized translationally invariant state at a critical field value. We find magnetization jumps (metamagnetism) between the partially polarized and fully polarized state for q>qmin, where we have calculated qmin=29 exactly. For q>qmin, two magnons (flipped spins on a fully polarized background) attract and form a bound state. Quantum Monte Carlo studies confirm that the bound state corresponds to the first step of an instability leading to a finite magnetization jump for q>qmin. Our results show that neither geometric frustration nor spin anisotropy are necessary conditions for metamagnetism. Working in the two-magnon subspace, we also find evidence pointing to the existence of metamagnetism in the unfrustrated J1−J2 chain (J1>0,J2<0), but only if J2 is spin anisotropic. In addition to the studies at zero temperature, we also investigate quantum-critical scaling near the transition into the fully polarized state for q≤qmin at T>0. While the expected “zero-scale-factor” universality is clearly seen for q=0 and q≪qmin, for q closer to qmin we find that extremely low temperatures are required to observe the asymptotic behavior, due to the influence of the tricritical point at qmin. In the low-energy theory, one can expect the quartic nonlinearity to vanish at qmin and a marginal sixth-order term should govern the scaling, which leads to a crossover at a temperature T∗(q) between logarithmic tricritical scaling and zero-scale-factor universality, with T∗(q)→0 when q→qmin."
ANDERS W SANDVIK,Critical level crossings and gapless spin liquid in the square-lattice spin-1/2 J1-J2 Heisenberg antiferromagnet,"We use the DMRG method to calculate several energy eigenvalues of the frustrated S = 1 / 2 square-lattice J 1 - J 2 Heisenberg model on 2 L × L cylinders with L ≤ 10. We identify excited-level crossings versus the coupling ratio g = J 2 /J 1 and study their drifts with the system size L . The lowest singlet-triplet and singlet-quintuplet crossings converge rapidly (with corrections ∝ L − 2 ) to different g values, and we argue that these correspond to ground-state transitions between the N ́eel antiferromagnet and a gapless spin liquid, at g c 1 ≈ 0 . 46, and between the spin liquid and a valence-bond-solid at g c 2 ≈ 0 . 52. Previous studies of order parameters were not able to positively discriminate between an extended spin liquid phase and a critical point. We expect level-crossing analysis to be a generically powerful tool in DMRG studies of quantum phase transitions."
ANDERS W SANDVIK,Random-singlet phase in disordered two-dimensional quantum magnets,"We study effects of disorder (randomness) in a 2D square-lattice S=1/2 quantum spin system, the J-Q model with a 6-spin interaction Q supplementing the Heisenberg exchange J. In the absence of disorder the system hosts antiferromagnetic (AFM) and columnar valence-bond-solid (VBS) ground states. The VBS breaks Z4 symmetry, and in the presence of arbitrarily weak disorder it forms domains. Using QMC simulations, we demonstrate two kinds of such disordered VBS states. Upon dilution, a removed site leaves a localized spin in the opposite sublattice. These spins form AFM order. For random interactions, we find a different state, with no order but algebraically decaying mean correlations. We identify localized spinons at the nexus of domain walls between different VBS patterns. These spinons form correlated groups with the same number of spinons and antispinons. Within such a group, there is a strong tendency to singlet formation, because of spinon-spinon interactions mediated by the domain walls. Thus, no long-range AFM order forms. We propose that this state is a 2D analog of the well-known 1D random singlet (RS) state, though the dynamic exponent z in 2D is finite. By studying the T-dependent magnetic susceptibility, we find that z varies, from z=2 at the AFM--RS phase boundary and larger in the RS phase The RS state discovered here in a system without geometric frustration should correspond to the same fixed point as the RS state recently proposed for frustrated systems, and the ability to study it without Monte Carlo sign problems opens up opportunities for further detailed characterization of its static and dynamic properties. We also discuss experimental evidence of the RS phase in the quasi-two-dimensional square-lattice random-exchange quantum magnets Sr2CuTe1−xWxO6."
ANDERS W SANDVIK,Dynamical signature of fractionalization at a deconfined quantum critical point,"Deconfined quantum critical points govern continuous quantum phase transitions at which fractionalized (deconfined) degrees of freedom emerge. Here we study dynamical signatures of the fractionalized excitations in a quantum magnet (the easy-plane J-Q model) that realize a deconfined quantum critical point with emergent O(4) symmetry. By means of large-scale quantum Monte Carlo simulations and stochastic analytic continuation of imaginary-time correlation functions, we obtain the dynamic spin-structure factors in the S x and S z channels. In both channels, we observe broad continua that originate from the deconfined excitations. We further identify several distinct spectral features of the deconfined quantum critical point, including the lower edge of the continuum and its form factor on moving through the Brillouin zone. We provide field-theoretical and lattice model calculations that explain the overall shapes of the computed spectra, which highlight the importance of interactions and gauge fluctuations to explain the spectral-weight distribution. We make further comparisons with the conventional Landau O(2) transition in a different quantum magnet, at which no signatures of fractionalization are observed. The distinctive spectral signatures of the deconfined quantum critical point suggest the feasibility of its experimental detection in neutron scattering and nuclear magnetic resonance experiments."
ANDERS W SANDVIK,Duality between the deconfined quantum-critical point and the bosonic topological transition,"Recently, significant progress has been made in (2+1)-dimensional conformal field theories without supersymmetry. In particular, it was realized that different Lagrangians may be related by hidden dualities; i.e., seemingly different field theories may actually be identical in the infrared limit. Among all the proposed dualities, one has attracted particular interest in the field of strongly correlated quantum-matter systems: the one relating the easy-plane noncompact CP^1 model (NCCP^1) and noncompact quantum electrodynamics (QED) with two flavors (N = 2) of massless two-component Dirac fermions. The easy-plane NCCP^1 model is the field theory of the putative deconfined quantum-critical point separating a planar (XY) antiferromagnet and a dimerized (valence-bond solid) ground state, while N = 2 noncompact QED is the theory for the transition between a bosonic symmetry-protected topological phase and a trivial Mott insulator. In this work, we present strong numerical support for the proposed duality. We realize the N = 2 noncompact QED at a critical point of an interacting fermion model on the bilayer honeycomb lattice and study it using determinant quantum Monte Carlo (QMC) simulations. Using stochastic series expansion QMC simulations, we study a planar version of the S = 1/2 J-Q spin Hamiltonian (a quantum XY model with additional multispin couplings) and show that it hosts a continuous transition between the XY magnet and the valence-bond solid. The duality between the two systems, following from a mapping of their phase diagrams extending from their respective critical points, is supported by the good agreement between the critical exponents according to the proposed duality relationships. In the J-Q model, we find both continuous and first-order transitions, depending on the degree of planar anisotropy, with deconfined quantum criticality surviving only up to moderate strengths of the anisotropy. This explains previous claims of no deconfined quantum criticality in planar two-component spin models, which were in the strong-anisotropy regime, and opens doors to further investigations of the global phase diagram of systems hosting deconfined quantum-critical points."
ANDERS W SANDVIK,Dynamic scaling of the restoration of rotational symmetry in Heisenberg quantum antiferromagnets,"We apply imaginary-time evolution with the operator e−τH to study relaxation dynamics of gapless quantum antiferromagnets described by the spin-rotation invariant Heisenberg Hamiltonian (H). Using quantum Monte Carlo simulations to obtain unbiased results, we propagate an initial state with maximal order parameter mzs (the staggered magnetization) in the z spin direction and monitor the expectation value ⟨ms⟩ as a function of imaginary-time τ. Results for different system sizes (lengths) L exhibit an initial essentially size-independent relaxation of ⟨ms⟩ toward its value in the infinite-size spontaneously symmetry-broken state, followed by a strongly size-dependent final decay to zero when the O(3) rotational symmetry of the order paraneter is restored. We develop a generic finite-size scaling theory that shows the relaxation time diverges asymptotically as Lz where z is the dynamic exponent of the low energy excitations. We use the scaling theory to develop a practical way of extracting the dynamic exponent from the numerical finite-size data, systematcally eliminating scaling corrections. We apply the method to spin-1/2 Heisenberg antiferromagnets on two different lattice geometries: the standard two-dimensional (2D) square lattice as well as a site- diluted 2D square lattice at the percolation threshold. In the 2D case we obtain z = 2.001(5), which is consistent with the known value z = 2, while for the site-dilutes lattice we find z = 3.90(1) or z = 2.056(8)Df , where Df = 91/48 is the fractal dimensionality of the percolating system. This is an improvement on previous estimates of z ≈ 3.7. The scaling results also show a fundamental difference between the two cases; for the 2D square lattice, the data can be collapsed onto a common scaling function even when ⟨ms⟩ is relatively large, reflecting the Anderson tower of quantum rotor states with a common dynamic exponent z = 2. For the diluted 2D square lattice, the scaling works well only for small ⟨ms⟩, indicating a mixture of different relaxation time scaling between the low energy states. Nevertheless, the low-energy dynamic here also corresponds to a tower of excitations."
ANDERS W SANDVIK,Indicators of conformal field theory: Entanglement entropy and multiple-point correlators,"The entanglement entropy (EE) of quantum systems is often used as a test of low-energy descriptions by conformal field theory (CFT). Here we point out that this is not a reliable indicator, as the EE often shows the same behavior even when a CFT description is not correct (as long as the system is asymptotically scale-invariant). We use constraints on the scaling dimension given by the CFT with SU(2) symmetry to provide alternative tests with two- and four-point correlation functions, showing examples for quantum spin models in 1+1 dimensions. In the case of a critical amplitude-product state expressed in the valence-bond basis (where the amplitudes decay as a power law of the bond length and the wave function is the product of all bond amplitudes), we show that even though the EE exhibits the expected CFT behavior, there is no CFT description of this state. We provide numerical tests of the behavior predicted by CFT for the correlation functions in the critical transverse-field Ising chain and the J−Q spin chain, where the conformal structure is well understood. That behavior is not reproduced in the amplitude-product state."
ANDERS W SANDVIK,Possible Exotic Phases in the One-Dimensional Extended Hubbard Model,"We investigate numerically the ground state phase diagram of the one-dimensional extended Hubbard model, including an on--site interaction U and a nearest--neighbor interaction V. We focus on the ground state phases of the model in the V >> U region, where previous studies have suggested the possibility of dominant superconducting pairing fluctuations before the system phase separates at a critical value V=V_PS. Using quantum Monte Carlo methods on lattices much larger than in previous Lanczos diagonalization studies, we determine the boundary of phase separation, the Luttinger Liquid correlation exponent K_rho, and other correlation functions in this region. We find that phase separation occurs for V significantly smaller than previously reported. In addition, for negative U, we find that a uniform state re-enters from phase separation as the electron density is increased towards half filling. For V < V_PS, our results show that superconducting fluctuations are not dominant. The system behaves asymptotically as a Luttinger Liquid with K_rho < 1, but we also find strong low-energy (but gapped) charge-density fluctuations at a momentum not expected for a standard Luttinger Liquid."
ANDERS W SANDVIK,1D valence bond solids in a magnetic field,"A Valence bond solid (VBS) is a nonmagnetic, long-range ordered state of a quantum spin system where local spin singlets are formed in some regular pattern. We here study the competition between VBS order and a fully polarized ferromagnetic state as function of an external magnetic field in a one-dimensional extended Heisenberg model---the J-Q2 model---using stochastic series expansion quantum Monte Carlo simulations with directed loop updates. We discuss the ground state phase diagram."
ANDERS W SANDVIK,Typicality at quantum-critical points,"We discuss the concept of typicality of quantum states at quantum-critical points, using projector Monte Carlo simulations of an S = 1/2 bilayer Heisenberg antiferromagnet as an illustration. With the projection (imaginary) time τ scaled as τ = aLz , L being the system length and z the dynamic critical exponent (which takes the value z = 1 in the bilayer model studied here), a critical point can be identified which asymptotically flows to the correct location and universality class with increasing L, independently of the prefactor a and the initial state. Varying the proportionality factor a and the initial state only changes the cross-over behavior into the asymptotic large-L behavior. In some cases, choosing an optimal factor a may also lead to the vanishing of the leading finite-size corrections. The observation of typicality can be used to speed up simulations of quantum criticality, not only within the Monte Carlo approach but also with other numerical methods where imaginary-time evolution is employed, e.g., tensor network states, as it is not necessary to evolve fully to the ground state but only for sufficiently long times to reach the typicality regime."
ANDERS W SANDVIK,Numerical investigations of SO(4) emergent extended symmetry in spin-1/2 Heisenberg antiferromagnetic chains,"The antiferromagnetic Heisenberg chain is expected to have an extended symmetry, [SU(2)×SU(2)]/Z2, in the infrared limit whose physical interpretation is that the spin and dimer order parameters form the components of a common four-dimensional pseudovector. Here we numerically investigate this emergent symmetry using quantum Monte Carlo simulations of a modified Heisenberg chain (the J−Q model) in which the logarithmic scaling corrections of the conventional Heisenberg chain can be avoided. We show how the two- and three-point spin and dimer correlation functions approach their forms constrained by conformal field theory as the system size increases and numerically confirm the expected effects of the extended symmetry on various correlation functions. We stress that sometimes the leading power laws of three-point (and higher) correlations are not given simply by the scaling dimensions of the lattice operators involved but can be faster decaying because of exact cancellations of contributions from the fields and currents under conformal symmetry."
ANDERS W SANDVIK,Dual time scales in simulated annealing of a two-dimensional Ising spin glass,"We apply a generalized Kibble-Zurek out-of-equilibrium scaling ansatz to simulated annealing when approaching the spin-glass transition at temperature T=0 of the two-dimensional Ising model with random J=±1 couplings. Analyzing the spin-glass order parameter and the excess energy as functions of the system size and the annealing velocity in Monte Carlo simulations with Metropolis dynamics, we find scaling where the energy relaxes slower than the spin-glass order parameter, i.e., there are two different dynamic exponents. The values of the exponents relating the relaxation time scales to the system length, τ∼Lz, are z=8.28±0.03 for the relaxation of the order parameter and z=10.31±0.04 for the energy relaxation. We argue that the behavior with dual time scales arises as a consequence of the entropy-driven ordering mechanism within droplet theory. We point out that the dynamic exponents found here for T→0 simulated annealing are different from the temperature-dependent equilibrium dynamic exponent zeq(T), for which previous studies have found a divergent behavior: zeq(T→0)→∞. Thus, our study shows that, within Metropolis dynamics, it is easier to relax the system to one of its degenerate ground states than to migrate at low temperatures between regions of the configuration space surrounding different ground states. In a more general context of optimization, our study provides an example of robust dense-region solutions for which the excess energy (the conventional cost function) may not be the best measure of success."
ANDERS W SANDVIK,Nearly deconfined spinon excitations in the square-lattice spin-1/2 Heisenberg antiferromagnet,"We study the spin-excitation spectrum (dynamic structure factor) of the spin-1/2 square-lattice Heisenberg antiferromagnet and an extended model (the J−Q model) including four-spin interactions Q in addition to the Heisenberg exchange J. Using an improved method for stochastic analytic continuation of imaginary-time correlation functions computed with quantum Monte Carlo simulations, we can treat the sharp (δ-function) contribution to the structure factor expected from spin-wave (magnon) excitations, in addition to resolving a continuum above the magnon energy. Spectra for the Heisenberg model are in excellent agreement with recent neutron-scattering experiments on Cu(DCOO)2⋅4D2O, where a broad spectral-weight continuum at wave vector q=(π,0) was interpreted as deconfined spinons, i.e., fractional excitations carrying half of the spin of a magnon. Our results at (π,0) show a similar reduction of the magnon weight and a large continuum, while the continuum is much smaller at q=(π/2,π/2) (as also seen experimentally). We further investigate the reasons for the small magnon weight at (π,0) and the nature of the corresponding excitation by studying the evolution of the spectral functions in the J−Q model. Upon turning on the Q interaction, we observe a rapid reduction of the magnon weight to zero, well before the system undergoes a deconfined quantum phase transition into a nonmagnetic spontaneously dimerized state. Based on these results, we reinterpret the picture of deconfined spinons at (π,0) in the experiments as nearly deconfined spinons—a precursor to deconfined quantum criticality. To further elucidate the picture of a fragile (π,0)-magnon pole in the Heisenberg model and its depletion in the J−Q model, we introduce an effective model of the excitations in which a magnon can split into two spinons that do not separate but fluctuate in and out of the magnon space (in analogy to the resonance between a photon and a particle-hole pair in the exciton-polariton problem). The model can reproduce the reduction of magnon weight and lowered excitation energy at (π,0) in the Heisenberg model, as well as the energy maximum and smaller continuum at (π/2,π/2). It can also account for the rapid loss of the (π,0) magnon with increasing Q and the remarkable persistence of a large magnon pole at q=(π/2,π/2) even at the deconfined critical point. The fragility of the magnons close to (π,0) in the Heisenberg model suggests that various interactions that likely are important in many materials—e.g., longer-range pair exchange, ring exchange, and spin-phonon interactions—may also destroy these magnons and lead to even stronger spinon signatures than in Cu(DCOO)2⋅4D2O."
ANDERS W SANDVIK,Dynamic scaling in the two-dimensional Ising spin glass with normal-distributed couplings,"We carry out simulated annealing and employ a generalized Kibble-Zurek scaling hypothesis to study the two-dimensional Ising spin glass with normal-distributed couplings. The system has an equilibrium glass transition at temperature T=0. From a scaling analysis when T→0 at different annealing velocities v, we find power-law scaling in the system size for the velocity required in order to relax toward the ground state, v∼L−(z+1/ν), the Kibble-Zurek ansatz where z is the dynamic critical exponent and ν the previously known correlation-length exponent, ν≈3.6. We find z≈13.6 for both the Edwards-Anderson spin-glass order parameter and the excess energy. This is different from a previous study of the system with bimodal couplings [Rubin et al., Phys. Rev. E 95, 052133 (2017)] where the dynamics is faster (z is smaller) and the above two quantities relax with different dynamic exponents (with that of the energy being larger). We argue that the different behaviors arise as a consequence of the different low-energy landscapes: for normal-distributed couplings the ground state is unique (up to a spin reflection), while the system with bimodal couplings is massively degenerate. Our results reinforce the conclusion of anomalous entropy-driven relaxation behavior in the bimodal Ising glass. In the case of a continuous coupling distribution, our results presented here also indicate that, although Kibble-Zurek scaling holds, the perturbative behavior normally applying in the slow limit breaks down, likely due to quasidegenerate states, and the scaling function takes a different form."
ANDERS W SANDVIK,Modulated phases in a three-dimensional Maier-Saupe model with competing interactions,"This work is dedicated to the study of the discrete version of the Maier-Saupe model in the presence of competing interactions. The competition between interactions favoring different orientational ordering produces a rich phase diagram including modulated phases. Using a mean-field approach and Monte Carlo simulations, we show that the proposed model exhibits isotropic and nematic phases and also a series of modulated phases that meet at a multicritical point, a Lifshitz point. Though the Monte Carlo and mean-field phase diagrams show some quantitative disagreements, the Monte Carlo simulations corroborate the general behavior found within the mean-field approximation."
ANDERS W SANDVIK,Dynamic scaling of topological ordering in classical systems,"We analyze scaling behaviors of simulated annealing carried out on various classical systems with topological order, obtained as appropriate limits of the toric code in two and three dimensions. We first consider the three-dimensional Z2 (Ising) lattice gauge model, which exhibits a continuous topological phase transition at finite temperature. We show that a generalized Kibble-Zurek scaling ansatz applies to this transition, in spite of the absence of a local order parameter. We find perimeter-law scaling of the magnitude of a nonlocal order parameter (defined using Wilson loops) and a dynamic exponent z=2.70±0.03, the latter in good agreement with previous results for the equilibrium dynamics (autocorrelations). We then study systems where (topological) order forms only at zero temperature—the Ising chain, the two-dimensional Z2 gauge model, and a three-dimensional star model (another variant of the Z2 gauge model). In these systems the correlation length diverges exponentially, in a way that is nonsmooth as a finite-size system approaches the zero temperature state. We show that the Kibble-Zurek theory does not apply in any of these systems. Instead, the dynamics can be understood in terms of diffusion and annihilation of topological defects, which we use to formulate a scaling theory in good agreement with our simulation results. We also discuss the effect of open boundaries where defect annihilation competes with a faster process of evaporation at the surface."
ANDERS W SANDVIK,Insulator to superfluid transition in coupled photonic cavities in two dimensions,"A system of coupled photonic cavities on a two-dimensional square lattice is systematically investigated using the stochastic series expansion quantum Monte Carlo method. The ground state phase diagram contains insulating phases with integer polariton densities surrounded by a superfluid phase. The finite-size scaling of the superfluid density is used to determine the phase boundaries accurately. We find that the critical behavior is that of the generic, density-driven Mott-superfluid transition with dynamic exponent z=2, with no special multicritical points with z=1 at the tips of the insulating-phase lobes (as exist in the case of the Bose-Hubbard model). This demonstrates a limitation of the description of polaritons as structureless bosons."
ANDERS W SANDVIK,Dynamical properties of the S=1/2 random Heisenberg chain,"We study dynamical properties at finite temperature (T) of Heisenberg spin chains with random antiferromagnetic exchange couplings, which realize the random singlet phase in the low-energy limit, using three complementary numerical methods: exact diagonalization, matrix-product-state algorithms, and stochastic analytic continuation of quantum Monte Carlo results in imaginary time. Specifically, we investigate the dynamic spin structure factor S(q,ω) and its ω→0 limit, which are closely related to inelastic neutron scattering and nuclear magnetic resonance (NMR) experiments (through the spin-lattice relaxation rate 1/T1). Our study reveals a continuous narrow band of low-energy excitations in S(q,ω), extending throughout the q space, instead of being restricted to q≈0 and q≈π as found in the uniform system. Close to q=π, the scaling properties of these excitations are well captured by the random-singlet theory, but disagreements also exist with some aspects of the predicted q dependence further away from q=π. Furthermore we also find spin diffusion effects close to q=0 that are not contained within the random-singlet theory but give non-negligible contributions to the mean 1/T1. To compare with NMR experiments, we consider the distribution of the local relaxation rates 1/T1. We show that the local 1/T1 values are broadly distributed, approximately according to a stretched exponential. The mean 1/T1 first decreases with T, but below a crossover temperature it starts to increase and likely diverges in the limit of a small nuclear resonance frequency ω0. Although a similar divergent behavior has been predicted and experimentally observed for the static uniform susceptibility, this divergent behavior of the mean 1/T1 has never been experimentally observed. Indeed, we show that the divergence of the mean 1/T1 is due to rare events in the disordered chains and is concealed in experiments, where the typical 1/T1 value is accessed."
ANDERS W SANDVIK,Metamagnetism and zero-scale-factor universality in the two-dimensional J - Q model,"Using a combination of quantum Monte Carlo and exact methods, we study the field-driven saturation transition of the two-dimensional J − Q model, in which the antiferromagnetic Heisenberg exchange ( J ) coupling competes with an additional four-spin interaction ( Q ) that favors valence-bond solid order. For small values of Q , the saturation transition is continuous and is expected to be governed by zero-scale-factor universality at its upper critical dimension, with a specific form of logarithmic corrections to scaling (first proposed by Sachdev et al. [Phys. Rev. B 50, 258 (1994)]). Our results conform to this expectation, but the logarithmic corrections to scaling do not match the form predicted by Sachdev et al. We also show that the saturation transition becomes first order above a critical coupling ratio ( Q / J ) min and is accompanied by magnetization jumps—metamagnetism. We obtain an exact solution for ( Q / J ) min using a high magnetization expansion, and confirm the existence of the magnetization jumps beyond this value of coupling using quantum Monte Carlo simulations."
ANDERS W SANDVIK,Bond-order-wave phase and quantum phase transitions in the one-dimensional extended Hubbard model,"We use a stochastic series-expansion quantum Monte Carlo method to study the phase diagram of the one-dimensional extended Hubbard model at half-filling for small to intermediate values of the on-site U and nearest-neighbor V repulsions. We confirm the existence of a novel, long-range-ordered bond-order-wave (BOW) phase recently predicted by Nakamura [J. Phys. Soc. Jpn. 68, 3123 (1999)] in a small region of the parameter space between the familiar charge-density-wave (CDW) state for V≳U/2 and the state with dominant spin-density-wave (SDW) fluctuations for V≲U/2. We discuss the nature of the transitions among these states and evaluate some of the critical exponents. Further, we determine accurately the position of the multicritical point, (Um,Vm)=(4.7±0.1,2.51±0.04) (in energy units where the hopping integral is normalized to unity), above which the two continuous SDW-BOW-CDW transitions are replaced by one discontinuous (first-order) direct SDW-CDW transition. We also discuss the evolution of the CDW and BOW states upon hole doping. We find that in both cases the ground state is a Luther-Emery liquid, i.e., the spin gap remains but the charge gap existing at half-filling is immediately closed upon doping. The charge and bond-order correlations decay with distance r as r−ᴷρ, where Kρ is approximately 0.5 for the parameters we have considered. We also discuss advantages of using parallel tempering (or exchange Monte Carlo)—an extended ensemble method that we here combine with quantum Monte Carlo—in studies of quantum phase transitions."
ANDERS W SANDVIK,Spin-Peierls transition in the Heisenberg chain with finite-frequency phonons,"We study the spin-Peierls transition in a Heisenberg spin chain coupled to optical bond phonons. Quantum Monte Carlo results for systems with up to N=256 spins show unambiguously that the transition occurs only when the spin-phonon coupling α exceeds a critical value α_c. Using sum rules, we show that the phonon spectral function has divergent (for N→∞) weight extending to zero frequency for α<α_c. The phonon correlations decay with distance r as 1/r. This behavior is characteristic for all 0<α<α_c and the q=π phonon does not soften (to zero frequency) at α=α_c."
ANDERS W SANDVIK,"Comment on ""Ground-state phase diagram of a half-filled one-dimensional extended Hubbard model""","In Phys. Rev. Lett. 89, 236401 (2002), Jeckelmann argued that the recently discovered bond-order-wave (BOW) phase of the 1D extended Hubbard model does not have a finite extent in the (U,V) plane, but exists only on a segment of a first-order SDW-CDW phase boundary. We here present quantum Monte Carlo result of higher precision and for larger system sizes than previously and reconfirm that the BOW phase does exist a finite distance away from the phase boundary, which hence is a BOW-CDW transition curve."
ANDERS W SANDVIK,Emergent O(4) symmetry at the phase transition from plaquette-singlet to antiferromagnetic order in quasi-two-dimensional quantum magnets*,"Recent experiments [Guo et al., Phys. Rev. Lett. 124 206602 (2020)] on thermodynamic properties of the frustrated layered quantum magnet SrCu_2(BO_3)_2 — the Shastry–Sutherland material — have provided strong evidence for a low-temperature phase transition between plaquette-singlet and antiferromagnetic order as a function of pressure. Further motivated by the recently discovered unusual first-order quantum phase transition with an apparent emergent O(4) symmetry of the antiferromagnetic and plaquette-singlet order parameters in a two-dimensional “checkerboard J-Q” quantum spin model [Zhao et al., Nat. Phys. 15 678 (2019)], we here study the same model in the presence of weak inter-layer couplings. Our focus is on the evolution of the emergent symmetry as the system crosses over from two to three dimensions and the phase transition extends from strictly zero temperature in two dimensions up to finite temperature as expected in SrCu_2(BO_3)_2. Using quantum Monte Carlo simulations, we map out the phase boundaries of the plaquette-singlet and antiferromagnetic phases, with particular focus on the triple point where these two ordered phases meet the paramagnetic phase for given strength of the inter-layer coupling. All transitions are first-order in the neighborhood of the triple point. We show that the emergent O(4) symmetry of the coexistence state breaks down clearly when the interlayer coupling becomes sufficiently large, but for a weak coupling, of the magnitude expected experimentally, the enlarged symmetry can still be observed at the triple point up to significant length scales. Thus, it is likely that the plaquette-singlet to antiferromagnetic transition in SrCu_2(BO_3)_2 exhibits remnants of emergent O(4) symmetry, which should be observable due to additional weakly gapped Goldstone modes."
ANDERS W SANDVIK,Z2 topological order and first-order quantum phase transitions in systems with combinatorial gauge symmetry topological order and first-order quantum phase transitions in systems with combinatorial gauge symmetry,"We study a generalization of the two-dimensional transverse-field Ising model, combining both ferromagnetic and antiferromagnetic two-body interactions, that hosts exact global and local Z 2 gauge symmetries. Using exact diagonalization and stochastic series expansion quantum Monte Carlo methods, we confirm the existence of the topological phase in line with previous theoretical predictions. Our simulation results show that the transition between the confined topological phase and the deconfined paramagnetic phase is of first order, in contrast to the conventional Z 2 lattice gauge model in which the transition maps onto that of the standard Ising model and is continuous. We further generalize the model by replacing the transverse field on the gauge spins with a ferromagnetic X X interaction while keeping the local gauge symmetry intact. We find that the Z 2 topological phase remains stable, while the paramagnetic phase is replaced by a ferromagnetic phase. The topological-ferromagnetic quantum phase transition is also of first order. For both models, we discuss the low-energy spinon and vison excitations of the topological phase and their avoided level crossings associated with the first-order quantum phase transitions."
ANDERS W SANDVIK,Fractional and composite excitations of antiferromagnetic quantum spin trimer chains,"Using quantum Monte Carlo, exact diagonalization, and perturbation theory, we study the spectrum of the S = 1/2 antiferromagnetic Heisenberg trimer chain by varying the ratio g = J2/J1 of the intertrimer and intratrimer coupling strengths. The doublet ground states of trimers form effective interacting S = 1/2 degrees of freedom described by a Heisenberg chain. Therefore, the conventional two-spinon continuum of width ∝ J1 when g = 1 evolves into to a similar continuum of width ∝ J2 when g → 0. The intermediate-energy and high-energy modes are termed doublons and quartons which fractionalize with increasing g to form the conventional spinon continuum. In particular, at g ≈ 0.716, the gap between the low-energy spinon branch and the high-energy band with mixed doublons, quartons, and spinons closes. These features should be observable in inelastic neutron scattering experiments if a quasi-one-dimensional quantum magnet with the linear trimer structure and J2 < J1 can be identified. Our results may open a window for exploring the high-energy fractional excitations."
ANDERS W SANDVIK,"Temperature dependence of the (π,0) anomaly in the excitation spectrum of the 2D quantum Heisenberg antiferromagnet","It is well established that in the low-temperature limit, the two-dimensional quantum Heisenberg antiferromagnet on a square lattice (2DQHAFSL) exhibits an anomaly in its spectrum at short-wavelengths on the zone-boundary. In the vicinity of the (π,0) point the pole in the one-magnon response exhibits a downward dispersion, is heavily damped and attenuated, giving way to an isotropic continuum of excitations extending to high energies. The origin of the anomaly and the presence of the continuum are of current theoretical interest, with suggestions focused around the idea that the latter evidences the existence of spinons in a two-dimensional system. Here we present the results of neutron inelastic scattering experiments and Quantum Monte Carlo calculations on the metallo-organic compound Cu(DCOO)2⋅4D2O (CFTD), an excellent physical realisation of the 2DQHAFSL, designed to investigate how the anomaly at (π,0) evolves up to finite temperatures T/J∼2/3. Our data reveal that on warming the anomaly survives the loss of long-range, three-dimensional order, and is thus a robust feature of the two-dimensional system. With further increase of temperature the zone-boundary response gradually softens and broadens, washing out the (π,0) anomaly. This is confirmed by a comparison of our data with the results of finite-temperature Quantum Monte Carlo simulations where the two are found to be in good accord. At lower energies, in the vicinity of the antiferromagnetic zone centre, there was no significant softening of the magnetic excitations over the range of temperatures investigated."
ANDERS W SANDVIK,Ground state phases of the half-filled one-dimensional extended Hubbard model,"Using quantum Monte Carlo simulations, results of a strong-coupling expansion, and Luttinger liquid theory, we determine quantitatively the ground state phase diagram of the one-dimensional extended Hubbard model with on-site and nearest-neighbor repulsions U and V. We show that spin frustration stabilizes a bond-ordered (dimerized) state for U≈V/2 up to U/t≈9, where t is the nearest-neighbor hopping. The transition from the dimerized state to the staggered charge-density-wave state for large V/U is continuous for U≲5.5 and first order for higher U."
ANDERS W SANDVIK,Quantum versus classical annealing: insights from scaling theory and results for spin glasses on 3-regular graphs,"We discuss an Ising spin glass where each S=1/2 spin is coupled antiferromagnetically to three other spins (3-regular graphs). Inducing quantum fluctuations by a time-dependent transverse field, we use out-of-equilibrium quantum Monte Carlo simulations to study dynamic scaling at the quantum glass transition. Comparing the dynamic exponent and other critical exponents with those of the classical (temperature-driven) transition, we conclude that quantum annealing is less efficient than classical simulated annealing in bringing the system into the glass phase. Quantum computing based on the quantum annealing paradigm is therefore inferior to classical simulated annealing for this class of problems. We also comment on previous simulations where a parameter is changed with the simulation time, which is very different from the true Hamiltonian dynamics simulated here."
ANDERS W SANDVIK,Universal dynamic scaling in three-dimensional Ising spin glasses,"We use a nonequilibrium Monte Carlo simulation method and dynamical scaling to study the phase transition in three-dimensional Ising spin glasses. The transition point is repeatedly approached at finite velocity v (temperature change versus time) in Monte Carlo simulations starting at a high temperature. This approach has the advantage that the equilibrium limit does not have to be strictly reached for a scaling analysis to yield critical exponents. For the dynamic exponent we obtain z=5.85(9) for bimodal couplings distribution and z=6.00(10) for the Gaussian case. Assuming universal dynamic scaling, we combine the two results and obtain z=5.93±0.07 for generic 3D Ising spin glasses."
ANDERS W SANDVIK,Deconfined quantum criticality in spin-1/2 chains with long-range interactions,"We study spin-1/2 chains with long-range power-law decaying unfrustrated (bipartite) Heisenberg exchange J_r ∝ r^-𝛂 and multi-spin interactions Q favoring a valence-bond solid (VBS) ground state. Employing quantum Monte Carlo techniques and Lanczos diagonalization, we analyze order parameters and excited-state level crossings to characterize quantum states and phase transitions in the (𝛂,Q) plane. For weak Q and sufficiently slowly decaying Heisenberg interactions (small 𝛂), the system has a long-range-ordered antiferromagnetic (AFM) ground state, and upon increasing 𝛂 there is a continuous transition into a quasi long-range ordered (QLRO) critical state of the type in the standard Heisenberg chain. For rapidly decaying long-range interactions, there is transition between QLRO and VBS ground states of the same kind as in the frustrated J_1-J_2 Heisenberg chain. Our most important finding is a direct continuous quantum phase transition between the AFM and VBS states - a close analogy to the 2D deconfined quantum-critical point. In previous 1D analogies the ordered phases both have gapped fractional excitations, and the critical point is a conventional Luttinger-Liquid. In our model the excitations fractionalize upon transitioning from the AFM state, changing from spin waves to deconfined spinons. We extract critical exponents at the AFM-VBS transition and use order-parameter distributions to study emergent symmetries. We find emergent O(4) symmetry of the O(3) AFM and scalar VBS order parameters. Thus, the order parameter fluctuations exhibit the covariance of a uniaxially deformed O(4) sphere (an ""elliptical"" symmetry). This unusual quantum phase transition does not yet have any known field theory description, and our detailed results can serve to guide its construction. We discuss possible experimental realizations."
ANDERS W SANDVIK,Quantum phases of SrCu₂(BO₃)₂ from high-pressure thermodynamics,"We report heat capacity measurements of SrCu₂(BO₃)₂ under high pressure along with simulations of relevant quantum spin models and map out the (P,T) phase diagram of the material. We find a first-order quantum phase transition between the low-pressure quantum dimer paramagnet and a phase with signatures of a plaquette-singlet state below T=2  K. At higher pressures, we observe a transition into a previously unknown antiferromagnetic state below 4 K. Our findings can be explained within the two-dimensional Shastry-Sutherland quantum spin model supplemented by weak interlayer couplings. The possibility to tune SrCu₂(BO₃)₂ between the plaquette-singlet and antiferromagnetic states opens opportunities for experimental tests of quantum field theories and lattice models involving fractionalized excitations, emergent symmetries, and gauge fluctuations."
ANDERS W SANDVIK,NMR relaxation in the spin-1 Heisenberg chain,"We consider the isotropic S=1 Heisenberg chain with a finite Haldane gap Δ and use state-of-the-art numerical techniques to investigate its dynamical properties at finite temperature, focusing on the nuclear spin-lattice relaxation rate 1/T1 measured in nuclear magnetic resonance (NMR) experiments, for instance. In particular, we analyze the contributions from modes with momenta close to q≈0 and q≈π as a function of temperature. At high-temperature we observe spin diffusion, while at low-temperature we argue that a simple activated behavior 1/T1∝exp(−Δ/T) can be observed only at temperatures much smaller than the gap Δ."
ANDERS W SANDVIK,Scaling and diabatic effects in quantum annealing with a D-Wave device,"We discuss quantum annealing of the two-dimensional transverse-field Ising model on a D-Wave device, encoded on L×L lattices with L≤32. Analyzing the residual energy and deviation from maximal magnetization in the final classical state, we find an optimal L dependent annealing rate v for which the two quantities are minimized. The results are well described by a phenomenological model with two powers of v and L-dependent prefactors to describe the competing effects of reduced quantum fluctuations (for which we see evidence of the Kibble-Zurek mechanism) and increasing noise impact when v is lowered. The same scaling form also describes results of numerical solutions of a transverse-field Ising model with the spins coupled to noise sources. We explain why the optimal annealing time is much longer than the coherence time of the individual qubits."
ANDERS W SANDVIK,Bose-Einstein condensation of deconfined spinons in two dimensions,"The transition between the Néel antiferromagnet and the valence-bond solid state in two dimensions has become a paradigmatic example of deconfined quantum criticality, a non-Landau transition characterized by fractionalized excitations (spinons). We consider an extension of this scenario whereby the deconfined spinons are subject to a magnetic field. The primary purpose is to identify the exotic scenario of a Bose-Einstein condensate of spinons. We employ quantum Monte Carlo simulations of the J−Q model with a magnetic field, and we perform a quantum field theoretic analysis of the magnetic field and temperature dependence of thermodynamic quantities. The combined analysis provides evidence for Bose-Einstein condensation of spinons and also demonstrates an extended temperature regime in which the system is best described as a gas of spinons interacting with an emergent gauge field."
ANDERS W SANDVIK,Existence of a spectral gap in the Affleck-Kennedy-Lieb-Tasaki model on the hexagonal lattice,"The S=1 Affleck-Kennedy-Lieb-Tasaki (AKLT) quantum spin chain was the first rigorous example of an isotropic spin system in the Haldane phase. The conjecture that the S=3/2 AKLT model on the hexagonal lattice is also in a gapped phase has remained open, despite being a fundamental problem of ongoing relevance to condensed-matter physics and quantum information theory. Here we confirm this conjecture by demonstrating the size-independent lower bound Δ>0.006 on the spectral gap of the hexagonal model with periodic boundary conditions in the thermodynamic limit. Our approach consists of two steps combining mathematical physics and high-precision computational physics. We first prove a mathematical finite-size criterion which gives an analytical, size-independent bound on the spectral gap if the gap of a particular cut-out subsystem of 36 spins exceeds a certain threshold value. Then we verify the finite-size criterion numerically by performing state-of-the-art DMRG calculations on the subsystem."
ANDERS W SANDVIK,Hilbert space fragmentation and Ashkin-Teller criticality in fluctuation coupled Ising models,We discuss the effects of exponential fragmentation of the Hilbert space on phase transitions in the context of coupled ferromagnetic Ising models in arbitrary dimension with special emphasis on the one-dimensional case. We show that the dynamics generated by quantum fluctuations is bounded within spatial partitions of the system and weak mixing of these partitions caused by global transverse fields leads to a zero temperature phase with ordering in the local product of both Ising copies but no long-range order in either species. This leads to a natural connection with the Ashkin-Teller universality class for general lattices. We confirm this for the periodic chain using quantum Monte Carlo simulations. We also point out that our treatment provides an explanation for pseudo-first-order behavior seen in the Binder cumulants of the classical frustrated J1−J2 Ising model and the q=4 Potts model in two dimensions.
ANDERS W SANDVIK,Monte Carlo renormalization flows in the space of relevant and irrelevant operators: application to three-dimensional clock models,"We study renormalization group flows in a space of observables computed by Monte Carlo simulations. As an example, we consider three-dimensional clock models, i.e., the XY spin model perturbed by a Z_{q} symmetric anisotropy field. For q=4, 5, 6, a scaling function with two relevant arguments describes all stages of the complex renormalization flow at the critical point and in the ordered phase, including the crossover from the U(1) Nambu-Goldstone fixed point to the ultimate Z_{q} symmetry-breaking fixed point. We expect our method to be useful in the context of quantum-critical points with inherent dangerously irrelevant operators that cannot be tuned away microscopically but whose renormalization flows can be analyzed as we do here for the clock models."
ANDERS W SANDVIK,Comment on “Gapless spin liquid ground state of the spin- 12 J1−J2 Heisenberg model on square lattices”,"Liu et al. [Phys. Rev. B 98, 241109 (2018)] used Monte Carlo sampling of the physical degrees of freedom of a projected entangled pair state type wave function for the S=1/2 frustrated J1−J2 Heisenberg model on the square lattice and found a nonmagnetic state argued to be a gapless spin liquid when the coupling ratio g=J2/J1 is in the range g∈[0.42,0.6]. Here we show that their definition of the order parameter for another candidate ground state within this coupling window—a spontaneously dimerized state—is problematic. The order parameter as defined will not detect dimer order when lattice symmeties are broken due to open boundaries or asymmetries originating from the calculation itself. Thus, a dimerized phase for some range of g cannot be excluded (and is likely based on several other recent works)."
ANDERS W SANDVIK,"Valence-bond solids, vestigial order, and emergent SO(5) symmetry in a two-dimensional quantum magnet","We introduce a quantum spin-1/2 model with many-body correlated Heisenberg-type interactions on the two-dimensional square lattice, designed so that the system can host a fourfold degenerate plaquette valence-bond solid (PVBS) ground state that spontaneously breaks Z4 symmetry. The system is sign-problem free and amenable to large-scale quantum Monte Carlo simulations, thus allowing us to carry out a detailed study of the quantum phase transition between the standard Néel antiferromagnetic (AFM) and PVBS states. We find a first-order transition, in contrast to previously studied continuous transitions from the AFM phase into a columnar valence-bond solid (CVBS) phase. The theory of deconfined quantum criticality predicts generic continuous AFM-CVBS and AFM-PVBS transitions, and, in one version of the theory, the two critical order parameters transform under SO(5) symmetry. Emergent SO(5) symmetry has indeed been observed in studies of the AFM-CVBS transition, and here we show that the first-order AFM-PVBS transition also exhibits SO(5) symmetry at the transition point. Such unexpected symmetry of the coexistence state, which implies a lack of energy barriers between the coexisting phases, has recently been observed at other first-order transitions, but the case presented here is the first example with SO(5) symmetry. The extended symmetry may indicate that the transition is connected to a deconfined critical point. We also discuss the first-order transition in the context of a recent proposal of spinons with fracton properties in the PVBS state, concluding that the fracton scenario is unlikely. Furthermore, we discover a novel type of eightfold degenerate VBS phase, arising when the PVBS state breaks a remaining Z2 symmetry. This second phase transition, which is continuous, implies that the PVBS phase can be regarded as an intermediate “vestigial” phase, a concept recently introduced to describe multistage phase transitions involving a continuous symmetry. Here we construct a six-dimensional order parameter and also introduce a general graph-theoretic approach to describe the two-stage discrete symmetry breaking. We discuss different ways of breaking the symmetries in one or two stages at zero and finite temperatures. In the latter case, we observe fluctuation-induced first-order transitions, which are hallmarks of vestigial phase transitions. We also mention possible connections of the AFM-PVBS transition to the SO(5) theory of high-T_c superconductivity."
ANDERS W SANDVIK,Unconventional U(1) to Zq crossover in quantum and classical q -state clock models,"We consider two-dimensional q-state quantum clock models with quantum fluctuations connecting states with all-to-all clock transitions with different choices for the matrix elements. We study the quantum phase transitions in these models using quantum Monte Carlo simulations and finite-size scaling, with the aim of characterizing the crossover from emergent U(1) symmetry at the transition (for q≥4) to Zq symmetry of the ordered state. We also study classical three-dimensional clock models with spatial anisotropy corresponding to the space-time anisotropy of the quantum systems. The U(1) to Zq symmetry crossover in all these systems is governed by a so-called dangerously irrelevant operator. We specifically study q=5 and q=6 models with different forms of the quantum fluctuations and different anisotropies in the classical models. In all cases, we find the expected classical XY critical exponents and scaling dimensions yq of the clock fields. However, the initial weak violation of the U(1) symmetry in the ordered phase, characterized by a Zq symmetric order parameter ϕq, scales in an unexpected way. As a function of the system size (length) L, close to the critical temperature ϕq∝Lp, where the known value of the exponent is p=2 in the classical isotropic clock model. In contrast, for strongly anisotropic classical models and the quantum models, we find p=3. For weakly anisotropic classical models, we observe a crossover from p=2 to p=3 scaling. The exponent p directly impacts the exponent ν′ governing the divergence of the U(1) to Zq crossover length scale ξ′ in the thermodynamic limit, according to the relationship ν′=ν(1+|yq|/p), where ν is the conventional correlation length exponent. We present a phenomenological argument for p=3 based on an anomalous renormalization of the clock field in the presence of anisotropy, possibly as a consequence of topological (vortex) line defects. Thus, our study points to an intriguing interplay between conventional and dangerously irrelevant perturbations, which may also affect other quantum systems with emergent symmetries."
ANDERS W SANDVIK,Multicritical deconfined quantum criticality and Lifshitz point of a helical valence-bond phase,"The S=1/2 square-lattice J-Q model hosts a deconfined quantum phase transition between antiferromagnetic and dimerized (valence-bond solid) ground states. We here study two deformations of this model-a term projecting staggered singlets, as well as a modulation of the J terms forming alternating ""staircases"" of strong and weak couplings. The first deformation preserves all lattice symmetries. Using quantum Monte Carlo simulations, we show that it nevertheless introduces a second relevant field, likely by producing topological defects. The second deformation induces helical valence-bond order. Thus, we identify the deconfined quantum critical point as a multicritical Lifshitz point-the end point of the helical phase and also the end point of a line of first-order transitions. The helical-antiferromagnetic transitions form a line of generic deconfined quantum-critical points. These findings extend the scope of deconfined quantum criticality and resolve a previously inconsistent critical-exponent bound from the conformal-bootstrap method."
ANDERS W SANDVIK,Quantum-critical scaling properties of the two-dimensional random-singlet state,"Using quantum Monte Carlo simulations, we study effects of disorder on the S=1/2 Heisenberg model with exchange constant J on the square lattice supplemented by multispin interactions Q. It was found recently [L. Liu et al., Phys. Rev. X 8, 041040 (2018)] that the ground state of this J−Q model with random couplings undergoes a quantum phase transition from the Néel antiferromagnetic state into a randomness-induced spin-liquid-like state that is a close analog to the well known random-singlet (RS) state of the Heisenberg chain with random couplings. This 2D RS state arises from a spontaneously symmetry-broken fourfold degenerate columnar valence-bond solid that is broken up by the disorder into finite domains, with spinons localized at topological defects. The interacting spinons form a critical collective many-body state without magnetic long range order but with the mean spin-spin correlations decaying with distance r as r−2, as in the one-dimensional RS state. The dynamic exponent z≥2, varying continuously with the model parameters. In this work, we further investigate the properties of the RS state in the J−Q model with random Q couplings. We study the temperature dependence of the specific heat and various susceptibilities for large enough systems to reach the thermodynamic limit. We also analyze the size dependence of the critical magnetic order parameter and its susceptibility in the ground state. For all these quantities, we find consistency with the conventional quantum-critical scaling laws when the condition implied by the r−2 form of the spin correlations is imposed. In particular, all the different quantities can be explained by the same value of the dynamic exponent z at fixed model parameters. We argue that the RS state identified in the J−Q model corresponds to a generic renormalization group fixed point that can be reached in many quantum magnets with random couplings and that it has already been observed experimentally."
ANDERS W SANDVIK,Fractional and composite excitations of antiferromagnetic quantum spin trimer chains,"Using Lanczos exact diagonalization, stochastic analytic continuation of quantum Monte Carlo data, and perturbation theory, we investigate the dynamic spin structure factor S(q, 𝜔) of the S=1/2 antiferromagnetic Heisenberg trimer chain. We systematically study the evolution of the spectrum by varying the ratio g=J_2/J_1 of the intertrimer and intratrimer coupling strengths and interpret the observed features using analytical and numerical calculations with the trimer eigenstates. The doublet ground states of the trimers form effective interacting S=1/2 degrees of freedom described by a Heisenberg chain with coupling J_eff=(4/9)J_2. Therefore, the conventional two-spinon continuum of width ∝ J_1 when g =1 evolves into to a similar continuum of width ∝ J_2 in the reduced Brillouin zone when g ⟶ 0. The high-energy modes (at 𝜔 ∝ J_1) for g ≈ 0.5 can be understood as weakly dispersing propagating internal trimer excitations (which we term doublons and quartons), and these fractionalize with increasing g to form the conventional spinon continuum when g is increased toward 1. The coexistence of two kinds of emergent spinon branches for intermediate values of g give rise to interesting spectral signatures, especially at g ≈ 0.7 where the gap between the low-energy spinon branch and the high energy band of mixed doublons, quartons, and spinons closes. These features should be observable in inelastic neutron scattering experiments if a quasi-one-dimensional quantum magnet with the linear trimer structure and J_2 < J_1 can be identified. We suggest that finding such materials would be useful, enabling detailed studies of coexisting exotic excitations and their interplay within a relatively simple theoretical framework."
ANDERS W SANDVIK,Tunable deconfined quantum criticality and interplay of different valence-bond solid phases,"We use quantum Monte Carlo simulations to study an S = 1/2 spin model with competing multi-spin interactions. We find a quantum phase transition between a columnar valence-bond solid (cVBS) and a Néel antiferromagnet (AFM), as in the scenario of deconfined quantum-critical points, as well as a transition between the AFM and a staggered valence-bond solid (sVBS). By continuously varying a parameter, the sVBS–AFM and AFM–cVBS boundaries merge into a direct sVBS–cVBS transition. Unlike previous models with putative deconfined AFM–cVBS transitions, e.g., the standard J–Q model, in our extended J–Q model with competing cVBS and sVBS inducing terms the transition can be tuned from continuous to first-order. We find the expected emergent U(1) symmetry of the microscopically Z4 symmetric cVBS order parameter when the transition is continuous. In contrast, when the transition changes to first-order, the clock-like Z4 fluctuations are absent and there is no emergent higher symmetry. We argue that the confined spinons in the sVBS phase are fracton-like. We also present results for an SU(3) symmetric model with a similar phase diagram. The new family of models can serve as a useful tool for further investigating open questions related to deconfined quantum criticality and its associated emergent symmetries."
ANDERS W SANDVIK,Extreme suppression of antiferromagnetic order and critical scaling in a two-dimensional random quantum magnet,"Sr_2CuTeO_6 is a square-lattice Néel antiferromagnet with superexchange between first-neighbor S=1/2 Cu spins mediated by plaquette centered Te ions. Substituting Te by W, the affected impurity plaquettes have predominantly second-neighbor interactions, thus causing local magnetic frustration. Here we report a study of Sr_2CuTe_1-xW_xO_6 using neutron diffraction and μSR techniques, showing that the Néel order vanishes already at x=0.025±0.005. We explain this extreme order suppression using a two-dimensional Heisenberg spin model, demonstrating that a W-type impurity induces a deformation of the order parameter that decays with distance as 1/r^2 at temperature T=0. The associated logarithmic singularity leads to loss of order for any x>0. Order for small x>0 and T>0 is induced by weak interplane couplings. In the nonmagnetic phase of Sr_2CuTe_1-x W_x O_6, the μSR relaxation rate exhibits quantum critical scaling with a large dynamic exponent, z≈3, consistent with a random-singlet state."
ANDERS W SANDVIK,Consistent scaling exponents at the deconfined quantum-critical point,"We report a quantum Monte Carlo study of the phase transition between antiferromagnetic and valence-bond solid ground states in the square-lattice S = 1/2 J–Q model. The critical correlation function of the Q terms gives a scaling dimension corresponding to the value ν = 0.455 ± 0.002 of the correlation-length exponent. This value agrees with previous (less precise) results from conventional methods, e.g., finite-size scaling of the near-critical order parameters. We also study the Q-derivatives of the Binder cumulants of the order parameters for L2 lattices with L up to 448. The slope grows as L1/ν with a value of ν consistent with the scaling dimension of the Q term. There are no indications of runaway flow to a first-order phase transition. The mutually consistent estimates of ν provide compelling support for a continuous deconfined quantum-critical point."
ANDERS W SANDVIK,Field-driven quantum phase transitions in S=1/2 spin chains,"We study the magnetization process of a 1D extended Heisenberg model, the J-Q model, as a function of an external magnetic field. In this model, J represents the traditional antiferromagnetic Heisenberg exchange and Q is the strength of a competing four-spin interaction. Without external field, this system hosts a twofold-degenerate dimerized (valence-bond solid) state above a critical value qc≈0.85 where q≡Q/J. The dimer order is destroyed and replaced by a partially polarized translationally invariant state at a critical field value. We find magnetization jumps (metamagnetism) between the partially polarized and fully polarized state for q>qmin, where we have calculated qmin=2/9 exactly. For q>qmin two magnons (flipped spins on a fully polarized background) attract and form a bound state. Quantum Monte Carlo studies confirm that the bound state corresponds to the first step of an instability leading to a finite magnetization jump for q>qmin. Our results show that neither geometric frustration nor spin-anisotropy are necessary conditions for metamagnetism. Working in the two-magnon subspace, we also find evidence pointing to the existence of metamagnetism in the unfrustrated J1-J2 chain (J1>0, J2<0), but only if J2 is spin-anisotropic. We also investigate quantum-critical scaling near the transition into the fully polarized state for q≤qmin at T>0. While the expected `zero-scale-factor' universality is clearly seen for q=0 and q≪qmin; closer to qmin we find that extremely low temperatures are required to observe the asymptotic behavior, due to the influence of the tricritical point at qmin, which leads to a cross-over at a temperature T∗(q) between logarithmic tricritical scaling and zero-scale-factor universality, with T∗(q)→0 when q→qmin."
ANDERS W SANDVIK,Anomalous quantum-critical scaling corrections in Two-dimensional antiferromagnets,"We study the Néel-paramagnetic quantum phase transition in two-dimensional dimerized S = 1 / 2 Heisenberg antiferromagnets using finite-size scaling of quantum Monte Carlo data. We resolve the long-standing issue of the role of cubic interactions arising in the bond-operator representation when the dimer pattern lacks a certain symmetry. We find nonmonotonic (monotonic) size dependence in the staggered (columnar) dimerized model, where cubic interactions are (are not) present. We conclude that there is a new irrelevant field in the staggered model, but, at variance with previous claims, it is not the leading irrelevant field. The new exponent is ω 2 ≈ 1.25 and the prefactor of the correction L − ω 2 is large and comes with a different sign from that of the conventional correction with ω 1 ≈ 0.78. Our study highlights competing scaling corrections at quantum critical points."
ANDERS W SANDVIK,Monte Carlo simulations of quantum spin systems in the valence bond basis,"We discuss a projector Monte Carlo method for quantum spin models formulated in the valence bond basis, using the S=1/2 Heisenberg antiferromagnet as an example. Its singlet ground state can be projected out of an arbitrary basis state as the trial state, but a more rapid convergence can be obtained using a good variational state. As an alternative to first carrying out a time consuming variational Monte Carlo calculation, we show that a very good trial state can be generated in an iterative fashion in the course of the simulation itself. We also show how the properties of the valence bond basis enable calculations of quantities that are difficult to obtain with the standard basis of Sz eigenstates. In particular, we discuss quantities involving finite-momentum states in the triplet sector, such as the dispersion relation and the spectral weight of the lowest triplet."
ANDERS W SANDVIK,Detecting signals of weakly first-order phase transitions in two-dimensional Potts models,"We investigate the first-order phase transitions of the q-state Potts models with q = 5, 6, 7, and 8 on the two- dimensional square lattice, using Monte Carlo simulations. At the very weakly first-order transition of the q = 5 system, the standard data-collapse procedure for the order parameter, carried out with results for a broad range of system sizes, works deceptively well and produces non-trivial critical exponents different from the trivial values expected for first- order transitions. However, we show a more systematic study on the “pseudo-critical” exponents as a function of the system size signals first-order phase transitions. We also derive a novel scaling behavior of Binder ratio based on a phenomenological theory for first-order transitions, which can detect the weakly first-order transitions in much smaller lattices than the correlation lengths. The results overall show that proper care is indispensable to diagnose the nature of a phase transition with limited system sizes."
ANDERS W SANDVIK,Symmetry enhanced first-order phase transition in a two-dimensional quantum magnet,Theoretical studies of quantum phase transitions have suggested critical points with higher symmetries than those of the underlying Hamiltonian. Here we demonstrate a surprising emergent symmetry of the coexistence state at a strongly discontinuous phase transition between two ordered ground states. We present a quantum Monte Carlo study of a two-dimensional S=1/2 quantum magnet hosting the antiferromagnetic (AFM) and plaquette-singlet solid (PSS) states recently detected in SrCu2(BO3)2. We observe that the O(3) symmetric AFM order and the Z2 symmetric PSS order form an O(4) vector at the transition. The control parameter g (a coupling ratio) rotates the vector between the AFM and PSS sectors and there are no energy barriers between the two at the transition point gc. This phenomenon may be observable in SrCu2(BO3)2.
ANDERS W SANDVIK,Progress on stochastic analytic continuation of quantum Monte Carlo data,"We report multipronged progress on the stochastic averaging approach to numerical analytic continuation of imaginary-time correlation functions computed by quantum Monte Carlo simulations. After reviewing the conventional maximum-entropy approach and established stochastic analytic continuation methods, we present several new developments in which the configurational entropy of the sampled spectrum plays a key role. Parametrizing the spectrum as a large number of 𝛿-functions in continuous frequency space, an exact calculation of the entropy lends support to a simple goodness-of-fit criterion for the optimal sampling temperature. We also compare spectra sampled in continuous frequency with those from amplitudes sampled on a fixed frequency grid. Insights into the functional form of the entropy in different cases allow us to demonstrate equivalence in a generalized thermodynamic limit (large number of degrees of freedom) of the average spectrum and the maximum-entropy solution, with different parametrizations corresponding to different forms of the entropy in the prior probability. These results revise prevailing notions of the maximum-entropy method and its relationship to stochastic analytic continuation. In further developments of the sampling approach, we explore various adjustable (optimized) constraints that allow sharp low-temperature spectral features to be resolved, in particular at the lower frequency edge. The constraints, e.g., the location of the edge or the spectral weight of a quasi-particle peak, are optimized using a statistical criterion based on entropy minimization under the condition of optimal fit. We show with several examples that this method can correctly reproduce both narrow and broad quasi-particle peaks. We next introduce a parametrization for more intricate spectral functions with sharp edges, e.g., power-law singularities. We present tests with synthetic data as well as with real simulation data for the spin-1/2 Heisenberg chain, where a divergent edge of the dynamic structure factor is due to deconfined spinon excitations. Our results demonstrate that distortions of sharp edges or quasi-particle peaks, which arise with other analytic continuation methods, propagate and cause artificial spectral features also at higher energies. The constrained sampling methods overcome this problem and allow analytic continuation of spectra with sharp edge features at unprecedented fidelity. We present results for 𝑆 = 1/2 Heisenberg 2-leg and 3-leg ladders to illustrate the ability of the methods to resolve spectral features arising from both elementary and composite excitations. Finally, we also propose how the methods developed here could be used as “pre processors” for analytic continuation by machine learning. Edge singularities and narrow quasi-particle peaks being ubiquitous in quantum many-body systems, we expect the new methods to be broadly useful and take numerical analytic continuation to a new quantitative level in many applications."
ANDERS W SANDVIK,Quantum Monte Carlo in the interaction representation: Application to a spin-Peierls model,"A quantum Monte Carlo algorithm is constructed starting from the standard perturbation expansion in the interaction representation. The resulting configuration space is strongly related to that of the stochastic series expansion (SSE) method, which is based on a direct power-series expansion of exp(−βH). Sampling procedures previously developed for the SSE method can therefore be used also in the interaction representation formulation. The method is tested on the S=1/2 Heisenberg chain. Then, as an application to a model of great current interest, a Heisenberg chain including phonon degrees of freedom is studied. Einstein phonons are coupled to the spins via a linear modulation of the nearest-neighbor exchange. The simulation algorithm is implemented in the phonon occupation-number basis, without Hilbert space truncations, and is exact. Results are presented for the magnetic properties of the system in a wide temperature regime, including the T→0 limit where the chain undergoes a spin-Peierls transition. Some aspects of the phonon dynamics are also discussed. The results suggest that the effects of dynamic phonons in spin-Peierls compounds such as GeCuO₃ and α′−NaV₂O₅ must be included in order to obtain a correct quantitative description of their magnetic properties, both above and below the dimerization temperature."
ANDERS W SANDVIK,Pseudoparticle description of the 1D Hubbard model electronic transport properties,We extend the pseudoparticle transport description of the Hubbard chain to all energy scales. In particular we compute the mean value of the electric current transported by any Bethe ansatz state and the transport masses of the charge carriers. We present numerical results for the optical conductivity of the model at half-filling for values of U/t = 3 and 4. We show that these are in good agreement with the pseudoparticle description of the finite-energy transitions involving new pseudoparticle energy bands.
ANDERS W SANDVIK,Possible exotic phases in the one-dimensional extended Hubbard model,"We investigate numerically the ground state phase diagram of the one-dimensional extended Hubbard model, including an on-site interaction U and a nearest-neighbor interaction V. We focus on the ground state phases of the model in the V≫U region, where previous studies have suggested the possibility of dominant superconducting pairing fluctuations before the system phase separates at a critical value V=V𝘱𝒔. Using quantum Monte Carlo methods on lattices much larger than in previous Lanczös diagonalization studies, we determine the boundary of phase separation, the Luttinger-liquid correlation exponent Kρ, and other correlation functions in this region. We find that phase separation occurs for V significantly smaller than previously reported. In addition, for negative U, we find that a uniform state reenters from phase separation as the electron density is increased towards half filling. For V<V𝘱𝑠, our results show that superconducting fluctuations are not dominant. The system behaves asymptotically as a Luttinger liquid with Kρ<1, but we also find strong low-energy (but gapped) charge-density fluctuations at a momentum not expected for a standard Luttinger liquid."
THOMAS D LITTLE,Optimal Scheduling of Secondary Content for Aggregation in Video-on-Demand Systems,"Dynamic service aggregation techniques can exploit skewed access popularity patterns to reduce the costs of building interactive VoD systems. These schemes seek to cluster and merge users into single streams by bridging the temporal skew between them, thus improving server and network utilization. Rate adaptation and secondary content insertion are two such schemes. In this paper, we present and evaluate an optimal scheduling algorithm for inserting secondary content in this scenario. The algorithm runs in polynomial time, and is optimal with respect to the total bandwidth usage over the merging interval. We present constraints on content insertion which make the overall QoS of the delivered stream acceptable, and show how our algorithm can satisfy these constraints. We report simulation results which quantify the excellent gains due to content insertion. We discuss dynamic scenarios with user arrivals and interactions, and show that content insertion reduces the channel bandwidth requirement to almost half. We also discuss differentiated service techniques, such as N-VoD and premium no-advertisement service, and show how our algorithm can support these as well."
THOMAS D LITTLE,Interference mitigation through user association and receiver field of view optimization in a multi-user indoor hybrid RF/VLC illuminance-constrained network,"In this paper we address interference mitigation through user association and receiver field of view (FOV) optimization in a multi-user indoor optical wireless communication (OWC) scenario. We explore several dynamic FOV receiver solutions including steerable (SDFOV) and non-steerable (DFOV) to optimize performance for multiple devices experiencing orientation dynamics. We compare their performance to a baseline fixed FOV receiver (FFOV). Through modeling and simulation we find that SDFOV receivers outperform DFOV by up to 2.6x and FFOV by up to 5.6x in terms of average minimum throughput gain using our test scenario. Similarly, DFOV receivers can achieve up to 2.2x gain over FFOV receivers. For multi-user environments, we compare the performance of coordinated versus distributed system control. Results show that in the worst case, the distributed greedy system performs on average 46%, 16%, and 57% below the coordinated system for SDFOV, DFOV, and FFOV, respectively at a reduced computational complexity compared to the centralized system. We also note that the performance gap in each system diminishes with increasing transmitter Lambertian order. This analysis is done under different room coverage achieved through optimizing the transmitted power to jointly maximize the minimum received power and the standard illuminance range probability at the working plane. Next, we show the impact of self- and random-human blockage at different Lambertian orders on the minimum and average user throughput values. Lastly, we show the gains from employing the hybrid RF/VLC network compared to a VLC-only mode for two different strategies: (1) minimum-throughput-enhancing and (2) sum-throughput-enhancing."
THOMAS D LITTLE,Indoor 3D localization with low-cost LiFi components,"Indoor positioning or localization is an enabling technology expected to have a profound impact on mobile applications. Various modalities of radio frequency, ultrasound, and light can be used for localization; in this paper we consider how visible light positioning can be realized for 3D positioning as a service comprised of optical sources as part of an overarching lighting infrastructure. Our approach, called Ray-Surface Positioning, uses one or more overhead luminaires, modulated as LiFi, and is used in conjunction with a steerable laser to realize position estimates in three dimensions. In this paper, we build and demonstrate Ray-Surface Positioning using low-cost commodity components in a test apparatus representing one quadrant of a 4m × 4m × 1m volume. Data are collected at regular intervals in the test volume representing 3D position estimates and is validated using a motion capture system. For the low-cost components used, results show position estimate errors of less than 30cm for 95% of the test volume. These results, generated with commodity components, show the potential for 3D positioning in the general case. When the plane of the receiver is known a priori, the position estimate error diminishes to the resolution of the steering mechanism."
THOMAS D LITTLE,Angle diversity to increase coverage and position accuracy in 3D visible light positioning,"The most common approach to light-based indoor positioning relies on multilateration of received signals to the mobile device. Any deficiencies in the fidelity of these light signals can significantly distort position estimates. In this paper, we propose a method to dynamically control the light distribution from the overhead luminaires to mitigate fading effects that would otherwise occur under static lighting. By manipulating the direction of the luminaire, effectively the dispersion pattern, we introduce signal diversity in the form of multiple pointing angles and light distributions. In addition to providing angle diversity, steering and then tracking sustains the maximal line-of-sight path between a source and receiver, which reduces angle-dependent attenuation and optimizes the signal-to-noise ratio for any coordinate without needing to change the physical properties of the source or receiver. This gain in signal strength combats the limited field-of-view of luminaires and photodiodes to provide better overall coverage, which translates directly to increase positioning accuracy, particularly in a 3D space. In the results, we show field-of-view gains of 43% and improvements in MSE of 20cm."
MICHAEL OTTO,A latent class analysis of parental bipolar disorder: examining associations with offspring psychopathology,"Bipolar disorder (BD) is highly heterogeneous, and course variations are associated with patient outcomes. This diagnostic complexity challenges identification of patients in greatest need of intervention. Additionally, course variations have implications for offspring risk. First, latent class analysis (LCA) categorized parents with BD based on salient illness characteristics: BD type, onset age, polarity of index episode, pole of majority of episodes, rapid cycling, psychosis, anxiety comorbidity, and substance dependence. Fit indices favored three parental classes with some substantively meaningful patterns. Two classes, labeled “Earlier-Onset Bipolar-I” (EO-I) and “Earlier-Onset Bipolar-II” (EO-II), comprised parents who had a mean onset age in mid-adolescence, with EO-I primarily BD-I parents and EO-II entirely BD-II parents. The third class, labeled “Later-Onset BD” (LO) had an average onset age in adulthood. Classes also varied on probability of anxiety comorbidity, substance dependence, psychosis, rapid cycling, and pole of majority of episodes. Second, we examined rates of disorders in offspring (ages 4–33, Mage=13.46) based on parental latent class membership. Differences emerged for offspring anxiety disorders only such that offspring of EO-I and EO-II parents had higher rates, compared to offspring of LO parents, particularly for daughters. Findings may enhance understanding of BD and its nosology"
MICHAEL OTTO,"D-cycloserine augmentation of exposure-based cognitive behavior therapy for anxiety, obsessive-compulsive, and posttraumatic stress disorders: a systematic review and meta-analysis of individual participant data","Importance: Whether and under which conditions D-cycloserine (DCS) augments the effects of exposure-based cognitive behavior therapy for anxiety, obsessive-compulsive, and posttraumatic stress disorders is unclear. Objective: To clarify whether DCS is superior to placebo in augmenting the effects of cognitive behavior therapy for anxiety, obsessive-compulsive, and posttraumatic stress disorders and to evaluate whether antidepressants interact with DCS and the effect of potential moderating variables. Data Sources: PubMed, EMBASE, and PsycINFO were searched from inception to February 10, 2016. Reference lists of previous reviews and meta-analyses and reports of randomized clinical trials were also checked. Study Selection: Studies were eligible for inclusion if they were (1) double-blind randomized clinical trials of DCS as an augmentation strategy for exposure-based cognitive behavior therapy and (2) conducted in humans diagnosed as having specific phobia, social anxiety disorder, panic disorder with or without agoraphobia, obsessive-compulsive disorder, or posttraumatic stress disorder. Data Extraction and Synthesis: Raw data were obtained from the authors and quality controlled. Data were ranked to ensure a consistent metric across studies (score range, 0-100). We used a 3-level multilevel model nesting repeated measures of outcomes within participants, who were nested within studies. Results: Individual participant data were obtained for 21 of 22 eligible trials, representing 1047 of 1073 eligible participants. When controlling for antidepressant use, participants receiving DCS showed greater improvement from pretreatment to posttreatment (mean difference, -3.62; 95% CI, -0.81 to -6.43; P = .01; d = -0.25) but not from pretreatment to midtreatment (mean difference, -1.66; 95% CI, -4.92 to 1.60; P = .32; d = -0.14) or from pretreatment to follow-up (mean difference, -2.98, 95% CI, -5.99 to 0.03; P = .05; d = -0.19). Additional analyses showed that participants assigned to DCS were associated with lower symptom severity than those assigned to placebo at posttreatment and at follow-up. Antidepressants did not moderate the effects of DCS. None of the prespecified patient-level or study-level moderators was associated with outcomes. Conclusions and Relevance: D-cycloserine is associated with a small augmentation effect on exposure-based therapy. This effect is not moderated by the concurrent use of antidepressants. Further research is needed to identify patient and/or therapy characteristics associated with DCS response."
MICHAEL OTTO,"Enhancement of psychosocial treatment with D-cycloserine: models, moderators, and future directions","Advances in the understanding of the neurobiology of fear extinction have resulted in the development of d-cycloserine (DCS), a partial glutamatergic N-methyl-D-aspartate agonist, as an augmentation strategy for exposure treatment. We review a decade of research that has focused on the efficacy of DCS for augmenting the mechanisms (e.g., fear extinction) and outcome of exposure treatment across the anxiety disorders. Following a series of small-scale studies offering strong support for this clinical application, more recent larger-scale studies have yielded mixed results, with some showing weak or no effects. We discuss possible explanations for the mixed findings, pointing to both patient and session (i.e., learning experiences) characteristics as possible moderators of efficacy, and offer directions for future research in this area. We also review recent studies that have aimed to extend the work on DCS augmentation of exposure therapy for the anxiety disorders to DCS enhancement of learning-based interventions for addiction, anorexia nervosa, schizophrenia, and depression. Here, we attend to both DCS effects on facilitating therapeutic outcomes and additional therapeutic mechanisms beyond fear extinction (e.g., appetitive extinction, hippocampal-dependent learning)."
MICHAEL OTTO,The desire to belong: Social identification as a predictor of treatment outcome in social anxiety disorder.,"OBJECTIVE: Perception of personal identity cannot be separated from the perception of the social context and one's social identity. Full involvement in group psychotherapy may require not only the awareness of personal impairment, but also social identification. The aim of the current study was to examine the association between social identification and symptom improvement in group-based psychotherapy. METHOD: 169 participants received 12 sessions of group-based cognitive behavioral therapy for social anxiety disorder. Social identification, the extent to which a person identifies with those who suffer from the same psychological problem as themselves and/or with those lacking psychopathology (non-sufferers), and clinical outcome were assessed at baseline, mid-and posttreatment, and 1, 3, and 6-months follow-up. RESULTS: At baseline, patients aspired for closeness with non-sufferers, and viewed themselves as distant from fellow sufferers and non-sufferers. After treatment, participants viewed not only themselves, but also other individuals with social anxiety, as closer to both non-sufferers and fellow sufferers. These ratings were related to clinical outcomes. CONCLUSIONS: The increase in closeness to both sufferers and non-sufferers across treatment may reflect a movement towards a more tolerant, less dichotomous and rigid, separation of ill and healthy that occurs with successful social anxiety treatment."
MICHAEL OTTO,Does d-cycloserine facilitate the effects of homework compliance on social anxiety symptom reduction?,"BACKGROUND: Prior studies examining the effect of d-cycloserine (DCS) on homework compliance and outcome in cognitive-behavior therapy (CBT) have yielded mixed results. The aim of this study was to investigate whether DCS facilitates the effects of homework compliance on symptom reduction in a large-scale study for social anxiety disorder (SAD). METHODS: 169 participants with generalized SAD received DCS or pill placebo during 12-session exposure-based group CBT. Improvements in social anxiety were assessed by independent raters at each session using the Liebowitz social anxiety scale (LSAS). RESULTS: Controlling for LSAS at the previous session, and irrespective of treatment condition, greater homework compliance in the week prior related to lower LSAS at the next session. However, DCS did not moderate the effect of homework compliance and LSAS, LSAS on homework compliance, or the overall augmenting effect of DCS on homework compliance. Furthermore, LSAS levels were not predictive of homework compliance in the following week. CONCLUSION: The findings support the general benefits of homework compliance on outcome, but not a DCS-augmenting effect. The comparably small number of DCS-enhanced sessions in this study could be one reason for the failure to find a facilitating effect of DCS."
MICHAEL OTTO,Dose timing of D-cycloserine to augment cognitive behavioral therapy for social anxiety: Study design and rationale,"The use of D-cycloserine (DCS) as a cognitive enhancer to augment exposure-based cognitive-behavioral therapy (CBT) represents a promising new translational research direction with the goal to accelerate and optimize treatment response for anxiety disorders. Some studies suggest that DCS may not only augment extinction learning but could also facilitate fear memory reconsolidation. Therefore, the effect of DCS may depend on fear levels reported at the end of exposure sessions. This paper presents the rationale and design for a randomized controlled trial examining the relative efficacy of tailoring DCS administration based on exposure success (i.e. end fear levels) during a 5-session group CBT protocol for social anxiety disorder (n = 156). Specifically, tailored post-session DCS administration will be compared against untailored post-session DCS, untailored pre-session DCS, and pill placebo in terms of reduction in social anxiety symptoms and responder status. In addition, a subset of participants (n = 96) will undergo a fear extinction retention experiment prior to the clinical trial in which they will be randomly assigned to receive either DCS or placebo prior to extinguishing a conditioned fear. The results from this experimental paradigm will clarify the mechanism of the effects of DCS on exposure procedures. This study aims to serve as the first step toward developing an algorithm for the personalized use of DCS during CBT for social anxiety disorder, with the ultimate goal of optimizing treatment outcome for anxiety disorders."
MICHAEL OTTO,Attending to Emotional Cues for Drug Abuse: Bridging the Gap Between Clinic and Home Behaviors,"Classical conditioning models of addiction provide keys to understanding the vexing discrepancy between substance abuse patients' desire to abstain when they are in therapy sessions and their tendency to relapse. Experiments using these models demonstrate the power of environmental relapse cues and support clinical approaches, including active exposure, aimed at helping patients recognize and withstand them. Internal cues, including emotions and somatic states such as withdrawal, can trigger urges as powerfully as external cues such as people, places, and things associated with prior abuse. The authors describe a cognitive-behavioral therapy approach that focuses on identifying and actively inducing each patient's high-risk emotions, then helping him or her develop and practice healthy responses. Clinical trials support the approach for patients with panic disorder who have trouble discontinuing benzodiazepines, and early trials suggest it may be useful for patients addicted to other drugs as well."
MICHAEL OTTO,"The therapeutic potential of exercise to improve mood, cognition, and sleep in Parkinson's disease","In addition to the classic motor symptoms, Parkinson's disease (PD) is associated with a variety of nonmotor symptoms that significantly reduce quality of life, even in the early stages of the disease. There is an urgent need to develop evidence‐based treatments for these symptoms, which include mood disturbances, cognitive dysfunction, and sleep disruption. We focus here on exercise interventions, which have been used to improve mood, cognition, and sleep in healthy older adults and clinical populations, but to date have primarily targeted motor symptoms in PD. We synthesize the existing literature on the benefits of aerobic exercise and strength training on mood, sleep, and cognition as demonstrated in healthy older adults and adults with PD, and suggest that these types of exercise offer a feasible and promising adjunct treatment for mood, cognition, and sleep difficulties in PD. Across stages of the disease, exercise interventions represent a treatment strategy with the unique ability to improve a range of nonmotor symptoms while also alleviating the classic motor symptoms of the disease. Future research in PD should include nonmotor outcomes in exercise trials with the goal of developing evidence‐based exercise interventions as a safe, broad‐spectrum treatment approach to improve mood, cognition, and sleep for individuals with PD."
PETER A BURKE,"Three red suns in the sky: A transiting, terrestrial planet in a triple M-dwarf system at 6.9 pc","We present the discovery from Transiting Exoplanet Survey Satellite (TESS) data of LTT 1445Ab. At a distance of 6.9 pc, it is the second nearest transiting exoplanet system found to date, and the closest one known for which the primary is an M dwarf. The host stellar system consists of three mid-to-late M dwarfs in a hierarchical configuration, which are blended in one TESS pixel. We use MEarth data and results from the Science Processing Operations Center data validation report to determine that the planet transits the primary star in the system. The planet has a radius of ${1.38}_{-0.12}^{+0.13}$ ${R}_{\oplus }$, an orbital period of ${5.35882}_{-0.00031}^{+0.00030}$ days, and an equilibrium temperature of ${433}_{-27}^{+28}$ K. With radial velocities from the High Accuracy Radial Velocity Planet Searcher, we place a 3σ upper mass limit of 8.4 ${M}_{\oplus }$ on the planet. LTT 1445Ab provides one of the best opportunities to date for the spectroscopic study of the atmosphere of a terrestrial world. We also present a detailed characterization of the host stellar system. We use high-resolution spectroscopy and imaging to rule out the presence of any other close stellar or brown dwarf companions. Nineteen years of photometric monitoring of A and BC indicate a moderate amount of variability, in agreement with that observed in the TESS light-curve data. We derive a preliminary astrometric orbit for the BC pair that reveals an edge-on and eccentric configuration. The presence of a transiting planet in this system hints that the entire system may be co-planar, implying that the system may have formed from the early fragmentation of an individual protostellar core."
PETER A BURKE,"The L 98-59 system: three transiting, terrestrial-size planets orbiting a nearby M dwarf","We report the Transiting Exoplanet Survey Satellite (TESS) discovery of three terrestrial-size planets transiting L 98-59 (TOI-175, TIC 307210830)—a bright M dwarf at a distance of 10.6 pc. Using the Gaia-measured distance and broadband photometry, we find that the host star is an M3 dwarf. Combined with the TESS transits from three sectors, the corresponding stellar parameters yield planet radii ranging from 0.8 R ⊕ to 1.6 R ⊕. All three planets have short orbital periods, ranging from 2.25 to 7.45 days with the outer pair just wide of a 2:1 period resonance. Diagnostic tests produced by the TESS Data Validation Report and the vetting package DAVE rule out common false-positive sources. These analyses, along with dedicated follow-up and the multiplicity of the system, lend confidence that the observed signals are caused by planets transiting L 98-59 and are not associated with other sources in the field. The L 98-59 system is interesting for a number of reasons: the host star is bright (V = 11.7 mag, K = 7.1 mag) and the planets are prime targets for further follow-up observations including precision radial-velocity mass measurements and future transit spectroscopy with the James Webb Space Telescope; the near-resonant configuration makes the system a laboratory to study planetary system dynamical evolution; and three planets of relatively similar size in the same system present an opportunity to study terrestrial planets where other variables (age, metallicity, etc.) can be held constant. L 98-59 will be observed in four more TESS sectors, which will provide a wealth of information on the three currently known planets and have the potential to reveal additional planets in the system."
CHRISTIAN ARBELAEZ,Does Modality of Survey Administration Impact Data Quality: Audio Computer Assisted Self Interview (ACASI) Versus Self-Administered Pen and Paper?,"BACKGROUND. In the context of a randomized controlled trial (RCT) on HIV testing in the emergency department (ED) setting, we evaluated preferences for survey modality and data quality arising from each modality. METHODS. Enrolled participants were offered the choice of answering a survey via audio computer assisted self-interview (ACASI) or pen and paper self-administered questionnaire (SAQ). We evaluated factors influencing choice of survey modality. We defined unusable data for a particular survey domain as answering fewer than 75% of the questions in the domain. We then compared ACASI and SAQ with respect to unusable data for domains that address sensitive topics. RESULTS. Of 758 enrolled ED patients, 218 (29%) chose ACASI, 343 chose SAQ (45%) and 197 (26%) opted not to complete either. Results of the log-binomial regression indicated that older (RR=1.08 per decade) and less educated participants (RR=1.25) were more likely to choose SAQ over ACASI. ACASI yielded substantially less unusable data than SAQ. CONCLUSIONS. In the ED setting there may be a tradeoff between increased participation with SAQ versus better data quality with ACASI. Future studies of novel approaches to maximize the use of ACASI in the ED setting are needed."
CHRISTIAN ARBELAEZ,Rapid HIV Testing Program Implementation: Lessons from the Emergency Department,"BACKGROUND. The US Centers for Disease Control and Prevention (CDC) guidelines and the World Health Organization (WHO) both recommend HIV testing in health-care settings. However, neither organization provides prescriptive details regarding how these recommendations should be adapted into clinical practice in an emergency department. METHODS. We have implemented an HIV-testing program in the ED of a major academic medical center within the scope of the Universal Screening for HIV Infection in the Emergency Room (USHER) Trial—a randomized clinical trial evaluating the feasibility and cost-effectiveness of HIV screening in this setting. RESULTS AND CONCLUSION. Drawing on our collective experiences in establishing programs domestically and internationally, we offer a practical framework of lessons learned so that others poised to embark on such HIV testing programs may benefit from our experiences."
CHRISTIAN ARBELAEZ,Oxidative stress cascades in spinal cord injury and therapeutic intervention development,"Secondary injury (SI) after spinal cord trauma is characterized by the multiple pathophysiological events that are set in motion by a primary mechanical insult to the spinal cord. SI produces a post-spinal cord injury (SCI) microenvironment that severely limits recovery, and results in more functional deficits. Many advances have been made to increase the scientific understanding of SI mechanisms, but there remains a lack of standardized clinical treatment to mitigate the damage caused by secondary pathophysiology. The classification of SI processes can be done based on the biological systems they take place: vascular (hemorrhagic necrosis, ischemia, edema), immune (cytokine and hormone release, residential inflammatory cells, peripheral inflammatory and immune cells), and neuronal/glial systems (excitotoxicity, mitochondrial dysfunction, ionic disturbances, overproduction of reactive oxygen and nitrogen species, cell death pathway activation, and axon degeneration). Regarding mechanisms underlying the SI processes (i.e., SI mechanisms: the biochemical events that exacerbate structural and functional losses), SCI-induced elevation of reactive oxygen and reactive nitrogen species (ROS/RNS) serve as one of the most potent triggers of neural cell and neurite death and inflammation, essentially affecting all aforementioned systems to worsen neurological dysfunction. In addition to the role in pathophysiological circumstances, ROS/RNS are also important players in mediating physiological functions (e.g., cell signaling). This feature requires that development of therapeutics for managing post-SCI ROS and RNS must specifically impede the detrimental effects of the radicals in propagating the SI scale without interfering with their physiological roles. In the context of SCI, regulation of ROS/RNS species at their homeostatic levels may serve as an effective therapeutic target for future clinical studies to mitigate neurodegeneration and neuroinflammation, and to promote a permissive environment for endogenous recovery. Based on these facts, Hydrogen Peroxide and Peroxynitrite (a representative ROS and RNS, respectively) were evaluated as SI mediators after SCI and for their potential to be targeted to develop interventions to treat SCI. Overall, the reviewed research findings suggested that oxidative damage cascades should be further studied in laboratory and clinical settings to advance understanding of SI processes of SCI for devising therapies."
TIMOTHY HEEREN,Evaluation of the Webler-Brown Model for Estimating Tetrachloroethylene Exposure from Vinyl-Lined Asbestos-Cement Pipes,"BACKGROUND: From May 1968 through March 1980, vinyl-lined asbestos-cement (VL/AC) water distribution pipes were installed in New England to avoid taste and odor problems associated with asbestos-cement pipes. The vinyl resin was applied to the inner pipe surface in a solution of tetrachloroethylene (perchloroethylene, PCE). Substantial amounts of PCE remained in the liner and subsequently leached into public drinking water supplies. METHODS: Once aware of the leaching problem and prior to remediation (April-November 1980), Massachusetts regulators collected drinking water samples from VL/AC pipes to determine the extent and severity of the PCE contamination. This study compares newly obtained historical records of PCE concentrations in water samples (n = 88) with concentrations estimated using an exposure model employed in epidemiologic studies on the cancer risk associated with PCE-contaminated drinking water. The exposure model was developed by Webler and Brown to estimate the mass of PCE delivered to subjects' residences. RESULTS: The mean and median measured PCE concentrations in the water samples were 66 and 0.5 μg/L, respectively, and the range extended from non-detectable to 2432 μg/L. The model-generated concentration estimates and water sample concentrations were moderately correlated (Spearman rank correlation coefficient = 0.48, p < 0.0001). Correlations were higher in samples taken at taps and spigots vs. hydrants (ρ = 0.84 vs. 0.34), in areas with simple vs. complex geometry (ρ = 0.51 vs. 0.38), and near pipes installed in 1973–1976 vs. other years (ρ = 0.56 vs. 0.42 for 1968–1972 and 0.37 for 1977–1980). Overall, 24% of the variance in measured PCE concentrations was explained by the model-generated concentration estimates (p < 0.0001). Almost half of the water samples had undetectable concentrations of PCE. Undetectable levels were more common in areas with the earliest installed VL/AC pipes, at the beginning and middle of VL/AC pipes, at hydrants, and in complex pipe configurations. CONCLUSION: PCE concentration estimates generated using the Webler-Brown model were moderately correlated with measured water concentrations. The present analysis suggests that the exposure assessment process used in prior epidemiological studies could be improved with more accurate characterization of water flow. This study illustrates one method of validating an exposure model in an epidemiological study when historical measurements are not available."
TIMOTHY HEEREN,The multiple hit hypothesis for Gulf War illness: self-reported chemical/biological weapons exposure and mild traumatic brain injury,"The Gulf War Illness Consortium (GWIC) was designed to identify objective biomarkers of Gulf War Illness (GWI) in 1991 Gulf War veterans. The symptoms of GWI include fatigue, pain, cognitive problems, gastrointestinal, respiratory, and skin problems. Neurotoxicant exposures during deployment, such as pesticides, sarin, and pyridostigmine bromide pills have been identified as contributors to GWI. We have also found an association between mild traumatic brain injury (mTBI) and increased rates of GWI. However, the combined impact of these physical and chemical exposures has not yet been explored in GWI. The objective of this study was to examine both self-reported mTBI and exposure to chemical/biological weapons (CBW) as a multiple or two hit model for increased risk of GWI and other chronic health conditions. The study population included 125 Gulf War (GW) veterans from the Boston GWIC. Exposure to CBW was reported in 47.2% of the study population, and 35.2% reported sustaining a mTBI during the war. Results confirmed that those with both exposures (mTBI and CBW) had higher rates of comorbid chronic health conditions while rates of GWI were equivalent for mTBI and CBW or mTBI alone. The timing of exposure to mTBI was found to be strikingly different between those with GWI and those without it. Correspondingly, 42.3% of GWI cases reported experiencing a mTBI during military service while none of the controls did (p = 0.0002). Rates of mTBI before and after the war did not differ between the cases and controls. In addition, 54% of cases compared to 14.3% of controls (p = <0.001) reported being exposed to CBW during military service. The current study examined the relation of the separate and combined effects of exposure to mTBI and CBW in 1991 GW veterans. The findings from this study suggest that both exposure to mTBI and CBW are associated with the development of GWI and multiple chronic health conditions and that combined exposure appears to lead to higher risk of chronic health effects."
TIMOTHY HEEREN,Performance of mixed effects models in the analysis of mediated longitudinal data,"BACKGROUND: Linear mixed effects models (LMMs) are a common approach for analyzing longitudinal data in a variety of settings. Although LMMs may be applied to complex data structures, such as settings where mediators are present, it is unclear whether they perform well relative to methods for mediational analyses such as structural equation models (SEMs), which have obvious appeal in such settings. For some researchers, SEMs may be more difficult than LMMs to implement, e.g. due to lack of training in the methodology or the need for specialized SEM software. It therefore is of interest to evaluate whether the LMM performs sufficiently in a scenario particularly suitable for SEMs. We focus on evaluation of the total effect (i.e. direct and indirect) of an exposure on an outcome of interest when a mediating factor is present. Our aim is to explore whether the LMM performs as well as the SEM in a setting that is conducive to using the SEM. METHODS: We simulated mediated longitudinal data from an SEM where a binary, main independent variable has both direct and indirect effects on a continuous outcome. We conducted analyses with both the LMM and SEM to evaluate the performance of the LMM in a setting where the SEM is expected to be preferable. Models were evaluated with respect to bias, coverage probability and power. Sample size, effect size and error distribution of the simulated data were varied. RESULTS: Both models performed well in a range of settings. Marginal increases in power estimates were observed for the SEM, although generally there were no major differences in performance. Power for both models was good with a sample of size of 250 and a small to medium effect size. Bias did not substantially increase for either model when data were generated from distributions that were both skewed and kurtotic. CONCLUSIONS: In settings where the goal is to evaluate the overall effects, the LMM excluding mediating variables appears to have good performance with respect to power, bias and coverage probability relative to the SEM. The major benefit of SEMs is that it simultaneously and efficiently models both the direct and indirect effects of the mediation process."
MICHAEL B PRINCE,A systematic search of Zwicky Transient Facility data for ultracompact binary LISA-detectable gravitational-wave sources,"Using photometry collected with the Zwicky Transient Facility, we are conducting an ongoing survey for binary systems with short orbital periods (P_b < 1 hr) with the goal of identifying new gravitational-wave sources detectable by the upcoming Laser Interferometer Space Antenna (LISA). We present a sample of 15 binary systems discovered thus far, with orbital periods ranging from 6.91 to 56.35 minutes. Of the 15 systems, seven are eclipsing systems that do not show signs of significant mass transfer. Additionally, we have discovered two AM Canum Venaticorum systems and six systems exhibiting primarily ellipsoidal variations in their lightcurves. We present follow-up spectroscopy and high-speed photometry confirming the nature of these systems, estimates of their LISA signal-to-noise ratios, and a discussion of their physical characteristics."
MICHAEL B PRINCE,A new class of Roche lobe–filling hot subdwarf binaries,"We present the discovery of the second binary with a Roche lobe–filling hot subdwarf transferring mass to a white dwarf (WD) companion. This 56 minute binary was discovered using data from the Zwicky Transient Facility. Spectroscopic observations reveal an He-sdOB star with an effective temperature of Teff = 33,700 ± 1000 K and a surface gravity of log(g) = 5.54 ± 0.11. The GTC+HiPERCAM light curve is dominated by the ellipsoidal deformation of the He-sdOB star and shows an eclipse of the He-sdOB by an accretion disk as well as a weak eclipse of the WD. We infer a He-sdOB mass of MsdOB = 0.41 ± 0.04 M⊙ and a WD mass of MWD = 0.68 ± 0.05 M⊙. The weak eclipses imply a WD blackbody temperature of 63,000 ± 10,000 K and a radius RWD = 0.0148 ± 0.0020 R⊙ as expected for a WD of such high temperature. The He-sdOB star is likely undergoing hydrogen shell burning and will continue transferring mass for ≈1 Myr at a rate of 10−9 M⊙ yr−1, which is consistent with the high WD temperature. The hot subdwarf will then turn into a WD and the system will merge in ≈30 Myr. We suggest that Galactic reddening could bias discoveries toward preferentially finding Roche lobe–filling systems during the short-lived shell-burning phase. Studies using reddening-corrected samples should reveal a large population of helium core–burning hot subdwarfs with Teff ≈ 25,000 K in binaries of 60–90 minutes with WDs. Though not yet in contact, these binaries would eventually come into contact through gravitational-wave emission and explode as a subluminous thermonuclear supernova or evolve into a massive single WD."
MICHAEL B PRINCE,The first ultracompact Roche lobe–filling hot subdwarf binary,"We report the discovery of the first short-period binary in which a hot subdwarf star (sdOB) filled its Roche lobe and started mass transfer to its companion. The object was discovered as part of a dedicated high-cadence survey of the Galactic plane named the Zwicky Transient Facility and exhibits a period of P = 39.3401(1) minutes, making it the most compact hot subdwarf binary currently known. Spectroscopic observations are consistent with an intermediate He-sdOB star with an effective temperature of T_eff = 42,400 ± 300 K and a surface gravity of log(g) = 5.77 ± 0.05. A high signal-to-noise ratio GTC+HiPERCAM light curve is dominated by the ellipsoidal deformation of the sdOB star and an eclipse of the sdOB by an accretion disk. We infer a low-mass hot subdwarf donor with a mass MsdOB = 0.337 ± 0.015 M_⊙ and a white dwarf accretor with a mass MWD = 0.545 ± 0.020 M_⊙. Theoretical binary modeling indicates the hot subdwarf formed during a common envelope phase when a 2.5–2.8 M_⊙ star lost its envelope when crossing the Hertzsprung gap. To match its current P_orb, T_eff, log(g), and masses, we estimate a post–common envelope period of P_orb ≈ 150 minutes and find that the sdOB star is currently undergoing hydrogen shell burning. We estimate that the hot subdwarf will become a white dwarf with a thick helium layer of ≈0.1 M_⊙, merge with its carbon/oxygen white dwarf companion after ≈17 Myr, and presumably explode as a thermonuclear supernova or form an R CrB star."
MICHAEL B PRINCE,A new class of large-amplitude radial-mode hot subdwarf pulsators,"Using high-cadence observations from the Zwicky Transient Facility at low Galactic latitudes, we have discovered a new class of pulsating, hot compact stars. We have found four candidates, exhibiting blue colors (g − r ≤ −0.1 mag), pulsation amplitudes of >5%, and pulsation periods of 200–475 s. Fourier transforms of the light curves show only one dominant frequency. Phase-resolved spectroscopy for three objects reveals significant radial velocity, T eff, and log(g) variations over the pulsation cycle, which are consistent with large-amplitude radial oscillations. The mean T eff and log(g) for these stars are consistent with hot subdwarf B (sdB) effective temperatures and surface gravities. We calculate evolutionary tracks using MESA and adiabatic pulsations using GYRE for low-mass, helium-core pre-white dwarfs (pre-WDs) and low-mass helium-burning stars. Comparison of low-order radial oscillation mode periods with the observed pulsation periods show better agreement with the pre-WD models. Therefore, we suggest that these new pulsators and blue large-amplitude pulsators (BLAPs) could be members of the same class of pulsators, composed of young ≈0.25–0.35 M ⊙ helium-core pre-WDs."
MICHAEL B PRINCE,Unfelt: the language of affect in the British enlightenment,
DAN CLEMENS,Multiwavelength stellar polarimetry of the filamentary cloud IC5146. I. Dust properties,"We present optical and near-infrared stellar polarization observations toward the dark filamentary clouds associated with IC5146. The data allow us to investigate the dust properties (this paper) and the magnetic field structure (Paper II). A total of 2022 background stars were detected in the Rc, i¢, H, and/or K bands to AV  25 mag. The ratio of the polarization percentage at different wavelengths provides an estimate of lmax, the wavelength of the peak polarization, which is an indicator of the small-size cutoff of the grain size distribution. The grain size distribution seems to significantly change at AV ~ 3 mag, where both the average and dispersion of P P R H c decrease. In addition, we found lmax ~ 0.6 0.9 – μm for AV > 2.5 mag, which is larger than the ∼0.55 μm in the general interstellar medium (ISM), suggesting that grain growth has already started in low-AV regions. Our data also reveal that polarization efficiency (PE º P A l V ) decreases with AV as a power law in the Rc, i¢, and K bands with indices of −0.71 ± 0.10, −1.23 ± 0.10, and −0.53 ± 0.09. However, H-band data show a power index change; the PE varies with AV steeply (index of −0.95 ± 0.30) when AV <  2.88 0.67 mag, but softly (index of −0.25 ± 0.06) for greater AV values. The soft decay of PE in high-AV regions is consistent with the radiative alignment torque model, suggesting that our data trace the magnetic field to AV ~ 20 mag. Furthermore, the breakpoint found in the H band is similar to that for AV, where we found the P P R H c dispersion significantly decreased. Therefore, the flat PE–AV in high-AV regions implies that the power-index changes result from additional grain growth."
DAN CLEMENS,Multiwavelength polarimetry of the filamentary cloud IC 5146. II. Magnetic field structures,"The IC 5146 cloud is a nearby star-forming region in Cygnus, consisting of molecular gas filaments in a variety of evolutionary stages. We used optical and near-infrared polarization data toward the IC 5146 cloud, reported in the first paper of this series, to reveal the magnetic fields in this cloud. Using the newly released Gaia data, we found that the IC 5146 cloud may contain two separate clouds: a first cloud, including the densest main filament at a distance of∼600 pc, and a second cloud, associated with the Cocoon Nebula at a distance of∼800 pc. The spatially averaged H-band polarization map revealed a well-ordered magnetic field morphology, with the polarization segments perpendicular to the main filament but parallel to the nearby sub filaments, consistent with models assuming that the magnetic field is regulating cloud evolution. We estimated the magnetic field strength using the Davis–Chandrasekhar–Fermi method and found that the magnetic field strength scales with volume density with a power-law index of∼0.5 in the density range from N_H2 ∼ 10 to 3000 cm_−3, which indicates an an isotropic cloud contraction with a preferred direction along the magnetic field. In addition, the mass-to-flux ratio of the cloud gradually changes from subcritical to supercritical from the cloud envelope to the deep regions. These features are consistent with strong magnetic field star formation models and suggest that the magnetic field is important in regulating the evolution of the IC 5146 cloud."
DAN CLEMENS,Probing interstellar grain growth through polarimetry in the Taurus cloud complex,"The optical and near-infrared (OIR) polarization of starlight is typically understood to arise from the dichroic extinction of that light by dust grains whose axes are aligned with respect to a local magnetic field. The size distribution of the aligned grain population can be constrained by measurements of the wavelength dependence of the polarization. The leading physical model for producing the alignment is that of radiative alignment torques (RAT), which predicts that the most efficiently aligned grains are those with sizes larger than the wavelengths of light composing the local radiation field. Therefore, for a given grain size distribution, the wavelength at which the polarization reaches a maximum (𝛌max) should correlate with the characteristic reddening along the line of sight between the dust grains and the illumination source. A correlation between 𝛌max and reddening has been previously established for extinctions up to AV ≈ 4 mag. We extend the study of this relationship to a larger sample of stars in the Taurus cloud complex, including extinctions AV > 10 mag. We confirm the earlier results for AV < 4 mag, but find that the 𝛌max vs. AV relationship bifurcates above AV ≈ 4 mag, with part of the sample continuing the previously observed relationship. The remaining sample exhibits a steeper rise in 𝛌max vs. AV . We propose that the data exhibiting the steep rise represent lines of sight of high-density “clumps”, where grain coagulation has taken place. We present RAT-based modeling supporting these hypotheses. These results indicate that multi-band OIR polarimetry is a powerful tool for tracing grain growth in molecular clouds, independent of uncertainties in the dust temperature and emissivity."
DAN CLEMENS,Magnetized filamentary gas flows feeding the young embedded cluster in Serpens South,"Observations indicate that molecular clouds are strongly magnetized, and that magnetic fields influence the formation of stars. A key observation supporting the conclusion that molecular clouds are significantly magnetized is that the orientation of their internal structure is closely related to that of the magnetic field. At low column densities, the structure aligns parallel with the field, whereas at higher column densities, the gas structure is typically oriented perpendicular to magnetic fields, with a transition at visual extinctions AV ≳ 3 mag. Here we use far-infrared polarimetric observations from the HAWC+ polarimeter on SOFIA to report the discovery of a further transition in relative orientation, that is, a return to parallel alignment at AV ≳ 21 mag in parts of the Serpens South cloud. This transition appears to be caused by gas flow and indicates that magnetic supercriticality sets in near AV ≳ 21 mag, allowing gravitational collapse and star cluster formation to occur even in the presence of relatively strong magnetic fields."
DAN CLEMENS,The magnetic field of L1544. I. Near-infrared polarimetry and the non-uniform envelope,"The magnetic field (B-field) of the starless dark cloud L1544 has been studied using near-infrared (NIR) background starlight polarimetry (BSP) and archival data in order to characterize the properties of the plane-of-sky B-field. NIR linear polarization measurements of over 1700 stars were obtained in the H band and 201 of these were also measured in the K band. The NIR BSP properties are correlated with reddening, as traced using the Rayleigh–Jeans color excess (H–M) method, and with thermal dust emission from the L1544 cloud and envelope seen in Herschel maps. The NIR polarization position angles change at the location of the cloud and exhibit their lowest dispersion there, offering strong evidence that NIR polarization traces the plane-of-sky B-field of L1544. In this paper, the uniformity of the plane-of-sky B-field in the envelope region of L1544 is quantitatively assessed. This allows evaluation of the approach of assuming uniform field geometry when measuring relative mass-to-flux ratios in the cloud envelope and core based on averaging of the radio Zeeman observations in the envelope, as done by Crutcher et al. In L1544, the NIR BSP shows the envelope B-field to be significantly non-uniform and likely not suitable for averaging Zeeman properties without treating intrinsic variations. Deeper analyses of the NIR BSP and related data sets, including estimates of the B-field strength and testing how it varies with position and gas density, are the subjects of later papers in this series."
DAN CLEMENS,Proper motion of the faint star near KIC 8462852 (Boyajian's Star)-not a binary system,"A faint star located 2 arcsec from KIC 8462852 was discovered in Keck 10 m adaptive optics imaging in the JHK near-infrared (NIR) in 2014 by Boyajian et al. (2016). The closeness of the star to KIC 8462852 suggested that the two could constitute a binary, which might have implications for the cause of the brightness dips seen by Kepler and in ground-based optical studies. Here, NIR imaging in 2017 using the Mimir instrument resolved the pair and enabled measuring their separation. The faint star had moved 67 ± 7 milliarcsec (mas) relative to KIC 8462852 since 2014. The relative proper motion of the faint star is 23.9 ± 2.6 mas yr−1, for a tangential velocity of 45 ± 5 km s−1 if it is at the same 390 pc distance as KIC 8462852. Circular velocity at the 750 au current projected separation is 1.5 km s−1, hence the star pair cannot be bound."
DAN CLEMENS,"Recent H-alpha results on pulsar B2224+65's bow-shock nebula, the ""Guitar""","We used the 4 m Discovery Channel Telescope (DCT) at Lowell observatory in 2014 to observe the Guitar Nebula, an Hα bow-shock nebula around the high-velocity radio pulsar B2224+65. Since the nebula`s discovery in 1992, the structure of the bow-shock has undergone significant dynamical changes. We have observed the limb structure, targeting the ""body"" and ""neck"" of the guitar. Comparing the DCT observations to 1995 observations with the Palomar 200-inch Hale telescope, we found changes in both spatial structure and surface brightness in the tip, head, and body of the nebula."
DAN CLEMENS,"Magnetic field uniformity across the GF 9-2 YSO, L1082C dense core, and GF 9 filamentary dark cloud","The orientation of the magnetic field (B field) in the filamentary dark cloud GF 9 was traced from the periphery of the cloud into the L1082C dense core that contains the low-mass, low-luminosity Class 0 young stellar object (YSO) GF 9-2 (IRAS 20503+6006). This was done using SOFIA HAWC+ dust thermal emission polarimetry (TEP) at 216 μm in combination with Mimir near-infrared background starlight polarimetry (BSP) conducted in the H band (1.6 μm) and K band (2.2 μm). These observations were augmented with published I-band (0.77 μm) BSP and Planck 850 μm TEP to probe B-field orientations with offset from the YSO in a range spanning 6000 au to 3 pc. No strong B-field orientation change with offset was found, indicating remarkable uniformity of the B-field from the cloud edge to the YSO environs. This finding disagrees with weak-field models of cloud core and YSO formation. The continuity of inferred B-field orientations for both TEP and BSP probes is strong evidence that both are sampling a common B field that uniformly threads the cloud, core, and YSO region. Bayesian analysis of Gaia DR2 stars matched to the Mimir BSP stars finds a distance to GF 9 of 270 ± 10 pc. No strong wavelength dependence of B-field orientation angle was found, contrary to previous claims."
DAN CLEMENS,"Magnetic field uniformity across the GF 9-2 YSO, L1082C dense core, and GF 9 filamentary dark cloud","The orientation of the magnetic field (B-field) in the lamentary dark cloud GF 9 was traced from the periphery of the cloud into the L1082C dense core that contains the low-mass, low-luminosity Class 0 young stellar object (YSO) GF 9-2 (IRAS 20503+6006). This was done using SOFIA HAWC+ dust thermal emission polarimetry (TEP) at 216 μm in combination with Mimir near-infrared background starlight polarimetry (BSP) conducted at H-band (1.6 μm) and K-band (2.2 μm). These observations were augmented with published I-band (0.77 μm) BSP and Planck 850 μm TEP to probe B-field orientations with offset from the YSO in a range spanning 6000 AU to 3 pc. No strong B-field orientation change with offset was found, indicating remarkable uniformity of the B-field from the cloud edge to the YSO environs. This finding disagrees with weak-field models of cloud core and YSO formation. The continuity of inferred B-field orientations for both TEP and BSP probes is strong evidence that both are sampling a common B-field that uniformly threads the cloud, core, and YSO region. Bayesian analysis of Gaia DR2 stars matched to the Mimir BSP stars finds a distance to GF 9 of 270 ± 10 pc. No strong wavelength dependence of B-field orientation angle was found, contrary to previous claims."
DAN CLEMENS,Tracing the magnetic field of IRDC G028.23-00.19 using NIR polarimetry,"The importance of the magnetic (B) field in the formation of infrared dark clouds (IRDCs) and massive stars is an ongoing topic of investigation. We studied the plane-of-sky B field for one IRDC, G028.23-00.19, to understand the interaction between the field and the cloud. We used near-IR background starlight polarimetry to probe the B field and performed several observational tests to assess the field importance. The polarimetric data, taken with the Mimir instrument, consisted of H-band and K-band observations, totaling 17,160 stellar measurements. We traced the plane-of-sky B-field morphology with respect to the sky-projected cloud elongation. We also found the relationship between the estimated B-field strength and gas volume density, and we computed estimates of the normalized mass-to-magnetic flux ratio. The B-field orientation with respect to the cloud did not show a preferred alignment, but it did exhibit a large-scale pattern. The plane-of-sky B-field strengths ranged from 10 to 165 μG, and the B-field strength dependence on density followed a power law with an index consistent with 2/3. The mass-to-magnetic flux ratio also increased as a function of density. The relative orientations and relationship between the B field and density imply that the B field was not dynamically important in the formation of the IRDC. The increase in mass-to-flux ratio as a function of density, though, indicates a dynamically important B field. Therefore, it is unclear whether the B field influenced the formation of G28.23. However, it is likely that the presence of the IRDC changed the local B-field morphology."
DAN CLEMENS,"O/IR polarimetry for the 2010 decade (CGT): Science at the edge, sharp tools for all",Science opportunities and recommendations concerning optical/infrared polarimetry for the upcoming decade in the field of extragalactic astrophysics. Community-based White Paper to Astro2010 in response to the call for such papers.
DAN CLEMENS,"The magnetic field of the L1544 starless dark cloud, traced using near-infrared background starlight (Presentation)","What roles do interstellar magnetic fields play in star formation processes? We have studied the B-field of L1544, a dark cloud with a starless dense core showing active gas infall, and located only 140 pc away in Taurus, via deep near-infrared (NIR) imaging polarimetry with the Mimir instrument. We find the B-field orientations in the plane of the sky change significantly at L1544, mimicking its shape and extent. The elongated spine of L1544 is also where the dispersion of NIR linear polarization position angles is smallest, suggesting strengthening of the B-field. Archival WISE, SCUPOL, Herschel, and Planck data were analyzed to characterize dust extinction and emission across L1544 and the field around it. Three-dimensional modeling, constrained through matching two-dimensional integrated model properties to observed dust distributions, led us to develop maps of effective gas mass densities and non-thermal gas velocity dispersions. These were combined with the NIR polarimetry, under the Chandrasekhar & Fermi (1953) approach, to yield a map of B-field strength across the entire 400 sq-arcmin region surveyed. The trends of B-field strength with gas volume density, mass-to-flux ratio with radius, and plane-of-sky B-field strengths with Zeeman-traced line-of-sight B-field strengths were found and compared to previous published work to establish the role of B-fields in L1544. We find field strengths in the 3 - 30 uG range, quite similar to the OH Zeeman values found by Crutcher et al. (2009) for L1544.This work was partially supported by grants to Boston University from NSF (AST-0907790, 1412269) and NASA (NNX15AE51G)."
DAN CLEMENS,The magnetic field of the dark cloud L1544: near-infrared polarimetry and the non-uniform envelope,Invited lunch seminar presentation
DAN CLEMENS,Exceptional outburst of the blazar CTA 102 in 2012: the GASP-WEBT campaign and its extension,"After several years of quiescence, the blazar CTA 102 underwent an exceptional outburst in 2012 September–October. The flare was tracked from γ-ray to near-infrared (NIR) frequencies, including Fermi and Swift data as well as photometric and polarimetric data from several observatories. An intensive Glast-Agile support programme of the Whole Earth Blazar Telescope (GASP–WEBT) collaboration campaign in optical and NIR bands, with an addition of previously unpublished archival data and extension through fall 2015, allows comparison of this outburst with the previous activity period of this blazar in 2004–2005. We find remarkable similarity between the optical and γ-ray behaviour of CTA 102 during the outburst, with a time lag between the two light curves of ≈1 h, indicative of cospatiality of the optical and γ-ray emission regions. The relation between the γ-ray and optical fluxes is consistent with the synchrotron self-Compton (SSC) mechanism, with a quadratic dependence of the SSC γ-ray flux on the synchrotron optical flux evident in the post-outburst stage. However, the γ-ray/optical relationship is linear during the outburst; we attribute this to changes in the Doppler factor. A strong harder-when-brighter spectral dependence is seen both the in γ-ray and optical non-thermal emission. This hardening can be explained by convexity of the UV–NIR spectrum that moves to higher frequencies owing to an increased Doppler shift as the viewing angle decreases during the outburst stage. The overall pattern of Stokes parameter variations agrees with a model of a radiating blob or shock wave that moves along a helical path down the jet."
DAN CLEMENS,Open clusters as probes of the galactic magnetic field. I. Cluster properties,"Stars in open clusters are powerful probes of the intervening Galactic magnetic field via background starlight polarimetry because they provide constraints on the magnetic field distances. We use 2MASS photometric data for a sample of 31 clusters in the outer Galaxy for which near-IR polarimetric data were obtained to determine the cluster distances, ages, and reddenings via fitting theoretical isochrones to cluster color–magnitude diagrams. The fitting approach uses an objective χ2 minimization technique to derive the cluster properties and their uncertainties. We found the ages, distances, and reddenings for 24 of the clusters, and the distances and reddenings for 6 additional clusters that were either sparse or faint in the near-IR. The derived ranges of log(age), distance, and E(B−V) were 7.25–9.63, ~670–6160 pc, and 0.02–1.46 mag, respectively. The distance uncertainties ranged from ~8% to 20%. The derived parameters were compared to previous studies, and most cluster parameters agree within our uncertainties. To test the accuracy of the fitting technique, synthetic clusters with 50, 100, or 200 cluster members and a wide range of ages were fit. These tests recovered the input parameters within their uncertainties for more than 90% of the individual synthetic cluster parameters. These results indicate that the fitting technique likely provides reliable estimates of cluster properties. The distances derived will be used in an upcoming study of the Galactic magnetic field in the outer Galaxy."
DAN CLEMENS,"O/IR polarimetry for the 2010 decade (GAN): Science at the edge, sharp tools for all",Science opportunities and recommendations concerning optical/infrared polarimetry for the upcoming decade in the field of Galactic science. Community-based White Paper to Astro2010 in response to the call for such papers.
DAN CLEMENS,"O/IR polarimetry for the 2010 decade (PSF): Science at the edge, sharp tools for all",Science opportunities and recommendations concerning optical/infrared polarimetry for the upcoming decade in the fields of planetary systems and star formation. Community-based White Paper to Astro2010 in response to the call for such papers.
DAN CLEMENS,"O/IR polarimetry for the 2010 decade (SSE): Science at the edge, sharp tools for all",Science opportunities and recommendations concerning optical/infrared polarimetry for the upcoming decade in the field of extragalactic astrophysics. Community-based White Paper to Astro2010 in response to the call for such papers.
DAN CLEMENS,Understanding polarized foreground from dust: Towards reliable measurements of CMB polarization,Science opportunities and recommendations concerning optical/infrared polarimetry for the upcoming decade in the field of cosmology. Community-based White Paper to Astro2010 in response to the call for such papers.
DAN CLEMENS,Galactic pane infrared polarization survey (GPIPS): Data Release 4,"The Galactic Plane Infrared Polarization Survey (GPIPS) seeks to characterize the magnetic field in the dusty Galactic disk using near-infrared stellar polarimetry. All GPIPS observations were completed using the 1.83 m Perkins telescope and Mimir instrument. GPIPS observations surveyed 76 deg2 of the northern Galactic plane, from Galactic longitudes 18°–56° and latitudes −1° to +1°, in the H band (1.6 μm). Surveyed stars span 7th–16th mag, resulting in nearly 10 million stars with measured linear polarizations. Of these stars, ones with m_H < 12.5 mag and polarization percentage uncertainties under 2% were judged to be high quality and number over one million. GPIPS data reveal plane-of-sky magnetic field orientations for numerous interstellar clouds for AV values to ∼30 mag. The average sky separation of stars with m_H < 12.5 mag is about 30″, or about 60 per Planck polarization resolution element. Matching to Gaia DR2 showed the brightest GPIPS stars are red giants with distances in the 0.6–7.5 kpc range. Polarization orientations are mostly parallel to the Galactic disk, with some zones showing significant orientation departures. Changes in orientations are stronger as a function of Galactic longitude than of latitude. Considered at 10′ angular scales, directions that show the greatest polarization fractions and narrowest polarization position angle distributions are confined to about 10 large, coherent structures that are not correlated with star-forming clouds. The GPIPS polarimetric and photometric data products (Data Release 4 catalogs and images) are publicly available for over 13 million stars."
NANCY SMITH-HEFNER,Too educated for love? Women and the marriage market in Indonesia,"Indonesia is among the shrinking number of Asian countries demographers identify as following a pattern of “universal marriage,” defined as a country in which fewer than 4% of women over the age of 40 have never married (G. Jones 2004). The marital imperative weighs particularly heavily on young women. Those who reach the age of 25 without finding a partner are considered to be “unmarketable” and placed in the category of “old maid.” Women who put off marriage to pursue an education are in an especially precarious position; since most Indonesian men look to marry “down” with regard to age and education, educated women face the challenge of finding someone who is an appropriate match in a narrowing field of candidates. They also face the perception held by at least some men, that educated women will assume a dominant position within the family. This paper presents the life stories of four educated Javanese women and examines the hurdles they face in finding and securing a marital partner. Although young Indonesians have embraced the idea of romantic love as the proper foundation for a modern, companionate marriage, and it is widely accepted that youth should make their own choice of “soul mate” (jodoh), educated women are finding it increasingly difficult to meet their match."
PHILLIP S LOBEL,Junior synonymy of Mulloides arntatus and intraspecific comparisons of the yellowstripe goatfish Mulloidichthys flavolineatus (Mullidae ) using a comprehensive alpha-taxonomy approach,
PHILLIP S LOBEL,The value of closed-circuit rebreathers for biological research,"Closed-circuit rebreathers have been used for underwater biological research since the late 1960s, but have only started to gain broader application within scientific diving organizations within the past two decades. Rebreathers offer certain specific advantages for such research, especially for research involving behavior and surveys that depend on unobtrusive observers or for a stealthy approach to wildlife for capture and tagging, research that benefits from extended durations underwater, and operations requiring access to relatively deep (>50 m) environments (especially in remote locations). Although many institutions have been slow to adopt rebreather technology within their diving programs, recent developments in rebreather technology that improve safety, standardize training requirements, and reduce costs of equipment and maintenance, will likely result in a trend of increasing utilization of rebreathers for underwater biological research."
PHILLIP S LOBEL,Underwater soundscape monitoring and fish bioacoustics: a review,"Soundscape ecology is a rapidly growing field with approximately 93% of all scientific articles on this topic having been published since 2010 (total about 610 publications since 1985). Current acoustic technology is also advancing rapidly, enabling new devices with voluminous data storage and automatic signal detection to define sounds. Future uses of passive acoustic monitoring (PAM) include biodiversity assessments, monitoring habitat health, and locating spawning fishes. This paper provides a review of ambient sound and soundscape ecology, fish acoustic monitoring, current recording and sampling methods used in long-term PAM, and parameters/metrics used in acoustic data analysis."
PHILLIP S LOBEL,"A genetic assessment of parentage in the blackspot sergeant damselfish, Abudefduf sordidus (Pisces: Pomacentridae)","Microsatellite markers were used to investigate the reproductive behavior of the damselfish Abudefduf sordidus at Johnston Atoll, Central Pacific Ocean. Genetic results indicated that ten males maintained guardianship over their nest territories for up to nine nest cycles during a 3.5 month period. Genotypes of 1025 offspring sampled from 68 nests (composed of 129 clutches) were consistent with 95% of the offspring being sired by the guardian male. Offspring lacking paternal alleles at two or more loci were found in 19 clutches, indicating that reproductive parasitism and subsequent alloparental care occurred. Reconstructed maternal genotypes allowed the identification of a minimum of 74 different females that spawned with these ten territorial males. Males were polygynous, mating with multiple females within and between cycles. Genetic data from nests, which consisted of up to four clutches during a reproductive cycle, indicated that each clutch usually had only one maternal contributor and that different clutches each had different dams. Females displayed sequential polyandry spawning with one male within a cycle but switched males in subsequent spawning cycles. These results highlight new findings regarding male parasitic spawning, polygyny, and sequential polyandry in a marine fish with exclusive male paternal care."
PHILLIP S LOBEL,"Damselfish embryos as a bioindicator for military contamination on Coral Reefs at Johnston Atoll, Pacific Ocean","This study investigated the association between sediment contamination, PCB accumulation in adult nesting males and the occurrence of embryonic abnormalities in the damselfish, Abudefduf sordidus, from two sites with high PCB contamination and three “reference” sites (contaminants very low or not measurable) within Johnston Atoll, Central Pacific Ocean. Developmental abnormalities were assessed in damselfish embryos collected in the field during four natural spawning seasons (1996, 1998, 1999, and 2001). Laboratory incubations of abnormal embryos demonstrated that the observed abnormalities were lethal. PCBs were measured in fish collected in three years. Mean whole-body concentrations of total PCBs ranged from 364.6 to 138,032.5 ng/g lipid. A significant residue–effect relationship was found between total PCB concentration and embryo abnormalities. The occurrence of embryo abnormalities was positively related to fish PCB concentration (other contaminants were also evaluated including metals and dioxins). This study demonstrates the utility of using damselfish embryos as a bioindicator tool for monitoring coral reefs. It also provides baseline-monitoring criteria and evaluates sediment quality benchmarks used for ecological risk assessments on coral reefs."
PHILLIP S LOBEL,Johnston Atoll: reef fish hybrid zone between Hawaii and the equatorial Pacific,"Johnston Atoll is isolated in the Central Pacific Ocean (16°45′ N 169°31′ W) about 1287 km (800 miles) southwest of Honolulu, Hawaii and 1440 km (900 miles) north of the equatorial Line Islands, Kiribati. The labrid species, Thalassoma lutescens, has a wide range of distribution in the equatorial Pacific. The related species, Thalassoma duperrey, is endemic to the Hawaiian Islands. The pelagic larvae of both species dispersed to Johnston Atoll, where we found a mix of adult phenotypes representing a range of hybridization events over generations. A hybrid acanthurid was also documented. In addition, the arrival and colonization of two pomacentrid (damselfish) species to the atoll was observed in 1999. These pomacentrid sister-species, Abudefduf abdominalis and A. vaigiensis, have become established populations with subsequent hybridization. The biogeography of the Johnston Atoll coral reef fish population shows some degree of local population retention. It is also evident that this biogeographic isolation is periodically compromised by large ocean current oscillations in the equatorial and central Pacific Ocean that bring larval fishes from either Hawaii or the Line Islands, and may distribute Johnston Atoll originating larvae elsewhere as well. The reef fauna and oceanography of this atoll provides the circumstances for improving scientific insight into marine fish speciation and island biogeography."
PHILLIP S LOBEL,Evolutionary patterns in sound production across fishes,
SHUBA SRINIVASAN,The impact of online display advertising and paid search advertising relative to offline advertising on firm performance and firm value,"This research examines the impact of online display advertising and paid search advertising relative to offline advertising on firm performance and firm value. Using proprietary data on annualized advertising expenditures for 1651 firms spanning seven years, we document that both display advertising and paid search advertising exhibit positive effects on firm performance (measured by sales) and firm value (measured by Tobin's q). Paid search advertising has a more positive effect on sales than offline advertising, consistent with paid search being closest to the actual purchase decision and having enhanced targeting abilities. Display advertising exhibits a relatively more positive effect on Tobin's q than offline advertising, consistent with its long-term effects. The findings suggest heterogeneous economic benefits across different types of advertising, with direct implications for managers in analyzing advertising effectiveness and external stakeholders in assessing firm performance."
SHUBA SRINIVASAN,Turning socio-political risk to your brand’s advantage,"Employment practices, civic responsibilities, philanthropy, environmental stewardship, the conduct of corporate executives and employees, the execution of marketing campaigns: All these topics can trigger brand risk events. The challenging branding environment calls for reimagining classic brand marketing through a refreshed and updated social risk management lens. Companies need to assess which socio-economic marketing opportunities can renew brand resonance. This involves not just identifying revenue generating opportunities, but also identifying, cataloging, and tracking SEP risk types in order for managers to understand the new landscape brands must now navigate. Then, they need to implement a framework to manage a brand’s social risks and to take advantage of potential opportunities. Fully embracing this responsibility changes the marketing executive’s role in a significant way: From top line revenue generation to a dual role that includes managing risks as well as returns."
SHUBA SRINIVASAN,Branding and the risk management imperative,"In an increasingly risky socioeconomic environment, management needs to proactively consider brand-related risks. To understand brands as tools for risk management, they need to understand four types of brand risk: brand reputation risk, brand dilution risk, brand cannibalization risk and brand stretch risk. Risk management is not a natural act for brand managers trained in astute execution of the 4 Ps, and contemporary market factors make this more challenging still. With an increasingly polarized society, it is almost impossible for brands to remain untouched by ideologies. In addition, the growth in digital advertising gives brand managers less control over advertising placement and context, and the mandate to keep growing adds executional risk. The more exposed a brand is to brand risk, the more attention this topic will need in the boardroom. To shift a company’s marketing philosophy toward risk, it is important to define marketing competences in a broader way, to be self-critical and to be proactive."
SHUBA SRINIVASAN,The frontlines of brand risk: interview with Patrick Marrinan,"Whether it be the NFL, Dove, Wells Fargo, VW or countless others–managers need only open a daily newspaper to see how things can go terribly wrong for brands. Decline can be fast and the landing hard. In a contemporary marketplace where ideologies reign and social media guarantees the spread of (mis)information at light speed, a lot of what we think we know about brand marketing needs to be rethought through a risk-management lens. “For me, brand risk is any event, action or condition with the potential to damage a brand’s value, thereby making revenue generation and a company’s market value less than it should or could have been,” Patrick Marrinan, Managing Principal of Marketing Scenario Analytica, states. In his talk with Susan Fournier and Shuba Srinivasan, Patrick illustrates the many facets of a risk that has only begun to be recognized as a serious threat to carefully cultivated brand assets. Here we share what to watch out for and what brands can do to protect against risk."
DEBBIE CHENG,Implications of Cannabis Use and Heavy Alcohol Use on HIV Drug Risk Behaviors in Russian Heroin Users,"Cannabis and heavy alcohol use potentially increase HIV transmission by increasing risky drug behaviors. We studied 404 subjects entering treatment for heroin dependence, in St. Petersburg, Russia. We used the HIV Risk Assessment Battery (RAB) drug subscale to measure risky drug behavior. Although all heavy alcohol users had risky drug behaviors, their drug RAB scores did not differ from non-heavy alcohol users in unadjusted or adjusted analyses. Cannabis use was significantly associated with drug RAB scores in unadjusted analyses (mean difference 1.7 points) and analyses adjusted for age, sex, and employment (mean difference 1.3 points). When also adjusting for stimulant use, the impact of cannabis use was attenuated and no longer statistically significant (mean difference 1.1 points). Because of the central role of risky drug behaviors in the Russian HIV epidemic, it is important to understand how the use of multiple substances, including cannabis and alcohol, impacts risky drug behaviors."
DEBBIE CHENG,Treating Homeless Opioid Dependent Patients with Buprenorphine in an Office-Based Setting,"CONTEXT Although office-based opioid treatment with buprenorphine (OBOT-B) has been successfully implemented in primary care settings in the US, its use has not been reported in homeless patients. OBJECTIVE To characterize the feasibility of OBOT-B in homeless relative to housed patients. DESIGN A retrospective record review examining treatment failure, drug use, utilization of substance abuse treatment services, and intensity of clinical support by a nurse care manager (NCM) among homeless and housed patients in an OBOT-B program between August 2003 and October 2004. Treatment failure was defined as elopement before completing medication induction, discharge after medication induction due to ongoing drug use with concurrent nonadherence with intensified treatment, or discharge due to disruptive behavior. RESULTS Of 44 homeless and 41 housed patients enrolled over 12 months, homeless patients were more likely to be older, nonwhite, unemployed, infected with HIV and hepatitis C, and report a psychiatric illness. Homeless patients had fewer social supports and more chronic substance abuse histories with a 3- to 6-fold greater number of years of drug use, number of detoxification attempts and percentage with a history of methadone maintenance treatment. The proportion of subjects with treatment failure for the homeless (21%) and housed (22%) did not differ (P=.94). At 12 months, both groups had similar proportions with illicit opioid use [Odds ratio (OR), 0.9 (95% CI, 0.5–1.7) P=.8], utilization of counseling (homeless, 46%; housed, 49%; P=.95), and participation in mutual-help groups (homeless, 25%; housed, 29%; P=.96). At 12 months, 36% of the homeless group was no longer homeless. During the first month of treatment, homeless patients required more clinical support from the NCM than housed patients. CONCLUSIONS Despite homeless opioid dependent patients' social instability, greater comorbidities, and more chronic drug use, office-based opioid treatment with buprenorphine was effectively implemented in this population comparable to outcomes in housed patients with respect to treatment failure, illicit opioid use, and utilization of substance abuse treatment."
DEBBIE CHENG,Performance of mixed effects models in the analysis of mediated longitudinal data,"BACKGROUND: Linear mixed effects models (LMMs) are a common approach for analyzing longitudinal data in a variety of settings. Although LMMs may be applied to complex data structures, such as settings where mediators are present, it is unclear whether they perform well relative to methods for mediational analyses such as structural equation models (SEMs), which have obvious appeal in such settings. For some researchers, SEMs may be more difficult than LMMs to implement, e.g. due to lack of training in the methodology or the need for specialized SEM software. It therefore is of interest to evaluate whether the LMM performs sufficiently in a scenario particularly suitable for SEMs. We focus on evaluation of the total effect (i.e. direct and indirect) of an exposure on an outcome of interest when a mediating factor is present. Our aim is to explore whether the LMM performs as well as the SEM in a setting that is conducive to using the SEM. METHODS: We simulated mediated longitudinal data from an SEM where a binary, main independent variable has both direct and indirect effects on a continuous outcome. We conducted analyses with both the LMM and SEM to evaluate the performance of the LMM in a setting where the SEM is expected to be preferable. Models were evaluated with respect to bias, coverage probability and power. Sample size, effect size and error distribution of the simulated data were varied. RESULTS: Both models performed well in a range of settings. Marginal increases in power estimates were observed for the SEM, although generally there were no major differences in performance. Power for both models was good with a sample of size of 250 and a small to medium effect size. Bias did not substantially increase for either model when data were generated from distributions that were both skewed and kurtotic. CONCLUSIONS: In settings where the goal is to evaluate the overall effects, the LMM excluding mediating variables appears to have good performance with respect to power, bias and coverage probability relative to the SEM. The major benefit of SEMs is that it simultaneously and efficiently models both the direct and indirect effects of the mediation process."
KATE SAENKO,Surprisingly simple semi-supervised domain adaptation with pretraining and consistency,"Most modern unsupervised domain adaptation (UDA) approaches are rooted in domain alignment, i.e., learning to align source and target features to learn a target domain classifier using source labels. In semi-supervised domain adaptation (SSDA), when the learner can access few target domain labels, prior approaches have followed UDA theory to use domain alignment for learning. We show that the case of SSDA is different and a good target classifier can be learned without needing alignment. We use self-supervised pretraining (via rotation prediction) and consistency regularization to achieve well separated target clusters, aiding in learning a low error target classifier. With our Pretraining and Consistency (PAC) approach, we achieve state of the art target accuracy on this semi-supervised domain adaptation task, surpassing multiple adversarial domain alignment methods, across multiple datasets. PAC, while using simple techniques, performs remarkably well on large and challenging SSDA benchmarks like DomainNet and Visda-17, often outperforming recent state of the art by sizeable margins. Code for our experiments can be found at https://github.com/venkatesh-saligrama/PAC."
KATE SAENKO,How transferable are video representations based on synthetic data?,
KATE SAENKO,Good actors can come in smaller sizes: a case study on the value of actor-critic asymmetry,
KATE SAENKO,Neural parameter allocation search,
KATE SAENKO,Why do these match? Explaining the behavior of image similarity models,"Explaining a deep learning model can help users understand its behavior and allow researchers to discern its shortcomings. Recent work has primarily focused on explaining models for tasks like image classiffication or visual question answering. In this paper, we introduce Salient Attributes for Network Explanation (SANE) to explain image similarity models, where a model's output is a score measuring the similarity of two inputs rather than a classiffication score. In this task, an explanation depends on both of the input images, so standard methods do not apply. Our SANE explanations pairs a saliency map identifying important image regions with an attribute that best explains the match. We find that our explanations provide additional information not typically captured by saliency maps alone, and can also improve performance on the classic task of attribute recognition. Our approach's ability to generalize is demonstrated on two datasets from diverse do- mains, Polyvore Outfits and Animals with Attributes 2. Code available at: https://github.com/VisionLearningGroup/SANE"
KATE SAENKO,Learning to scale multilingual representations for vision-language tasks,"Current multilingual vision-language models either require a large number of additional parameters for each supported language, or suffer performance degradation as languages are added. In this paper, we propose a Scalable Multilingual Aligned Language Representation (SMALR) that supports many languages with few model parameters without sacrificing downstream task performance. SMALR learns a fixed size language-agnostic representation for most words in a multilingual vocabulary, keeping language-specific features for just a few. We use a masked cross-language modeling loss to align features with context from other languages. Additionally, we propose a cross-lingual consistency module that ensures predictions made for a query and its machine translation are comparable. The effectiveness of SMALR is demonstrated with ten diverse languages, over twice the number supported in vision-language tasks to date. We evaluate on multilingual image-sentence retrieval and outperform prior work by 3-4% with less than 1/5th the training parameters compared to other word embedding methods."
KATE SAENKO,Detecting cross-modal inconsistency to defend against neural fake news,"Large-scale dissemination of disinformation online intended to mislead or deceive the general population is a major societal problem. Rapid progression in image, video, and natural language generative models has only exacerbated this situation and intensified our need for an effective defense mechanism. While existing approaches have been proposed to defend against neural fake news, they are generally constrained to the very limited setting where articles only have text and metadata such as the title and authors. In this paper, we introduce the more realistic and challenging task of defending against machine-generated news that also includes images and captions. To identify the possible weaknesses that adversaries can exploit, we create a NeuralNews dataset which is comprised of 4 different types of generated articles as well as conduct a series of human user study experiments based on this dataset. Coupled with providing a relatively effective approach based on detecting visual-semantic inconsistencies, the valuable insights gleaned from our user study experiments and, consequently, this paper will serve as an effective first line of defense and a valuable reference for future work in defending against machine-generated disinformation."
KATE SAENKO,Surprisingly simple semi-supervised domain adaptation with pretraining and consistency,"Visual domain adaptation involves learning to classify images from a target visual domain using labels available in a different source domain. A range of prior work uses adversarial domain alignment to try and learn a domain invariant feature space, where a good source classifier can perform well on target data. This however, can lead to errors where class A features in the target domain get aligned to class B features in source. We show that in the presence of a few target labels, simple techniques like selfsupervision (via rotation prediction) and consistency regularization can be effective without any adversarial alignment to learn a good target classifier. Our Pretraining and Consistency (PAC) approach, can achieve state of the art accuracy on this semi-supervised domain adaptation task, surpassing multiple adversarial domain alignment methods, across multiple datasets. Notably, it outperforms all recent approaches by 3-5% on the large and challenging DomainNet benchmark, showing the strength of these simple techniques in fixing errors made by adversarial alignment"
KATE SAENKO,How to train your quadrotor: a framework for consistently smooth and responsive flight control via reinforcement learning,"We focus on the problem of reliably training Reinforcement Learning (RL) models (agents) for stable low-level control in embedded systems and test our methods on a high-performance, custom-built quadrotor platform. A common but often under-studied problem in developing RL agents for continuous control is that the control policies developed are not always smooth. This lack of smoothness can be a major problem when learning controllers as it can result in control instability and hardware failure. Issues of noisy control are further accentuated when training RL agents in simulation due to simulators ultimately being imperfect representations of reality — what is known as the reality gap. To combat issues of instability in RL agents, we propose a systematic framework, ‘REinforcement-based transferable Agents through Learning’ (RE+AL), for designing simulated training environments which preserve the quality of trained agents when transferred to real platforms. RE+AL is an evolution of the Neuroflight infrastructure detailed in technical reports prepared by members of our research group. Neuroflight is a state-of-the-art framework for training RL agents for low-level attitude control. RE+AL improves and completes Neuroflight by solving a number of important limitations that hindered the deployment of Neuroflight to real hardware. We benchmark RE+AL on the NF1 racing quadrotor developed as part of Neuroflight. We demonstrate that RE+AL significantly mitigates the previously observed issues of smoothness in RL agents. Additionally, RE+AL is shown to consistently train agents that are flight-capable and with minimal degradation in controller quality upon transfer. RE+AL agents also learn to perform better than a tuned PID controller, with better tracking errors, smoother control and reduced power consumption. To the best of our knowledge, RE+AL agents are the first RL-based controllers trained in simulation to outperform a well-tuned PID controller on a real-world controls problem that is solvable with classical control."
KATE SAENKO,Regularizing action policies for smooth control with reinforcement learning,"A critical problem with the practical utility of controllers trained with deep Reinforcement Learning (RL) is the notable lack of smoothness in the actions learned by the RL policies. This trend often presents itself in the form of control signal oscillation and can result in poor control, high power consumption, and undue system wear. We introduce Conditioning for Action Policy Smoothness (CAPS), an effective yet intuitive regularization on action policies, which offers consistent improvement in the smoothness of the learned state-toaction mappings of neural network controllers, reflected in the elimination of high-frequency components in the control signal. Tested on a real system, improvements in controller smoothness on a quadrotor drone resulted in an almost 80% reduction in power consumption while consistently training flight-worthy controllers."
KATE SAENKO,Learning to scale multilingual representations for vision-language tasks,"Current multilingual vision-language models either require a large number of additional parameters for each supported language, or suffer performance degradation as languages are added. In this paper, we propose a Scalable Multilingual Aligned Language Representation (SMALR) that supports many languages with few model parameters without sacrificing downstream task performance. SMALR learns a fixed size language-agnostic representation for most words in a multilingual vocabulary, keeping language-specific features for just a few. We use a masked cross-language modeling loss to align features with context from other languages. Additionally, we propose a cross-lingual consistency module that ensures predictions made for a query and its machine translation are comparable. The effectiveness of SMALR is demonstrated with ten diverse languages, over twice the number supported in vision-language tasks to date. We evaluate on multilingual image-sentence retrieval and outperform prior work by 3-4% with less than 1/5th the training parameters compared to other word embedding methods."
KATE SAENKO,Task2Sim: towards effective pre-training and transfer from synthetic data,
KATE SAENKO,SynCDR : training cross domain retrieval models with synthetic data,
ELIZABETH COHEN,Reading problems of the bottom third: grades one through six,
ELIZABETH COHEN,Ecological Impacts of the 2015/16 El Niño in the Central Equatorial Pacific,
ELIZABETH COHEN,"BMQ : Boston medical quarterly: v. 15, no. 1-4",
ELIZABETH COHEN,CATALISE: a multinational and multidisciplinary Delphi consensus study. Identifying language impairments in children,"Delayed or impaired language development is a common developmental concern, yet there is little agreement about the criteria used to identify and classify language impairments in children. Children's language difficulties are at the interface between education, medicine and the allied professions, who may all adopt different approaches to conceptualising them. Our goal in this study was to use an online Delphi technique to see whether it was possible to achieve consensus among professionals on appropriate criteria for identifying children who might benefit from specialist services. We recruited a panel of 59 experts representing ten disciplines (including education, psychology, speech-language therapy/pathology, paediatrics and child psychiatry) from English-speaking countries (Australia, Canada, Ireland, New Zealand, United Kingdom and USA). The starting point for round 1 was a set of 46 statements based on articles and commentaries in a special issue of a journal focusing on this topic. Panel members rated each statement for both relevance and validity on a seven-point scale, and added free text comments. These responses were synthesised by the first two authors, who then removed, combined or modified items with a view to improving consensus. The resulting set of statements was returned to the panel for a second evaluation (round 2). Consensus (percentage reporting 'agree' or 'strongly agree') was at least 80 percent for 24 of 27 round 2 statements, though many respondents qualified their response with written comments. These were again synthesised by the first two authors. The resulting consensus statement is reported here, with additional summary of relevant evidence, and a concluding commentary on residual disagreements and gaps in the evidence base."
ELIZABETH COHEN,Phase 2 of CATALISE: a multinational and multidisciplinary Delphi consensus study of problems with language development: terminology,"BACKGROUND: Lack of agreement about criteria and terminology for children's language problems affects access to services as well as hindering research and practice. We report the second phase of a study using an online Delphi method to address these issues. In the first phase, we focused on criteria for language disorder. Here we consider terminology. METHODS: The Delphi method is an iterative process in which an initial set of statements is rated by a panel of experts, who then have the opportunity to view anonymised ratings from other panel members. On this basis they can either revise their views or make a case for their position. The statements are then revised based on panel feedback, and again rated by and commented on by the panel. In this study, feedback from a second round was used to prepare a final set of statements in narrative form. The panel included 57 individuals representing a range of professions and nationalities. RESULTS: We achieved at least 78% agreement for 19 of 21 statements within two rounds of ratings. These were collapsed into 12 statements for the final consensus reported here. The term ‘Language Disorder’ is recommended to refer to a profile of difficulties that causes functional impairment in everyday life and is associated with poor prognosis. The term, ‘Developmental Language Disorder’ (DLD) was endorsed for use when the language disorder was not associated with a known biomedical aetiology. It was also agreed that (a) presence of risk factors (neurobiological or environmental) does not preclude a diagnosis of DLD, (b) DLD can co‐occur with other neurodevelopmental disorders (e.g. ADHD) and (c) DLD does not require a mismatch between verbal and nonverbal ability. CONCLUSIONS: This Delphi exercise highlights reasons for disagreements about terminology for language disorders and proposes standard definitions and nomenclature."
ELIZABETH COHEN,Reproductive inequality in humans and other mammals,"To address claims of human exceptionalism, we determine where humans fit within the greater mammalian distribution of reproductive inequality. We show that humans exhibit lower reproductive skew (i.e., inequality in the number of surviving offspring) among males and smaller sex differences in reproductive skew than most other mammals, while nevertheless falling within the mammalian range. Additionally, female reproductive skew is higher in polygynous human populations than in polygynous nonhumans mammals on average. This patterning of skew can be attributed in part to the prevalence of monogamy in humans compared to the predominance of polygyny in nonhuman mammals, to the limited degree of polygyny in the human societies that practice it, and to the importance of unequally held rival resources to women's fitness. The muted reproductive inequality observed in humans appears to be linked to several unusual characteristics of our species-including high levels of cooperation among males, high dependence on unequally held rival resources, complementarities between maternal and paternal investment, as well as social and legal institutions that enforce monogamous norms."
LOUIS CHUDE-SOKEI,Wilson Harris: an ontological promiscuity,"[Excerpt] ""I’ve always thought that the problem with the literary and cultural politics of the Anglophone world was that we’ve never had an actual, formal surrealist movement. Yes, there are writers and thinkers in the English-speaking world that are verifiably surreal (though not members of the official movement) and many that are described as surrealist, for example the writer who is the focus of this essay, the recently deceased Guyanese novelist, critic, and visionary, Wilson Harris, who passed away in March of this year. And yes, the impact of the Surrealist International was global. As I will discuss, it had a significant impact in the Caribbean, which is partly what justifies discussing Wilson Harris in this context. Though seen as a minor or cult figure, or an example of “art brut,” I’d like to help make clear his standing in a richer tradition of thinking and writing than previously acknowledged. I’d like to also suggest ways that his legacy can and should make a difference."""
LOUIS CHUDE-SOKEI,Introduction: what was Black Studies?,
LOUIS CHUDE-SOKEI,"“Dr. Satan’s Echo Chamber: reggae, technology and the diaspora process,” reprint.",Reprint of an essay first published in 1997.
LOUIS CHUDE-SOKEI,Machines and the ethics of miscegenation,
LOUIS CHUDE-SOKEI,"Prognosticating echoes: Race, sound and naturalizing technology",
MARK W GRINSTAFF,Meta-analysis and systematic review of skin graft donor-site dressings with future guidelines.,"Background: Many types of split-thickness skin graft (STSG) donor-site dressings are available with little consensus from the literature on the optimal dressing type. The purpose of this systematic review was to analyze the most recent outcomes regarding moist and nonmoist dressings for STSG donor sites. Methods: A comprehensive systematic review was conducted across PubMed/MEDLINE, EMBASE, and Cochrane Library databases to search for comparative studies evaluating different STSG donor-site dressings in adult subjects published between 2008 and 2017. The quality of randomized controlled trials was assessed using the Jadad scale. Data were collected on donor-site pain, rate of epithelialization, infection rate, cosmetic appearance, and cost. Meta-analysis was performed for reported pain scores. Results: A total of 41 articles were included comparing 44 dressings. Selected studies included analysis of donor-site pain (36 of 41 articles), rate of epithelialization (38 of 41), infection rate (25 of 41), cosmetic appearance (20 of 41), and cost (10 of 41). Meta-analysis revealed moist dressings result in lower pain (pooled effect size = 1.44). A majority of articles (73%) reported better reepithelialization rates with moist dressings. Conclusion: The literature on STSG donor-site dressings has not yet identified an ideal dressing. Although moist dressings provide superior outcomes with regard to pain control and wound healing, there continues to be a lack of standardization. The increasing commercial availability and marketing of novel dressings necessitates the development of standardized research protocols to design better comparison studies and assess true efficacy."
MARK W GRINSTAFF,OvoAMtht from Methyloversatilis thermotolerans ovothiol biosynthesis is a bifunction enzyme: thiol oxygenase and sulfoxide synthase activities,"Mononuclear non-heme iron enzymes are a large class of enzymes catalyzing a wide-range of reactions. In this work, we report that a non-heme iron enzyme in Methyloversatilis thermotolerans, OvoAMtht, has two different activities, as a thiol oxygenase and a sulfoxide synthase. When cysteine is presented as the only substrate, OvoAMtht is a thiol oxygenase. In the presence of both histidine and cysteine as substrates, OvoAMtht catalyzes the oxidative coupling between histidine and cysteine (a sulfoxide synthase). Additionally, we demonstrate that both substrates and the active site iron's secondary coordination shell residues exert exquisite control over the dual activities of OvoAMtht (sulfoxide synthase vs. thiol oxygenase activities). OvoAMtht is an excellent system for future detailed mechanistic investigation on how metal ligands and secondary coordination shell residues fine-tune the iron-center electronic properties to achieve different reactivities."
MARK W GRINSTAFF,Sustainable polycarbonate adhesives for dry and aqueous conditions with thermoresponsive properties,"Pressure sensitive adhesives are ubiquitous in commodity products such as tapes, bandages, labels, packaging, and insulation. With single use plastics comprising almost half of yearly plastic production, it is essential that the design, synthesis, and decomposition products of future materials, including polymer adhesives, are within the context of a healthy ecosystem along with comparable or superior performance to conventional materials. Here we show a series of sustainable polymeric adhesives, with an eco-design, that perform in both dry and wet environments. The terpolymerization of propylene oxide, glycidyl butyrate, and CO2, catalyzed by a cobalt salen complex bearing a quaternary ammonium salt, yields the poly(propylene-co-glycidyl butyrate carbonate)s (PPGBC)s. This polymeric adhesive system, composed of environmentally benign building blocks, implements carbon dioxide sequestration techniques, poses minimal environmental hazards, exhibits varied peel strengths from scotch tape to hot-melt wood-glue, and adheres to metal, glass, wood, and Teflon® surfaces."
MARK W GRINSTAFF,Implications for an imidazol-2-yl carbene intermediate in the rhodanase-catalyzed C-S bond formation reaction of anaerobic ergothioneine biosynthesis,"In the anaerobic ergothioneine biosynthetic pathway, a rhodanese domain containing enzyme (EanB) activates tne hercynine's sp2 ε-C-H Dona ana replaces it with a C-S bond to produce ergothioneine. The key intermediate for this trans-sulfuration reaction is the Cys412 persulfide. Substitution of the EanB-Cys412 persulfide with a Cys412 perselenide does not yield the selenium analog of ergothioneine, selenoneine. However, in deuterated buffer, the perselenide-modified EanB catalyzes the deuterium exchange between hercynine's sp2 ε-C-H bond and D2O. Results from QM/MM calculations suggest that the reaction involves a carbene intermediate and that Tyr353 plays a key role. We hypothesize that modulating the pKa of Tyr353 will affect the deuterium-exchange rate. Indeed, the 3,5-difluoro tyrosine containing EanB catalyzes the deuterium exchange reaction with k ex of ~10-fold greater than the wild-type EanB (EanBWT). With regards to potential mechanisms, these results support the involvement of a carbene intermediate in EanB-catalysis, rendering EanB as one of the few carbene-intermediate involving enzymatic systems."
MARK W GRINSTAFF,Single-step replacement of an unreactive C-H bond by a C-S bond using polysulfide as the direct sulfur source in anaerobic ergothioneine biosynthesis,"Ergothioneine, a natural longevity vitamin and antioxidant, is a thiol-histidine derivative. Recently, two types of biosynthetic pathways were reported. In the aerobic ergothioneine biosynthesis, a non-heme iron enzyme incorporates a sulfoxide to an sp2 C-H bond in trimethyl-histidine (hercynine) through oxidation reactions. In contrast, in the anaerobic ergothioneine biosynthetic pathway in a green sulfur bacterium, Chlorobium limicola, a rhodanese domain containing protein (EanB) directly replaces this unreactive hercynine C-H bond with a C-S bond. Herein, we demonstrate that polysulfide (HSSnSR) is the direct sulfur-source in EanB-catalysis. After identifying EanB's substrates, X-ray crystallography of several intermediate states along with mass spectrometry results provide additional mechanistic details for this reaction. Further, quantum mechanics/molecular mechanics (QM/MM) calculations reveal that protonation of Nπ of hercynine by Tyr353 with the assistance of Thr414 is a key activation step for the hercynine sp2 C-H bond in this trans-sulfuration reaction."
MARK W GRINSTAFF,"Fluorescent dendritic micro-hydrogels: synthesis, analysis and use in single-cell detection","Hydrogels are of keen interest for a wide range of medical and biotechnological applications including as 3D substrate structures for the detection of proteins, nucleic acids, and cells. Hydrogel parameters such as polymer wt % and crosslink density are typically altered for a specific application; now, fluorescence can be incorporated into such criteria by specific macromonomer selection. Intrinsic fluorescence was observed at λmax 445 nm from hydrogels polymerized from lysine and aldehyde- terminated poly(ethylene glycol) macromonomers upon excitation with visible light. The hydrogel’s photochemical properties are consistent with formation of a nitrone functionality. Printed hydrogels of 150 μm were used to detect individual cell adherence via a decreased in fluorescence. The use of such intrinsically fluorescent hydrogels as a platform for cell sorting and detection expands the current repertoire of tools available."
MARK W GRINSTAFF,A progesterone biosensor derived from microbial screening,"Bacteria are an enormous and largely untapped reservoir of biosensing proteins. We describe an approach to identify and isolate bacterial allosteric transcription factors (aTFs) that recognize a target analyte and to develop these TFs into biosensor devices. Our approach utilizes a combination of genomic screens and functional assays to identify and isolate biosensing TFs, and a quantum-dot Förster Resonance Energy Transfer (FRET) strategy for transducing analyte recognition into real-time quantitative measurements. We use this approach to identify a progesterone-sensing bacterial aTF and to develop this TF into an optical sensor for progesterone. The sensor detects progesterone in artificial urine with sufficient sensitivity and specificity for clinical use, while being compatible with an inexpensive and portable electronic reader for point-of-care applications. Our results provide proof-of-concept for a paradigm of microbially-derived biosensors adaptable to inexpensive, real-time sensor devices."
MARK W GRINSTAFF,Triple contrast CT method enables simultaneous evaluation of articular cartilage composition and segmentation,"Early degenerative changes of articular cartilage are detected using contrast-enhanced computed tomography (CT) with a cationic contrast agent (CA). However, cationic CA diffusion into degenerated cartilage decreases with proteoglycan depletion and increases with elevated water content, thus hampering tissue evaluation at early diffusion time points. Furthermore, the contrast at synovial fluid-cartilage interface diminishes as a function of diffusion time hindering accurate cartilage segmentation. For the first time, we employ quantitative dual-energy CT (QDECT) imaging utilizing a mixture of three CAs (cationic CA4+ and non-ionic gadoteridol which are sensitive to proteoglycan and water contents, respectively, and bismuth nanoparticles which highlight the cartilage surface) to simultaneously segment the articulating surfaces and determine of the cartilage condition. Intact healthy, proteoglycan-depleted, and mechanically injured bovine cartilage samples (n = 27) were halved and imaged with synchrotron microCT 2-h post immersion in triple CA or in dual CA (CA4+ and gadoteridol). CA4+ and gadoteridol partitions were determined using QDECT, and pairwise evaluation of these partitions was conducted for samples immersed in dual and triple CAs. In conclusion, the triple CA method is sensitive to proteoglycan depletion while maintaining sufficient contrast at the articular surface to enable detection of cartilage lesions caused by mechanical impact."
STEPHAN W ANDERSON,Bayesian reconstruction of magnetic resonance images using Gaussian processes,"A central goal of modern magnetic resonance imaging (MRI) is to reduce the time required to produce high-quality images. Efforts have included hardware and software innovations such as parallel imaging, compressed sensing, and deep learning-based reconstruction. Here, we propose and demonstrate a Bayesian method to build statistical libraries of magnetic resonance (MR) images in k-space and use these libraries to identify optimal subsampling paths and reconstruction processes. Specifically, we compute a multivariate normal distribution based upon Gaussian processes using a publicly available library of T1-weighted images of healthy brains. We combine this library with physics-informed envelope functions to only retain meaningful correlations in k-space. This covariance function is then used to select a series of ring-shaped subsampling paths using Bayesian optimization such that they optimally explore space while remaining practically realizable in commercial MRI systems. Combining optimized subsampling paths found for a range of images, we compute a generalized sampling path that, when used for novel images, produces superlative structural similarity and error in comparison to previously reported reconstruction processes (i.e. 96.3% structural similarity and < 0.003 normalized mean squared error from sampling only 12.5% of the k-space data). Finally, we use this reconstruction process on pathological data without retraining to show that reconstructed images are clinically useful for stroke identification. Since the model trained on images of healthy brains could be directly used for predictions in pathological brains without retraining, it shows the inherent transferability of this approach and opens doors to its widespread use."
SUSAN MCGURK,"The relationship between cognitive functioning, age and employment in people with severe mental illnesses in an urban area in India: a longitudinal study.","Although there is substantial evidence of the association between cognitive impairment and work in people with severe mental illnesses (SMI) in developed countries, less is known about this relationship in developing countries such as India. Studies showing higher rates of employment in people with SMI in developing countries than developed ones raise the question of whether cognitive functioning is related to work status and characteristics of work (e.g., wages earned). We conducted a one-year follow-up study to investigate the relationship between employment and cognitive functioning, assessed with the Montreal Cognitive Assessment (MoCA), in 150 participants with SMI (92% schizophrenia) living in an urban area and receiving psychiatric outpatient treatment at a public hospital in India. The MoCA had good internal reliability and test-retest reliability over the one-year period. Better cognitive functioning was associated with younger age, shorter duration of illness, higher education, and male gender. Both younger and older participants with higher cognitive functioning at baseline were more likely to be employed at baseline and one year later. Work status at baseline and one year follow-up was consistently related to executive functions among younger participants, and to attention among older participants, suggesting changes over the course of illness in the importance of specific cognitive domains for achieving satisfactory work performance. The findings suggest that cognitive functioning is associated with employment in people with SMI in India. Attention to impaired cognitive functioning may be critical to improving employment outcomes in this population."
SUSAN MCGURK,Cognitive remediation and psychosocial rehabilitation for individuals with severe mental illness,
SUSAN MCGURK,Report on ISCTM consensus meeting on clinical assessment of response to treatment of cognitive impairment in schizophrenia,
SUSAN MCGURK,Cognitive and metacognitive factors predict engagement in employment in individuals with first episode psychosis,"BACKGROUND: Research has demonstrated that cognitive abilities predict work outcomes in people with psychosis. Cognitive Remediation Programs go some way in improving work outcomes, but individuals still experience difficulty maintaining employment. Metacognition has been demonstrated to predict work performance in individuals with schizophrenia, but this has not yet been applied to First Episode Psychosis (FEP). This study assessed whether metacognition, intellectual aptitude and functional capacity can predict engagement in work and number of hours of work within FEP. METHODS: Fifty-two individuals with psychosis, from an Early Intervention in Psychosis service, completed measures of IQ, metacognition (Metacognitive Assessment Interview), functional capacity (UPSA), and functional outcome (hours spent in structured activity per week, including employment). RESULTS: Twenty-six participants (22 males, 4 females) were employed and twenty-six (22 males, 4 females) were not employed. IQ and metacognition were significantly associated with whether the individual was engaged in employment [IQ (p = .02) and metacognition (p = 006)]. When controlling for IQ, metacognition (differentiation subscale) remained significant (p = .04). Next, including only those employed, no cognitive nor metacognitive factors predicted number of hours in employment. DISCUSSION: This is the first study to directly assess metacognition as a predictor of work hours for individuals with FEP. This study highlights the importance of enhancing metacognitive ability in order to improve likelihood of, and engagement in, employment for those with FEP."
SUSAN MCGURK,Psychometric properties of the mock interview rating scale for schizophrenia and other serious mental illnesses,"BACKGROUND: Over the past 10 years, job interview training has emerged as an area of study among adults with schizophrenia and other serious mental illnesses who face significant challenges when navigating job interviews. The field of mental health services research has limited access to assessments of job interview skills with rigorously evaluated psychometric properties. OBJECTIVE: We sought to evaluate the initial psychometric properties of a measure assessing job interview skills via role-play performance. METHODS: As part of a randomized controlled trial, 90 adults with schizophrenia or other serious mental illnesses completed a job interview role-play assessment with eight items (and scored using anchors) called the mock interview rating scale (MIRS). A classical test theory analysis was conducted including confirmatory factor analyses, Rasch model analysis and calibration, and differential item functioning; along with inter-rater, internal consistency, and test-retest reliabilities. Pearson correlations were used to evaluate construct, convergent, divergent, criterion, and predictive validity by correlating the MIRS with demographic, clinical, cognitive, work history measures, and employment outcomes. RESULTS: Our analyses resulted in the removal of a single item (sounding honest) and yielded a unidimensional total score measurement with support for its inter-rater reliability, internal consistency, and test-retest reliability. There was initial support for the construct, convergent, criterion, and predictive validities of the MIRS, as it correlated with measures of social competence, neurocognition, valuing job interview training, and employment outcomes. Meanwhile, the lack of correlations with race, physical health, and substance abuse lent support for divergent validity. CONCLUSION: This study presents initial evidence that the seven-item version of the MIRS has acceptable psychometric properties supporting its use to assess job interview skills reliably and validly among adults with schizophrenia and other serious mental illnesses. CLINICAL TRIAL REGISTRATION: NCT03049813."
DOROTHY KELLY,The construction of a vocabulary test for the intermediate grades,
JAMES E GALAGAN,A Blind Deconvolution Approach to High-Resolution Mapping of Transcription Factor Binding Sites from ChIP-Seq Data,"CSdeconv is a novel method for determining the location of transcription factor binding from ChIP-seq data that discriminates closely-spaced sites. We present CSDeconv, a computational method that determines locations of transcription factor binding from ChIP-seq data. CSDeconv differs from prior methods in that it uses a blind deconvolution approach that allows closely-spaced binding sites to be called accurately. We apply CSDeconv to novel ChIP-seq data for DosR binding in Mycobacterium tuberculosis and to existing data for GABP in humans and show that it can discriminate binding sites separated by as few as 40 bp."
JAMES E GALAGAN,Large-Scale Identification of Genetic Design Strategies Using Local Search,"In the past decade, computational methods have been shown to be well suited to unraveling the complex web of metabolic reactions in biological systems. Methods based on flux–balance analysis (FBA) and bi-level optimization have been used to great effect in aiding metabolic engineering. These methods predict the result of genetic manipulations and allow for the best set of manipulations to be found computationally. Bi-level FBA is, however, limited in applicability because the required computational time and resources scale poorly as the size of the metabolic system and the number of genetic manipulations increase. To overcome these limitations, we have developed Genetic Design through Local Search (GDLS), a scalable, heuristic, algorithmic method that employs an approach based on local search with multiple search paths, which results in effective, low-complexity search of the space of genetic manipulations. Thus, GDLS is able to find genetic designs with greater in silico production of desired metabolites than can feasibly be found using a globally optimal search and performs favorably in comparison with heuristic searches based on evolutionary algorithms and simulated annealing."
JAMES E GALAGAN,Interpreting Expression Data with Metabolic Flux Models: Predicting Mycobacterium tuberculosis Mycolic Acid Production,"Metabolism is central to cell physiology, and metabolic disturbances play a role in numerous disease states. Despite its importance, the ability to study metabolism at a global scale using genomic technologies is limited. In principle, complete genome sequences describe the range of metabolic reactions that are possible for an organism, but cannot quantitatively describe the behaviour of these reactions. We present a novel method for modeling metabolic states using whole cell measurements of gene expression. Our method, which we call E-Flux (as a combination of flux and expression), extends the technique of Flux Balance Analysis by modeling maximum flux constraints as a function of measured gene expression. In contrast to previous methods for metabolically interpreting gene expression data, E-Flux utilizes a model of the underlying metabolic network to directly predict changes in metabolic flux capacity. We applied E-Flux to Mycobacterium tuberculosis, the bacterium that causes tuberculosis (TB). Key components of mycobacterial cell walls are mycolic acids which are targets for several first-line TB drugs. We used E-Flux to predict the impact of 75 different drugs, drug combinations, and nutrient conditions on mycolic acid biosynthesis capacity in M. tuberculosis, using a public compendium of over 400 expression arrays. We tested our method using a model of mycolic acid biosynthesis as well as on a genome-scale model of M. tuberculosis metabolism. Our method correctly predicts seven of the eight known fatty acid inhibitors in this compendium and makes accurate predictions regarding the specificity of these compounds for fatty acid biosynthesis. Our method also predicts a number of additional potential modulators of TB mycolic acid biosynthesis. E-Flux thus provides a promising new approach for algorithmically predicting metabolic state from gene expression data. Author Summary The ability of cells to survive and grow depends on their ability to metabolize nutrients and create products vital for cell function. This is done through a complex network of reactions controlled by many genes. Changes in cellular metabolism play a role in a wide variety of diseases. However, despite the availability of genome sequences and of genome-scale expression data, which give information about which genes are present and how active they are, our ability to use these data to understand changes in cellular metabolism has been limited. We present a new approach to this problem, linking gene expression data with models of cellular metabolism. We apply the method to predict the effects of drugs and agents on Mycobacterium tuberculosis (M. tb). Virulence, growth in human hosts, and drug resistance are all related to changes in M. tb's metabolism. We predict the effects of a variety of conditions on the production of mycolic acids, essential cell wall components. Our method successfully identifies seven of the eight known mycolic acid inhibitors in a compendium of 235 conditions, and identifies the top anti-TB drugs in this dataset. We anticipate that the method will have a range of applications in metabolic engineering, the characterization of disease states, and drug discovery."
JAMES E GALAGAN,TB Database: An Integrated Platform for Tuberculosis Research,"The effective control of tuberculosis (TB) has been thwarted by the need for prolonged, complex and potentially toxic drug regimens, by reliance on an inefficient vaccine and by the absence of biomarkers of clinical status. The promise of the genomics era for TB control is substantial, but has been hindered by the lack of a central repository that collects and integrates genomic and experimental data about this organism in a way that can be readily accessed and analyzed. The Tuberculosis Database (TBDB) is an integrated database providing access to TB genomic data and resources, relevant to the discovery and development of TB drugs, vaccines and biomarkers. The current release of TBDB houses genome sequence data and annotations for 28 different Mycobacterium tuberculosis strains and related bacteria. TBDB stores pre- and post-publication gene-expression data from M. tuberculosis and its close relatives. TBDB currently hosts data for nearly 1500 public tuberculosis microarrays and 260 arrays for Streptomyces. In addition, TBDB provides access to a suite of comparative genomics and microarray analysis software. By bringing together M. tuberculosis genome annotation and gene-expression data with a suite of analysis tools, TBDB (http://www.tbdb.org) provides a unique discovery platform for TB research."
JAMES E GALAGAN,Short-Term Genome Evolution of Listeria Monocytogenes in a Non-Controlled Environment,"BACKGROUND: While increasing data on bacterial evolution in controlled environments are available, our understanding of bacterial genome evolution in natural environments is limited. We thus performed full genome analyses on four Listeria monocytogenes, including human and food isolates from both a 1988 case of sporadic listeriosis and a 2000 listeriosis outbreak, which had been linked to contaminated food from a single processing facility. All four isolates had been shown to have identical subtypes, suggesting that a specific L. monocytogenes strain persisted in this processing plant over at least 12 years. While a genome sequence for the 1988 food isolate has been reported, we sequenced the genomes of the 1988 human isolate as well as a human and a food isolate from the 2000 outbreak to allow for comparative genome analyses. RESULTS: The two L. monocytogenes isolates from 1988 and the two isolates from 2000 had highly similar genome backbone sequences with very few single nucleotide (nt) polymorphisms (1 – 8 SNPs/isolate; confirmed by re-sequencing). While no genome rearrangements were identified in the backbone genome of the four isolates, a 42 kb prophage inserted in the chromosomal comK gene showed evidence for major genome rearrangements. The human-food isolate pair from each 1988 and 2000 had identical prophage sequence; however, there were significant differences in the prophage sequences between the 1988 and 2000 isolates. Diversification of this prophage appears to have been caused by multiple homologous recombination events or possibly prophage replacement. In addition, only the 2000 human isolate contained a plasmid, suggesting plasmid loss or acquisition events. Surprisingly, besides the polymorphisms found in the comK prophage, a single SNP in the tRNA Thr-4 prophage represents the only SNP that differentiates the 1988 isolates from the 2000 isolates. CONCLUSION: Our data support the hypothesis that the 2000 human listeriosis outbreak was caused by a L. monocytogenes strain that persisted in a food processing facility over 12 years and show that genome sequencing is a valuable and feasible tool for retrospective epidemiological analyses. Short-term evolution of L. monocytogenes in non-controlled environments appears to involve limited diversification beyond plasmid gain or loss and prophage diversification, highlighting the importance of phages in bacterial evolution."
JAMES E GALAGAN,A progesterone biosensor derived from microbial screening,"Bacteria are an enormous and largely untapped reservoir of biosensing proteins. We describe an approach to identify and isolate bacterial allosteric transcription factors (aTFs) that recognize a target analyte and to develop these TFs into biosensor devices. Our approach utilizes a combination of genomic screens and functional assays to identify and isolate biosensing TFs, and a quantum-dot Förster Resonance Energy Transfer (FRET) strategy for transducing analyte recognition into real-time quantitative measurements. We use this approach to identify a progesterone-sensing bacterial aTF and to develop this TF into an optical sensor for progesterone. The sensor detects progesterone in artificial urine with sufficient sensitivity and specificity for clinical use, while being compatible with an inexpensive and portable electronic reader for point-of-care applications. Our results provide proof-of-concept for a paradigm of microbially-derived biosensors adaptable to inexpensive, real-time sensor devices."
JAMES E GALAGAN,Comparative genomic characterization of Francisella tularensis strains belonging to low and high virulence subspecies,"Tularemia is a geographically widespread, severely debilitating, and occasionally lethal disease in humans. It is caused by infection by a gram-negative bacterium, Francisella tularensis. In order to better understand its potency as an etiological agent as well as its potential as a biological weapon, we have completed draft assemblies and report the first complete genomic characterization of five strains belonging to the following different Francisella subspecies (subsp.): the F. tularensis subsp. tularensis FSC033, F. tularensis subsp. holarctica FSC257 and FSC022, and F. tularensis subsp. novicida GA99-3548 and GA99-3549 strains. Here, we report the sequencing of these strains and comparative genomic analysis with recently available public Francisella sequences, including the rare F. tularensis subsp. mediasiatica FSC147 strain isolate from the Central Asian Region. We report evidence for the occurrence of large-scale rearrangement events in strains of the holarctica subspecies, supporting previous proposals that further phylogenetic subdivisions of the Type B clade are likely. We also find a significant enrichment of disrupted or absent ORFs proximal to predicted breakpoints in the FSC022 strain, including a genetic component of the Type I restriction-modification defense system. Many of the pseudogenes identified are also disrupted in the closely related rarely human pathogenic F. tularensis subsp. mediasiatica FSC147 strain, including modulator of drug activity B (mdaB) (FTT0961), which encodes a known NADPH quinone reductase involved in oxidative stress resistance. We have also identified genes exhibiting sequence similarity to effectors of the Type III (T3SS) and components of the Type IV secretion systems (T4SS). One of the genes, msrA2 (FTT1797c), is disrupted in F. tularensis subsp. mediasiatica and has recently been shown to mediate bacterial pathogen survival in host organisms. Our findings suggest that in addition to the duplication of the Francisella Pathogenicity Island, and acquisition of individual loci, adaptation by gene loss in the more recently emerged tularensis, holarctica, and mediasiatica subspecies occurred and was distinct from evolutionary events that differentiated these subspecies, and the novicida subspecies, from a common ancestor. Our findings are applicable to future studies focused on variations in Francisella subspecies pathogenesis, and of broader interest to studies of genomic pathoadaptation in bacteria. Author SummaryTularemia is a zoonotic disease that is widely disseminated throughout the Northern Hemisphere and is caused by different strain types of bacteria belonging to the genus Francisella. In general, Francisella tularensis subspecies are able to infect a wide range of mammals including humans and are often transmitted via insect vectors such as ticks. Depending on the strain and route of infection the disease may be fatal in humans. In order to better understand F. tularensis as an etiological agent as well as its potential as a biological weapon, we have completed draft sequence assemblies of five globally diverse strains. We have performed a comparative analysis of these sequences with other available public Francisella sequences of strains of differing virulence. Our analysis suggests that genome rearrangements and gene loss in specific Francisella subspecies may underlie the evolution of niche adaptation and virulence of this pathogen."
DAVID M GREER,Prospects for beyond the standard model physics searches at the deep underground neutrino experiment: DUNE collaboration,"The Deep Underground Neutrino Experiment (DUNE) will be a powerful tool for a variety of physics topics. The high-intensity proton beams provide a large neutrino flux, sampled by a near detector system consisting of a combination of capable precision detectors, and by the massive far detector system located deep underground. This configuration sets up DUNE as a machine for discovery, as it enables opportunities not only to perform precision neutrino measurements that may uncover deviations from the present three-flavor mixing paradigm, but also to discover new particles and unveil new interactions and symmetries beyond those predicted in the Standard Model (SM). Of the many potential beyond the Standard Model (BSM) topics DUNE will probe, this paper presents a selection of studies quantifying DUNE's sensitivities to sterile neutrino mixing, heavy neutral leptons, non-standard interactions, CPT symmetry violation, Lorentz invariance violation, neutrino trident production, dark matter from both beam induced and cosmogenic sources, baryon number violation, and other new physics topics that complement those at high-energy colliders and significantly extend the present reach."
DAVID M GREER,Identification and reconstruction of low-energy electrons in the ProtoDUNE-SP detector,
DAVID M GREER,Scintillation light detection in the 6-m drift-length ProtoDUNE Dual Phase liquid argon TPC,"DUNE is a dual-site experiment for long-baseline neutrino oscillation studies, neutrino astrophysics and nucleon decay searches. ProtoDUNE Dual Phase (DP) is a 6  ×  6  ×  6 m 3 liquid argon time-projection-chamber (LArTPC) that recorded cosmic-muon data at the CERN Neutrino Platform in 2019-2020 as a prototype of the DUNE Far Detector. Charged particles propagating through the LArTPC produce ionization and scintillation light. The scintillation light signal in these detectors can provide the trigger for non-beam events. In addition, it adds precise timing capabilities and improves the calorimetry measurements. In ProtoDUNE-DP, scintillation and electroluminescence light produced by cosmic muons in the LArTPC is collected by photomultiplier tubes placed up to 7 m away from the ionizing track. In this paper, the ProtoDUNE-DP photon detection system performance is evaluated with a particular focus on the different wavelength shifters, such as PEN and TPB, and the use of Xe-doped LAr, considering its future use in giant LArTPCs. The scintillation light production and propagation processes are analyzed and a comparison of simulation to data is performed, improving understanding of the liquid argon properties."
DAVID M GREER,Event Horizon Telescope imaging of the archetypal blazar 3C 279 at an extreme 20 microarcsecond resolution,"3C 279 is an archetypal blazar with a prominent radio jet that show broadband flux density variability across the entire electromagnetic spectrum. We use an ultra-high angular resolution technique – global Very Long Baseline Interferometry (VLBI) at 1.3 mm (230 GHz) – to resolve the innermost jet of 3C 279 in order to study its fine-scale morphology close to the jet base where highly variable γ-ray emission is thought to originate, according to various models. The source was observed during four days in April 2017 with the Event Horizon Telescope at 230 GHz, including the phased Atacama Large Millimeter/submillimeter Array (ALMA), at an angular resolution of ∼20 μas (at a redshift of z = 0.536 this corresponds to ∼0.13 pc  ∼ 1700 Schwarzschild radii with a black hole mass M<jats:sub>BH</jats:sub> = 8 × 10^8 M_⊙). Imaging and model-fitting techniques were applied to the data to parameterize the fine-scale source structure and its variation. We find a multicomponent inner jet morphology with the northernmost component elongated perpendicular to the direction of the jet, as imaged at longer wavelengths. The elongated nuclear structure is consistent on all four observing days and across different imaging methods and model-fitting techniques, and therefore appears robust. Owing to its compactness and brightness, we associate the northern nuclear structure as the VLBI “core”. This morphology can be interpreted as either a broad resolved jet base or a spatially bent jet. We also find significant day-to-day variations in the closure phases, which appear most pronounced on the triangles with the longest baselines. Our analysis shows that this variation is related to a systematic change of the source structure. Two inner jet components move non-radially at apparent speeds of ∼15 c and ∼20 c (∼1.3 and ∼1.7 μas day^−1, respectively), which more strongly supports the scenario of traveling shocks or instabilities in a bent, possibly rotating jet. The observed apparent speeds are also coincident with the 3C 279 large-scale jet kinematics observed at longer (cm) wavelengths, suggesting no significant jet acceleration between the 1.3 mm core and the outer jet. The intrinsic brightness temperature of the jet components are ≲10^10 K, a magnitude or more lower than typical values seen at ≥7 mm wavelengths. The low brightness temperature and morphological complexity suggest that the core region of 3C 279 becomes optically thin at short (mm) wavelengths."
DAVID M GREER,First M87 Event Horizon Telescope results. II. Array and instrumentation,"The Event Horizon Telescope (EHT) is a very long baseline interferometry (VLBI) array that comprises millimeter- and submillimeter-wavelength telescopes separated by distances comparable to the diameter of the Earth. At a nominal operating wavelength of ~1.3 mm, EHT angular resolution (λ/D) is ~25 μas, which is sufficient to resolve nearby supermassive black hole candidates on spatial and temporal scales that correspond to their event horizons. With this capability, the EHT scientific goals are to probe general relativistic effects in the strong-field regime and to study accretion and relativistic jet formation near the black hole boundary. In this Letter we describe the system design of the EHT, detail the technology and instrumentation that enable observations, and provide measures of its performance. Meeting the EHT science objectives has required several key developments that have facilitated the robust extension of the VLBI technique to EHT observing wavelengths and the production of instrumentation that can be deployed on a heterogeneous array of existing telescopes and facilities. To meet sensitivity requirements, high-bandwidth digital systems were developed that process data at rates of 64 gigabit s−1, exceeding those of currently operating cm-wavelength VLBI arrays by more than an order of magnitude. Associated improvements include the development of phasing systems at array facilities, new receiver installation at several sites, and the deployment of hydrogen maser frequency standards to ensure coherent data capture across the array. These efforts led to the coordination and execution of the first Global EHT observations in 2017 April, and to event-horizon-scale imaging of the supermassive black hole candidate in M87."
DAVID M GREER,First M87 Event Horizon Telescope results. I. The shadow of the supermassive black hole,"When surrounded by a transparent emission region, black holes are expected to reveal a dark shadow caused by gravitational light bending and photon capture at the event horizon. To image and study this phenomenon, we have assembled the Event Horizon Telescope, a global very long baseline interferometry array observing at a wavelength of 1.3 mm. This allows us to reconstruct event-horizon-scale images of the supermassive black hole candidate in the center of the giant elliptical galaxy M87. We have resolved the central compact radio source as an asymmetric bright emission ring with a diameter of 42 ± 3 μas, which is circular and encompasses a central depression in brightness with a flux ratio gsim10:1. The emission ring is recovered using different calibration and imaging schemes, with its diameter and width remaining stable over four different observations carried out in different days. Overall, the observed image is consistent with expectations for the shadow of a Kerr black hole as predicted by general relativity. The asymmetry in brightness in the ring can be explained in terms of relativistic beaming of the emission from a plasma rotating close to the speed of light around a black hole. We compare our images to an extensive library of ray-traced general-relativistic magnetohydrodynamic simulations of black holes and derive a central mass of M = (6.5 ± 0.7) × 10^9 M ⊙. Our radio-wave observations thus provide powerful evidence for the presence of supermassive black holes in centers of galaxies and as the central engines of active galactic nuclei. They also present a new tool to explore gravity in its most extreme limit and on a mass scale that was so far not accessible."
MARK KRAMER,First M87 Event Horizon Telescope results. III. Data processing and calibration,"We present the calibration and reduction of Event Horizon Telescope (EHT) 1.3 mm radio wavelength observations of the supermassive black hole candidate at the center of the radio galaxy M87 and the quasar 3C 279, taken during the 2017 April 5–11 observing campaign. These global very long baseline interferometric observations include for the first time the highly sensitive Atacama Large Millimeter/submillimeter Array (ALMA); reaching an angular resolution of 25 μas, with characteristic sensitivity limits of ~1 mJy on baselines to ALMA and ~10 mJy on other baselines. The observations present challenges for existing data processing tools, arising from the rapid atmospheric phase fluctuations, wide recording bandwidth, and highly heterogeneous array. In response, we developed three independent pipelines for phase calibration and fringe detection, each tailored to the specific needs of the EHT. The final data products include calibrated total intensity amplitude and phase information. They are validated through a series of quality assurance tests that show consistency across pipelines and set limits on baseline systematic errors of 2% in amplitude and 1° in phase. The M87 data reveal the presence of two nulls in correlated flux density at ~3.4 and ~8.3 Gλ and temporal evolution in closure quantities, indicating intrinsic variability of compact structure on a timescale of days, or several light-crossing times for a few billion solar-mass black hole. These measurements provide the first opportunity to image horizon-scale structure in M87."
MARK KRAMER,First M87 Event Horizon Telescope results. V. Physical origin of the asymmetric ring,"The Event Horizon Telescope (EHT) has mapped the central compact radio source of the elliptical galaxy M87 at 1.3 mm with unprecedented angular resolution. Here we consider the physical implications of the asymmetric ring seen in the 2017 EHT data. To this end, we construct a large library of models based on general relativistic magnetohydrodynamic (GRMHD) simulations and synthetic images produced by general relativistic ray tracing. We compare the observed visibilities with this library and confirm that the asymmetric ring is consistent with earlier predictions of strong gravitational lensing of synchrotron emission from a hot plasma orbiting near the black hole event horizon. The ring radius and ring asymmetry depend on black hole mass and spin, respectively, and both are therefore expected to be stable when observed in future EHT campaigns. Overall, the observed image is consistent with expectations for the shadow of a spinning Kerr black hole as predicted by general relativity. If the black hole spin and M87's large scale jet are aligned, then the black hole spin vector is pointed away from Earth. Models in our library of non-spinning black holes are inconsistent with the observations as they do not produce sufficiently powerful jets. At the same time, in those models that produce a sufficiently powerful jet, the latter is powered by extraction of black hole spin energy through mechanisms akin to the Blandford-Znajek process. We briefly consider alternatives to a black hole for the central compact object. Analysis of existing EHT polarization data and data taken simultaneously at other wavelengths will soon enable new tests of the GRMHD models, as will future EHT campaigns at 230 and 345 GHz."
MARK KRAMER,First M87 Event Horizon Telescope results. VI. The shadow and mass of the central black hole,"We present measurements of the properties of the central radio source in M87 using Event Horizon Telescope data obtained during the 2017 campaign. We develop and fit geometric crescent models (asymmetric rings with interior brightness depressions) using two independent sampling algorithms that consider distinct representations of the visibility data. We show that the crescent family of models is statistically preferred over other comparably complex geometric models that we explore. We calibrate the geometric model parameters using general relativistic magnetohydrodynamic (GRMHD) models of the emission region and estimate physical properties of the source. We further fit images generated from GRMHD models directly to the data. We compare the derived emission region and black hole parameters from these analyses with those recovered from reconstructed images. There is a remarkable consistency among all methods and data sets. We find that >50% of the total flux at arcsecond scales comes from near the horizon, and that the emission is dramatically suppressed interior to this region by a factor >10, providing direct evidence of the predicted shadow of a black hole. Across all methods, we measure a crescent diameter of 42 ± 3 μas and constrain its fractional width to be <0.5. Associating the crescent feature with the emission surrounding the black hole shadow, we infer an angular gravitational radius of GM/Dc^2 = 3.8 ± 0.4 μas. Folding in a distance measurement of {16.8}_{-0.7}^{+0.8}{Mpc} gives a black hole mass of M = 6.5 ± 0.2{| }_{stat} ± 0.7{| }_{sys} × {10}^{9} {M}_{odot }. This measurement from lensed emission near the event horizon is consistent with the presence of a central Kerr black hole, as predicted by the general theory of relativity."
MARK KRAMER,Timing matters: impact of anticonvulsant drug treatment and spikes on seizure risk in benign epilepsy with centrotemporal spikes,"OBJECTIVE: Benign epilepsy with centrotemporal spikes (BECTS) is a common, self-limited epilepsy syndrome affecting school-age children. Classic interictal epileptiform discharges (IEDs) confirm diagnosis, and BECTS is presumed to be pharmacoresponsive. As seizure risk decreases in time with this disease, we hypothesize that the impact of IEDs and anticonvulsive drug (ACD) treatment on the risk of subsequent seizure will differ based on disease duration. METHODS: We calculate subsequent seizure risk following diagnosis in a large retrospective cohort of children with BECTS (n = 130), evaluating the impact of IEDs and ACD treatment in the first, second, third, and fourth years of disease. We use a Kaplan-Meier survival analysis and logistic regression models. Patients were censored if they were lost to follow-up or if they changed group status. RESULTS: Two-thirds of children had a subsequent seizure within 2 years of diagnosis. The majority of children had a subsequent seizure within 3 years despite treatment. The presence of IEDs on electroencephalography (EEG) did not impact subsequent seizure risk early in the disease. By the fourth year of disease, all children without IEDs remained seizure free, whereas one-third of children with IEDs at this stage had a subsequent seizure. Conversely, ACD treatment corresponded with lower risk of seizure early in the disease but did not impact seizure risk in later years. SIGNIFICANCE: In this cohort, the majority of children with BECTS had a subsequent seizure despite treatment. In addition, ACD treatment and IEDs predicted seizure risk at specific points of disease duration. Future prospective studies are needed to validate these exploratory findings."
MARK KRAMER,Rhythm generation through period concatenation in rat somatosensory cortex,"Rhythmic voltage oscillations resulting from the summed activity of neuronal populations occur in many nervous systems. Contemporary observations suggest that coexistent oscillations interact and, in time, may switch in dominance. We recently reported an example of these interactions recorded from in vitro preparations of rat somatosensory cortex. We found that following an initial interval of coexistent gamma (∼25 ms period) and beta2 (∼40 ms period) rhythms in the superficial and deep cortical layers, respectively, a transition to a synchronous beta1 (∼65 ms period) rhythm in all cortical layers occurred. We proposed that the switch to beta1 activity resulted from the novel mechanism of period concatenation of the faster rhythms: gamma period (25 ms)+beta2 period (40 ms) = beta1 period (65 ms). In this article, we investigate in greater detail the fundamental mechanisms of the beta1 rhythm. To do so we describe additional in vitro experiments that constrain a biologically realistic, yet simplified, computational model of the activity. We use the model to suggest that the dynamic building blocks (or motifs) of the gamma and beta2 rhythms combine to produce a beta1 oscillation that exhibits cross-frequency interactions. Through the combined approach of in vitro experiments and mathematical modeling we isolate the specific components that promote or destroy each rhythm. We propose that mechanisms vital to establishing the beta1 oscillation include strengthened connections between a population of deep layer intrinsically bursting cells and a transition from antidromic to orthodromic spike generation in these cells. We conclude that neural activity in the superficial and deep cortical layers may temporally combine to generate a slower oscillation."
MARK KRAMER,Period concatenation underlies interactions between gamma and beta rhythms in neocortex,"The neocortex generates rhythmic electrical activity over a frequency range covering many decades. Specific cognitive and motor states are associated with oscillations in discrete frequency bands within this range, but it is not known whether interactions and transitions between distinct frequencies are of functional importance. When coexpressed rhythms have frequencies that differ by a factor of two or more interactions can be seen in terms of phase synchronization. Larger frequency differences can result in interactions in the form of nesting of faster frequencies within slower ones by a process of amplitude modulation. It is not known how coexpressed rhythms, whose frequencies differ by less than a factor of two may interact. Here we show that two frequencies (gamma – 40 Hz and beta2 – 25 Hz), coexpressed in superficial and deep cortical laminae with low temporal interaction, can combine to generate a third frequency (beta1 – 15 Hz) showing strong temporal interaction. The process occurs via period concatenation, with basic rhythm-generating microcircuits underlying gamma and beta2 rhythms forming the building blocks of the beta1 rhythm by a process of addition. The mean ratio of adjacent frequency components was a constant – approximately the golden mean – which served to both minimize temporal interactions, and permit multiple transitions, between frequencies. The resulting temporal landscape may provide a framework for multiplexing – parallel information processing on multiple temporal scales."
MARK KRAMER,Are different rhythms good for different functions?,"This essay discusses the relationship between the physiology of rhythms and potential functional roles. We focus on how the biophysics underlying different rhythms can give rise to different abilities of a network to form and manipulate cell assemblies. We also discuss how changes in the modulatory setting of the rhythms can change the flow of information through cortical circuits, again tying physiology to computation. We suggest that diverse rhythms, or variations of a rhythm, can support different components of a cognitive act, with multiple rhythms potentially playing multiple roles."
MARK KRAMER,New dynamics in cerebellar purkinje cells: torus canards,"We describe a transition from bursting to rapid spiking in a reduced mathematical model of a cerebellar Purkinje cell. We perform a slow-fast analysis of the system and find that—after a saddle node bifurcation of limit cycles—the full model dynamics temporarily follow a repelling branch of limit cycles. We propose that the system exhibits a dynamical phenomenon new to realistic, biophysical applications: torus canards."
MARK KRAMER,The Event Horizon general relativistic magnetohydrodynamic code comparison project,"Recent developments in compact object astrophysics, especially the discovery of merging neutron stars by LIGO, the imaging of the black hole in M87 by the Event Horizon Telescope, and high- precision astrometry of the Galactic Center at close to the event horizon scale by the GRAVITY experiment motivate the development of numerical source models that solve the equations of general relativistic magnetohydrodynamics (GRMHD). Here we compare GRMHD solutions for the evolution of a magnetized accretion flow where turbulence is promoted by the magnetorotational instability from a set of nine GRMHD codes: Athena++, BHAC, Cosmos++, ECHO, H-AMR, iharm3D, HARM-Noble, IllinoisGRMHD, and KORAL. Agreement among the codes improves as resolution increases, as measured by a consistently applied, specially developed set of code performance metrics. We conclude that the community of GRMHD codes is mature, capable, and consistent on these test problems."
MARK KRAMER,Gravitational test beyond the first post-Newtonian order with the shadow of the M87 black hole,"The 2017 Event Horizon Telescope (EHT) observations of the central source in M87 have led to the first measurement of the size of a black-hole shadow. This observation offers a new and clean gravitational test of the black-hole metric in the strong-field regime. We show analytically that spacetimes that deviate from the Kerr metric but satisfy weak-field tests can lead to large deviations in the predicted black-hole shadows that are inconsistent with even the current EHT measurements. We use numerical calculations of regular, parametric, non-Kerr metrics to identify the common characteristic among these different parametrizations that control the predicted shadow size. We show that the shadow-size measurements place significant constraints on deviation parameters that control the second post-Newtonian and higher orders of each metric and are, therefore, inaccessible to weak-field tests. The new constraints are complementary to those imposed by observations of gravitational waves from stellar-mass sources."
MARK KRAMER,First M87 Event Horizon Telescope results. IV. Imaging the central supermassive black hole,
MARK KRAMER,Unique contributions of parvalbumin and cholinergic interneurons in organizing striatal networks during movement,"Striatal pavalbumin (PV) and cholinergic (CHI) interneurons are poised to play major roles in behavior by coordinating the networks of medium spiny cells that relay motor output. However, the small numbers and scattered distribution of these cells has made it difficult to directly assess their contribution to activity in networks of MSNs during behavior. Here, we build upon recent improvements in single cell calcium imaging combined with optogenetics to test the capacity of PVs and CHIs to affect MSN activity and behavior in mice engaged in voluntarily locomotion. We find that PVs and CHIs have unique effects on MSN activity and dissociable roles in supporting movement. PV cells facilitate movement by refining the activation of MSN networks responsible for movement execution. CHIs, in contrast, synchronize activity within MSN networks to signal the end of a movement bout. These results provide new insights into the striatal network activity that supports movement."
MARK KRAMER,Verification of radiative transfer schemes for the EHT,"The Event Horizon Telescope (EHT) Collaboration has recently produced the first resolved images of the central supermassive black hole in the giant elliptical galaxy M87. Here we report on tests of the consistency and accuracy of the general relativistic radiative transfer codes used within the collaboration to model M87* and Sgr A*. We compare and evaluate (1) deflection angles for equatorial null geodesics in a Kerr spacetime; (2) images calculated from a series of simple, parameterized matter distributions in the Kerr metric using simplified emissivities and absorptivities; (3) for a subset of codes, images calculated from general relativistic magnetohydrodynamics simulations using different realistic synchrotron emissivities and absorptivities; (4) observables for the 2017 configuration of EHT, including visibility amplitudes and closure phases. The error in total flux is of order 1% when the codes are run with production numerical parameters. The dominant source of discrepancies for small camera distances is the location and detailed setup of the software ""camera"" that each code uses to produce synthetic images. We find that when numerical parameters are suitably chosen and the camera is sufficiently far away the images converge and that for given transfer coefficients, numerical uncertainties are unlikely to limit parameter estimation for the current generation of EHT observations. The purpose of this paper is to describe a verification and comparison of EHT radiative transfer codes. It is not to verify EHT models more generally."
MARK KRAMER,Monitoring the mmorphology of M87* in 2009–2017 with the Event Horizon Telescope,"The Event Horizon Telescope (EHT) has recently delivered the first resolved images of M87*, the supermassive black hole in the center of the M87 galaxy. These images were produced using 230 GHz observations performed in April 2017. Additional observations are required to investigate the persistence of the primary image feature – a ring with azimuthal brightness asymmetry – and to quantify the image variability on event horizon scales. To address this need, we analyze M87* data collected with prototype EHT arrays in 2009, 2011, 2012, and 2013. While these observations do not contain enough information to produce images, they are sufficient to constrain simple geometric models. We develop a modeling approach based on the framework utilized for the 2017 EHT data analysis and validate our procedures using synthetic data. Applying the same approach to the observational data sets, we find the M87* morphology in 2009–2017 to be consistent with a persistent asymmetric ring of 40 as diameter. The position angle of peak intensity varies in time. In particular, we find a significant difference between the position angle measured in 2013 and 2017. These variations are in broad agreement with predictions of a subset of general relativistic magnetohydrodynamic simulations. We show that quantifying the variability across multiple observational epochs has the potential to constrain physical properties of the source, such as the accretion state or the black hole spin."
MARK KRAMER,THEMIS: a parameter estimation framework for the Event Horizon Telescope,"The Event Horizon Telescope (EHT) provides the unprecedented ability to directly resolve the structure and dynamics of black hole emission regions on scales smaller than their horizons. This has the potential to critically probe the mechanisms by which black holes accrete and launch outflows, and the structure of supermassive black hole spacetimes. However, accessing this information is a formidable analysis challenge for two reasons. First, the EHT natively produces a variety of data types that encode information about the image structure in nontrivial ways; these are subject to a variety of systematic effects associated with very long baseline interferometry and are supplemented by a wide variety of auxiliary data on the primary EHT targets from decades of other observations. Second, models of the emission regions and their interaction with the black hole are complex, highly uncertain, and computationally expensive to construct. As a result, the scientific utilization of EHT observations requires a flexible, extensible, and powerful analysis framework. We present such a framework, Themis, which defines a set of interfaces between models, data, and sampling algorithms that facilitates future development. We describe the design and currently existing components of Themis, how Themis has been validated thus far, and present additional analyses made possible by Themis that illustrate its capabilities. Importantly, we demonstrate that Themis is able to reproduce prior EHT analyses, extend these, and do so in a computationally efficient manner that can efficiently exploit modern high-performance computing facilities. Themis has already been used extensively in the scientific analysis and interpretation of the first EHT observations of M87."
MARK KRAMER,First Sagittarius A* Event Horizon Telescope results. V. Testing astrophysical models of the galactic center black hole,"In this paper we provide a first physical interpretation for the Event Horizon Telescope's (EHT) 2017 observations of Sgr A*. Our main approach is to compare resolved EHT data at 230 GHz and unresolved non-EHT observations from radio to X-ray wavelengths to predictions from a library of models based on time-dependent general relativistic magnetohydrodynamics simulations, including aligned, tilted, and stellar-wind-fed simulations; radiative transfer is performed assuming both thermal and nonthermal electron distribution functions. We test the models against 11 constraints drawn from EHT 230 GHz data and observations at 86 GHz, 2.2 μm, and in the X-ray. All models fail at least one constraint. Light-curve variability provides a particularly severe constraint, failing nearly all strongly magnetized (magnetically arrested disk (MAD)) models and a large fraction of weakly magnetized models. A number of models fail only the variability constraints. We identify a promising cluster of these models, which are MAD and have inclination i ≤ 30°. They have accretion rate (5.2–9.5) × 10−9 M ⊙ yr−1, bolometric luminosity (6.8–9.2) × 1035 erg s−1, and outflow power (1.3–4.8) × 1038 erg s−1. We also find that all models with i ≥ 70° fail at least two constraints, as do all models with equal ion and electron temperature; exploratory, nonthermal model sets tend to have higher 2.2 μm flux density; and the population of cold electrons is limited by X-ray constraints due to the risk of bremsstrahlung overproduction. Finally, we discuss physical and numerical limitations of the models, highlighting the possible importance of kinetic effects and duration of the simulations."
MARK KRAMER,First M87 Event Horizon Telescope results. VII. Polarization of the ring,"In 2017 April, the Event Horizon Telescope (EHT) observed the near-horizon region around the supermassive black hole at the core of the M87 galaxy. These 1.3 mm wavelength observations revealed a compact asymmetric ring-like source morphology. This structure originates from synchrotron emission produced by relativistic plasma located in the immediate vicinity of the black hole. Here we present the corresponding linear-polarimetric EHT images of the center of M87. We find that only a part of the ring is significantly polarized. The resolved fractional linear polarization has a maximum located in the southwest part of the ring, where it rises to the level of ∼15%. The polarization position angles are arranged in a nearly azimuthal pattern. We perform quantitative measurements of relevant polarimetric properties of the compact emission and find evidence for the temporal evolution of the polarized source structure over one week of EHT observations. The details of the polarimetric data reduction and calibration methodology are provided. We carry out the data analysis using multiple independent imaging and modeling techniques, each of which is validated against a suite of synthetic data sets. The gross polarimetric structure and its apparent evolution with time are insensitive to the method used to reconstruct the image. These polarimetric images carry information about the structure of the magnetic fields responsible for the synchrotron emission. Their physical interpretation is discussed in an accompanying publication."
MARK KRAMER,First M87 Event Horizon Telescope results. VIII. Magnetic field structure near The Event Horizon,"Event Horizon Telescope (EHT) observations at 230 GHz have now imaged polarized emission around the supermassive black hole in M87 on event-horizon scales. This polarized synchrotron radiation probes the structure of magnetic fields and the plasma properties near the black hole. Here we compare the resolved polarization structure observed by the EHT, along with simultaneous unresolved observations with the Atacama Large Millimeter/submillimeter Array, to expectations from theoretical models. The low fractional linear polarization in the resolved image suggests that the polarization is scrambled on scales smaller than the EHT beam, which we attribute to Faraday rotation internal to the emission region. We estimate the average density n_e ∼ 10^4–7 cm^−3, magnetic field strength B ∼ 1–30 G, and electron temperature T_e ∼ (1–12) × 10^10 K of the radiating plasma in a simple one-zone emission model. We show that the net azimuthal linear polarization pattern may result from organized, poloidal magnetic fields in the emission region. In a quantitative comparison with a large library of simulated polarimetric images from general relativistic magnetohydrodynamic (GRMHD) simulations, we identify a subset of physical models that can explain critical features of the polarimetric EHT observations while producing a relativistic jet of sufficient power. The consistent GRMHD models are all of magnetically arrested accretion disks, where near-horizon magnetic fields are dynamically important. We use the models to infer a mass accretion rate onto the black hole in M87 of (3–20) × 10^−4 M ⊙ yr^−1."
MARK KRAMER,Resolving the inner parsec of the blazar J1924–2914 with the event horizon telescope,"The blazar J1924–2914 is a primary Event Horizon Telescope (EHT) calibrator for the Galactic center’s black hole Sagittarius A*. Here we present the first total and linearly polarized intensity images of this source obtained with the unprecedented 20 μas resolution of the EHT. J1924–2914 is a very compact flat-spectrum radio source with strong optical variability and polarization. In April 2017 the source was observed quasi-simultaneously with the EHT (April 5–11), the Global Millimeter VLBI Array (April 3), and the Very Long Baseline Array (April 28), giving a novel view of the source at four observing frequencies, 230, 86, 8.7, and 2.3 GHz. These observations probe jet properties from the subparsec to 100 pc scales. We combine the multifrequency images of J1924–2914 to study the source morphology. We find that the jet exhibits a characteristic bending, with a gradual clockwise rotation of the jet projected position angle of about 90° between 2.3 and 230 GHz. Linearly polarized intensity images of J1924–2914 with the extremely fine resolution of the EHT provide evidence for ordered toroidal magnetic fields in the blazar compact core."
MARK KRAMER,Successful reconstruction of a physiological circuit with known connectivity from spiking activity alone,"Identifying the structure and dynamics of synaptic interactions between neurons is the first step to understanding neural network dynamics. The presence of synaptic connections is traditionally inferred through the use of targeted stimulation and paired recordings or by post-hoc histology. More recently, causal network inference algorithms have been proposed to deduce connectivity directly from electrophysiological signals, such as extracellularly recorded spiking activity. Usually, these algorithms have not been validated on a neurophysiological data set for which the actual circuitry is known. Recent work has shown that traditional network inference algorithms based on linear models typically fail to identify the correct coupling of a small central pattern generating circuit in the stomatogastric ganglion of the crab Cancer borealis. In this work, we show that point process models of observed spike trains can guide inference of relative connectivity estimates that match the known physiological connectivity of the central pattern generator up to a choice of threshold. We elucidate the necessary steps to derive faithful connectivity estimates from a model that incorporates the spike train nature of the data. We then apply the model to measure changes in the effective connectivity pattern in response to two pharmacological interventions, which affect both intrinsic neural dynamics and synaptic transmission. Our results provide the first successful application of a network inference algorithm to a circuit for which the actual physiological synapses between neurons are known. The point process methodology presented here generalizes well to larger networks and can describe the statistics of neural populations. In general we show that advanced statistical models allow for the characterization of effective network structure, deciphering underlying network dynamics and estimating information-processing capabilities."
MARK KRAMER,Kinship ties across the lifespan in human communities,"A hypothesis for the evolution of long post-reproductive lifespans in the human lineage involves asymmetries in relatedness between young immigrant females and the older females in their new groups. In these circumstances, inter-generational reproductive conflicts between younger and older females are predicted to resolve in favour of the younger females, who realize fewer inclusive fitness benefits from ceding reproduction to others. This conceptual model anticipates that immigrants to a community initially have few kin ties to others in the group, gradually showing greater relatedness to group members as they have descendants who remain with them in the group. We examine this prediction in a cross-cultural sample of communities, which vary in their sex-biased dispersal patterns and other aspects of social organization. Drawing on genealogical and demographic data, the analysis provides general but not comprehensive support for the prediction that average relatedness of immigrants to other group members increases as they age. In rare cases, natal members of the community also exhibit age-related increases in relatedness. We also find large variation in the proportion of female group members who are immigrants, beyond simple traditional considerations of patrilocality or matrilocality, which raises questions about the circumstances under which this hypothesis of female competition are met. We consider possible explanations for these heterogenous results, and we address methodological considerations that merit increased attention for research on kinship and reproductive conflict in human societies.This article is part of the theme issue ‘The evolution of female-biased kinship in humans and other mammals’."
MARK KRAMER,Period Concatenation Underlies Interactions Between Gamma and Beta Rhythms in Neocortex,"The neocortex generates rhythmic electrical activity over a frequency range covering many decades. Specific cognitive and motor states are associated with oscillations in discrete frequency bands within this range, but it is not known whether interactions and transitions between distinct frequencies are of functional importance. When coexpressed rhythms have frequencies that differ by a factor of two or more interactions can be seen in terms of phase synchronization. Larger frequency differences can result in interactions in the form of nesting of faster frequencies within slower ones by a process of amplitude modulation. It is not known how coexpressed rhythms, whose frequencies differ by less than a factor of two may interact. Here we show that two frequencies (gamma - 40Hz and beta2 - 25Hz), coexpressed in superficial and deep cortical laminae with low temporal interaction, can combine to generate a third frequency (beta1 - 15Hz) showing strong temporal interaction. The process occurs via period concatenation, with basic rhythm-generating microcircuits underlying gamma and beta2 rhythms forming the building blocks of the beta1 rhythm by a process of addition. The mean ratio of adjacent frequency components was a constant - approximately the golden mean - which served to both minimize temporal interactions, and permit multiple transitions, between frequencies. The resulting temporal landscape may provide a framework for multiplexing - parallel information processing on multiple temporal scales."
MARK KRAMER,Are Different Rhythms Good for Different Functions?,"This essay discusses the relationship between the physiology of rhythms and potential functional roles. We focus on how the biophysics underlying different rhythms can give rise to different abilities of a network to form and manipulate cell assemblies. We also discuss how changes in the modulatory setting of the rhythms can change the flow of information through cortical circuits, again tying physiology to computation. We suggest that diverse rhythms, or variations of a rhythm, can support different components of a cognitive act, with multiple rhythms potentially playing multiple roles."
MARK KRAMER,Temporal Interactions Between Cortical Rhythms,"Multiple local neuronal circuits support different, discrete frequencies of network rhythm in neocortex. Relationships between different frequencies correspond to mechanisms designed to minimise interference, couple activity via stable phase interactions, and control the amplitude of one frequency relative to the phase of another. These mechanisms are proposed to form a framework for spectral information processing. Individual local circuits can also transform their frequency through changes in intrinsic neuronal properties and interactions with other oscillating microcircuits. Here we discuss a frequency transformation in which activity in two co-active local circuits may combine sequentially to generate a third frequency whose period is the concatenation sum of the original two. With such an interaction, the intrinsic periodicity in each component local circuit is preserved - alternate, single periods of each original rhythm form one period of a new frequency - suggesting a robust mechanism for combining information processed on multiple concurrent spatiotemporal scales."
MARK KRAMER,Dysmature superficial white matter microstructure in developmental focal epilepsy,"Benign epilepsy with centrotemporal spikes is a common childhood epilepsy syndrome that predominantly affects boys, characterized by self-limited focal seizures arising from the perirolandic cortex and fine motor abnormalities. Concurrent with the age-specific presentation of this syndrome, the brain undergoes a developmentally choreographed sequence of white matter microstructural changes, including maturation of association u-fibres abutting the cortex. These short fibres mediate local cortico-cortical communication and provide an age-sensitive structural substrate that could support a focal disease process. To test this hypothesis, we evaluated the microstructural properties of superficial white matter in regions corresponding to u-fibres underlying the perirolandic seizure onset zone in children with this epilepsy syndrome compared with healthy controls. To verify the spatial specificity of these features, we characterized global superficial and deep white matter properties. We further evaluated the characteristics of the perirolandic white matter in relation to performance on a fine motor task, gender and abnormalities observed on EEG. Children with benign epilepsy with centrotemporal spikes (n = 20) and healthy controls (n = 14) underwent multimodal testing with high-resolution MRI including diffusion tensor imaging sequences, sleep EEG recordings and fine motor assessment. We compared white matter microstructural characteristics (axial, radial and mean diffusivity, and fractional anisotropy) between groups in each region. We found distinct abnormalities corresponding to the perirolandic u-fibre region, with increased axial, radial and mean diffusivity and fractional anisotropy values in children with epilepsy (P = 0.039, P = 0.035, P = 0.042 and P = 0.017, respectively). Increased fractional anisotropy in this region, consistent with decreased integrity of crossing sensorimotor u-fibres, correlated with inferior fine motor performance (P = 0.029). There were gender-specific differences in white matter microstructure in the perirolandic region; males and females with epilepsy and healthy males had higher diffusion and fractional anisotropy values than healthy females (P ≤ 0.035 for all measures), suggesting that typical patterns of white matter development disproportionately predispose boys to this developmental epilepsy syndrome. Perirolandic white matter microstructure showed no relationship to epilepsy duration, duration seizure free, or epileptiform burden. There were no group differences in diffusivity or fractional anisotropy in superficial white matter outside of the perirolandic region. Children with epilepsy had increased radial diffusivity (P = 0.022) and decreased fractional anisotropy (P = 0.027) in deep white matter, consistent with a global delay in white matter maturation. These data provide evidence that atypical maturation of white matter microstructure is a basic feature in benign epilepsy with centrotemporal spikes and may contribute to the epilepsy, male predisposition and clinical comorbidities observed in this disorder."
MARK KRAMER,A universal power-law prescription for variability from synthetic images of black hole accretion flows,"We present a framework for characterizing the spatiotemporal power spectrum of the variability expected from the horizon-scale emission structure around supermassive black holes, and we apply this framework to a library of general relativistic magnetohydrodynamic (GRMHD) simulations and associated general relativistic ray-traced images relevant for Event Horizon Telescope (EHT) observations of Sgr A*. We find that the variability power spectrum is generically a red-noise process in both the temporal and spatial dimensions, with the peak in power occurring on the longest timescales and largest spatial scales. When both the time-averaged source structure and the spatially integrated light-curve variability are removed, the residual power spectrum exhibits a universal broken power-law behavior. On small spatial frequencies, the residual power spectrum rises as the square of the spatial frequency and is proportional to the variance in the centroid of emission. Beyond some peak in variability power, the residual power spectrum falls as that of the time-averaged source structure, which is similar across simulations; this behavior can be naturally explained if the variability arises from a multiplicative random field that has a steeper high-frequency power-law index than that of the time-averaged source structure. We briefly explore the ability of power spectral variability studies to constrain physical parameters relevant for the GRMHD simulations, which can be scaled to provide predictions for black holes in a range of systems in the optically thin regime. We present specific expectations for the behavior of the M87* and Sgr A* accretion flows as observed by the EHT."
MARK KRAMER,Millimeter light curves of Sagittarius A* observed during the 2017 Event Horizon Telescope campaign,"The Event Horizon Telescope (EHT) observed the compact radio source, Sagittarius A* (Sgr A*), in the Galactic Center on 2017 April 5–11 in the 1.3 mm wavelength band. At the same time, interferometric array data from the Atacama Large Millimeter/submillimeter Array and the Submillimeter Array were collected, providing Sgr A* light curves simultaneous with the EHT observations. These data sets, complementing the EHT very long baseline interferometry, are characterized by a cadence and signal-to-noise ratio previously unattainable for Sgr A* at millimeter wavelengths, and they allow for the investigation of source variability on timescales as short as a minute. While most of the light curves correspond to a low variability state of Sgr A*, the April 11 observations follow an X-ray flare and exhibit strongly enhanced variability. All of the light curves are consistent with a red-noise process, with a power spectral density (PSD) slope measured to be between −2 and −3 on timescales between 1 minute and several hours. Our results indicate a steepening of the PSD slope for timescales shorter than 0.3 hr. The spectral energy distribution is flat at 220 GHz, and there are no time lags between the 213 and 229 GHz frequency bands, suggesting low optical depth for the event horizon scale source. We characterize Sgr A*’s variability, highlighting the different behavior observed just after the X-ray flare, and use Gaussian process modeling to extract a decorrelation timescale and a PSD slope. We also investigate the systematic calibration uncertainties by analyzing data from independent data reduction pipelines."
MARK KRAMER,Selective dynamical imaging of interferometric data,"Recent developments in very long baseline interferometry (VLBI) have made it possible for the Event Horizon Telescope (EHT) to resolve the innermost accretion flows of the largest supermassive black holes on the sky. The sparse nature of the EHT’s (u, v)-coverage presents a challenge when attempting to resolve highly time-variable sources. We demonstrate that the changing (u, v)-coverage of the EHT can contain regions of time over the course of a single observation that facilitate dynamical imaging. These optimal time regions typically have projected baseline distributions that are approximately angularly isotropic and radially homogeneous. We derive a metric of coverage quality based on baseline isotropy and density that is capable of ranking array configurations by their ability to produce accurate dynamical reconstructions. We compare this metric to existing metrics in the literature and investigate their utility by performing dynamical reconstructions on synthetic data from simulated EHT observations of sources with simple orbital variability. We then use these results to make recommendations for imaging the 2017 EHT Sgr A* data set."
MARK KRAMER,Temporal interactions between cortical rhythms,"Multiple local neuronal circuits support different, discrete frequencies of network rhythm in neocortex. Relationships between different frequencies correspond to mechanisms designed to minimise interference, couple activity via stable phase interactions, and control the amplitude of one frequency relative to the phase of another. These mechanisms are proposed to form a framework for spectral information processing. Individual local circuits can also transform their frequency through changes in intrinsic neuronal properties and interactions with other oscillating microcircuits. Here we discuss a frequency transformation in which activity in two co-active local circuits may combine sequentially to generate a third frequency whose period is the concatenation sum of the original two. With such an interaction, the intrinsic periodicity in each component local circuit is preserved ÃƒÂ¢Ã‚Â€Ã‚Â“ alternate, single periods of each original rhythm form one period of a new frequency ÃƒÂ¢Ã‚Â€Ã‚Â“ suggesting a robust mechanism for combining information processed on multiple concurrent spatiotemporal scales."
MARK KRAMER,"Caribbean Corals in Crisis: Record Thermal Stress, Bleaching, and Mortality in 2005","BACKGROUND. The rising temperature of the world's oceans has become a major threat to coral reefs globally as the severity and frequency of mass coral bleaching and mortality events increase. In 2005, high ocean temperatures in the tropical Atlantic and Caribbean resulted in the most severe bleaching event ever recorded in the basin. METHODOLOGY/PRINCIPAL FINDINGS. Satellite-based tools provided warnings for coral reef managers and scientists, guiding both the timing and location of researchers' field observations as anomalously warm conditions developed and spread across the greater Caribbean region from June to October 2005. Field surveys of bleaching and mortality exceeded prior efforts in detail and extent, and provided a new standard for documenting the effects of bleaching and for testing nowcast and forecast products. Collaborators from 22 countries undertook the most comprehensive documentation of basin-scale bleaching to date and found that over 80% of corals bleached and over 40% died at many sites. The most severe bleaching coincided with waters nearest a western Atlantic warm pool that was centered off the northern end of the Lesser Antilles. CONCLUSIONS/SIGNIFICANCE. Thermal stress during the 2005 event exceeded any observed from the Caribbean in the prior 20 years, and regionally-averaged temperatures were the warmest in over 150 years. Comparison of satellite data against field surveys demonstrated a significant predictive relationship between accumulated heat stress (measured using NOAA Coral Reef Watch's Degree Heating Weeks) and bleaching intensity. This severe, widespread bleaching and mortality will undoubtedly have long-term consequences for reef ecosystems and suggests a troubled future for tropical marine ecosystems under a warming climate."
MARK KRAMER,Rhythm Generation through Period Concatenation in Rat Somatosensory Cortex,"Rhythmic voltage oscillations resulting from the summed activity of neuronal populations occur in many nervous systems. Contemporary observations suggest that coexistent oscillations interact and, in time, may switch in dominance. We recently reported an example of these interactions recorded from in vitro preparations of rat somatosensory cortex. We found that following an initial interval of coexistent gamma (∼25 ms period) and beta2 (∼40 ms period) rhythms in the superficial and deep cortical layers, respectively, a transition to a synchronous beta1 (∼65 ms period) rhythm in all cortical layers occurred. We proposed that the switch to beta1 activity resulted from the novel mechanism of period concatenation of the faster rhythms: gamma period (25 ms)+beta2 period (40 ms) = beta1 period (65 ms). In this article, we investigate in greater detail the fundamental mechanisms of the beta1 rhythm. To do so we describe additional in vitro experiments that constrain a biologically realistic, yet simplified, computational model of the activity. We use the model to suggest that the dynamic building blocks (or motifs) of the gamma and beta2 rhythms combine to produce a beta1 oscillation that exhibits cross-frequency interactions. Through the combined approach of in vitro experiments and mathematical modeling we isolate the specific components that promote or destroy each rhythm. We propose that mechanisms vital to establishing the beta1 oscillation include strengthened connections between a population of deep layer intrinsically bursting cells and a transition from antidromic to orthodromic spike generation in these cells. We conclude that neural activity in the superficial and deep cortical layers may temporally combine to generate a slower oscillation. Author SummarySince the late 19th century, rhythmic electrical activity has been observed in the mammalian brain. Although subject to intense scrutiny, only a handful of these rhythms are understood in terms of the biophysical elements that produce the oscillations. Even less understood are the mechanisms that underlie interactions between rhythms; how do rhythms of different frequencies coexist and affect one another in the dynamic environment of the brain? In this article, we consider a recent proposal for a novel mechanism of cortical rhythm generation: period concatenation, in which the periods of faster rhythms sum to produce a slower oscillation. To model this phenomenon, we implement simple—yet biophysical—computational models of the individual neurons that produce the brain's voltage activity. We utilize established models for the faster rhythms, and unite these in a particular way to generate a slower oscillation. Through the combined approach of experimental recordings (from thin sections of rat cortex) and mathematical modeling, we identify the cell types, synaptic connections, and ionic currents involved in rhythm generation through period concatenation. In this way the brain may generate new activity through the combination of preexisting elements."
MARK KRAMER,First Sagittarius A* Event Horizon Telescope results. VI. Testing the black hole metric,"Astrophysical black holes are expected to be described by the Kerr metric. This is the only stationary, vacuum, axisymmetric metric, without electromagnetic charge, that satisfies Einstein’s equations and does not have pathologies outside of the event horizon. We present new constraints on potential deviations from the Kerr prediction based on 2017 EHT observations of Sagittarius A* (Sgr A*). We calibrate the relationship between the geometrically defined black hole shadow and the observed size of the ring-like images using a library that includes both Kerr and non-Kerr simulations. We use the exquisite prior constraints on the mass-to-distance ratio for Sgr A* to show that the observed image size is within ∼10% of the Kerr predictions. We use these bounds to constrain metrics that are parametrically different from Kerr, as well as the charges of several known spacetimes. To consider alternatives to the presence of an event horizon, we explore the possibility that Sgr A* is a compact object with a surface that either absorbs and thermally reemits incident radiation or partially reflects it. Using the observed image size and the broadband spectrum of Sgr A*, we conclude that a thermal surface can be ruled out and a fully reflective one is unlikely. We compare our results to the broader landscape of gravitational tests. Together with the bounds found for stellar-mass black holes and the M87 black hole, our observations provide further support that the external spacetimes of all black holes are described by the Kerr metric, independent of their mass."
MARK KRAMER,Polarimetric properties of Event Horizon Telescope targets from ALMA,"We present the results from a full polarization study carried out with the Atacama Large Millimeter/submillimeter Array (ALMA) during the first Very Long Baseline Interferometry (VLBI) campaign, which was conducted in 2017 April in the λ3 mm and λ1.3 mm bands, in concert with the Global mm-VLBI Array (GMVA) and the Event Horizon Telescope (EHT), respectively. We determine the polarization and Faraday properties of all VLBI targets, including Sgr A*, M87, and a dozen radio-loud active galactic nuclei (AGNs), in the two bands at several epochs in a time window of 10 days. We detect high linear polarization fractions (2%–15%) and large rotation measures (RM &gt; 103.3–105.5 rad m−2), confirming the trends of previous AGN studies at millimeter wavelengths. We find that blazars are more strongly polarized than other AGNs in the sample, while exhibiting (on average) order-of-magnitude lower RM values, consistent with the AGN viewing angle unification scheme. For Sgr A* we report a mean RM of (−4.2 ± 0.3) × 105 rad m−2 at 1.3 mm, consistent with measurements over the past decade and, for the first time, an RM of (–2.1 ± 0.1) × 105 rad m−2 at 3 mm, suggesting that about half of the Faraday rotation at 1.3 mm may occur between the 3 mm photosphere and the 1.3 mm source. We also report the first unambiguous measurement of RM toward the M87 nucleus at millimeter wavelengths, which undergoes significant changes in magnitude and sign reversals on a one year timescale, spanning the range from −1.2 to 0.3 × 105 rad m−2 at 3 mm and −4.1 to 1.5 × 105 rad m−2 at 1.3 mm. Given this time variability, we argue that, unlike the case of Sgr A*, the RM in M87 does not provide an accurate estimate of the mass accretion rate onto the black hole. We put forward a two-component model, comprised of a variable compact region and a static extended region, that can simultaneously explain the polarimetric properties observed by both the EHT (on horizon scales) and ALMA (which observes the combined emission from both components). These measurements provide critical constraints for the calibration, analysis, and interpretation of simultaneously obtained VLBI data with the EHT and GMVA."
MARK KRAMER,"First Sagittarius A* Event Horizon Telescope results. IV. Variability, morphology, and black hole mass","In this paper we quantify the temporal variability and image morphology of the horizon-scale emission from Sgr A*, as observed by the EHT in 2017 April at a wavelength of 1.3 mm. We find that the Sgr A* data exhibit variability that exceeds what can be explained by the uncertainties in the data or by the effects of interstellar scattering. The magnitude of this variability can be a substantial fraction of the correlated flux density, reaching ∼100% on some baselines. Through an exploration of simple geometric source models, we demonstrate that ring-like morphologies provide better fits to the Sgr A* data than do other morphologies with comparable complexity. We develop two strategies for fitting static geometric ring models to the time-variable Sgr A* data; one strategy fits models to short segments of data over which the source is static and averages these independent fits, while the other fits models to the full data set using a parametric model for the structural variability power spectrum around the average source structure. Both geometric modeling and image-domain feature extraction techniques determine the ring diameter to be 51.8 ± 2.3 μas (68% credible intervals), with the ring thickness constrained to have an FWHM between ∼30% and 50% of the ring diameter. To bring the diameter measurements to a common physical scale, we calibrate them using synthetic data generated from GRMHD simulations. This calibration constrains the angular size of the gravitational radius to be 4.8_-0.7^+1.4 μas, which we combine with an independent distance measurement from maser parallaxes to determine the mass of Sgr A* to be 4.0_-0.6^+10^6 M⊙."
MARK KRAMER,"First Sagittarius A* Event Horizon Telescope results. II. EHT and multiwavelength observations, data processing, and calibration","We present Event Horizon Telescope (EHT) 1.3 mm measurements of the radio source located at the position of the supermassive black hole Sagittarius A* (Sgr A*), collected during the 2017 April 5–11 campaign. The observations were carried out with eight facilities at six locations across the globe. Novel calibration methods are employed to account for Sgr A*'s flux variability. The majority of the 1.3 mm emission arises from horizon scales, where intrinsic structural source variability is detected on timescales of minutes to hours. The effects of interstellar scattering on the image and its variability are found to be subdominant to intrinsic source structure. The calibrated visibility amplitudes, particularly the locations of the visibility minima, are broadly consistent with a blurred ring with a diameter of ∼50 μas, as determined in later works in this series. Contemporaneous multiwavelength monitoring of Sgr A* was performed at 22, 43, and 86 GHz and at near-infrared and X-ray wavelengths. Several X-ray flares from Sgr A* are detected by Chandra, one at low significance jointly with Swift on 2017 April 7 and the other at higher significance jointly with NuSTAR on 2017 April 11. The brighter April 11 flare is not observed simultaneously by the EHT but is followed by a significant increase in millimeter flux variability immediately after the X-ray outburst, indicating a likely connection in the emission physics near the event horizon. We compare Sgr A*’s broadband flux during the EHT campaign to its historical spectral energy distribution and find that both the quiescent emission and flare emission are consistent with its long-term behavior."
MARK KRAMER,Human seizures couple across spatial scales through travelling wave dynamics,"Epilepsy-the propensity toward recurrent, unprovoked seizures-is a devastating disease affecting 65 million people worldwide. Understanding and treating this disease remains a challenge, as seizures manifest through mechanisms and features that span spatial and temporal scales. Here we address this challenge through the analysis and modelling of human brain voltage activity recorded simultaneously across microscopic and macroscopic spatial scales. We show that during seizure large-scale neural populations spanning centimetres of cortex coordinate with small neural groups spanning cortical columns, and provide evidence that rapidly propagating waves of activity underlie this increased inter-scale coupling. We develop a corresponding computational model to propose specific mechanisms-namely, the effects of an increased extracellular potassium concentration diffusing in space-that support the observed spatiotemporal dynamics. Understanding the multi-scale, spatiotemporal dynamics of human seizures-and connecting these dynamics to specific biological mechanisms-promises new insights to treat this devastating disease."
MARK KRAMER,The polarized image of a synchrotron-emitting ring of gas orbiting a black hole,"Synchrotron radiation from hot gas near a black hole results in a polarized image. The image polarization is determined by effects including the orientation of the magnetic field in the emitting region, relativistic motion of the gas, strong gravitational lensing by the black hole, and parallel transport in the curved spacetime. We explore these effects using a simple model of an axisymmetric, equatorial accretion disk around a Schwarzschild black hole. By using an approximate expression for the null geodesics derived by Beloborodov and conservation of the Walker–Penrose constant, we provide analytic estimates for the image polarization. We test this model using currently favored general relativistic magnetohydrodynamic simulations of M87*, using ring parameters given by the simulations. For a subset of these with modest Faraday effects, we show that the ring model broadly reproduces the polarimetric image morphology. Our model also predicts the polarization evolution for compact flaring regions, such as those observed from Sgr A* with GRAVITY. With suitably chosen parameters, our simple model can reproduce the EVPA pattern and relative polarized intensity in Event Horizon Telescope images of M87*. Under the physically motivated assumption that the magnetic field trails the fluid velocity, this comparison is consistent with the clockwise rotation inferred from total intensity images."
MARK KRAMER,Reproductive inequality in humans and other mammals,"To address claims of human exceptionalism, we determine where humans fit within the greater mammalian distribution of reproductive inequality. We show that humans exhibit lower reproductive skew (i.e., inequality in the number of surviving offspring) among males and smaller sex differences in reproductive skew than most other mammals, while nevertheless falling within the mammalian range. Additionally, female reproductive skew is higher in polygynous human populations than in polygynous nonhumans mammals on average. This patterning of skew can be attributed in part to the prevalence of monogamy in humans compared to the predominance of polygyny in nonhuman mammals, to the limited degree of polygyny in the human societies that practice it, and to the importance of unequally held rival resources to women's fitness. The muted reproductive inequality observed in humans appears to be linked to several unusual characteristics of our species-including high levels of cooperation among males, high dependence on unequally held rival resources, complementarities between maternal and paternal investment, as well as social and legal institutions that enforce monogamous norms."
MARK KRAMER,First Sagittarius A* Event Horizon Telescope results. I. The shadow of the supermassive black hole in the center of the Milky Way,"We present the first Event Horizon Telescope (EHT) observations of Sagittarius A* (Sgr A*), the Galactic center source associated with a supermassive black hole. These observations were conducted in 2017 using a global interferometric array of eight telescopes operating at a wavelength of λ = 1.3 mm. The EHT data resolve a compact emission region with intrahour variability. A variety of imaging and modeling analyses all support an image that is dominated by a bright, thick ring with a diameter of 51.8 ± 2.3 μas (68% credible interval). The ring has modest azimuthal brightness asymmetry and a comparatively dim interior. Using a large suite of numerical simulations, we demonstrate that the EHT images of Sgr A* are consistent with the expected appearance of a Kerr black hole with mass ∼4 × 106 M ⊙, which is inferred to exist at this location based on previous infrared observations of individual stellar orbits, as well as maser proper-motion studies. Our model comparisons disfavor scenarios where the black hole is viewed at high inclination (i &gt; 50°), as well as nonspinning black holes and those with retrograde accretion disks. Our results provide direct evidence for the presence of a supermassive black hole at the center of the Milky Way, and for the first time we connect the predictions from dynamical measurements of stellar orbits on scales of 103–105 gravitational radii to event-horizon-scale images and variability. Furthermore, a comparison with the EHT results for the supermassive black hole M87* shows consistency with the predictions of general relativity spanning over three orders of magnitude in central mass."
MARK KRAMER,Broadband multi-wavelength properties of M87 during the 2017 Event Horizon Telescope campaign,"In 2017, the Event Horizon Telescope (EHT) Collaboration succeeded in capturing the first direct image of the center of the M87 galaxy. The asymmetric ring morphology and size are consistent with theoretical expectations for a weakly accreting supermassive black hole of mass ∼6.5 × 109 M ⊙. The EHTC also partnered with several international facilities in space and on the ground, to arrange an extensive, quasi-simultaneous multi-wavelength campaign. This Letter presents the results and analysis of this campaign, as well as the multi-wavelength data as a legacy data repository. We captured M87 in a historically low state, and the core flux dominates over HST-1 at high energies, making it possible to combine core flux constraints with the more spatially precise very long baseline interferometry data. We present the most complete simultaneous multi-wavelength spectrum of the active nucleus to date, and discuss the complexity and caveats of combining data from different spatial scales into one broadband spectrum. We apply two heuristic, isotropic leptonic single-zone models to provide insight into the basic source properties, but conclude that a structured jet is necessary to explain M87’s spectrum. We can exclude that the simultaneous γ-ray emission is produced via inverse Compton emission in the same region producing the EHT mm-band emission, and further conclude that the γ-rays can only be produced in the inner jets (inward of HST-1) if there are strongly particle-dominated regions. Direct synchrotron emission from accelerated protons and secondaries cannot yet be excluded."
MARK KRAMER,"Assessing dynamics, spatial scale, and uncertainty in task-related brain network analyses","The brain is a complex network of interconnected elements, whose interactions evolve dynamically in time to cooperatively perform specific functions. A common technique to probe these interactions involves multi-sensor recordings of brain activity during a repeated task. Many techniques exist to characterize the resulting task-related activity, including establishing functional networks, which represent the statistical associations between brain areas. Although functional network inference is commonly employed to analyze neural time series data, techniques to assess the uncertainty—both in the functional network edges and the corresponding aggregate measures of network topology—are lacking. To address this, we describe a statistically principled approach for computing uncertainty in functional networks and aggregate network measures in task-related data. The approach is based on a resampling procedure that utilizes the trial structure common in experimental recordings. We show in simulations that this approach successfully identifies functional networks and associated measures of confidence emergent during a task in a variety of scenarios, including dynamically evolving networks. In addition, we describe a principled technique for establishing functional networks based on predetermined regions of interest using canonical correlation. Doing so provides additional robustness to the functional network inference. Finally, we illustrate the use of these methods on example invasive brain voltage recordings collected during an overt speech task. The general strategy described here—appropriate for static and dynamic network inference and different statistical measures of coupling—permits the evaluation of confidence in network measures in a variety of settings common to neuroscience."
MARK KRAMER,Event Horizon Telescope imaging of the archetypal blazar 3C 279 at an extreme 20 microarcsecond resolution,"3C 279 is an archetypal blazar with a prominent radio jet that show broadband flux density variability across the entire electromagnetic spectrum. We use an ultra-high angular resolution technique – global Very Long Baseline Interferometry (VLBI) at 1.3 mm (230 GHz) – to resolve the innermost jet of 3C 279 in order to study its fine-scale morphology close to the jet base where highly variable γ-ray emission is thought to originate, according to various models. The source was observed during four days in April 2017 with the Event Horizon Telescope at 230 GHz, including the phased Atacama Large Millimeter/submillimeter Array (ALMA), at an angular resolution of ∼20 μas (at a redshift of z = 0.536 this corresponds to ∼0.13 pc  ∼ 1700 Schwarzschild radii with a black hole mass M<jats:sub>BH</jats:sub> = 8 × 10^8 M_⊙). Imaging and model-fitting techniques were applied to the data to parameterize the fine-scale source structure and its variation. We find a multicomponent inner jet morphology with the northernmost component elongated perpendicular to the direction of the jet, as imaged at longer wavelengths. The elongated nuclear structure is consistent on all four observing days and across different imaging methods and model-fitting techniques, and therefore appears robust. Owing to its compactness and brightness, we associate the northern nuclear structure as the VLBI “core”. This morphology can be interpreted as either a broad resolved jet base or a spatially bent jet. We also find significant day-to-day variations in the closure phases, which appear most pronounced on the triangles with the longest baselines. Our analysis shows that this variation is related to a systematic change of the source structure. Two inner jet components move non-radially at apparent speeds of ∼15 c and ∼20 c (∼1.3 and ∼1.7 μas day^−1, respectively), which more strongly supports the scenario of traveling shocks or instabilities in a bent, possibly rotating jet. The observed apparent speeds are also coincident with the 3C 279 large-scale jet kinematics observed at longer (cm) wavelengths, suggesting no significant jet acceleration between the 1.3 mm core and the outer jet. The intrinsic brightness temperature of the jet components are ≲10^10 K, a magnitude or more lower than typical values seen at ≥7 mm wavelengths. The low brightness temperature and morphological complexity suggest that the core region of 3C 279 becomes optically thin at short (mm) wavelengths."
MARK KRAMER,First M87 Event Horizon Telescope results. II. Array and instrumentation,"The Event Horizon Telescope (EHT) is a very long baseline interferometry (VLBI) array that comprises millimeter- and submillimeter-wavelength telescopes separated by distances comparable to the diameter of the Earth. At a nominal operating wavelength of ~1.3 mm, EHT angular resolution (λ/D) is ~25 μas, which is sufficient to resolve nearby supermassive black hole candidates on spatial and temporal scales that correspond to their event horizons. With this capability, the EHT scientific goals are to probe general relativistic effects in the strong-field regime and to study accretion and relativistic jet formation near the black hole boundary. In this Letter we describe the system design of the EHT, detail the technology and instrumentation that enable observations, and provide measures of its performance. Meeting the EHT science objectives has required several key developments that have facilitated the robust extension of the VLBI technique to EHT observing wavelengths and the production of instrumentation that can be deployed on a heterogeneous array of existing telescopes and facilities. To meet sensitivity requirements, high-bandwidth digital systems were developed that process data at rates of 64 gigabit s−1, exceeding those of currently operating cm-wavelength VLBI arrays by more than an order of magnitude. Associated improvements include the development of phasing systems at array facilities, new receiver installation at several sites, and the deployment of hydrogen maser frequency standards to ensure coherent data capture across the array. These efforts led to the coordination and execution of the first Global EHT observations in 2017 April, and to event-horizon-scale imaging of the supermassive black hole candidate in M87."
MARK KRAMER,First M87 Event Horizon Telescope results. I. The shadow of the supermassive black hole,"When surrounded by a transparent emission region, black holes are expected to reveal a dark shadow caused by gravitational light bending and photon capture at the event horizon. To image and study this phenomenon, we have assembled the Event Horizon Telescope, a global very long baseline interferometry array observing at a wavelength of 1.3 mm. This allows us to reconstruct event-horizon-scale images of the supermassive black hole candidate in the center of the giant elliptical galaxy M87. We have resolved the central compact radio source as an asymmetric bright emission ring with a diameter of 42 ± 3 μas, which is circular and encompasses a central depression in brightness with a flux ratio gsim10:1. The emission ring is recovered using different calibration and imaging schemes, with its diameter and width remaining stable over four different observations carried out in different days. Overall, the observed image is consistent with expectations for the shadow of a Kerr black hole as predicted by general relativity. The asymmetry in brightness in the ring can be explained in terms of relativistic beaming of the emission from a plasma rotating close to the speed of light around a black hole. We compare our images to an extensive library of ray-traced general-relativistic magnetohydrodynamic simulations of black holes and derive a central mass of M = (6.5 ± 0.7) × 10^9 M ⊙. Our radio-wave observations thus provide powerful evidence for the presence of supermassive black holes in centers of galaxies and as the central engines of active galactic nuclei. They also present a new tool to explore gravity in its most extreme limit and on a mass scale that was so far not accessible."
MARK KRAMER,First Sagittarius A* Event Horizon Telescope results. III. Imaging of the Galactic center supermassive black hole,"We present the first event-horizon-scale images and spatiotemporal analysis of Sgr A* taken with the Event Horizon Telescope in 2017 April at a wavelength of 1.3 mm. Imaging of Sgr A* has been conducted through surveys over a wide range of imaging assumptions using the classical CLEAN algorithm, regularized maximum likelihood methods, and a Bayesian posterior sampling method. Different prescriptions have been used to account for scattering effects by the interstellar medium toward the Galactic center. Mitigation of the rapid intraday variability that characterizes Sgr A* has been carried out through the addition of a “variability noise budget” in the observed visibilities, facilitating the reconstruction of static full-track images. Our static reconstructions of Sgr A* can be clustered into four representative morphologies that correspond to ring images with three different azimuthal brightness distributions and a small cluster that contains diverse nonring morphologies. Based on our extensive analysis of the effects of sparse (u, v)-coverage, source variability, and interstellar scattering, as well as studies of simulated visibility data, we conclude that the Event Horizon Telescope Sgr A* data show compelling evidence for an image that is dominated by a bright ring of emission with a ring diameter of ∼50 μas, consistent with the expected “shadow” of a 4 × 106 M⊙ black hole in the Galactic center located at a distance of 8 kpc."
MARK KRAMER,Characterizing and mitigating intraday variability: reconstructing source structure in accreting black holes with mm-VLBI,"The extraordinary physical resolution afforded by the Event Horizon Telescope has opened a window onto the astrophysical phenomena unfolding on horizon scales in two known black holes, M87* and Sgr A*. However, with this leap in resolution has come a new set of practical complications. Sgr A* exhibits intraday variability that violates the assumptions underlying Earth aperture synthesis, limiting traditional image reconstruction methods to short timescales and data sets with very sparse (u, v) coverage. We present a new set of tools to detect and mitigate this variability. We develop a data-driven, model-agnostic procedure to detect and characterize the spatial structure of intraday variability. This method is calibrated against a large set of mock data sets, producing an empirical estimator of the spatial power spectrum of the brightness fluctuations. We present a novel Bayesian noise modeling algorithm that simultaneously reconstructs an average image and statistical measure of the fluctuations about it using a parameterized form for the excess variance in the complex visibilities not otherwise explained by the statistical errors. These methods are validated using a variety of simulated data, including general relativistic magnetohydrodynamic simulations appropriate for Sgr A* and M87*. We find that the reconstructed source structure and variability are robust to changes in the underlying image model. We apply these methods to the 2017 EHT observations of M87*, finding evidence for variability across the EHT observing campaign. The variability mitigation strategies presented are widely applicable to very long baseline interferometry observations of variable sources generally, for which they provide a data-informed averaging procedure and natural characterization of inter-epoch image consistency."
ABRAHAM SEIDMANN,Pick the right tactics when online sales go live: an empirical analysis of livestreaming for Amazon sellers,
CARL FRANZBLAU,CityLab at Boston University – thirty years of innovation and partnerships,"Boston University’s (BU) CityLab program was created in 1991 as a partnership between faculty membersof BU’s School of Medicine and School of Education in response to the first call for proposals under the Science Education Partnership Awards initiative of the National Institutes of Health. CityLab’s founders recognized the need for CityLab, a centrally-located facility for pre-college teachers and students to explore the burgeoning world of biotechnology. The mis-sion has always been to share the excitement of science with students and teachers by engaging them in hands-on laboratory experiences, thereby fostering the development of a robust pool of scientists and physicians and a scientifically-literate popu-lace. In order to reach more schools, particularly those that could not come to CityLab’s facility in Boston, the CityLab team pioneered the mobile science laboratory concept with the launch of its MobileLab in 1998. Both CityLab and MobileLab have been replicated in the U.S. and abroad. CityLab has sustained itself because it has benefited from stable leadership, built and disseminated models for hands-on STEM education, embraced innovation by creating new programs to serve additional populations, and developed diverse funding streams. The CityLab program has been remarkable in its outreach, success, and longevity."
CARL FRANZBLAU,Boston University Medical Center Annual Report: 1968-1969,
ANDREW A KING,The dangerous allure of win-win strategies,"For the past 30 years, celebrated academics and business leaders have promoted the idea that companies often profit by addressing social and environmental problems. Although these proposals have been hailed as promising breakthroughs, they are unscientific and counterproductive."
ANDREW A KING,Building knowledge by mapping model uncertainty in six studies of social and financial performance,"RESEARCH SUMMARY: Many scholars bemoan the difficulty of learning from individual research reports. Replication is often prescribed as a salve, but few replications are conducted, and even fewer allow the formation of a coherent understanding. In this article, we propose a complement to replication that emphasizes the mapping of epistemic uncertainties. We demonstrate our approach by exploring the results of six related studies on the link between social and financial performance. We show that our method allows the synthesis of seemingly conflicting findings, and we propose that it should be used proactively, prior to replication, to speed the growth of knowledge. MANAGERIAL SUMMARY: Any single empirical study provides a weak basis for inference. As a result, scholars advocate repeated analysis of important issues, but evidence from replications can be hard to integrate into a coherent understanding. For example, six important studies of the link between corporate social and financial performance have been published in this journal, but their conflicting results have defied integration. We show that a new approach to empirical research allows their reconciliation: all six suggest that across firms, social and financial performance are correlated but that improvements in social performance seldom precede increased financial performance."
ANDREW A KING,Corporate sustainability: a model uncertainty analysis,"For decades, scholars have searched for a connection between a corporation’s current performance with respect to sustainability and the future returns of its stock. In 2016, Khan, Serafeim, and Yoon published an apparent breakthrough in this quest: guidance on materiality from the Sustainability Accounting Standards Board allowed the construction of corporate sustainability scales that reliably predicted stock returns. Their finding had immediate and broad impact, but it remains, in its authors own words, just “first evidence.” Here, we further explore the relationship between material-sustainability and stock return by performing a “model uncertainty analysis.” We reproduce the original estimate but conclude that it is a statistical artifact. We then use machine learning to explore the practicality of employing historical associations to determine which aspects of sustainability are material to investors. We conclude that, for one popular source of data on corporate sustainability, accurate guidance on materiality may be difficult to achieve."
ANDREW A KING,Prospects for beyond the standard model physics searches at the deep underground neutrino experiment: DUNE collaboration,"The Deep Underground Neutrino Experiment (DUNE) will be a powerful tool for a variety of physics topics. The high-intensity proton beams provide a large neutrino flux, sampled by a near detector system consisting of a combination of capable precision detectors, and by the massive far detector system located deep underground. This configuration sets up DUNE as a machine for discovery, as it enables opportunities not only to perform precision neutrino measurements that may uncover deviations from the present three-flavor mixing paradigm, but also to discover new particles and unveil new interactions and symmetries beyond those predicted in the Standard Model (SM). Of the many potential beyond the Standard Model (BSM) topics DUNE will probe, this paper presents a selection of studies quantifying DUNE's sensitivities to sterile neutrino mixing, heavy neutral leptons, non-standard interactions, CPT symmetry violation, Lorentz invariance violation, neutrino trident production, dark matter from both beam induced and cosmogenic sources, baryon number violation, and other new physics topics that complement those at high-energy colliders and significantly extend the present reach."
ANDREW A KING,Identification and reconstruction of low-energy electrons in the ProtoDUNE-SP detector,
ANDREW A KING,Scintillation light detection in the 6-m drift-length ProtoDUNE Dual Phase liquid argon TPC,"DUNE is a dual-site experiment for long-baseline neutrino oscillation studies, neutrino astrophysics and nucleon decay searches. ProtoDUNE Dual Phase (DP) is a 6  ×  6  ×  6 m 3 liquid argon time-projection-chamber (LArTPC) that recorded cosmic-muon data at the CERN Neutrino Platform in 2019-2020 as a prototype of the DUNE Far Detector. Charged particles propagating through the LArTPC produce ionization and scintillation light. The scintillation light signal in these detectors can provide the trigger for non-beam events. In addition, it adds precise timing capabilities and improves the calorimetry measurements. In ProtoDUNE-DP, scintillation and electroluminescence light produced by cosmic muons in the LArTPC is collected by photomultiplier tubes placed up to 7 m away from the ionizing track. In this paper, the ProtoDUNE-DP photon detection system performance is evaluated with a particular focus on the different wavelength shifters, such as PEN and TPB, and the use of Xe-doped LAr, considering its future use in giant LArTPCs. The scintillation light production and propagation processes are analyzed and a comparison of simulation to data is performed, improving understanding of the liquid argon properties."
ANDREW A KING,Writing a useful empirical journal article,"The impact of research depends on how well readers can understand and use published reports, yet as researchers we often fail to consider what our readers need. Instead, we structure reports to match a socially accepted, but often inappropriate, standard design – the “hourglass”. In this essay, I suggest that when writing a report, we should adjust our reporting style to suit our target readers. I outline four reporting templates for quantitative empirical studies that are suited to differing reader needs."
DONALD K WRIGHT,Monitoring the mmorphology of M87* in 2009–2017 with the Event Horizon Telescope,"The Event Horizon Telescope (EHT) has recently delivered the first resolved images of M87*, the supermassive black hole in the center of the M87 galaxy. These images were produced using 230 GHz observations performed in April 2017. Additional observations are required to investigate the persistence of the primary image feature – a ring with azimuthal brightness asymmetry – and to quantify the image variability on event horizon scales. To address this need, we analyze M87* data collected with prototype EHT arrays in 2009, 2011, 2012, and 2013. While these observations do not contain enough information to produce images, they are sufficient to constrain simple geometric models. We develop a modeling approach based on the framework utilized for the 2017 EHT data analysis and validate our procedures using synthetic data. Applying the same approach to the observational data sets, we find the M87* morphology in 2009–2017 to be consistent with a persistent asymmetric ring of 40 as diameter. The position angle of peak intensity varies in time. In particular, we find a significant difference between the position angle measured in 2013 and 2017. These variations are in broad agreement with predictions of a subset of general relativistic magnetohydrodynamic simulations. We show that quantifying the variability across multiple observational epochs has the potential to constrain physical properties of the source, such as the accretion state or the black hole spin."
DONALD K WRIGHT,Optical calibration of the SNO+ detector in the water phase with deployed sources,"SNO+ is a large-scale liquid scintillator experiment with the primary goal of searching for neutrinoless double beta decay, and is located approximately 2 km underground in SNOLAB, Sudbury, Canada. The detector acquired data for two years as a pure water Cherenkov detector, starting in May 2017. During this period, the optical properties of the detector were measured in situ using a deployed light diffusing sphere, with the goal of improving the detector model and the energy response systematic uncertainties. The measured parameters included the water attenuation coefficients, effective attenuation coefficients for the acrylic vessel, and the angular response of the photomultiplier tubes and their surrounding light concentrators, all across different wavelengths. The calibrated detector model was validated using a deployed tagged gamma source, which showed a 0.6% variation in energy scale across the primary target volume."
DONALD K WRIGHT,The SNO+ experiment,
DONALD K WRIGHT,Current status and future prospects of the SNO+ experiment,"SNO+ is a large liquid scintillator-based experiment located 2 km underground at SNOLAB, Sudbury, Canada. It reuses the Sudbury Neutrino Observatory detector, consisting of a 12 m diameter acrylic vessel which will be filled with about 780 tonnes of ultra-pure liquid scintillator. Designed as a multipurpose neutrino experiment, the primary goal of SNO+ is a search for the neutrinoless double-beta decay (0νββ) of ^130Te. In Phase I, the detector will be loaded with 0.3% natural tellurium, corresponding to nearly 800 kg of ^130Te, with an expected effective Majorana neutrino mass sensitivity in the region of 55–133 meV, just above the inverted mass hierarchy. Recently, the possibility of deploying up to ten times more natural tellurium has been investigated, which would enable SNO+ to achieve sensitivity deep into the parameter space for the inverted neutrino mass hierarchy in the future. Additionally, SNO+ aims to measure reactor antineutrino oscillations, low energy solar neutrinos, and geoneutrinos, to be sensitive to supernova neutrinos, and to search for exotic physics. A first phase with the detector filled with water will begin soon, with the scintillator phase expected to start after a few months of water data taking. The 0νββ Phase I is foreseen for 2017."
DONALD K WRIGHT,Event Horizon Telescope imaging of the archetypal blazar 3C 279 at an extreme 20 microarcsecond resolution,"3C 279 is an archetypal blazar with a prominent radio jet that show broadband flux density variability across the entire electromagnetic spectrum. We use an ultra-high angular resolution technique – global Very Long Baseline Interferometry (VLBI) at 1.3 mm (230 GHz) – to resolve the innermost jet of 3C 279 in order to study its fine-scale morphology close to the jet base where highly variable γ-ray emission is thought to originate, according to various models. The source was observed during four days in April 2017 with the Event Horizon Telescope at 230 GHz, including the phased Atacama Large Millimeter/submillimeter Array (ALMA), at an angular resolution of ∼20 μas (at a redshift of z = 0.536 this corresponds to ∼0.13 pc  ∼ 1700 Schwarzschild radii with a black hole mass M<jats:sub>BH</jats:sub> = 8 × 10^8 M_⊙). Imaging and model-fitting techniques were applied to the data to parameterize the fine-scale source structure and its variation. We find a multicomponent inner jet morphology with the northernmost component elongated perpendicular to the direction of the jet, as imaged at longer wavelengths. The elongated nuclear structure is consistent on all four observing days and across different imaging methods and model-fitting techniques, and therefore appears robust. Owing to its compactness and brightness, we associate the northern nuclear structure as the VLBI “core”. This morphology can be interpreted as either a broad resolved jet base or a spatially bent jet. We also find significant day-to-day variations in the closure phases, which appear most pronounced on the triangles with the longest baselines. Our analysis shows that this variation is related to a systematic change of the source structure. Two inner jet components move non-radially at apparent speeds of ∼15 c and ∼20 c (∼1.3 and ∼1.7 μas day^−1, respectively), which more strongly supports the scenario of traveling shocks or instabilities in a bent, possibly rotating jet. The observed apparent speeds are also coincident with the 3C 279 large-scale jet kinematics observed at longer (cm) wavelengths, suggesting no significant jet acceleration between the 1.3 mm core and the outer jet. The intrinsic brightness temperature of the jet components are ≲10^10 K, a magnitude or more lower than typical values seen at ≥7 mm wavelengths. The low brightness temperature and morphological complexity suggest that the core region of 3C 279 becomes optically thin at short (mm) wavelengths."
DONALD K WRIGHT,First M87 Event Horizon Telescope results. II. Array and instrumentation,"The Event Horizon Telescope (EHT) is a very long baseline interferometry (VLBI) array that comprises millimeter- and submillimeter-wavelength telescopes separated by distances comparable to the diameter of the Earth. At a nominal operating wavelength of ~1.3 mm, EHT angular resolution (λ/D) is ~25 μas, which is sufficient to resolve nearby supermassive black hole candidates on spatial and temporal scales that correspond to their event horizons. With this capability, the EHT scientific goals are to probe general relativistic effects in the strong-field regime and to study accretion and relativistic jet formation near the black hole boundary. In this Letter we describe the system design of the EHT, detail the technology and instrumentation that enable observations, and provide measures of its performance. Meeting the EHT science objectives has required several key developments that have facilitated the robust extension of the VLBI technique to EHT observing wavelengths and the production of instrumentation that can be deployed on a heterogeneous array of existing telescopes and facilities. To meet sensitivity requirements, high-bandwidth digital systems were developed that process data at rates of 64 gigabit s−1, exceeding those of currently operating cm-wavelength VLBI arrays by more than an order of magnitude. Associated improvements include the development of phasing systems at array facilities, new receiver installation at several sites, and the deployment of hydrogen maser frequency standards to ensure coherent data capture across the array. These efforts led to the coordination and execution of the first Global EHT observations in 2017 April, and to event-horizon-scale imaging of the supermassive black hole candidate in M87."
DONALD K WRIGHT,First M87 Event Horizon Telescope results. I. The shadow of the supermassive black hole,"When surrounded by a transparent emission region, black holes are expected to reveal a dark shadow caused by gravitational light bending and photon capture at the event horizon. To image and study this phenomenon, we have assembled the Event Horizon Telescope, a global very long baseline interferometry array observing at a wavelength of 1.3 mm. This allows us to reconstruct event-horizon-scale images of the supermassive black hole candidate in the center of the giant elliptical galaxy M87. We have resolved the central compact radio source as an asymmetric bright emission ring with a diameter of 42 ± 3 μas, which is circular and encompasses a central depression in brightness with a flux ratio gsim10:1. The emission ring is recovered using different calibration and imaging schemes, with its diameter and width remaining stable over four different observations carried out in different days. Overall, the observed image is consistent with expectations for the shadow of a Kerr black hole as predicted by general relativity. The asymmetry in brightness in the ring can be explained in terms of relativistic beaming of the emission from a plasma rotating close to the speed of light around a black hole. We compare our images to an extensive library of ray-traced general-relativistic magnetohydrodynamic simulations of black holes and derive a central mass of M = (6.5 ± 0.7) × 10^9 M ⊙. Our radio-wave observations thus provide powerful evidence for the presence of supermassive black holes in centers of galaxies and as the central engines of active galactic nuclei. They also present a new tool to explore gravity in its most extreme limit and on a mass scale that was so far not accessible."
JOHN A PORCO,Rocaglates induce gain-of-function alterations to eIF4A and eIF4F,"Rocaglates are a diverse family of biologically active molecules that have gained tremendous interest in recent years due to their promising activities in pre-clinical cancer studies. As a result, this family of compounds has been significantly expanded through the development of efficient synthetic schemes. However, it is unknown whether all of the members of the rocaglate family act through similar mechanisms of action. Here, we present a comprehensive study comparing the biological activities of >200 rocaglates to better understand how the presence of different chemical entities influences their biological activities. Through this, we find that most rocaglates preferentially repress the translation of mRNAs containing purine-rich 5' leaders, but certain rocaglates lack this bias in translation repression. We also uncover an aspect of rocaglate mechanism of action in which the pool of translationally active eIF4F is diminished due to the sequestration of the complex onto RNA."
JOHN A PORCO,Identification of a novel polyprenylated acylphloroglucinol‑derived SIRT1 inhibitor with cancer‑specific anti-proliferative and invasion-suppressing activities,"SIRT1, a class III histone deacetylase, plays a critical role in regulating cancer cell growth, migration and invasion, which makes it a potential target for cancer therapeutics. In this study, we screened derivatives of several groups of natural products and identified a novel SIRT1 inhibitor JQ-101, a synthetic derivative of the polyprenylated acylphloroglucinol (PPAP) natural products, with an IC(50) for SIRT1 of 30 µM in vitro, with 5-fold higher activity for SIRT1 vs. SIRT2. Exposure of tumor cells to JQ-101 significantly enhanced acetylation of p53 and histone H4K16 at known sites of SIRT1 deacetylation, validating SIRT1 as its cellular target. JQ-101 suppressed cancer cell growth and survival by targeting SIRT1, and also exhibited selective cytotoxicity towards a panel of human tumor cell lines, while producing no toxicity in two normal human cell types at comparable concentrations. JQ-101 induced both apoptosis and cell senescence, and suppressed cancer cell invasion in vitro. In summary, we have identified JQ-101 as a new SIRT1 inhibitor which may have potential application in cancer treatment through its ability to induce tumor cell apoptosis and senescence and suppress cancer cell invasion."
JOHN A PORCO,Atropselective syntheses of (-) and (+) rugulotrosin A utilizing point-to-axial chirality transfer,"Chiral, dimeric natural products containing complex structures and interesting biological properties have inspired chemists and biologists for decades. A seven-step total synthesis of the axially chiral, dimeric tetrahydroxanthone natural product rugulotrosin A is described. The synthesis employs a one-pot Suzuki coupling/dimerization to generate the requisite 2,2'-biaryl linkage. Highly selective point-to-axial chirality transfer was achieved using palladium catalysis with achiral phosphine ligands. Single X-ray crystal diffraction data were obtained to confirm both the atropisomeric configuration and absolute stereochemistry of rugulotrosin A. Computational studies are described to rationalize the atropselectivity observed in the key dimerization step. Comparison of the crude fungal extract with synthetic rugulotrosin A and its atropisomer verified that nature generates a single atropisomer of the natural product."
JOHN A PORCO,nanoCAGE reveals 5' UTR features that define specific modes of translation of functionally related MTOR-sensitive mRNAs,"The diversity of MTOR-regulated mRNA translation remains unresolved. Whereas ribosome-profiling suggested that MTOR almost exclusively stimulates translation of the TOP (terminal oligopyrimidine motif) and TOP-like mRNAs, polysome-profiling indicated that MTOR also modulates translation of mRNAs without the 5' TOP motif (non-TOP mRNAs). We demonstrate that in ribosome-profiling studies, detection of MTOR-dependent changes in non-TOP mRNA translation was obscured by low sensitivity and methodology biases. Transcription start site profiling using nano-cap analysis of gene expression (nanoCAGE) revealed that not only do many MTOR-sensitive mRNAs lack the 5' TOP motif but that 5' UTR features distinguish two functionally and translationally distinct subsets of MTOR-sensitive mRNAs: (1) mRNAs with short 5' UTRs enriched for mitochondrial functions, which require EIF4E but are less EIF4A1-sensitive; and (2) long 5' UTR mRNAs encoding proliferation- and survival-promoting proteins, which are both EIF4E- and EIF4A1-sensitive. Selective inhibition of translation of mRNAs harboring long 5' UTRs via EIF4A1 suppression leads to sustained expression of proteins involved in respiration but concomitant loss of those protecting mitochondrial structural integrity, resulting in apoptosis. Conversely, simultaneous suppression of translation of both long and short 5' UTR mRNAs by MTOR inhibitors results in metabolic dormancy and a predominantly cytostatic effect. Thus, 5' UTR features define different modes of MTOR-sensitive translation of functionally distinct subsets of mRNAs, which may explain the diverse impact of MTOR and EIF4A inhibitors on neoplastic cells."
JOHN A PORCO,Total synthesis of aurofusarin: studies on the atropisomeric stability of bis-naphthoquinones,An efficient annulation involving pyrone addition to a quinone and Dieckmann condensation was developed for rapid assembly of a γ-naphthopyrone monomeric precursor to the bis-naphthoquinone natural product aurofusarin. Dimerization was achieved through PdII -catalyzed dehydrogenative coupling. Further studies employing asymmetric nucleophilic epoxidation indicate that the atropisomers of aurofusarin and derivatives are not configurationally stable at ambient temperature.
JOHN A PORCO,Sensitization of renal carcinoma cells to TRAIL-induced apoptosis by rocaglamide and analogs,"Rocaglamide has been reported to sensitize several cell types to TRAIL-induced apoptosis. In recent years, advances in synthetic techniques have led to generation of novel rocaglamide analogs. However, these have not been extensively analyzed as TRAIL sensitizers, particularly in TRAIL-resistant renal cell carcinoma cells. Evaluation of rocaglamide and analogs identified 29 compounds that are able to sensitize TRAIL-resistant ACHN cells to TRAIL-induced, caspase-dependent apoptosis with sub-µM potency which correlated with their potency as protein synthesis inhibitors and with loss of cFLIP protein in the same cells. Rocaglamide alone induced cell cycle arrest, but not apoptosis. Rocaglates averaged 4–5-fold higher potency as TRAIL sensitizers than as protein synthesis inhibitors suggesting a potential window for maximizing TRAIL sensitization while minimizing effects of general protein synthesis inhibition. A wide range of other rocaglate effects (e.g. on JNK or RAF-MEK-ERK signaling, death receptor levels, ROS, ER stress, eIF4E phosphorylation) were assessed, but did not contribute to TRAIL sensitization. Other than a rapid loss of MCL-1, rocaglates had minimal effects on mitochondrial apoptotic pathway proteins. The identification of structurally diverse/mechanistically similar TRAIL sensitizing rocaglates provides insights into both rocaglate structure and function and potential further development for use in RCC-directed combination therapy."
JOHN A PORCO,Targeting cap-dependent translation blocks converging survival signals by AKT and PIM kinases in lymphoma,"New anticancer drugs that target oncogenic signaling molecules have greatly improved the treatment of certain cancers. However, resistance to targeted therapeutics is a major clinical problem and the redundancy of oncogenic signaling pathways provides back-up mechanisms that allow cancer cells to escape. For example, the AKT and PIM kinases produce parallel oncogenic signals and share many molecular targets, including activators of cap-dependent translation. Here, we show that PIM kinase expression can affect the clinical outcome of lymphoma chemotherapy. We observe the same in animal lymphoma models. Whereas chemoresistance caused by AKT is readily reversed with rapamycin, PIM-mediated resistance is refractory to mTORC1 inhibition. However, both PIM- and AKT-expressing lymphomas depend on cap-dependent translation, and genetic or pharmacological blockade of the translation initiation complex is highly effective against these tumors. The therapeutic effect of blocking cap-dependent translation is mediated, at least in part, by decreased production of short-lived oncoproteins including c-MYC, Cyclin D1, MCL1, and the PIM1/2 kinases themselves. Hence, targeting the convergence of oncogenic survival signals on translation initiation is an effective alternative to combinations of kinase inhibitors."
JOHN A PORCO,Multidimensional reaction screening for photochemical transformations as a tool for discovering new chemotypes,"We have developed an automated photochemical microfluidics platform that integrates a 1 kW high-pressure Hg vapor lamp and allows for analytical pulse flow or preparative continuous flow reactions. Herein, we will discuss the use of this platform toward the discovery of new chemotypes through multidimensional reaction screening. We will highlight the ability to discretely control wavelengths with optical filters, allowing for control of reaction outcomes."
JOHN A PORCO,Divergent total syntheses of rhodomyrtosones A and B,"Herein, we report total syntheses of the tetramethyldihydroxanthene natural product rhodomyrtosone B and the related bis-furan β-triketone natural product rhodomyrtosone A. Nickel-(II)-catalyzed 1,4-conjugate addition of an α-alkylidene-β-dicarbonyl substrate was developed to access the congener rhodomyrtosone B, and oxygenation of the same monoalkylidene derivative followed by cyclization was employed to obtain the bis-furan natural product rhodomyrtosone A."
JOHN A PORCO,"Syntheses of (+)-30-epi-, (-)-6-epi-, (±)-6,30-epi-13,14-didehydroxyisogarcinol and (±)-6,30-epi-garcimultiflorone A utilizing highly diastereoselective, Lewis acid-controlled cyclizations","The first syntheses of 13,14-didehydroxyisogarcinol (6) and garcimultiflorone A (5) stereoisomers are reported in six steps from a commercially available phloroglucinol. Lewis acid-controlled, diastereoselective cationic oxycyclizations enabled asymmetric syntheses of (-)-6-epi-6 and (+)-30-epi-6. A similar strategy enabled production of the meso-dervied isomers (±)-6,30-epi-6 and (±)-6,30-epi-5. Finally, a convenient strategy for gram scale synthesis was developed utilizing diastereomer separation at a later stage in the synthesis that minimized the number of necessary synthetic operations to access all possible stereoisomers."
JOHN A PORCO,Asymmetric syntheses of the flavonoid Diels-Alder natural products sanggenons C and O,"Metal-catalyzed, double Claisen rearrangement of a bis-allyloxyflavone has been utilized to enable a concise synthesis of the hydrobenzofuro[3,2-b]chromenone core structure of the natural products sanggenon A and sanggenol F. In addition, catalytic, enantioselective [4+2] cycloadditions of 2'-hydroxychalcones have been accomplished using B(OPh)3/BINOL complexes. Asymmetric syntheses of the flavonoid Diels-Alder natural products sanggenons C and O have been achieved employing a stereodivergent reaction of a racemic mixture (stereodivergent RRM) involving [4+2] cycloaddition."
JOHN A PORCO,Biomimetic total synthesis of (+/-)-griffipavixanthone via a cationic cycloaddition-cyclization cascade,"We report the concise, biomimetic total synthesis of the dimeric, Diels-Alder natural product griffipavixanthone from a readily accessible prenylated xanthone monomer. The key step utilizes a novel intermolecular [4+2] cycloaddition-cyclization cascade between a vinyl p-quinone methide and an in situ generated isomeric diene promoted by either Lewis or Brønsted acids. Experimental and computational studies of the reaction pathway suggest that a stepwise, cationic Diels-Alder cycloaddition is operative."
JOHN A PORCO,Fine-tuning of macrophage activation using synthetic rocaglate derivatives.,"Drug-resistant bacteria represent a significant global threat. Given the dearth of new antibiotics, host-directed therapies (HDTs) are especially desirable. As IFN-gamma (IFNγ) plays a central role in host resistance to intracellular bacteria, including Mycobacterium tuberculosis, we searched for small molecules to augment the IFNγ response in macrophages. Using an interferon-inducible nuclear protein Ipr1 as a biomarker of macrophage activation, we performed a high-throughput screen and identified molecules that synergized with low concentration of IFNγ. Several active compounds belonged to the flavagline (rocaglate) family. In primary macrophages a subset of rocaglates 1) synergized with low concentrations of IFNγ in stimulating expression of a subset of IFN-inducible genes, including a key regulator of the IFNγ network, Irf1; 2) suppressed the expression of inducible nitric oxide synthase and type I IFN and 3) induced autophagy. These compounds may represent a basis for macrophage-directed therapies that fine-tune macrophage effector functions to combat intracellular pathogens and reduce inflammatory tissue damage. These therapies would be especially relevant to fighting drug-resistant pathogens, where improving host immunity may prove to be the ultimate resource."
JOHN A PORCO,Suppression of eukaryotic initiation factor 4E prevents chemotherapy-induced alopecia,"BACKGROUND: Chemotherapy-induced hair loss (alopecia) (CIA) is one of the most feared side effects of chemotherapy among cancer patients. There is currently no pharmacological approach to minimize CIA, although one strategy that has been proposed involves protecting normal cells from chemotherapy by transiently inducing cell cycle arrest. Proof-of-concept for this approach, known as cyclotherapy, has been demonstrated in cell culture settings. METHODS: The eukaryotic initiation factor (eIF) 4E is a cap binding protein that stimulates ribosome recruitment to mRNA templates during the initiation phase of translation. Suppression of eIF4E is known to induce cell cycle arrest. Using a novel inducible and reversible transgenic mouse model that enables RNAi-mediated suppression of eIF4E in vivo, we assessed the consequences of temporal eIF4E suppression on CIA. RESULTS: Our results demonstrate that transient inhibition of eIF4E protects against cyclophosphamide-induced alopecia at the organismal level. At the cellular level, this protection is associated with an accumulation of cells in G1, reduced apoptotic indices, and was phenocopied using small molecule inhibitors targeting the process of translation initiation. CONCLUSIONS: Our data provide a rationale for exploring suppression of translation initiation as an approach to prevent or minimize cyclophosphamide-induced alopecia."
JOHN A PORCO,RNA G-quadruplexes cause eIF4A-dependent oncogene translation in cancer,"The translational control of oncoprotein expression is implicated in many cancers. Here we report an eIF4A RNA helicase-dependent mechanism of translational control that contributes to oncogenesis and underlies the anticancer effects of silvestrol and related compounds. For example, eIF4A promotes T-cell acute lymphoblastic leukaemia development in vivo and is required for leukaemia maintenance. Accordingly, inhibition of eIF4A with silvestrol has powerful therapeutic effects against murine and human leukaemic cells in vitro and in vivo. We use transcriptome-scale ribosome footprinting to identify the hallmarks of eIF4A-dependent transcripts. These include 5' untranslated region (UTR) sequences such as the 12-nucleotide guanine quartet (CGG)4 motif that can form RNA G-quadruplex structures. Notably, among the most eIF4A-dependent and silvestrol-sensitive transcripts are a number of oncogenes, superenhancer-associated transcription factors, and epigenetic regulators. Hence, the 5' UTRs of select cancer genes harbour a targetable requirement for the eIF4A RNA helicase."
JOHN A PORCO,Inhibition of oncogenic transcription factor REL by the natural product derivative calafianin monomer 101 induces proliferation arrest and apoptosis in human B-lymphoma cell lines,"Increased activity of transcription factor NF-κB has been implicated in many B-cell lymphomas. We investigated effects of synthetic compound calafianin monomer (CM101) on biochemical and biological properties of NF-κB. In human 293 cells, CM101 selectively inhibited DNA binding by overexpressed NF-κB subunits REL (human c-Rel) and p65 as compared to NF-κB p50, and inhibition of REL and p65 DNA binding by CM101 required a conserved cysteine residue. CM101 also inhibited DNA binding by REL in human B-lymphoma cell lines, and the sensitivity of several B-lymphoma cell lines to CM101-induced proliferation arrest and apoptosis correlated with levels of cellular and nuclear REL. CM101 treatment induced both phosphorylation and decreased expression of anti-apoptotic protein Bcl-XL, a REL target gene product, in sensitive B-lymphoma cell lines. Ectopic expression of Bcl-XL protected SUDHL-2 B-lymphoma cells against CM101-induced apoptosis, and overexpression of a transforming mutant of REL decreased the sensitivity of BJAB B-lymphoma cells to CM101-induced apoptosis. Lipopolysaccharide-induced activation of NF-κB signaling upstream components occurred in RAW264.7 macrophages at CM101 concentrations that blocked NF-κB DNA binding. Direct inhibitors of REL may be useful for treating B-cell lymphomas in which REL is active, and may inhibit B-lymphoma cell growth at doses that do not affect some immune-related responses in normal cells."
JOHN A PORCO,Development of a potent and selective HDAC8 inhibitor,"A novel, isoform-selective inhibitor of histone deacetylase 8 (HDAC8) has been discovered by the repurposing of a diverse compound collection. Medicinal chemistry optimization led to the identification of a highly potent (0.8 nM) and selective inhibitor of HDAC8."
JOHN A PORCO,Total synthesis and stereochemical assignment of (±)-sorbiterrin A.,"A concise, biomimetic approach to sorbiterrin A has been developed employing consecutive Michael additions of a 4-hydroxypyrone to a sorbicillinol derivative and silver nanoparticle-mediated bridged aldol/dehydration to construct the [3.3.1] ring system. The relative stereochemistry of sorbiterrin A was unambiguously confirmed by X-ray crystallographic analysis."
JOHN A PORCO,"CRISPR-mediated drug-target validation reveals selective pharmacological inhibition of the RNA Helicase, eIF4A","Targeting translation initiation is an emerging anti-neoplastic strategy that capitalizes on de-regulated upstream MAPK and PI3K-mTOR signaling pathways in cancers. A key regulator of translation that controls ribosome recruitment flux is eukaryotic initiation factor (eIF) 4F, a hetero-trimeric complex composed of the cap binding protein eIF4E, the scaffolding protein eIF4G, and the RNA helicase eIF4A. Small molecule inhibitors targeting eIF4F display promising anti-neoplastic activity in preclinical settings. Among these are some rocaglate family members that are well tolerated in vivo, deplete eIF4F of its eIF4A helicase subunit, have shown activity as single agents in several xenograft models, and can reverse acquired resistance to MAPK and PI3K-mTOR targeted therapies. Herein, we highlight the power of using genetic complementation approaches and CRISPR/Cas9-mediated editing for drug-target validation ex vivo and in vivo, linking the anti-tumor properties of rocaglates to eIF4A inhibition."
JOHN A PORCO,A photochemical flow reactor for large scale syntheses of aglain and rocaglate natural product analogues,"Herein, we report the development of continuous flow photoreactors for large scale ESIPT-mediated [3+2]-photocycloaddition of 2-(p-methoxyphenyl)-3-hydroxyflavone and cinnamate-derived dipolarophiles. These reactors can be efficiently numbered up to increase throughput two orders of magnitude greater than the corresponding batch reactions."
JOHN A PORCO,"Syntheses of dimeric tetrahydroxanthones with varied linkages: investigation of ""shapeshifting"" properties","The 2,4'- and 4,4'-linked variants of the cytotoxic agent secalonic acid A and their analogues have been synthesized. Kinetic resolution of an unprotected tetrahydroxanthone scaffold followed by copper-mediated biaryl coupling allowed for efficient access to these compounds. Evaluation of the ""shapeshifting"" properties of 2,2'-, 2,4'-, and 4,4'-linked variants of the secalonic acids A in a polar solvent in conjunction with assays of the compounds against select cancer cell lines was conducted to study possible correlations between linkage variation and cytotoxicity."
JOHN A PORCO,Remodeling of fumagillol: discovery of an oxygen-directed oxidative Mannich reaction,"An efficient, two-step construction of highly complex alkaloid-like compounds from the natural product fumagillol is described. This approach, which mimics a biosynthetic cyclase/oxidase sequence, allows for rapid and efficient structure elaboration of the basic fumagillol scaffold with a variety of readily available coupling partners. Mechanistic experiments leading to the discovery of an oxygen-directed oxidative Mannich reaction are also described."
JOHN A PORCO,Remodelling of the natural product fumagillol employing a reaction discovery approach,"In the search for new biologically active molecules, diversity-oriented synthetic strategies break through the limitation of traditional library synthesis by sampling new chemical space. Many natural products can be regarded as intriguing starting points for diversity-oriented synthesis, wherein stereochemically rich core structures may be reorganized into chemotypes that are distinctly different from the parent structure. Ideally, to be suited to library applications, such transformations should be general and involve few steps. With this objective in mind, the highly oxygenated natural product fumagillol has been successfully remodelled in several ways using a reaction-discovery-based approach. In reactions with amines, excellent regiocontrol in a bis-epoxide opening/cyclization sequence can be obtained by size-dependent interaction of an appropriate catalyst with the parent molecule, forming either perhydroisoindole or perhydroisoquinoline products. Perhydroisoindoles can be further remodelled by cascade processes to afford either morpholinone or bridged 4,1-benzoxazepine-containing structures."
JOHN A PORCO,Thiourea-catalyzed enantioselective addition of indoles to pyrones: alkaloid cores with quaternary carbons,We report the development of a catalytic method for the enantioselective addition of indoles to pyrone-derived electrophiles. Arylpyrrolidino-derived thioureas catalyze the addition with high stereoselectivity in the presence of catalytic quantities of an achiral Brønsted acid. The indole-pyrone adducts feature a quaternary stereocenter and represent an unusual class of indolines bearing structural resemblance to the hybrid natural product pleiocarpamine.
JOHN A PORCO,Targeting the eIF4A RNA helicase blocks translation of the MUC1-C oncoprotein,"The oncogenic MUC1-C subunit is aberrantly overexpressed in most human breast cancers by mechanisms that are not well understood. The present studies demonstrate that stimulation of non- malignant MCF-10A cells with epidermal growth factor (EGF) or heregulin (HRG) results in marked upregulation of MUC1-C translation. Growth factor-induced MUC1-C translation was found to be mediated by PI3K->AKT, and not MEK->ERK1/2, signaling. We also show that activation of the mTORC1->S6K1 pathway decreases PDCD4, an inhibitor of the eIF4A RNA helicase, and contributes to the induction of MUC1-C translation. In concert with these results, treatment of growth factor-stimulated MCF-10A cells with the eIF4A RNA helicase inhibitors, silvestrol and CR-1-31-B, blocked increases in MUC1-C abundance. The functional significance of the increase in MUC1-C translation is supported by the demonstration that MUC1-C, in turn, forms complexes with EGFR and promotes EGFR-mediated activation of the PI3K->AKT pathway and the induction of growth. Compared to MCF-10A cells, constitutive overexpression of MUC1-C in breast cancer cells was unaffected by EGF stimulation, but was blocked by inhibiting PI3K->AKT signaling. The overexpression of MUC1-C in breast cancer cells was also inhibited by blocking eIF4A RNA helicase activity with silvestrol and CR-1-31-B. These findings indicate that EGF-induced MUC1-C expression is mediated by the PI3K->AKT pathway and the eIF4A RNA helicase, and that this response promotes EGFR signaling in an autoinductive loop. The findings also indicate that targeting the eIF4A RNA helicase is a novel approach for blocking MUC1-C overexpression in breast cancer cells."
JOHN A PORCO,"Eucalyptusdimers A-C, dimeric phloroglucinol phellandrene meroterpenoids from Eucalyptus robusta","Eucalyptusdimers A–C, three dimeric phellandrene-derived meroterpenoids featuring an unprecedented, fused skeleton between two phellandrene and two acylphloroglucinol subunits, along with one biogenetically related intermediate, (±)-eucalyprobusone A, were isolated from the fruits of Eucalyptus robusta. Their structures and absolute configurations were elucidated using spectroscopic data, X-ray crystallography, and electronic circular dichroism analysis. The isolated meroterpenoids were evaluated for their anti-inflammatory, acetylcholinesterase inhibitory, and protein tyrosine phosphatase 1B inhibitory effects."
JOHN A PORCO,High-throughput screening in larval zebrafish identifies novel potent sedative-hypnotics,"BACKGROUND: Many general anesthetics were discovered empirically, but primary screens to find new sedative-hypnotics in drug libraries have not used animals, limiting the types of drugs discovered. The authors hypothesized that a sedative-hypnotic screening approach using zebrafish larvae responses to sensory stimuli would perform comparably to standard assays, and efficiently identify new active compounds. METHODS: The authors developed a binary outcome photomotor response assay for zebrafish larvae using a computerized system that tracked individual motions of up to 96 animals simultaneously. The assay was validated against tadpole loss of righting reflexes, using sedative-hypnotics of widely varying potencies that affect various molecular targets. A total of 374 representative compounds from a larger library were screened in zebrafish larvae for hypnotic activity at 10 µM. Molecular mechanisms of hits were explored in anesthetic-sensitive ion channels using electrophysiology, or in zebrafish using a specific reversal agent. RESULTS: Zebrafish larvae assays required far less drug, time, and effort than tadpoles. In validation experiments, zebrafish and tadpole screening for hypnotic activity agreed 100% (n = 11; P = 0.002), and potencies were very similar (Pearson correlation, r > 0.999). Two reversible and potent sedative-hypnotics were discovered in the library subset. CMLD003237 (EC50, ~11 µM) weakly modulated γ-aminobutyric acid type A receptors and inhibited neuronal nicotinic receptors. CMLD006025 (EC50, ~13 µM) inhibited both N-methyl-D-aspartate and neuronal nicotinic receptors. CONCLUSIONS: Photomotor response assays in zebrafish larvae are a mechanism-independent platform for high-throughput screening to identify novel sedative-hypnotics. The variety of chemotypes producing hypnosis is likely much larger than currently known."
JOHN A PORCO,Rocaglates as dual-targeting agents for experimental cerebral malaria,"Cerebral malaria (CM) is a severe and rapidly progressing complication of infection by Plasmodium parasites that is associated with high rates of mortality and morbidity. Treatment options are currently few, and intervention with artemisinin (Art) has limited efficacy, a problem that is compounded by the emergence of resistance to Art in Plasmodium parasites. Rocaglates are a class of natural products derived from plants of the Aglaia genus that have been shown to interfere with eukaryotic initiation factor 4A (eIF4A), ultimately blocking initiation of protein synthesis. Here, we show that the rocaglate CR-1-31B perturbs association of Plasmodium falciparum eIF4A (PfeIF4A) with RNA. CR-1-31B shows potent prophylactic and therapeutic antiplasmodial activity in vivo in mouse models of infection with Plasmodium berghei (CM) and Plasmodium chabaudi (blood-stage malaria), and can also block replication of different clinical isolates of P. falciparum in human erythrocytes infected ex vivo, including drug-resistant P. falciparum isolates. In vivo, a single dosing of CR-1-31B in P. berghei-infected animals is sufficient to provide protection against lethality. CR-1-31B is shown to dampen expression of the early proinflammatory response in myeloid cells in vitro and dampens the inflammatory response in vivo in P. berghei-infected mice. The dual activity of CR-1-31B as an antiplasmodial and as an inhibitor of the inflammatory response in myeloid cells should prove extremely valuable for therapeutic intervention in human cases of CM."
JOHN A PORCO,Synthesis of aza-rocaglates via ESIPT-mediated (3+2) photocycloaddition,"Synthesis of aza-rocaglates, nitrogen-containing analogues of the rocaglate natural products, is reported. The route features ESIPT-mediated (3+2) photocycloaddition of 1-alkyl-2-aryl-3-hydroxyquinolinones with the dipolarophile methyl cinnamate. A continuous photoflow reactor was utilized for photocycloadditions. An array of compounds bearing the hexahydrocyclopenta[b]indole core structure was synthesized and evaluated in translation inhibition assays."
JOHN A PORCO,A novel class of small molecule compounds that inhibit hepatitis C virus infection by targeting the prohibitin-CRaf pathway,"Identification of novel drug targets and affordable therapeutic agents remains a high priority in the fight against chronic hepatitis C virus (HCV) infection. Here, we report that the cellular proteins prohibitin 1 (PHB1) and 2 (PHB2) are pan-genotypic HCV entry factors functioning at a post-binding step. While predominantly found in mitochondria, PHBs localize to the plasma membrane of hepatocytes through their transmembrane domains and interact with both EGFR and CRaf. Targeting PHB by rocaglamide (Roc-A), a natural product that binds PHB1 and 2, reduced cell surface PHB1 and 2, disrupted PHB-CRaf interaction, and inhibited HCV entry at low nanomolar concentrations. A structure-activity analysis of 32 synthetic Roc-A analogs indicated that the chiral, racemic version of aglaroxin C, a natural product biosynthetically related to Roc-A, displayed improved potency and therapeutic index against HCV infection. This study reveals a new class of HCV entry inhibitors that target the PHB1/2-CRaf pathway."
JOHN A PORCO,Asymmetric dearomatization/cyclization enables access to polycyclic chemotypes,"Enantioenriched, polycyclic compounds were obtained from a simple acylphloroglucinol scaffold. Highly enantioselective dearomatization was accomplished using a Trost ligand-palladium(0) complex. A computational DFT model was developed to rationalize observed enantioselectivities and revealed a key reactant-ligand hydrogen bonding interaction. Dearomatized products were used in visible light-mediated photocycloadditions and oxidative free radical cyclizations to obtain novel polycyclic chemotypes including tricyclo[4.3.1.01,4]decan-10-ones, bicyclo[3.2.1]octan-8-ones and highly-substituted cycloheptanones."
JOHN A PORCO,Diastereodivergent synthesis of chiral tetrahydropyrrolodiazepinediones via a one-pot intramolecular aza-Michael/lactamization sequence,"A modular and diastereodivergent synthesis of tetrahydro-1H-pyrrolo[1,2d]diazepine-(2,5)-diones is presented. The tetrahydropyrrolodiazepinedione scaffold is obtained via a base-mediated three-step isomerization/tandem cyclization of amino acid-coupled homoallylic amino esters. Diastereoselectivity of the process is mediated by the interplay of a kinetic cyclization event and a propensity for thermodynamic epimerization at two labile chiral centers, giving rise to two distinct major diastereomers dependent on starting material stereochemistry and reaction conditions selected. Herein, we present a synthetic and computational study for this tandem process on a variety of amino ester substrates."
JOHN A PORCO,Regiodivergent photocyclization of dearomatized acylphloroglucinols: asymmetric syntheses of (—)-nemorosone and (—)-6-epi-garcimultiflorone A,"Regiodivergent photocyclization of dearomatized acylphloroglucinol substrates has been developed to produce type A polycyclic polyprenylated acylphloroglucinol (PPAP) derivatives using an excited-state intramolecular proton transfer (ESIPT) process. Using this strategy, we achieved the enantioselective total syntheses of the type A PPAPs (—)-nemorosone and (—)-6-epi-garcimultiflorone A. Diverse photocyclization substrates have been investigated leading to divergent photocyclization processes as a function of tether length. Photophysical studies were performed, and photocyclization mechanisms were proposed based on investigation of various substrates as well as deuterium-labeling experiments."
JOHN A PORCO,Targeting oncoprotein translation with rocaglates in MYC-driven lymphoma,"MYC-driven lymphomas, especially those with concurrent MYC and BCL2 dysregulation, are currently a challenge in clinical practice due to rapid disease progression, resistance to standard chemotherapy and high risk of refractory disease. MYC plays a central role by coordinating hyperactive protein synthesis with upregulated transcription in order to support rapid proliferation of tumor cells. Translation initiation inhibitor rocaglates have been identified as the most potent drugs in MYC-driven lymphomas as they efficiently inhibit MYC expression and tumor cell viability. We found that this class of compounds can overcome eIF4A abundance by stabilizing target mRNA-eIF4A interaction that directly prevents translation. Proteome-wide quantification demonstrated selective repression of multiple critical oncoproteins in addition to MYC in B cell lymphoma including NEK2, MCL1, AURKA, PLK1, and several transcription factors that are generally considered undruggable. Finally, (−)-SDS-1–021, the most promising synthetic rocaglate, was confirmed to be highly potent as a single agent, and displayed significant synergy with the BCL2 inhibitor ABT199 in inhibiting tumor growth and survival in primary lymphoma cells in vitro and in patient-derived xenograft mouse models. Overall, our findings support the strategy of using rocaglates to target oncoprotein synthesis in MYC-driven lymphomas."
JOHN A PORCO,Photochemical Approaches to Complex Chemotypes: Applications in Natural Product Synthesis.,"The use of photochemical transformations is a powerful strategy that allows for the formation of a high degree of molecular complexity from relatively simple building blocks in a single step. A central feature of all light-promoted transformations is the involvement of electronically excited states, generated upon absorption of photons. This produces transient reactive intermediates and significantly alters the reactivity of a chemical compound. The input of energy provided by light thus offers a means to produce strained and unique target compounds that cannot be assembled using thermal protocols. This review aims at highlighting photochemical transformations as a tool for rapidly accessing structurally and stereochemically diverse scaffolds. Synthetic designs based on photochemical transformations have the potential to afford complex polycyclic carbon skeletons with impressive efficiency, which are of high value in total synthesis."
JOHN A PORCO,Isolation and synthesis of novel meroterpenoids from rhodomyrtus tomentosa: investigation of a reactive enetrione intermediate,"Rhodomyrtusials A-C, the first examples of triketone-sesquiterpene meroterpenoids featuring a unique 6/5/5/9/4 fused pentacyclic ring system were isolated from Rhodomyrtus tomentosa, along with several biogenetically-related dihydropyran isomers. Two bis-furans and one dihydropyran isomer showed acetylcholinesterase (AChE) inhibitory activity. Structures of the isolates were unambiguously established by a combination of spectroscopic data, ECD analysis, and total synthesis. Bioinspired total syntheses of six isolates were achieved in six steps utilizing a reactive enetrione intermediate generated in situ from a readily available hydroxy-endoperoxide precursor."
JOHN A PORCO,Small molecule amyloid-beta protein precursor processing modulators lower amyloid-beta peptide levels via cKit signaling,"Alzheimer’s disease (AD) is characterized by the accumulation of neurotoxic amyloid-β (Aβ) peptides consisting of 39-43 amino acids, proteolytically derived fragments of the amyloid-β protein precursor (AβPP), and the accumulation of the hyperphosphorylated microtubule-associated protein tau. Inhibiting Aβ production may reduce neurodegeneration and cognitive dysfunction associated with AD. We have previously used an AβPP-firefly luciferase enzyme complementation assay to conduct a high throughput screen of a compound library for inhibitors of AβPP dimerization, and identified a compound that reduces Aβ levels. In the present study, we have identified an analog, compound Y10, which also reduced Aβ. Initial kinase profiling assays identified the receptor tyrosine kinase cKit as a putative Y10 target. To elucidate the precise mechanism involved, AβPP phosphorylation was examined by IP-western blotting. We found that Y10 inhibits cKit phosphorylation and increases AβPP phosphorylation mainly on tyrosine residue Y743, according to AβPP751 numbering. A known cKit inhibitor and siRNA specific to cKit were also found to increase AβPP phosphorylation and lower Aβ levels. We also investigated a cKit downstream signaling molecule, the Shp2 phosphatase, and found that known Shp2 inhibitors and siRNA specific to Shp2 also increase AβPP phosphorylation, suggesting that the cKit signaling pathway is also involved in AβPP phosphorylation and Aβ production. We further found that inhibitors of both cKit and Shp2 enhance AβPP surface localization. Thus, regulation of AβPP phosphorylation by small molecules should be considered as a novel therapeutic intervention for AD."
JOHN A PORCO,Targeting translation initiation by synthetic rocaglates for treating MYC-driven lymphomas.,"MYC-driven lymphomas, especially those with concurrent MYC and BCL2 dysregulation, are currently a challenge in clinical practice due to rapid disease progression, resistance to standard chemotherapy, and high risk of refractory disease. MYC plays a central role by coordinating hyperactive protein synthesis with upregulated transcription in order to support rapid proliferation of tumor cells. Translation initiation inhibitor rocaglates have been identified as the most potent drugs in MYC-driven lymphomas as they efficiently inhibit MYC expression and tumor cell viability. We found that this class of compounds can overcome eIF4A abundance by stabilizing target mRNA-eIF4A interaction that directly prevents translation. Proteome-wide quantification demonstrated selective repression of multiple critical oncoproteins in addition to MYC in B-cell lymphoma including NEK2, MCL1, AURKA, PLK1, and several transcription factors that are generally considered undruggable. Finally, (-)-SDS-1-021, the most promising synthetic rocaglate, was confirmed to be highly potent as a single agent, and displayed significant synergy with the BCL2 inhibitor ABT199 in inhibiting tumor growth and survival in primary lymphoma cells in vitro and in patient-derived xenograft mouse models. Overall, our findings support the strategy of using rocaglates to target oncoprotein synthesis in MYC-driven lymphomas."
JOHN A PORCO,Intercepted retro-Nazarov reaction: syntheses of amidino-rocaglate derivatives and their biological evaluation as eIF4A inhibitors,"Rocaglates are a family of natural products isolated from the genus Aglaia which possess a highly substituted cyclopenta[b]benzofuran skeleton and inhibit cap-dependent protein synthesis. Rocaglates are attractive compounds due to their potential for inhibiting tumor cell maintenance in vivo by specifically targeting eukaryotic initiation factor 4A (eIF4A) and interfering with recruitment of ribosomes to mRNA. In this paper, we describe an intercepted retro-Nazarov reaction utilizing intramolecular tosyl migration to generate a reactive oxyallyl cation on the rocaglate skeleton. Trapping of the oxyallyl cation with a diverse range of nucleophiles has been used to generate over 50 novel amidino-rocaglate (ADR) and amino-rocaglate derivatives. Subsequently, these derivatives were evaluated for their ability to inhibit cap-dependent protein synthesis where they were found to outperform previous lead compounds including the rocaglate hydroxamate CR-1-31-B."
JOHN A PORCO,Amidino-rocaglates: a potent class of eIF4A inhibitors,"Rocaglates share a common cyclopenta[b]benzofuran core that inhibits eukaryotic translation initiation by modifying the behavior of the RNA helicase, eIF4A. Working as interfacial inhibitors, rocaglates stabilize the association between eIF4A and RNA, which can lead to the formation of steric barriers that block initiating ribosomes. There is significant interest in the development and expansion of rocaglate derivatives, as several members of this family have been shown to possess potent anti-neoplastic activity in vitro and in vivo. To further our understanding of rocaglate diversity and drug design, herein we explore the RNA clamping activity of >200 unique rocaglate derivatives. Through this, we report on the identification and characterization of a potent class of synthetic rocaglates called amidino-rocaglates. These compounds are among the most potent rocaglates documented to date and, taken together, this work offers important information that will guide the future design of rocaglates with improved biological properties."
JOHN A PORCO,"Canvass: a crowd-sourced, natural-product screening library for exploring biological space",
JOHN A PORCO,Synthesis of neocannabinoids using controlled friedel-crafts reactions,"A one-step transformation to produce 8,9-dihydrocannabidiol (H2CBD) and related ""neocannabinoids"" via controlled Friedel-Crafts reactions is reported. Experimental and computational studies probing the mechanism of neocannabinoid synthesis from cyclic allylic alcohol and substituted resorcinol reaction partners provide understanding of the kinetic and thermodynamic factors driving regioselectivity for the reaction. Herein, we present the reaction scope for neocannabinoid synthesis including the production of both normal and abnormal isomers under both kinetic and thermodynamic control. Discovery and optimization of this one-step protocol between various allylic alcohols and resorcinol derivatives are discussed and supported with density functional theory calculations."
JOHN A PORCO,BRCA mutational status shapes the stromal microenvironment of pancreatic cancer linking clusterin expression in cancer associated fibroblasts with HSF1 signaling,"Tumors initiate by mutations in cancer cells, and progress through interactions of the cancer cells with non-malignant cells of the tumor microenvironment. Major players in the tumor microenvironment are cancer-associated fibroblasts (CAFs), which support tumor malignancy, and comprise up to 90% of the tumor mass in pancreatic cancer. CAFs are transcriptionally rewired by cancer cells. Whether this rewiring is differentially affected by different mutations in cancer cells is largely unknown. Here we address this question by dissecting the stromal landscape of BRCA-mutated and BRCA Wild-type pancreatic ductal adenocarcinoma. We comprehensively analyze pancreatic cancer samples from 42 patients, revealing different CAF subtype compositions in germline BRCA-mutated vs. BRCA Wild-type tumors. In particular, we detect an increase in a subset of immune-regulatory clusterin-positive CAFs in BRCA-mutated tumors. Using cancer organoids and mouse models we show that this process is mediated through activation of heat-shock factor 1, the transcriptional regulator of clusterin. Our findings unravel a dimension of stromal heterogeneity influenced by germline mutations in cancer cells, with direct implications for clinical research."
JOHN A PORCO,Inhibition of the translation initiation factor eIF4A enhances tumor cell radiosensitivity,"A fundamental component of cellular radioresponse is the translational control of gene expression. Because a critical regulator of translational control is the eukaryotic translation initiation factor 4F (eIF4F) cap binding complex, we investigated whether eIF4A, the RNA helicase component of eIF4F, can serve as a target for radiosensitization. Knockdown of eIF4A using siRNA reduced translational efficiency, as determined from polysome profiles, and enhanced tumor cell radiosensitivity as determined by clonogenic survival. The increased radiosensitivity was accompanied by a delayed dispersion of radiation-induced γH2AX foci, suggestive of an inhibition of DNA double-strand break repair. Studies were then extended to (-)-SDS-1-021, a pharmacologic inhibitor of eIF4A. Treatment of cells with the rocaglate (-)-SDS-1-021 resulted in a decrease in translational efficiency as well as protein synthesis. (-)-SDS-1-021 treatment also enhanced the radiosensitivity of tumor cell lines. This (-)-SDS-1-021-induced radiosensitization was accompanied by a delay in radiation-induced γH2AX foci dispersal, consistent with a causative role for the inhibition of double-strand break repair. In contrast, although (-)-SDS-1-021 inhibited translation and protein synthesis in a normal fibroblast cell line, it had no effect on radiosensitivity of normal cells. Subcutaneous xenografts were then used to evaluate the in vivo response to (-)-SDS-1-021 and radiation. Treatment of mice bearing subcutaneous xenografts with (-)-SDS-1-021 decreased tumor translational efficiency as determined by polysome profiles. Although (-)-SDS-1-021 treatment alone had no effect on tumor growth, it significantly enhanced the radiation-induced growth delay. These results suggest that eIF4A is a tumor-selective target for radiosensitization."
JOHN A PORCO,Identification of structurally re-engineered rocaglates as inhibitors against hepatitis E virus replication,"Hepatitis E virus (HEV) infections are a leading cause of acute viral hepatitis in humans and pose a considerable threat to public health. Current standard of care treatment is limited to the off-label use of nucleoside-analog ribavirin (RBV) and PEGylated interferon-α, both of which are associated with significant side effects and provide limited efficacy. In the past few years, a promising natural product compound class of eukaryotic initiation factor 4A (eIF4A) inhibitors (translation initiation inhibitors), called rocaglates, were identified as antiviral agents against RNA virus infections. In the present study, we evaluated a total of 205 synthetic rocaglate derivatives from the BU-CMD compound library for their antiviral properties against HEV. At least eleven compounds showed inhibitory activities against the HEV genotype 3 (HEV-3) subgenomic replicon below 30 nM (EC50 value) as determined by Gaussia luciferase assay. Three amidino-rocaglates (ADRs) (CMLD012073, CMLD012118, and CMLD012612) possessed antiviral activity against HEV with EC50 values between 1 and 9 nM. In addition, these three selected compounds inhibited subgenomic replicons of different genotypes (HEV-1 [Sar55], wild boar HEV-3 [83-2] and human HEV-3 [p6]) in a dose-dependent manner and at low nanomolar concentrations. Furthermore, tested ADRs tend to be better tolerated in primary hepatocytes than hepatoma cancer cell lines and combination treatment of CMLD012118 with RBV and interferon-α (IFN-α) showed that CMLD012118 acts additive to RBV and IFN-α treatment. In conclusion, our results indicate that ADRs, especially CMLD012073, CMLD012118, and CMLD012612 may prove to be potential therapeutic candidates for the treatment of HEV infections and may contribute to the discovery of pan-genotypic inhibitors in the future."
JOHN A PORCO,"Unified, asymmetric total synthesis of the asnovolins and related spiromeroterpenoids: a fragment coupling approach","3,5-Dimethylorsellinic acid (DMOA)-derived spiromeroterpenoids are a unique natural product family with attractive structures, unconventional stereochemistry, and potent biological activities. Herein, we report the first asymmetric total syntheses of the asnovolins, DMOA-derived spiromeroterpenoids. The spirocyclic skeleton was efficiently assembled through a sterically hindered bis-neopentyl 1,2-addition coupling/oxidative Michael addition sequence. The unusual axial C12-methyl stereochemistry was established via metal hydrogen atom transfer (MHAT) reduction involving a chair-to-boat conformational change. The mechanism of the HAT process was studied through both deuterium labeling and computational studies. Attempted late-stage alkene isomerization of an exocyclic enone proved to be challenging and resulted in hetero-Diels-Alder dimerization, which ultimately led to development of an alternative desaturation/coupling sequence. Endgame core modifications including orthogonal desaturation, Sc(III)-promoted regioselective Baeyer-Villiger oxidation, and Meerwein-Ponndorf-Verley reduction enabled collective syntheses of five asnovolin-related natural products. This study demonstrates the utility of anionic fragment coupling to assemble a sterically congested molecular framework and provides a foundation for the synthesis of spiromeroterpenoid congeners with higher oxidation states for biological studies."
JOHN A PORCO,Structural basis for species-selective targeting of Hsp90 in a pathogenic fungus,
JOHN A PORCO,Oxo-aglaiastatin-mediated inhibition of translation initiation,
JOHN A PORCO,Asymmetric synthesis of griffipavixanthone employing a chiral phosphoric acid-catalyzed cycloaddition,"Asymmetric synthesis of the biologically active xanthone dimer griffipavixanthone is reported along with its absolute stereochemistry determination. Synthesis of the natural product is accomplished via dimerization of a p-quinone methide using a chiral phosphoric acid catalyst to afford a protected precursor in excellent diastereo- and enantioselectivity. Mechanistic studies, including an unbiased computational investigation of chiral ion-pairs using parallel tempering, were performed in order to probe the mode of asymmetric induction."
JOHN A PORCO,Discovery of macrocyclic inhibitors of apurinic/apyrimidinic endonuclease 1,"Apurinic/apyrimidinic endonuclease 1 (APE1) is an essential base excision repair enzyme that is upregulated in a number of cancers, contributes to resistance of tumors treated with DNA-alkylating or -oxidizing agents, and has recently been identified as an important therapeutic target. In this work, we identified hot spots for binding of small organic molecules experimentally in high resolution crystal structures of APE1 and computationally through the use of FTMAP analysis (http://ftmap.bu.edu). Guided by these hot spots, a library of drug-like macrocycles was docked and then screened for inhibition of APE1 endonuclease activity. In an iterative process, hot-spot-guided docking, characterization of inhibition of APE1 endonuclease, and cytotoxicity of cancer cells were used to design next generation macrocycles. To assess target selectivity in cells, selected macrocycles were analyzed for modulation of DNA damage. Taken together, our studies suggest that macrocycles represent a promising class of compounds for inhibition of APE1 in cancer cells."
JOHN A PORCO,Asymmetric synthesis of gonytolide A: strategic use of an aryl halide blocking group for oxidative coupling,"The first synthesis of the chromanone lactone dimer gonytolide A has been achieved employing vanadium(V)-mediated oxidative coupling of the monomer gonytolide C. An o-bromine blocking group strategy was employed to favor para- para coupling and to enable kinetic resolution of (±)-gonytolide C. Asymmetric conjugate reduction enabled practical kinetic resolution of a chiral, racemic precursor and the asymmetric synthesis of (+)-gonytolide A and its atropisomer."
JOHN A PORCO,Channeling macrophage polarization by rocaglates increases macrophage resistance to Mycobacterium tuberculosis,"Macrophages contribute to host immunity and tissue homeostasis via alternative activation programs. M1-like macrophages control intracellular bacterial pathogens and tumor progression. In contrast, M2-like macrophages shape reparative microenvironments that can be conducive for pathogen survival or tumor growth. An imbalance of these macrophages phenotypes may perpetuate sites of chronic unresolved inflammation, such as infectious granulomas and solid tumors. We have found that plant-derived and synthetic rocaglates sensitize macrophages to low concentrations of the M1-inducing cytokine IFN-gamma and inhibit their responsiveness to IL-4, a prototypical activator of the M2-like phenotype. Treatment of primary macrophages with rocaglates enhanced phagosome-lysosome fusion and control of intracellular mycobacteria. Thus, rocaglates represent a novel class of immunomodulators that can direct macrophage polarization toward the M1-like phenotype in complex microenvironments associated with hypofunction of type 1 and/or hyperactivation of type 2 immunity, e.g., chronic bacterial infections, allergies, and, possibly, certain tumors."
JOHN A PORCO,Eukaryotic translation initiation factor 4AI: a potential novel target in neuroblastoma,"Neuroblastoma (NB) is the most common extracranial pediatric solid tumor. Children suffering from high-risk and/or metastatic NB often show no response to therapy, and new therapeutic approaches are urgently needed. Malignant tumor development has been shown to be driven by the dysregulation of eukaryotic initiation factors (eIFs) at the translation initiation. Especially the activity of the heterotrimeric eIF4F complex is often altered in malignant cells, since it is the direct connection to key oncogenic signaling pathways such as the PI3K/AKT/mTOR-pathway. A large body of literature exists that demonstrates targeting the translational machinery as a promising anti-neoplastic approach. The objective of this study was to determine whether eIF4F complex members are aberrantly expressed in NB and whether targeting parts of the complex may be a therapeutic strategy against NB. We show that eIF4AI is overexpressed in NB patient tissue using immunohistochemistry, immunoblotting, and RT-qPCR. NB cell lines exhibit decreased viability, increased apoptosis rates as well as changes in cell cycle distribution when treated with the synthetic rocaglate CR-1-31-B, which clamps eIF4A and eIF4F onto mRNA, resulting in a translational block. Additionally, this study reveals that CR-1-31-B is effective against NB cell lines at low nanomolar doses (≤20 nM), which have been shown to not affect non-malignant cells in previous studies. Thus, our study provides information of the expression status on eIF4AI in NB and offers initial promising insight into targeting translation initiation as an anti-tumorigenic approach for NB."
JOHN A PORCO,An oxindole efflux inhibitor potentiates azoles and impairs virulence in the fungal pathogen Candida auris,"Candida auris is an emerging fungal pathogen that exhibits resistance to multiple drugs, including the most commonly prescribed antifungal, fluconazole. Here, we use a combinatorial screening approach to identify a bis-benzodioxolylindolinone (azoffluxin) that synergizes with fluconazole against C. auris. Azoffluxin enhances fluconazole activity through the inhibition of efflux pump Cdr1, thus increasing intracellular fluconazole levels. This activity is conserved across most C. auris clades, with the exception of clade III. Azoffluxin also inhibits efflux in highly azole-resistant strains of Candida albicans, another human fungal pathogen, increasing their susceptibility to fluconazole. Furthermore, azoffluxin enhances fluconazole activity in mice infected with C. auris, reducing fungal burden. Our findings suggest that pharmacologically targeting Cdr1 in combination with azoles may be an effective strategy to control infection caused by azole-resistant isolates of C. auris."
JOHN A PORCO,Heat Shock Factor 1-dependent extracellular matrix remodeling mediates the transition from chronic intestinal inflammation to colon cancer,"In the colon, long-term exposure to chronic inflammation drives colitis-associated colon cancer (CAC) in patients with inflammatory bowel disease. While the causal and clinical links are well established, molecular understanding of how chronic inflammation leads to the development of colon cancer is lacking. Here we deconstruct the evolving microenvironment of CAC by measuring proteomic changes and extracellular matrix (ECM) organization over time in a mouse model of CAC. We detect early changes in ECM structure and composition, and report a crucial role for the transcriptional regulator heat shock factor 1 (HSF1) in orchestrating these events. Loss of HSF1 abrogates ECM assembly by colon fibroblasts in cell-culture, prevents inflammation-induced ECM remodeling in mice and inhibits progression to CAC. Establishing relevance to human disease, we find high activation of stromal HSF1 in CAC patients, and detect the HSF1-dependent proteomic ECM signature in human colorectal cancer. Thus, HSF1-dependent ECM remodeling plays a crucial role in mediating inflammation-driven colon cancer."
JOHN A PORCO,Synthesis and multiplexed activity profiling of synthetic acylphloroglucinol scaffolds,"Reported here are novel formic-acid-mediated rearrangements of dearomatized acylphloroglucinols to access a structurally diverse group of synthetic acylphloroglucinol scaffolds (SASs). Density-functional theory (DFT) optimized orbital and stereochemical analyses shed light on the mechanism of these rearrangements. Products were evaluated by multiplexed activity profiling (MAP), an unbiased platform which assays multiple biological readouts simultaneously at single-cell resolution for markers of cell signaling, and can aid in distinguishing genuine activity from assay interference. MAP identified a number of SASs that suppressed pS6 (Ser235/236), a marker for activation of the mTOR and ERK signaling pathways. These results illustrate how biomimetic synthesis and multiplexed activity profiling can reveal the pharmacological potential of novel chemotypes by diversity-oriented synthesis."
JOHN A PORCO,How proteins bind macrocycles,"The potential utility of synthetic macrocycles (MCs) as drugs, particularly against low-druggability targets such as protein-protein interactions, has been widely discussed. There is little information, however, to guide the design of MCs for good target protein-binding activity or bioavailability. To address this knowledge gap, we analyze the binding modes of a representative set of MC-protein complexes. The results, combined with consideration of the physicochemical properties of approved macrocyclic drugs, allow us to propose specific guidelines for the design of synthetic MC libraries with structural and physicochemical features likely to favor strong binding to protein targets as well as good bioavailability. We additionally provide evidence that large, natural product-derived MCs can bind targets that are not druggable by conventional, drug-like compounds, supporting the notion that natural product-inspired synthetic MCs can expand the number of proteins that are druggable by synthetic small molecules."
JOHN A PORCO,Biomimetic kinetic resolution: highly enantio- and diastereoselective transfer hydrogenation of aglain ketones to access flavagline natural products,"We have previously reported asymmetric syntheses and absolute configuration assignments of the aglains (+)-ponapensin and (+)-elliptifoline and proposed a biosynthetic kinetic resolution process to produce enantiomeric rocaglamides and aglains. Herein, we report a biomimetic approach for the synthesis of enantiomerically enriched aglains and rocaglamides via kinetic resolution of a bridged ketone utilizing enantioselective transfer hydrogenation. The methodology has been employed to synthesize and confirm the absolute stereochemistries of the pyrimidone rocaglamides (+)-aglaiastatin and (-)-aglaroxin C. Additionally, the enantiomers and racemate of each metabolite were assayed for inhibition of the heat-shock response, cytotoxicity, and translation inhibition."
JOHN A PORCO,Translation inhibition by rocaglates is independent of eIF4E phosphorylation status,"Rocaglates are natural products that inhibit protein synthesis in eukaryotes and exhibit antineoplastic activity. In vitro biochemical assays, affinity chromatography experiments coupled with mass spectrometry analysis, and in vivo genetic screens have identified eukaryotic initiation factor (eIF) 4A as a direct molecular target of rocaglates. eIF4A is the RNA helicase subunit of eIF4F, a complex that mediates cap-dependent ribosome recruitment to mRNA templates. The eIF4F complex has been implicated in tumor initiation and maintenance through elevated levels or increased phosphorylation status of its cap-binding subunit, eIF4E, thus furthering the interest toward developing rocaglates as antineoplastic agents. Recent experiments have indicated that rocaglates also interact with prohibitins 1 and 2, proteins implicated in c-Raf-MEK-ERK signaling. Because increased ERK signaling stimulates eIF4E phosphorylation status, rocaglates are also expected to inhibit eIF4E phosphorylation status, a point that has not been thoroughly investigated. It is currently unknown whether the effects on translation observed with rocaglates are solely through eIF4A inhibition or also a feature of blocking eIF4E phosphorylation. Here, we show that rocaglates inhibit translation through an eIF4E phosphorylation-independent mechanism."
JOHN A PORCO,Gold(I)-mediated cycloisomerization/cycloaddition enables bioinspired syntheses of neonectrolides B-E and analogues,Development of a synthetic route to the oxaphenalenone (OP) natural products neonectrolides B-E is described. The synthesis relies on gold-catalyzed 6-endo-dig hydroarylation of an unusual enynol substrate as well as a one-pot Rieche formylation/cyclization/deprotection sequence to efficiently construct the tricyclic oxaphenalenone framework in the form of a masked ortho-quinone methide (o-QM). A tandem cycloisomerization/[4 + 2] cycloaddition strategy was employed to quickly construct molecules resembling the neonectrolides. The tricyclic OP natural product SF226 could be converted to corymbiferan lactone E and a related masked o-QM. Our study culminates with the application of the tandem reaction sequence to syntheses of neonectrolides B-E as well as previously unreported exo-diastereomers.
JOHN A PORCO,Acylphloroglucinols with acetylcholinesterase inhibitory effects from the fruits of Eucalyptus robusta,"Eleven new acylphloroglucinols, including six new formylated phloroglucinol-monoterpene meroterpenoids, eucalyprobusals A-F (1-6), one monomeric acylphloroglucinol, eucalyprobusone B (7), and four dimeric acylphloroglucinols, eucalyprobusones C-F (8-11) were purified from the fruits of Eucalyptus robusta. The establishment of the structures of 1-11 was achieved by a combination of NMR and HRESIMS data analyses, electron circular dichroism (ECD), and single-crystal X-ray diffraction. Compounds 6, 8, and an inseparable mixture of 10 and 11 were found to be potent AChE inhibitors with IC50 values of 3.22 ± 0.36, 3.82 ± 0.22, and 2.55 ± 0.28 μΜ, respectively. Possible interaction sites of 6, 8, 10, and 11 with AChE were investigated by means of molecular docking studies, and the results revealed that AChE residues Asn87, Ser125, Thr83, Tyr133, Tyr124, Tyr337, and Tyr341 played crucial roles in the observed activity of the aforementioned compounds."
JOHN A PORCO,Biomimetic synthesis of meroterpenoids by dearomatization-driven polycyclization,"A biomimetic route to farnesyl pyrophosphate and dimethyl orsellinic acid (DMOA)-derived meroterpenoid scaffolds has yet to be reported despite great interest from the chemistry and biomedical research communities. A concise synthetic route with the potential to access DMOA-derived meroterpenoids is highly desirable to create a library of related compounds. Herein, we report novel dearomatization methodology followed by polyene cyclization to access DMOA-derived meroterpenoid frameworks in six steps from commercially available starting materials. Furthermore, several farnesyl alkene substrates were used to generate structurally novel, DMOA-derived meroterpenoid derivatives. DFT calculations combined with experimentation provided a rationale for the observed thermodynamic distribution of polycyclization products."
JOHN A PORCO,Drug-induced stress granule formation protects sensory hair cells in mouse cochlear explants during ototoxicity,"Stress granules regulate RNA translation during cellular stress, a mechanism that is generally presumed to be protective, since stress granule dysregulation caused by mutation or ageing is associated with neurodegenerative disease. Here, we investigate whether pharmacological manipulation of the stress granule pathway in the auditory organ, the cochlea, affects the survival of sensory hair cells during aminoglycoside ototoxicity, a common cause of acquired hearing loss. We show that hydroxamate (-)-9, a silvestrol analogue that inhibits eIF4A, induces stress granule formation in both an auditory cell line and ex-vivo cochlear cultures and that it prevents ototoxin-induced hair-cell death. In contrast, preventing stress granule formation using the small molecule inhibitor ISRIB increases hair-cell death. Furthermore, we provide the first evidence of stress granule formation in mammalian hair cells in-vivo triggered by aminoglycoside treatment. Our results demonstrate that pharmacological induction of stress granules enhances cell survival in native-tissue, in a clinically-relevant context. This establishes stress granules as a viable therapeutic target not only for hearing loss but also other neurodegenerative diseases."
JOHN A PORCO,Translation inhibition by rocaglates activates a species-specific cell death program in the emerging fungal pathogen Candida auris,"Fungal infections are a major contributor to infectious disease-related deaths worldwide. Recently, global emergence of the fungal pathogen Candida auris has caused considerable concern because most C. auris isolates are resistant to fluconazole, the most commonly administered antifungal, and some isolates are resistant to drugs from all three major antifungal classes. To identify novel agents with bioactivity against C. auris, we screened 2,454 compounds from a diversity-oriented synthesis collection. Of the five hits identified, most shared a common rocaglate core structure and displayed fungicidal activity against C. auris These rocaglate hits inhibited translation in C. auris but not in its pathogenic relative Candida albicans Species specificity was contingent on variation at a single amino acid residue in Tif1, a fungal member of the eukaryotic initiation factor 4A (eIF4A) family of translation initiation factors known to be targeted by rocaglates. Rocaglate-mediated inhibition of translation in C. auris activated a cell death program characterized by loss of mitochondrial membrane potential, increased caspase-like activity, and disrupted vacuolar homeostasis. In a rocaglate-sensitized C. albicans mutant engineered to express translation initiation factor 1 (Tif1) with the variant amino acid that we had identified in C. auris, translation was inhibited but no programmed cell death phenotypes were observed. This surprising finding suggests divergence between these related fungal pathogens in their pathways of cellular responses to translation inhibition. From a therapeutic perspective, the chemical biology that we have uncovered reveals species-specific vulnerability in C. auris and identifies a promising target for development of new, mechanistically distinct antifungals in the battle against this emerging pathogen. IMPORTANCE Emergence of the fungal pathogen Candida auris has ignited intrigue and alarm within the medical community and the public at large. This pathogen is unusually resistant to antifungals, threatening to overwhelm current management options. By screening a library of structurally diverse molecules, we found that C. auris is surprisingly sensitive to translation inhibition by a class of compounds known as rocaglates (also known as flavaglines). Despite the high level of conservation across fungi in their protein synthesis machinery, these compounds inhibited translation initiation and activated a cell death program in C. auris but not in its relative Candida albicans Our findings highlight a surprising divergence across the cell death programs operating in Candida species and underscore the need to understand the specific biology of a pathogen in attempting to develop more-effective treatments against it."
JOHN A PORCO,eIF4A inhibitors suppress cell-cycle feedback response and acquired resistance to CDK4/6 inhibition in cancer,"CDK4/6 inhibitors are FDA-approved drugs for estrogen receptor-positive (ER+) breast cancer and are being evaluated to treat other tumor types, including KRAS-mutant non-small cell lung cancer (NSCLC). However, their clinical utility is often limited by drug resistance. Here, we sought to better understand the resistant mechanisms and help devise potential strategies to overcome this challenge. We show that treatment with CDK4/6 inhibitors in both ER+ breast cancer and KRAS-mutant NSCLC cells induces feedback upregulation of cyclin D1, CDK4, and cyclin E1, mediating drug resistance. We demonstrate that rocaglates, which preferentially target translation of key cell-cycle regulators, effectively suppress this feedback upregulation induced by CDK4/6 inhibition. Consequently, combination treatment of CDK4/6 inhibitor palbociclib with the eukaryotic initiation factor (eIF) 4A inhibitor, CR-1-31-B, is synergistic in suppressing the growth of these cancer cells in vitro and in vivo Furthermore, ER+ breast cancer and KRAS-mutant NSCLC cells that acquired resistance to palbociclib after chronic drug exposure are also highly sensitive to this combination treatment strategy. Our findings reveal a novel strategy using eIF4A inhibitors to suppress cell-cycle feedback response and to overcome resistance to CDK4/6 inhibition in cancer."
JOHN A PORCO,Remodeling natural products: chemistry and serine hydrolase activity of a rocaglate-derived β-lactone,"Flavaglines are a class of natural products with potent insecticidal and anticancer activities. β-Lactones are a privileged structural motif found in both therapeutic agents and chemical probes. Herein, we report the synthesis, unexpected light-driven di-epimerization, and activity-based protein profiling of a novel rocaglate-derived β-lactone. In addition to in vitro inhibition of the serine hydrolases ABHD10 and ACOT1/2, the most potent β-lactone enantiomer was also found to inhibit these enzymes, as well as the serine peptidases CTSA and SCPEP1, in PC3 cells."
JOHN A PORCO,Rapid synthesis of polyprenylated acylphloroglucinol analogs via dearomative conjunctive allylic annulation,"Polyprenylated acylphloroglucinols (PPAPs) are structurally complex natural products with promising biological activities. Herein, we present a biosynthesis-inspired, diversity-oriented synthesis approach for rapid construction of PPAP analogs via double decarboxylative allylation (DcA) of acylphloroglucinol scaffolds to access allyl-desoxyhumulones followed by dearomative conjunctive allylic alkylation (DCAA)."
JOHN A PORCO,Exploiting the potential of meroterpenoid cyclases to expand the chemical space of fungal meroterpenoids,"Fungal meroterpenoids are a diverse group of hybrid natural products with impressive structural complexity and high potential as drug candidates. In this work, we evaluate the promiscuity of the early structure diversity-generating step in fungal meroterpenoid biosynthetic pathways: the multibond-forming polyene cyclizations catalyzed by the yet poorly understood family of fungal meroterpenoid cyclases. In total, 12 unnatural meroterpenoids were accessed chemoenzymatically using synthetic substrates. Their complex structures were determined by 2D NMR studies as well as crystalline-sponge-based X-ray diffraction analyses. The results obtained revealed a high degree of enzyme promiscuity and experimental results, together with quantum chemical calculations provided a deeper insight into the catalytic activity of this new family of non-canonical terpene cyclases. The knowledge obtained paves the way to design and engineer artificial pathways towards second generation meroterpenoids with valuable bioactivities based on combinatorial biosynthetic strategies."
JOHN A PORCO,A forward genetic screen identifies modifiers of rocaglate responsiveness,"Rocaglates are a class of eukaryotic translation initiation inhibitors that are being explored as chemotherapeutic agents. They function by targeting eukaryotic initiation factor (eIF) 4A, an RNA helicase critical for recruitment of the 40S ribosome (and associated factors) to mRNA templates. Rocaglates perturb eIF4A activity by imparting a gain-of-function activity to eIF4A and mediating clamping to RNA. To appreciate how rocaglates could best be enabled in the clinic, an understanding of resistance mechanisms is important, as this could inform on strategies to bypass such events as well as identify responsive tumor types. Here, we report on the results of a positive selection, ORFeome screen aimed at identifying cDNAs capable of conferring resistance to rocaglates. Two of the most potent modifiers of rocaglate response identified were the transcription factors FOXP3 and NR1I3, both of which have been implicated in ABCB1 regulation-the gene encoding P-glycoprotein (Pgp). Pgp has previously been implicated in conferring resistance to silvestrol, a naturally occurring rocaglate, and we show here that this extends to additional synthetic rocaglate derivatives. In addition, FOXP3 and NR1I3 impart a multi-drug resistant phenotype that is reversed upon inhibition of Pgp, suggesting a potential therapeutic combination strategy."
JOHN A PORCO,"Divergent, C-C bond forming macrocyclizations using modular sulfonylhydrazone and derived substrates","A divergent approach to C-C bond forming macrocycle construction is described. Modular sulfonylhydrazone and derived pyridotriazole substrates with three key building blocks have been constructed and cyclized to afford diverse macrocyclic frameworks. Broad substrate scope and functional group tolerance have been demonstrated. In addition, site-selective postfunctionalization allowed for further diversification of macrocyclic cores."
JOHN A PORCO,EIF4A supports an oncogenic translation program in pancreatic ductal adenocarcinoma.,"Pancreatic ductal adenocarcinoma (PDA) is a lethal malignancy with limited treatment options. Although metabolic reprogramming is a hallmark of many cancers, including PDA, previous attempts to target metabolic changes therapeutically have been stymied by drug toxicity and tumour cell plasticity. Here, we show that PDA cells engage an eIF4F-dependent translation program that supports redox and central carbon metabolism. Inhibition of the eIF4F subunit, eIF4A, using the synthetic rocaglate CR-1-31-B (CR-31) reduced the viability of PDA organoids relative to their normal counterparts. In vivo, CR-31 suppresses tumour growth and extends survival of genetically-engineered murine models of PDA. Surprisingly, inhibition of eIF4A also induces glutamine reductive carboxylation. As a consequence, combined targeting of eIF4A and glutaminase activity more effectively inhibits PDA cell growth both in vitro and in vivo. Overall, our work demonstrates the importance of eIF4A in translational control of pancreatic tumour metabolism and as a therapeutic target against PDA."
JOHN A PORCO,"Antitumor Activity and Mechanism of Action of the Cyclopenta[b]benzofuran, Silvestrol","BACKGROUND. Flavaglines are a family of natural products from the genus Aglaia that exhibit anti-cancer activity in vitro and in vivo and inhibit translation initiation. They have been shown to modulate the activity of eIF4A, the DEAD-box RNA helicase subunit of the eukaryotic initiation factor (eIF) 4F complex, a complex that stimulates ribosome recruitment during translation initiation. One flavagline, silvestrol, is capable of modulating chemosensitivity in a mechanism-based mouse model. METHODOLOGY/PRINCIPAL FINDINGS. Among a number of flavagline family members tested herein, we find that silvestrol is the more potent translation inhibitor among these. We find that silvestrol impairs the ribosome recruitment step of translation initiation by affecting the composition of the eukaryotic initiation factor (eIF) 4F complex. We show that silvestrol exhibits significant anticancer activity in human breast and prostate cancer xenograft models, and that this is associated with increased apoptosis, decreased proliferation, and inhibition of angiogenesis. We demonstrate that targeting translation by silvestrol results in preferential inhibition of weakly initiating mRNAs. CONCLUSIONS/SIGNIFICANCE. Our results indicate that silvestrol is a potent anti-cancer compound in vivo that exerts its activity by affecting survival pathways as well as angiogenesis. We propose that silvestrol mediates its effects by preferentially inhibiting translation of malignancy-related mRNAs. Silvestrol appears to be well tolerated in animals."
JOHN A PORCO,Tracing MYC expression for small molecule discovery,"Our inability to effectively ""drug"" targets such as MYC for therapeutic purposes requires the development of new approaches. We report on the implementation of a phenotype-based assay for monitoring MYC expression in multiple myeloma cells. The open reading frame (ORF) encoding an unstable variant of GFP was engineered immediately downstream of the MYC ORF using CRISPR/Cas9, resulting in co-expression of both proteins from the endogenous MYC locus. Using fluorescence readout as a surrogate for MYC expression, we implemented a pilot screen in which ∼10,000 compounds were prosecuted. Among known MYC expression inhibitors, we identified cardiac glycosides and cytoskeletal disruptors to be quite potent. We demonstrate the power of CRISPR/Cas9 engineering in establishing phenotype-based assays to identify gene expression modulators."
JOHN A PORCO,Exploiting diverse chemical collections to uncover novel antifungals,"The rise in drug resistance amongst pathogenic fungi, paired with the limited arsenal of antifungals available is an imminent threat to our medical system. To address this, we screened two distinct compound libraries to identify novel strategies to expand the antifungal armamentarium. The first collection wasthe RIKEN Natural Product Depository (NPDepo), which was screened for antifungal activity against four major human fungal pathogens: Candida albicans, Candida glabrata, Candida auris, and Cryptococcus neoformans. Through a prioritization pipeline, one compound, NPD6433, emerged as having broad-spectrum antifungal activity and minimal mammalian cytotoxicity. Chemical-genetic and biochemical assays demonstrated that NPD6433 inhibits the essential fungal enzyme fatty acid synthase 1 (Fas1). Treatment with NPD6433 inhibited various virulence traits in C. neoformans and C. auris, and rescued mammalian cell growth in a co-culture model with C. auris. The second compound library screened was adiversity-oriented collectionfrom Boston University. This chemical screen was focused on identifying novel molecules that enhance the activity of the widely deployed antifungal, fluconazole, against C. auris. Through this endeavour, we discovered a potent compound that enhanced fluconazole efficacy against C. auris through increasing azole intracellular accumulation. This activity was dependent on expression of the multidrug transporter geneCDR1, suggesting that this compound targets efflux mechanisms. Furthermore, this molecule significantly reduced fungal burden alone and in combination with fluconazole in a murine model of C. auris disseminated infection. Overall, this work identifies novel compounds with bioactivity against fungal pathogens, revealing important biology, and paving the way for the critical development of therapeutic strategies."
JOHN A PORCO,Inhibition of translation initiation factor eIF4a inactivates heat shock factor 1 (HSF1) and exerts anti-leukemia activity in AML,"Eukaryotic initiation factor 4A (eIF4A), the enzymatic core of the eIF4F complex essential for translation initiation, plays a key role in the oncogenic reprogramming of protein synthesis, and thus is a putative therapeutic target in cancer. As important component of its anticancer activity, inhibition of translation initiation can alleviate oncogenic activation of HSF1, a stress-inducible transcription factor that enables cancer cell growth and survival. Here, we show that primary acute myeloid leukemia (AML) cells exhibit the highest transcript levels of eIF4A1 compared to other cancer types. eIF4A inhibition by the potent and specific compound rohinitib (RHT) inactivated HSF1 in these cells, and exerted pronounced in vitro and in vivo anti-leukemia effects against progenitor and leukemia-initiating cells, especially those with FLT3-internal tandem duplication (ITD). In addition to its own anti-leukemic activity, genetic knockdown of HSF1 also sensitized FLT3-mutant AML cells to clinical FLT3 inhibitors, and this synergy was conserved in FLT3 double-mutant cells carrying both ITD and tyrosine kinase domain mutations. Consistently, the combination of RHT and FLT3 inhibitors was highly synergistic in primary FLT3-mutated AML cells. Our results provide a novel therapeutic rationale for co-targeting eIF4A and FLT3 to address the clinical challenge of treating FLT3-mutant AML."
JOHN A PORCO,Single cell profiling distinguishes leukemia-selective chemotypes,"A central challenge in chemical biology is to distinguish molecular families in which small structural changes trigger large changes in cell biology. Such families might be ideal scaffolds for developing cell-selective chemical effectors - for example, molecules that activate DNA damage responses in malignant cells while sparing healthy cells. Across closely related structural variants, subtle structural changes have the potential to result in contrasting bioactivity patterns across different cell types. Here, we tested a 600-compound Diversity Set of screening molecules from the Boston University Center for Molecular Discovery (BU-CMD) in a novel phospho-flow assay that tracked fundamental cell biological processes, including DNA damage response, apoptosis, M-phase cell cycle, and protein synthesis in MV411 leukemia cells. Among the chemotypes screened, synthetic congeners of the rocaglate family were especially bioactive. In follow-up studies, 37 rocaglates were selected and deeply characterized using 12 million additional cellular measurements across MV411 leukemia cells and healthy peripheral blood mononuclear cells. Of the selected rocaglates, 92% displayed significant bioactivity in human cells, and 65% selectively induced DNA damage responses in leukemia and not healthy human blood cells. Furthermore, the signaling and cell-type selectivity were connected to structural features of rocaglate subfamilies. In particular, three rocaglates from the rocaglate pyrimidinone (RP) structural subclass were the only molecules that activated exceptional DNA damage responses in leukemia cells without activating a detectable DNA damage response in healthy cells. These results indicate that the RP subset should be extensively characterized for anticancer therapeutic potential as it relates to the DNA damage response. This single cell profiling approach advances a chemical biology platform to dissect how systematic variations in chemical structure can profoundly and differentially impact basic functions of healthy and diseased cells."
JOHN A PORCO,A chemical screen identifies structurally diverse metal chelators with activity against the fungal pathogen Candida albicans,"Candida albicans, one of the most prevalent human fungal pathogens, causes diverse diseases extending from superficial infections to deadly systemic mycoses. Currently, only three major classes of antifungal drugs are available to treat systemic infections: azoles, polyenes, and echinocandins. Alarmingly, the efficacy of these antifungals against C. albicans is hindered both by basal tolerance toward the drugs and the development of resistance mechanisms such as alterations of the drug's target, modulation of stress responses, and overexpression of efflux pumps. Thus, the need to identify novel antifungal strategies is dire. To address this challenge, we screened 3,049 structurally-diverse compounds from the Boston University Center for Molecular Discovery (BU-CMD) chemical library against a C. albicans clinical isolate and identified 17 molecules that inhibited C. albicans growth by >80% relative to controls. Among the most potent compounds were CMLD013360, CMLD012661, and CMLD012693, molecules representing two distinct chemical scaffolds, including 3-hydroxyquinolinones and a xanthone natural product. Based on structural insights, CMLD013360, CMLD012661, and CMLD012693 were hypothesized to exert antifungal activity through metal chelation. Follow-up investigations revealed all three compounds exerted antifungal activity against non-albicans Candida, including Candida auris and Candida glabrata, with the xanthone natural product CMLD013360 also displaying activity against the pathogenic mould Aspergillus fumigatus. Media supplementation with metallonutrients, namely ferric or ferrous iron, rescued C. albicans growth, confirming these compounds act as metal chelators. Thus, this work identifies and characterizes two chemical scaffolds that chelate iron to inhibit the growth of the clinically relevant fungal pathogen C. albicansIMPORTANCEThe worldwide incidence of invasive fungal infections is increasing at an alarming rate. Systemic candidiasis caused by the opportunistic pathogen Candida albicans is the most common cause of life-threatening fungal infection. However, due to the limited number of antifungal drug classes available and the rise of antifungal resistance, an urgent need exists for the identification of novel treatments. By screening a compound collection from the Boston University Center for Molecular Discovery (BU-CMD), we identified three compounds representing two distinct chemical scaffolds that displayed activity against C. albicans. Follow-up analyses confirmed these molecules were also active against other pathogenic fungal species including Candida auris and Aspergillus fumigatus. Finally, we determined that these compounds inhibit the growth of C. albicans in culture through iron chelation. Overall, this observation describes two novel chemical scaffolds with antifungal activity against diverse fungal pathogens."
JOHN A PORCO,The MYCN 5' UTR as a therapeutic target in neuroblastoma,"Tumor MYCN amplification is seen in high-risk neuroblastoma, yet direct targeting of this oncogenic transcription factor has been challenging. Here, we take advantage of the dependence of MYCN-amplified neuroblastoma cells on increased protein synthesis to inhibit the activity of eukaryotic translation initiation factor 4A1 (eIF4A1) using an amidino-rocaglate, CMLD012824. Consistent with the role of this RNA helicase in resolving structural barriers in 5' untranslated regions (UTRs), CMLD012824 increased eIF4A1 affinity for polypurine-rich 5' UTRs, including that of the MYCN and associated transcripts with critical roles in cell proliferation. CMLD012824-mediated clamping of eIF4A1 spanned the full lengths of mRNAs, while translational inhibition was mediated through 5' UTR binding in a cap-dependent and -independent manner. Finally, CMLD012824 led to growth inhibition in MYCN-amplified neuroblastoma models without generalized toxicity. Our studies highlight the key role of eIF4A1 in MYCN-amplified neuroblastoma and demonstrate the therapeutic potential of disrupting its function."
JOHN A PORCO,Proteomic discovery of RNA-protein molecular clamps using a thermal shift assay with ATP and RNA (TSAR),"Uncompetitive inhibition is an effective strategy for suppressing dysregulated enzymes and their substrates, but discovery of suitable ligands depends on often-unavailable structural knowledge and serendipity. Hence, despite surging interest in mass spectrometry-based target identification, proteomic studies of substrate-dependent target engagement remain sparse. Herein, we describe the Thermal Shift Assay with ATP and RNA (TSAR) as a template for proteome-wide discovery of substrate-dependent ligand binding. Using proteomic thermal shift assays, we show that simple biochemical additives can facilitate detection of target engagement in native cell lysates. We apply our approach to rocaglates, a family of molecules that specifically clamp RNA to eukaryotic translation initiation factor 4A (eIF4A), DEAD-box helicase 3X (DDX3X), and potentially other members of the DEAD-box (DDX) family of RNA helicases. To identify unexpected interactions, we optimized a target class-specific thermal denaturation window and evaluated ATP analog and RNA probe dependencies for key rocaglate-DDX interactions. We report novel DDX targets of the rocaglate clamping spectrum, confirm that DDX3X is a common target of several widely studied analogs, and provide structural insights into divergent DDX3X affinities between synthetic rocaglates. We independently validate novel targets of high-profile rocaglates, including the clinical candidate Zotatifin (eFT226), using limited proteolysis-mass spectrometry and fluorescence polarization experiments. Taken together, our study provides a model for screening uncompetitive inhibitors using a systematic chemical-proteomics approach to uncover actionable DDX targets, clearing a path towards characterization of novel molecular clamps and associated RNA helicase targets."
JOHN A PORCO,Asymmetric synthesis of nidulalin A and nidulaxanthone A: selective carbonyl desaturation using an oxoammonium salt,"Nidulaxanthone A is a dimeric, dihydroxanthone natural product that was isolated in 2020 from Aspergillus sp. Structurally, the compound features an unprecedented heptacyclic 6/6/6/6/6/6/6 ring system which is unusual for natural xanthone dimers. Biosynthetically, nidulaxanthone A originates from the monomer nidulalin A via stereoselective Diels-Alder dimerization. To expedite the synthesis of nidulalin A and study the proposed dimerization, we developed methodology involving the use of allyl triflate for chromone ester activation, followed by vinylogous addition, to rapidly forge the nidulalin A scaffold in a four-step sequence which also features ketone desaturation using Bobbitt's oxoammonium salt. An asymmetric synthesis of nidulalin A was achieved using acylative kinetic resolution (AKR) of chiral, racemic 2H-nidulalin A. Dimerization of enantioenriched nidulalin A to nidulaxanthone A was achieved using solvent-free, thermolytic conditions. Computational studies have been conducted to probe both the oxoammonium-mediated desaturation and (4 + 2) dimerization events."
JOHN A PORCO,Inhibiting the oncogenic translation program is an effective therapeutic strategy in multiple myeloma,"Multiple myeloma (MM) is a frequently incurable hematological cancer in which overactivity of MYC plays a central role, notably through up-regulation of ribosome biogenesis and translation. To better understand the oncogenic program driven by MYC and investigate its potential as a therapeutic target, we screened a chemically diverse small-molecule library for anti-MM activity. The most potent hits identified were rocaglate scaffold inhibitors of translation initiation. Expression profiling of MM cells revealed reversion of the oncogenic MYC-driven transcriptional program by CMLD010509, the most promising rocaglate. Proteome-wide reversion correlated with selective depletion of short-lived proteins that are key to MM growth and survival, most notably MYC, MDM2, CCND1, MAF, and MCL-1. The efficacy of CMLD010509 in mouse models of MM confirmed the therapeutic relevance of these findings in vivo and supports the feasibility of targeting the oncogenic MYC-driven translation program in MM with rocaglates."
JOHN A PORCO,Triple-dearomative photocycloaddition: a strategy to construct caged molecular frameworks,"An unprecedented caged 2H-benzo-dioxo-pentacycloundecane framework was serendipitously obtained in a single transformation via triple-dearomative photocycloaddition of chromone esters with furans. This caged structure was generated as part of an effort to access a tricyclic, oxygen-bridged intermediate enroute to the dihydroxanthone natural product nidulalin A. Reaction scope and limitations were thoroughly investigated, revealing the ability to access a multitude of synthetically challenging caged scaffolds in a two-step sequence. Photophysical studies provided key mechanistic insights on the process for formation of the novel caged scaffold."
JEFFREY SIRACUSE,"Development and assessment of a new framework for disease surveillance, prediction, and risk adjustment: the diagnostic items classification system","IMPORTANCE: Current disease risk-adjustment formulas in the US rely on diagnostic classification frameworks that predate the International Classification of Diseases, Tenth Revision, Clinical Modification (ICD-10-CM). OBJECTIVE: To develop an ICD-10-CM-based classification framework for predicting diverse health care payment, quality, and performance outcomes. DESIGN SETTING AND PARTICIPANTS: Physician teams mapped all ICD-10-CM diagnoses into 3 types of diagnostic items (DXIs): main effect DXIs that specify diseases; modifiers, such as laterality, timing, and acuity; and scaled variables, such as body mass index, gestational age, and birth weight. Every diagnosis was mapped to at least 1 DXI. Stepwise and weighted least-squares estimation predicted cost and utilization outcomes, and their performance was compared with models built on (1) the Agency for Healthcare Research and Quality Clinical Classifications Software Refined (CCSR) categories, and (2) the Health and Human Services Hierarchical Condition Categories (HHS-HCC) used in the Affordable Care Act Marketplace. Each model's performance was validated using R 2, mean absolute error, the Cumming prediction measure, and comparisons of actual to predicted outcomes by spending percentiles and by diagnostic frequency. The IBM MarketScan Commercial Claims and Encounters Database, 2016 to 2018, was used, which included privately insured, full- or partial-year eligible enrollees aged 0 to 64 years in plans with medical, drug, and mental health/substance use coverage. MAIN OUTCOMES AND MEASURES: Fourteen concurrent outcomes were predicted: overall and plan-paid health care spending (top-coded and not top-coded); enrollee out-of-pocket spending; hospital days and admissions; emergency department visits; and spending for 6 types of services. The primary outcome was annual health care spending top-coded at $250 000. RESULTS: A total of 65 901 460 person-years were split into 90% estimation/10% validation samples (n = 6 604 259). In all, 3223 DXIs were created: 2435 main effects, 772 modifiers, and 16 scaled items. Stepwise regressions predicting annual health care spending (mean [SD], $5821 [$17 653]) selected 76% of the main effect DXIs with no evidence of overfitting. Validated R 2 was 0.589 in the DXI model, 0.539 for CCSR, and 0.428 for HHS-HCC. Use of DXIs reduced underpayment for enrollees with rare (1-in-a-million) diagnoses by 83% relative to HHS-HCCs. CONCLUSIONS: In this diagnostic modeling study, the new DXI classification system showed improved predictions over existing diagnostic classification systems for all spending and utilization outcomes considered."
JEFFREY SIRACUSE,"Diagnostic category prevalence in 3 classification systems across the transition to the International Classification of Diseases, Tenth Revision, Clinical Modification","IMPORTANCE: On October 1, 2015, the US transitioned to the International Classification of Diseases, Tenth Revision, Clinical Modification (ICD-10-CM) for recording diagnoses, symptoms, and procedures. It is unknown whether this transition was associated with changes in diagnostic category prevalence based on diagnosis classification systems commonly used for payment and quality reporting. OBJECTIVE: To assess changes in diagnostic category prevalence associated with the ICD-10-CM transition. Design, Setting, and Participants: This interrupted time series analysis and cross-sectional study examined level and trend changes in diagnostic category prevalence associated with the ICD-10-CM transition and clinically reviewed a subset of diagnostic categories with changes of 20% or more. Data included insurance claim diagnoses from the IBM MarketScan Commercial Database from January 1, 2010, to December 31, 2017, for more than 18 million people aged 0 to 64 years with private insurance. Diagnoses were mapped using 3 common diagnostic classification systems: World Health Organization (WHO) disease chapters, Department of Health and Human Services Hierarchical Condition Categories (HHS-HCCs), and Agency for Healthcare Research and Quality Clinical Classification System (AHRQ-CCS). Data were analyzed from December 1, 2018, to January 21, 2020. EXPOSURES: US implementation of ICD-10-CM. Main Outcomes and Measures: Monthly rates of individuals with at least 1 diagnosis in a diagnostic classification category per 10 000 eligible members. Results: The analytic sample contained information on 2.1 billion enrollee person-months with 3.4 billion clinically assigned diagnoses; the mean (range) monthly sample size was 22.1 (18.4 to 27.1 ) million individuals. While diagnostic category prevalence changed minimally for WHO disease chapters, the ICD-10-CM transition was associated with level changes of 20% or more among 20 of 127 HHS-HCCs (15.7%) and 46 of 282 AHRQ-CCS categories (16.3%) and with trend changes of 20% or more among 12 of 127 of HHS-HCCs (9.4%) and 27 of 282 of AHRQ-CCS categories (9.6%). For HHS-HCCs, monthly rates of individuals with any acute myocardial infarction diagnosis increased 131.5% (95% CI, 124.1% to 138.8%), primarily because HHS added non-ST-segment-elevation myocardial infarction diagnoses to this category. The HHS-HCC for diabetes with chronic complications increased by 92.4% (95% CI, 84.2% to 100.5%), primarily from including new diabetes-related hypoglycemia and hyperglycemia codes, and the rate for completed pregnancy with complications decreased by 54.5% (95% CI, -58.7% to -50.2%) partly due to removing vaginal birth after cesarean delivery as a complication. CONCLUSIONS AND RELEVANCE: These findings suggest that the ICD-10-CM transition was associated with large prevalence changes for many diagnostic categories. Diagnostic classification systems developed using ICD-9-CM may need to be refined using ICD-10-CM data to avoid unintended consequences for disease surveillance, performance assessment, and risk-adjusted payments."
SUNIL SHARMA,Reading the acts and lives of performers in Mughal Persian texts,"Examining materials from early modern and contemporary North India and Pakistan, Tellings and Texts brings together seventeen first-rate papers on the relations between written and oral texts, their performance, and the musical traditions these performances have entailed. The contributions from some of the best scholars in the field cover a wide range of literary genres and social and cultural contexts across the region. The texts and practices are contextualized in relation to the broader social and political background in which they emerged, showing how religious affiliations, caste dynamics and political concerns played a role in shaping social identities as well as aesthetic sensibilities. By doing so this book sheds light into theoretical issues of more general significance, such as textual versus oral norms; the features of oral performance and improvisation; the role of the text in performance; the aesthetics and social dimension of performance; the significance of space in performance history and important considerations on repertoires of story-telling."
JAMES ANDERSON,Framework for Leadership and Training of Biosafety Level 4 Laboratory Workers,"One-sentence summary for table of contents: Training should include theoretical consideration of biocontainment principles, practical hands-on training, and mentored on-the-job experience. Construction of several new Biosafety Level 4 (BSL-4) laboratories and expansion of existing operations have created an increased international demand for well-trained staff and facility leaders. Directors of most North American BSL-4 laboratories met and agreed upon a framework for leadership and training of biocontainment research and operations staff. They agreed on essential preparation and training that includes theoretical consideration of biocontainment principles, practical hands-on training, and mentored on-the-job experiences relevant to positional responsibilities as essential preparation before a person's independent access to a BSL-4 facility. They also agreed that the BSL-4 laboratory director is the key person most responsible for ensuring that staff members are appropriately prepared for BSL-4 operations. Although standardized certification of training does not formally exist, the directors agreed that facility-specific, time-limited documentation to recognize specific skills and experiences of trained persons is needed."
JAMES ANDERSON,An Assessment of Potential Exposure and Risk from Estrogens in Drinking Water,"BACKGROUND. Detection of estrogens in the environment has raised concerns in recent years because of their potential to affect both wildlife and humans. OBJECTIVES. We compared exposures to prescribed and naturally occurring estrogens in drinking water to exposures to naturally occurring background levels of estrogens in the diet of children and adults and to four independently derived acceptable daily intakes (ADIs) to determine whether drinking water intakes are larger or smaller than dietary intake or ADIs. METHODS. We used the Pharmaceutical Assessment and Transport Evaluation (PhATE) model to predict concentrations of estrogens potentially present in drinking water. Predicted drinking water concentrations were combined with default water intake rates to estimate drinking water exposures. Predicted drinking water intakes were compared to dietary intakes and also to ADIs. We present comparisons for individual estrogens as well as combined estrogens. RESULTS. In the analysis we estimated that a child's exposures to individual prescribed estrogens in drinking water are 730-480,000 times lower (depending upon estrogen type) than exposure to background levels of naturally occurring estrogens in milk. A child's exposure to total estrogens in drinking water (prescribed and naturally occurring) is about 150 times lower than exposure from milk. Adult margins of exposure (MOEs) based on total dietary exposure are about 2 times smaller than those for children. Margins of safety (MOSs) for an adult's exposure to total prescribed estrogens in drinking water vary from about 135 to > 17,000, depending on ADI. MOSs for exposure to total estrogens in drinking water are about 2 times lower than MOSs for prescribed estrogens. Depending on the ADI that is used, MOSs for young children range from 28 to 5,120 for total estrogens (including both prescribed and naturally occurring sources) in drinking water. CONCLUSIONS. The consistently large MOEs and MOSs strongly suggest that prescribed and total estrogens that may potentially be present in drinking water in the United States are not causing adverse effects in U.S. residents, including sensitive subpopulations."
JAMES ANDERSON,Personal power and agency when dealing with interactive voice response systems and alternative modalities,"In summer 2015, we conducted an exploratory study of how people in the U.S. use and respond to robot-like systems in order to achieve their needs through mediated customer service interfaces. To understand this process, we carried out three focus groups sessions along with 50 in-depth interviews. Strikingly we found that people perceive (correctly or not) that interactive voice response customer service technology is set up to deter them from pursuing further contact. And yet, for the most part, people were unwilling to simply give up on the goals that motivated their initial contact. Consequently, they had to innovate ways to communicate with the automated systems that essentially serve as gatekeepers to their desired ends. These results have implications for communication theory and system design, especially since these systems will be increasingly presented to consumers as social media affordances evolve."
JAMES ANDERSON,Experiments in genetics and evolution on the pigeon ..,
JAMES ANDERSON,"A master list of topics and objectives for general music in grades 7, 8, and 9 and their relative importance in general education.",
JAMES ANDERSON,Thoracolumbar Injury Classification and Severity Score: A New Paradigm for the Treatment of Thoracolumbar Spine Trauma,"BACKGROUND Contemporary understanding of the biomechanics, natural history, and methods of treating thoracolumbar spine injuries continues to evolve. Current classification schemes of these injuries, however, can be either too simplified or overly complex for clinical use. METHODS The Spine Trauma Group was given a survey to identify similarities in treatment algorithms for common thoracolumbar injuries, as well as to identify characteristics of injury that played a key role in the decision-making process. RESULTS Based on the survey, the Spine Trauma Group has developed a classification system and an injury severity score (thoracolumbar injury classification and severity score, or TLICS), which may facilitate communication between physicians and serve as a guideline for treating these injuries. The classification system is based on the morphology of the injury, integrity of the posterior ligamentous complex, and neurological status of the patient. Points are assigned for each category, and the final total points suggest a possible treatment option. CONCLUSIONS The usefulness of this new system will have to be proven in future studies investigating inter- and intraobserver reliability, as well as long-term outcome studies for operative and nonoperative treatment methods."
JAMES ANDERSON,"Cosmology intertwined: a review of the particle physics, astrophysics, and cosmology associated with the cosmological tensions and anomalies",
JAMES ANDERSON,The eighteenth data release of the Sloan Digital Sky Surveys: targeting and first spectra from SDSS-V,"The eighteenth data release (DR18) of the Sloan Digital Sky Survey (SDSS) is the first one for SDSS-V, the fifth generation of the survey. SDSS-V comprises three primary scientific programs or “Mappers”: the Milky Way Mapper (MWM), the Black Hole Mapper (BHM), and the Local Volume Mapper. This data release contains extensive targeting information for the two multiobject spectroscopy programs (MWM and BHM), including input catalogs and selection functions for their numerous scientific objectives. We describe the production of the targeting databases and their calibration and scientifically focused components. DR18 also includes ∼25,000 new SDSS spectra and supplemental information for X-ray sources identified by eROSITA in its eFEDS field. We present updates to some of the SDSS software pipelines and preview changes anticipated for DR19. We also describe three value-added catalogs (VACs) based on SDSS-IV data that have been published since DR17, and one VAC based on the SDSS-V data in the eFEDS field."
JAMES ANDERSON,"Making the great transformation, November 13, 14, and 15, 2003","The conference discussants and participants analyze why transitions happen, and why they matter. Transitions are those wide-ranging changes in human organization and well being that can be convincingly attributed to a concerted set of choices that make the world that was significantly and recognizably different from the world that becomes. Transition scholars argue that that history does not just stumble along a pre-determined path, but that human ingenuity and entrepreneurship have the ability to fundamentally alter its direction. However, our ability to ‘will’ such transitions remains in doubt. These doubts cannot be removed until we have a better understanding of how transitions work."
JAMES ANDERSON,"BMQ : Boston medical quarterly: v. 10, no. 1-4",
JAMES ANDERSON,"Concussion, microvascular injury, and early tauopathy in young athletes after impact head injury and an impact concussion mouse model","The mechanisms underpinning concussion, traumatic brain injury, and chronic traumatic encephalopathy, and the relationships between these disorders, are poorly understood. We examined post-mortem brains from teenage athletes in the acute-subacute period after mild closed-head impact injury and found astrocytosis, myelinated axonopathy, microvascular injury, perivascular neuroinflammation, and phosphorylated tau protein pathology. To investigate causal mechanisms, we developed a mouse model of lateral closed-head impact injury that uses momentum transfer to induce traumatic head acceleration. Unanaesthetized mice subjected to unilateral impact exhibited abrupt onset, transient course, and rapid resolution of a concussion-like syndrome characterized by altered arousal, contralateral hemiparesis, truncal ataxia, locomotor and balance impairments, and neurobehavioural deficits. Experimental impact injury was associated with axonopathy, blood–brain barrier disruption, astrocytosis, microgliosis (with activation of triggering receptor expressed on myeloid cells, TREM2), monocyte infiltration, and phosphorylated tauopathy in cerebral cortex ipsilateral and subjacent to impact. Phosphorylated tauopathy was detected in ipsilateral axons by 24 h, bilateral axons and soma by 2 weeks, and distant cortex bilaterally at 5.5 months post-injury. Impact pathologies co-localized with serum albumin extravasation in the brain that was diagnostically detectable in living mice by dynamic contrast-enhanced MRI. These pathologies were also accompanied by early, persistent, and bilateral impairment in axonal conduction velocity in the hippocampus and defective long-term potentiation of synaptic neurotransmission in the medial prefrontal cortex, brain regions distant from acute brain injury. Surprisingly, acute neurobehavioural deficits at the time of injury did not correlate with blood–brain barrier disruption, microgliosis, neuroinflammation, phosphorylated tauopathy, or electrophysiological dysfunction. Furthermore, concussion-like deficits were observed after impact injury, but not after blast exposure under experimental conditions matched for head kinematics. Computational modelling showed that impact injury generated focal point loading on the head and seven-fold greater peak shear stress in the brain compared to blast exposure. Moreover, intracerebral shear stress peaked before onset of gross head motion. By comparison, blast induced distributed force loading on the head and diffuse, lower magnitude shear stress in the brain. We conclude that force loading mechanics at the time of injury shape acute neurobehavioural responses, structural brain damage, and neuropathological sequelae triggered by neurotrauma. These results indicate that closed-head impact injuries, independent of concussive signs, can induce traumatic brain injury as well as early pathologies and functional sequelae associated with chronic traumatic encephalopathy. These results also shed light on the origins of concussion and relationship to traumatic brain injury and its aftermath."
JAMES ANDERSON,Prospects for beyond the standard model physics searches at the deep underground neutrino experiment: DUNE collaboration,"The Deep Underground Neutrino Experiment (DUNE) will be a powerful tool for a variety of physics topics. The high-intensity proton beams provide a large neutrino flux, sampled by a near detector system consisting of a combination of capable precision detectors, and by the massive far detector system located deep underground. This configuration sets up DUNE as a machine for discovery, as it enables opportunities not only to perform precision neutrino measurements that may uncover deviations from the present three-flavor mixing paradigm, but also to discover new particles and unveil new interactions and symmetries beyond those predicted in the Standard Model (SM). Of the many potential beyond the Standard Model (BSM) topics DUNE will probe, this paper presents a selection of studies quantifying DUNE's sensitivities to sterile neutrino mixing, heavy neutral leptons, non-standard interactions, CPT symmetry violation, Lorentz invariance violation, neutrino trident production, dark matter from both beam induced and cosmogenic sources, baryon number violation, and other new physics topics that complement those at high-energy colliders and significantly extend the present reach."
JAMES ANDERSON,Identification and reconstruction of low-energy electrons in the ProtoDUNE-SP detector,
JAMES ANDERSON,The long-baseline neutrino experiment: exploring fundamental symmetries of the universe,"The preponderance of matter over antimatter in the early Universe, the dynamics of the supernova bursts that produced the heavy elements necessary for life and whether protons eventually decay --- these mysteries at the forefront of particle physics and astrophysics are key to understanding the early evolution of our Universe, its current state and its eventual fate. The Long-Baseline Neutrino Experiment (LBNE) represents an extensively developed plan for a world-class experiment dedicated to addressing these questions. LBNE is conceived around three central components: (1) a new, high-intensity neutrino source generated from a megawatt-class proton accelerator at Fermi National Accelerator Laboratory, (2) a near neutrino detector just downstream of the source, and (3) a massive liquid argon time-projection chamber deployed as a far detector deep underground at the Sanford Underground Research Facility. This facility, located at the site of the former Homestake Mine in Lead, South Dakota, is approximately 1,300 km from the neutrino source at Fermilab -- a distance (baseline) that delivers optimal sensitivity to neutrino charge-parity symmetry violation and mass ordering effects. This ambitious yet cost-effective design incorporates scalability and flexibility and can accommodate a variety of upgrades and contributions. With its exceptional combination of experimental configuration, technical capabilities, and potential for transformative discoveries, LBNE promises to be a vital facility for the field of particle physics worldwide, providing physicists from around the globe with opportunities to collaborate in a twenty to thirty year program of exciting science. In this document we provide a comprehensive overview of LBNE's scientific objectives, its place in the landscape of neutrino physics worldwide, the technologies it will incorporate and the capabilities it will possess."
JAMES ANDERSON,Scintillation light detection in the 6-m drift-length ProtoDUNE Dual Phase liquid argon TPC,"DUNE is a dual-site experiment for long-baseline neutrino oscillation studies, neutrino astrophysics and nucleon decay searches. ProtoDUNE Dual Phase (DP) is a 6  ×  6  ×  6 m 3 liquid argon time-projection-chamber (LArTPC) that recorded cosmic-muon data at the CERN Neutrino Platform in 2019-2020 as a prototype of the DUNE Far Detector. Charged particles propagating through the LArTPC produce ionization and scintillation light. The scintillation light signal in these detectors can provide the trigger for non-beam events. In addition, it adds precise timing capabilities and improves the calorimetry measurements. In ProtoDUNE-DP, scintillation and electroluminescence light produced by cosmic muons in the LArTPC is collected by photomultiplier tubes placed up to 7 m away from the ionizing track. In this paper, the ProtoDUNE-DP photon detection system performance is evaluated with a particular focus on the different wavelength shifters, such as PEN and TPB, and the use of Xe-doped LAr, considering its future use in giant LArTPCs. The scintillation light production and propagation processes are analyzed and a comparison of simulation to data is performed, improving understanding of the liquid argon properties."
JAMES ANDERSON,Whorl: 2024,
JAMES ANDERSON,GJ 1252 b: A 1.2 R ⊕ planet transiting an M3 dwarf at 20.4 pc,"We report the discovery of GJ 1252 b, a planet with a radius of 1.193 ± 0.074 R⊕ and an orbital period of 0.52 days around an M3-type star (0.381 ± 0.019 M⊙, 0.391 ± 0.020 R⊙) located 20.385 ± 0.019 pc away. We use TESS data, ground-based photometry and spectroscopy, Gaia astrometry, and high angular resolution imaging to show that the transit signal seen in the TESS data must originate from a transiting planet. We do so by ruling out all false positive scenarios that attempt to explain the transit signal as originating from an eclipsing stellar binary. Precise Doppler monitoring also leads to a tentative mass measurement of 2.09 ± 0.56 M⊕. The host star proximity, brightness (V = 12.19 mag, K = 7.92 mag), low stellar activity, and the system’s short orbital period make this planet an attractive target for detailed characterization, including precise mass measurement, looking for other objects in the system, and planet atmosphere characterization."
QIANG CUI,"Analysis of the conformational properties of amine ligands at the gold/water interface with QM, MM and QM/MM simulations","We describe a strategy of integrating quantum mechanical (QM), hybrid quantum mechanical/molecular mechanical (QM/MM) and MM simulations to analyze the physical properties of a solid/water interface. This protocol involves using a correlated ab initio (CCSD(T)) method to first calibrate Density Functional Theory (DFT) as the QM approach, which is then used in QM/MM simulations to compute relevant free energy quantities at the solid/water interface using a mean-field approximation of Yang et al. that decouples QM and MM thermal fluctuations; gas-phase QM/MM and periodic DFT calculations are used to determine the proper QM size in the QM/MM simulations. Finally, the QM/MM free energy results are compared with those obtained from MM simulations to directly calibrate the force field model for the solid/water interface. This protocol is illustrated by examining the orientations of an alkyl amine ligand at the gold/water interface, since the ligand conformation is expected to impact the chemical properties (e.g., charge) of the solid surface. DFT/MM and MM simulations using the INTERFACE force field lead to consistent results, suggesting that the effective gold/ligand interactions can be adequately described by a van der Waals model, while electrostatic and induction effects are largely quenched by solvation. The observed differences among periodic DFT, QM/MM and MM simulations, nevertheless, suggest that explicitly including electronic polarization and potentially charge transfer in the MM model can be important to the quantitative accuracy. The strategy of integrating multiple computational methods to cross-validate each other for complex interfaces is applicable to many problems that involve both inorganic/metallic and organic/biomolecular components, such as functionalized nanoparticles."
QIANG CUI,Implications for an imidazol-2-yl carbene intermediate in the rhodanase-catalyzed C-S bond formation reaction of anaerobic ergothioneine biosynthesis,"In the anaerobic ergothioneine biosynthetic pathway, a rhodanese domain containing enzyme (EanB) activates tne hercynine's sp2 ε-C-H Dona ana replaces it with a C-S bond to produce ergothioneine. The key intermediate for this trans-sulfuration reaction is the Cys412 persulfide. Substitution of the EanB-Cys412 persulfide with a Cys412 perselenide does not yield the selenium analog of ergothioneine, selenoneine. However, in deuterated buffer, the perselenide-modified EanB catalyzes the deuterium exchange between hercynine's sp2 ε-C-H bond and D2O. Results from QM/MM calculations suggest that the reaction involves a carbene intermediate and that Tyr353 plays a key role. We hypothesize that modulating the pKa of Tyr353 will affect the deuterium-exchange rate. Indeed, the 3,5-difluoro tyrosine containing EanB catalyzes the deuterium exchange reaction with k ex of ~10-fold greater than the wild-type EanB (EanBWT). With regards to potential mechanisms, these results support the involvement of a carbene intermediate in EanB-catalysis, rendering EanB as one of the few carbene-intermediate involving enzymatic systems."
QIANG CUI,Single-step replacement of an unreactive C-H bond by a C-S bond using polysulfide as the direct sulfur source in anaerobic ergothioneine biosynthesis,"Ergothioneine, a natural longevity vitamin and antioxidant, is a thiol-histidine derivative. Recently, two types of biosynthetic pathways were reported. In the aerobic ergothioneine biosynthesis, a non-heme iron enzyme incorporates a sulfoxide to an sp2 C-H bond in trimethyl-histidine (hercynine) through oxidation reactions. In contrast, in the anaerobic ergothioneine biosynthetic pathway in a green sulfur bacterium, Chlorobium limicola, a rhodanese domain containing protein (EanB) directly replaces this unreactive hercynine C-H bond with a C-S bond. Herein, we demonstrate that polysulfide (HSSnSR) is the direct sulfur-source in EanB-catalysis. After identifying EanB's substrates, X-ray crystallography of several intermediate states along with mass spectrometry results provide additional mechanistic details for this reaction. Further, quantum mechanics/molecular mechanics (QM/MM) calculations reveal that protonation of Nπ of hercynine by Tyr353 with the assistance of Thr414 is a key activation step for the hercynine sp2 C-H bond in this trans-sulfuration reaction."
SUSHRUT S. WAIKAR,Deep-learning-driven quantification of interstitial fibrosis in digitized kidney biopsies,"Interstitial fibrosis and tubular atrophy (IFTA) on a renal biopsy are strong indicators of disease chronicity and prognosis. Techniques that are typically used for IFTA grading remain manual, leading to variability among pathologists. Accurate IFTA estimation using computational techniques can reduce this variability and provide quantitative assessment. Using trichrome-stained whole-slide images (WSIs) processed from human renal biopsies, we developed a deep-learning framework that captured finer pathologic structures at high resolution and overall context at the WSI level to predict IFTA grade. WSIs (n = 67) were obtained from The Ohio State University Wexner Medical Center. Five nephropathologists independently reviewed them and provided fibrosis scores that were converted to IFTA grades: ≤10% (none or minimal), 11% to 25% (mild), 26% to 50% (moderate), and >50% (severe). The model was developed by associating the WSIs with the IFTA grade determined by majority voting (reference estimate). Model performance was evaluated on WSIs (n = 28) obtained from the Kidney Precision Medicine Project. There was good agreement on the IFTA grading between the pathologists and the reference estimate (κ = 0.622 ± 0.071). The accuracy of the deep-learning model was 71.8% ± 5.3% on The Ohio State University Wexner Medical Center and 65.0% ± 4.2% on Kidney Precision Medicine Project data sets. Our approach to analyzing microscopic- and WSI-level changes in renal biopsies attempts to mimic the pathologist and provides a regional and contextual estimation of IFTA. Such methods can assist clinicopathologic diagnosis."
JONATHAN KLAWANS,The pseudo-Jewishness of Pseudo-Phocylides,"For over 150 years, The Sentences of Pseudo-Phocylides has been considered a Jewish work, though scholars have struggled to identify its purpose. This article revisits the question on definitional, evidentiary, and even moral grounds. On definitional grounds, it is problematic to speak of a Jewish work that displays no distinctive Jewish concerns. On evidentiary grounds, we know that the work was transmitted and used by Christians, and we can establish that its selective approach to biblical ethics aligns with identifiably Christian priorities. A Jewish provenance can be hypothesized, but we need not imagine a Christian context for the work. Finally, on moral grounds, we must avoid prejudicial assumptions, such that only a Jew could know the Pentateuch well enough to produce The Sentences. Pseudo-Phocylides's Jewishness is a pseudo-Jewishness. The evidence suggests its Christian use, its Christian allegiance and, therefore, its Christian authorship."
JONATHAN KLAWANS,The Essene Hypothesis: Insights from Religion 101,"General insights from the discipline of religious studies may contribute to a better understanding of the Essene Hypothesis. In its “softer” form, the Essene hypothesis posits a sub-group relationship between the Qumran community and a larger Essene movement as described, above all, by Josephus. This effort to accommodate differences between accounts of the Essenes and the Scrolls can be better understood when contextualized in light of the so-called “insider/outsider” problem. Josephus’s use of the term “Essene” can be productively compared to generalized labels for groups of sub-groups, like “Quaker,” “Mormon,” “Hasidic” and “Gnostic”—terms used more often by outsiders, and frequently by writers of introductory religion textbooks. The Essene Hypothesis makes a greater deal of sense when seen in light of the ways generalized labels are used in a variety of descriptions of religious groups, both ancient and modern."
JONATHAN KLAWANS,Something bigger than Girard,"René Girard’s works on religion and violence remain important, above all, for having called attention to the question of religious violence well before the significance of this problem seemed obvious. Despite Girard’s insistence on the scientific nature of his project, various religious aspects of his work can be identified, and his work is often treated religiously by his followers. Mimetic theory will have to accept its limitations if it is to win over its critics."
JONATHAN KLAWANS,"Deceptive intentions: forgeries, falsehoods and the study of ancient Judaism","This essay probes and problematizes purported distinctions between religious pseudepigraphy and literary deceit. When we attend to what ancient religious pseudepigraphs say about lying, we may be more inclined to recognize the intention to deceive. Apologies for ancient religious pseudepigraphs sometimes resemble defenses for alleged modern forgeries, raising the possibility that academics may not be sufficiently alert to the extent of dishonesty lurking in our source material. In this respect, grappling with ancient lies may also help us recognize modern ones. In any event, the current moment—marked by crises of forgery and falsehood—call for a greater awareness, and increased suspicion."
JONATHAN KLAWANS,Judaism was a civilization: toward a reconstruction of ancient Jewish peoplehood,"Contemporary scholars of ancient Judaism struggle to describe Jewishness, recognizing that ancient Jewish concepts of social collectivity do not fully correspond with modern understandings of ethnicity, nationality, race, or even religion. But despite current efforts, categorical anachronism may be inherently inescapable. Scholars should therefore evaluate modern descriptive terms based on their analytic utility. By this standard, scholars would do well to consider embracing an obviously anachronistic—but nevertheless utilitarian—term to understand ancient Jewishness: “peoplehood.” Mordecai Kaplan's conceptions of “civilization” and “peoplehood” were developed as conscious rejections of more limited (and Protestant) understandings of religion. Kaplan proposed a more nuanced understanding of Jewishness, straddling the same divides (between ethnicity, nationality, and religion) that confound scholars of ancient Judaism today. Kaplan's understanding of the Jewish civilization—land, language, folkways, sanctions, institutions, and arts—presents a striking (if inexact) articulation of the way scholars of ancient Judaism discern Jewishness in our ancient evidence. This in turn justifies utilizing “peoplehood” as an analytic category for the understanding of ancient Jewishness."
JENNIFER WEUVE,Clinicopathological evaluation of chronic traumatic encephalopathy in players of American football,"IMPORTANCE: Players of American football may be at increased risk of long-term neurological conditions, particularly chronic traumatic encephalopathy (CTE). OBJECTIVE: To determine the neuropathological and clinical features of deceased football players with CTE. DESIGN, SETTING, AND PARTICIPANTS: Case series of 202 football players whose brains were donated for research. Neuropathological evaluations and retrospective telephone clinical assessments (including head trauma history) with informants were performed blinded. Online questionnaires ascertained athletic and military history. EXPOSURES: Participation in American football at any level of play. MAIN OUTCOMES AND MEASURES: Neuropathological diagnoses of neurodegenerative diseases, including CTE, based on defined diagnostic criteria; CTE neuropathological severity (stages I to IV or dichotomized into mild [stages I and II] and severe [stages III and IV]); informant-reported athletic history and, for players who died in 2014 or later, clinical presentation, including behavior, mood, and cognitive symptoms and dementia. RESULTS: Among 202 deceased former football players (median age at death, 66 years [interquartile range, 47-76 years]), CTE was neuropathologically diagnosed in 177 players (87%; median age at death, 67 years [interquartile range, 52-77 years]; mean years of football participation, 15.1 [SD, 5.2]), including 0 of 2 pre–high school, 3 of 14 high school (21%), 48 of 53 college (91%), 9 of 14 semiprofessional (64%), 7 of 8 Canadian Football League (88%), and 110 of 111 National Football League (99%) players. Neuropathological severity of CTE was distributed across the highest level of play, with all 3 former high school players having mild pathology and the majority of former college (27 [56%]), semiprofessional (5 [56%]), and professional (101 [86%]) players having severe pathology. Among 27 participants with mild CTE pathology, 26 (96%) had behavioral or mood symptoms or both, 23 (85%) had cognitive symptoms, and 9 (33%) had signs of dementia. Among 84 participants with severe CTE pathology, 75 (89%) had behavioral or mood symptoms or both, 80 (95%) had cognitive symptoms, and 71 (85%) had signs of dementia. CONCLUSIONS AND RELEVANCE: In a convenience sample of deceased football players who donated their brains for research, a high proportion had neuropathological evidence of CTE, suggesting that CTE may be related to prior participation in football."
JENNIFER WEUVE,Cumulative Community-Level Lead Exposure and Pulse Pressure: The Normative Aging Study,"BACKGROUND. Pulse pressure increases with age in industrialized societies as a manifestation of arterial stiffening. Lead accumulates in the vasculature and is associated with vascular oxidative stress, which can promote functional and structural vascular disease. OBJECTIVES. We tested the hypothesis that cumulative community-level lead exposure, measured with K-X-ray fluorescence, is associated with pulse pressure in a cohort of adult men. METHODS AND RESULTS. In a cross-sectional analysis of 593 men not treated with antihypertensive medication, tibia lead was positively associated with pulse pressure (p < 0.001). Adjusting for age, race, diabetes, family history of hypertension, education, waist circumference, alcohol intake, smoking history, height, heart rate, fasting glucose, and total cholesterol-to-HDL ratio, increasing quintiles of tibia lead remained associated with increased pulse pressure (ptrend = 0.02). Men with tibia lead above the median (19.0 μg/g) had, on average, a 4.2-mmHg (95% confidence interval, 1.9-6.5) higher pulse pressure than men with tibia lead level below the median. In contrast, blood lead level was not associated with pulse pressure. CONCLUSIONS. These data indicate that lead exposure may contribute to the observed increase in pulse pressure that occurs with aging in industrialized societies. Lead accumulation may contribute to arterial aging, perhaps providing mechanistic insight into the observed association of low-level lead exposure with cardiovascular mortality."
JENNIFER WEUVE,Cumulative Lead Exposure and Tooth Loss in Men: The Normative Aging Study,"BACKGROUND. Individuals previously exposed to lead remain at risk because of endogenous release of lead stored in their skeletal compartments. However, it is not known if long-term cumulative lead exposure is a risk factor for tooth loss. OBJECTIVES. We examined the association of bone lead concentrations with loss of natural teeth. METHODS. We examined 333 men enrolled in the Veterans Affairs Normative Aging Study. We used a validated K-shell X-ray fluorescence (KXRF) method to measure lead concentrations in the tibial midshaft and patella. A dentist recorded the number of teeth remaining, and tooth loss was categorized as 0, 1-8 or ≥ 9 missing teeth. We used proportional odds models to estimate the association of bone lead biomarkers with tooth loss, adjusting for age, smoking, diabetes, and other putative confounders. RESULTS. Participants with ≥ 9 missing teeth had significantly higher bone lead concentrations than those who had not experienced tooth loss. In multivariable-adjusted analyses, men in the highest tertile of tibia lead (> 23 μg/g) and patella lead (> 36 μg/g) had approximately three times the odds of having experienced an elevated degree of tooth loss (≥ 9 vs. 0-8 missing teeth or ≥ 1 vs. 0 missing teeth) as those in the lowest tertile [prevalence odds ratio (OR) = 3.03; 95% confidence interval (CI), 1.60-5.76 and OR = 2.41; 95% CI, 1.30-4.49, respectively]. Associations between bone lead biomarkers and tooth loss were similar in magnitude to the increased odds observed in participants who were current smokers. CONCLUSION. Long-term cumulative lead exposure is associated with increased odds of tooth loss."
JENNIFER WEUVE,Association of Cumulative Lead Exposure with Parkinson's Disease,"BACKGROUND. Research using reconstructed exposure histories has suggested an association between heavy metal exposures, including lead, and Parkinson's disease (PD), but the only study that used bone lead, a biomarker of cumulative lead exposure, found a nonsignificant increase in risk of PD with increasing bone lead. OBJECTIVES. We sought to assess the association between bone lead and PD. METHODS. Bone lead concentrations were measured using 109Cd excited K-shell X-ray fluorescence from 330 PD patients (216 men, 114 women) and 308 controls (172 men, 136 women) recruited from four clinics for movement disorders and general-community cohorts. Adjusted odds ratios (ORs) for PD were calculated using logistic regression. RESULTS. The average age of cases and controls at bone lead measurement was 67 (SD = 10) and 69 (SD = 9) years of age, respectively. In primary analyses of cases and controls recruited from the same groups, compared with the lowest quartile of tibia lead, the OR for PD in the highest quartile was 3.21 [95% confidence interval (CI), 1.17-8.83]. Results were similar but slightly weaker in analyses restricted to cases and controls recruited from the movement disorders clinics only (fourth-quartile OR = 2.57; 95% CI, 1.11-5.93) or when we included controls recruited from sites that did not also contribute cases (fourth-quartile OR = 1.91; 95% CI, 1.01-3.60). We found no association with patella bone lead. CONCLUSIONS. These findings, using an objective biological marker of cumulative lead exposure among typical PD patients seen in our movement disorders clinics, strengthen the evidence that cumulative exposure to lead increases the risk of PD."
JENNIFER WEUVE,Modifying Effects of the HFE Polymorphisms on the Association between Lead Burden and Cognitive Decline,"BACKGROUND. As iron and lead promote oxidative damage, and hemochromatosis (HFE) gene polymorphisms increase body iron burden, HFE variant alleles may modify the lead burden and cognitive decline relationship. OBJECTIVE. Our goal was to assess the modifying effects of HFE variants on the lead burden and cognitive decline relation in older adults. METHODS. We measured tibia and patella lead using K-X-ray fluorescence (1991-1999) among participants of the Normative Aging Study, a longitudinal study of community-dwelling men from greater Boston. We assessed cognitive function with the Mini-Mental State Examination (MMSE) twice (1993-1998 and 1995-2000) and genotyped participants for HFE polymorphisms. We estimated the adjusted mean differences in lead-associated annual cognitive decline across HFE genotype groups (n = 358). RESULTS. Higher tibia lead was associated with steeper cognitive decline among participants with at least one HFE variant allele compared with men with only wild-type alleles (p interaction = 0.03), such that a 15 μg/g increase in tibia lead was associated with a 0.2 point annual decrement in MMSE score among HFE variant allele carriers. This difference in scores among men with at least one variant allele was comparable to the difference in baseline MMSE scores that we observed among men who were 4 years apart in age. Moreover, the deleterious association between tibia lead and cognitive decline appeared progressively worse in participants with increasingly more copies of HFE variant alleles (p-trend = 0.008). Results for patella lead were similar. CONCLUSION. Our findings suggest that HFE polymorphisms greatly enhance susceptibility to lead-related cognitive impairment in a pattern consistent with allelelic dose."
JENNIFER WEUVE,"Association of Exposure to Phthalates with Endometriosis and Uterine Leiomyomata: Findings from NHANES, 1999-2004","BACKGROUND. Phthalates are ubiquitous chemicals used in consumer products. Some phthalates are reproductive toxicants in experimental animals, but human data are limited. OBJECTIVE. We conducted a cross-sectional study of urinary phthalate metabolite concentrations in relation to self-reported history of endometriosis and uterine leiomyomata among 1,227 women 20-54 years of age from three cycles of the National Health and Nutrition Examination Survey (NHANES), 1999-2004. METHODS. We examined four phthalate metabolites: mono(2-ethylhexyl) phthalate (MEHP), monobutyl phthalate (MBP), monoethyl phthalate (MEP), and monobenzyl phthalate (MBzP). From the last two NHANES cycles, we also examined mono(2-ethyl-5-hydroxyhexyl) phthalate (MEHHP) and mono(2-ethyl-5-oxohexyl) phthalate (MEOHP). We used logistic regression to estimate odds ratios (ORs) and 95% confidence intervals (CIs), adjusting for potential confounders. RESULTS. Eighty-seven (7%) and 151 (12%) women reported diagnoses of endometriosis and leiomyomata, respectively. The ORs comparing the highest versus lowest three quartiles of urinary MBP were 1.36 (95% CI, 0.77-2.41) for endometriosis, 1.56 (95% CI, 0.93-2.61) for leiomyomata, and 1.71 (95% CI, 1.07-2.75) for both conditions combined. The corresponding ORs for MEHP were 0.44 (95% CI, 0.19-1.02) for endometriosis, 0.63 (95% CI, 0.35-1.12) for leiomyomata, and 0.59 (95% CI, 0.37-0.95) for both conditions combined. Findings for MEHHP and MEOHP agreed with findings for MEHP with respect to endometriosis only. We observed null associations for MEP and MBzP. Associations were similar when we excluded women diagnosed > 7 years before their NHANES evaluation. CONCLUSION. The positive associations for MBP and inverse associations for MEHP in relation to endometriosis and leiomyomata warrant investigation in prospective studies."
WILLIAM KLEIN,"Scope: v. 1, no. 1-8",
WILLIAM KLEIN,"First Sagittarius A* Event Horizon Telescope results. II. EHT and multiwavelength observations, data processing, and calibration","We present Event Horizon Telescope (EHT) 1.3 mm measurements of the radio source located at the position of the supermassive black hole Sagittarius A* (Sgr A*), collected during the 2017 April 5–11 campaign. The observations were carried out with eight facilities at six locations across the globe. Novel calibration methods are employed to account for Sgr A*'s flux variability. The majority of the 1.3 mm emission arises from horizon scales, where intrinsic structural source variability is detected on timescales of minutes to hours. The effects of interstellar scattering on the image and its variability are found to be subdominant to intrinsic source structure. The calibrated visibility amplitudes, particularly the locations of the visibility minima, are broadly consistent with a blurred ring with a diameter of ∼50 μas, as determined in later works in this series. Contemporaneous multiwavelength monitoring of Sgr A* was performed at 22, 43, and 86 GHz and at near-infrared and X-ray wavelengths. Several X-ray flares from Sgr A* are detected by Chandra, one at low significance jointly with Swift on 2017 April 7 and the other at higher significance jointly with NuSTAR on 2017 April 11. The brighter April 11 flare is not observed simultaneously by the EHT but is followed by a significant increase in millimeter flux variability immediately after the X-ray outburst, indicating a likely connection in the emission physics near the event horizon. We compare Sgr A*’s broadband flux during the EHT campaign to its historical spectral energy distribution and find that both the quiescent emission and flare emission are consistent with its long-term behavior."
WILLIAM KLEIN,First Sagittarius A* Event Horizon Telescope results. I. The shadow of the supermassive black hole in the center of the Milky Way,"We present the first Event Horizon Telescope (EHT) observations of Sagittarius A* (Sgr A*), the Galactic center source associated with a supermassive black hole. These observations were conducted in 2017 using a global interferometric array of eight telescopes operating at a wavelength of λ = 1.3 mm. The EHT data resolve a compact emission region with intrahour variability. A variety of imaging and modeling analyses all support an image that is dominated by a bright, thick ring with a diameter of 51.8 ± 2.3 μas (68% credible interval). The ring has modest azimuthal brightness asymmetry and a comparatively dim interior. Using a large suite of numerical simulations, we demonstrate that the EHT images of Sgr A* are consistent with the expected appearance of a Kerr black hole with mass ∼4 × 106 M ⊙, which is inferred to exist at this location based on previous infrared observations of individual stellar orbits, as well as maser proper-motion studies. Our model comparisons disfavor scenarios where the black hole is viewed at high inclination (i &gt; 50°), as well as nonspinning black holes and those with retrograde accretion disks. Our results provide direct evidence for the presence of a supermassive black hole at the center of the Milky Way, and for the first time we connect the predictions from dynamical measurements of stellar orbits on scales of 103–105 gravitational radii to event-horizon-scale images and variability. Furthermore, a comparison with the EHT results for the supermassive black hole M87* shows consistency with the predictions of general relativity spanning over three orders of magnitude in central mass."
WILLIAM KLEIN,Prospects for beyond the standard model physics searches at the deep underground neutrino experiment: DUNE collaboration,"The Deep Underground Neutrino Experiment (DUNE) will be a powerful tool for a variety of physics topics. The high-intensity proton beams provide a large neutrino flux, sampled by a near detector system consisting of a combination of capable precision detectors, and by the massive far detector system located deep underground. This configuration sets up DUNE as a machine for discovery, as it enables opportunities not only to perform precision neutrino measurements that may uncover deviations from the present three-flavor mixing paradigm, but also to discover new particles and unveil new interactions and symmetries beyond those predicted in the Standard Model (SM). Of the many potential beyond the Standard Model (BSM) topics DUNE will probe, this paper presents a selection of studies quantifying DUNE's sensitivities to sterile neutrino mixing, heavy neutral leptons, non-standard interactions, CPT symmetry violation, Lorentz invariance violation, neutrino trident production, dark matter from both beam induced and cosmogenic sources, baryon number violation, and other new physics topics that complement those at high-energy colliders and significantly extend the present reach."
WILLIAM KLEIN,Identification and reconstruction of low-energy electrons in the ProtoDUNE-SP detector,
WILLIAM KLEIN,The long-baseline neutrino experiment: exploring fundamental symmetries of the universe,"The preponderance of matter over antimatter in the early Universe, the dynamics of the supernova bursts that produced the heavy elements necessary for life and whether protons eventually decay --- these mysteries at the forefront of particle physics and astrophysics are key to understanding the early evolution of our Universe, its current state and its eventual fate. The Long-Baseline Neutrino Experiment (LBNE) represents an extensively developed plan for a world-class experiment dedicated to addressing these questions. LBNE is conceived around three central components: (1) a new, high-intensity neutrino source generated from a megawatt-class proton accelerator at Fermi National Accelerator Laboratory, (2) a near neutrino detector just downstream of the source, and (3) a massive liquid argon time-projection chamber deployed as a far detector deep underground at the Sanford Underground Research Facility. This facility, located at the site of the former Homestake Mine in Lead, South Dakota, is approximately 1,300 km from the neutrino source at Fermilab -- a distance (baseline) that delivers optimal sensitivity to neutrino charge-parity symmetry violation and mass ordering effects. This ambitious yet cost-effective design incorporates scalability and flexibility and can accommodate a variety of upgrades and contributions. With its exceptional combination of experimental configuration, technical capabilities, and potential for transformative discoveries, LBNE promises to be a vital facility for the field of particle physics worldwide, providing physicists from around the globe with opportunities to collaborate in a twenty to thirty year program of exciting science. In this document we provide a comprehensive overview of LBNE's scientific objectives, its place in the landscape of neutrino physics worldwide, the technologies it will incorporate and the capabilities it will possess."
WILLIAM KLEIN,Scintillation light detection in the 6-m drift-length ProtoDUNE Dual Phase liquid argon TPC,"DUNE is a dual-site experiment for long-baseline neutrino oscillation studies, neutrino astrophysics and nucleon decay searches. ProtoDUNE Dual Phase (DP) is a 6  ×  6  ×  6 m 3 liquid argon time-projection-chamber (LArTPC) that recorded cosmic-muon data at the CERN Neutrino Platform in 2019-2020 as a prototype of the DUNE Far Detector. Charged particles propagating through the LArTPC produce ionization and scintillation light. The scintillation light signal in these detectors can provide the trigger for non-beam events. In addition, it adds precise timing capabilities and improves the calorimetry measurements. In ProtoDUNE-DP, scintillation and electroluminescence light produced by cosmic muons in the LArTPC is collected by photomultiplier tubes placed up to 7 m away from the ionizing track. In this paper, the ProtoDUNE-DP photon detection system performance is evaluated with a particular focus on the different wavelength shifters, such as PEN and TPB, and the use of Xe-doped LAr, considering its future use in giant LArTPCs. The scintillation light production and propagation processes are analyzed and a comparison of simulation to data is performed, improving understanding of the liquid argon properties."
WILLIAM KLEIN,Event Horizon Telescope imaging of the archetypal blazar 3C 279 at an extreme 20 microarcsecond resolution,"3C 279 is an archetypal blazar with a prominent radio jet that show broadband flux density variability across the entire electromagnetic spectrum. We use an ultra-high angular resolution technique – global Very Long Baseline Interferometry (VLBI) at 1.3 mm (230 GHz) – to resolve the innermost jet of 3C 279 in order to study its fine-scale morphology close to the jet base where highly variable γ-ray emission is thought to originate, according to various models. The source was observed during four days in April 2017 with the Event Horizon Telescope at 230 GHz, including the phased Atacama Large Millimeter/submillimeter Array (ALMA), at an angular resolution of ∼20 μas (at a redshift of z = 0.536 this corresponds to ∼0.13 pc  ∼ 1700 Schwarzschild radii with a black hole mass M<jats:sub>BH</jats:sub> = 8 × 10^8 M_⊙). Imaging and model-fitting techniques were applied to the data to parameterize the fine-scale source structure and its variation. We find a multicomponent inner jet morphology with the northernmost component elongated perpendicular to the direction of the jet, as imaged at longer wavelengths. The elongated nuclear structure is consistent on all four observing days and across different imaging methods and model-fitting techniques, and therefore appears robust. Owing to its compactness and brightness, we associate the northern nuclear structure as the VLBI “core”. This morphology can be interpreted as either a broad resolved jet base or a spatially bent jet. We also find significant day-to-day variations in the closure phases, which appear most pronounced on the triangles with the longest baselines. Our analysis shows that this variation is related to a systematic change of the source structure. Two inner jet components move non-radially at apparent speeds of ∼15 c and ∼20 c (∼1.3 and ∼1.7 μas day^−1, respectively), which more strongly supports the scenario of traveling shocks or instabilities in a bent, possibly rotating jet. The observed apparent speeds are also coincident with the 3C 279 large-scale jet kinematics observed at longer (cm) wavelengths, suggesting no significant jet acceleration between the 1.3 mm core and the outer jet. The intrinsic brightness temperature of the jet components are ≲10^10 K, a magnitude or more lower than typical values seen at ≥7 mm wavelengths. The low brightness temperature and morphological complexity suggest that the core region of 3C 279 becomes optically thin at short (mm) wavelengths."
WILLIAM KLEIN,First M87 Event Horizon Telescope results. II. Array and instrumentation,"The Event Horizon Telescope (EHT) is a very long baseline interferometry (VLBI) array that comprises millimeter- and submillimeter-wavelength telescopes separated by distances comparable to the diameter of the Earth. At a nominal operating wavelength of ~1.3 mm, EHT angular resolution (λ/D) is ~25 μas, which is sufficient to resolve nearby supermassive black hole candidates on spatial and temporal scales that correspond to their event horizons. With this capability, the EHT scientific goals are to probe general relativistic effects in the strong-field regime and to study accretion and relativistic jet formation near the black hole boundary. In this Letter we describe the system design of the EHT, detail the technology and instrumentation that enable observations, and provide measures of its performance. Meeting the EHT science objectives has required several key developments that have facilitated the robust extension of the VLBI technique to EHT observing wavelengths and the production of instrumentation that can be deployed on a heterogeneous array of existing telescopes and facilities. To meet sensitivity requirements, high-bandwidth digital systems were developed that process data at rates of 64 gigabit s−1, exceeding those of currently operating cm-wavelength VLBI arrays by more than an order of magnitude. Associated improvements include the development of phasing systems at array facilities, new receiver installation at several sites, and the deployment of hydrogen maser frequency standards to ensure coherent data capture across the array. These efforts led to the coordination and execution of the first Global EHT observations in 2017 April, and to event-horizon-scale imaging of the supermassive black hole candidate in M87."
WILLIAM KLEIN,First M87 Event Horizon Telescope results. I. The shadow of the supermassive black hole,"When surrounded by a transparent emission region, black holes are expected to reveal a dark shadow caused by gravitational light bending and photon capture at the event horizon. To image and study this phenomenon, we have assembled the Event Horizon Telescope, a global very long baseline interferometry array observing at a wavelength of 1.3 mm. This allows us to reconstruct event-horizon-scale images of the supermassive black hole candidate in the center of the giant elliptical galaxy M87. We have resolved the central compact radio source as an asymmetric bright emission ring with a diameter of 42 ± 3 μas, which is circular and encompasses a central depression in brightness with a flux ratio gsim10:1. The emission ring is recovered using different calibration and imaging schemes, with its diameter and width remaining stable over four different observations carried out in different days. Overall, the observed image is consistent with expectations for the shadow of a Kerr black hole as predicted by general relativity. The asymmetry in brightness in the ring can be explained in terms of relativistic beaming of the emission from a plasma rotating close to the speed of light around a black hole. We compare our images to an extensive library of ray-traced general-relativistic magnetohydrodynamic simulations of black holes and derive a central mass of M = (6.5 ± 0.7) × 10^9 M ⊙. Our radio-wave observations thus provide powerful evidence for the presence of supermassive black holes in centers of galaxies and as the central engines of active galactic nuclei. They also present a new tool to explore gravity in its most extreme limit and on a mass scale that was so far not accessible."
WILLIAM KLEIN,"The medical student: v. 16, no. 1-8.",
BARTON L LIPMAN,Acquisition of/stochastic evidence,"We explore two highly interrelated models of ""hard information."" In the evidence-acquisition model, an agent with private information searches for evidence to show to the principal about her type. In the signal-choice model, a privately informed agent chooses an action which generates a random signal whose realization may be correlated with her type. We show that the signal-choice model is a special case and, under certain conditions, a reduced form of the evidence-acquisition model. We develop tools for characterizing optimal mechanisms for these models by giving conditions under which some aspects of the principal's optimal choices can be identified only from the information structure, without regard to the utility functions or the principal's priors. We also give a novel result on conditions under which there is no value to commitment for the principal."
BARTON L LIPMAN,Disclosure and choice,"An agent chooses among projects with random outcomes. His payoff is increasing in the outcome and in an observer's expectation of the outcome. With some probability, the agent can disclose the true outcome to the observer. We show that choice is inefficient: the agent favors riskier projects even with lower expected returns. If information can be disclosed by a challenger who prefers lower beliefs of the observer, the chosen project is excessively risky when the agent has better access to information, excessively risk{averse when the challenger has better access, and efficient otherwise. We also characterize the agent's worst-case equilibrium payoff."
JAMES ROHLF,Observation of the production of three massive gauge bosons at √s=13 TeV,"The first observation is reported of the combined production of three massive gauge bosons (VVV with V=W, Z) in proton-proton collisions at a center-of-mass energy of 13 TeV. The analysis is based on a data sample recorded by the CMS experiment at the CERN LHC corresponding to an integrated luminosity of 137  fb^−1. The searches for individual WWW, WWZ, WZZ, and ZZZ production are performed in final states with three, four, five, and six leptons (electrons or muons), or with two same-sign leptons plus one or two jets. The observed (expected) significance of the combined VVV production signal is 5.7 (5.9) standard deviations and the corresponding measured cross section relative to the standard model prediction is 1.02^+0.26_−0.23. The significances of the individual WWW and WWZ production are 3.3 and 3.4 standard deviations, respectively. Measured production cross sections for the individual triboson processes are also reported."
JAMES ROHLF,Measurements of production cross sections of WZ and same-sign WW boson pairs in association with two jets in proton-proton collisions at s=13TeV,
JAMES ROHLF,The Apollo ATCA platform,"We have developed a novel and generic open-source platform - Apollo - which simplifies the design of custom Advanced Telecommunications Computing Architecture (ATCA) blades by factoring the design into generic infrastructure and application-specific parts. The Apollo ""Service Module"" provides the required ATCA Intelligent Platform Management Controller, power entry and conditioning, a powerful system-on-module (SoM) computer, and flexible clock and communications infrastructure. The Apollo ""Command Module"" is customized for each application and typically includes two large field-programmable gate arrays, several hundred optical fiber interfaces operating at speeds up to 28 Gbps, memories, and other supporting infrastructure. The command and service module boards can be operated together or independently on the bench without need for an ATCA shelf."
MARCUS WINTERS,Representation in the classroom: The effect of own-race teachers on student achievement,"Previous research suggests that there are academic benefits when students and teachers share the same race/ethnicity because such teachers can serve as role models, mentors, advocates, or cultural translators. In this paper, we obtain estimates of achievement changes as students are assigned to teachers of different races/ethnicities from grades 3 through 10 utilizing a large administrative dataset provided by the Florida Department of Education that follows the universe of test-taking students in Florida public schools from 2001–2002 through 2008–2009. We find small but significant positive effects when black and white students are assigned to race-congruent teachers in reading (.004–.005 standard deviations) and for black, white and Asian/Pacific Island students in math (.007–.041 standard deviations). We also examine the effects of race matching by students' prior performance level, finding that lower-performing black and white students appear to particularly benefit from being assigned to a race-congruent teacher."
MARCUS WINTERS,"Are low-performing students more likely to exit charter schools? Evidence from New York City and Denver, Colorado","A common criticism of charter schools is that they systematically remove or “counsel out” their lowest performing students. However, relatively little is currently known about whether low-performing students are in fact more likely to exit charter schools than surrounding traditional public schools. We use longitudinal student-level data from two large urban school systems that prior research has found to have effective charter school sectors–New York City and Denver, Colorado–to evaluate whether there is a differential relationship between low-performance on standardized test scores and the probability that students exit their schools by sector attended. We find no evidence of a differential relationship between prior performance and the likelihood of exiting a school by sector. Low-performing students in both cities are either equally likely or less likely to exit their schools than are student in traditional public schools."
MARCUS WINTERS,"Does attending a charter school Reduce the likelihood of being placed into special education? Evidence from Denver, Colorado","We use administrative data to measure whether attending a charter school in Denver, Colorado, reduces the likelihood that students are newly classified as having a disability in primary grades. We employ an observational approach that takes advantage of Denver’s Common Enrollment System, which allows us to observe each school that the student listed a preference to attend. We find evidence that attending a Denver charter school reduces the likelihood that a student is classified as having a specific learning disability, which is the largest and most subjectively diagnosed disability category. We find no evidence that charter attendance reduces the probability of being classified as having a speech or language disability or autism, which are two more objectively diagnosed classifications."
MARCUS WINTERS,How pensions contribute to the premium paid to experienced public school teachers,"Many argue that public school systems should stop linking teachers’ salaries so closely to their years of experience. However, the effect of deferred retirement compensation on the premium paid to experienced teachers has, to date, been underappreciated. To shed more light on this issue, we calculate the total compensation earned by teachers in New York City and Philadelphia from both salary and deferred retirement compensation under each system’s currently operating defined-benefit plan. Retirement compensation in both cities is back-loaded, which substantially increases the premium paid to highly experienced teachers. In late-career years, teachers often earn a larger compensation premium from the accrual of pension benefits than from salary. We show that cash-balance retirement plans, which are less back-loaded, would substantially reduce experience premiums without reducing the total compensation for the average entering teacher."
MARCUS WINTERS,The effects of test-based retention on student outcomes over time: Regression discontinuity evidence from Florida,"Many American states require that students lacking basic reading proficiency after third grade be retained and remediated. We exploit a discontinuity in retention probabilities under Florida's test-based promotion policy to study its effects on student outcomes through high school. We find large positive effects on achievement that fade out entirely when retained students are compared to their same-age peers, but remain substantial through grade 10 when compared to students in the same grade. Being retained in third grade due to missing the promotion standard increases students' grade point averages and leads them to take fewer remedial courses in high school but has no effect on their probability of graduating."
MARCUS WINTERS,"For teachers, a better kind of pension plan",
MARCUS WINTERS,Regulatory arbitrage in teacher hiring and retention: evidence from Massachusetts charter schools,
ROBERT E B LUCAS,Magnetic-field measurement and analysis for the Muon g−2 Experiment at Fermilab,"The Fermi National Accelerator Laboratory (FNAL) Muon g−2 Experiment has measured the anomalous precession frequency aμ≡(gμ−2)/2 of the muon to a combined precision of 0.46 parts per million with data collected during its first physics run in 2018. This paper documents the measurement of the magnetic field in the muon storage ring. The magnetic field is monitored by systems and calibrated in terms of the equivalent proton spin precession frequency in a spherical water sample at 34.7∘C. The measured field is weighted by the muon distribution resulting in ˜ω′p, the denominator in the ratio ωa/˜ω′p that together with known fundamental constants yields aμ. The reported uncertainty on ˜ω′p for the Run-1 data set is 114 ppb consisting of uncertainty contributions from frequency extraction, calibration, mapping, tracking, and averaging of 56 ppb, and contributions from fast transient fields of 99 ppb."
ROBERT E B LUCAS,The Event Horizon general relativistic magnetohydrodynamic code comparison project,"Recent developments in compact object astrophysics, especially the discovery of merging neutron stars by LIGO, the imaging of the black hole in M87 by the Event Horizon Telescope, and high- precision astrometry of the Galactic Center at close to the event horizon scale by the GRAVITY experiment motivate the development of numerical source models that solve the equations of general relativistic magnetohydrodynamics (GRMHD). Here we compare GRMHD solutions for the evolution of a magnetized accretion flow where turbulence is promoted by the magnetorotational instability from a set of nine GRMHD codes: Athena++, BHAC, Cosmos++, ECHO, H-AMR, iharm3D, HARM-Noble, IllinoisGRMHD, and KORAL. Agreement among the codes improves as resolution increases, as measured by a consistently applied, specially developed set of code performance metrics. We conclude that the community of GRMHD codes is mature, capable, and consistent on these test problems."
ROBERT E B LUCAS,Mines and migrants in South Africa,"Between 1971 and 1978 wages of more than half a million nonwhite laborers in the South African mines tripled in real terms. In the same period, the nonwhites employed in the mines switched from being 62 percent foreign to 62 percent domestic. These changes followed a period -- from 1911 to 1971 -during which real wages of black gold miners did not rise, and terminated almost a century of reliance on foreign labor ""reserves"" for the majority of such labor. These dramatic events are examined here in the context of an econometric model of the demand for labor by the South African mining sector from 1946 to 1980. This affords an unusual opportunity to study the demand side of a market for internal and international migrants, in a society where racial discrimination is formalized in- the apartheid system, where powerful mining houses wield potential monopsony power, and political factors in the region are major determinants of economic behavior. To comprehend the derived demand for workers in this sector, it is essential to outline at least certain aspects of the industry's organization and that of the market for labor. This is undertaken in section I, section II develops a stylized model, which is then estimated, from data described in III, for the gold, diamond, coal and other minerals sectors separately in section IV."
ROBERT E B LUCAS,"International migration: economic causes, consequences, evaluation, and policies","The 70's have witnessed a number of events which are again focussing attention of economists on international migration: the dramatic emergence of the Middle East labor markets is affecting the economies of the low income Middle Eastern nations and of South Asia on a large scale; the world recession with its associated unemployment in North America and Western Europe has stimulated increased conservatism with regard to immigration policy; increasing tension in Southern Africa has raised interest in the continuing role of South Africa as a major employer of workers from neighboring countries; and the longstanding rhetoric against the brain-drain from poorer nations has persisted. The present paper attempts to provide a synopsis of some of the economic issues pertinent to the consideration of these phenomena, and is organized into four main sections. Section I deliberates some of the causes of international migration, drawing in part on the literature on internal migration. Section II then proceeds to consider some of the economic consequences of international migration. This section is subdivided into two parts: the first dealing with migration in promoting global and national productive efficiency -- a subject of much attention in economic theory; the second addressing some of the questions on international and internal distribution of incomes. Section III deals with an area in which almost no literature exists: building on the results of the previous sections, a framework for considering cost-benefit evaluation studies of international migration is briefly outlined. Finally, the fourth section discusses some of the policy instruments used today, or which might be adopted, for promoting, discouraging or changing the nature of international migration a discussion based on the summary of causes and consequences, and ultimately requiring some form of evaluation prior to implementation."
ROBERT E B LUCAS,"Canvass: a crowd-sourced, natural-product screening library for exploring biological space",
ROBERT E B LUCAS,Beam dynamics corrections to the Run-1 measurement of the muon anomalous magnetic moment at Fermilab,"This paper presents the beam dynamics systematic corrections and their uncertainties for the Run-1 dataset of the Fermilab Muon g−2 Experiment. Two corrections to the measured muon precession frequency ωma are associated with well-known effects owing to the use of electrostatic quadrupole (ESQ) vertical focusing in the storage ring. An average vertically oriented motional magnetic field is felt by relativistic muons passing transversely through the radial electric field components created by the ESQ system. The correction depends on the stored momentum distribution and the tunes of the ring, which has relatively weak vertical focusing. Vertical betatron motions imply that the muons do not orbit the ring in a plane exactly orthogonal to the vertical magnetic field direction. A correction is necessary to account for an average pitch angle associated with their trajectories. A third small correction is necessary, because muons that escape the ring during the storage time are slightly biased in initial spin phase compared to the parent distribution. Finally, because two high-voltage resistors in the ESQ network had longer than designed RC time constants, the vertical and horizontal centroids and envelopes of the stored muon beam drifted slightly, but coherently, during each storage ring fill. This led to the discovery of an important phase-acceptance relationship that requires a correction. The sum of the corrections to ωma is 0.50±0.09  ppm; the uncertainty is small compared to the 0.43 ppm statistical precision of ωma."
ROBERT E B LUCAS,The distribution and efficiency of crop production in tribal areas of Botswana,
ROBERT E B LUCAS,Migration amongst the Batswana,"INTRODUCTION The hypothesis presented in Todaro (1969), that the likelihood of finding work in town influences the rate of rural-urban migration, now enjoys the status of a received doctrine. Assuming potential migrants indeed respond to this employment probability, the model of Harris and Todaro (1970) demonstrates that, in certain parametric ranges, urban job creation may actually exacerbate unemployment and even reduce national product. This result has had considerable influence on policy formulation in LDC, by emphasizing that, in the urban sector, the social opportunity cost of labor may not be insignificant despite burgeoning unemployment. Yet neither the Todaro hypothesis nor prevalence of the Harris-Todaro parametric range has been adequately, empirically explored. Many estimates of macro migration equations do exist, normally relating the proportion of population migrating to average wages in differing locations and occasionally to average population characteristics. But in Lucas (1975), I show that the popular nonlinear specification of such macro functions may well display serious specification error bias; a nonlinear function of the aggregate variables is not simply the average of underlying micro migration decisions related to the disaggregated variables. Thus, although a few estimates of macro migration equations have also incorporated average unemployment rates, usually in developed country contexts and with mixed results, these analyses are at best very circumscribed tests of the Todaro and Harris-Todaro theories. [TRUNCATED]"
ROBERT E B LUCAS,The distribution of wages and employment in rural Botswana,
ROBERT E B LUCAS,"Cosmology intertwined: a review of the particle physics, astrophysics, and cosmology associated with the cosmological tensions and anomalies",
ROBERT E B LUCAS,Measurement of the anomalous precession frequency of the muon in the Fermilab Muon g − 2 Experiment,"The Muon g−2 Experiment at Fermi National Accelerator Laboratory (FNAL) has measured the muon anomalous precession frequency ωma to an uncertainty of 434 parts per billion (ppb), statistical, and 56 ppb, systematic, with data collected in four storage ring configurations during its first physics run in 2018. When combined with a precision measurement of the magnetic field of the experiment’s muon storage ring, the precession frequency measurement determines a muon magnetic anomaly of aμ(FNAL)=116592040(54)×10−11 (0.46 ppm). This article describes the multiple techniques employed in the reconstruction, analysis, and fitting of the data to measure the precession frequency. It also presents the averaging of the results from the 11 separate determinations of ωma, and the systematic uncertainties on the result."
ROBERT E B LUCAS,Measurement of the positive muon anomalous magnetic moment to 0.46 ppm,"We present the first results of the Fermilab National Accelerator Laboratory (FNAL) Muon g-2 Experiment for the positive muon magnetic anomaly a_{μ}≡(g_{μ}-2)/2. The anomaly is determined from the precision measurements of two angular frequencies. Intensity variation of high-energy positrons from muon decays directly encodes the difference frequency ω_{a} between the spin-precession and cyclotron frequencies for polarized muons in a magnetic storage ring. The storage ring magnetic field is measured using nuclear magnetic resonance probes calibrated in terms of the equivalent proton spin precession frequency ω[over ˜]_{p}^{'} in a spherical water sample at 34.7 °C. The ratio ω_{a}/ω[over ˜]_{p}^{'}, together with known fundamental constants, determines a_{μ}(FNAL)=116 592 040(54)×10^{-11} (0.46 ppm). The result is 3.3 standard deviations greater than the standard model prediction and is in excellent agreement with the previous Brookhaven National Laboratory (BNL) E821 measurement. After combination with previous measurements of both μ^{+} and μ^{-}, the new experimental average of a_{μ}(Exp)=116 592 061(41)×10^{-11} (0.35 ppm) increases the tension between experiment and theory to 4.2 standard deviations."
VENKATESH SALIGRAMA,Resource constrained structured prediction,"We study the problem of structured prediction under test-time budget constraints. We propose a novel approach applicable to a wide range of structured prediction problems in computer vision and natural language processing. Our approach seeks to adaptively generate computationally costly features during test-time in order to reduce the computational cost of prediction while maintaining prediction performance. We show that training the adaptive feature generation system can be reduced to a series of structured learning problems, resulting in efficient training using existing structured learning algorithms. This framework provides theoretical justification for several existing heuristic approaches found in literature. We evaluate our proposed adaptive system on two structured prediction tasks, optical character recognition (OCR) and dependency parsing and show strong performance in reduction of the feature costs without degrading accuracy."
VENKATESH SALIGRAMA,Surprisingly simple semi-supervised domain adaptation with pretraining and consistency,"Most modern unsupervised domain adaptation (UDA) approaches are rooted in domain alignment, i.e., learning to align source and target features to learn a target domain classifier using source labels. In semi-supervised domain adaptation (SSDA), when the learner can access few target domain labels, prior approaches have followed UDA theory to use domain alignment for learning. We show that the case of SSDA is different and a good target classifier can be learned without needing alignment. We use self-supervised pretraining (via rotation prediction) and consistency regularization to achieve well separated target clusters, aiding in learning a low error target classifier. With our Pretraining and Consistency (PAC) approach, we achieve state of the art target accuracy on this semi-supervised domain adaptation task, surpassing multiple adversarial domain alignment methods, across multiple datasets. PAC, while using simple techniques, performs remarkably well on large and challenging SSDA benchmarks like DomainNet and Visda-17, often outperforming recent state of the art by sizeable margins. Code for our experiments can be found at https://github.com/venkatesh-saligrama/PAC."
VENKATESH SALIGRAMA,Crowdsourcing with sparsely interacting workers,"We consider estimation of worker skills from worker-task interaction data (with unknown labels) for the single-coin crowd-sourcing binary classification model in symmetric noise. We define the (worker) interaction graph whose nodes are workers and an edge between two nodes indicates whether or not the two workers participated in a common task. We show that skills are asymptotically identifiable if and only if an appropriate limiting version of the interaction graph is irreducible and has odd-cycles. We then formulate a weighted rank-one optimization problem to estimate skills based on observations on an irreducible, aperiodic interaction graph. We propose a gradient descent scheme and show that for such interaction graphs estimates converge asymptotically to the global minimum. We characterize noise robustness of the gradient scheme in terms of spectral properties of signless Laplacians of the interaction graph. We then demonstrate that a plug-in estimator based on the estimated skills achieves state-of-art performance on a number of real-world datasets. Our results have implications for rank-one matrix completion problem in that gradient descent can provably recover W×W rank-one matrices based on W+1 off-diagonal observations of a connected graph with a single odd-cycle."
VENKATESH SALIGRAMA,On the non-existence of unbiased estimators in constrained estimation problems,"We address the problem of existence of unbiased constrained parameter estimators. We show that if the constrained set of parameters is compact and the hypothesized distributions are absolutely continuous with respect to one another, then there exists no unbiased estimator. Weaker conditions for the absence of unbiased constrained estimators are also specified. We provide several examples which demonstrate the utility of these conditions."
VENKATESH SALIGRAMA,How transferable are video representations based on synthetic data?,
VENKATESH SALIGRAMA,Gradient descent for sparse rank-one matrix completion for crowd-sourced aggregation of sparsely interacting workers,"We consider worker skill estimation for the singlecoin Dawid-Skene crowdsourcing model. In practice skill-estimation is challenging because worker assignments are sparse and irregular due to the arbitrary, and uncontrolled availability of workers. We formulate skill estimation as a rank-one correlation-matrix completion problem, where the observed components correspond to observed label correlation between workers. We show that the correlation matrix can be successfully recovered and skills identifiable if and only if the sampling matrix (observed components) is irreducible and aperiodic. We then propose an efficient gradient descent scheme and show that skill estimates converges to the desired global optima for such sampling matrices. Our proof is original and the results are surprising in light of the fact that even the weighted rank-one matrix factorization problem is NP hard in general. Next we derive sample complexity bounds for the noisy case in terms of spectral properties of the signless Laplacian of the sampling matrix. Our proposed scheme achieves state-of-art performance on a number of real-world datasets."
VENKATESH SALIGRAMA,Time adaptive recurrent neural network,"We propose a learning method that, dynamically modi- fies the time-constants of the continuous-time counterpart of a vanilla RNN. The time-constants are modified based on the current observation and hidden state. Our proposal overcomes the issues of RNN trainability, by mitigating ex- ploding and vanishing gradient phenomena based on placing novel constraints on the parameter space, and by suppress- ing noise in inputs based on pondering over informative inputs to strengthen their contribution in the hidden state. As a result, our method is computationally efficient overcoming overheads of many existing methods that also attempt to improve RNN training. Our RNNs, despite being simpler and having light memory footprint, shows competitive per- formance against standard LSTMs and baseline RNN models on many benchmark datasets including those that require long-term memory."
VENKATESH SALIGRAMA,Sequential learning without feedback,"In many security and healthcare systems a sequence of features/sensors/tests are used for detection and diagnosis. Each test outputs a prediction of the latent state, and carries with it inherent costs. Our objective is to {\it learn} strategies for selecting tests to optimize accuracy \& costs. Unfortunately it is often impossible to acquire in-situ ground truth annotations and we are left with the problem of unsupervised sensor selection (USS). We pose USS as a version of stochastic partial monitoring problem with an {\it unusual} reward structure (even noisy annotations are unavailable). Unsurprisingly no learner can achieve sublinear regret without further assumptions. To this end we propose the notion of weak-dominance. This is a condition on the joint probability distribution of test outputs and latent state and says that whenever a test is accurate on an example, a later test in the sequence is likely to be accurate as well. We empirically verify that weak dominance holds on real datasets and prove that it is a maximal condition for achieving sublinear regret. We reduce USS to a special case of multi-armed bandit problem with side information and develop polynomial time algorithms that achieve sublinear regret."
VENKATESH SALIGRAMA,Federated learning based on dynamic regularization,
VENKATESH SALIGRAMA,Learning joint feature adaptation for zero-shot recognition,"Zero-shot recognition (ZSR) aims to recognize target-domain data instances of unseen classes based on the models learned from associated pairs of seen-class source and target domain data. One of the key challenges in ZSR is the relative scarcity of source-domain features (e.g. one feature vector per class), which do not fully account for wide variability in target-domain instances. In this paper we propose a novel framework of learning data-dependent feature transforms for scoring similarity between an arbitrary pair of source and target data instances to account for the wide variability in target domain. Our proposed approach is based on optimizing over a parameterized family of local feature displacements that maximize the source-target adaptive similarity functions. Accordingly we propose formulating zero-shot learning (ZSL) using latent structural SVMs to learn our similarity functions from training data. As demonstration we design a specific algorithm under the proposed framework involving bilinear similarity functions and regularized least squares as penalties for feature displacement. We test our approach on several benchmark datasets for ZSR and show significant improvement over the state-of-the-art. For instance, on aP&Y dataset we can achieve 80.89% in terms of recognition accuracy, outperforming the state-of-the-art by 11.15%."
VENKATESH SALIGRAMA,Training recurrent neural networks via forward propagation through time,"Back-propagation through time (BPTT) has been widely used for training Recurrent Neural Networks (RNNs). BPTT updates RNN parameters on an instance by back-propagating the error in time over the entire sequence length, and as a result, leads to poor trainability due to the well-known gradient explosion/decay phenomena. While a number of prior works have proposed to mitigate vanishing/explosion effect through careful RNN architecture design, these RNN variants still train with BPTT.We propose a novel forwardpropagation algorithm, FPTT , where at each time, for an instance, we update RNN parameters by optimizing an instantaneous risk function. Our proposed risk is a regularization penalty at time t that evolves dynamically based on previously observed losses, and allows for RNN parameter updates to converge to a stationary solution of the empirical RNN objective. We consider both sequence-to-sequence as well as terminal loss problems. Empirically FPTT outperforms BPTT on a number of well-known benchmark tasks, thus enabling architectures like LSTMs to solve long range dependencies problems."
VENKATESH SALIGRAMA,On the non-existence of unbiased estimators in constrained estimation problems,"We address the problem of existence of unbiased constrained parameter estimators. We show that if the constrained set of parameters is compact and the hypothesized distributions are absolutely continuous with respect to one another, then there exists no unbiased estimator.Weaker conditions for the absence of unbiased constrained estimators are also specified. We provide several examples which demonstrate the utility of these conditions."
VENKATESH SALIGRAMA,Sequential optimization for efficient high-quality object proposal generation,"We are motivated by the need for a generic object proposal generation algorithm which achieves good balance between object detection recall, proposal localization quality and computational efficiency. We propose a novel object proposal algorithm, BING ++, which inherits the virtue of good computational efficiency of BING [1] but significantly improves its proposal localization quality. At high level we formulate the problem of object proposal generation from a novel probabilistic perspective, based on which our BING++ manages to improve the localization quality by employing edges and segments to estimate object boundaries and update the proposals sequentially. We propose learning the parameters efficiently by searching for approximate solutions in a quantized parameter space for complexity reduction. We demonstrate the generalization of BING++ with the same fixed parameters across different object classes and datasets. Empirically our BING++ can run at half speed of BING on CPU, but significantly improve the localization quality by 18.5 and 16.7 percent on both VOC2007 and Microhsoft COCO datasets, respectively. Compared with other state-of-the-art approaches, BING++ can achieve comparable performance, but run significantly faster."
VENKATESH SALIGRAMA,Clustering and community detection with imbalanced clusters,"Spectral clustering methods that are frequently used in clustering and community detection applications are sensitive to the specific graph constructions particularly when imbalanced clusters are present. We show that ratio cut (RCut) or normalized cut (NCut) objectives are not tailored to imbalanced cluster sizes since they tend to emphasize cut sizes over cut values. We propose a graph partitioning problem that seeks minimum cut partitions under minimum size constraints on partitions to deal with imbalanced cluster sizes. Our approach parameterizes a family of graphs by adaptively modulating node degrees on a fixed node set, yielding a set of parameter dependent cuts reflecting varying levels of imbalance. The solution to our problem is then obtained by optimizing over these parameters.We present rigorous limit cut analysis results to justify our approach and demonstrate the superiority of our method through experiments on synthetic and real datasets for data clustering, semisupervised learning and community detection."
VENKATESH SALIGRAMA,Multi-stage classifier design,
VENKATESH SALIGRAMA,Piecewise linear regression via a difference of convex functions,"We present a new piecewise linear regression methodology that utilizes fitting a difference of convex functions (DC functions) to the data. These are functions f that may be represented as the difference 𝜙_1- 𝜙_2 for a choice of convex functions 𝜙_1,𝜙_2. The method proceeds by estimating piecewise-liner convex functions, in a manner similar to max-affine regression, whose difference approximates the data. The choice of the function is regularised by a new seminorm over the class of DC functions that controls the 𝓁_∞ Lipschitz constant of the estimate. The resulting methodology can be efficiently implemented via Quadratic programming even in high dimensions, and is shown to have close to minimax statistical risk. We empirically validate the method, showing it to be practically implementable, and to have comparable performance to existing egression/classification methods on real-world datasets."
VENKATESH SALIGRAMA,Online algorithm for unsupervised sequential selection with contextual information,"In this paper, we study Contextual Unsupervised Sequential Selection (USS), a new variant of the stochastic contextual bandits problem where the loss of an arm cannot be inferred from the observed feedback. In our setup, arms are associated with fixed costs and are ordered, forming a cascade. In each round, a context is presented, and the learner selects the arms sequentially till some depth. The total cost incurred by stopping at an arm is the sum of fixed costs of arms selected and the stochastic loss associated with the arm. The learner’s goal is to learn a decision rule that maps contexts to arms with the goal of minimizing the total expected loss. The problem is challenging as we are faced with an unsupervised setting as the total loss cannot be estimated. Clearly, learning is feasible only if the optimal arm can be inferred (explicitly or implicitly) from the problem structure. We observe that learning is still possible when the problem instance satisfies the so-called ‘Contextual Weak Dominance’ (CWD) property. Under CWD, we propose an algorithm for the contextual USS problem and demonstrate that it has sub-linear regret. Experiments on synthetic and real datasets validate our algorithm."
VENKATESH SALIGRAMA,RNNs evolving on an equilibrium manifold: a panacea for vanishing and exploding gradients?,"Recurrent neural networks (RNNs) are particularly well-suited for modeling longterm dependencies in sequential data, but are notoriously hard to train because the error backpropagated in time either vanishes or explodes at an exponential rate. While a number of works attempt to mitigate this effect through gated recurrent units, skip-connections, parametric constraints and design choices, we propose a novel incremental RNN (iRNN), where hidden state vectors keep track of incremental changes, and as such approximate state-vector increments of Rosenblatt’s (1962) continuous-time RNNs. iRNN exhibits identity gradients and is able to account for long-term dependencies (LTD). We show that our method is computationally efficient overcoming overheads of many existing methods that attempt to improve RNN training, while suffering no performance degradation. We demonstrate the utility of our approach with extensive experiments and show competitive performance against standard LSTMs on LTD and other non-LTD tasks."
VENKATESH SALIGRAMA,Unique contributions of parvalbumin and cholinergic interneurons in organizing striatal networks during movement,"Striatal pavalbumin (PV) and cholinergic (CHI) interneurons are poised to play major roles in behavior by coordinating the networks of medium spiny cells that relay motor output. However, the small numbers and scattered distribution of these cells has made it difficult to directly assess their contribution to activity in networks of MSNs during behavior. Here, we build upon recent improvements in single cell calcium imaging combined with optogenetics to test the capacity of PVs and CHIs to affect MSN activity and behavior in mice engaged in voluntarily locomotion. We find that PVs and CHIs have unique effects on MSN activity and dissociable roles in supporting movement. PV cells facilitate movement by refining the activation of MSN networks responsible for movement execution. CHIs, in contrast, synchronize activity within MSN networks to signal the end of a movement bout. These results provide new insights into the striatal network activity that supports movement."
VENKATESH SALIGRAMA,Quantifying and reducing stereotypes in word embeddings,"Machine learning algorithms are optimized to model statistical properties of the training data. If the input data reflects stereotypes and biases of the broader society, then the output of the learning algorithm also captures these stereotypes. In this paper, we initiate the study of gender stereotypes in word embedding, a popular framework to represent text data. As their use becomes increasingly common, applications can inadvertently amplify unwanted stereotypes. We show across multiple datasets that the embeddings contain significant gender stereotypes, especially with regard to professions. We created a novel gender analogy task and combined it with crowdsourcing to systematically quantify the gender bias in a given embedding. We developed an efficient algorithm that reduces gender stereotype using just a handful of training examples while preserving the useful geometric properties of the embedding. We evaluated our algorithm on several metrics. While we focus on male/female stereotypes, our framework may be applicable to other types of embedding biases."
VENKATESH SALIGRAMA,Surprisingly simple semi-supervised domain adaptation with pretraining and consistency,"Visual domain adaptation involves learning to classify images from a target visual domain using labels available in a different source domain. A range of prior work uses adversarial domain alignment to try and learn a domain invariant feature space, where a good source classifier can perform well on target data. This however, can lead to errors where class A features in the target domain get aligned to class B features in source. We show that in the presence of a few target labels, simple techniques like selfsupervision (via rotation prediction) and consistency regularization can be effective without any adversarial alignment to learn a good target classifier. Our Pretraining and Consistency (PAC) approach, can achieve state of the art accuracy on this semi-supervised domain adaptation task, surpassing multiple adversarial domain alignment methods, across multiple datasets. Notably, it outperforms all recent approaches by 3-5% on the large and challenging DomainNet benchmark, showing the strength of these simple techniques in fixing errors made by adversarial alignment"
VENKATESH SALIGRAMA,Early detection of lung cancer using nano-nose-a review,"Lung cancer is one of the malignancies causing deaths worldwide. The yet to be developed non-invasive diagnostic techniques, are a challenge for early detection of cancer before it progresses to its later stages. The currently available diagnostic methods are expensive or invasive, and are not fit for general screening purposes. Early identification not only helps in detecting primary cancer, but also in treating its secondaries; which creates a need for easily applicable tests to screen individuals at risk. A detailed review of the various screening methods, including the latest trend of breath analysis using gold nanoparticles, to identify cancer at its early stage, are studied here. The VOC based breath biomarkers are used to analyze the exhaled breath of the patients. These biomarkers are utilized by Chemiresistors coated with gold nanoparticles, which are found to be the most suited technique for early detection of lung cancer. This technique is highly accurate and is relatively easy to operate and was tested on smokers and non-smokers. This review also gives as an outline of the fabrication and working of the device Na-Nose. The Chemiresistors coated with Gold nanoparticles, show a great potential in being an non-invasive and cost-effective diagnostic technique for early detection of lung cancer."
VENKATESH SALIGRAMA,Graph resistance and learning from pairwise comparisons,"We consider the problem of learning the qualities of a collection of items by performing noisy comparisons among them. Following the standard paradigm, we assume there is a fixed “comparison graph” and every neighboring pair of items in this graph is compared k times according to the Bradley-Terry-Luce model (where the probability than an item wins a comparison is proportional the item quality). We are interested in how the relative error in quality estimation scales with the comparison graph in the regime where k is large. We show that, asymptotically, the relevant graph-theoretic quantity is the square root of the resistance of the comparison graph. Specifically, we provide an algorithm with relative error decay that scales with the square root of the graph resistance, and provide a lower bound showing that (up to log factors) a better scaling is impossible. The performance guarantee of our algorithm, both in terms of the graph and the skewness of the item quality distribution, significantly outperforms earlier results."
VENKATESH SALIGRAMA,Cost aware Inference for IoT Devices,"Networked embedded devices (IoTs) of limitedCPU, memory and power resources are revo-lutionizing data gathering, remote monitoringand planning in many consumer and businessapplications. Nevertheless, resource limita-tions place a significant burden on their ser-vice life and operation, warranting cost-awaremethods that are capable of distributivelyscreening redundancies in device informationand transmitting informative data. We pro-pose to train a decentralized gated networkthat, given an observed instance at test-time,allows for activation of select devices to trans-mit information to a central node, which thenperforms inference. We analyze our proposedgradient descent algorithm for Gaussian fea-tures and establish convergence guaranteesunder good initialization. We conduct exper-iments on a number of real-world datasetsarising in IoT applications and show that ourmodel results in over 1.5X service life withnegligible accuracy degradation relative to aperformance achievable by a neural network."
VENKATESH SALIGRAMA,Connected subgraph detection with mirror descent on SDPs,"We propose a novel, computationally efficient mirror-descent based optimization framework for subgraph detection in graph-structured data. Our aim is to discover anomalous patterns present in a connected subgraph of a given graph. This problem arises in many applications such as detection of network intrusions, community detection, detection of anomalous events in surveillance videos or disease outbreaks. Since optimization over connected subgraphs is a combinatorial and computationally difficult problem, we propose a convex relaxation that offers a principled approach to incorporating connectivity and conductance constraints on candidate subgraphs. We develop a novel efficient algorithm to solve the relaxed problem, establish convergence guarantees and demonstrate its feasibility and performance with experiments on real and very large simulated networks."
VENKATESH SALIGRAMA,Man is to computer programmer as woman is to homemaker? debiasing word embeddings,"The blind application of machine learning runs the risk of amplifying biases present in data. Such a danger is facing us with word embedding, a popular framework to represent text data as vectors which has been used in many machine learning and natural language processing tasks. We show that even word embeddings trained on Google News articles exhibit female/male gender stereotypes to a disturbing extent. This raises concerns because their widespread use, as we describe, often tends to amplify these biases. Geometrically, gender bias is first shown to be captured by a direction in the word embedding. Second, gender neutral words are shown to be linearly separable from gender definition words in the word embedding. Using these properties, we provide a methodology for modifying an embedding to remove gender stereotypes, such as the association between the words receptionist and female, while maintaining desired associations such as between the words queen and female. Using crowd-worker evaluation as well as standard benchmarks, we empirically demonstrate that our algorithms significantly reduce gender bias in embeddings while preserving the its useful properties such as the ability to cluster related concepts and to solve analogy tasks. The resulting embeddings can be used in applications without amplifying gender bias."
VENKATESH SALIGRAMA,Remember the curse of dimensionality: the case of goodness-of-fit testing in arbitrary dimension,"Despite a substantial literature on nonparametric two-sample goodness-of-fit testing in arbitrary dimensions, there is no mention there of any curse of dimensionality. In fact, in some publications, a parametric rate is derived. As we discuss below, this is because a directional alternative is considered. Indeed, even in dimension one, Ingster, Y. I. [(1987). Minimax testing of nonparametric hypotheses on a distribution density in the l_p metrics. Theory of Probability & Its Applications, 31(2), 333–337] has shown that the minimax rate is not parametric. In this paper, we extend his results to arbitrary dimension and confirm that the minimax rate is not only nonparametric, exhibits but also a prototypical curse of dimensionality. We further extend Ingster's work to show that the chi-squared test achieves the minimax rate. Moreover, we show that the test adapts to the intrinsic dimensionality of the data. Finally, in the spirit of Ingster, Y. I. [(2000). Adaptive chi-square tests. Journal of Mathematical Sciences, 99(2), 1110–1119], we consider a multiscale version of the chi-square test, showing that one can adapt to unknown smoothness without much loss in power."
VENKATESH SALIGRAMA,Cheap bandits,"We consider stochastic sequential learning problems where the learner can observe the \textit{average reward of several actions}. Such a setting is interesting in many applications involving monitoring and surveillance, where the set of the actions to observe represent some (geographical) area. The importance of this setting is that in these applications, it is actually \textit{cheaper} to observe average reward of a group of actions rather than the reward of a single action. We show that when the reward is \textit{smooth} over a given graph representing the neighboring actions, we can maximize the cumulative reward of learning while \textit{minimizing the sensing cost}. In this paper we propose CheapUCB, an algorithm that matches the regret guarantees of the known algorithms for this setting and at the same time guarantees a linear cost again over them. As a by-product of our analysis, we establish a Ω(dT‾‾‾√) lower bound on the cumulative regret of spectral bandits for a class of graphs with effective dimension d."
VENKATESH SALIGRAMA,PRISM: Person Reidentification via Structured Matching,"Person reidentification (re-id), an emerging problem in visual surveillance, deals with maintaining the identities of individuals while they traverse various locations surveilled by a camera network. Motivated by real-world scenarios, we propose a method that seeks to simultaneously identify who among a group of individuals viewed in one view are present/absent in the other. From a visual perspective, re-id is challenging due to significant changes in visual appearance of individuals in cameras with different pose, illumination, and calibration. Globally, the challenge arises from the need to maintain structurally consistent matches among all the individual entities across different camera views. We propose person re-id via structured matching (PRISM), an SM method to jointly account for these challenges. We view the global problem as a weighted graph matching problem and estimate edge weights by learning to predict them based on the co-occurrences of visual patterns in the training examples. These co-occurrence-based scores in turn account for appearance changes by inferring likely and unlikely visual co-occurrences appearing in training instances. We implement PRISM on single-shot and multishot scenarios. PRISM uniformly outperforms state of the art in terms of matching rate while being robust and computationally efficient."
VENKATESH SALIGRAMA,RNN training along locally optimal trajectoriesvia Frank-Wolfe algorithm,"We propose a novel and efficient training method for RNNs by iteratively seeking a local minima on the loss surface within a small region, and leverage this directional vector for the update, in an outer-loop. We propose to utilize the Frank-Wolfe (FW) algorithm in this context. Although, FW implicitly involves normalized gradients, which can lead to a slow convergence rate, we develop a novel RNN training method that, surprisingly, even with the additional cost, the overall training cost is empirically observed to be lower than backpropagation. Our method leads to a new Frank-Wolfe method, that is in essence an SGD algorithm with a restart scheme. We prove that under certain conditions our algorithm has a sublinear convergence rate of O (1/ϵ) for ϵ error. We then conduct empirical experiments on several benchmark datasets including those that exhibit long-term dependencies, and show significant performance improvement. We also experiment with deep RNN architectures and show efficient training performance. Finally, we demonstrate that our training method is robust to noisy data."
VENKATESH SALIGRAMA,Limits on testing structural changes in Ising models,"We present novel information-theoretic limits on detecting sparse changes in Ising models, a problem that arises in many applications where network changes can occur due to some external stimuli. We show that the sample complexity for detecting sparse changes, in a minimax sense, is no better than learning the entire model even in settings with local sparsity. This is a surprising fact in light of prior work rooted in sparse recovery methods, which suggest that sample complexity in this context scales only with the number of network changes. To shed light on when change detection is easier than structured learning, we consider testing of edge deletion in forest-structured graphs, and high-temperature ferromagnets as case studies. We show for these that testing of small changes is similarly hard, but testing of \emph{large} changes is well-separated from structure learning. These results imply that testing of graphical models may not be amenable to concepts such as restricted strong convexity leveraged for sparsity pattern recovery, and algorithm development instead should be directed towards detection of large changes."
VENKATESH SALIGRAMA,Active hedge: hedge meets active learning,
VENKATESH SALIGRAMA,Behavior subtraction,"Background subtraction has been a driving engine for many computer vision and video analytics tasks. Although its many variants exist, they all share the underlying assumption that photometric scene properties are either static or exhibit temporal stationarity. While this works in many applications, the model fails when one is interested in discovering changes in scene dynamics instead of changes in scene's photometric properties; the detection of unusual pedestrian or motor traffic patterns are but two examples. We propose a new model and computational framework that assume the dynamics of a scene, not its photometry, to be stationary, i.e., a dynamic background serves as the reference for the dynamics of an observed scene. Central to our approach is the concept of an event, which we define as short-term scene dynamics captured over a time window at a specific spatial location in the camera field of view. Unlike in our earlier work, we compute events by time-aggregating vector object descriptors that can combine multiple features, such as object size, direction of movement, speed, etc. We characterize events probabilistically, but use low-memory, low-complexity surrogates in a practical implementation. Using these surrogates amounts to behavior subtraction, a new algorithm for effective and efficient temporal anomaly detection and localization. Behavior subtraction is resilient to spurious background motion, such as due to camera jitter, and is content-blind, i.e., it works equally well on humans, cars, animals, and other objects in both uncluttered and highly cluttered scenes. Clearly, treating video as a collection of events rather than colored pixels opens new possibilities for video analytics."
VENKATESH SALIGRAMA,Condensing CNNs with partial differential equations,
VENKATESH SALIGRAMA,Task2Sim: towards effective pre-training and transfer from synthetic data,
VENKATESH SALIGRAMA,Strategies for safe multi-armed bandits with logarithmic regret and risk,
VENKATESH SALIGRAMA,Faster algorithms for learning convex functions,
VENKATESH SALIGRAMA,Learning to approximate a Bregman divergence,"Bregman divergences generalize measures such as the squared Euclidean distance and the KL divergence, and arise throughout many areas of machine learning. In this paper, we focus on the problem of approximating an arbitrary Bregman divergence from supervision, and we provide a well-principled approach to analyzing such approximations. We develop a formulation and algorithm for learning arbitrary Bregman divergences based on approximating their underlying convex generating function via a piecewise linear function. We provide theoretical approximation bounds using our parameterization and show that the generalization error Op(m^-1/2) for metric learning using our framework matches the known generalization error in the strictly less general Mahalanobis metric learning setting. We further demonstrate empirically that our method performs well in comparison to existing metric learning methods, particularly for clustering and ranking problems."
VENKATESH SALIGRAMA,Learning to drive anywhere,"Human drivers can seamlessly adapt their driving decisions across geographical locations with diverse conditions and rules of the road, e.g., left vs. right-hand traffic. In contrast, existing models for autonomous driving have been thus far only deployed within restricted operational domains, i.e., without accounting for varying driving behaviors across locations or model scalability. In this work, we propose AnyD, a single geographically-aware conditional imitation learning (CIL) model that can efficiently learn from heterogeneous and globally distributed data with dynamic environmental, traffic, and social characteristics. Our key insight is to introduce a high-capacity geo-location-based channel attention mechanism that effectively adapts to local nuances while also flexibly modeling similarities among regions in a data-driven manner. By optimizing a contrastive imitation objective, our proposed approach can efficiently scale across the inherently imbalanced data distributions and location-dependent events. We demonstrate the benefits of our AnyD agent across multiple datasets, cities, and scalable deployment paradigms, i.e., centralized, semi-supervised, and distributed agent training. Specifically, AnyD outperforms CIL baselines by over 14% in open-loop evaluation and 30% in closed-loop testing on CARLA."
VENKATESH SALIGRAMA,Faster algorithms for learning convex functions,
VENKATESH SALIGRAMA,Scaffolding a student to instill knowledge,"We propose a novel knowledge distillation (KD) method to selectively instill teacher knowledge into a student model motivated by situations where the student’s capacity is significantly smaller than that of the teachers. In vanilla KD, the teacher primarily sets a predictive target for the student to follow, and we posit that this target is overly optimistic due to the student’s lack of capacity. We develop a novel scaffolding scheme where the teacher, in addition to setting a predictive target, also scaffolds the student’s prediction by censoring hard-to-learn examples. The student model utilizes the same information as the teacher’s soft-max predictions as inputs, and in this sense, our proposal can be viewed as a natural variant of vanilla KD. We show on synthetic examples that censoring hard-examples leads to smoothening the student’s loss landscape so that the student encounters fewer local minima. As a result, it has good generalization properties. Against vanilla KD, we achieve improved performance and are comparable to more intrusive techniques that leverage feature matching on benchmark datasets"
VENKATESH SALIGRAMA,SynCDR : training cross domain retrieval models with synthetic data,
VENKATESH SALIGRAMA,Efficient edge inference by selective query,"Edge devices provide inference on predictive tasks to many end-users. However, deploying deep neural networks that achieve state-of-the-art accuracy on these devices is infeasible due to edge resource constraints. Nevertheless, cloud-only processing, the de-facto standard, is also problematic, since uploading large amounts of data imposes severe communication bottlenecks. We propose a novel end-to-end hybrid learning framework that allows the edge to selectively query only those hard examples that the cloud can classify correctly. Our framework optimizes over neural architectures and trains edge predictors and routing models so that the overall accuracy remains high while minimizing the overall latency. Training a hybrid learner is difficult since we lack annotations of hard edge-examples. We introduce a novel proxy supervision in this context and show that our method adapts seamlessly and near optimally across different latency regimes. On the ImageNet dataset, our proposed method deployed on a micro-controller unit exhibits 25% reduction in latency compared to cloud-only processing while suffering no excess loss."
ROBERT P WELLER,"Salvaging silence: exile, death, and the anthropology of the unknowable",
ROBERT P WELLER,"Globalizing religious change and Chinese societies: Secularization, religionization, rationalization, and embodiment [全球宗教变迁与华人社会：世俗化、宗教化、理性化、与躯体化】",
ROBERT P WELLER,Overnight urbanization and changing spirits: disturbed ecosystems in Southern Jiangsu,"Three Chinese cases involving ghost attacks, the increase of spirit mediums, and innovations in the forms and objects of temple worship suggest how nonequilibrium ecology, broadly conceived, can clarify processes of urban change. They extend Ingold’s call for “ecologies of life” by clarifying how latent potentials become manifest and how new symbiotic assemblages can be created in disturbed ecosystems. These cases arise from the rapid urban expansion in wealthy parts of China, accompanied by the resettlement of many villagers into high-rise buildings, wiping out farms, village temples, and rural graves and making earlier forms of social organization impossible. The territorially based religion described in much of the anthropological and historical literature has thus become increasingly untenable. Contrary to many expectations, the expanding urban edge at our field sites in southern Jiangsu cities has fostered an especially creative zone of innovation."
ROBERT P WELLER,"Censorship, foreclosure, and the three deaths of Fengzhen","This article draws on Judith Butler’s distinction between censorship and foreclosure, and on Saidiya Hartman’s work about how to narrate the silences of the slave trade, to explore two photographs. The first is a dismembered and reassembled family photograph that suggests a distinction between present absences and totally absent ones. The second opens up the case of the three deaths of the goddess Fengzhen caused by China’s very rapid urbanization: first as a woman, then as a deity’s statue-body, and finally as the photographic center of a ritual. In both photographs the silences of censorship and foreclosure create forms of haunting that help reveal their different structures of power. The focus on the haunting power of the silenced also shows the importance of adding the nondiscursive world to the more discourse-centered analysis of Butler and Hartman. The discussion emphasizes the difficulties of writing about the silences of censorship and foreclosure without breaking them, and suggests some possibilities through invocation, evocation, and a bypassing of the “archive” through the continuing presence of the absent."
ROBERT P WELLER,Goddess unbound: Chinese popular religion and the varieties of boundary,"Mazu is an important deity who spread widely within and beyond China. The hardening of internal and external boundaries during the Cold War greatly limited the flow of the cult on the mainland, and completely cut the tie to temples in Taiwan and abroad. The end of the Cold War, however, brought many new possibilities, which are best understood by opening up the concept of ""boundary."" The Cold War had strengthened the idea that borders are meant to be unambiguous and well defended. This vision of the boundary as a brick wall, however, is incomplete. This essay explores two further aspects of boundaries: the oozing of people, goods and deities through pores in the boundaries (more cell wall than brick wall); and the boundaries that are crossed through the rhythms of ritual, fostering moves back and forth across both political and spiritual lines (a ""tennis net"" wall, crossed by the moving ball)."
ROBERT P WELLER,On the boundaries between good and evil: Constructing multiple moralities in China,"This essay discusses three contrasting versions of the relationship between good and evil in contemporary China: a spirit medium who maneuvers between them, a charismatic Christian group that forges an identity by defending the border between them, and an official state and religious discourse of banal goodness and universal love that that seeks to annihilate evil. Each defines good and evil differently, but more importantly, each imagines the nature of the boundary itself differently – as permeable and negotiable, clear and defensible, or simply intolerable. These varied conceptions help to shape alternate views of empathy, pluralism, and the problem of how to live with otherness."
ROBERT P WELLER,Global religious changes and civil life in two Chinese societies: a comparison of Jiangsu and Taiwan,
ROBERT P WELLER,The future of North American trade policy: lessons from NAFTA,"This Task Force Report written by an international group of trade policy experts calls for significant reforms to address adverse economic, environmental, labor and societal impacts created by the 1994 North American Free Trade Agreement (NAFTA). The report is intended to contribute to the discussion and decisions stemming from ongoing reviews of proposed reforms to NAFTA as well as to help shape future trade agreements. It offers detailed proposals on topics including services, manufacturing, agriculture, investment, intellectual property, labor, environment, and migration. Fifteen years after NAFTA was enacted, there is widespread agreement that the trade treaty among the United States, Canada and Mexico has fallen short of its stated goals. While proponents credit the agreement with stimulating the flow of goods, services, and investment among the North American countries, critics in all three countries argue that this has not brought improvements in the standards of living of most people. Rather than triggering a convergence across the three nations, NAFTA has accentuated the economic and regulatory asymmetries that had existed among the three countries. [TRUNCATED]"
ROBERT P WELLER,"Respecting silence: longing, rhythm, and Chinese temples in an age of bulldozers","This essay distinguishes the silence that makes rhythm (and thus ritual) possible, and the silence of loss and longing. It argues that both, as they intertwine, are crucial parts of the adjustment to traumatic change. The interaction between these two kinds of silence offers an alternative to theories that focus primarily on speaking as a way of overcoming trauma, or on silence as antisocial. The ethnographic evidence comes from a surgical case that illustrates the basic approach, followed by a case of rapid urbanization on the outskirts of a large Chinese city, involving the resettlement of 100,000 people. Both cases show the two kinds of silence as they resonate with each other. The analysis argues that silence is not just the absence of sound, but a necessary part of all the rhythms of life, not replaced but invoked by speech."
ROBERT P WELLER,Symposium: the achievement of David Martin,This brief introduction notes some of the salient aspects of David Martin’s career and thought. It further presents and frames the following eight essays in this symposium devoted to different aspects of David Martin’s work.
ROBERT P WELLER,David Martin and the sociology of hope,"David Martin’s work has always bridged many worlds: the sacred and the secular, the world of power politics and of religious visions, of individual and society, and of poetry and rational analysis. His trenchant and uncompromising analyses of human social formations and their ideational concomitants have nevertheless provided many with a vision of that hope which must sustain scholarly analysis if it is not to become tedious and moribund. His sensitivity to tradition, to ritual, to received knowledge and the debt we owe to the past – even while appreciating the frisson of the radically new (as in his studies of Pentacostalism) – have made him one of only a small handful of scholars who could address the broad range of human religious expression and its implications for life in the world. This paper explores some of these themes in terms of what we understand as the overwhelming sense of hope that is a permanent feature of David’s scholarly contributions."
DINA CASTRO,"Equity is quality, quality is equity. Operationalizing equity in quality rating and improvement systems",
JOHN MATTHEWS,First M87 Event Horizon Telescope results. III. Data processing and calibration,"We present the calibration and reduction of Event Horizon Telescope (EHT) 1.3 mm radio wavelength observations of the supermassive black hole candidate at the center of the radio galaxy M87 and the quasar 3C 279, taken during the 2017 April 5–11 observing campaign. These global very long baseline interferometric observations include for the first time the highly sensitive Atacama Large Millimeter/submillimeter Array (ALMA); reaching an angular resolution of 25 μas, with characteristic sensitivity limits of ~1 mJy on baselines to ALMA and ~10 mJy on other baselines. The observations present challenges for existing data processing tools, arising from the rapid atmospheric phase fluctuations, wide recording bandwidth, and highly heterogeneous array. In response, we developed three independent pipelines for phase calibration and fringe detection, each tailored to the specific needs of the EHT. The final data products include calibrated total intensity amplitude and phase information. They are validated through a series of quality assurance tests that show consistency across pipelines and set limits on baseline systematic errors of 2% in amplitude and 1° in phase. The M87 data reveal the presence of two nulls in correlated flux density at ~3.4 and ~8.3 Gλ and temporal evolution in closure quantities, indicating intrinsic variability of compact structure on a timescale of days, or several light-crossing times for a few billion solar-mass black hole. These measurements provide the first opportunity to image horizon-scale structure in M87."
JOHN MATTHEWS,First M87 Event Horizon Telescope results. V. Physical origin of the asymmetric ring,"The Event Horizon Telescope (EHT) has mapped the central compact radio source of the elliptical galaxy M87 at 1.3 mm with unprecedented angular resolution. Here we consider the physical implications of the asymmetric ring seen in the 2017 EHT data. To this end, we construct a large library of models based on general relativistic magnetohydrodynamic (GRMHD) simulations and synthetic images produced by general relativistic ray tracing. We compare the observed visibilities with this library and confirm that the asymmetric ring is consistent with earlier predictions of strong gravitational lensing of synchrotron emission from a hot plasma orbiting near the black hole event horizon. The ring radius and ring asymmetry depend on black hole mass and spin, respectively, and both are therefore expected to be stable when observed in future EHT campaigns. Overall, the observed image is consistent with expectations for the shadow of a spinning Kerr black hole as predicted by general relativity. If the black hole spin and M87's large scale jet are aligned, then the black hole spin vector is pointed away from Earth. Models in our library of non-spinning black holes are inconsistent with the observations as they do not produce sufficiently powerful jets. At the same time, in those models that produce a sufficiently powerful jet, the latter is powered by extraction of black hole spin energy through mechanisms akin to the Blandford-Znajek process. We briefly consider alternatives to a black hole for the central compact object. Analysis of existing EHT polarization data and data taken simultaneously at other wavelengths will soon enable new tests of the GRMHD models, as will future EHT campaigns at 230 and 345 GHz."
JOHN MATTHEWS,First M87 Event Horizon Telescope results. VI. The shadow and mass of the central black hole,"We present measurements of the properties of the central radio source in M87 using Event Horizon Telescope data obtained during the 2017 campaign. We develop and fit geometric crescent models (asymmetric rings with interior brightness depressions) using two independent sampling algorithms that consider distinct representations of the visibility data. We show that the crescent family of models is statistically preferred over other comparably complex geometric models that we explore. We calibrate the geometric model parameters using general relativistic magnetohydrodynamic (GRMHD) models of the emission region and estimate physical properties of the source. We further fit images generated from GRMHD models directly to the data. We compare the derived emission region and black hole parameters from these analyses with those recovered from reconstructed images. There is a remarkable consistency among all methods and data sets. We find that >50% of the total flux at arcsecond scales comes from near the horizon, and that the emission is dramatically suppressed interior to this region by a factor >10, providing direct evidence of the predicted shadow of a black hole. Across all methods, we measure a crescent diameter of 42 ± 3 μas and constrain its fractional width to be <0.5. Associating the crescent feature with the emission surrounding the black hole shadow, we infer an angular gravitational radius of GM/Dc^2 = 3.8 ± 0.4 μas. Folding in a distance measurement of {16.8}_{-0.7}^{+0.8}{Mpc} gives a black hole mass of M = 6.5 ± 0.2{| }_{stat} ± 0.7{| }_{sys} × {10}^{9} {M}_{odot }. This measurement from lensed emission near the event horizon is consistent with the presence of a central Kerr black hole, as predicted by the general theory of relativity."
JOHN MATTHEWS,Shared and distinct transcriptomic cell types across neocortical areas,"The neocortex contains a multitude of cell types that are segregated into layers and functionally distinct areas. To investigate the diversity of cell types across the mouse neocortex, here we analysed 23,822 cells from two areas at distant poles of the mouse neocortex: the primary visual cortex and the anterior lateral motor cortex. We define 133 transcriptomic cell types by deep, single-cell RNA sequencing. Nearly all types of GABA (γ-aminobutyric acid)-containing neurons are shared across both areas, whereas most types of glutamatergic neurons were found in one of the two areas. By combining single-cell RNA sequencing and retrograde labelling, we match transcriptomic types of glutamatergic neurons to their long-range projection specificity. Our study establishes a combined transcriptomic and projectional taxonomy of cortical cell types from functionally distinct areas of the adult mouse cortex."
JOHN MATTHEWS,How do master football coaches develop team confidence?: a study of strategies and conceptualizations in the psychology of collective-efficacy,"Bandura (1986,1997) asserts that a group's belief in its members' co-joint abilities, or its collective-efficacy, influences the degree to which that group seeks challenging goals, puts forth effort, and persists in the face of adversity. Group leaders serve important functions in the development of successful groups (Yalom, 1995). Since successful coaches are able to consistently demonstrate the ability to mold a group of individuals into a winning team, it is important to understand what methods coaches employ to develop team confidence. The purpose of this study was to understand how master football coaches develop team confidence. The participants for this interview-based, qualitative study included twenty ""master"" football coaches (6 professional and 14 collegiate). Criteria for inclusion were as follows: each participant had been a head football coach for at least ten years, and had a consistent record of success. Seventeen of the twenty had achieved success with three or more different teams. The findings reveal that these coaches employ a wealth of psychological strategies in different situations to enhance the development of team confidence. Their selective deployment of these strategies takes place throughout a series of developmental tasks, here described as the ""Team Confidence Cycle."" This includes seven key tasks: 1. Set the Course, 2. Create a Confidence Environment, 3. Promote Mastery, 4. Get Them to Perform, 5. Assess Performance, 6. Stay the Course and 7. Maintain High Performance. In the interviews the coaches revealed that team confidence was essential to their view of how teams achieve success. The constructs of team confidence and success were considered closely intertwined. Promoting mastery experiences, therefore, was primary among those strategies used by the master coaches to build team confidence. A second key strategy was that they pointed out successful experiences to their team(s). These coaches thus placed the greatest importance on ""demonstrating ability"" and then ensuring that improvement was noted. These findings are in accordance with Bandura (1997). Implications for coaches, especially of youth sport, are outlined in the final chapter."
JOHN MATTHEWS,Genetic variation and gene expression across multiple tissues and developmental stages in a nonhuman primate,"By analyzing multitissue gene expression and genome-wide genetic variation data in samples from a vervet monkey pedigree, we generated a transcriptome resource and produced the first catalog of expression quantitative trait loci (eQTLs) in a nonhuman primate model. This catalog contains more genome-wide significant eQTLs per sample than comparable human resources and identifies sex- and age-related expression patterns. Findings include a master regulatory locus that likely has a role in immune function and a locus regulating hippocampal long noncoding RNAs (lncRNAs), whose expression correlates with hippocampal volume. This resource will facilitate genetic investigation of quantitative traits, including brain and behavioral phenotypes relevant to neuropsychiatric disorders."
JOHN MATTHEWS,"Journal of African Christian Biography: v. 8, no. 4","[This issue of the Journal of African Christian Biography highlights some of the entries in the DACB that profile participants in the twentieth-century ecumenical movement in southern Africa. The overwhelming impression one gets of this subject is that of gaps: there is urgent need for more entries that address the myriad ways in which African Christian leaders engaged the ecumenical movement as a network through which to build social capital during the critical period after the Second World War. As African nations became independent of European colonial control, church-educated leaders often acted as global spokesmen for postcolonial visions of society. They cultivated international support structures and led regional independence movements. Ecumenical networks played crucial roles in maintaining structures for education and peace-building in conflictive situations. Nelson Mandela himself, for example, attended Healdtown, a Methodist mission that became the largest high school in the country and educated many of the most important black nationalist leaders at mid century. The entries highlighted in this issue are the tip of the iceberg of what needs to be researched and written. This issue, then, appeals for scholars and church leaders to step up and to provide biographies of “ecumenists”—those who located their commitment to the Body of Christ in an international vision of peace, equality, and justice, in collaboration with other Christians from across Africa and around the world, as well as those who worked at the local level of cooperative church movements.]"
JOHN MATTHEWS,Transcription factor binding to Caenorhabditis elegans first introns reveals lack of redundancy with gene promoters,"Gene expression is controlled through the binding of transcription factors (TFs) to regulatory genomic regions. First introns are longer than other introns in multiple eukaryotic species and are under selective constraint. Here we explore the importance of first introns in TF binding in the nematode Caenorhabditis elegans by combining computational predictions and experimentally derived TF-DNA interaction data. We found that first introns of C. elegans genes, particularly those for families enriched in long first introns, are more conserved in length, have more conserved predicted TF interactions and are bound by more TFs than other introns. We detected a significant positive correlation between first intron size and the number of TF interactions obtained from chromatin immunoprecipitation assays or determined by yeast one-hybrid assays. TFs that bind first introns are largely different from those binding promoters, suggesting that the different interactions are complementary rather than redundant. By combining first intron and promoter interactions, we found that genes that share a large fraction of TF interactions are more likely to be co-expressed than when only TF interactions with promoters are considered. Altogether, our data suggest that C. elegans gene regulation may be additive through the combined effects of multiple regulatory regions."
JOHN MATTHEWS,Mapping and analysis of Caenorhabditis elegans transcription factor sequence specificities,"Caenorhabditis elegans is a powerful model for studying gene regulation, as it has a compact genome and a wealth of genomic tools. However, identification of regulatory elements has been limited, as DNA-binding motifs are known for only 71 of the estimated 763 sequencespecific transcription factors (TFs). To address this problem, we performed protein binding microarray experiments on representatives of canonical TF families in C. elegans, obtaining motifs for 129 TFs. Additionally, we predict motifs for many TFs that have DNA-binding domains similar to those already characterized, increasing coverage of binding specificities to 292 C. elegans TFs (∼40%). These data highlight the diversification of binding motifs for the nuclear hormone receptor and C2H2 zinc finger families and reveal unexpected diversity of motifs for T-box and DM families. Motif enrichment in promoters of functionally related genes is consistent with known biology and also identifies putative regulatory roles for unstudied TFs."
JOHN MATTHEWS,3'-UTR SIRF: A database for identifying clusters of short interspersed repeats in 3' untranslated regions,"BACKGROUND:Short (~5 nucleotides) interspersed repeats regulate several aspects of post-transcriptional gene expression. Previously we developed an algorithm (REPFIND) that assigns P-values to all repeated motifs in a given nucleic acid sequence and reliably identifies clusters of short CAC-containing motifs required for mRNA localization in Xenopus oocytes.DESCRIPTION:In order to facilitate the identification of genes possessing clusters of repeats that regulate post-transcriptional aspects of gene expression in mammalian genes, we used REPFIND to create a database of all repeated motifs in the 3' untranslated regions (UTR) of genes from the Mammalian Gene Collection (MGC). The MGC database includes seven vertebrate species: human, cow, rat, mouse and three non-mammalian vertebrate species. A web-based application was developed to search this database of repeated motifs to generate species-specific lists of genes containing specific classes of repeats in their 3'-UTRs. This computational tool is called 3'-UTR SIRF (Short Interspersed Repeat Finder), and it reveals that hundreds of human genes contain an abundance of short CAC-rich and CAG-rich repeats in their 3'-UTRs that are similar to those found in mRNAs localized to the neurites of neurons. We tested four candidate mRNAs for localization in rat hippocampal neurons by in situ hybridization. Our results show that two candidate CAC-rich (Syntaxin 1B and Tubulin beta4) and two candidate CAG-rich (Sec61alpha and Syntaxin 1A) mRNAs are localized to distal neurites, whereas two control mRNAs lacking repeated motifs in their 3'-UTR remain primarily in the cell body.CONCLUSION:Computational data generated with 3'-UTR SIRF indicate that hundreds of mammalian genes have an abundance of short CA-containing motifs that may direct mRNA localization in neurons. In situ hybridization shows that four candidate mRNAs are localized to distal neurites of cultured hippocampal neurons. These data suggest that short CA-containing motifs may be part of a widely utilized genetic code that regulates mRNA localization in vertebrate cells. The use of 3'-UTR SIRF to search for new classes of motifs that regulate other aspects of gene expression should yield important information in future studies addressing cis-regulatory information located in 3'-UTRs."
JOHN MATTHEWS,The Event Horizon general relativistic magnetohydrodynamic code comparison project,"Recent developments in compact object astrophysics, especially the discovery of merging neutron stars by LIGO, the imaging of the black hole in M87 by the Event Horizon Telescope, and high- precision astrometry of the Galactic Center at close to the event horizon scale by the GRAVITY experiment motivate the development of numerical source models that solve the equations of general relativistic magnetohydrodynamics (GRMHD). Here we compare GRMHD solutions for the evolution of a magnetized accretion flow where turbulence is promoted by the magnetorotational instability from a set of nine GRMHD codes: Athena++, BHAC, Cosmos++, ECHO, H-AMR, iharm3D, HARM-Noble, IllinoisGRMHD, and KORAL. Agreement among the codes improves as resolution increases, as measured by a consistently applied, specially developed set of code performance metrics. We conclude that the community of GRMHD codes is mature, capable, and consistent on these test problems."
JOHN MATTHEWS,"Oriental lore in the Tatler, Spectator, Guardian, and Freeholder of Addison and Steele",
JOHN MATTHEWS,Highly multiplexed label-free imaging sensor for accurate quantification of small-molecule binding kinetics,"Investigating the binding interaction of small molecules to large ligands is a compelling task for the field of drug development, as well as agro-biotechnology, since a common trait of drugs and toxins is often a low molecular weight (MW). Here, we improve the limit of detection of the Interferometric Reflectance Imaging Sensor (IRIS), a label-free, highly multiplexed biosensor, to perform small-molecule screening. In this work, characterization of small molecules binding to immobilized probes in a microarray format is demonstrated, with a limit of detection of 1 pg/mm2 in mass density. First, as a proof of concept to show the impact of spatial and temporal averaging on the system noise, detection of biotin (MW = 244.3 Da) binding to a streptavidin-functionalized chip is performed and the parameters are tuned to achieve maximum signal-to-noise ratio (SNR ≈ 34). The optimized system is then applied to the screening of a 20-multiplexed antibody chip against fumonisin B1 (MW = 721.8 Da), a mycotoxin found in cereal grains. The simultaneously recorded binding curves yield an SNR ≈ 8. Five out of twenty antibodies are also screened against the toxin in a lateral flow assay, obtaining consistent results. With the demonstrated noise characteristics, further sensitivity improvements are expected with the advancement of camera sensor technology."
JOHN MATTHEWS,Targeting oncoprotein translation with rocaglates in MYC-driven lymphoma,"MYC-driven lymphomas, especially those with concurrent MYC and BCL2 dysregulation, are currently a challenge in clinical practice due to rapid disease progression, resistance to standard chemotherapy and high risk of refractory disease. MYC plays a central role by coordinating hyperactive protein synthesis with upregulated transcription in order to support rapid proliferation of tumor cells. Translation initiation inhibitor rocaglates have been identified as the most potent drugs in MYC-driven lymphomas as they efficiently inhibit MYC expression and tumor cell viability. We found that this class of compounds can overcome eIF4A abundance by stabilizing target mRNA-eIF4A interaction that directly prevents translation. Proteome-wide quantification demonstrated selective repression of multiple critical oncoproteins in addition to MYC in B cell lymphoma including NEK2, MCL1, AURKA, PLK1, and several transcription factors that are generally considered undruggable. Finally, (−)-SDS-1–021, the most promising synthetic rocaglate, was confirmed to be highly potent as a single agent, and displayed significant synergy with the BCL2 inhibitor ABT199 in inhibiting tumor growth and survival in primary lymphoma cells in vitro and in patient-derived xenograft mouse models. Overall, our findings support the strategy of using rocaglates to target oncoprotein synthesis in MYC-driven lymphomas."
JOHN MATTHEWS,Boston Hospitality Review: Winter 2013,
JOHN MATTHEWS,Targeting translation initiation by synthetic rocaglates for treating MYC-driven lymphomas.,"MYC-driven lymphomas, especially those with concurrent MYC and BCL2 dysregulation, are currently a challenge in clinical practice due to rapid disease progression, resistance to standard chemotherapy, and high risk of refractory disease. MYC plays a central role by coordinating hyperactive protein synthesis with upregulated transcription in order to support rapid proliferation of tumor cells. Translation initiation inhibitor rocaglates have been identified as the most potent drugs in MYC-driven lymphomas as they efficiently inhibit MYC expression and tumor cell viability. We found that this class of compounds can overcome eIF4A abundance by stabilizing target mRNA-eIF4A interaction that directly prevents translation. Proteome-wide quantification demonstrated selective repression of multiple critical oncoproteins in addition to MYC in B-cell lymphoma including NEK2, MCL1, AURKA, PLK1, and several transcription factors that are generally considered undruggable. Finally, (-)-SDS-1-021, the most promising synthetic rocaglate, was confirmed to be highly potent as a single agent, and displayed significant synergy with the BCL2 inhibitor ABT199 in inhibiting tumor growth and survival in primary lymphoma cells in vitro and in patient-derived xenograft mouse models. Overall, our findings support the strategy of using rocaglates to target oncoprotein synthesis in MYC-driven lymphomas."
JOHN MATTHEWS,Developing an evidence-based fall prevention curriculum for community health workers,"This perspective paper describes processes in the development of an evidence-based fall prevention curriculum for community health workers/promotores (CHW/P) that highlights the development of the curriculum and addresses: (1) the need and rationale for involving CHW/P in fall prevention; (2) involvement of CHW/P and content experts in the curriculum development; (3) best practices utilized in the curriculum development and training implementation; and (4) next steps for dissemination and utilization of the CHW/P fall prevention curriculum. The project team of CHW/P and content experts developed, pilot tested, and revised bilingual in-person training modules about fall prevention among older adults. The curriculum incorporated the following major themes: (1) fall risk factors and strategies to reduce/prevent falls; (2) communication strategies to reduce risk of falling and strategies for developing fall prevention plans; and (3) health behavior change theories utilized to prevent and reduce falls. Three separate fall prevention modules were developed for CHW/P and CHW/P Instructors to be used during in-person trainings. Module development incorporated a five-step process: (1) conduct informal focus groups with CHW/P to inform content development; (2) develop three in-person modules in English and Spanish with input from content experts; (3) pilot-test the modules with CHW/P; (4) refine and finalize modules based on pilot-test feedback; and (5) submit modules for approval of continuing education units. This project contributes to the existing evidence-based literature by examining the role of CHW/P in fall prevention among older adults. By including evidence-based communication strategies such as message tailoring, the curriculum design allows CHW/P to personalize the information for individuals, which can result in an effective dissemination of a curriculum that is evidence-based and culturally appropriate."
JOHN MATTHEWS,Variability of Jupiter's main auroral emission and satellite footprints observed with HST during the Galileo era,"Hubble Space Telescope images of Jupiter's UV aurora show that the main emission occasionally contracts or expands, shifting toward or away from the magnetic pole by several degrees in response to changes in the solar wind dynamic pressure and Io's volcanic activity. When the auroral footprints of the Galilean satellites move with the main emission this indicates a change in the stretched field line configuration that shifts the ionospheric mapping of a given radial distance at the equator. However, in some cases, the main emission shifts independently from the satellite footprints, indicating that the variability stems from some other part of the corotation enforcement current system that produces Jupiter's main auroral emissions. Here, we analyze HST images from the Galileo era (1996–2003) and compare latitudinal shifts of the Ganymede footprint and the main auroral emission. We focus on images with overlapping Galileo measurements because concurrent measurements are available of the current sheet strength, which indicates the amount of field line stretching and can influence both the main emission and satellite footprints. We show that the Ganymede footprint and main auroral emission typically, but do not always, move together. Additionally, we find that the auroral shifts are only weakly linked to changes in the current sheet strength measured by Galileo. We discuss implications of the observed auroral shifts in terms of the magnetospheric mapping. Finally, we establish how the statistical reference main emission contours vary with central meridian longitude and show that the dependence results from magnetospheric local time asymmetries."
JOHN MATTHEWS,First M87 Event Horizon Telescope results. IV. Imaging the central supermassive black hole,
JOHN MATTHEWS,Specialized sledge dogs accompanied Inuit dispersal across the North American Arctic,"Domestic dogs have been central to life in the North American Arctic for millennia. The ancestors of the Inuit were the first to introduce the widespread usage of dog sledge transportation technology to the Americas, but whether the Inuit adopted local Palaeo-Inuit dogs or introduced a new dog population to the region remains unknown. To test these hypotheses, we generated mitochondrial DNA and geometric morphometric data of skull and dental elements from a total of 922 North American Arctic dogs and wolves spanning over 4500 years. Our analyses revealed that dogs from Inuit sites dating from 2000 BP possess morphological and genetic signatures that distinguish them from earlier Palaeo-Inuit dogs, and identified a novel mitochondrial clade in eastern Siberia and Alaska. The genetic legacy of these Inuit dogs survives today in modern Arctic sledge dogs despite phenotypic differences between archaeological and modern Arctic dogs. Together, our data reveal that Inuit dogs derive from a secondary pre-contact migration of dogs distinct from Palaeo-Inuit dogs, and probably aided the Inuit expansion across the North American Arctic beginning around 1000 BP."
JOHN MATTHEWS,Task force on immigration and higher education in Central Massachusetts,"In August 2007, the Colleges of Worcester Consortium, Inc. created a task force to examine the issue of immigration and higher education in Central Massachusetts. It has become increasingly clear from recent demographic and economic studies and projections that the population in the northeast, and certainly in Central Massachusetts, is showing minimal growth. There is evidence that a decline in the “native-born” population is caused by significant out-migration due to a number of factors, including the high cost of living, limited career opportunities and a declining birth rate. The limited population growth that is evident is due primarily to the recent influx of immigrants to this area, with the most significant numbers in Worcester coming from Ghana, Brazil, the Dominican Republic, Kenya, El Salvador, Albania and Liberia. It is also clear that the area’s economy is becoming more knowledge-based with an increasing percentage of all new jobs requiring some form of postsecondary education. According to the 2007 Massachusetts Department of Workforce Development’s Job Vacancy Survey, 38 percent of current job vacancies in Massachusetts require an associate’s degree or higher. This represents an increase from 30 percent in 2003. Consequently, the level of education that the immigrant population attains is of vital importance to everyone—not only to immigrant students and their families but also to the economic well-being of the entire region. The Task Force was charged with researching the barriers to higher education faced by this new wave of immigrants and suggesting recommendations to address those barriers. The 36-member Task Force was made up of representatives from Consortium member institutions; federal, state and local governments; community and faithbased organizations; the Worcester Public Schools; the Massachusetts Board of Higher Education; and the Massachusetts Immigrant and Refugee Advocacy (MIRA) Coalition. Meetings were held over six months, during which the Task Force identified three main barriers faced by immigrant communities in accessing higher education, and sub-committees were created to work on each of these. Speakers were invited to present on topics of interest. Two public hearings were held, the first of which was conducted at Worcester State College in October. It attracted community representatives, as well as college and high school faculty and administrators. The second hearing, held at the downtown branch of Quinsigamond Community College (QCC) in December, was attended by immigrants (English for Speakers of Other Languages – ESOL and GED) students as well as QCC staff."
JOHN MATTHEWS,"Canvass: a crowd-sourced, natural-product screening library for exploring biological space",
JOHN MATTHEWS,Verification of radiative transfer schemes for the EHT,"The Event Horizon Telescope (EHT) Collaboration has recently produced the first resolved images of the central supermassive black hole in the giant elliptical galaxy M87. Here we report on tests of the consistency and accuracy of the general relativistic radiative transfer codes used within the collaboration to model M87* and Sgr A*. We compare and evaluate (1) deflection angles for equatorial null geodesics in a Kerr spacetime; (2) images calculated from a series of simple, parameterized matter distributions in the Kerr metric using simplified emissivities and absorptivities; (3) for a subset of codes, images calculated from general relativistic magnetohydrodynamics simulations using different realistic synchrotron emissivities and absorptivities; (4) observables for the 2017 configuration of EHT, including visibility amplitudes and closure phases. The error in total flux is of order 1% when the codes are run with production numerical parameters. The dominant source of discrepancies for small camera distances is the location and detailed setup of the software ""camera"" that each code uses to produce synthetic images. We find that when numerical parameters are suitably chosen and the camera is sufficiently far away the images converge and that for given transfer coefficients, numerical uncertainties are unlikely to limit parameter estimation for the current generation of EHT observations. The purpose of this paper is to describe a verification and comparison of EHT radiative transfer codes. It is not to verify EHT models more generally."
JOHN MATTHEWS,Monitoring the mmorphology of M87* in 2009–2017 with the Event Horizon Telescope,"The Event Horizon Telescope (EHT) has recently delivered the first resolved images of M87*, the supermassive black hole in the center of the M87 galaxy. These images were produced using 230 GHz observations performed in April 2017. Additional observations are required to investigate the persistence of the primary image feature – a ring with azimuthal brightness asymmetry – and to quantify the image variability on event horizon scales. To address this need, we analyze M87* data collected with prototype EHT arrays in 2009, 2011, 2012, and 2013. While these observations do not contain enough information to produce images, they are sufficient to constrain simple geometric models. We develop a modeling approach based on the framework utilized for the 2017 EHT data analysis and validate our procedures using synthetic data. Applying the same approach to the observational data sets, we find the M87* morphology in 2009–2017 to be consistent with a persistent asymmetric ring of 40 as diameter. The position angle of peak intensity varies in time. In particular, we find a significant difference between the position angle measured in 2013 and 2017. These variations are in broad agreement with predictions of a subset of general relativistic magnetohydrodynamic simulations. We show that quantifying the variability across multiple observational epochs has the potential to constrain physical properties of the source, such as the accretion state or the black hole spin."
JOHN MATTHEWS,THEMIS: a parameter estimation framework for the Event Horizon Telescope,"The Event Horizon Telescope (EHT) provides the unprecedented ability to directly resolve the structure and dynamics of black hole emission regions on scales smaller than their horizons. This has the potential to critically probe the mechanisms by which black holes accrete and launch outflows, and the structure of supermassive black hole spacetimes. However, accessing this information is a formidable analysis challenge for two reasons. First, the EHT natively produces a variety of data types that encode information about the image structure in nontrivial ways; these are subject to a variety of systematic effects associated with very long baseline interferometry and are supplemented by a wide variety of auxiliary data on the primary EHT targets from decades of other observations. Second, models of the emission regions and their interaction with the black hole are complex, highly uncertain, and computationally expensive to construct. As a result, the scientific utilization of EHT observations requires a flexible, extensible, and powerful analysis framework. We present such a framework, Themis, which defines a set of interfaces between models, data, and sampling algorithms that facilitates future development. We describe the design and currently existing components of Themis, how Themis has been validated thus far, and present additional analyses made possible by Themis that illustrate its capabilities. Importantly, we demonstrate that Themis is able to reproduce prior EHT analyses, extend these, and do so in a computationally efficient manner that can efficiently exploit modern high-performance computing facilities. Themis has already been used extensively in the scientific analysis and interpretation of the first EHT observations of M87."
JOHN MATTHEWS,First Sagittarius A* Event Horizon Telescope results. V. Testing astrophysical models of the galactic center black hole,"In this paper we provide a first physical interpretation for the Event Horizon Telescope's (EHT) 2017 observations of Sgr A*. Our main approach is to compare resolved EHT data at 230 GHz and unresolved non-EHT observations from radio to X-ray wavelengths to predictions from a library of models based on time-dependent general relativistic magnetohydrodynamics simulations, including aligned, tilted, and stellar-wind-fed simulations; radiative transfer is performed assuming both thermal and nonthermal electron distribution functions. We test the models against 11 constraints drawn from EHT 230 GHz data and observations at 86 GHz, 2.2 μm, and in the X-ray. All models fail at least one constraint. Light-curve variability provides a particularly severe constraint, failing nearly all strongly magnetized (magnetically arrested disk (MAD)) models and a large fraction of weakly magnetized models. A number of models fail only the variability constraints. We identify a promising cluster of these models, which are MAD and have inclination i ≤ 30°. They have accretion rate (5.2–9.5) × 10−9 M ⊙ yr−1, bolometric luminosity (6.8–9.2) × 1035 erg s−1, and outflow power (1.3–4.8) × 1038 erg s−1. We also find that all models with i ≥ 70° fail at least two constraints, as do all models with equal ion and electron temperature; exploratory, nonthermal model sets tend to have higher 2.2 μm flux density; and the population of cold electrons is limited by X-ray constraints due to the risk of bremsstrahlung overproduction. Finally, we discuss physical and numerical limitations of the models, highlighting the possible importance of kinetic effects and duration of the simulations."
JOHN MATTHEWS,First M87 Event Horizon Telescope results. VII. Polarization of the ring,"In 2017 April, the Event Horizon Telescope (EHT) observed the near-horizon region around the supermassive black hole at the core of the M87 galaxy. These 1.3 mm wavelength observations revealed a compact asymmetric ring-like source morphology. This structure originates from synchrotron emission produced by relativistic plasma located in the immediate vicinity of the black hole. Here we present the corresponding linear-polarimetric EHT images of the center of M87. We find that only a part of the ring is significantly polarized. The resolved fractional linear polarization has a maximum located in the southwest part of the ring, where it rises to the level of ∼15%. The polarization position angles are arranged in a nearly azimuthal pattern. We perform quantitative measurements of relevant polarimetric properties of the compact emission and find evidence for the temporal evolution of the polarized source structure over one week of EHT observations. The details of the polarimetric data reduction and calibration methodology are provided. We carry out the data analysis using multiple independent imaging and modeling techniques, each of which is validated against a suite of synthetic data sets. The gross polarimetric structure and its apparent evolution with time are insensitive to the method used to reconstruct the image. These polarimetric images carry information about the structure of the magnetic fields responsible for the synchrotron emission. Their physical interpretation is discussed in an accompanying publication."
JOHN MATTHEWS,First M87 Event Horizon Telescope results. VIII. Magnetic field structure near The Event Horizon,"Event Horizon Telescope (EHT) observations at 230 GHz have now imaged polarized emission around the supermassive black hole in M87 on event-horizon scales. This polarized synchrotron radiation probes the structure of magnetic fields and the plasma properties near the black hole. Here we compare the resolved polarization structure observed by the EHT, along with simultaneous unresolved observations with the Atacama Large Millimeter/submillimeter Array, to expectations from theoretical models. The low fractional linear polarization in the resolved image suggests that the polarization is scrambled on scales smaller than the EHT beam, which we attribute to Faraday rotation internal to the emission region. We estimate the average density n_e ∼ 10^4–7 cm^−3, magnetic field strength B ∼ 1–30 G, and electron temperature T_e ∼ (1–12) × 10^10 K of the radiating plasma in a simple one-zone emission model. We show that the net azimuthal linear polarization pattern may result from organized, poloidal magnetic fields in the emission region. In a quantitative comparison with a large library of simulated polarimetric images from general relativistic magnetohydrodynamic (GRMHD) simulations, we identify a subset of physical models that can explain critical features of the polarimetric EHT observations while producing a relativistic jet of sufficient power. The consistent GRMHD models are all of magnetically arrested accretion disks, where near-horizon magnetic fields are dynamically important. We use the models to infer a mass accretion rate onto the black hole in M87 of (3–20) × 10^−4 M ⊙ yr^−1."
JOHN MATTHEWS,Resolving the inner parsec of the blazar J1924–2914 with the event horizon telescope,"The blazar J1924–2914 is a primary Event Horizon Telescope (EHT) calibrator for the Galactic center’s black hole Sagittarius A*. Here we present the first total and linearly polarized intensity images of this source obtained with the unprecedented 20 μas resolution of the EHT. J1924–2914 is a very compact flat-spectrum radio source with strong optical variability and polarization. In April 2017 the source was observed quasi-simultaneously with the EHT (April 5–11), the Global Millimeter VLBI Array (April 3), and the Very Long Baseline Array (April 28), giving a novel view of the source at four observing frequencies, 230, 86, 8.7, and 2.3 GHz. These observations probe jet properties from the subparsec to 100 pc scales. We combine the multifrequency images of J1924–2914 to study the source morphology. We find that the jet exhibits a characteristic bending, with a gradual clockwise rotation of the jet projected position angle of about 90° between 2.3 and 230 GHz. Linearly polarized intensity images of J1924–2914 with the extremely fine resolution of the EHT provide evidence for ordered toroidal magnetic fields in the blazar compact core."
JOHN MATTHEWS,Gold(I)-mediated cycloisomerization/cycloaddition enables bioinspired syntheses of neonectrolides B-E and analogues,Development of a synthetic route to the oxaphenalenone (OP) natural products neonectrolides B-E is described. The synthesis relies on gold-catalyzed 6-endo-dig hydroarylation of an unusual enynol substrate as well as a one-pot Rieche formylation/cyclization/deprotection sequence to efficiently construct the tricyclic oxaphenalenone framework in the form of a masked ortho-quinone methide (o-QM). A tandem cycloisomerization/[4 + 2] cycloaddition strategy was employed to quickly construct molecules resembling the neonectrolides. The tricyclic OP natural product SF226 could be converted to corymbiferan lactone E and a related masked o-QM. Our study culminates with the application of the tandem reaction sequence to syntheses of neonectrolides B-E as well as previously unreported exo-diastereomers.
JOHN MATTHEWS,"Climate of crisis: how cities can use climate action to close the equity gap, drive economic recovery, and improve public health","This report is the second of three that provides community leaders, inside and outside of local government, with guidance about navigating their climate-action priorities through the gauntlet of challenges created by the COVID-19 pandemic and the ensuing economic crisis. Each report, based on a synthesis of peer-reviewed research, expert interviews, and the analysis of local climate action, address a different topic: 1. More Urgency, Not Less: The COVID-19 Pandemic’s Lessons for Local Climate Leadership (Published June 2020) 2. Climate of Crisis: How Cities Can Use Climate Action to Close the Equity Gap, Drive Economic Recovery, and Improve Public Health (Published September 2020) 3. A Survey of U.S. City Climate Leaders: The Prospects for Climate Action in the COVID-19 Era (October 2020) This work is supported by The Summit Foundation and The Grantham Foundation for the Protection of the Environment."
JOHN MATTHEWS,A universal power-law prescription for variability from synthetic images of black hole accretion flows,"We present a framework for characterizing the spatiotemporal power spectrum of the variability expected from the horizon-scale emission structure around supermassive black holes, and we apply this framework to a library of general relativistic magnetohydrodynamic (GRMHD) simulations and associated general relativistic ray-traced images relevant for Event Horizon Telescope (EHT) observations of Sgr A*. We find that the variability power spectrum is generically a red-noise process in both the temporal and spatial dimensions, with the peak in power occurring on the longest timescales and largest spatial scales. When both the time-averaged source structure and the spatially integrated light-curve variability are removed, the residual power spectrum exhibits a universal broken power-law behavior. On small spatial frequencies, the residual power spectrum rises as the square of the spatial frequency and is proportional to the variance in the centroid of emission. Beyond some peak in variability power, the residual power spectrum falls as that of the time-averaged source structure, which is similar across simulations; this behavior can be naturally explained if the variability arises from a multiplicative random field that has a steeper high-frequency power-law index than that of the time-averaged source structure. We briefly explore the ability of power spectral variability studies to constrain physical parameters relevant for the GRMHD simulations, which can be scaled to provide predictions for black holes in a range of systems in the optically thin regime. We present specific expectations for the behavior of the M87* and Sgr A* accretion flows as observed by the EHT."
JOHN MATTHEWS,Millimeter light curves of Sagittarius A* observed during the 2017 Event Horizon Telescope campaign,"The Event Horizon Telescope (EHT) observed the compact radio source, Sagittarius A* (Sgr A*), in the Galactic Center on 2017 April 5–11 in the 1.3 mm wavelength band. At the same time, interferometric array data from the Atacama Large Millimeter/submillimeter Array and the Submillimeter Array were collected, providing Sgr A* light curves simultaneous with the EHT observations. These data sets, complementing the EHT very long baseline interferometry, are characterized by a cadence and signal-to-noise ratio previously unattainable for Sgr A* at millimeter wavelengths, and they allow for the investigation of source variability on timescales as short as a minute. While most of the light curves correspond to a low variability state of Sgr A*, the April 11 observations follow an X-ray flare and exhibit strongly enhanced variability. All of the light curves are consistent with a red-noise process, with a power spectral density (PSD) slope measured to be between −2 and −3 on timescales between 1 minute and several hours. Our results indicate a steepening of the PSD slope for timescales shorter than 0.3 hr. The spectral energy distribution is flat at 220 GHz, and there are no time lags between the 213 and 229 GHz frequency bands, suggesting low optical depth for the event horizon scale source. We characterize Sgr A*’s variability, highlighting the different behavior observed just after the X-ray flare, and use Gaussian process modeling to extract a decorrelation timescale and a PSD slope. We also investigate the systematic calibration uncertainties by analyzing data from independent data reduction pipelines."
JOHN MATTHEWS,Selective dynamical imaging of interferometric data,"Recent developments in very long baseline interferometry (VLBI) have made it possible for the Event Horizon Telescope (EHT) to resolve the innermost accretion flows of the largest supermassive black holes on the sky. The sparse nature of the EHT’s (u, v)-coverage presents a challenge when attempting to resolve highly time-variable sources. We demonstrate that the changing (u, v)-coverage of the EHT can contain regions of time over the course of a single observation that facilitate dynamical imaging. These optimal time regions typically have projected baseline distributions that are approximately angularly isotropic and radially homogeneous. We derive a metric of coverage quality based on baseline isotropy and density that is capable of ranking array configurations by their ability to produce accurate dynamical reconstructions. We compare this metric to existing metrics in the literature and investigate their utility by performing dynamical reconstructions on synthetic data from simulated EHT observations of sources with simple orbital variability. We then use these results to make recommendations for imaging the 2017 EHT Sgr A* data set."
JOHN MATTHEWS,"Does a Functional Activity Programme Improve Function, Quality of Life, and Falls for Residents in Long Term Care? Cluster Randomised Controlled Trial","Objective To assess the effectiveness of an activity programme in improving function, quality of life, and falls in older people in residential care. Design Cluster randomised controlled trial with one year follow-up. Setting 41 low level dependency residential care homes in New Zealand. Participants 682 people aged 65 years or over. Interventions 330 residents were offered a goal setting and individualised activities of daily living activity programme by a gerontology nurse, reinforced by usual healthcare assistants; 352 residents received social visits. Main outcome measures Function (late life function and disability instruments, elderly mobility scale, FICSIT-4 balance test, timed up and go test), quality of life (life satisfaction index, EuroQol), and falls (time to fall over 12 months). Secondary outcomes were depressive symptoms and hospital admissions. Results 473 (70%) participants completed the trial. The programme had no impact overall. However, in contrast to residents with impaired cognition (no differences between intervention and control group), those with normal cognition in the intervention group may have maintained overall function (late life function and disability instrument total function, P=0.024) and lower limb function (late life function and disability instrument basic lower extremity, P=0.015). In residents with cognitive impairment, the likelihood of depression increased in the intervention group. No other outcomes differed between groups. Conclusion A programme of functional rehabilitation had minimal impact for elderly people in residential care with normal cognition but was not beneficial for those with poor cognition. Trial registration Australian Clinical Trials Register ACTRN12605000667617."
JOHN MATTHEWS,Boston Hospitality Review: Summer 2013,
JOHN MATTHEWS,Boston Hospitality Review: Spring 2014,
JOHN MATTHEWS,3'-UTR SIRF: A Database for Identifying Clusters of Whort Interspersed Repeats in 3' Untranslated Regions,"BACKGROUND. Short (~5 nucleotides) interspersed repeats regulate several aspects of post-transcriptional gene expression. Previously we developed an algorithm (REPFIND) that assigns P-values to all repeated motifs in a given nucleic acid sequence and reliably identifies clusters of short CAC-containing motifs required for mRNA localization in Xenopus oocytes. DESCRIPTION. In order to facilitate the identification of genes possessing clusters of repeats that regulate post-transcriptional aspects of gene expression in mammalian genes, we used REPFIND to create a database of all repeated motifs in the 3' untranslated regions (UTR) of genes from the Mammalian Gene Collection (MGC). The MGC database includes seven vertebrate species: human, cow, rat, mouse and three non-mammalian vertebrate species. A web-based application was developed to search this database of repeated motifs to generate species-specific lists of genes containing specific classes of repeats in their 3'-UTRs. This computational tool is called 3'-UTR SIRF (Short Interspersed Repeat Finder), and it reveals that hundreds of human genes contain an abundance of short CAC-rich and CAG-rich repeats in their 3'-UTRs that are similar to those found in mRNAs localized to the neurites of neurons. We tested four candidate mRNAs for localization in rat hippocampal neurons by in situ hybridization. Our results show that two candidate CAC-rich (Syntaxin 1B and Tubulin β4) and two candidate CAG-rich (Sec61α and Syntaxin 1A) mRNAs are localized to distal neurites, whereas two control mRNAs lacking repeated motifs in their 3'-UTR remain primarily in the cell body. CONCLUSION. Computational data generated with 3'-UTR SIRF indicate that hundreds of mammalian genes have an abundance of short CA-containing motifs that may direct mRNA localization in neurons. In situ hybridization shows that four candidate mRNAs are localized to distal neurites of cultured hippocampal neurons. These data suggest that short CA-containing motifs may be part of a widely utilized genetic code that regulates mRNA localization in vertebrate cells. The use of 3'-UTR SIRF to search for new classes of motifs that regulate other aspects of gene expression should yield important information in future studies addressing cis-regulatory information located in 3'-UTRs."
JOHN MATTHEWS,Transcriptomic and Metabolite Analyses of Cabernet Sauvignon Grape Berry Development,"BACKGROUND: Grape berry development is a dynamic process that involves a complex series of molecular genetic and biochemical changes divided into three major phases. During initial berry growth (Phase I), berry size increases along a sigmoidal growth curve due to cell division and subsequent cell expansion, and organic acids (mainly malate and tartrate), tannins, and hydroxycinnamates accumulate to peak levels. The second major phase (Phase II) is defined as a lag phase in which cell expansion ceases and sugars begin to accumulate. Véraison (the onset of ripening) marks the beginning of the third major phase (Phase III) in which berries undergo a second period of sigmoidal growth due to additional mesocarp cell expansion, accumulation of anthocyanin pigments for berry color, accumulation of volatile compounds for aroma, softening, peak accumulation of sugars (mainly glucose and fructose), and a decline in organic acid accumulation. In order to understand the transcriptional network responsible for controlling berry development, mRNA expression profiling was conducted on berries of V. vinifera Cabernet Sauvignon using the Affymetrix GeneChip® Vitis oligonucleotide microarray ver. 1.0 spanning seven stages of berry development from small pea size berries (E-L stages 31 to 33 as defined by the modified E-L system), through véraison (E-L stages 34 and 35), to mature berries (E-L stages 36 and 38). Selected metabolites were profiled in parallel with mRNA expression profiling to understand the effect of transcriptional regulatory processes on specific metabolite production that ultimately influence the organoleptic properties of wine. RESULTS: Over the course of berry development whole fruit tissues were found to express an average of 74.5% of probes represented on the Vitis microarray, which has 14,470 Unigenes. Approximately 60% of the expressed transcripts were differentially expressed between at least two out of the seven stages of berry development (28% of transcripts, 4,151 Unigenes, had pronounced (≥2 fold) differences in mRNA expression) illustrating the dynamic nature of the developmental process. The subset of 4,151 Unigenes was split into twenty well-correlated expression profiles. Expression profile patterns included those with declining or increasing mRNA expression over the course of berry development as well as transient peak or trough patterns across various developmental stages as defined by the modified E-L system. These detailed surveys revealed the expression patterns for genes that play key functional roles in phytohormone biosynthesis and response, calcium sequestration, transport and signaling, cell wall metabolism mediating expansion, ripening, and softening, flavonoid metabolism and transport, organic and amino acid metabolism, hexose sugar and triose phosphate metabolism and transport, starch metabolism, photosynthesis, circadian cycles and pathogen resistance. In particular, mRNA expression patterns of transcription factors, abscisic acid (ABA) biosynthesis, and calcium signaling genes identified candidate factors likely to participate in the progression of key developmental events such as véraison and potential candidate genes associated with such processes as auxin partitioning within berry cells, aroma compound production, and pathway regulation and sequestration of flavonoid compounds. Finally, analysis of sugar metabolism gene expression patterns indicated the existence of an alternative pathway for glucose and triose phosphate production that is invoked from véraison to mature berries. CONCLUSION: These results reveal the first high-resolution picture of the transcriptome dynamics that occur during seven stages of grape berry development. This work also establishes an extensive catalog of gene expression patterns for future investigations aimed at the dissection of the transcriptional regulatory hierarchies that govern berry development in a widely grown cultivar of wine grape. More importantly, this analysis identified a set of previously unknown genes potentially involved in critical steps associated with fruit development that can now be subjected to functional testing."
JOHN MATTHEWS,Tissue-Specific mRNA Expression Profiling in Grape Berry Tissues,"BACKGROUND: Berries of grape (Vitis vinifera) contain three major tissue types (skin, pulp and seed) all of which contribute to the aroma, color, and flavor characters of wine. The pericarp, which is composed of the exocarp (skin) and mesocarp (pulp), not only functions to protect and feed the developing seed, but also to assist in the dispersal of the mature seed by avian and mammalian vectors. The skin provides volatile and nonvolatile aroma and color compounds, the pulp contributes organic acids and sugars, and the seeds provide condensed tannins, all of which are important to the formation of organoleptic characteristics of wine. In order to understand the transcriptional network responsible for controlling tissue-specific mRNA expression patterns, mRNA expression profiling was conducted on each tissue of mature berries of V. vinifera Cabernet Sauvignon using the Affymetrix GeneChip® Vitis oligonucleotide microarray ver. 1.0. In order to monitor the influence of water-deficit stress on tissue-specific expression patterns, mRNA expression profiles were also compared from mature berries harvested from vines subjected to well-watered or water-deficit conditions. RESULTS: Overall, berry tissues were found to express approximately 76% of genes represented on the Vitis microarray. Approximately 60% of these genes exhibited significant differential expression in one or more of the three major tissue types with more than 28% of genes showing pronounced (2-fold or greater) differences in mRNA expression. The largest difference in tissue-specific expression was observed between the seed and pulp/skin. Exocarp tissue, which is involved in pathogen defense and pigment production, showed higher mRNA abundance relative to other berry tissues for genes involved with flavonoid biosynthesis, pathogen resistance, and cell wall modification. Mesocarp tissue, which is considered a nutritive tissue, exhibited a higher mRNA abundance of genes involved in cell wall function and transport processes. Seeds, which supply essential resources for embryo development, showed higher mRNA abundance of genes encoding phenylpropanoid biosynthetic enzymes, seed storage proteins, and late embryogenesis abundant proteins. Water-deficit stress affected the mRNA abundance of 13% of the genes with differential expression patterns occurring mainly in the pulp and skin. In pulp and seed tissues transcript abundance in most functional categories declined in water-deficit stressed vines relative to well-watered vines with transcripts for storage proteins and novel (no-hit) functional assignments being over represented. In the skin of berries from water-deficit stressed vines, however, transcripts from several functional categories including general phenypropanoid and ethylene metabolism, pathogenesis-related responses, energy, and interaction with the environment were significantly over-represented. CONCLUSION: These results revealed novel insights into the tissue-specific expression mRNA expression patterns of an extensive repertoire of genes expressed in berry tissues. This work also establishes an extensive catalogue of gene expression patterns for future investigations aimed at the dissection of the transcriptional regulatory hierarchies that govern tissue-specific expression patterns associated with tissue differentiation within berries. These results also confirmed that water-deficit stress has a profound effect on mRNA expression patterns particularly associated with the biosynthesis of aroma and color metabolites within skin and pulp tissues that ultimately impact wine quality."
JOHN MATTHEWS,First Sagittarius A* Event Horizon Telescope results. VI. Testing the black hole metric,"Astrophysical black holes are expected to be described by the Kerr metric. This is the only stationary, vacuum, axisymmetric metric, without electromagnetic charge, that satisfies Einstein’s equations and does not have pathologies outside of the event horizon. We present new constraints on potential deviations from the Kerr prediction based on 2017 EHT observations of Sagittarius A* (Sgr A*). We calibrate the relationship between the geometrically defined black hole shadow and the observed size of the ring-like images using a library that includes both Kerr and non-Kerr simulations. We use the exquisite prior constraints on the mass-to-distance ratio for Sgr A* to show that the observed image size is within ∼10% of the Kerr predictions. We use these bounds to constrain metrics that are parametrically different from Kerr, as well as the charges of several known spacetimes. To consider alternatives to the presence of an event horizon, we explore the possibility that Sgr A* is a compact object with a surface that either absorbs and thermally reemits incident radiation or partially reflects it. Using the observed image size and the broadband spectrum of Sgr A*, we conclude that a thermal surface can be ruled out and a fully reflective one is unlikely. We compare our results to the broader landscape of gravitational tests. Together with the bounds found for stellar-mass black holes and the M87 black hole, our observations provide further support that the external spacetimes of all black holes are described by the Kerr metric, independent of their mass."
JOHN MATTHEWS,Priorities for synthesis research in ecology and environmental science,
JOHN MATTHEWS,Polarimetric properties of Event Horizon Telescope targets from ALMA,"We present the results from a full polarization study carried out with the Atacama Large Millimeter/submillimeter Array (ALMA) during the first Very Long Baseline Interferometry (VLBI) campaign, which was conducted in 2017 April in the λ3 mm and λ1.3 mm bands, in concert with the Global mm-VLBI Array (GMVA) and the Event Horizon Telescope (EHT), respectively. We determine the polarization and Faraday properties of all VLBI targets, including Sgr A*, M87, and a dozen radio-loud active galactic nuclei (AGNs), in the two bands at several epochs in a time window of 10 days. We detect high linear polarization fractions (2%–15%) and large rotation measures (RM &gt; 103.3–105.5 rad m−2), confirming the trends of previous AGN studies at millimeter wavelengths. We find that blazars are more strongly polarized than other AGNs in the sample, while exhibiting (on average) order-of-magnitude lower RM values, consistent with the AGN viewing angle unification scheme. For Sgr A* we report a mean RM of (−4.2 ± 0.3) × 105 rad m−2 at 1.3 mm, consistent with measurements over the past decade and, for the first time, an RM of (–2.1 ± 0.1) × 105 rad m−2 at 3 mm, suggesting that about half of the Faraday rotation at 1.3 mm may occur between the 3 mm photosphere and the 1.3 mm source. We also report the first unambiguous measurement of RM toward the M87 nucleus at millimeter wavelengths, which undergoes significant changes in magnitude and sign reversals on a one year timescale, spanning the range from −1.2 to 0.3 × 105 rad m−2 at 3 mm and −4.1 to 1.5 × 105 rad m−2 at 1.3 mm. Given this time variability, we argue that, unlike the case of Sgr A*, the RM in M87 does not provide an accurate estimate of the mass accretion rate onto the black hole. We put forward a two-component model, comprised of a variable compact region and a static extended region, that can simultaneously explain the polarimetric properties observed by both the EHT (on horizon scales) and ALMA (which observes the combined emission from both components). These measurements provide critical constraints for the calibration, analysis, and interpretation of simultaneously obtained VLBI data with the EHT and GMVA."
JOHN MATTHEWS,"First Sagittarius A* Event Horizon Telescope results. IV. Variability, morphology, and black hole mass","In this paper we quantify the temporal variability and image morphology of the horizon-scale emission from Sgr A*, as observed by the EHT in 2017 April at a wavelength of 1.3 mm. We find that the Sgr A* data exhibit variability that exceeds what can be explained by the uncertainties in the data or by the effects of interstellar scattering. The magnitude of this variability can be a substantial fraction of the correlated flux density, reaching ∼100% on some baselines. Through an exploration of simple geometric source models, we demonstrate that ring-like morphologies provide better fits to the Sgr A* data than do other morphologies with comparable complexity. We develop two strategies for fitting static geometric ring models to the time-variable Sgr A* data; one strategy fits models to short segments of data over which the source is static and averages these independent fits, while the other fits models to the full data set using a parametric model for the structural variability power spectrum around the average source structure. Both geometric modeling and image-domain feature extraction techniques determine the ring diameter to be 51.8 ± 2.3 μas (68% credible intervals), with the ring thickness constrained to have an FWHM between ∼30% and 50% of the ring diameter. To bring the diameter measurements to a common physical scale, we calibrate them using synthetic data generated from GRMHD simulations. This calibration constrains the angular size of the gravitational radius to be 4.8_-0.7^+1.4 μas, which we combine with an independent distance measurement from maser parallaxes to determine the mass of Sgr A* to be 4.0_-0.6^+10^6 M⊙."
JOHN MATTHEWS,"First Sagittarius A* Event Horizon Telescope results. II. EHT and multiwavelength observations, data processing, and calibration","We present Event Horizon Telescope (EHT) 1.3 mm measurements of the radio source located at the position of the supermassive black hole Sagittarius A* (Sgr A*), collected during the 2017 April 5–11 campaign. The observations were carried out with eight facilities at six locations across the globe. Novel calibration methods are employed to account for Sgr A*'s flux variability. The majority of the 1.3 mm emission arises from horizon scales, where intrinsic structural source variability is detected on timescales of minutes to hours. The effects of interstellar scattering on the image and its variability are found to be subdominant to intrinsic source structure. The calibrated visibility amplitudes, particularly the locations of the visibility minima, are broadly consistent with a blurred ring with a diameter of ∼50 μas, as determined in later works in this series. Contemporaneous multiwavelength monitoring of Sgr A* was performed at 22, 43, and 86 GHz and at near-infrared and X-ray wavelengths. Several X-ray flares from Sgr A* are detected by Chandra, one at low significance jointly with Swift on 2017 April 7 and the other at higher significance jointly with NuSTAR on 2017 April 11. The brighter April 11 flare is not observed simultaneously by the EHT but is followed by a significant increase in millimeter flux variability immediately after the X-ray outburst, indicating a likely connection in the emission physics near the event horizon. We compare Sgr A*’s broadband flux during the EHT campaign to its historical spectral energy distribution and find that both the quiescent emission and flare emission are consistent with its long-term behavior."
JOHN MATTHEWS,First radial velocity results from the MINiature Exoplanet Radial Velocity Array (MINERVA),"The MINiature Exoplanet Radial Velocity Array (MINERVA) is a dedicated observatory of four 0.7 m robotic telescopes fiber-fed to a KiwiSpec spectrograph. The MINERVA mission is to discover super-Earths in the habitable zones of nearby stars. This can be accomplished with MINERVA's unique combination of high precision and high cadence over long time periods. In this work, we detail changes to the MINERVA facility that have occurred since our previous paper. We then describe MINERVA's robotic control software, the process by which we perform 1D spectral extraction, and our forward modeling Doppler pipeline. In the process of improving our forward modeling procedure, we found that our spectrograph's intrinsic instrumental profile is stable for at least nine months. Because of that, we characterized our instrumental profile with a time-independent, cubic spline function based on the profile in the cross dispersion direction, with which we achieved a radial velocity precision similar to using a conventional ""sum-of-Gaussians"" instrumental profile: 1.8 m s−1 over 1.5 months on the RV standard star HD 122064. Therefore, we conclude that the instrumental profile need not be perfectly accurate as long as it is stable. In addition, we observed 51 Peg and our results are consistent with the literature, confirming our spectrograph and Doppler pipeline are producing accurate and precise radial velocities."
JOHN MATTHEWS,"The L 98-59 system: three transiting, terrestrial-size planets orbiting a nearby M dwarf","We report the Transiting Exoplanet Survey Satellite (TESS) discovery of three terrestrial-size planets transiting L 98-59 (TOI-175, TIC 307210830)—a bright M dwarf at a distance of 10.6 pc. Using the Gaia-measured distance and broadband photometry, we find that the host star is an M3 dwarf. Combined with the TESS transits from three sectors, the corresponding stellar parameters yield planet radii ranging from 0.8 R ⊕ to 1.6 R ⊕. All three planets have short orbital periods, ranging from 2.25 to 7.45 days with the outer pair just wide of a 2:1 period resonance. Diagnostic tests produced by the TESS Data Validation Report and the vetting package DAVE rule out common false-positive sources. These analyses, along with dedicated follow-up and the multiplicity of the system, lend confidence that the observed signals are caused by planets transiting L 98-59 and are not associated with other sources in the field. The L 98-59 system is interesting for a number of reasons: the host star is bright (V = 11.7 mag, K = 7.1 mag) and the planets are prime targets for further follow-up observations including precision radial-velocity mass measurements and future transit spectroscopy with the James Webb Space Telescope; the near-resonant configuration makes the system a laboratory to study planetary system dynamical evolution; and three planets of relatively similar size in the same system present an opportunity to study terrestrial planets where other variables (age, metallicity, etc.) can be held constant. L 98-59 will be observed in four more TESS sectors, which will provide a wealth of information on the three currently known planets and have the potential to reveal additional planets in the system."
JOHN MATTHEWS,"Minerva-Australis. I. design, commissioning, and first photometric results","The Minerva-Australis telescope array is a facility dedicated to the follow-up, confirmation, characterization, and mass measurement of planets orbiting bright stars discovered by the Transiting Exoplanet Survey Satellite (TESS)—a category in which it is almost unique in the Southern Hemisphere. It is located at the University of Southern Queensland's Mount Kent Observatory near Toowoomba, Australia. Its flexible design enables multiple 0.7 m robotic telescopes to be used both in combination, and independently, for high-resolution spectroscopy and precision photometry of TESS transit planet candidates. Minerva-Australis also enables complementary studies of exoplanet spin–orbit alignments via Doppler observations of the Rossiter–McLaughlin effect, radial velocity searches for nontransiting planets, planet searches using transit timing variations, and ephemeris refinement for TESS planets. In this first paper, we describe the design, photometric instrumentation, software, and science goals of Minerva-Australis, and note key differences from its Northern Hemisphere counterpart, the Minerva array. We use recent transit observations of four planets, WASP-2b, WASP-44b, WASP-45b, and HD 189733b, to demonstrate the photometric capabilities of Minerva-Australis."
JOHN MATTHEWS,The polarized image of a synchrotron-emitting ring of gas orbiting a black hole,"Synchrotron radiation from hot gas near a black hole results in a polarized image. The image polarization is determined by effects including the orientation of the magnetic field in the emitting region, relativistic motion of the gas, strong gravitational lensing by the black hole, and parallel transport in the curved spacetime. We explore these effects using a simple model of an axisymmetric, equatorial accretion disk around a Schwarzschild black hole. By using an approximate expression for the null geodesics derived by Beloborodov and conservation of the Walker–Penrose constant, we provide analytic estimates for the image polarization. We test this model using currently favored general relativistic magnetohydrodynamic simulations of M87*, using ring parameters given by the simulations. For a subset of these with modest Faraday effects, we show that the ring model broadly reproduces the polarimetric image morphology. Our model also predicts the polarization evolution for compact flaring regions, such as those observed from Sgr A* with GRAVITY. With suitably chosen parameters, our simple model can reproduce the EVPA pattern and relative polarized intensity in Event Horizon Telescope images of M87*. Under the physically motivated assumption that the magnetic field trails the fluid velocity, this comparison is consistent with the clockwise rotation inferred from total intensity images."
JOHN MATTHEWS,"Looking ahead: forecasting and planning for the longer-range future, April 1, 2, and 3, 2005","The conference allowed for many highly esteemed scholars and professionals from a broad range of fields to come together to discuss strategies designed for the 21st century and beyond. The speakers and discussants covered a broad range of subjects including: long-term policy analysis, forecasting for business and investment, the National Intelligence Council Global Trends 2020 report, Europe’s transition from the Marshal plan to the EU, forecasting global transitions, foreign policy planning, and forecasting for defense."
JOHN MATTHEWS,First Sagittarius A* Event Horizon Telescope results. I. The shadow of the supermassive black hole in the center of the Milky Way,"We present the first Event Horizon Telescope (EHT) observations of Sagittarius A* (Sgr A*), the Galactic center source associated with a supermassive black hole. These observations were conducted in 2017 using a global interferometric array of eight telescopes operating at a wavelength of λ = 1.3 mm. The EHT data resolve a compact emission region with intrahour variability. A variety of imaging and modeling analyses all support an image that is dominated by a bright, thick ring with a diameter of 51.8 ± 2.3 μas (68% credible interval). The ring has modest azimuthal brightness asymmetry and a comparatively dim interior. Using a large suite of numerical simulations, we demonstrate that the EHT images of Sgr A* are consistent with the expected appearance of a Kerr black hole with mass ∼4 × 106 M ⊙, which is inferred to exist at this location based on previous infrared observations of individual stellar orbits, as well as maser proper-motion studies. Our model comparisons disfavor scenarios where the black hole is viewed at high inclination (i &gt; 50°), as well as nonspinning black holes and those with retrograde accretion disks. Our results provide direct evidence for the presence of a supermassive black hole at the center of the Milky Way, and for the first time we connect the predictions from dynamical measurements of stellar orbits on scales of 103–105 gravitational radii to event-horizon-scale images and variability. Furthermore, a comparison with the EHT results for the supermassive black hole M87* shows consistency with the predictions of general relativity spanning over three orders of magnitude in central mass."
JOHN MATTHEWS,The variability of the black hole image in M87 at the dynamical timescale,"The black hole images obtained with the Event Horizon Telescope (EHT) are expected to be variable at the dynamical timescale near their horizons. For the black hole at the center of the M87 galaxy, this timescale (5–61 days) is comparable to the 6 day extent of the 2017 EHT observations. Closure phases along baseline triangles are robust interferometric observables that are sensitive to the expected structural changes of the images but are free of station-based atmospheric and instrumental errors. We explored the day-to-day variability in closure-phase measurements on all six linearly independent nontrivial baseline triangles that can be formed from the 2017 observations. We showed that three triangles exhibit very low day-to-day variability, with a dispersion of ∼3°–5°. The only triangles that exhibit substantially higher variability (∼90°–180°) are the ones with baselines that cross the visibility amplitude minima on the u–v plane, as expected from theoretical modeling. We used two sets of general relativistic magnetohydrodynamic simulations to explore the dependence of the predicted variability on various black hole and accretion-flow parameters. We found that changing the magnetic field configuration, electron temperature model, or black hole spin has a marginal effect on the model consistency with the observed level of variability. On the other hand, the most discriminating image characteristic of models is the fractional width of the bright ring of emission. Models that best reproduce the observed small level of variability are characterized by thin ring-like images with structures dominated by gravitational lensing effects and thus least affected by turbulence in the accreting plasmas."
JOHN MATTHEWS,Constraints on black-hole charges with the 2017 EHT observations of M87*,
JOHN MATTHEWS,SYMBA: an end-to-end VLBI synthetic data generation pipeline,"CONTEXT: Realistic synthetic observations of theoretical source models are essential for our understanding of real observational data. In using synthetic data, one can verify the extent to which source parameters can be recovered and evaluate how various data corruption effects can be calibrated. These studies are the most important when proposing observations of new sources, in the characterization of the capabilities of new or upgraded instruments, and when verifying model-based theoretical predictions in a direct comparison with observational data. AIMS: We present the SYnthetic Measurement creator for long Baseline Arrays (SYMBA), a novel synthetic data generation pipeline for Very Long Baseline Interferometry (VLBI) observations. SYMBA takes into account several realistic atmospheric, instrumental, and calibration effects. METHODS: We used SYMBA to create synthetic observations for the Event Horizon Telescope (EHT), a millimetre VLBI array, which has recently captured the first image of a black hole shadow. After testing SYMBA with simple source and corruption models, we study the importance of including all corruption and calibration effects, compared to the addition of thermal noise only. Using synthetic data based on two example general relativistic magnetohydrodynamics (GRMHD) model images of M 87, we performed case studies to assess the image quality that can be obtained with the current and future EHT array for different weather conditions. RESULTS: Our synthetic observations show that the effects of atmospheric and instrumental corruptions on the measured visibilities are significant. Despite these effects, we demonstrate how the overall structure of our GRMHD source models can be recovered robustly with the EHT2017 array after performing calibration steps, which include fringe fitting, a priori amplitude and network calibration, and self-calibration. With the planned addition of new stations to the EHT array in the coming years, images could be reconstructed with higher angular resolution and dynamic range. In our case study, these improvements allowed for a distinction between a thermal and a non-thermal GRMHD model based on salient features in reconstructed images."
JOHN MATTHEWS,The long-baseline neutrino experiment: exploring fundamental symmetries of the universe,"The preponderance of matter over antimatter in the early Universe, the dynamics of the supernova bursts that produced the heavy elements necessary for life and whether protons eventually decay --- these mysteries at the forefront of particle physics and astrophysics are key to understanding the early evolution of our Universe, its current state and its eventual fate. The Long-Baseline Neutrino Experiment (LBNE) represents an extensively developed plan for a world-class experiment dedicated to addressing these questions. LBNE is conceived around three central components: (1) a new, high-intensity neutrino source generated from a megawatt-class proton accelerator at Fermi National Accelerator Laboratory, (2) a near neutrino detector just downstream of the source, and (3) a massive liquid argon time-projection chamber deployed as a far detector deep underground at the Sanford Underground Research Facility. This facility, located at the site of the former Homestake Mine in Lead, South Dakota, is approximately 1,300 km from the neutrino source at Fermilab -- a distance (baseline) that delivers optimal sensitivity to neutrino charge-parity symmetry violation and mass ordering effects. This ambitious yet cost-effective design incorporates scalability and flexibility and can accommodate a variety of upgrades and contributions. With its exceptional combination of experimental configuration, technical capabilities, and potential for transformative discoveries, LBNE promises to be a vital facility for the field of particle physics worldwide, providing physicists from around the globe with opportunities to collaborate in a twenty to thirty year program of exciting science. In this document we provide a comprehensive overview of LBNE's scientific objectives, its place in the landscape of neutrino physics worldwide, the technologies it will incorporate and the capabilities it will possess."
JOHN MATTHEWS,Broadband multi-wavelength properties of M87 during the 2017 Event Horizon Telescope campaign,"In 2017, the Event Horizon Telescope (EHT) Collaboration succeeded in capturing the first direct image of the center of the M87 galaxy. The asymmetric ring morphology and size are consistent with theoretical expectations for a weakly accreting supermassive black hole of mass ∼6.5 × 109 M ⊙. The EHTC also partnered with several international facilities in space and on the ground, to arrange an extensive, quasi-simultaneous multi-wavelength campaign. This Letter presents the results and analysis of this campaign, as well as the multi-wavelength data as a legacy data repository. We captured M87 in a historically low state, and the core flux dominates over HST-1 at high energies, making it possible to combine core flux constraints with the more spatially precise very long baseline interferometry data. We present the most complete simultaneous multi-wavelength spectrum of the active nucleus to date, and discuss the complexity and caveats of combining data from different spatial scales into one broadband spectrum. We apply two heuristic, isotropic leptonic single-zone models to provide insight into the basic source properties, but conclude that a structured jet is necessary to explain M87’s spectrum. We can exclude that the simultaneous γ-ray emission is produced via inverse Compton emission in the same region producing the EHT mm-band emission, and further conclude that the γ-rays can only be produced in the inner jets (inward of HST-1) if there are strongly particle-dominated regions. Direct synchrotron emission from accelerated protons and secondaries cannot yet be excluded."
JOHN MATTHEWS,Event Horizon Telescope imaging of the archetypal blazar 3C 279 at an extreme 20 microarcsecond resolution,"3C 279 is an archetypal blazar with a prominent radio jet that show broadband flux density variability across the entire electromagnetic spectrum. We use an ultra-high angular resolution technique – global Very Long Baseline Interferometry (VLBI) at 1.3 mm (230 GHz) – to resolve the innermost jet of 3C 279 in order to study its fine-scale morphology close to the jet base where highly variable γ-ray emission is thought to originate, according to various models. The source was observed during four days in April 2017 with the Event Horizon Telescope at 230 GHz, including the phased Atacama Large Millimeter/submillimeter Array (ALMA), at an angular resolution of ∼20 μas (at a redshift of z = 0.536 this corresponds to ∼0.13 pc  ∼ 1700 Schwarzschild radii with a black hole mass M<jats:sub>BH</jats:sub> = 8 × 10^8 M_⊙). Imaging and model-fitting techniques were applied to the data to parameterize the fine-scale source structure and its variation. We find a multicomponent inner jet morphology with the northernmost component elongated perpendicular to the direction of the jet, as imaged at longer wavelengths. The elongated nuclear structure is consistent on all four observing days and across different imaging methods and model-fitting techniques, and therefore appears robust. Owing to its compactness and brightness, we associate the northern nuclear structure as the VLBI “core”. This morphology can be interpreted as either a broad resolved jet base or a spatially bent jet. We also find significant day-to-day variations in the closure phases, which appear most pronounced on the triangles with the longest baselines. Our analysis shows that this variation is related to a systematic change of the source structure. Two inner jet components move non-radially at apparent speeds of ∼15 c and ∼20 c (∼1.3 and ∼1.7 μas day^−1, respectively), which more strongly supports the scenario of traveling shocks or instabilities in a bent, possibly rotating jet. The observed apparent speeds are also coincident with the 3C 279 large-scale jet kinematics observed at longer (cm) wavelengths, suggesting no significant jet acceleration between the 1.3 mm core and the outer jet. The intrinsic brightness temperature of the jet components are ≲10^10 K, a magnitude or more lower than typical values seen at ≥7 mm wavelengths. The low brightness temperature and morphological complexity suggest that the core region of 3C 279 becomes optically thin at short (mm) wavelengths."
JOHN MATTHEWS,First M87 Event Horizon Telescope results. II. Array and instrumentation,"The Event Horizon Telescope (EHT) is a very long baseline interferometry (VLBI) array that comprises millimeter- and submillimeter-wavelength telescopes separated by distances comparable to the diameter of the Earth. At a nominal operating wavelength of ~1.3 mm, EHT angular resolution (λ/D) is ~25 μas, which is sufficient to resolve nearby supermassive black hole candidates on spatial and temporal scales that correspond to their event horizons. With this capability, the EHT scientific goals are to probe general relativistic effects in the strong-field regime and to study accretion and relativistic jet formation near the black hole boundary. In this Letter we describe the system design of the EHT, detail the technology and instrumentation that enable observations, and provide measures of its performance. Meeting the EHT science objectives has required several key developments that have facilitated the robust extension of the VLBI technique to EHT observing wavelengths and the production of instrumentation that can be deployed on a heterogeneous array of existing telescopes and facilities. To meet sensitivity requirements, high-bandwidth digital systems were developed that process data at rates of 64 gigabit s−1, exceeding those of currently operating cm-wavelength VLBI arrays by more than an order of magnitude. Associated improvements include the development of phasing systems at array facilities, new receiver installation at several sites, and the deployment of hydrogen maser frequency standards to ensure coherent data capture across the array. These efforts led to the coordination and execution of the first Global EHT observations in 2017 April, and to event-horizon-scale imaging of the supermassive black hole candidate in M87."
JOHN MATTHEWS,First M87 Event Horizon Telescope results. I. The shadow of the supermassive black hole,"When surrounded by a transparent emission region, black holes are expected to reveal a dark shadow caused by gravitational light bending and photon capture at the event horizon. To image and study this phenomenon, we have assembled the Event Horizon Telescope, a global very long baseline interferometry array observing at a wavelength of 1.3 mm. This allows us to reconstruct event-horizon-scale images of the supermassive black hole candidate in the center of the giant elliptical galaxy M87. We have resolved the central compact radio source as an asymmetric bright emission ring with a diameter of 42 ± 3 μas, which is circular and encompasses a central depression in brightness with a flux ratio gsim10:1. The emission ring is recovered using different calibration and imaging schemes, with its diameter and width remaining stable over four different observations carried out in different days. Overall, the observed image is consistent with expectations for the shadow of a Kerr black hole as predicted by general relativity. The asymmetry in brightness in the ring can be explained in terms of relativistic beaming of the emission from a plasma rotating close to the speed of light around a black hole. We compare our images to an extensive library of ray-traced general-relativistic magnetohydrodynamic simulations of black holes and derive a central mass of M = (6.5 ± 0.7) × 10^9 M ⊙. Our radio-wave observations thus provide powerful evidence for the presence of supermassive black holes in centers of galaxies and as the central engines of active galactic nuclei. They also present a new tool to explore gravity in its most extreme limit and on a mass scale that was so far not accessible."
JOHN MATTHEWS,"Corals of the genus Porites are a locally abundant component of the epibiont community on mangrove prop roots at Calabash Caye, Turneffe Atoll, Belize","Mangroves are generally regarded as inhospitable for corals, but recent reports suggest they provide ecological refuge for some species. We surveyed diverse mangrove habitats on Turneffe Atoll, Belize, documenting 127 colonies of Porites divaricata (Thin Finger Coral) along 1858 m of mangrove prop roots at Calabash Caye and a much more diverse coral assemblage at Crooked Creek. At Calabash, corals were highly clumped, and varied widely in size and morphology, including large well-arborized colonies, encrusting forms with few branches, and new recruits with no branches, suggesting an age-structuredpopulation exhibiting extensive morphological plasticity. The data described here contributeto an emerging picture of mangroves as potentially critical habitat for many Caribbeancoral species."
JOHN MATTHEWS,A new paradigm for pandemic preparedness,"PURPOSE OF REVIEW: Preparing for pandemics requires a degree of interdisciplinary work that is challenging under the current paradigm. This review summarizes the challenges faced by the field of pandemic science and proposes how to address them. RECENT FINDINGS: The structure of current siloed systems of research organizations hinders effective interdisciplinary pandemic research. Moreover, effective pandemic preparedness requires stakeholders in public policy and health to interact and integrate new findings rapidly, relying on a robust, responsive, and productive research domain. Neither of these requirements are well supported under the current system. SUMMARY: We propose a new paradigm for pandemic preparedness wherein interdisciplinary research and close collaboration with public policy and health practitioners can improve our ability to prevent, detect, and treat pandemics through tighter integration among domains, rapid and accurate integration, and translation of science to public policy, outreach and education, and improved venues and incentives for sustainable and robust interdisciplinary work."
JOHN MATTHEWS,First Sagittarius A* Event Horizon Telescope results. III. Imaging of the Galactic center supermassive black hole,"We present the first event-horizon-scale images and spatiotemporal analysis of Sgr A* taken with the Event Horizon Telescope in 2017 April at a wavelength of 1.3 mm. Imaging of Sgr A* has been conducted through surveys over a wide range of imaging assumptions using the classical CLEAN algorithm, regularized maximum likelihood methods, and a Bayesian posterior sampling method. Different prescriptions have been used to account for scattering effects by the interstellar medium toward the Galactic center. Mitigation of the rapid intraday variability that characterizes Sgr A* has been carried out through the addition of a “variability noise budget” in the observed visibilities, facilitating the reconstruction of static full-track images. Our static reconstructions of Sgr A* can be clustered into four representative morphologies that correspond to ring images with three different azimuthal brightness distributions and a small cluster that contains diverse nonring morphologies. Based on our extensive analysis of the effects of sparse (u, v)-coverage, source variability, and interstellar scattering, as well as studies of simulated visibility data, we conclude that the Event Horizon Telescope Sgr A* data show compelling evidence for an image that is dominated by a bright ring of emission with a ring diameter of ∼50 μas, consistent with the expected “shadow” of a 4 × 106 M⊙ black hole in the Galactic center located at a distance of 8 kpc."
JOHN MATTHEWS,Characterizing and mitigating intraday variability: reconstructing source structure in accreting black holes with mm-VLBI,"The extraordinary physical resolution afforded by the Event Horizon Telescope has opened a window onto the astrophysical phenomena unfolding on horizon scales in two known black holes, M87* and Sgr A*. However, with this leap in resolution has come a new set of practical complications. Sgr A* exhibits intraday variability that violates the assumptions underlying Earth aperture synthesis, limiting traditional image reconstruction methods to short timescales and data sets with very sparse (u, v) coverage. We present a new set of tools to detect and mitigate this variability. We develop a data-driven, model-agnostic procedure to detect and characterize the spatial structure of intraday variability. This method is calibrated against a large set of mock data sets, producing an empirical estimator of the spatial power spectrum of the brightness fluctuations. We present a novel Bayesian noise modeling algorithm that simultaneously reconstructs an average image and statistical measure of the fluctuations about it using a parameterized form for the excess variance in the complex visibilities not otherwise explained by the statistical errors. These methods are validated using a variety of simulated data, including general relativistic magnetohydrodynamic simulations appropriate for Sgr A* and M87*. We find that the reconstructed source structure and variability are robust to changes in the underlying image model. We apply these methods to the 2017 EHT observations of M87*, finding evidence for variability across the EHT observing campaign. The variability mitigation strategies presented are widely applicable to very long baseline interferometry observations of variable sources generally, for which they provide a data-informed averaging procedure and natural characterization of inter-epoch image consistency."
JOHN MATTHEWS,GJ 1252 b: A 1.2 R ⊕ planet transiting an M3 dwarf at 20.4 pc,"We report the discovery of GJ 1252 b, a planet with a radius of 1.193 ± 0.074 R⊕ and an orbital period of 0.52 days around an M3-type star (0.381 ± 0.019 M⊙, 0.391 ± 0.020 R⊙) located 20.385 ± 0.019 pc away. We use TESS data, ground-based photometry and spectroscopy, Gaia astrometry, and high angular resolution imaging to show that the transit signal seen in the TESS data must originate from a transiting planet. We do so by ruling out all false positive scenarios that attempt to explain the transit signal as originating from an eclipsing stellar binary. Precise Doppler monitoring also leads to a tentative mass measurement of 2.09 ± 0.56 M⊕. The host star proximity, brightness (V = 12.19 mag, K = 7.92 mag), low stellar activity, and the system’s short orbital period make this planet an attractive target for detailed characterization, including precise mass measurement, looking for other objects in the system, and planet atmosphere characterization."
JOHN MATTHEWS,Psychometric properties of the mock interview rating scale for schizophrenia and other serious mental illnesses,"BACKGROUND: Over the past 10 years, job interview training has emerged as an area of study among adults with schizophrenia and other serious mental illnesses who face significant challenges when navigating job interviews. The field of mental health services research has limited access to assessments of job interview skills with rigorously evaluated psychometric properties. OBJECTIVE: We sought to evaluate the initial psychometric properties of a measure assessing job interview skills via role-play performance. METHODS: As part of a randomized controlled trial, 90 adults with schizophrenia or other serious mental illnesses completed a job interview role-play assessment with eight items (and scored using anchors) called the mock interview rating scale (MIRS). A classical test theory analysis was conducted including confirmatory factor analyses, Rasch model analysis and calibration, and differential item functioning; along with inter-rater, internal consistency, and test-retest reliabilities. Pearson correlations were used to evaluate construct, convergent, divergent, criterion, and predictive validity by correlating the MIRS with demographic, clinical, cognitive, work history measures, and employment outcomes. RESULTS: Our analyses resulted in the removal of a single item (sounding honest) and yielded a unidimensional total score measurement with support for its inter-rater reliability, internal consistency, and test-retest reliability. There was initial support for the construct, convergent, criterion, and predictive validities of the MIRS, as it correlated with measures of social competence, neurocognition, valuing job interview training, and employment outcomes. Meanwhile, the lack of correlations with race, physical health, and substance abuse lent support for divergent validity. CONCLUSION: This study presents initial evidence that the seven-item version of the MIRS has acceptable psychometric properties supporting its use to assess job interview skills reliably and validly among adults with schizophrenia and other serious mental illnesses. CLINICAL TRIAL REGISTRATION: NCT03049813."
JOHN M BUTLER,"Bostonia: v. 16, no. 1-9",
JOHN M BUTLER,"Bostonia: v. 19, no. 1-2, 4-9",
JOHN M BUTLER,"The medical student: v. 13, no. 1, 3-8.",
JOHN M BUTLER,"The medical student: v. 12, no. 2-8",
THOMAS BERGER,How to keep it adequate: a protocol for ensuring validity in agent-based simulation,
THOMAS BERGER,The long-baseline neutrino experiment: exploring fundamental symmetries of the universe,"The preponderance of matter over antimatter in the early Universe, the dynamics of the supernova bursts that produced the heavy elements necessary for life and whether protons eventually decay --- these mysteries at the forefront of particle physics and astrophysics are key to understanding the early evolution of our Universe, its current state and its eventual fate. The Long-Baseline Neutrino Experiment (LBNE) represents an extensively developed plan for a world-class experiment dedicated to addressing these questions. LBNE is conceived around three central components: (1) a new, high-intensity neutrino source generated from a megawatt-class proton accelerator at Fermi National Accelerator Laboratory, (2) a near neutrino detector just downstream of the source, and (3) a massive liquid argon time-projection chamber deployed as a far detector deep underground at the Sanford Underground Research Facility. This facility, located at the site of the former Homestake Mine in Lead, South Dakota, is approximately 1,300 km from the neutrino source at Fermilab -- a distance (baseline) that delivers optimal sensitivity to neutrino charge-parity symmetry violation and mass ordering effects. This ambitious yet cost-effective design incorporates scalability and flexibility and can accommodate a variety of upgrades and contributions. With its exceptional combination of experimental configuration, technical capabilities, and potential for transformative discoveries, LBNE promises to be a vital facility for the field of particle physics worldwide, providing physicists from around the globe with opportunities to collaborate in a twenty to thirty year program of exciting science. In this document we provide a comprehensive overview of LBNE's scientific objectives, its place in the landscape of neutrino physics worldwide, the technologies it will incorporate and the capabilities it will possess."
DIMITRIJE STAMENOVIC,Dependence of tensional homeostasis on cell type and on cell-cell interactions,"INTRODUCTION The ability to maintain a homeostatic level of cell tension is essential for many physiological processes. Our group has recently reported that multicellularity is required for tensional homeostasis in endothelial cells. However, other studies have shown that isolated fibroblasts also maintain constant tension over short time scales without the need of cell–cell contacts. Therefore, in this study, our aim was to determine how different cell types regulate tension as isolated cells or in small clustered groupings and to investigate the role of cell–cell adhesion molecules, such as E-cadherin, in this system. METHODS Micropattern traction force microscopy was used to determine how bovine aortic endothelial cells, bovine vascular smooth muscle cells, mouse embryonic fibroblasts, and human gastric adenocarcinoma cells, with or without cell–cell interactions due to E-cadherin, maintain tensional homeostasis over time. Tension temporal fluctuations in single cells and cell clusters were evaluated. RESULTS We found that only endothelial cells require clustering for tensional homeostasis. The same was not verified in fibroblasts or vascular smooth muscle cells. Of relevance, in adenocarcinoma cells, we verified that tensional homeostasis was dependent on the competence of the adhesion molecule E-cadherin at both the single cells and multicellular levels. CONCLUSION These findings indicate that cell–cell contacts may be critical for tensional homeostasis and, potentially, for barrier function of the endothelium. Furthermore, the cell–cell adhesion molecule E-cadherin is an important regulator of tensional homeostasis, even in the absence of cadherin engagement with neighboring cells, which demonstrates its relevance not only as a structural molecule but also as a signaling moiety."
ELKE MUHLBERGER,Reverse Genetic Characterization of the Natural Genomic Deletion in SARS-Coronavirus Strain Frankfurt-1 Open Reading Frame 7B Reveals an Attenuating Function of the 7B Protein in-Vitro and in-Vivo,"During the outbreak of SARS in 2002/3, a prototype virus was isolated from a patient in Frankfurt/Germany (strain Frankfurt-1). As opposed to all other SARS-Coronavirus strains, Frankfurt-1 has a 45-nucleotide deletion in the transmembrane domain of its ORF 7b protein. When over-expressed in HEK 293 cells, the full-length protein but not the variant with the deletion caused interferon beta induction and cleavage of procaspase 3. To study the role of ORF 7b in the context of virus replication, we cloned a full genome cDNA copy of Frankfurt-1 in a bacterial artificial chromosome downstream of a T7 RNA polymerase promoter. Transfection of capped RNA transcribed from this construct yielded infectious virus that was indistinguishable from the original virus isolate. The presumed Frankfurt-1 ancestor with an intact ORF 7b was reconstructed. In CaCo-2 and HUH7 cells, but not in Vero cells, the variant carrying the ORF 7b deletion had a replicative advantage against the parental virus (4- and 6-fold increase of virus RNA in supernatant, respectively). This effect was neither associated with changes in the induction or secretion of type I interferon, nor with altered induction of apoptosis in cell culture. However, pretreatment of cells with interferon beta caused the deleted virus to replicate to higher titers than the parental strain (3.4-fold in Vero cells, 7.9-fold in CaCo-2 cells). In Syrian Golden Hamsters inoculated intranasally with 10e4 plaque forming units of either virus, mean titers of infectious virus and viral RNA in the lungs after 24 h were increased 23- and 94.8-fold, respectively, with the deleted virus. This difference could explain earlier observations of enhanced virulence of Frankfurt-1 in Hamsters as compared to other SARS-Coronavirus reference strains and identifies the SARS-CoV 7b protein as an attenuating factor with the SARS-Coronavirus genome. Because attenuation was focused on the early phase of infection in-vivo, ORF 7b might have contributed to the delayed accumulation of virus in patients that was suggested to have limited the spread of the SARS epidemic."
ELKE MUHLBERGER,Marburg Virus Evades Interferon Responses by a Mechanism Distinct from Ebola Virus,"Previous studies have demonstrated that Marburg viruses (MARV) and Ebola viruses (EBOV) inhibit interferon (IFN)-α/β signaling but utilize different mechanisms. EBOV inhibits IFN signaling via its VP24 protein which blocks the nuclear accumulation of tyrosine phosphorylated STAT1. In contrast, MARV infection inhibits IFN/α/β induced tyrosine phosphorylation of STAT1 and STAT2. MARV infection is now demonstrated to inhibit not only IFNα/β but also IFNγ-induced STAT phosphorylation and to inhibit the IFNα/β and IFNγ-induced tyrosine phosphorylation of upstream Janus (Jak) family kinases. Surprisingly, the MARV matrix protein VP40, not the MARV VP24 protein, has been identified to antagonize Jak and STAT tyrosine phosphorylation, to inhibit IFNα/β or IFNγ-induced gene expression and to inhibit the induction of an antiviral state by IFNα/β. Global loss of STAT and Jak tyrosine phosphorylation in response to both IFNa/α/β and IFNγ is reminiscent of the phenotype seen in Jak1-null cells. Consistent with this model, MARV infection and MARV VP40 expression also inhibit the Jak1-dependent, IL-6-induced tyrosine phosphorylation of STAT1 and STAT3. Finally, expression of MARV VP40 is able to prevent the tyrosine phosphorylation of Jak1, STAT1, STAT2 or STAT3 which occurs following over-expression of the Jak1 kinase. In contrast, MARV VP40 does not detectably inhibit the tyrosine phosphorylation of STAT2 or Tyk2 when Tyk2 is over-expressed. Mutation of the VP40 late domain, essential for efficient VP40 budding, has no detectable impact on inhibition of IFN signaling. This study shows that MARV inhibits IFN signaling by a mechanism different from that employed by the related EBOV. It identifies a novel function for the MARV VP40 protein and suggests that MARV may globally inhibit Jak1-dependent cytokine signaling. Author SummaryThe closely related members of the filovirus family, Ebola virus (EBOV) and Marburg virus (MARV), cause severe hemorrhagic disease in humans with high fatality rates. Infected individuals exhibit dysregulated immune responses which appear to result from several factors, including virus-mediated impairment of innate immune responses. Previous studies demonstrated that both MARV and EBOV block the type I interferon-induced Jak-STAT signaling pathway. For EBOV, the viral protein VP24 mediates the inhibitory effects by interfering with the nuclear translocation of activated STAT proteins. Here, we show that MARV uses a distinct mechanism to block IFN signaling pathways. Our data revealed that MARV blocks the phosphorylation of Janus kinases and their target STAT proteins in response to type I and type II interferon and interleukin 6. Surprisingly, the observed inhibition is not achieved by the MARV VP24 protein, but by the matrix protein VP40 which also mediates viral budding. Over-expression studies indicate that MARV VP40 globally antagonizes Jak1-dependent signaling. Further, we show that a MARV VP40 mutant defective for budding retains interferon antagonist function. Our results highlight a basic difference between EBOV and MARV, define a new function for MARV VP40 and reveal new targets for the development of anti-MARV therapies."
URI T EDEN,The problem of perfect predictors in statistical spike train models,
URI T EDEN,Characterizing the spiking dynamics of subthalamic nucleus neurons in Parkinson's disease using generalized linear models,"Accurately describing the spiking patterns of neurons in the subthalamic nucleus (STN) of patients suffering from Parkinson's disease (PD) is important for understanding the pathogenesis of the disease and for achieving the maximum therapeutic benefit from deep brain stimulation (DBS). We analyze the spiking activity of 24 subthalamic neurons recorded in Parkinson's patients during a directed hand movement task by using a point process generalized linear model (GLM). The model relates each neuron's spiking probability simultaneously to factors associated with movement planning and execution, directional selectivity, refractoriness, bursting, and oscillatory dynamics. The model indicated that while short-term history dependence related to refractoriness and bursting are most informative in predicting spiking activity, nearly all of the neurons analyzed have a structured pattern of long-term history dependence such that the spiking probability was reduced 20-30 ms and then increased 30-60 ms after a previous spike. This suggests that the previously described oscillatory firing of neurons in the STN of Parkinson's patients during volitional movements is composed of a structured pattern of inhibition and excitation. This point process model provides a systematic framework for characterizing the dynamics of neuronal activity in STN."
URI T EDEN,A common goodness-of-fit framework for neural population models using marked point process time-rescaling,"A critical component of any statistical modeling procedure is the ability to assess the goodness-of-fit between a model and observed data. For spike train models of individual neurons, many goodness-of-fit measures rely on the time-rescaling theorem and assess model quality using rescaled spike times. Recently, there has been increasing interest in statistical models that describe the simultaneous spiking activity of neuron populations, either in a single brain region or across brain regions. Classically, such models have used spike sorted data to describe relationships between the identified neurons, but more recently clusterless modeling methods have been used to describe population activity using a single model. Here we develop a generalization of the time-rescaling theorem that enables comprehensive goodness-of-fit analysis for either of these classes of population models. We use the theory of marked point processes to model population spiking activity, and show that under the correct model, each spike can be rescaled individually to generate a uniformly distributed set of events in time and the space of spike marks. After rescaling, multiple well-established goodness-of-fit procedures and statistical tests are available. We demonstrate the application of these methods both to simulated data and real population spiking in rat hippocampus. We have made the MATLAB and Python code used for the analyses in this paper publicly available through our Github repository at https://github.com/Eden-Kramer-Lab/popTRT."
URI T EDEN,Timing matters: impact of anticonvulsant drug treatment and spikes on seizure risk in benign epilepsy with centrotemporal spikes,"OBJECTIVE: Benign epilepsy with centrotemporal spikes (BECTS) is a common, self-limited epilepsy syndrome affecting school-age children. Classic interictal epileptiform discharges (IEDs) confirm diagnosis, and BECTS is presumed to be pharmacoresponsive. As seizure risk decreases in time with this disease, we hypothesize that the impact of IEDs and anticonvulsive drug (ACD) treatment on the risk of subsequent seizure will differ based on disease duration. METHODS: We calculate subsequent seizure risk following diagnosis in a large retrospective cohort of children with BECTS (n = 130), evaluating the impact of IEDs and ACD treatment in the first, second, third, and fourth years of disease. We use a Kaplan-Meier survival analysis and logistic regression models. Patients were censored if they were lost to follow-up or if they changed group status. RESULTS: Two-thirds of children had a subsequent seizure within 2 years of diagnosis. The majority of children had a subsequent seizure within 3 years despite treatment. The presence of IEDs on electroencephalography (EEG) did not impact subsequent seizure risk early in the disease. By the fourth year of disease, all children without IEDs remained seizure free, whereas one-third of children with IEDs at this stage had a subsequent seizure. Conversely, ACD treatment corresponded with lower risk of seizure early in the disease but did not impact seizure risk in later years. SIGNIFICANCE: In this cohort, the majority of children with BECTS had a subsequent seizure despite treatment. In addition, ACD treatment and IEDs predicted seizure risk at specific points of disease duration. Future prospective studies are needed to validate these exploratory findings."
URI T EDEN,Estimating dynamic signals from trial data with censored values,"Censored data occur commonly in trial-structured behavioral experiments and many other forms of longitudinal data. They can lead to severe bias and reduction of statistical power in subsequent analyses. Principled approaches for dealing with censored data, such as data imputation and methods based on the complete data's likelihood, work well for estimating fixed features of statistical models but have not been extended to dynamic measures, such as serial estimates of an underlying latent variable over time. Here we propose an approach to the censored-data problem for dynamic behavioral signals. We developed a state-space modeling framework with a censored observation process at the trial timescale. We then developed a filter algorithm to compute the posterior distribution of the state process using the available data. We showed that special cases of this framework can incorporate the three most common approaches to censored observations: ignoring trials with censored data, imputing the censored data values, or using the full information available in the data likelihood. Finally, we derived a computationally efficient approximate Gaussian filter that is similar in structure to a Kalman filter, but that efficiently accounts for censored data. We compared the performances of these methods in a simulation study and provide recommendations of approaches to use, based on the expected amount of censored data in an experiment. These new techniques can broadly be applied in many research domains in which censored data interfere with estimation, including survival analysis and other clinical trial applications."
URI T EDEN,Measuring the signal-to-noise ratio of a neuron,"The signal-to-noise ratio (SNR), a commonly used measure of fidelity in physical systems, is defined as the ratio of the squared amplitude or variance of a signal relative to the variance of the noise. This definition is not appropriate for neural systems in which spiking activity is more accurately represented as point processes. We show that the SNR estimates a ratio of expected prediction errors and extend the standard definition to one appropriate for single neurons by representing neural spiking activity using point process generalized linear models (PP-GLM). We estimate the prediction errors using the residual deviances from the PP-GLM fits. Because the deviance is an approximate χ2 random variable, we compute a bias-corrected SNR estimate appropriate for single-neuron analysis and use the bootstrap to assess its uncertainty. In the analyses of four systems neuroscience experiments, we show that the SNRs are −10 dB to −3 dB for guinea pig auditory cortex neurons, −18 dB to −7 dB for rat thalamic neurons, −28 dB to −14 dB for monkey hippocampal neurons, and −29 dB to −20 dB for human subthalamic neurons. The new SNR definition makes explicit in the measure commonly used for physical systems the often-quoted observation that single neurons have low SNRs. The neuron’s spiking history is frequently a more informative covariate for predicting spiking propensity than the applied stimulus. Our new SNR definition extends to any GLM system in which the factors modulating the response can be expressed as separate components of a likelihood function."
URI T EDEN,Successful reconstruction of a physiological circuit with known connectivity from spiking activity alone,"Identifying the structure and dynamics of synaptic interactions between neurons is the first step to understanding neural network dynamics. The presence of synaptic connections is traditionally inferred through the use of targeted stimulation and paired recordings or by post-hoc histology. More recently, causal network inference algorithms have been proposed to deduce connectivity directly from electrophysiological signals, such as extracellularly recorded spiking activity. Usually, these algorithms have not been validated on a neurophysiological data set for which the actual circuitry is known. Recent work has shown that traditional network inference algorithms based on linear models typically fail to identify the correct coupling of a small central pattern generating circuit in the stomatogastric ganglion of the crab Cancer borealis. In this work, we show that point process models of observed spike trains can guide inference of relative connectivity estimates that match the known physiological connectivity of the central pattern generator up to a choice of threshold. We elucidate the necessary steps to derive faithful connectivity estimates from a model that incorporates the spike train nature of the data. We then apply the model to measure changes in the effective connectivity pattern in response to two pharmacological interventions, which affect both intrinsic neural dynamics and synaptic transmission. Our results provide the first successful application of a network inference algorithm to a circuit for which the actual physiological synapses between neurons are known. The point process methodology presented here generalizes well to larger networks and can describe the statistics of neural populations. In general we show that advanced statistical models allow for the characterization of effective network structure, deciphering underlying network dynamics and estimating information-processing capabilities."
URI T EDEN,Dysmature superficial white matter microstructure in developmental focal epilepsy,"Benign epilepsy with centrotemporal spikes is a common childhood epilepsy syndrome that predominantly affects boys, characterized by self-limited focal seizures arising from the perirolandic cortex and fine motor abnormalities. Concurrent with the age-specific presentation of this syndrome, the brain undergoes a developmentally choreographed sequence of white matter microstructural changes, including maturation of association u-fibres abutting the cortex. These short fibres mediate local cortico-cortical communication and provide an age-sensitive structural substrate that could support a focal disease process. To test this hypothesis, we evaluated the microstructural properties of superficial white matter in regions corresponding to u-fibres underlying the perirolandic seizure onset zone in children with this epilepsy syndrome compared with healthy controls. To verify the spatial specificity of these features, we characterized global superficial and deep white matter properties. We further evaluated the characteristics of the perirolandic white matter in relation to performance on a fine motor task, gender and abnormalities observed on EEG. Children with benign epilepsy with centrotemporal spikes (n = 20) and healthy controls (n = 14) underwent multimodal testing with high-resolution MRI including diffusion tensor imaging sequences, sleep EEG recordings and fine motor assessment. We compared white matter microstructural characteristics (axial, radial and mean diffusivity, and fractional anisotropy) between groups in each region. We found distinct abnormalities corresponding to the perirolandic u-fibre region, with increased axial, radial and mean diffusivity and fractional anisotropy values in children with epilepsy (P = 0.039, P = 0.035, P = 0.042 and P = 0.017, respectively). Increased fractional anisotropy in this region, consistent with decreased integrity of crossing sensorimotor u-fibres, correlated with inferior fine motor performance (P = 0.029). There were gender-specific differences in white matter microstructure in the perirolandic region; males and females with epilepsy and healthy males had higher diffusion and fractional anisotropy values than healthy females (P ≤ 0.035 for all measures), suggesting that typical patterns of white matter development disproportionately predispose boys to this developmental epilepsy syndrome. Perirolandic white matter microstructure showed no relationship to epilepsy duration, duration seizure free, or epileptiform burden. There were no group differences in diffusivity or fractional anisotropy in superficial white matter outside of the perirolandic region. Children with epilepsy had increased radial diffusivity (P = 0.022) and decreased fractional anisotropy (P = 0.027) in deep white matter, consistent with a global delay in white matter maturation. These data provide evidence that atypical maturation of white matter microstructure is a basic feature in benign epilepsy with centrotemporal spikes and may contribute to the epilepsy, male predisposition and clinical comorbidities observed in this disorder."
URI T EDEN,Estimating fluctuations in neural representations of uncertain environments,"Neural Coding analyses often reflect an assumption that neural populations respond uniquely and consistently to particular stimuli. For example, analyses of spatial remapping in hippocampal populations often assume that each environment has one unique representation and that remapping occurs over long time scales as an animal traverses between distinct environments. However, as neuroscience experiments begin to explore more naturalistic tasks and stimuli, and reflect more ambiguity in neural representations, methods for analyzing population neural codes must adapt to reflect these features. In this paper, we develop a new state-space modeling framework to address two important issues related to remapping. First, neurons may exhibit significant trial-to-trial or moment-to-moment variability in the firing patterns used to represent a particular environment or stimulus. Second, in ambiguous environments and tasks that involve cognitive uncertainty, neural populations may rapidly fluctuate between multiple representations. The statespace model addresses these two issues by integrating an observation model, which allows for multiple representations of the same stimulus or environment, with a state model, which characterizes the moment-by-moment probability of a shift in the neural representation. These models allow us to compute instantaneous estimates of the stimulus or environment currently represented by the population. We demonstrate the application of this approach to the analysis of population activity in the CA1 region of hippocampus of a mouse moving through ambiguous virtual environments. Our analyses demonstrate that many hippocampal cells express significant trial-to-trial variability in their representations and that the population representation can fluctuate rapidly between environments within a single trial when spatial cues are most ambiguous."
URI T EDEN,Human seizures couple across spatial scales through travelling wave dynamics,"Epilepsy-the propensity toward recurrent, unprovoked seizures-is a devastating disease affecting 65 million people worldwide. Understanding and treating this disease remains a challenge, as seizures manifest through mechanisms and features that span spatial and temporal scales. Here we address this challenge through the analysis and modelling of human brain voltage activity recorded simultaneously across microscopic and macroscopic spatial scales. We show that during seizure large-scale neural populations spanning centimetres of cortex coordinate with small neural groups spanning cortical columns, and provide evidence that rapidly propagating waves of activity underlie this increased inter-scale coupling. We develop a corresponding computational model to propose specific mechanisms-namely, the effects of an increased extracellular potassium concentration diffusing in space-that support the observed spatiotemporal dynamics. Understanding the multi-scale, spatiotemporal dynamics of human seizures-and connecting these dynamics to specific biological mechanisms-promises new insights to treat this devastating disease."
URI T EDEN,Human seizures couple across spatial scales through travelling wave dynamics,"Epilepsy—the propensity toward recurrent, unprovoked seizures—is a devastating disease affecting 65 million people worldwide. Understanding and treating this disease remains a challenge, as seizures manifest through mechanisms and features that span spatial and temporal scales. Here we address this challenge through the analysis and modelling of human brain voltage activity recorded simultaneously across microscopic and macroscopic spatial scales. We show that during seizure large-scale neural populations spanning centimetres of cortex coordinate with small neural groups spanning cortical columns, and provide evidence that rapidly propagating waves of activity underlie this increased inter-scale coupling. We develop a corresponding computational model to propose specific mechanisms—namely, the effects of an increased extracellular potassium concentration diffusing in space—that support the observed spatiotemporal dynamics. Understanding the multi-scale, spatiotemporal dynamics of human seizures—and connecting these dynamics to specific biological mechanisms—promises new insights to treat this devastating disease."
URI T EDEN,"Assessing dynamics, spatial scale, and uncertainty in task-related brain network analyses","The brain is a complex network of interconnected elements, whose interactions evolve dynamically in time to cooperatively perform specific functions. A common technique to probe these interactions involves multi-sensor recordings of brain activity during a repeated task. Many techniques exist to characterize the resulting task-related activity, including establishing functional networks, which represent the statistical associations between brain areas. Although functional network inference is commonly employed to analyze neural time series data, techniques to assess the uncertainty—both in the functional network edges and the corresponding aggregate measures of network topology—are lacking. To address this, we describe a statistically principled approach for computing uncertainty in functional networks and aggregate network measures in task-related data. The approach is based on a resampling procedure that utilizes the trial structure common in experimental recordings. We show in simulations that this approach successfully identifies functional networks and associated measures of confidence emergent during a task in a variety of scenarios, including dynamically evolving networks. In addition, we describe a principled technique for establishing functional networks based on predetermined regions of interest using canonical correlation. Doing so provides additional robustness to the functional network inference. Finally, we illustrate the use of these methods on example invasive brain voltage recordings collected during an overt speech task. The general strategy described here—appropriate for static and dynamic network inference and different statistical measures of coupling—permits the evaluation of confidence in network measures in a variety of settings common to neuroscience."
IAN J SUE WING,Increasing ambient temperature reduces emotional well-being,"This study examines the impact of ambient temperature on emotional well-being in the U.S. population aged 18+. The U.S. is an interesting test case because of its resources, technology and variation in climate across different areas, which also allows us to examine whether adaptation to different climates could weaken or even eliminate the impact of heat on well-being. Using survey responses from 1.9 million Americans over the period from 2008 to 2013, we estimate the effect of temperature on well-being from exogenous day-to-day temperature variation within respondents’ area of residence and test whether this effect varies across areas with different climates. We find that increasing temperatures significantly reduce well-being. Compared to average daily temperatures in the 50–60 °F (10–16 °C) range, temperatures above 70 °F (21 °C) reduce positive emotions (e.g. joy, happiness), increase negative emotions (e.g. stress, anger), and increase fatigue (feeling tired, low energy). These effects are particularly strong among less educated and older Americans. However, there is no consistent evidence that heat effects on well-being differ across areas with mild and hot summers, suggesting limited variation in heat adaptation."
IAN J SUE WING,"Making the great transformation, November 13, 14, and 15, 2003","The conference discussants and participants analyze why transitions happen, and why they matter. Transitions are those wide-ranging changes in human organization and well being that can be convincingly attributed to a concerted set of choices that make the world that was significantly and recognizably different from the world that becomes. Transition scholars argue that that history does not just stumble along a pre-determined path, but that human ingenuity and entrepreneurship have the ability to fundamentally alter its direction. However, our ability to ‘will’ such transitions remains in doubt. These doubts cannot be removed until we have a better understanding of how transitions work."
AHMAD KHALIL,Design and modular assembly of synthetic intramembrane proteolysis receptors for custom gene regulation in therapeutic cells,"[Synthetic biology has established powerful tools to precisely control cell function. Engineering these systems to meet clinical requirements has enormous medical implications. Here, we adopted a clinically driven design process to build receptors for the autonomous control of therapeutic cells. We examined the function of key domains involved in regulated intramembrane proteolysis and showed that systematic modular engineering can generate a class of receptors we call SyNthetic Intramembrane Proteolysis Receptors (SNIPRs) that have tunable sensing and transcriptional response abilities. We demonstrate the potential transformative utility of the receptor platform by engineering human primary T cells for multi-antigen recognition and production of dosed, bioactive payloads relevant to the treatment of disease. Our design framework enables the development of fully humanized and customizable transcriptional receptors for the programming of therapeutic cells suitable for clinical translation.]"
AHMAD KHALIL,Engineering clinically-approved drug gated CAR circuits,"[Chimeric antigen receptor (CAR) T cell immunotherapy has the potential to revolutionize cancer medicine. However, excessive CAR activation, lack of tumor-specific surface markers, and antigen escape have limited the safety and efficacy of CAR T cell therapy. A multi-antigen targeting CAR system that is regulated by safe, clinically-approved pharmaceutical agents is urgently needed, yet only a few simple systems have been developed, and even fewer have been evaluated for efficacy in vivo. Here, we present NASCAR (NS3 ASsociated CAR), a collection of induc-ible ON and OFF switch CAR circuits engineered with a NS3 protease domain deriving from the Hepatitis C Virus (HCV). We establish their ability to regulate CAR activity using multiple FDA-approved antiviral protease inhibitors, including grazoprevir (GZV), both in vitro and in a xenograft tumor model. In addition, we have engineered several dual-gated NASCAR circuits, consisting of an AND logic gate CAR, universal ON-OFF CAR, and a switchboard CAR. These engineered receptors enhance control over T cell activity and tumor-targeting specificity. Together, our com-prehensive set of multiplex drug-gated CAR circuits represent a dynamic, tunable, and clinically-ready set of modules for enhancing the safety of CAR T cell therapy.]"
AHMAD KHALIL,Clinically-driven design of synthetic gene regulatory programs in human cells,"Synthetic biology seeks to enable the rational design of regulatory molecules and circuits to reprogram cellular behavior. The application of this approach to human cells could lead to powerful gene and cell-based therapies that provide transformative ways to combat complex diseases. To date, however, synthetic genetic circuits are challenging to implement in clinically-relevant cell types and their components often present translational incompatibilities, greatly limiting the feasibility, efficacy and safety of this approach. Here, using a clinically-driven design process, we developed a toolkit of programmable synthetic transcription regulators that feature a compact human protein-based design, enable precise genome-orthogonal regulation, and can be modulated by FDA-approved small molecules. We demonstrate the toolkit by engineering therapeutic human immune cells with genetic programs that enable titratable production of immunotherapeutics, drug-regulated control of tumor killing in vivo and in 3D spheroid models, and the first multi-channel synthetic switch for independent control of immunotherapeutic genes. Our work establishes a powerful platform for engineering custom gene expression programs in mammalian cells with the potential to accelerate clinical translation of synthetic systems."
CHING-TI LIU,Patterns of Co-Expression for Protein Complexes by Size in Saccharomyces Cerevisiae,"Many successful functional studies by gene expression profiling in the literature have led to the perception that profile similarity is likely to imply functional association. But how true is the converse of the above statement? Do functionally associated genes tend to be co-regulated at the transcription level? In this paper, we focus on a set of well-validated yeast protein complexes provided by Munich Information Center for Protein Sequences (MIPS). Using four well-known large-scale microarray expression data sets, we computed the correlations between genes from the same complex. We then analyzed the relationship between the distribution of correlations and the complex size (the number of genes in a protein complex). We found that except for a few large protein complexes, such as mitochondrial ribosomal and cytoplasmic ribosomal proteins, the correlations are on the average not much higher than that from a pair of randomly selected genes. The global impact of large complexes on the expression of other genes in the genome is also studied. Our result also showed that the expression of over 85% of the genes are affected by six large complexes: the cytoplasmic ribosomal complex, mitochondrial ribosomal complex, proteasome complex, F0/F1 ATP synthase (complex V) (size 18), rRNA splicing (size 24) and H+- transporting ATPase, vacular (size 15)."
CHING-TI LIU,Incorporating Biological Knowledge in the Search for Gene × Gene Interaction in Genome-Wide Association Studies,"We sought to find significant gene × gene interaction in a genome-wide association analysis of rheumatoid arthritis (RA) by performing pair-wise tests of interaction among collections of single-nucleotide polymorphisms (SNPs) obtained by one of two methods. The first method involved screening the results of the genome-wide association analysis for main effects p-values < 1 × 10-4. The second method used biological databases such as the Gene Ontology and Kyoto Encyclopedia of Genes and Genomes to define gene collections that each contained one of four genes with known associations with RA: PTPN22, STAT4, TRAF1, and C5. We used a permutation approach to determine whether any of these SNP sets had empirical enrichment of significant interaction effects. We found that the SNP set obtained by the first method was significantly enriched with significant interaction effects (empirical p = 0.003). Additionally, we found that the ""protein complex assembly"" collection of genes from the Gene Ontology collection containing the TRAF1 gene was significantly enriched with interaction effects with p-values < 1 × 10-8 (empirical p = 0.012)."
DAVID SOMERS,Functional correlates of optic flow motion processing in Parkinson’s disease,"The visual input created by the relative motion between an individual and the environment, also called optic flow, influences the sense of self-motion, postural orientation, veering of gait, and visuospatial cognition. An optic flow network comprising visual motion areas V6, V3A, and MT+, as well as visuo-vestibular areas including posterior insula vestibular cortex (PIVC) and cingulate sulcus visual area (CSv), has been described as uniquely selective for parsing egomotion depth cues in humans. Individuals with Parkinson’s disease (PD) have known behavioral deficits in optic flow perception and visuospatial cognition compared to age- and education-matched control adults (MC). The present study used functional magnetic resonance imaging (fMRI) to investigate neural correlates related to impaired optic flow perception in PD. We conducted fMRI on 40 non-demented participants (23 PD and 17 MC) during passive viewing of simulated optic flow motion and random motion. We hypothesized that compared to the MC group, PD participants would show abnormal neural activity in regions comprising this optic flow network. MC participants showed robust activation across all regions in the optic flow network, consistent with studies in young adults, suggesting intact optic flow perception at the neural level in healthy aging. PD participants showed diminished activity compared to MC particularly within visual motion area MT+ and the visuo-vestibular region CSv. Further, activation in visuo-vestibular region CSv was associated with disease severity. These findings suggest that behavioral reports of impaired optic flow perception and visuospatial performance may be a result of impaired neural processing within visual motion and visuo-vestibular regions in PD."
DAVID SOMERS,Visuospatial attention to single and multiple objects Is independently impaired in Parkinson's disease,"Parkinson’s disease (PD) is associated with deficits in visuospatial attention. It is as yet unknown whether these attentional deficits begin at a perceptual level or instead reflect disruptions in oculomotor or higher-order processes. In the present study, non-demented individuals with PD and matched normal control adults (NC) participated in two tasks requiring sustained visuospatial attention, both based on a multiple object tracking paradigm. Eye tracking was used to ensure central fixation. In Experiment 1 (26 PD, 21 NC), a pair of identical red dots (one target, one distractor) rotated randomly for three seconds at varied speeds. The task was to maintain the identity of the sole target, which was labeled prior to each trial. PD were less accurate than NC overall (p = .049). When considering only trials where fixation was maintained, however, there was no significant group difference, suggesting that the deficit’s origin is closely related to oculomotor processing. To determine whether PD had additional impairment in multifocal attention, in Experiment 2 (25 PD, 15 NC), two targets were presented along with distractors at a moderate speed, along with a control condition in which dots remained stationary. PD were less accurate than NC for moving (p = 0.02) but not stationary targets. This group difference remained significant when considering only trials where fixation was maintained, suggesting the source of the PD deficit was independent from oculomotor processing. Taken together, the results implicate separate mechanisms for single vs. multiple object tracking deficits in PD."
DAVID SOMERS,Synchronized Oscillations During Cooperative Feature Lining in a Cortical Model of Visual Perception,"A neural network model of synchronized oscillations in visual cortex is presented to account for recent neurophysiological findings that such synchronization may reflect global properties of the stimulus. In these experiments, synchronization of oscillatory firing responses to moving bar stimuli occurred not only for nearby neurons, but also occurred between neurons separated by several cortical columns (several mm of cortex) when these neurons shared some receptive field preferences specific to the stimuli. These results were obtained for single bar stimuli and also across two disconnected, but colinear, bars moving in the same direction. Our model and computer simulations obtain these synchrony results across both single and double bar stimuli using different, but formally related, models of preattentive visual boundary segmentation and attentive visual object recognition, as well as nearest-neighbor and randomly coupled models."
DAVID SOMERS,Synchronized Oscillations During Cooperative Feature Linking in a Cortical Model of Visual Perception,"A neural network model of synchronized oscillator activity in visual cortex is presented in order to account for recent neurophysiological findings that such synchronization may reflect global properties of the stimulus. In these recent experiments, it was reported that synchronization of oscillatory firing responses to moving bar stimuli occurred not only for nearby neurons, but also occurred between neurons separated by several cortical columns (several mm of cortex) when these neurons shared some receptive field preferences specific to the stimuli. These results were obtained not only for single bar stimuli but also across two disconnected, but colinear, bars moving in the same direction. Our model and computer simulations obtain these synchrony results across both single and double bar stimuli. For the double bar case, synchronous oscillations are induced in the region between the bars, but no oscillations are induced in the regions beyond the stimuli. These results were achieved with cellular units that exhibit limit cycle oscillations for a robust range of input values, but which approach an equilibrium state when undriven. Single and double bar synchronization of these oscillators was achieved by different, but formally related, models of preattentive visual boundary segmentation and attentive visual object recognition, as well as nearest-neighbor and randomly coupled models. In preattentive visual segmentation, synchronous oscillations may reflect the binding of local feature detectors into a globally coherent grouping. In object recognition, synchronous oscillations may occur during an attentive resonant state that triggers new learning. These modelling results support earlier theoretical predictions of synchronous visual cortical oscillations and demonstrate the robustness of the mechanisms capable of generating synchrony."
DAVID SOMERS,Towards neuroscience of the everyday world (NEW) using functional near infrared spectroscopy,"Functional near-infrared spectroscopy (fNIRS) assesses human brain activity by noninvasively measuring changes of cerebral hemoglobin concentrations caused by modulation of neuronal activity. Recent progress in signal processing and advances in system design, such as miniaturization, wearability, and system sensitivity, have strengthened fNIRS as a viable and cost-effective complement to functional magnetic resonance imaging, expanding the repertoire of experimental studies that can be performed by the neuroscience community. The availability of fNIRS and electroencephalography for routine, increasingly unconstrained, and mobile brain imaging is leading toward a new domain that we term “Neuroscience of the Everyday World” (NEW). In this light, we review recent advances in hardware, study design, and signal processing, and discuss challenges and future directions."
SANJAY KRISHNAN,Reading Globalization from the Margin: The Case of Abdullah Munshi,"In this essay I argue that the global perspective, established in the era of modernEuropean imperialism, is given institutional expression as a way of seeing that is engaged—both by ruler and ruled— as the frame of adequate representation. Briefly outlining how this frame operates in historical and cultural studies today, I examine its deployment in mid-nineteenth-century Melaka and Singapore through a reading of the Hikayat Abdullah, a seminal Malay-language text composed by Abdullah bin Abdul Kadir. Although Abdullah self-consciously sets about reproducing the global perspective, I show how this mode of thematization is interrupted and displaced as it brings about an encounter between the diverse and uneven contexts of the native and European worlds."
ELIZABETH HATCH,Urogenital Abnormalities in Men Exposed to Diethylstilbestrol in Utero: A Cohort Study,"BACKGROUND: Diethylstilbestrol (DES), a synthetic estrogen widely prescribed to pregnant women during the 1940s-70s, has been shown to cause reproductive problems in the daughters. Studies of prenatally-exposed males have yielded conflicting results. METHODS: In data from a collaborative follow-up of three U.S. cohorts of DES-exposed sons, we examined the relation of prenatal DES exposure to occurrence of male urogenital abnormalities. Exposure status was determined through review of prenatal records. Mailed questionnaires (1994, 1997, 2001) asked about specified abnormalities of the urogenital tract. Risk ratios (RR) were estimated by Cox regression with constant time at risk and control for year of birth. RESULTS: Prenatal DES exposure was not associated with varicocele, structural abnormalities of the penis, urethral stenosis, benign prostatic hypertrophy, or inflammation/infection of the prostate, urethra, or epididymus. However, RRs were 1.9 (95% confidence interval 1.1-3.4) for cryptorchidism, 2.5 (1.5-4.3) for epididymal cyst, and 2.4 (1.5-4.4) for testicular inflammation/infection. Stronger associations were observed for DES exposure that began before the 11th week of pregnancy: RRs were 2.9 (1.6-5.2) for cryptorchidism, 3.5 (2.0-6.0) for epididymal cyst, and 3.0 (1.7-5.4) for inflammation/infection of testes. CONCLUSION: These results indicate that prenatal exposure to DES increases risk of male urogenital abnormalities and that the association is strongest for exposure that occurs early in gestation. The findings support the hypothesis that endocrine disrupting chemicals may be a cause of the increased prevalence of cryptorchidism that has been seen in recent years."
ELIZABETH HATCH,"Association of urinary phthalate metabolite concentrations with body mass index and waist circumference: a cross-sectional study of NHANES data, 1999–2002","BACKGROUND: Although diet and activity are key factors in the obesity epidemic, laboratory studies suggest that endocrine disrupting chemicals may also affect obesity. METHODS: We analyzed associations between six phthalate metabolites measured in urine and body mass index (BMI) and waist circumference (WC) in National Health and Nutrition Examination Survey (NHANES) participants aged 6–80. We included 4369 participants from NHANES 1999–2002, with data on mono-ethyl (MEP), mono-2-ethylhexyl (MEHP), mono-n-butyl (MBP), and mono-benzyl (MBzP) phthalate; 2286 also had data on mono-2-ethyl-5-hydroxyhexyl (MEHHP) and mono-2-ethyl-5-oxohexyl (MEOHP) phthalate (2001–2002). Using multiple regression, we computed mean BMI and WC within phthalate quartiles in eight age/gender specific models. RESULTS: The most consistent associations were in males aged 20–59; BMI and WC increased across quartiles of MBzP (adjusted mean BMI = 26.7, 27.2, 28.4, 29.0, p-trend = 0.0002), and positive associations were also found for MEOHP, MEHHP, MEP, and MBP. In females, BMI and WC increased with MEP quartile in adolescent girls (adjusted mean BMI = 22.9, 23.8, 24.1, 24.7, p-trend = 0.03), and a similar but less strong pattern was seen in 20–59 year olds. In contrast, MEHP was inversely related to BMI in adolescent girls (adjusted mean BMI = 25.4, 23.8, 23.4, 22.9, p-trend = 0.02) and females aged 20–59 (adjusted mean BMI = 29.9, 29.9, 27.9, 27.6, p-trend = 0.02). There were no important associations among children, but several inverse associations among 60–80 year olds. CONCLUSION: This exploratory, cross-sectional analysis revealed a number of interesting associations with different phthalate metabolites and obesity outcomes, including notable differences by gender and age subgroups. Effects of endocrine disruptors, such as phthalates, may depend upon endogenous hormone levels, which vary dramatically by age and gender. Individual phthalates also have different biologic and hormonal effects. Although our study has limitations, both of these factors could explain some of the variation in the observed associations. These preliminary data support the need for prospective studies in populations at risk for obesity."
ELIZABETH HATCH,"Exposure to Polyfluoroalkyl Chemicals and Cholesterol, Body Weight, and Insulin Resistance in the General U.S. Population","BACKGROUND. Polyfluoroalkyl chemicals (PFCs) are used commonly in commercial applications and are detected in humans and the environment worldwide. Concern has been raised that they may disrupt lipid and weight regulation. OBJECTIVES. We investigated the relationship between PFC serum concentrations and lipid and weight outcomes in a large publicly available data set. METHODS. We analyzed data from the 2003-2004 National Health and Nutrition Examination Survey (NHANES) for participants 12-80 years of age. Using linear regression to control for covariates, we studied the association between serum concentrations of perfluorooctanoic acid (PFOA), perfluorononanoic acid (PFNA), perfluorooctane sulfonic acid (PFOS), and perfluorohexane sulfonic acid (PFHxS) and measures of cholesterol, body size, and insulin resistance. RESULTS. We observed a positive association between concentrations of PFOS, PFOA, and PFNA and total and non-high-density cholesterol. We found the opposite for PFHxS. Those in the highest quartile of PFOS exposure had total cholesterol levels 13.4 mg/dL [95% confidence interval (CI), 3.8-23.0] higher than those in the lowest quartile. For PFOA, PFNA, and PFHxS, effect estimates were 9.8 (95% CI, -0.2 to 19.7), 13.9 (95% CI, 1.9-25.9), and -7.0 (95% CI, -13.2 to -0.8), respectively. A similar pattern emerged when exposures were modeled continuously. We saw little evidence of a consistent association with body size or insulin resistance. CONCLUSIONS. This exploratory cross-sectional study is consistent with other epidemiologic studies in finding a positive association between PFOS and PFOA and cholesterol, despite much lower exposures in NHANES. Results for PFNA and PFHxS are novel, emphasizing the need to study PFCs other than PFOS and PFOA."
ELIZABETH HATCH,Secondary Sex Ratio among Women Exposed to Diethylstilbestrol in Utero,"BACKGROUND. Diethylstilbestrol (DES), a synthetic estrogen widely prescribed to pregnant women during the mid-1900s, is a potent endocrine disruptor. Previous studies have suggested an association between endocrine-disrupting compounds and secondary sex ratio. METHODS. Data were provided by women participating in the National Cancer Institute (NCI) DES Combined Cohort Study. We used generalized estimating equations to estimate odds ratios (ORs) and 95% confidence intervals (CIs) for the relation of in utero DES exposure to sex ratio (proportion of male births). Models were adjusted for maternal age, child's birth year, parity, and cohort, and accounted for clustering among women with multiple pregnancies. RESULTS. The OR for having a male birth comparing DES-exposed to unexposed women was 1.05 (95% CI, 0.95-1.17). For exposed women with complete data on cumulative DES dose and timing (33%), those first exposed to DES earlier in gestation and to higher doses had the highest odds of having a male birth. The ORs were 0.91 (95% C, 0.65-1.27) for first exposure at ≥ 13 weeks gestation to < 5 g DES; 0.95 (95% CI, 0.71-1.27) for first exposure at ≥ 13 weeks to ≥ 5 g; 1.16 (95% CI, 0.96-1.41) for first exposure at < 13 weeks to < 5 g; and 1.24 (95% CI, 1.04-1.48) for first exposure at < 13 weeks to ≥ 5 g compared with no exposure. Results did not vary appreciably by maternal age, parity, cohort, or infertility history. CONCLUSIONS. Overall, no association was observed between in utero DES exposure and secondary sex ratio, but a significant increase in the proportion of male births was found among women first exposed to DES earlier in gestation and to a higher cumulative dose."
ELIZABETH HATCH,Breast Cancer Risk Factors in Relation to Breast Density (United States),"OBJECTIVES: Evaluate known breast cancer risk factors in relation to breast density. METHODS. We examined factors in relation to breast density in 144,018 New Hampshire (NH) women with at least one mammogram recorded in a statewide mammography registry. Mammographic breast density was measured by radiologists using the BI-RADS classification; risk factors of interest were obtained from patient intake forms and questionnaires. RESULTS: Initial analyses showed a strong inverse influence of age and body mass index (BMI) on breast density. In addition, women with late age at menarche, late age at first birth, premenopausal women, and those currently using hormone therapy (HT) tended to have higher breast density, while those with greater parity tended to have less dense breasts. Analyses stratified on age and BMI suggested interactions, which were formally assessed in a multivariable model. The impact of current HT use, relative to nonuse, differed across age groups, with an inverse association in younger women, and a positive association in older women (p < 0.0001 for the interaction). The positive effects of age at menarche and age at first birth, and the inverse influence of parity were less apparent in women with low BMI than in those with high BMI (p = 0.04, p < 0.0001 and p = 0.01, respectively, for the interactions). We also noted stronger positive effects for age at first birth in postmenopausal women (p = 0.004 for the interaction). The multivariable model indicated a slight positive influence of family history of breast cancer. CONCLUSIONS: The influence of age at menarche and reproductive factors on breast density is less evident in women with high BMI. Density is reduced in young women using HT, but increased in HT users of age 50 or more."
DAVID BOAS,MATLAB code and data processing guide for g1OCTA,This guide is for post data processing of g1OCTA which outputs 3D vascular structure with flow direction and minimized tail artifacts.
DAVID BOAS,High-density microfibers as a potential optical interface to reach deep brain regions,"OBJECTIVE: Optical techniques for recording and manipulating neural activity have traditionally been constrained to superficial brain regions due to light scattering. New techniques are needed to extend optical access to large 3D volumes in deep brain areas, while retaining local connectivity. APPROACH: We have developed a method to implant bundles of hundreds or thousands of optical microfibers, each with a diameter of 8 μm. During insertion, each fiber moves independently, following a path of least resistance. The fibers achieve near total internal reflection, enabling optically interfacing with the tissue near each fiber aperture. MAIN RESULTS: At a depth of 3 mm, histology shows fibers consistently splay over 1 mm in diameter throughout the target region. Immunohistochemical staining after chronic implants reveals neurons in close proximity to the fiber tips. Models of photon fluence indicate that fibers can be used as a stimulation light source to precisely activate distinct patterns of neurons by illuminating a subset of fibers in the bundle. By recording fluorescent beads diffusing in water, we demonstrate the recording capability of the fibers. SIGNIFICANCE: Our histology, modeling and fluorescent bead recordings suggest that the optical microfibers may provide a minimally invasive, stable, bidirectional interface for recording or stimulating genetic probes in deep brain regions-a hyper-localized form of fiber photometry."
DAVID BOAS,Phasor analysis of NADH FLIM identifies pharmacological disruptions to mitochondrial metabolic processes in the rodent cerebral cortex,"Investigating cerebral metabolism in vivo at a microscopic level is essential for understanding brain function and its pathological alterations. The intricate signaling and metabolic dynamics between neurons, glia, and microvasculature requires much more detailed understanding to better comprehend the mechanisms governing brain function and its disease-related changes. We recently demonstrated that pharmacologically-induced alterations to different steps of cerebral metabolism can be distinguished utilizing 2-photon fluorescence lifetime imaging of endogenous reduced nicotinamide adenine dinucleotide (NADH) fluorescence in vivo. Here, we evaluate the ability of the phasor analysis method to identify these pharmacological metabolic alterations and compare the method's performance with more conventional nonlinear curve-fitting analysis. Visualization of phasor data, both at the fundamental laser repetition frequency and its second harmonic, enables resolution of pharmacologically-induced alterations to mitochondrial metabolic processes from baseline cerebral metabolism. Compared to our previous classification models based on nonlinear curve-fitting, phasor-based models required fewer parameters and yielded comparable or improved classification accuracy. Fluorescence lifetime imaging of NADH and phasor analysis shows utility for detecting metabolic alterations and will lead to a deeper understanding of cerebral energetics and its pathological changes."
DAVID BOAS,as-PSOCT: Volumetric microscopic imaging of human brain architecture and connectivity.,"Polarization sensitive optical coherence tomography (PSOCT) with serial sectioning has enabled the investigation of 3D structures in mouse and human brain tissue samples. By using intrinsic optical properties of back-scattering and birefringence, PSOCT reliably images cytoarchitecture, myeloarchitecture and fiber orientations. In this study, we developed a fully automatic serial sectioning polarization sensitive optical coherence tomography (as-PSOCT) system to enable volumetric reconstruction of human brain samples with unprecedented sample size and resolution. The 3.5 μm in-plane resolution and 50 μm through-plane voxel size allow inspection of cortical layers that are a single-cell in width, as well as small crossing fibers. We show the abilities of as-PSOCT in quantifying layer thicknesses of the cerebellar cortex and creating microscopic tractography of intricate fiber networks in the subcortical nuclei and internal capsule regions, all based on volumetric reconstructions. as-PSOCT provides a viable tool for studying quantitative cytoarchitecture and myeloarchitecture and mapping connectivity with microscopic resolution in the human brain."
DAVID BOAS,Pericyte degeneration leads to neurovascular uncoupling and limits oxygen supply to brain,"Pericytes are perivascular mural cells of brain capillaries. They are positioned centrally in the neurovascular unit between endothelial cells, astrocytes and neurons. This position allows them to regulate key neurovascular functions of the brain. The role of pericytes in the regulation of cerebral blood flow (CBF) and neurovascular coupling remains, however, under debate. Using loss-of-function pericyte-deficient mice, here we show that pericyte degeneration diminishes global and individual capillary CBF responses to neuronal stimuli, resulting in neurovascular uncoupling, reduced oxygen supply to the brain and metabolic stress. Neurovascular deficits lead over time to impaired neuronal excitability and neurodegenerative changes. Thus, pericyte degeneration as seen in neurological disorders such as Alzheimer's disease may contribute to neurovascular dysfunction and neurodegeneration associated with human disease."
DAVID BOAS,Two-photon high-resolution measurement of partial pressure of oxygen in cerebral vasculature and tissue,"Measurements of oxygen partial pressure (pO(2)) with high temporal and spatial resolution in three dimensions is crucial for understanding oxygen delivery and consumption in normal and diseased brain. Among existing pO(2) measurement methods, phosphorescence quenching is optimally suited for the task. However, previous attempts to couple phosphorescence with two-photon laser scanning microscopy have faced substantial difficulties because of extremely low two-photon absorption cross-sections of conventional phosphorescent probes. Here we report to our knowledge the first practical in vivo two-photon high-resolution pO(2) measurements in small rodents' cortical microvasculature and tissue, made possible by combining an optimized imaging system with a two-photon-enhanced phosphorescent nanoprobe. The method features a measurement depth of up to 250 microm, sub-second temporal resolution and requires low probe concentration. The properties of the probe allowed for direct high-resolution measurement of cortical extravascular (tissue) pO(2), opening many possibilities for functional metabolic brain studies."
DAVID BOAS,Choosing a laser for laser speckle contrast imaging,"The use of laser speckle contrast imaging (LSCI) has expanded rapidly for characterizing the motion of scattering particles. Speckle contrast is related to the dynamics of the scattering particles via a temporal autocorrelation function, but the quality of various elements of the imaging system can adversely affect the quality of the signal recorded by LSCI. While it is known that the laser coherence affects the speckle contrast, it is generally neglected in in vivo LSCI studies and was not thoroughly addressed in a practical matter. In this work, we address the question of how the spectral width of the light source affects the speckle contrast both experimentally and through numerical simulations. We show that commonly used semiconductor laser diodes have a larger than desired spectral width that results in a significantly reduced speckle contrast compared with ideal narrow band lasers. This results in a reduced signal-to-noise ratio for estimating changes in the motion of scattering particles. We suggest using a volume holographic grating stabilized laser diode or other diodes that have a spectrum of emitted light narrower than ≈1 nm to improve the speckle contrast."
DAVID BOAS,In vivo imaging of cerebral energy metabolism with two-photon fluorescence lifetime microscopy of NADH,"Minimally invasive, specific measurement of cellular energy metabolism is crucial for understanding cerebral pathophysiology. Here, we present high-resolution, in vivo observations of autofluorescence lifetime as a biomarker of cerebral energy metabolism in exposed rat cortices. We describe a customized two-photon imaging system with time correlated single photon counting detection and specialized software for modeling multiple-component fits of fluorescence decay and monitoring their transient behaviors. In vivo cerebral NADH fluorescence suggests the presence of four distinct components, which respond differently to brief periods of anoxia and likely indicate different enzymatic formulations. Individual components show potential as indicators of specific molecular pathways involved in oxidative metabolism."
DAVID BOAS,fNIRS can robustly measure brain activity during memory encoding and retrieval in healthy subjects,"Early intervention in Alzheimer’s Disease (AD) requires novel biomarkers that can capture changes in brain activity at an early stage. Current AD biomarkers are expensive and/or invasive and therefore unsuitable for use as screening tools, but a non-invasive, inexpensive, easily accessible screening method could be useful in both clinical and research settings. Prior studies suggest that especially paired-associate learning tasks may be useful in detecting the earliest memory impairment in AD. Here, we investigated the utility of functional Near Infrared Spectroscopy in measuring brain activity from prefrontal, parietal and temporal cortices of healthy adults (n = 19) during memory encoding and retrieval under a face-name paired-associate learning task. Our findings demonstrate that encoding of novel face-name pairs compared to baseline as well as compared to repeated face-name pairs resulted in significant activation in left dorsolateral prefrontal cortex while recalling resulted in activation in dorsolateral prefrontal cortex bilaterally. Moreover, brain response to recalling was significantly higher than encoding in medial, superior and middle frontal cortices for novel faces. Overall, this study shows that fNIRS can reliably measure cortical brain activation during a face-name paired-associate learning task. Future work will include similar measurements in populations with progressing memory deficits."
DAVID BOAS,Intrinsic optical signal imaging of the blood volume changes is sufficient for mapping the resting state functional connectivity in the rodent cortex,"Objective. Resting state functional connectivity (RSFC) allows the study of functional organization in normal and diseased brain by measuring the spontaneous brain activity generated under resting conditions. Intrinsic optical signal imaging (IOSI) based on multiple illumination wavelengths has been used successfully to compute RSFC maps in animal studies. The IOSI setup complexity would be greatly reduced if only a single wavelength can be used to obtain comparable RSFC maps. Approach. We used anesthetized mice and performed various comparisons between the RSFC maps based on single wavelength as well as oxy-, deoxy- and total hemoglobin concentration changes. Main results. The RSFC maps based on IOSI at a single wavelength selected for sensitivity to the blood volume changes are quantitatively comparable to the RSFC maps based on oxy- and total hemoglobin concentration changes obtained by the more complex IOSI setups. Moreover, RSFC maps do not require CCD cameras with very high frame acquisition rates, since our results demonstrate that they can be computed from the data obtained at frame rates as low as 5 Hz. Significance. Our results will have general utility for guiding future RSFC studies based on IOSI and making decisions about the IOSI system designs."
DAVID BOAS,Prolonged monitoring of cerebral blood flow and autoregulation with diffuse correlation spectroscopy in neurocritical care patients,"Monitoring of cerebral blood flow (CBF) and autoregulation are essential components of neurocritical care, but continuous noninvasive methods for CBF monitoring are lacking. Diffuse correlation spectroscopy (DCS) is a noninvasive diffuse optical modality that measures a CBF index ( CBF i ) in the cortex microvasculature by monitoring the rapid fluctuations of near-infrared light diffusing through moving red blood cells. We tested the feasibility of monitoring CBF i with DCS in at-risk patients in the Neurosciences Intensive Care Unit. DCS data were acquired continuously for up to 20 h in six patients with aneurysmal subarachnoid hemorrhage, as permitted by clinical care. Mean arterial blood pressure was recorded synchronously, allowing us to derive autoregulation curves and to compute an autoregulation index. The autoregulation curves suggest disrupted cerebral autoregulation in most patients, with the severity of disruption and the limits of preserved autoregulation varying between subjects. Our findings suggest the potential of the DCS modality for noninvasive, long-term monitoring of cerebral perfusion, and autoregulation."
DAVID BOAS,Using prerecorded hemodynamic response functions in detecting prefrontal pain response: a functional near-infrared spectroscopy study,"Currently, there is no method for providing a nonverbal objective assessment of pain. Recent work using functional near-infrared spectroscopy (fNIRS) has revealed its potential for objective measures. We conducted two fNIRS scans separated by 30 min and measured the hemodynamic response to the electrical noxious and innocuous stimuli over the anterior prefrontal cortex (aPFC) in 14 subjects. Based on the estimated hemodynamic response functions (HRFs), we first evaluated the test–retest reliability of using fNIRS in measuring the pain response over the aPFC. We then proposed a general linear model (GLM)-based detection model that employs the subject-specific HRFs from the first scan to detect the pain response in the second scan. Our results indicate that fNIRS has a reasonable reliability in detecting the hemodynamic changes associated with noxious events, especially in the medial portion of the aPFC. Compared with a standard HRF with a fixed shape, including the subject-specific HRFs in the GLM allows for a significant improvement in the detection sensitivity of aPFC pain response. This study supports the potential application of individualized analysis in using fNIRS and provides a robust model to perform objective determination of pain perception."
DAVID BOAS,"Modeling of cerebral oxygen transport based on in vivo microscopic imaging of microvascular network structure, blood flow, and oxygenation","Oxygen is delivered to brain tissue by a dense network of microvessels, which actively control cerebral blood flow (CBF) through vasodilation and contraction in response to changing levels of neural activity. Understanding these network-level processes is immediately relevant for (1) interpretation of functional Magnetic Resonance Imaging (fMRI) signals, and (2) investigation of neurological diseases in which a deterioration of neurovascular and neuro-metabolic physiology contributes to motor and cognitive decline. Experimental data on the structure, flow and oxygen levels of microvascular networks are needed, together with theoretical methods to integrate this information and predict physiologically relevant properties that are not directly measurable. Recent progress in optical imaging technologies for high-resolution in vivo measurement of the cerebral microvascular architecture, blood flow, and oxygenation enables construction of detailed computational models of cerebral hemodynamics and oxygen transport based on realistic three-dimensional microvascular networks. In this article, we review state-of-the-art optical microscopy technologies for quantitative in vivo imaging of cerebral microvascular structure, blood flow and oxygenation, and theoretical methods that utilize such data to generate spatially resolved models for blood flow and oxygen transport. These ""bottom-up"" models are essential for the understanding of the processes governing brain oxygenation in normal and disease states and for eventual translation of the lessons learned from animal studies to humans."
DAVID BOAS,Measurement of shear-induced diffusion of red blood cells using dynamic light scattering-optical coherence tomography,"Dynamic Light Scattering-Optical Coherence Tomography (DLS-OCT) takes the advantages of using DLS to measure particle flow and diffusion within an OCT resolution-constrained 3D volume, enabling the simultaneous measurements of absolute RBC velocity and diffusion coefficient with high spatial resolution. In this work, we applied DLS-OCT to measure both RBC velocity and the shear-induced diffusion coefficient within penetrating venules of the somatosensory cortex of anesthetized mice. Blood flow laminar profile measurements indicate a blunted laminar flow profile, and the degree of blunting decreases with increasing vessel diameter. The measured shear-induced diffusion coefficient was proportional to the flow shear rate with a magnitude of ~ 0.1 to 0.5 × 10-6 mm2 . These results provide important experimental support for the recent theoretical explanation for why DCS is dominantly sensitive to RBC diffusive motion."
DAVID BOAS,Cardiac pulsatility mapping and vessel type identification using laser speckle contrast imaging,"Systemic flow variations caused by the cardiac cycle can play a role or be an important marker in both normal and pathological conditions. The shape, magnitude and propagation speed of the flow pulse reflect mechanical properties of the vasculature and are known to vary significantly with vascular diseases. Most conventional techniques are not capable of imaging cardiac activity in the microcirculation due to spatial and/or temporal resolution limitations and instead make inferences about propagation speed by making measurements at two points along an artery. Here, we apply laser speckle contrast imaging to images with high spatial resolution in the high frequency harmonics of cardiac activity in the cerebral cortex of a mouse. We reveal vessel dependent variation in the cardiac pulse activity and use this information to automatically identify arteries and veins."
DAVID BOAS,Morphine attenuates fNIRS signal associated with painful stimuli in the medial frontopolar cortex (medial BA 10),"Functional near infrared spectroscopy (fNIRS) is a non-invasive optical imaging method that provides continuous measure of cortical brain functions. One application has been its use in the evaluation of pain. Previous studies have delineated a deoxygenation process associated with pain in the medial anterior prefrontal region, more specifically, the medial Brodmann Area 10 (BA 10). Such response to painful stimuli has been consistently observed in awake, sedated and anesthetized patients. In this study, we administered oral morphine (15 mg) or placebo to 14 healthy male volunteers with no history of pain or opioid abuse in a crossover double blind design, and performed fNIRS scans prior to and after the administration to assess the effect of morphine on the medial BA 10 pain signal. Morphine is the gold standard for inhibiting nociceptive processing, most well described for brain effects on sensory and emotional regions including the insula, the somatosensory cortex (the primary somatosensory cortex, S1, and the secondary somatosensory cortex, S2), and the anterior cingulate cortex (ACC). Our results showed an attenuation effect of morphine on the fNIRS-measured pain signal in the medial BA 10, as well as in the contralateral S1 (although observed in a smaller number of subjects). Notably, the extent of signal attenuation corresponded with the temporal profile of the reported plasma concentration for the drug. No clear attenuation by morphine on the medial BA 10 response to innocuous stimuli was observed. These results provide further evidence for the role of medial BA 10 in the processing of pain, and also suggest that fNIRS may be used as an objective measure of drug-brain profiles independent of subjective reports."
DAVID BOAS,MATLAB code and data processing guide for Dynamic Light Scattering-Optical Coherence Tomography,"This guide is for post data processing of DLSOCT, which outputs axial velocity (Vz), transverse velocity (Vx), total velocity(V), the ratio of static component (Ms), the ratio of dynamic component (Mf), and fitting accuracy (R). The speed upper limit is determined by OCT system Aline rate and 3Dvoxel size."
DAVID BOAS,MATLAB code and data processing guide for Optical Coherence Tomography Angiography,This guide is for post data processing of OCTA which outputs the vascular structure.
DAVID BOAS,MATLAB code and data processing guide for phase resolved Doppler Optical Coherence Tomography,"This guide and the MATLAB code are for post data processing of prDOCT, which outputs 3D vascular blood flow velocity."
DAVID BOAS,Compromised microvascular oxygen delivery increases brain tissue vulnerability with age,
DAVID BOAS,Development and characterization of a multidistance and multiwavelength diffuse correlation spectroscopy system,"This paper presents a multidistance and multiwavelength diffuse correlation spectroscopy (DCS) approach and its implementation to simultaneously measure the optical proprieties of deep tissue as well as the blood flow. The system consists of three long coherence length lasers at different wavelengths in the near-infrared, eight single-photon detectors, and a correlator board. With this approach, we collect both light intensity and DCS data at multiple distances and multiple wavelengths, which provide unique information to fit for all the parameters of interest: scattering, blood flow, and hemoglobin concentration. We present the characterization of the system and its validation with phantom measurements."
DAVID BOAS,Performance assessment of diffuse optical spectroscopic imaging instruments in a 2-year multicenter breast cancer,"We present a framework for characterizing the performance of an experimental imaging technology, diffuse optical spectroscopic imaging (DOSI), in a 2-year multicenter American College of Radiology Imaging Network (ACRIN) breast cancer study (ACRIN-6691). DOSI instruments combine broadband frequency-domain photon migration with time-independent near-infrared (650 to 1000 nm) spectroscopy to measure tissue absorption and reduced scattering spectra and tissue hemoglobin, water, and lipid composition. The goal of ACRIN-6691 was to test the effectiveness of optically derived imaging endpoints in predicting the final pathologic response of neoadjuvant chemotherapy (NAC). Sixty patients were enrolled over a 2-year period at participating sites and received multiple DOSI scans prior to and during 3- to 6-month NAC. The impact of three sources of error on accuracy and precision, including different operators, instruments, and calibration standards, was evaluated using a broadband reflectance standard and two different solid tissue-simulating optical phantoms. Instruments showed <0.0010  mm−1 (10.3%) and 0.06  mm−1 (4.7%) deviation in broadband absorption and reduced scattering, respectively, over the 2-year duration of ACRIN-6691. These variations establish a useful performance criterion for assessing instrument stability. The proposed procedures and tests are not limited to DOSI; rather, they are intended to provide methods to characterize performance of any instrument used in translational optical imaging."
DAVID BOAS,Capillary red blood cell velocimetry by phase-resolved optical coherence tomography,"We present a phase-resolved optical coherence tomography (OCT) method to extend Doppler OCT for the accurate measurement of the red blood cell (RBC) velocity in cerebral capillaries. OCT data were acquired with an M-mode scanning strategy (repeated A-scans) to account for the single-file passage of RBCs in a capillary, which were then high-pass filtered to remove the stationary component of the signal to ensure an accurate measurement of phase shift of flowing RBCs. The angular frequency of the signal from flowing RBCs was then quantified from the dynamic component of the signal and used to calculate the axial speed of flowing RBCs in capillaries. We validated our measurement by RBC passage velocimetry using the signal magnitude of the same OCT time series data."
DAVID BOAS,Improving the characterization of ex vivo human brain optical properties using high numerical aperture optical coherence tomography by spatially constraining the confocal parameters,"SIGNIFICANCE: The optical properties of biological samples provide information about the structural characteristics of the tissue and any changes arising from pathological conditions. Optical coherence tomography (OCT) has proven to be capable of extracting tissue's optical properties using a model that combines the exponential decay due to tissue scattering and the axial point spread function that arises from the confocal nature of the detection system, particularly for higher numerical aperture (NA) measurements. A weakness in estimating the optical properties is the inter-parameter cross-talk between tissue scattering and the confocal parameters defined by the Rayleigh range and the focus depth. AIM: In this study, we develop a systematic method to improve the characterization of optical properties with high-NA OCT. APPROACH: We developed a method that spatially parameterizes the confocal parameters in a previously established model for estimating the optical properties from the depth profiles of high-NA OCT. RESULTS: The proposed parametrization model was first evaluated on a set of intralipid phantoms and then validated using a low-NA objective in which cross-talk from the confocal parameters is negligible. We then utilize our spatially parameterized model to characterize optical property changes introduced by a tissue index matching process using a simple immersion agent, 2,2'-thiodiethonal. CONCLUSIONS: Our approach improves the confidence of parameter estimation by reducing the degrees of freedom in the non-linear fitting model."
DAVID BOAS,Motion artifact detection and correction in functional near-infrared spectroscopy: a new hybrid method based on spline interpolation method and Savitzky–Golay filtering,"Motion artifact contamination in near-infrared spectroscopy (NIRS) data has become an important challenge in realizing the full potential of NIRS for real-life applications. Various motion correction algorithms have been used to alleviate the effect of motion artifacts on the estimation of the hemodynamic response function. While smoothing methods, such as wavelet filtering, are excellent in removing motion-induced sharp spikes, the baseline shifts in the signal remain after this type of filtering. Methods, such as spline interpolation, on the other hand, can properly correct baseline shifts; however, they leave residual high-frequency spikes. We propose a hybrid method that takes advantage of different correction algorithms. This method first identifies the baseline shifts and corrects them using a spline interpolation method or targeted principal component analysis. The remaining spikes, on the other hand, are corrected by smoothing methods: Savitzky–Golay (SG) filtering or robust locally weighted regression and smoothing. We have compared our new approach with the existing correction algorithms in terms of hemodynamic response function estimation using the following metrics: mean-squared error, peak-to-peak error ( Ep), Pearson’s correlation ( R2), and the area under the receiver operator characteristic curve. We found that spline-SG hybrid method provides reasonable improvements in all these metrics with a relatively short computational time. The dataset and the code used in this study are made available online for the use of all interested researchers."
DAVID BOAS,Differential effects of anesthetics on resting state functional connectivity in the mouse,"Blood oxygen level-dependent (BOLD) functional MRI (fMRI) is a standard approach to examine resting state functional connectivity (RSFC), but fMRI in animal models is challenging. Recently, functional optical intrinsic signal imaging-which relies on the same hemodynamic signal underlying BOLD fMRI-has been developed as a complementary approach to assess RSFC in mice. Since it is difficult to ensure that an animal is in a truly resting state while awake, RSFC measurements under anesthesia remain an important approach. Therefore, we systematically examined measures of RSFC using non-invasive, widefield optical intrinsic signal imaging under five different anesthetics in male C57BL/6J mice. We find excellent seed-based, global, and interhemispheric connectivity using tribromoethanol (Avertin) and ketamine-xylazine, comparable to results in the literature including awake animals. Urethane anesthesia yielded intermediate results, while chloral hydrate and isoflurane were both associated with poor RSFC. Furthermore, we found a correspondence between the strength of RSFC and the power of low-frequency hemodynamic fluctuations. In conclusion, Avertin and ketamine-xylazine provide robust and reproducible measures of RSFC in mice, whereas chloral hydrate and isoflurane do not."
DAVID BOAS,Altered low frequency oscillations of cortical vessels in patients with cerebrovascular occlusive disease – a NIRS study,"Analysis of cerebral autoregulation by measuring spontaneous oscillations in the low frequency spectrum of cerebral cortical vessels might be a useful tool for assessing risk and investigating different treatment strategies in carotid artery disease and stroke. Near infrared spectroscopy (NIRS) is a non-invasive optical method to investigate regional changes in oxygenated (oxyHb) and deoxygenated hemoglobin (deoxyHb) in the outermost layers of the cerebral cortex. In the present study we examined oxyHb low frequency oscillations, believed to reflect cortical cerebral autoregulation, in 16 patients with both symptomatic carotid occlusive disease and cerebral hypoperfusion in comparison to healthy controls. Each hemisphere was examined with two NIRS channels using a 3 cm source detector distance. Arterial blood pressure (ABP) was measured via a finger plethysmograph. Using transfer function analysis ABP-oxyHb phase shift and gain as well as inter-hemispheric phase shift and amplitude ratio were assessed. We found that inter-hemispheric amplitude ratio was significantly altered in hypoperfusion patients compared to healthy controls (P = 0.010), because of relatively lower amplitude on the hypoperfusion side. The inter-hemispheric phase shift showed a trend (P = 0.061) toward increased phase shift in hypoperfusion patients compared to controls. We found no statistical difference between hemispheres in hypoperfusion patients for phase shift or gain values. There were no differences between the hypoperfusion side and controls for phase shift or gain values. These preliminary results suggest an impairment of autoregulation in hypoperfusion patients at the cortical level detected by NIRS."
DAVID BOAS,"Recruiting large online samples in the United States and India: Facebook, Mechanical Turk and Qualtrics","This article examines online recruitment via Facebook, Mechanical Turk (MTurk), and Qualtrics panels in India and the United States. It compares over 7300 respondents—1000 or more from each source and country—to nationally representative benchmarks in terms of demographics, political attitudes and knowledge, cooperation, and experimental replication. In the United States, MTurk offers the cheapest and fastest recruitment, Qualtrics is most demographically and politically representative, and Facebook facilitates targeted sampling. The India samples look much less like the population, though Facebook offers broad geographical coverage. We find online convenience samples often provide valid inferences into how partisanship moderates treatment effects. Yet they are typically unrepresentative on such political variables, which has implications for the external validity of sample average treatment effects."
DAVID BOAS,Awake mouse imaging: from two-photon microscopy to blood oxygen level-dependent functional magnetic resonance imaging,"BACKGROUND: Functional magnetic resonance imaging (fMRI) in awake behaving mice is well positioned to bridge the detailed cellular-level view of brain activity, which has become available owing to recent advances in microscopic optical imaging and genetics, to the macroscopic scale of human noninvasive observables. However, though microscopic (e.g., two-photon imaging) studies in behaving mice have become a reality in many laboratories, awake mouse fMRI remains a challenge. Owing to variability in behavior among animals, performing all types of measurements within the same subject is highly desirable and can lead to higher scientific rigor. METHODS: We demonstrated blood oxygenation level-dependent fMRI in awake mice implanted with long-term cranial windows that allowed optical access for microscopic imaging modalities and optogenetic stimulation. We started with two-photon imaging of single-vessel diameter changes (n = 1). Next, we implemented intrinsic optical imaging of blood oxygenation and flow combined with laser speckle imaging of blood flow obtaining a mesoscopic picture of the hemodynamic response (n = 16). Then we obtained corresponding blood oxygenation level-dependent fMRI data (n = 5). All measurements could be performed in the same mice in response to identical sensory and optogenetic stimuli. RESULTS: The cranial window did not deteriorate the quality of fMRI and allowed alternation between imaging modalities in each subject. CONCLUSIONS: This report provides a proof of feasibility for multiscale imaging approaches in awake mice. In the future, this protocol could be extended to include complex cognitive behaviors translatable to humans, such as sensory discrimination or attention."
DAVID BOAS,Dependence of the MR signal on the magnetic susceptibility of blood studied with models based on real microvascular networks,"PURPOSE: The primary goal of this study was to estimate the value of beta , the exponent in the power law relating changes of the transverse relaxation rate and intra-extravascular local magnetic susceptibility differences as Delta R 2 * proportional, variant ( Delta chi ) beta. The secondary objective was to evaluate any differences that might exist in the value of beta obtained using a deoxyhemoglobin-weighted Delta chi distribution versus a constant Delta chi distribution assumed in earlier computations. The third objective was to estimate the value of beta that is relevant for methods based on susceptibility contrast agents with a concentration of Delta chi higher than that used for BOLD fMRI calculations. METHODS: Our recently developed model of real microvascular anatomical networks is used to extend the original simplified Monte-Carlo simulations to compute beta from the first principles. RESULTS: Our results show that beta = 1 for most BOLD fMRI measurements of real vascular networks, as opposed to earlier predictions of beta = 1 .5 using uniform Delta chi distributions. For perfusion or fMRI methods based on contrast agents, which generate larger values for Delta chi , beta = 1 for B 0 </= 9.4 T, whereas at 14 T beta can drop below 1 and the variation across subjects is large, indicating that a lower concentration of contrast agent with a lower value of Delta chi is desired for experiments at high B0. CONCLUSION: These results improve our understanding of the relationship between R2 (*) and the underlying microvascular properties. The findings will help to infer the cerebral metabolic rate of oxygen and cerebral blood volume from BOLD and perfusion MRI, respectively."
DAVID BOAS,Development of a beam propagation method to simulate the point spread function degradation in scattering media,"Scattering is one of the main issues that limit the imaging depth in deep tissue optical imaging. To characterize the role of scattering, we have developed a forward model based on the beam propagation method and established the link between the macroscopic optical properties of the media and the statistical parameters of the phase masks applied to the wavefront. Using this model, we have analyzed the degradation of the point-spread function of the illumination beam in the transition regime from ballistic to diffusive light transport. Our method provides a wave-optic simulation toolkit to analyze the effects of scattering on image quality degradation in scanning microscopy. Our open-source implementation is available at https://github.com/BUNPC/Beam-Propagation-Method."
DAVID BOAS,Towards neuroscience of the everyday world (NEW) using functional near infrared spectroscopy,"Functional near-infrared spectroscopy (fNIRS) assesses human brain activity by noninvasively measuring changes of cerebral hemoglobin concentrations caused by modulation of neuronal activity. Recent progress in signal processing and advances in system design, such as miniaturization, wearability, and system sensitivity, have strengthened fNIRS as a viable and cost-effective complement to functional magnetic resonance imaging, expanding the repertoire of experimental studies that can be performed by the neuroscience community. The availability of fNIRS and electroencephalography for routine, increasingly unconstrained, and mobile brain imaging is leading toward a new domain that we term “Neuroscience of the Everyday World” (NEW). In this light, we review recent advances in hardware, study design, and signal processing, and discuss challenges and future directions."
DAVID BOAS,Brain correlates of motor complexity during observed and executed actions,"Recently, cortical areas with motor properties have attracted attention widely to their involvement in both action generation and perception. Inferior frontal gyrus (IFG), ventral premotor cortex (PMv) and inferior parietal lobule (IPL), presumably consisting of motor-related areas, are of particular interest, given that they respond to motor behaviors both when they are performed and observed. Converging neuroimaging evidence has shown the functional roles of IFG, PMv and IPL in action understanding. Most studies have focused on the effects of modulations in goals and kinematics of observed actions on the brain response, but little research has explored the effects of manipulations in motor complexity. To address this, we used fNIRS to examine the brain activity in the frontal, motor, parietal and occipital regions, aiming to better understand the brain correlates involved in encoding motor complexity. Twenty-one healthy adults executed and observed two hand actions that differed in motor complexity. We found that motor complexity sensitive brain regions were present in the pars opercularis IFG/PMv, primary motor cortex (M1), IPL/supramarginal gyrus and middle occipital gyrus (MOG) during action execution, and in pars opercularis IFG/PMv and M1 during action observation. Our findings suggest that the processing of motor complexity involves not only M1 but also pars opercularis IFG, PMv and IPL, each of which plays a critical role in action perception and execution."
DAVID BOAS,Cell type specificity of neurovascular coupling in cerebral cortex,"Identification of the cellular players and molecular messengers that communicate neuronal activity to the vasculature driving cerebral hemodynamics is important for (1) the basic understanding of cerebrovascular regulation and (2) interpretation of functional Magnetic Resonance Imaging (fMRI) signals. Using a combination of optogenetic stimulation and 2-photon imaging in mice, we demonstrate that selective activation of cortical excitation and inhibition elicits distinct vascular responses and identify the vasoconstrictive mechanism as Neuropeptide Y (NPY) acting on Y1 receptors. The latter implies that task-related negative Blood Oxygenation Level Dependent (BOLD) fMRI signals in the cerebral cortex under normal physiological conditions may be mainly driven by the NPY-positive inhibitory neurons. Further, the NPY-Y1 pathway may offer a potential therapeutic target in cerebrovascular disease."
DAVID BOAS,More homogeneous capillary flow and oxygenation in deeper cortical layers correlate with increased oxygen extraction,"Our understanding of how capillary blood flow and oxygen distribute across cortical layers to meet the local metabolic demand is incomplete. We addressed this question by using two-photon imaging of resting-state microvascular oxygen partial pressure (PO2) and flow in the whisker barrel cortex in awake mice. Our measurements in layers I-V show that the capillary red-blood-cell flux and oxygenation heterogeneity, and the intracapillary resistance to oxygen delivery, all decrease with depth, reaching a minimum around layer IV, while the depth-dependent oxygen extraction fraction is increased in layer IV, where oxygen demand is presumably the highest. Our findings suggest that more homogeneous distribution of the physiological observables relevant to oxygen transport to tissue is an important part of the microvascular network adaptation to local brain metabolism. These results will inform the biophysical models of layer-specific cerebral oxygen delivery and consumption and improve our understanding of the diseases that affect cerebral microcirculation."
DAVID BOAS,"Due to intravascular multiple sequential scattering, diffuse correlation spectroscopy of tissue primarily measures relative red blood cell motion within vessels","We suggest that Diffuse Correlation Spectroscopy (DCS) measurements of tissue blood flow primarily probe relative red blood cell (RBC) motion, due to the occurrence of multiple sequential scattering events within blood vessels. The magnitude of RBC shear-induced diffusion is known to correlate with flow velocity, explaining previous reports of linear scaling of the DCS ""blood flow index"" with tissue perfusion despite the observed diffusion-like auto-correlation decay. Further, by modeling RBC mean square displacement using a formulation that captures the transition from ballistic to diffusive motion, we improve the fit to experimental data and recover effective diffusion coefficients and velocity de-correlation time scales in the range expected from previous blood rheology studies."
DAVID BOAS,Shear-induced diffusion of red blood cells measured with dynamic light scattering-optical coherence tomography,"Quantitative measurements of intravascular microscopic dynamics, such as absolute blood flow velocity, shear stress and the diffusion coefficient of red blood cells (RBCs), are fundamental in understanding the blood flow behavior within the microcirculation, and for understanding why diffuse correlation spectroscopy (DCS) measurements of blood flow are dominantly sensitive to the diffusive motion of RBCs. Dynamic light scattering-optical coherence tomography (DLS-OCT) takes the advantages of using DLS to measure particle flow and diffusion within an OCT resolution-constrained three-dimensional volume, enabling the simultaneous measurements of absolute RBC velocity and diffusion coefficient with high spatial resolution. In this work, we applied DLS-OCT to measure both RBC velocity and the shear-induced diffusion coefficient within penetrating venules of the somatosensory cortex of anesthetized mice. Blood flow laminar profile measurements indicate a blunted laminar flow profile and the degree of blunting decreases with increasing vessel diameter. The measured shear-induced diffusion coefficient was proportional to the flow shear rate with a magnitude of ~0.1 to 0.5 × 10-6  mm2 . These results provide important experimental support for the recent theoretical explanation for why DCS is dominantly sensitive to RBC diffusive motion."
DAVID BOAS,Code distribution and example data for submission of Functional ultrasound speckle decorrelation-based velocimetry of the brain,
DAVID BOAS,Model-based inference from microvascular measurements: Combining experimental measurements and model predictions using a Bayesian probabilistic approach,"OBJECTIVE: In vivo imaging of the microcirculation and network-oriented modeling have emerged as powerful means of studying microvascular function and understanding its physiological significance. Network-oriented modeling may provide the means of summarizing vast amounts of data produced by high-throughput imaging techniques in terms of key, physiological indices. To estimate such indices with sufficient certainty, however, network-oriented analysis must be robust to the inevitable presence of uncertainty due to measurement errors as well as model errors. METHODS: We propose the Bayesian probabilistic data analysis framework as a means of integrating experimental measurements and network model simulations into a combined and statistically coherent analysis. The framework naturally handles noisy measurements and provides posterior distributions of model parameters as well as physiological indices associated with uncertainty. RESULTS: We applied the analysis framework to experimental data from three rat mesentery networks and one mouse brain cortex network. We inferred distributions for more than 500 unknown pressure and hematocrit boundary conditions. Model predictions were consistent with previous analyses, and remained robust when measurements were omitted from model calibration. CONCLUSION: Our Bayesian probabilistic approach may be suitable for optimizing data acquisition and for analyzing and reporting large data sets acquired as part of microvascular imaging studies."
DAVID BOAS,Characterizing the optical properties of human brain tissue with high numerical aperture optical coherence tomography,"Quantification of tissue optical properties with optical coherence tomography (OCT) has proven to be useful in evaluating structural characteristics and pathological changes. Previous studies primarily used an exponential model to analyze low numerical aperture (NA) OCT measurements and obtain the total attenuation coefficient for biological tissue. In this study, we develop a systematic method that includes the confocal parameter for modeling the depth profiles of high NA OCT, when the confocal parameter cannot be ignored. This approach enables us to quantify tissue optical properties with higher lateral resolution. The model parameter predictions for the scattering coefficients were tested with calibrated microsphere phantoms. The application of the model to human brain tissue demonstrates that the scattering and back-scattering coefficients each provide unique information, allowing us to differentially identify laminar structures in primary visual cortex and distinguish various nuclei in the midbrain. The combination of the two optical properties greatly enhances the power of OCT to distinguish intricate structures in the human brain beyond what is achievable with measured OCT intensity information alone, and therefore has the potential to enable objective evaluation of normal brain structure as well as pathological conditions in brain diseases. These results represent a promising step for enabling the quantification of tissue optical properties from high NA OCT."
DAVID BOAS,Extracting individual neural activity recorded through splayed optical microfibers,"Previously introduced bundles of hundreds or thousands of microfibers have the potential to extend optical access to deep brain regions, sampling fluorescence activity throughout a three-dimensional volume. Each fiber has a small diameter (8  μm) and follows a path of least resistance, splaying during insertion. By superimposing the fiber sensitivity profile for each fiber, we model the interface properties for a simulated neural population. Our modeling results suggest that for small (&lt;200) bundles of fibers, each fiber will collect fluorescence from a small number of nonoverlapping neurons near the fiber apertures. As the number of fibers increases, the bundle delivers more uniform excitation power to the region, moving to a regime where fibers collect fluorescence from more neurons and there is greater overlap between neighboring fibers. Under these conditions, it becomes feasible to apply source separation to extract individual neural contributions. In addition, we demonstrate a source separation technique particularly suited to the interface. Our modeling helps establish performance expectations for this interface and provides a framework for estimating neural contributions under a range of conditions."
DAVID BOAS,Colocalization of neurons in optical coherence microscopy and Nissl-stained histology in Brodmann’s area 32 and area 21,"Optical coherence tomography is an optical technique that uses backscattered light to highlight intrinsic structure, and when applied to brain tissue, it can resolve cortical layers and fiber bundles. Optical coherence microscopy (OCM) is higher resolution (i.e., 1.25 µm) and is capable of detecting neurons. In a previous report, we compared the correspondence of OCM acquired imaging of neurons with traditional Nissl stained histology in entorhinal cortex layer II. In the current method-oriented study, we aimed to determine the colocalization success rate between OCM and Nissl in other brain cortical areas with different laminar arrangements and cell packing density. We focused on two additional cortical areas: medial prefrontal, pre-genual Brodmann area (BA) 32 and lateral temporal BA 21. We present the data as colocalization matrices and as quantitative percentages. The overall average colocalization in OCM compared to Nissl was 67% for BA 32 (47% for Nissl colocalization) and 60% for BA 21 (52% for Nissl colocalization), but with a large variability across cases and layers. One source of variability and confounds could be ascribed to an obscuring effect from large and dense intracortical fiber bundles. Other technical challenges, including obstacles inherent to human brain tissue, are discussed. Despite limitations, OCM is a promising semi-high throughput tool for demonstrating detail at the neuronal level, and, with further development, has distinct potential for the automatic acquisition of large databases as are required for the human brain."
DAVID BOAS,A systematic comparison of motion artifact correction techniques for functional near-infrared spectroscopy,"Near-infrared spectroscopy (NIRS) is susceptible to signal artifacts caused by relative motion between NIRS optical fibers and the scalp. These artifacts can be very damaging to the utility of functional NIRS, particularly in challenging subject groups where motion can be unavoidable. A number of approaches to the removal of motion artifacts from NIRS data have been suggested. In this paper we systematically compare the utility of a variety of published NIRS motion correction techniques using a simulated functional activation signal added to 20 real NIRS datasets which contain motion artifacts. Principle component analysis, spline interpolation, wavelet analysis, and Kalman filtering approaches are compared to one another and to standard approaches using the accuracy of the recovered, simulated hemodynamic response function (HRF). Each of the four motion correction techniques we tested yields a significant reduction in the mean-squared error (MSE) and significant increase in the contrast-to-noise ratio (CNR) of the recovered HRF when compared to no correction and compared to a process of rejecting motion-contaminated trials. Spline interpolation produces the largest average reduction in MSE (55%) while wavelet analysis produces the highest average increase in CNR (39%). On the basis of this analysis, we recommend the routine application of motion correction techniques (particularly spline interpolation or wavelet analysis) to minimize the impact of motion artifacts on functional NIRS data."
DAVID BOAS,Motion correction for phase-resolved dynamic optical coherence tomography imaging of rodent cerebral cortex,"Cardiac and respiratory motions in animals are the primary source of image quality degradation in dynamic imaging studies, especially when using phase-resolved imaging modalities such as spectral-domain optical coherence tomography (SD-OCT), whose phase signal is very sensitive to movements of the sample. This study demonstrates a method with which to compensate for motion artifacts in dynamic SD-OCT imaging of the rodent cerebral cortex. We observed that respiratory and cardiac motions mainly caused, respectively, bulk image shifts (BISs) and global phase fluctuations (GPFs). A cross-correlation maximization-based shift correction algorithm was effective in suppressing BISs, while GPFs were significantly reduced by removing axial and lateral global phase variations. In addition, a non-origin-centered GPF correction algorithm was examined. Several combinations of these algorithms were tested to find an optimized approach that improved image stability from 0.5 to 0.8 in terms of the cross-correlation over 4 s of dynamic imaging, and reduced phase noise by two orders of magnitude in ~8% voxels."
DAVID BOAS,Time domain diffuse correlation spectroscopy: modeling the effects of laser coherence length and instrument response function,"Diffuse correlation spectroscopy (DCS) is an optical technique that non-invasively quantifies an index of blood flow (BFi) by measuring the temporal autocorrelation function of the intensity fluctuations of light diffusely remitted from the tissue. Traditional DCS measurements use continuous wave (CW) lasers with coherence lengths longer than the photon path lengths in the sample to ensure that the diffusely remitted light is coherent and generates a speckle pattern. Recently, we proposed time domain DCS (TD-DCS) to allow measurements of the speckle fluctuations for specific path lengths of light through the tissue, which has the distinct advantage of permitting an analysis of selected long path lengths of light to improve the depth sensitivity of the measurement. However, compared to CW-DCS, factors including the instrument response function (IRF), the detection gate width, and the finite coherence length need to be considered in the model analysis of the experimental data. Here we present a TD-DCS model describing how the intensity autocorrelation functions measured for different path lengths of light depend on the coherence length, pulse width of the laser, detection gate width, IRF, BFi, and optical properties of the scattering sample. Predictions of the model are compared with experimental results using a homogeneous liquid phantom sample that mimics human tissue optical properties. The BFis obtained from the TD-DCS model for different path lengths of light agree with the BFi obtained from CW-DCS measurements, while the standard simplified model underestimates the BFi by a factor of ∼2. This Letter establishes the theoretical foundation of the TD-DCS technique and provides guidance for future BFi measurements in tissue."
DAVID BOAS,In vivo imaging and analysis of cerebrovascular hemodynamic responses and tissue oxygenation in the mouse brain,"Cerebrovascular dysfunction has an important role in the pathogenesis of multiple brain disorders. Measurement of hemodynamic responses in vivo can be challenging, particularly as techniques are often not described in sufficient detail and vary between laboratories. We present a set of standardized in vivo protocols that describe high-resolution two-photon microscopy and intrinsic optical signal (IOS) imaging to evaluate capillary and arteriolar responses to a stimulus, regional hemodynamic responses, and oxygen delivery to the brain. The protocol also describes how to measure intrinsic NADH fluorescence to understand how blood O2 supply meets the metabolic demands of activated brain tissue, and to perform resting-state absolute oxygen partial pressure (pO2) measurements of brain tissue. These methods can detect cerebrovascular changes at far higher resolution than MRI techniques, although the optical nature of these techniques limits their achievable imaging depths. Each individual procedure requires 1–2 h to complete, with two to three procedures typically performed per animal at a time. These protocols are broadly applicable in studies of cerebrovascular function in healthy and diseased brain in any of the existing mouse models of neurological and vascular disorders. All these procedures can be accomplished by a competent graduate student or experienced technician, except the two-photon measurement of absolute pO2 level, which is better suited to a more experienced, postdoctoral-level researcher."
DAVID BOAS,Normalized field autocorrelation function-based optical coherence tomography three-dimensional angiography,"Optical coherence tomography angiography (OCTA) has been widely used for en face visualization of the microvasculature, but is challenged for real three-dimensional (3-D) topologic imaging due to the ""tail"" artifacts that appear below large vessels. Further, OCTA is generally incapable of differentiating descending arterioles from ascending venules. We introduce a normalized field autocorrelation function-based OCTA (g1-OCTA), which minimizes the tail artifacts and is capable of distinguishing penetrating arterioles from venules in the 3-D image. g1   (  τ  )   is calculated from repeated optical coherence tomography (OCT) acquisitions for each spatial location. The decay amplitude of g1   (  τ  )   is retrieved to represent the dynamics for each voxel. To account for the small g1   (  τ  )   decay in capillaries where red blood cells are flowing slowly and discontinuously, Intralipid is injected to enhance the OCT signal. We demonstrate that the proposed technique realizes 3-D OCTA with negligible tail projections and the penetrating arteries are readily identified. In addition, compared to regular OCTA, the proposed g1-OCTA largely increased the depth-of-field. This technique provides a more accurate rendering of the vascular 3-D anatomy and has the potential for more quantitative characterization of vascular networks."
DAVID BOAS,Chronic cranial windows for long term multimodal neurovascular imaging in mice,"Chronic cranial windows allow for longitudinal brain imaging experiments in awake, behaving mice. Different imaging technologies have their unique advantages and combining multiple imaging modalities offers measurements of a wide spectrum of neuronal, glial, vascular, and metabolic parameters needed for comprehensive investigation of physiological and pathophysiological mechanisms. Here, we detail a suite of surgical techniques for installation of different cranial windows targeted for specific imaging technologies and their combination. Following these techniques and practices will yield higher experimental success and reproducibility of results."
DAVID BOAS,Large arteriolar component of oxygen delivery implies a safe margin of oxygen supply to cerebral tissue,"What is the organization of cerebral microvascular oxygenation and morphology that allows adequate tissue oxygenation at different activity levels? We address this question in the mouse cerebral cortex using microscopic imaging of intravascular O2 partial pressure and blood flow combined with numerical modelling. Here we show that parenchymal arterioles are responsible for 50% of the extracted O2 at baseline activity, and the majority of the remaining O2 exchange takes place within the first few capillary branches. Most capillaries release little O2 at baseline acting as an O2 reserve that is recruited during increased neuronal activity or decreased blood flow. Our results challenge the common perception that capillaries are the major site of O2 delivery to cerebral tissue. The understanding of oxygenation distribution along arterio-capillary paths may have profound implications for the interpretation of blood-oxygen-level dependent (BOLD) contrast in functional magnetic resonance imaging and for evaluating microvascular O2 delivery capacity to support cerebral tissue in disease."
DAVID BOAS,Targeted principle component analysis: a new motion artifact correction approach for near-infrared spectroscopy,"As near-infrared spectroscopy (NIRS) broadens its application area to different age and disease groups, motion artifacts in the NIRS signal due to subject movement is becoming an important challenge. Motion artifacts generally produce signal fluctuations that are larger than physiological NIRS signals, thus it is crucial to correct for them before obtaining an estimate of stimulus evoked hemodynamic responses. There are various methods for correction such as principle component analysis (PCA), wavelet-based filtering and spline interpolation. Here, we introduce a new approach to motion artifact correction, targeted principle component analysis (tPCA), which incorporates a PCA filter only on the segments of data identified as motion artifacts. It is expected that this will overcome the issues of filtering desired signals that plagues standard PCA filtering of entire data sets. We compared the new approach with the most effective motion artifact correction algorithms on a set of data acquired simultaneously with a collodion-fixed probe (low motion artifact content) and a standard Velcro probe (high motion artifact content). Our results show that tPCA gives statistically better results in recovering hemodynamic response function (HRF) as compared to wavelet-based filtering and spline interpolation for the Velcro probe. It results in a significant reduction in mean-squared error (MSE) and significant enhancement in Pearson's correlation coefficient to the true HRF. The collodion-fixed fiber probe with no motion correction performed better than the Velcro probe corrected for motion artifacts in terms of MSE and Pearson's correlation coefficient. Thus, if the experimental study permits, the use of a collodion-fixed fiber probe may be desirable. If the use of a collodion-fixed probe is not feasible, then we suggest the use of tPCA in the processing of motion artifact contaminated data."
DAVID BOAS,Efficient non-degenerate two-photon excitation for fluorescence microscopy,"Non-degenerate two-photon excitation (ND-TPE) has been explored in two-photon excitation microscopy. However, a systematic study of the efficiency of ND-TPE to guide the selection of fluorophore excitation wavelengths is missing. We measured the relative non-degenerate two-photon absorption cross-section (ND-TPACS) of several commonly used fluorophores (two fluorescent proteins and three small-molecule dyes) and generated 2-dimensional ND-TPACS spectra. We observed that the shape of a ND-TPACS spectrum follows that of the corresponding degenerate two-photon absorption cross-section (D-TPACS) spectrum, but is higher in magnitude. We found that the observed enhancements are higher than theoretical predictions."
DAVID BOAS,Cerebral tissue pO2 response to stimulation is preserved with age in awake mice,"Compromised oxygen supply to cerebral tissue could be an important mechanism contributing to age-related cognition decline. We recently showed in awake mice that resting cerebral tissue pO2 decreases with age, a phenomenon that manifests mainly after middle-age. To extend these findings, here we aimed to study how tissue pO2 response to neuronal stimulation is affected by aging. We used two-photon phosphorescence lifetime microscopy to directly measure the brain tissue pO2 response to whisker stimulation in healthy awake young, middle-aged and old mice. We show that despite a decrease in baseline tissue pO2, the amplitude of the tissue pO2 response to stimulation is well preserved with age. However, the response dynamics are altered towards a slower response with reduced post-stimulus undershoot in older ages, possibly due to stiffer vessel wall among other factors. An estimation of the net oxygen consumption rate using a modified Krogh model suggests that the O2 overshoot during stimulation may be necessary to secure a higher capillary O2 delivery to the tissue proportional to increased CMRO2 to maintain the capillary tissue pO2. It was observed that the coupling between the CMRO2 and capillary O2 delivery is preserved with age."
DAVID BOAS,Two-photon phosphorescence lifetime microscopy of retinal capillary plexus oxygenation in mice,"Impaired oxygen delivery and/or consumption in the retinal tissue underlies the pathophysiology of many retinal diseases. However, the essential tools for measuring oxygen concentration in retinal capillaries and studying oxygen transport to retinal tissue are still lacking. We show that two-photon phosphorescence lifetime microscopy can be used to map absolute partial pressures of oxygen (pO2) in the retinal capillary plexus. Measurements were performed at various retinal depths in anesthetized mice under systemic normoxic and hyperoxic conditions. We used a newly developed two-photon phosphorescent oxygen probe, based on a two-photon absorbing platinum tetraphthalimidoporphyrin, and commercially available optics without correction for optical aberrations of the eye. The transverse and axial distances within the tissue volume were calibrated using a model of the eye's optical system. We believe this is the first demonstration of in vivo depth-resolved imaging of pO2 in retinal capillaries. Application of this method has the potential to advance our understanding of oxygen delivery on the microvascular scale and help elucidate mechanisms underlying various retinal diseases."
DAVID BOAS,Quantitative cerebral blood flow with Optical Coherence Tomography,"Absolute measurements of cerebral blood flow (CBF) are an important endpoint in studies of cerebral pathophysiology. Currently no accepted method exists for in vivo longitudinal monitoring of CBF with high resolution in rats and mice. Using three-dimensional Doppler Optical Coherence Tomography and cranial window preparations, we present methods and algorithms for regional CBF measurements in the rat cortex. Towards this end, we develop and validate a quantitative statistical model to describe the effect of static tissue on velocity sensitivity. This model is used to design scanning protocols and algorithms for sensitive 3D flow measurements and angiography of the cortex. We also introduce a method of absolute flow calculation that does not require explicit knowledge of vessel angles. We show that OCT estimates of absolute CBF values in rats agree with prior measures by autoradiography, suggesting that Doppler OCT can perform absolute flow measurements in animal models."
DAVID BOAS,Single-shot 3D widefield fluorescence imaging with a computational miniature mesoscope,"Fluorescence imaging is indispensable to biology and neuroscience. The need for largescale imaging in freely behaving animals has further driven the development in miniaturized microscopes (miniscopes). However, conventional microscopes / miniscopes are inherently constrained by their limited space-bandwidth-product, shallow depth-of-field, and inability to resolve 3D distributed emitters. Here, we present a Computational Miniature Mesoscope (CM2) that overcomes these bottlenecks and enables single-shot 3D imaging across an 8 × 7-mm2 field-of-view and 2.5-mm depth-of-field, achieving 7-μm lateral resolution and better than 200-μm axial resolution. Notably, the CM2 has a compact lightweight design that integrates a microlens array for imaging and an LED array for excitation in a single platform. Its expanded imaging capability is enabled by computational imaging that augments the optics by algorithms. We experimentally validate the mesoscopic 3D imaging capability on volumetrically distributed fluorescent beads and fibers. We further quantify the effects of bulk scattering and background fluorescence on phantom experiments."
DAVID BOAS,Improved physiological noise regression in fNIRS: a multimodal extension of the General Linear Model using temporally embedded Canonical Correlation Analysis,"For the robust estimation of evoked brain activity from functional Near-Infrared Spectroscopy (fNIRS) signals, it is crucial to reduce nuisance signals from systemic physiology and motion. The current best practice incorporates short-separation (SS) fNIRS measurements as regressors in a General Linear Model (GLM). However, several challenging signal characteristics such as non-instantaneous and non-constant coupling are not yet addressed by this approach and additional auxiliary signals are not optimally exploited. We have recently introduced a new methodological framework for the unsupervised multivariate analysis of fNIRS signals using Blind Source Separation (BSS) methods. Building onto the framework, in this manuscript we show how to incorporate the advantages of regularized temporally embedded Canonical Correlation Analysis (tCCA) into the supervised GLM. This approach allows flexible integration of any number of auxiliary modalities and signals. We provide guidance for the selection of optimal parameters and auxiliary signals for the proposed GLM extension. Its performance in the recovery of evoked HRFs is then evaluated using both simulated ground truth data and real experimental data and compared with the GLM with short-separation regression. Our results show that the GLM with tCCA significantly improves upon the current best practice, yielding significantly better results across all applied metrics: Correlation (HbO max. +45%), Root Mean Squared Error (HbO max. -55%), F-Score (HbO up to 3.25-fold) and p-value as well as power spectral density of the noise floor. The proposed method can be incorporated into the GLM in an easily applicable way that flexibly combines any available auxiliary signals into optimal nuisance regressors. This work has potential significance both for conventional neuroscientific fNIRS experiments as well as for emerging applications of fNIRS in everyday environments, medicine and BCI, where high Contrast to Noise Ratio is of importance for single trial analysis."
DAVID BOAS,Insight into the fundamental trade-offs of diffusion MRI from polarization-sensitive optical coherence tomography in ex vivo human brain,"In the first study comparing high angular resolution diffusion MRI (dMRI) in the human brain to axonal orientation measurements from polarization-sensitive optical coherence tomography (PSOCT), we compare the accuracy of orientation estimates from various dMRI sampling schemes and reconstruction methods. We find that, if the reconstruction approach is chosen carefully, single-shell dMRI data can yield the same accuracy as multi-shell data, and only moderately lower accuracy than a full Cartesian-grid sampling scheme. Our results suggest that current dMRI reconstruction approaches do not benefit substantially from ultra-high b-values or from very large numbers of diffusion-encoding directions. We also show that accuracy remains stable across dMRI voxel sizes of 1 mm or smaller but degrades at 2 mm, particularly in areas of complex white-matter architecture. We also show that, as the spatial resolution is reduced, axonal configurations in a dMRI voxel can no longer be modeled as a small set of distinct axon populations, violating an assumption that is sometimes made by dMRI reconstruction techniques. Our findings have implications for in vivo studies and illustrate the value of PSOCT as a source of ground-truth measurements of white-matter organization that does not suffer from the distortions typical of histological techniques."
DAVID BOAS,Using the general linear model to improve performance in fNIRS single trial analysis and classification: a perspective,"Within a decade, single trial analysis of functional Near Infrared Spectroscopy (fNIRS) signals has gained significant momentum, and fNIRS joined the set of modalities frequently used for active and passive Brain Computer Interfaces (BCI). A great variety of methods for feature extraction and classification have been explored using state-of-the-art Machine Learning methods. In contrast, signal preprocessing and cleaning pipelines for fNIRS often follow simple recipes and so far rarely incorporate the available state-of-the-art in adjacent fields. In neuroscience, where fMRI and fNIRS are established neuroimaging tools, evoked hemodynamic brain activity is typically estimated across multiple trials using a General Linear Model (GLM). With the help of the GLM, subject, channel, and task specific evoked hemodynamic responses are estimated, and the evoked brain activity is more robustly separated from systemic physiological interference using independent measures of nuisance regressors, such as short-separation fNIRS measurements. When correctly applied in single trial analysis, e.g., in BCI, this approach can significantly enhance contrast to noise ratio of the brain signal, improve feature separability and ultimately lead to better classification accuracy. In this manuscript, we provide a brief introduction into the GLM and show how to incorporate it into a typical BCI preprocessing pipeline and cross-validation. Using a resting state fNIRS data set augmented with synthetic hemodynamic responses that provide ground truth brain activity, we compare the quality of commonly used fNIRS features for BCI that are extracted from (1) conventionally preprocessed signals, and (2) signals preprocessed with the GLM and physiological nuisance regressors. We show that the GLM-based approach can provide better single trial estimates of brain activity as well as a new feature type, i.e., the weight of the individual and channel-specific hemodynamic response function (HRF) regressor. The improved estimates yield features with higher separability, that significantly enhance accuracy in a binary classification task when compared to conventional preprocessing-on average +7.4% across subjects and feature types. We propose to adapt this well-established approach from neuroscience to the domain of single-trial analysis and preprocessing wherever the classification of evoked brain activity is of concern, for instance in BCI."
DAVID BOAS,Erratum: Prolonged monitoring of cerebral blood flow and autoregulation with diffuse correlation spectroscopy in neurocritical care patients,Corrected disclosures for the article “Prolonged monitoring of cerebral blood flow and autoregulation with diffuse correlation spectroscopy in neurocritical care patients.” DOI: 10.1117/1.NPh.5.4.045005.
DAVID BOAS,A deep learning approach to 3D segmentation of brain vasculature,The segmentation of blood-vessels is an important preprocessing step for the quantitative analysis of brain vasculature. We approach the segmentation task for two-photon brain angiograms using a fully convolutional 3D deep neural network.
DAVID BOAS,Contribution of speckle noise in near-infrared spectroscopy measurements,"Near-infrared spectroscopy (NIRS) is widely used in biomedical optics with applications ranging from basic science, such as in functional neuroimaging, to clinical, as in pulse oximetry. Despite the relatively low absorption of tissue in the near-infrared, there is still a significant amount of optical attenuation produced by the highly scattering nature of tissue. Because of this, designers of NIRS systems have to balance source optical power and source–detector separation to maximize the signal-to-noise ratio (SNR). However, theoretical estimations of SNR neglect the effects of speckle. Speckle manifests as fluctuations of the optical power received at the detector. These fluctuations are caused by interference of the multiple random paths taken by photons in tissue. We present a model for the NIRS SNR that includes the effects of speckle. We performed experimental validations with a NIRS system to show that it agrees with our model. Additionally, we performed computer simulations based on the model to estimate the contribution of speckle noise for different collection areas and source–detector separations. We show that at short source–detector separation, speckle contributes most of the noise when using long coherence length sources. Considering this additional noise is especially important for hybrid applications that use NIRS and speckle contrast simultaneously, such as in diffuse correlation spectroscopy."
DAVID BOAS,Awake chronic mouse model of targeted pial vessel occlusion via photothrombosis,"Animal models of stroke are used extensively to study the mechanisms involved in the acute and chronic phases of recovery following stroke. A translatable animal model that closely mimics the mechanisms of a human stroke is essential in understanding recovery processes as well as developing therapies that improve functional outcomes. We describe a photothrombosis stroke model that is capable of targeting a single distal pial branch of the middle cerebral artery with minimal damage to the surrounding parenchyma in awake head-fixed mice. Mice are implanted with chronic cranial windows above one hemisphere of the brain that allow optical access to study recovery mechanisms for over a month following occlusion. Additionally, we study the effect of laser spot size used for occlusion and demonstrate that a spot size with small axial and lateral resolution has the advantage of minimizing unwanted photodamage while still monitoring macroscopic changes to cerebral blood flow during photothrombosis. We show that temporally guiding illumination using real-time feedback of blood flow dynamics also minimized unwanted photodamage to the vascular network. Finally, through quantifiable behavior deficits and chronic imaging we show that this model can be used to study recovery mechanisms or the effects of therapeutics longitudinally."
DAVID BOAS,Capturing pain in the cortex during general anesthesia: near infrared spectroscopy measures in patients undergoing catheter ablation of arrhythmias,"The predictability of pain makes surgery an ideal model for the study of pain and the development of strategies for analgesia and reduction of perioperative pain. As functional near-infrared spectroscopy reproduces the known functional magnetic resonance imaging activations in response to a painful stimulus, we evaluated the feasibility of functional near-infrared spectroscopy to measure cortical responses to noxious stimulation during general anesthesia. A multichannel continuous wave near-infrared imager was used to measure somatosensory and frontal cortical activation in patients undergoing catheter ablation of arrhythmias under general anesthesia. Anesthetic technique was standardized and intraoperative NIRS signals recorded continuously with markers placed in the data set for the timing and duration of each cardiac ablation event. Frontal cortical signals only were suitable for analysis in five of eight patients studied (mean age 14 ± 1 years, weight 66.7 ± 17.6 kg, 2 males). Thirty ablative lesions were recorded for the five patients. Radiofrequency or cryoablation was temporally associated with a hemodynamic response function in the frontal cortex characterized by a significant decrease in oxyhemoglobin concentration (paired t-test, p<0.05) with the nadir occurring in the period 4 to 6 seconds after application of the ablative lesion. Cortical signals produced by catheter ablation of arrhythmias in patients under general anesthesia mirrored those seen with noxious stimulation in awake, healthy volunteers, during sedation for colonoscopy, and functional Magnetic Resonance Imaging activations in response to pain. This study demonstrates the feasibility and potential utility of functional near-infrared spectroscopy as an objective measure of cortical activation under general anesthesia."
DAVID BOAS,Frontal lobe hemodynamic responses to painful stimulation: a potential brain marker of nociception,"The purpose of this study was to use functional near-infrared spectroscopy (fNIRS) to examine patterns of both activation and deactivation that occur in the frontal lobe in response to noxious stimuli. The frontal lobe was selected because it has been shown to be activated by noxious stimuli in functional magnetic resonance imaging studies. The brain region is located behind the forehead which is devoid of hair, providing a relative ease of placement for fNIRS probes on this area of the head. Based on functional magnetic resonance imaging studies showing blood-oxygenation-level dependent changes in the frontal lobes, we evaluated functional near-infrared spectroscopy measures in response to two levels of electrical pain in awake, healthy human subjects (n = 10; male = 10). Each subject underwent two recording sessions separated by a 30-minute resting period. Data collected from 7 subjects were analyzed, containing a total of 38/36 low/high intensity pain stimuli for the first recording session and 27/31 pain stimuli for the second session. Our results show that there is a robust and significant deactivation in sections of the frontal cortices. Further development and definition of the specificity and sensitivity of the approach may provide an objective measure of nociceptive activity in the brain that can be easily applied in the surgical setting."
DAVID BOAS,Automatic graph-based modeling of brain microvessels captured with two-photon microscopy,"Graph models of cerebral vasculature derived from two-photon microscopy have shown to be relevant to study brain microphysiology. Automatic graphing of these microvessels remain problematic due to the vascular network complexity and two-photon sensitivity limitations with depth. In this paper, we propose a fully automatic processing pipeline to address this issue. The modeling scheme consists of a fully-convolution neural network to segment microvessels, a three-dimensional surface model generator, and a geometry contraction algorithm to produce graphical models with a single connected component. Based on a quantitative assessment using NetMets metrics, at a tolerance of 60 μm, false negative and false positive geometric error 19 rates are 3.8% and 4.2%, respectively, whereas false nega- 20 tive and false positive topological error rates are 6.1% and 4.5%, respectively. Our qualitative evaluation confirms the efficiency of our scheme in generating useful and accurate graphical models."
DAVID BOAS,Capillary red blood cell velocimetry by phase-resolved optical coherence tomography,
DAVID BOAS,Specificity of hemodynamic brain responses to painful stimuli: a functional near-infrared spectroscopy study,"Assessing pain in individuals not able to communicate (e.g. infants, under surgery, or following stroke) is difficult due to the lack of non-verbal objective measures of pain. Near-infrared spectroscopy (NIRS) being a portable, non-invasive and inexpensive method of monitoring cerebral hemodynamic activity has the potential to provide such a measure. Here we used functional NIRS to evaluate brain activation to an innocuous and a noxious electrical stimulus on healthy human subjects (n = 11). For both innocuous and noxious stimuli, we observed a signal change in the primary somatosensory cortex contralateral to the stimulus. The painful and non-painful stimuli can be differentiated based on their signal size and profile. We also observed that repetitive noxious stimuli resulted in adaptation of the signal. Furthermore, the signal was distinguishable from a skin sympathetic response to pain that tended to mask it. Our results support the notion that functional NIRS has a potential utility as an objective measure of pain."
DAVID BOAS,Validation of diffuse correlation spectroscopy measurements of rodent cerebral blood flow with simultaneous arterial spin labeling MRI; towards MRI-optical continuous cerebral metabolic monitoring.,"Cerebral blood flow (CBF) during stepped hypercapnia was measured simultaneously in the rat brain using near-infrared diffuse correlation spectroscopy (DCS) and arterial spin labeling MRI (ASL). DCS and ASL CBF values agree very well, with high correlation (R=0.86, p< 10(-9)), even when physiological instability perturbed the vascular response. A partial volume effect was evident in the smaller magnitude of the optical CBF response compared to the MRI values (averaged over the cortical area), primarily due to the inclusion of white matter in the optically sampled volume. The 8.2 and 11.7 mm mid-separation channels of the multi-distance optical probe had the lowest partial volume impact, reflecting ~75 % of the MR signal change. Using a multiplicative correction factor, the ASL CBF could be predicted with no more than 10% relative error, affording an opportunity for real-time relative cerebral metabolism monitoring in conjunction with MR measurement of cerebral blood volume using super paramagnetic contrast agents."
DAVID BOAS,"Targeted micro-fiber arrays for measuring and manipulating localized multi-scale neural dynamics over large, deep brain volumes during behavior","Neural population dynamics relevant for behavior vary over multiple spatial and temporal scales across 3-dimensional volumes. Current optical approaches lack the spatial coverage and resolution necessary to measure and manipulate naturally occurring patterns of large-scale, distributed dynamics within and across deep brain regions such as the striatum. We designed a new micro-fiber array and imaging approach capable of chronically measuring and optogenetically manipulating local dynamics across over 100 targeted locations simultaneously in head-fixed and freely moving mice. We developed a semi-automated micro-CT based strategy to precisely localize positions of each optical fiber. This highly-customizable approach enables investigation of multi-scale spatial and temporal patterns of cell-type and neurotransmitter specific signals over arbitrary 3-D volumes at a spatial resolution and coverage previously inaccessible. We applied this method to resolve rapid dopamine release dynamics across the striatum volume which revealed distinct, modality specific spatiotemporal patterns in response to salient sensory stimuli extending over millimeters of tissue. Targeted optogenetics through our fiber arrays enabled flexible control of neural signaling on multiple spatial scales, better matching endogenous signaling patterns, and spatial localization of behavioral function across large circuits."
DAVID BOAS,Special section guest editorial: thirty years of functional near-infrared spectroscopy,
GUSTAVO MOSTOSLAVSKY,Shb Deficient Mice Display an Augmented TH2 Response in Peripheral CD4+ T Cells,"BACKGROUND: Shb, a ubiquitously expressed Src homology 2 domain-containing adaptor protein has previously been implicated in the signaling of various tyrosine kinase receptors including the TCR. Shb associates with SLP76, LAT and Vav, all important components in the signaling cascade governing T cell function and development. A Shb knockout mouse was recently generated and the aim of the current study was to address the importance of Shb deficiency on T cell development and function. RESULTS: Shb knockout mice did not display any major changes in thymocyte development despite an aberrant TCR signaling pattern, including increased basal activation and reduced stimulation-induced phosphorylation. The loss of Shb expression did however affect peripheral CD4+ TH cells resulting in an increased proliferative response to TCR stimulation and an elevated IL-4 production of naïve TH cells. This suggests a TH2 skewing of the Shb knockout immune system, seemingly caused by an altered TCR signaling pattern. CONCLUSION: Our results indicate that Shb appears to play an important modulating role on TCR signaling, thus regulating the peripheral CD4+ TH2 cell response."
GUSTAVO MOSTOSLAVSKY,An ES-Like Pluripotent State in FGF-Dependent Murine iPS cells,"Recent data demonstrates that stem cells can exist in two morphologically, molecularly and functionally distinct pluripotent states; a naïve LIF-dependent pluripotent state which is represented by murine embryonic stem cells (mESCs) and an FGF-dependent primed pluripotent state represented by murine and rat epiblast stem cells (EpiSCs). We find that derivation of induced pluripotent stem cells (iPSCs) under EpiSC culture conditions yields FGF-dependent iPSCs from hereon called FGF-iPSCs) which, unexpectedly, display naïve ES-like/ICM properties. FGF-iPSCs display X-chromosome activation, multi-lineage differentiation, teratoma competence and chimera contribution in vivo. Our findings suggest that in 129 and Bl6 mouse strains, iPSCs can dominantly adopt a naive pluripotent state regardless of culture growth factor conditions. Characterization of the key molecular signalling pathways revealed FGF-iPSCs to depend on the Activin/Nodal and FGF pathways, while signalling through the JAK-STAT pathway is not required for FGF-iPS cell maintenance. Our findings suggest that in 129 and Bl6 mouse strains, iPSCs can dominantly adopt a naive pluripotent state regardless of culture growth factor conditions."
STEVEN T KATZ,"The role of religion in the longer-range future, April 6, 7, and 8, 2006","The conference brought together some 40 experts from various disciplines to ponder upon the “great dilemma” of how science, religion, and the human future interact. In particular, different panels looked at trends in what is happening to religion around the world, questions about how religion is impacting the current political and economic order, and how the social dynamics unleashed by science and by religion can be reconciled."
STEVEN T KATZ,Elie Wiesel: the man and his legacy,
STEVEN T KATZ,The “Great War” and the Jewish people: a review essay,
STEVEN T KATZ,CD1c Bypasses Lysosomes to Present a Lipopeptide Antigen with 12 Amino Acids,"The recent discovery of dideoxymycobactin (DDM) as a ligand for CD1a demonstrates how a nonribosomal lipopeptide antigen is presented to T cells. DDM contains an unusual acylation motif and a peptide sequence present only in mycobacteria, but its discovery raises the possibility that ribosomally produced viral or mammalian proteins that commonly undergo lipidation might also function as antigens. To test this, we measured T cell responses to synthetic acylpeptides that mimic lipoproteins produced by cells and viruses. CD1c presented an N-acyl glycine dodecamer peptide (lipo-12) to human T cells, and the response was specific for the acyl linkage as well as the peptide length and sequence. Thus, CD1c represents the second member of the CD1 family to present lipopeptides. lipo-12 was efficiently recognized when presented by intact cells, and unlike DDM, it was inactivated by proteases and augmented by protease inhibitors. Although lysosomes often promote antigen presentation by CD1, rerouting CD1c to lysosomes by mutating CD1 tail sequences caused reduction in lipo-12 presentation. Thus, although certain antigens require antigen processing in lysosomes, others are destroyed there, providing a hypothesis for the evolutionary conservation of large CD1 families containing isoforms that survey early endosomal pathways."
STEVEN T KATZ,Does Modality of Survey Administration Impact Data Quality: Audio Computer Assisted Self Interview (ACASI) Versus Self-Administered Pen and Paper?,"BACKGROUND. In the context of a randomized controlled trial (RCT) on HIV testing in the emergency department (ED) setting, we evaluated preferences for survey modality and data quality arising from each modality. METHODS. Enrolled participants were offered the choice of answering a survey via audio computer assisted self-interview (ACASI) or pen and paper self-administered questionnaire (SAQ). We evaluated factors influencing choice of survey modality. We defined unusable data for a particular survey domain as answering fewer than 75% of the questions in the domain. We then compared ACASI and SAQ with respect to unusable data for domains that address sensitive topics. RESULTS. Of 758 enrolled ED patients, 218 (29%) chose ACASI, 343 chose SAQ (45%) and 197 (26%) opted not to complete either. Results of the log-binomial regression indicated that older (RR=1.08 per decade) and less educated participants (RR=1.25) were more likely to choose SAQ over ACASI. ACASI yielded substantially less unusable data than SAQ. CONCLUSIONS. In the ED setting there may be a tradeoff between increased participation with SAQ versus better data quality with ACASI. Future studies of novel approaches to maximize the use of ACASI in the ED setting are needed."
RAN CANETTI,Universally composable end-to-end secure messaging,
RAN CANETTI,The impossibility of obfuscation with auxiliary input or a universal simulator,"In this paper we show that the existence of general indistinguishability obfuscators conjectured in a few recent works implies, somewhat counterintuitively, strong impossibility results for virtual black box obfuscation. In particular, we show that indistinguishability obfuscation for all circuits implies: * The impossibility of average-case virtual black box obfuscation with auxiliary input for any circuit family with super-polynomial pseudo-entropy. Such circuit families include all pseudo-random function families, and all families of encryption algorithms and randomized digital signatures that generate their required coin flips pseudo-randomly. Impossibility holds even when the auxiliary input depends only on the public circuit family, and not the specific circuit in the family being obfuscated. * The impossibility of average-case virtual black box obfuscation with a universal simulator (with or without any auxiliary input) for any circuit family with super-polynomial pseudo-entropy. These bounds significantly strengthen the impossibility results of Goldwasser and Kalai (STOC 2005)."
RAN CANETTI,"The random oracle methodology, revisited","We take a critical look at the relationship between the security of cryptographic schemes in the Random Oracle Model, and the security of the schemes that result from implementing the random oracle by so called “cryptographic hash functions”. The main result of this paper is a negative one: There exist signature and encryption schemes that are secure in the Random Oracle Model, but for which any implementation of the random oracle results in insecure schemes. In the process of devising the above schemes, we consider possible definitions for the notion of a “good implementation” of a random oracle, pointing out limitations and challenges"
RAN CANETTI,Anonymous collocation discovery: taming the coronavirus while preserving privacy,"Successful containment of the Coronavirus pandemic rests on the ability to quickly and reliably identify those who have been in close proximity to a contagious individual. Existing tools for doing so rely on the collection of exact location information of individuals over lengthy time periods, and combining this information with other personal information. This unprecedented encroachment on individual privacy at national scales has created an outcry and risks rejection of these tools. We propose an alternative: an extremely simple scheme for providing fine-grained and timely alerts to users who have been in the close vicinity of an infected individual. Crucially, this is done while preserving the anonymity of all individuals, and without collecting or storing any personal information or location history. Our approach is based on using short-range communication mechanisms, like Bluetooth, that are available in all modern cell phones. It can be deployed with very little infrastructure, and incurs a relatively low false-positive rate compared to other collocation methods. We also describe a number of extensions and tradeoffs. We believe that the privacy guarantees provided by the scheme will encourage quick and broad voluntary adoption. When combined with sufficient testing capacity and existing best practices from healthcare professionals, we hope that this may significantly reduce the infection rate."
RAN CANETTI,On the universally composable security of OpenStack,"We initiate an effort to demonstrate how we can provide a rigorous, perceptible and holistic security analysis of a very large scale system. We choose OpenStack to exemplify our approach. OpenStack is the prevalent open-source, non-proprietary package for managing cloud services and data centers. It is highly complex and consists of multiple inter-related components which are developed by separate, loosely coordinated groups. All of these properties make the security analysis of OpenStack both a crucial mission and a challenging one. We base our modeling and security analysis in the universally composable (UC) security framework, which has been so far used mainly for analyzing security of cryptographic protocols. Indeed, demonstrating how the UC framework can be used to argue about security-sensitive systems which are mostly non-cryptographic, in nature, is one of the main contributions of this work. Our analysis has the following key features: 1. It is user-centric: It stresses the security guarantees given to users of the system, in terms of privacy, correctness, and timeliness of the services. 2. It provides defense in depth: It considers the security of OpenStack even when some of the components are compromised. This departs from the traditional design approach of OpenStack, which assumes that all services are fully trusted. 3. It is modular: It formulates security properties for individual components and uses them to assert security properties of the overall system. 4. It is extendable: Due to the scale of OpenStack, we limit the analysis to some core services of OpenStack at a high level. The analysis is extendable to more detail of the services, and other services can be added to the model using the same methodology, without much conceptual difficulty. Because of the modularity of the analysis, new services can be added one by one, almost independently of each other. Although our analysis covers only a number of core components of OpenStack, it formulates some basic and important security trade offs in the design. It also naturally paves the way to a more comprehensive analysis of OpenStack. In addition, as a by-product result of our modeling, we introduce a novel tokening mechanism, RAFT, which is backward compatible with Fernet Token currently used in OpenStack. By applying the UC framework, we prove that RAFT's one-time use tokens can realize a more secure OpenStack cloud than bearer tokens do."
RAN CANETTI,Automated exposure notification for COVID-19,"In the current COVID-19 pandemic, various Automated Exposure Notification (AEN) systems have been proposed to help quickly identify potential contacts of infected individuals. All these systems try to leverage the current understanding of the following factors: transmission risk, technology to address risk modeling, system policies and privacy considerations. While AEN holds promise for mitigating the spread of COVID-19, using short-range communication channels (Bluetooth) in smartphones to detect close individual contacts may be inaccurate for modeling and informing transmission risk. This work finds that the current close contact definitions may be inadequate to reduce viral spread using AEN technology. Consequently, relying on distance measurements from Bluetooth Low-Energy may not be optimal for determining risks of exposure and protecting privacy. This paper's literature analysis suggests that AEN may perform better by using broadly accessible technologies to sense the respiratory activity, mask status, or environment of participants. Moreover, the paper remains cognizant that smartphone sensors can leak private information and thus recommends additional objectives for maintaining user privacy without compromising utility for population health. This literature review and analysis will simultaneously interest (i) health professionals who desire a fundamental understanding of the design and utility of AEN systems and (ii) technologists interested in understanding their epidemiological basis in the light of recent research. Ultimately, the two disparate communities need to understand each other to assess the value of AEN systems in mitigating viral spread, whether for the COVID-19 pandemic or for future ones."
RAN CANETTI,Privacy-preserving automated exposure notification,"Contact tracing is an essential component of public health efforts to slow the spread of COVID-19 and other infectious diseases. Automating parts of the contact tracing process has the potential to significantly increase its scalability and efficacy, but also raises an array of privacy concerns, including the risk of unwanted identification of infected individuals and clandestine collection of privacy-invasive data about the population at large. In this paper, we focus on automating the exposure notification part of contact tracing, which notifies people who have been in close proximity to infected people of their potential exposure to the virus. This work is among the first to focus on the privacy aspects of automated exposure notification. We introduce two privacy-preserving exposure notification schemes based on proximity detection. Both systems are decentralized - no central entity has access to sensitive data. The first scheme is simple and highly efficient, and provides strong privacy for non-diagnosed individuals and some privacy for diagnosed individuals. The second scheme provides enhanced privacy guarantees for diagnosed individuals, at some cost to efficiency. We provide formal definitions for automated exposure notification and its security, and we prove the security of our constructions with respect to these definitions."
RAN CANETTI,From soft classifiers to hard decisions,"A popular methodology for building binary decision-making classifiers in the presence of imperfect information is to first construct a calibrated non-binary ""scoring"" classifier, and then to post-process this score to obtain a binary decision. We study various fairness (or, error-balance) properties of this methodology, when the non-binary scores are calibrated over all protected groups, and with a variety of post-processing algorithms. Specifically, we show: First, there does not exist a general way to post-process a calibrated classifier to equalize protected groups' positive or negative predictive value (PPV or NPV). For certain ""nice"" calibrated classifiers, either PPV or NPV can be equalized when the post-processor uses different thresholds across protected groups. Still, when the post-processing consists of a single global threshold across all groups, natural fairness properties, such as equalizing PPV in a nontrivial way, do not hold even for ""nice"" classifiers. Second, when the post-processing stage is allowed to defer on some decisions (that is, to avoid making a decision by handing off some examples to a separate process), then for the non-deferred decisions, the resulting classifier can be made to equalize PPV, NPV, false positive rate (FPR) and false negative rate (FNR) across the protected groups. This suggests a way to partially evade the impossibility results of Chouldechova and Kleinberg et al., which preclude equalizing all of these measures simultaneously. We also present different deferring strategies and show how they affect the fairness properties of the overall system. We evaluate our post-processing techniques using the COMPAS data set from 2016."
RAN CANETTI,On the universally composable security of OpenStack,"We initiate an effort to demonstrate how we can provide a rigorous, perceptible and holistic security analysis of a very large scale system. We choose OpenStack to exemplify our approach. OpenStack is the prevalent open-source, non-proprietary package for managing cloud services and data centers. It is highly complex and consists of multiple inter-related components which are developed by separate, loosely coordinated groups. All of these properties make the security analysis of OpenStack both a crucial mission and a challenging one. We base our modeling and security analysis in the universally composable (UC) security framework, which has been so far used mainly for analyzing security of cryptographic protocols. Indeed, demonstrating how the UC framework can be used to argue about security-sensitive systems which are mostly non-cryptographic, in nature, is one of the main contributions of this work. Our analysis has the following key features: 1. It is user-centric: It stresses the security guarantees given to users of the system, in terms of privacy, correctness, and timeliness of the services. 2. It provides defense in depth: It considers the security of OpenStack even when some of the components are compromised. This departs from the traditional design approach of OpenStack, which assumes that all services are fully trusted. 3. It is modular: It formulates security properties for individual components and uses them to assert security properties of the overall system. 4. It is extendable: Due to the scale of OpenStack, we limit the analysis to some core services of OpenStack at a high level. The analysis is extendable to more detail of the services, and other services can be added to the model using the same methodology, without much conceptual di culty. Because of the modularity of the analysis, new services can be added one by one, almost independently of each other. Although our analysis covers only a number of core components of OpenStack, it formulates some basic and important security trade o s in the design. It also naturally paves the way to a more comprehensive analysis of OpenStack. In addition, as a by-product result of our modeling, we introduce a novel tokening mechanism, RAFT, which is backward compatible with Fernet Token currently used in OpenStack. By applying the UC framework, we prove that RAFT's one-time use tokens can realize a more secure OpenStack cloud than bearer tokens do."
RAN CANETTI,EasyUC: using EasyCrypt to mechanize proofs of universally composable security,"We present a methodology for using the EasyCrypt proof assistant (originally designed for mechanizing the generation of proofs of game-based security of cryptographic schemes and protocols) to mechanize proofs of security of cryptographic protocols within the universally composable (UC) security framework. This allows, for the first time, the mechanization and formal verification of the entire sequence of steps needed for proving simulation-based security in a modular way: Specifying a protocol and the desired ideal functionality; Constructing a simulator and demonstrating its validity, via reduction to hard computational problems; Invoking the universal composition operation and demonstrating that it indeed preserves security. We demonstrate our methodology on a simple example: stating and proving the security of secure message communication via a one-time pad, where the key comes from a Diffie-Hellman key-exchange, assuming ideally authenticated communication. We first put together EasyCrypt-verified proofs that: (a) the Diffie-Hellman protocol UC-realizes an ideal key-exchange functionality, assuming hardness of the Decisional Diffie-Hellman problem, and (b) one-time-pad encryption, with a key obtained using ideal key-exchange, UC-realizes an ideal secure-communication functionality. We then mechanically combine the two proofs into an EasyCrypt-verified proof that the composed protocol realizes the same ideal secure-communication functionality. Although formulating a methodology that is both sound and workable has proven to be a complex task, we are hopeful that it will prove to be the basis for mechanized UC security analyses for significantly more complex protocols and tasks."
MARSHALL VAN ALSTYNE,Using Markets and Spam to Combat Malware,"We propose an economic mechanism to reduce the incidence of malware that delivers spam. Earlier research proposed attention markets as a solution for unwanted messages, and showed they could provide more net benefit than alternatives such as filtering and taxes. Because it uses a currency system, Attention Bonds faces a challenge. Zombies, botnets, and various forms of malware might steal valuable currency instead of stealing unused CPU cycles. We resolve this problem by taking advantage of the fact that the spam-bot problem has been reduced to financial fraud. As such, the large body of existing work in that realm can be brought to bear. By drawing an analogy between sending and spending, we show how a market mechanism can detect and prevent spam malware. We prove that by using a currency (i) each instance of spam increases the probability of detecting infections, and (ii) the value of eradicating infections can justify insuring users against fraud. This approach attacks spam at the source, a virtue missing from filters that attack spam at the destination. Additionally, the exchange of currency provides signals of interest that can improve the targeting of ads. ISPs benefit from data management services and consumers benefit from the higher average value of messages they receive. We explore these and other secondary effects of attention markets, and find them to offer, on the whole, attractive economic benefits for all – including consumers, advertisers, and the ISPs."
MARSHALL VAN ALSTYNE,A response to fake news as a response to Citizens United,
MARSHALL VAN ALSTYNE,'In situ' data rights,Improving on data portability.
MARSHALL VAN ALSTYNE,How APIs create growth by inverting the firm,"Traditional asset management strategy has emphasized building barriers to entry or closely guarding unique assets to maintain a firm’s comparative advantage. A new ""Inverted Firm"" paradigm, however, has emerged. Under this strategy, firms share data seeking to become platforms by opening digital services to third-parties and capturing part of their external surplus. This contrasts with a ""pipeline"" strategy where the firm itself creates value. This paper quantitatively estimates the effect of adopting an inverted firm strategy through the lens of Application Programming Interfaces (APIs), a key enabling technology. Using both public data and that of a private API development firm, we document rapid growth of the API network and connecting apps since 2005. We then perform difference-in-difference and synthetic control analyses and find that public firms adopting public APIs grew an additional 38.7% over sixteen years relative to similar non-adopters. We find no significant effect from the use of APIs purely for internal productivity, the pipeline strategy. Within the subset of firms that adopt public APIs, those that attract more third-party complementors and those that become more central to the network see faster growth. Using variation in network centrality caused by API degradation, an instrumental variables analysis confirms a causal role for APIs in firm market value. Finally, we document an important downside of public APIs: increased risk of data breach. Overall, these facts lead us to conclude that APIs have a large and positive impact on economic growth and do so primarily by enabling an inverted firm strategy."
ROBERT A STERN,Clinicopathological evaluation of chronic traumatic encephalopathy in players of American football,"IMPORTANCE: Players of American football may be at increased risk of long-term neurological conditions, particularly chronic traumatic encephalopathy (CTE). OBJECTIVE: To determine the neuropathological and clinical features of deceased football players with CTE. DESIGN, SETTING, AND PARTICIPANTS: Case series of 202 football players whose brains were donated for research. Neuropathological evaluations and retrospective telephone clinical assessments (including head trauma history) with informants were performed blinded. Online questionnaires ascertained athletic and military history. EXPOSURES: Participation in American football at any level of play. MAIN OUTCOMES AND MEASURES: Neuropathological diagnoses of neurodegenerative diseases, including CTE, based on defined diagnostic criteria; CTE neuropathological severity (stages I to IV or dichotomized into mild [stages I and II] and severe [stages III and IV]); informant-reported athletic history and, for players who died in 2014 or later, clinical presentation, including behavior, mood, and cognitive symptoms and dementia. RESULTS: Among 202 deceased former football players (median age at death, 66 years [interquartile range, 47-76 years]), CTE was neuropathologically diagnosed in 177 players (87%; median age at death, 67 years [interquartile range, 52-77 years]; mean years of football participation, 15.1 [SD, 5.2]), including 0 of 2 pre–high school, 3 of 14 high school (21%), 48 of 53 college (91%), 9 of 14 semiprofessional (64%), 7 of 8 Canadian Football League (88%), and 110 of 111 National Football League (99%) players. Neuropathological severity of CTE was distributed across the highest level of play, with all 3 former high school players having mild pathology and the majority of former college (27 [56%]), semiprofessional (5 [56%]), and professional (101 [86%]) players having severe pathology. Among 27 participants with mild CTE pathology, 26 (96%) had behavioral or mood symptoms or both, 23 (85%) had cognitive symptoms, and 9 (33%) had signs of dementia. Among 84 participants with severe CTE pathology, 75 (89%) had behavioral or mood symptoms or both, 80 (95%) had cognitive symptoms, and 71 (85%) had signs of dementia. CONCLUSIONS AND RELEVANCE: In a convenience sample of deceased football players who donated their brains for research, a high proportion had neuropathological evidence of CTE, suggesting that CTE may be related to prior participation in football."
ALAN C MOSS,The long-baseline neutrino experiment: exploring fundamental symmetries of the universe,"The preponderance of matter over antimatter in the early Universe, the dynamics of the supernova bursts that produced the heavy elements necessary for life and whether protons eventually decay --- these mysteries at the forefront of particle physics and astrophysics are key to understanding the early evolution of our Universe, its current state and its eventual fate. The Long-Baseline Neutrino Experiment (LBNE) represents an extensively developed plan for a world-class experiment dedicated to addressing these questions. LBNE is conceived around three central components: (1) a new, high-intensity neutrino source generated from a megawatt-class proton accelerator at Fermi National Accelerator Laboratory, (2) a near neutrino detector just downstream of the source, and (3) a massive liquid argon time-projection chamber deployed as a far detector deep underground at the Sanford Underground Research Facility. This facility, located at the site of the former Homestake Mine in Lead, South Dakota, is approximately 1,300 km from the neutrino source at Fermilab -- a distance (baseline) that delivers optimal sensitivity to neutrino charge-parity symmetry violation and mass ordering effects. This ambitious yet cost-effective design incorporates scalability and flexibility and can accommodate a variety of upgrades and contributions. With its exceptional combination of experimental configuration, technical capabilities, and potential for transformative discoveries, LBNE promises to be a vital facility for the field of particle physics worldwide, providing physicists from around the globe with opportunities to collaborate in a twenty to thirty year program of exciting science. In this document we provide a comprehensive overview of LBNE's scientific objectives, its place in the landscape of neutrino physics worldwide, the technologies it will incorporate and the capabilities it will possess."
MICHAEL MENDILLO,Long‐term observations and physical processes in the Moon's extended sodium tail,"The lunar surface is constantly bombarded by the solar wind, photons, and meteoroids, which can liberate Na atoms from the regolith. These atoms are subsequently accelerated by solar photon pressure to form a long comet-like tail opposite the sun. Near new moon, these atoms encounter the Earth's gravity and are “focused” into a beam of enhanced density. This beam appears as the ∼3° diameter Sodium Moon Spot (SMS). Data from the all sky imager at the El Leoncito Observatory have been analyzed for changes in the SMS shape and brightness. New geometry-based relationships have been found that affect the SMS brightness. The SMS is brighter when the Moon is north of the ecliptic at new moon; the SMS is brighter when new moon occurs near perigee; and the SMS peaks in brightness ∼5 h after new moon. After removing these effects, the data were analyzed for long term and seasonal patterns that could be attributed to changes in source mechanisms. No correlation was found between the SMS brightness and the 11-year solar-cycle, the proton or the He++ flow pressure, the density, the speed or the plasma temperature of the solar wind, but an annual pattern was found. A ∼0.83 correlation (Pearson's “r”) was found between the SMS brightness and a 4-year average of sporadic meteor rates at Earth, suggesting a cause-and-effect. The new insights gained from this long-term study put new constraints on the variability of the potential sources of the Na atoms escaping from the Moon."
MICHAEL MENDILLO,Celestial Images: Antiquarian Astronomical Charts and Maps from the Mendillo Collection,
H. DENIS WU,The impact of language and systemic factors on tweeted countries of the world,"This study is intended to unveil the difference of social mediated world via major languages and investigates the volume of tweets individual countries received during 2015–2016 in nine languages –Arabic, Chinese, English, French, German, Japanese, Portuguese, Russian, and Spanish. Shared language, country attributes, economic power, and communication resources were used in predicting country mention. The salient countries on Twitter overall are vastly diverse and vary from language to language. Based on cluster analysis, English and Japanese tweets distinguish themselves from other languages; yet the result from rank-order correlation test shows Arabic and French tweets treat countries differently from the rest. Core nations are still covered more in English- and French-language tweets. Shared language factor is found to predict well for tweets in Chinese, Arabic, Spanish, French, and German but not in English and Portuguese."
H. DENIS WU,Facebook users’ engagement and perceived life satisfaction,"This study extends existing research on Facebook’s impact on users’ life satisfaction. The results from two surveys of college students demonstrate a tension between Facebook use and users’ perceived contentment with their lives. Existing literature indicates students use Facebook to enhance self-esteem, yet the results from this study connect increased Facebook use to lower self-reported levels of happiness. In particular, respondents’ interactions with photos and videos increase users’ dissatisfaction. This phenomenon may be due to the impact photos have on the ways users engage in social comparisons with Facebook “friends” and the self-construals they create based on these comparisons."
H. DENIS WU,Which countries does the world talk about? An examination of factors that shape country presence on Twitter,"This study investigates which countries were mentioned most on Twitter during 2013 and what factors—country attributes, communication and economic resources, and contexts—can explain country presence on Twitter. Tweet mentions from 210 countries were derived using full fire hose archival searches. We identify trends that differ from the patterns found in news flow literature. And the results suggest a new era of international communication via Web-based social networks. Although core and semiperiphery countries are mentioned more than periphery countries, mobile phone penetration and sociopolitical instability have reshaped the contours of country images, and only 28% of the 50 most-mentioned countries on Twitter were core countries. This study discusses the implications of evolving social media for traditional news media outlets, world politics, and international relations."
H. DENIS WU,Assessing China's news coverage and soft power in Latin America in the wake of the Belt and Road Initiative (2013–2021),"This study assesses the impact of China's mediated communication strategies implemented in Latin America from 2013 to 2021. We content-analyzed the press coverage about China in nine countries and unveiled tones, topics, and frames which were then examined further across different sources. Public sentiments toward China were gauged to infer a potential link to China's communication efforts. Overall, China's program to influence Latin American media outlets and public opinion has not resulted in substantial gains; the region's sentiment toward China actually deteriorated. There is a communication gap between Chinese and Latin American publics, which is inimical to public diplomacy."
H. DENIS WU,Making war and peace with emotion: examining the Iraq and Iran cases via presidential speech and media coverage,"This study investigates emotions conveyed in US presidential speeches and media coverage regarding the Iraq War and the Iran nuclear deal during 2003 and 2015. The researchers gathered and examined news stories about the two policies, all official speeches delivered by George W Bush and Barack Obama, and opinion polls conducted during the respective six-month period in those two years. Nine discrete emotions were coded to capture the valence and volume in the speeches and news media content. The study finds that emotions appear more frequently in the Iraq discourse than in the Iran counterpart. President Bush used more negative emotions while President Obama employed more positive emotions. Emotion in the media coverage is constant and stable across the two policy periods; yet negative emotions are more prevalent than positive counterparts in the media despite distinct foreign policies. The study also examines public opinion trends toward the two policies for inferring potential linkage. This article contributes to the conceptual nexus among emotional persuasion, journalism pattern, and foreign policy-making process."
H. DENIS WU,Post-truth public diplomacy: a detrimental trend of cross-national communication and how open societies address it,
H. DENIS WU,Polarized x-rays from a magnetar,"Magnetars are neutron stars with ultrastrong magnetic fields, which can be observed in x-rays. Polarization measurements could provide information on their magnetic fields and surface properties. We observed polarized x-rays from the magnetar 4U 0142+61 using the Imaging X-ray Polarimetry Explorer and found a linear polarization degree of 13.5 ± 0.8% averaged over the 2- to 8-kilo-electron volt band. The polarization changes with energy: The degree is 15.0 ± 1.0% at 2 to 4 kilo-electron volts, drops below the instrumental sensitivity ~4 to 5 kilo-electron volts, and rises to 35.2 ± 7.1% at 5.5 to 8 kilo-electron volts. The polarization angle also changes by 90° at ~4 to 5 kilo-electron volts. These results are consistent with a model in which thermal radiation from the magnetar surface is reprocessed by scattering off charged particles in the magnetosphere."
H. DENIS WU,The X-ray polarimetry view of the accreting pulsar Cen X-3,Cen X-3 is the first X-ray pulsar discovered 50 years ago. Radiation from such objects is expected to be highly polarized due to birefringence of plasma and vacuum associated with propagation of photons in presence of the strong magnetic field. Here we present results of the observations of Cen X-3 performed with the Imaging X-ray Polarimetry Explorer. The source exhibited significant flux variability and was observed in two states different by a factor of ~20 in flux. In the low-luminosity state no significant polarization was found either in pulse phase-averaged (with the 3𝜎 upper limit of 12%) or phase-resolved data (the 3𝜎 upper limits are 20-30%). In the bright state the polarization degree of 5.8％ ± 0.3% and polarization angle of $49.°6 ±1.°5 with significance of about 20σ was measured from the spectro-polarimetric analysis of the phase-averaged data. The phase-resolved analysis showed a significant anti-correlation between the flux and the polarization degree as well as strong variations of the polarization angle. The fit with the rotating vector model indicates a position angle of the pulsar spin axis of about 49° and a magnetic obliquity of 17°. The detected relatively low polarization can be explained if the upper layers of the neutron star surface are overheated by the accreted matter and the conversion of the polarization modes occurs within the transition region between the upper hot layer and a cooler underlying atmosphere. A fraction of polarization signal can also be produced by reflection of radiation from the neutron star surface and the accretion curtain.
JULIE R PALMER,Urogenital Abnormalities in Men Exposed to Diethylstilbestrol in Utero: A Cohort Study,"BACKGROUND: Diethylstilbestrol (DES), a synthetic estrogen widely prescribed to pregnant women during the 1940s-70s, has been shown to cause reproductive problems in the daughters. Studies of prenatally-exposed males have yielded conflicting results. METHODS: In data from a collaborative follow-up of three U.S. cohorts of DES-exposed sons, we examined the relation of prenatal DES exposure to occurrence of male urogenital abnormalities. Exposure status was determined through review of prenatal records. Mailed questionnaires (1994, 1997, 2001) asked about specified abnormalities of the urogenital tract. Risk ratios (RR) were estimated by Cox regression with constant time at risk and control for year of birth. RESULTS: Prenatal DES exposure was not associated with varicocele, structural abnormalities of the penis, urethral stenosis, benign prostatic hypertrophy, or inflammation/infection of the prostate, urethra, or epididymus. However, RRs were 1.9 (95% confidence interval 1.1-3.4) for cryptorchidism, 2.5 (1.5-4.3) for epididymal cyst, and 2.4 (1.5-4.4) for testicular inflammation/infection. Stronger associations were observed for DES exposure that began before the 11th week of pregnancy: RRs were 2.9 (1.6-5.2) for cryptorchidism, 3.5 (2.0-6.0) for epididymal cyst, and 3.0 (1.7-5.4) for inflammation/infection of testes. CONCLUSION: These results indicate that prenatal exposure to DES increases risk of male urogenital abnormalities and that the association is strongest for exposure that occurs early in gestation. The findings support the hypothesis that endocrine disrupting chemicals may be a cause of the increased prevalence of cryptorchidism that has been seen in recent years."
JULIE R PALMER,Secondary Sex Ratio among Women Exposed to Diethylstilbestrol in Utero,"BACKGROUND. Diethylstilbestrol (DES), a synthetic estrogen widely prescribed to pregnant women during the mid-1900s, is a potent endocrine disruptor. Previous studies have suggested an association between endocrine-disrupting compounds and secondary sex ratio. METHODS. Data were provided by women participating in the National Cancer Institute (NCI) DES Combined Cohort Study. We used generalized estimating equations to estimate odds ratios (ORs) and 95% confidence intervals (CIs) for the relation of in utero DES exposure to sex ratio (proportion of male births). Models were adjusted for maternal age, child's birth year, parity, and cohort, and accounted for clustering among women with multiple pregnancies. RESULTS. The OR for having a male birth comparing DES-exposed to unexposed women was 1.05 (95% CI, 0.95-1.17). For exposed women with complete data on cumulative DES dose and timing (33%), those first exposed to DES earlier in gestation and to higher doses had the highest odds of having a male birth. The ORs were 0.91 (95% C, 0.65-1.27) for first exposure at ≥ 13 weeks gestation to < 5 g DES; 0.95 (95% CI, 0.71-1.27) for first exposure at ≥ 13 weeks to ≥ 5 g; 1.16 (95% CI, 0.96-1.41) for first exposure at < 13 weeks to < 5 g; and 1.24 (95% CI, 1.04-1.48) for first exposure at < 13 weeks to ≥ 5 g compared with no exposure. Results did not vary appreciably by maternal age, parity, cohort, or infertility history. CONCLUSIONS. Overall, no association was observed between in utero DES exposure and secondary sex ratio, but a significant increase in the proportion of male births was found among women first exposed to DES earlier in gestation and to a higher cumulative dose."
JULIE R PALMER,Lung Cancer Occurrence in Never-Smokers: An Analysis of 13 Cohorts and 22 Cancer Registry Studies,"BACKGROUND. Better information on lung cancer occurrence in lifelong nonsmokers is needed to understand gender and racial disparities and to examine how factors other than active smoking influence risk in different time periods and geographic regions. METHODS AND FINDINGS. We pooled information on lung cancer incidence and/or death rates among self-reported never-smokers from 13 large cohort studies, representing over 630,000 and 1.8 million persons for incidence and mortality, respectively. We also abstracted population-based data for women from 22 cancer registries and ten countries in time periods and geographic regions where few women smoked. Our main findings were: (1) Men had higher death rates from lung cancer than women in all age and racial groups studied; (2) male and female incidence rates were similar when standardized across all ages 40+ y, albeit with some variation by age; (3) African Americans and Asians living in Korea and Japan (but not in the US) had higher death rates from lung cancer than individuals of European descent; (4) no temporal trends were seen when comparing incidence and death rates among US women age 40–69 y during the 1930s to contemporary populations where few women smoke, or in temporal comparisons of never-smokers in two large American Cancer Society cohorts from 1959 to 2004; and (5) lung cancer incidence rates were higher and more variable among women in East Asia than in other geographic areas with low female smoking. CONCLUSIONS. These comprehensive analyses support claims that the death rate from lung cancer among never-smokers is higher in men than in women, and in African Americans and Asians residing in Asia than in individuals of European descent, but contradict assertions that risk is increasing or that women have a higher incidence rate than men. Further research is needed on the high and variable lung cancer rates among women in Pacific Rim countries. Michael Thun and colleagues pooled and analyzed comprehensive data on lung cancer incidence and death rates among never-smokers to examine what factors other than active smoking affect lung cancer risk."
DOUGLAS L ROSENE,Statistical physics approach to quantifying differences in myelinated nerve fibers,"We present a new method to quantify differences in myelinated nerve fibers. These differences range from morphologic characteristics of individual fibers to differences in macroscopic properties of collections of fibers. Our method uses statistical physics tools to improve on traditional measures, such as fiber size and packing density. As a case study, we analyze cross–sectional electron micrographs from the fornix of young and old rhesus monkeys using a semi-automatic detection algorithm to identify and characterize myelinated axons. We then apply a feature selection approach to identify the features that best distinguish between the young and old age groups, achieving a maximum accuracy of 94% when assigning samples to their age groups. This analysis shows that the best discrimination is obtained using the combination of two features: the fraction of occupied axon area and the effective local density. The latter is a modified calculation of axon density, which reflects how closely axons are packed. Our feature analysis approach can be applied to characterize differences that result from biological processes such as aging, damage from trauma or disease or developmental differences, as well as differences between anatomical regions such as the fornix and the cingulum bundle or corpus callosum."
DOUGLAS L ROSENE,Automated identification of neurons and their locations,"Individual locations of many neuronal cell bodies (>10^4) are needed to enable statistically significant measurements of spatial organization within the brain such as nearest‐neighbour and microcolumnarity measurements. In this paper, we introduce an Automated Neuron Recognition Algorithm (ANRA) which obtains the (x, y) location of individual neurons within digitized images of Nissl‐stained, 30 μm thick, frozen sections of the cerebral cortex of the Rhesus monkey. Identification of neurons within such Nissl‐stained sections is inherently difficult due to the variability in neuron staining, the overlap of neurons, the presence of partial or damaged neurons at tissue surfaces, and the presence of non‐neuron objects, such as glial cells, blood vessels, and random artefacts. To overcome these challenges and identify neurons, ANRA applies a combination of image segmentation and machine learning. The steps involve active contour segmentation to find outlines of potential neuron cell bodies followed by artificial neural network training using the segmentation properties (size, optical density, gyration, etc.) to distinguish between neuron and non‐neuron segmentations. ANRA positively identifies 86 ± 5% neurons with 15 ± 8% error (mean ± SD) on a wide range of Nissl‐stained images, whereas semi‐automatic methods obtain 80 ± 7%/17 ± 12%. A further advantage of ANRA is that it affords an unlimited increase in speed from semi‐automatic methods, and is computationally efficient, with the ability to recognize ∼100 neurons per minute using a standard personal computer. ANRA is amenable to analysis of huge photo‐montages of Nissl‐stained tissue, thereby opening the door to fast, efficient and quantitative analysis of vast stores of archival material that exist in laboratories and research collections around the world."
DOUGLAS L ROSENE,Do the Socioeconomic Impacts of Antiretroviral Therapy Vary by Gender? A Longitudinal Study of Kenyan Agricultural Worker Employment Outcomes,"BACKGROUND. As access to antiretroviral therapy (ART) has grown in Africa, attention has turned to evaluating the socio-economic impacts of ART. One key issue is the extent to which improvements in health resulting from ART allows individuals to return to work and earn income. Improvements in health from ART may also be associated with reduced impaired presenteeism, which is the loss of productivity when an ill or disabled individual attends work but accomplishes less at his or her usual tasks or shifts to other, possibly less valuable, tasks. METHODS. Longitudinal data for this analysis come from company payroll records for 97 HIV-infected tea estate workers (the index group, 56 women, 41 men) and a comparison group of all workers assigned to the same work teams (n = 2485, 1691 men, 794 women) for a 37-month period covering two years before and one year after initiating ART. We used nearest neighbour matching methods to estimate the impacts of HIV/AIDS and ART on three monthly employment outcomes for tea estate workers in Kenya – days plucking tea, days assigned to non-plucking assignments, and kilograms harvested when plucking. RESULTS. The female index group worked 30% fewer days plucking tea monthly than the matched female comparison group during the final 9 months pre-ART. They also worked 87% more days on non-plucking assignments. While the monthly gap between the two groups narrowed after beginning ART, the female index group worked 30% fewer days plucking tea and about 100% more days on non-plucking tasks than the comparison group after one year on ART. The male index group was able to maintain a similar pattern of work as their comparison group except during the initial five months on therapy. CONCLUSION. Significant impaired presenteeism continued to exist among the female index group after one year on ART. Future research needs to explore further the socio-economic implications of HIV-infected female workers on ART being less productive than the general female workforce over sustained periods of time."
DOUGLAS L ROSENE,Quantitative birefringence microscopy for imaging the structural integrity of CNS myelin following circumscribed cortical injury in the rhesus monkey,"Significance: Myelin breakdown is likely a key factor in the loss of cognitive and motor function associated with many neurodegenerative diseases. Aim: New methods for imaging myelin structure are needed to characterize and quantify the degradation of myelin in standard whole-brain sections of nonhuman primates and in human brain. Approach: Quantitative birefringence microscopy (qBRM) is a label-free technique for rapid histopathological assessment of myelin structural breakdown following cortical injury in rhesus monkeys. Results: We validate birefringence microscopy for structural imaging of myelin in rhesus monkey brain sections, and we demonstrate the power of qBRM by characterizing the breakdown of myelin following cortical injury, as a model of stroke, in the motor cortex. Conclusions: Birefringence microscopy is a valuable tool for histopathology of myelin and for quantitative assessment of myelin structure. Compared to conventional methods, this label-free technique is sensitive to subtle changes in myelin structure, is fast, and enables more quantitative assessment, without the variability inherent in labeling procedures such as immunohistochemistry."
DOUGLAS L ROSENE,The long-baseline neutrino experiment: exploring fundamental symmetries of the universe,"The preponderance of matter over antimatter in the early Universe, the dynamics of the supernova bursts that produced the heavy elements necessary for life and whether protons eventually decay --- these mysteries at the forefront of particle physics and astrophysics are key to understanding the early evolution of our Universe, its current state and its eventual fate. The Long-Baseline Neutrino Experiment (LBNE) represents an extensively developed plan for a world-class experiment dedicated to addressing these questions. LBNE is conceived around three central components: (1) a new, high-intensity neutrino source generated from a megawatt-class proton accelerator at Fermi National Accelerator Laboratory, (2) a near neutrino detector just downstream of the source, and (3) a massive liquid argon time-projection chamber deployed as a far detector deep underground at the Sanford Underground Research Facility. This facility, located at the site of the former Homestake Mine in Lead, South Dakota, is approximately 1,300 km from the neutrino source at Fermilab -- a distance (baseline) that delivers optimal sensitivity to neutrino charge-parity symmetry violation and mass ordering effects. This ambitious yet cost-effective design incorporates scalability and flexibility and can accommodate a variety of upgrades and contributions. With its exceptional combination of experimental configuration, technical capabilities, and potential for transformative discoveries, LBNE promises to be a vital facility for the field of particle physics worldwide, providing physicists from around the globe with opportunities to collaborate in a twenty to thirty year program of exciting science. In this document we provide a comprehensive overview of LBNE's scientific objectives, its place in the landscape of neutrino physics worldwide, the technologies it will incorporate and the capabilities it will possess."
DAVID ROSENBLOOM,The transformation of community hospitals through the transition to value-based care: Lessons from Massachusetts,"Enabling community hospitals to provide efficient and effective care and maintain competition on par with their academic medical center (AMC) counterparts remain challenges for most states. Advancing accountable care readiness adds to the complexity of these challenges. Community hospitals experience narrower operating margins and more limited access to large populations than their AMC counterparts, making the shift to value-based care difficult. Massachusetts has taken legislative action to ensure a statewide focus on reducing healthcare costs, which includes a nearly $120-million grant program supporting community hospital and system transformation toward a value-based environment. The Massachusetts Health Policy Commission’s Community Hospital Acceleration, Revitalization and Transformation (CHART) investment program is the state’s largest effort to date aimed at readying community hospitals for value-based care. In doing so, Massachusetts has created the largest state-driven, all-payer (payer-blind) readmission reduction initiative in the country. n this paper, we examine the design and evolution of CHART Phases 1 and 2 and offer insights for other states contemplating innovative approaches to bolstering community hospital participation in value-based care models."
ROBINSON W FULWEILER,Ecological control of nitrite in the upper ocean,"Microorganisms oxidize organic nitrogen to nitrate in a series of steps. Nitrite, an intermediate product, accumulates at the base of the sunlit layer in the subtropical ocean, forming a primary nitrite maximum, but can accumulate throughout the sunlit layer at higher latitudes. We model nitrifying chemoautotrophs in a marine ecosystem and demonstrate that microbial community interactions can explain the nitrite distributions. Our theoretical framework proposes that nitrite can accumulate to a higher concentration than ammonium because of differences in underlying redox chemistry and cell size between ammonia- and nitrite-oxidizing chemoautotrophs. Using ocean circulation models, we demonstrate that nitrifying microorganisms are excluded in the sunlit layer when phytoplankton are nitrogen-limited, but thrive at depth when phytoplankton become light-limited, resulting in nitrite accumulation there. However, nitrifying microorganisms may coexist in the sunlit layer when phytoplankton are iron- or light-limited (often in higher latitudes). These results improve understanding of the controls on nitrification, and provide a framework for representing chemoautotrophs and their biogeochemical effects in ocean models."
ROBINSON W FULWEILER,Molecular evidence for sediment nitrogen fixation in a temperate New England estuary,"Primary production in coastal waters is generally nitrogen (N) limited with denitrification outpacing nitrogen fixation (N2-fixation). However, recent work suggests that we have potentially underestimated the importance of heterotrophic sediment N2-fixation in marine ecosystems. We used clone libraries to examine transcript diversity of nifH (a gene associated with N2-fixation) in sediments at three sites in a temperate New England estuary (Waquoit Bay, Massachusetts, USA) and compared our results to net sediment N2 fluxes previously measured at these sites. We observed nifH expression at all sites, including a site heavily impacted by anthropogenic N. At this N impacted site, we also observed mean net sediment N2-fixation, linking the geochemical rate measurement with nifH expression. This same site also had the lowest diversity (non-parametric Shannon = 2.75). At the two other sites, we also detected nifH transcripts, however, the mean N2 flux indicated net denitrification. These results suggest that N2-fixation and denitrification co-occur in these sediments. Of the unique sequences in this study, 67% were most closely related to uncultured bacteria from various marine environments, 17% to Cluster III, 15% to Cluster I, and only 1% to Cluster II. These data add to the growing body of literature that sediment heterotrophic N2-fixation, even under high inorganic nitrogen concentrations, may be an important yet overlooked source of N in coastal systems."
ROBINSON W FULWEILER,"Testing assumptions of nitrogen cycling between a temperate, model coral host and its facultative symbiont: symbiotic contributions to dissolved inorganic nitrogen assimilation","Coral symbioses are predicated on the need for mutual nutrient acquisition and translocation between partners. Carbon translocation is well-studied in this classic mutualism, while nitrogen (N) has received comparatively less attention. Quantifying the mechanisms and dynamics of N assimilation is critical to understanding the functional ecology of coral organisms. Given the importance of symbiosis to the coral holobiont, it is important to determine what role photosynthetic symbionts play in N acquisition. We used the facultatively symbiotic temperate coral Astrangia poculata and ^15N labeling to test the effects of symbiotic state and trophic status on N acquisition. We tracked assimilation of 2 forms of isotopically labeled dissolved inorganic N (DIN: ammonium, ^15NH_4+ and nitrate, ^15NO_3^-) by fed and starved colonies of both symbiotic and aposymbiotic A. poculata. Coral holobiont tissue was subsequently analyzed for δ^15N and changes in photosynthetic efficiency. Results suggest that corals acquired the most N from DIN via their symbiont Breviolum psygmophilum and that NH_4+ is more readily assimilated than NO_3^-. Photosynthetic efficiency increased with the addition of NH_4^+, but only for fed, symbiotic treatments. NO_3^- adversely affected photosynthetic efficiency among starved corals. Our results suggest that symbiosis is advantageous for DIN acquisition, that dysbiosis inhibits corals’ mixotrophic strategy of nutrient acquisition, and that either feeding or symbiosis alone does not fully provide the energetic advantage of both. This study lends support to the emerging hypothesis that symbionts are mutualists in optimal conditions but shift to a parasitic paradigm when resources or energy are scarce."
ROBINSON W FULWEILER,Soil warming accelerates biogeochemical silica cycling in a temperate forest,"Biological cycling of silica plays an important role in terrestrial primary production. Soil warming stemming from climate change can alter the cycling of elements, such as carbon and nitrogen, in forested ecosystems. However, the effects of soil warming on the biogeochemical cycle of silica in forested ecosystems remain unexplored. Here we examine long-term forest silica cycling under ambient and warmed conditions over a 15-year period of experimental soil warming at Harvard Forest (Petersham, MA). Specifically, we measured silica concentrations in organic and mineral soils, and in the foliage and litter of two dominant species (Acer rubrum and Quercus rubra), in a large (30 × 30 m) heated plot and an adjacent control plot (30 × 30 m). In 2016, we also examined effects of heating on dissolved silica in the soil solution, and conducted a litter decomposition experiment using four tree species (Acer rubrum, Quercus rubra, Betula lenta, Tsuga canadensis) to examine effects of warming on the release of biogenic silica (BSi) from plants to soils. We find that tree foliage maintained constant silica concentrations in the control and warmed plots, which, coupled with productivity enhancements under warming, led to an increase in total plant silica uptake. We also find that warming drove an acceleration in the release of silica from decaying litter in three of the four species we examined, and a substantial increase in the silica dissolved in soil solution. However, we observe no changes in soil BSi stocks with warming. Together, our data indicate that warming increases the magnitude of silica uptake by vegetation and accelerates the internal cycling of silica in in temperate forests, with possible, and yet unresolved, effects on the delivery of silica from terrestrial to marine systems."
ROBINSON W FULWEILER,Does elevated CO2 alter silica uptake in trees?,
ROBINSON W FULWEILER,More foxes than hedgehogs: the case for nitrogen fixation in coastal marine sediments,"Nitrogen fixation is an important process connecting the vast atmospheric pool of di‐nitrogen gas to the biosphere. Nitrogen fixation is an energy intensive process, and it is historically thought to occur only to meet nitrogen demands. However, over the last two decades, research has demonstrated that sediment nitrogen fixation occurs in a variety of coastal environments, including those where reduced nitrogen is abundant. This can be met with skepticism when nitrogen fixation is viewed solely as a nitrogen limitation relief mechanism. Here, I propose that coastal sediments are actually ideal environments for nitrogen fixation and synthesize ideas on why this is the case. My goal is to help the community embrace a new paradigm for sediment nitrogen fixation and to see it as an important and even expected process. In doing so, I hope to motivate future research on the spatial and temporal rate dynamics of sediment nitrogen fixation as well as on the sediment nitrogen fixation community composition and activity."
RANGA B MYNENI,Analysis of global LAI/FPAR Pproducts from VIIRS and MODIS sensors for spatio-temporal consistency and uncertainty from 2012-2016,"The operational Moderate Resolution Imaging Spectroradiometer (MODIS) Leaf Area Index (LAI) and Fraction of Photosynthetically Active Radiation absorbed by vegetation (FPAR) algorithm has been successfully implemented for Visible Infrared Imager Radiometer Suite (VIIRS) observations by optimizing a small set of configurable parameters in Look-Up-Tables (LUTs). Our preliminary evaluation showed reasonable agreement between VIIRS and MODIS LAI/FPAR retrievals. However, there is a need for a more comprehensive investigation to assure continuity of multi-sensor global LAI/FPAR time series, as the preliminary evaluation was spatiotemporally limited. In this study, we use a multi-year (2012–2016) global LAI/FPAR product generated from VIIRS and MODIS to evaluate for spatiotemporal consistency. We also quantify uncertainty of the product by utilizing available ground measurements. For both consistency and uncertainty evaluation, we account for variations in biome type and temporal resolution. Our results indicate that the LAI/FPAR retrievals from VIIRS and MODIS are consistent at different spatial (i.e., global and site) and temporal (i.e., 8-day, seasonal and annual) scales. The estimate of mean discrepancy (−0.006 ± 0.013 for LAI and −0.002 ± 0.002 for FPAR) meets the stability requirement for long-term LAI/FPAR Earth System Data Records (ESDRs) from multi-sensors as suggested by the Global Climate Observing System (GCOS). It is noteworthy that the rate of retrievals from the radiative transfer-based main algorithm is also comparable between two sensors. However, a relatively larger discrepancy over tropical forests was observed due to reflectance saturation and an unexpected interannual variation of main algorithm success was noticed due to instability in input surface reflectances. The uncertainties/relative uncertainties of VIIRS and MODIS LAI (FPAR) products assessed through comparisons to ground measurements are estimated to be 0.60/42.2% (0.10/24.4%) and 0.55/39.3% (0.11/26%), respectively. Note that the validated LAI were only distributed in low domains (~2.5), resulting in large relative uncertainty. Therefore, more ground measurements are needed to achieve a more comprehensive evaluation result of product uncertainty. The results presented here generally imbue confidence in the consistency between VIIRS and MODIS LAI/FPAR products and the feasibility of generating long-term multi-sensor LAI/FPAR ESDRs time series."
RANGA B MYNENI,Abiotic controls on macroscale variations of humid tropical forest height,"Spatial variation of tropical forest tree height is a key indicator of ecological processes associated with forest growth and carbon dynamics. Here we examine the macroscale variations of tree height of humid tropical forests across three continents and quantify the climate and edaphic controls on these variations. Forest tree heights are systematically sampled across global humid tropical forests with more than 2.5 million measurements from Geoscience Laser Altimeter System (GLAS) satellite observations (2004–2008). We used top canopy height (TCH) of GLAS footprints to grid the statistical mean and variance and the 90 percentile height of samples at 0.5 degrees to capture the regional variability of average and large trees globally. We used the spatial regression method (spatial eigenvector mapping-SEVM) to evaluate the contributions of climate, soil and topography in explaining and predicting the regional variations of forest height. Statistical models suggest that climate, soil, topography, and spatial contextual information together can explain more than 60% of the observed forest height variation, while climate and soil jointly explain 30% of the height variations. Soil basics, including physical compositions such as clay and sand contents, chemical properties such as PH values and cation-exchange capacity, as well as biological variables such as the depth of organic matter, all present independent but statistically significant relationships to forest height across three continents. We found significant relations between the precipitation and tree height with shorter trees on the average in areas of higher annual water stress, and large trees occurring in areas with low stress and higher annual precipitation but with significant differences across the continents. Our results confirm other landscape and regional studies by showing that soil fertility, topography and climate may jointly control a significant variation of forest height and influencing patterns of aboveground biomass stocks and dynamics. Other factors such as biotic and disturbance regimes, not included in this study, may have less influence on regional variations but strongly mediate landscape and small-scale forest structure and dynamics."
RANGA B MYNENI,Estimation of leaf area index and its sunlit portion from DSCOVR EPIC data: theoretical basis,"This paper presents the theoretical basis of the algorithm designed for the generation of leaf area index and diurnal course of its sunlit portion from NASA's Earth Polychromatic Imaging Camera (EPIC) onboard NOAA's Deep Space Climate Observatory (DSCOVR). The Look-up-Table (LUT) approach implemented in the MODIS operational LAI/FPAR algorithm is adopted. The LUT, which is the heart of the approach, has been significantly modified. First, its parameterization incorporates the canopy hot spot phenomenon and recent advances in the theory of canopy spectral invariants. This allows more accurate decoupling of the structural and radiometric components of the measured Bidirectional Reflectance Factor (BRF), improves scaling properties of the LUT and consequently simplifies adjustments of the algorithm for data spatial resolution and spectral band compositions. Second, the stochastic radiative transfer equations are used to generate the LUT for all biome types. The equations naturally account for radiative effects of the three-dimensional canopy structure on the BRF and allow for an accurate discrimination between sunlit and shaded leaf areas. Third, the LUT entries are measurable, i.e., they can be independently derived from both below canopy measurements of the transmitted and above canopy measurements of reflected radiation fields. This feature makes possible direct validation of the LUT, facilitates identification of its deficiencies and development of refinements. Analyses of field data on canopy structure and leaf optics collected at 18 sites in the Hyytiälä forest in southern boreal zone in Finland and hyperspectral images acquired by the EO-1 Hyperion sensor support the theoretical basis."
RANGA B MYNENI,Prototyping of LAI and FPAR retrievals from MODIS multi-angle implementation of atmospheric correction (MAIAC) data,"Leaf area index (LAI) and fraction of photosynthetically active radiation (FPAR) absorbed by vegetation are key variables in many global models of climate, hydrology, biogeochemistry, and ecology. These parameters are being operationally produced from Terra and Aqua MODIS bidirectional reflectance factor (BRF) data. The MODIS science team has developed, and plans to release, a new version of the BRF product using the multi-angle implementation of atmospheric correction (MAIAC) algorithm from Terra and Aqua MODIS observations. This paper presents analyses of LAI and FPAR retrievals generated with the MODIS LAI/FPAR operational algorithm using Terra MAIAC BRF data. Direct application of the operational algorithm to MAIAC BRF resulted in an underestimation of the MODIS Collection 6 (C6) LAI standard product by up to 10%. The difference was attributed to the disagreement between MAIAC and MODIS BRFs over the vegetation by −2% to +8% in the red spectral band, suggesting different accuracies in the BRF products. The operational LAI/FPAR algorithm was adjusted for uncertainties in the MAIAC BRF data. Its performance evaluated on a limited set of MAIAC BRF data from North and South America suggests an increase in spatial coverage of the best quality, high-precision LAI retrievals of up to 10%. Overall MAIAC LAI and FPAR are consistent with the standard C6 MODIS LAI/FPAR. The increase in spatial coverage of the best quality LAI retrievals resulted in a better agreement of MAIAC LAI with field data compared to the C6 LAI product, with the RMSE decreasing from 0.80 LAI units (C6) down to 0.67 (MAIAC) and the R2 increasing from 0.69 to 0.80. The slope (intercept) of the satellite-derived vs. field-measured LAI regression line has changed from 0.89 (0.39) to 0.97 (0.25)."
RANGA B MYNENI,Implications of whole-disc DSCOVR EPIC spectral observations for estimating Earth's spectral reflectivity based on low-earth-orbiting and geostationary observations,"Earth’s reflectivity is among the key parameters of climate research. National Aeronautics and Space Administration (NASA)’s Earth Polychromatic Imaging Camera (EPIC) onboard National Oceanic and Atmospheric Administration (NOAA)’s Deep Space Climate Observatory (DSCOVR) spacecraft provides spectral reflectance of the entire sunlit Earth in the near backscattering direction every 65 to 110 min. Unlike EPIC, sensors onboard the Earth Orbiting Satellites (EOS) sample reflectance over swaths at a specific local solar time (LST) or over a fixed area. Such intrinsic sampling limits result in an apparent Earth’s reflectivity. We generated spectral reflectance over sampling areas using EPIC data. The difference between the EPIC and EOS estimates is an uncertainty in Earth’s reflectivity. We developed an Earth Reflector Type Index (ERTI) to discriminate between major Earth atmosphere components: clouds, cloud-free ocean, bare and vegetated land. Temporal variations in Earth’s reflectivity are mostly determined by clouds. The sampling area of EOS sensors may not be sufficient to represent cloud variability, resulting in biased estimates. Taking EPIC reflectivity as a reference, low-earth-orbiting-measurements at the sensor-specific LST tend to overestimate EPIC values by 0.8% to 8%. Biases in geostationary orbiting approximations due to a limited sampling area are between −0.7% and 12%. Analyses of ERTI-based Earth component reflectivity indicate that the disagreement between EPIC and EOS estimates depends on the sampling area, observation time and vary between −10% and 23%."
RANGA B MYNENI,Generating global products of LAI and FPAR from SNPP-VIIRS data: theoretical background and implementation,"Leaf area index (LAI) and fraction of photosynthetically active radiation (FPAR) absorbed by vegetation have been successfully generated from the Moderate Resolution Imaging Spectroradiometer (MODIS) data since early 2000. As the Visible Infrared Imaging Radiometer Suite (VIIRS) instrument onboard, the Suomi National Polar-orbiting Partnership (SNPP) has inherited the scientific role of MODIS, and the development of a continuous, consistent, and well-characterized VIIRS LAI/FPAR data set is critical to continue the MODIS time series. In this paper, we build the radiative transfer-based VIIRS-specific lookup tables by achieving minimal difference with the MODIS data set and maximal spatial coverage of retrievals from the main algorithm. The theory of spectral invariants provides the configurable physical parameters, i.e., single scattering albedos (SSAs) that are optimized for VIIRS-specific characteristics. The effort finds a set of smaller red-band SSA and larger near-infraredband SSA for VIIRS compared with the MODIS heritage. The VIIRS LAI/FPAR is evaluated through comparisons with one year of MODIS product in terms of both spatial and temporal patterns. Further validation efforts are still necessary to ensure the product quality. Current results, however, imbue confidence in the VIIRS data set and suggest that the efforts described here meet the goal of achieving the operationally consistent multisensor LAI/FPAR data sets. Moreover, the strategies of parametric adjustment and LAI/FPAR evaluation applied to SNPP-VIIRS can also be employed to the subsequent Joint Polar Satellite System VIIRS or other instruments."
RANGA B MYNENI,Evaluation of MODIS LAI/FPAR product Collection 6. Part 2: Validation and intercomparison,"The aim of this paper is to assess the latest version of the MODIS LAI/FPAR product (MOD15A2H), namely Collection 6 (C6). We comprehensively evaluate this product through three approaches: validation with field measurements, intercomparison with other LAI/FPAR products and comparison with climate variables. Comparisons between ground measurements and C6, as well as C5 LAI/FPAR indicate: (1) MODIS LAI is closer to true LAI than effective LAI; (2) the C6 product is considerably better than C5 with RMSE decreasing from 0.80 down to 0.66; (3) both C5 and C6 products overestimate FPAR over sparsely-vegetated areas. Intercomparisons with three existing global LAI/FPAR products (GLASS, CYCLOPES and GEOV1) are carried out at site, continental and global scales. MODIS and GLASS (CYCLOPES and GEOV1) agree better with each other. This is expected because the surface reflectances, from which these products were derived, were obtained from the same instrument. Considering all biome types, the RMSE of LAI (FPAR) derived from any two products ranges between 0.36 (0.05) and 0.56 (0.09). Temporal comparisons over seven sites for the 2001–2004 period indicate that all products properly capture the seasonality in different biomes, except evergreen broadleaf forests, where infrequent observations due to cloud contamination induce unrealistic variations. Thirteen years of C6 LAI, temperature and precipitation time series data are used to assess the degree of correspondence between their variations. The statistically-significant associations between C6 LAI and climate variables indicate that C6 LAI has the potential to provide reliable biophysical information about the land surface when diagnosing climate-driven vegetation responses."
RANGA B MYNENI,Analyses of impact of needle surface properties on estimation of needle absorption spectrum: case study with coniferous needle and shoot samples,"Leaf scattering spectrum is the key optical variable that conveys information about leaf absorbing constituents from remote sensing. It cannot be directly measured from space because the radiation scattered from leaves is affected by the 3D canopy structure. In addition, some radiation is specularly reflected at the surface of leaves. This portion of reflected radiation is partly polarized, does not interact with pigments inside the leaf and therefore contains no information about its interior. Very little empirical data are available on the spectral and angular scattering properties of leaf surfaces. Whereas canopy-structure effects are well understood, the impact of the leaf surface reflectance on estimation of leaf absorption spectra remains uncertain. This paper presents empirical and theoretical analyses of angular, spectral, and polarimetric measurements of light reflected by needles and shoots ofPinus koraiensisandPicea koraiensisspecies. Our results suggest that ignoring the leaf surface reflected radiation can result in an inaccurate estimation of the leaf absorption spectrum. Polarization measurements may be useful to account for leaf surface effects because radiation reflected from the leaf surface is partly polarized, whereas that from the leaf interior is not."
RANGA B MYNENI,A bibliometric visualization review of the MODIS LAI/FPAR products from 1995 to 2020,"The MODIS LAI/FPAR products have been widely used in various fields since their first public release in 2000. This review intends to summarize the history, development trends, scientific collaborations, disciplines involved, and research hotspots of these products. Its aim is to intrigue researchers and stimulate new research direction. Based on literature data from the Web of Science (WOS) and associated funding information, we conducted a bibliometric visualization review of the MODIS LAI/FPAR products from 1995 to 2020 using bibliometric and social network analysis (SNA) methods. We drew the following conclusions: (1) research based on the MODIS LAI/FPAR shows an upward trend with a multiyear average growth rate of 24.9% in the number of publications. (2) Researchers from China and the USA are the backbone of this research area, among which the Chinese Academy of Sciences (CAS) is the core research institution. (3) Research based on the MODIS LAI/FPAR covers a wide range of disciplines but mainly focus on environmental science and ecology. (4) Ecology, crop production estimation, algorithm improvement, and validation are the hotspots of these studies. (5) Broadening the research field, improving the algorithms, and overcoming existing difficulties in heterogeneous surface, scale effects, and complex terrains will be the trend of future research. Our work provides a clear view of the development of the MODIS LAI/FPAR products and valuable information for scholars to broaden their research fields."
RANGA B MYNENI,Vegetation hot spot signatures from synergy of EPIC/DSCOVR and EOS/SUOMI sensors to monitor changes in global forests,
RANGA B MYNENI,Vegetation angular signatures of equatorial forests from DSCOVR EPIC and Terra MISR observations,"In vegetation canopies cross-shading between finite dimensional leaves leads to a peak in reflectance in the retro-illumination direction. This effect is called the hot spot in optical remote sensing. The hotspot region in reflectance of vegetated surfaces represents the most information-rich directions in the angular distribution of canopy reflected radiation. This paper presents a new approach for generating hot spot signatures of equatorial forests from synergistic analyses of multiangle observations from the Multiangle Imaging SpectroRadiometer (MISR) on Terra platform and near backscattering reflectance data from the Earth Polychromatic Imaging Camera (EPIC) onboard NOAA’s Deep Space Climate Observatory (DSCOVR). A canopy radiation model parameterized in terms of canopy spectral invariants underlies the theoretical basis for joining Terra MISR and DSCOVR EPIC data. The proposed model can accurately reproduce both MISR angular signatures acquired at 10:30 local solar time and diurnal courses of EPIC reflectance (NRMSE < 9%, R^2 > 0.8). Analyses of time series of the hot spot signature suggest its ability to unambiguously detect seasonal changes of equatorial forests."
RANGA B MYNENI,Vegetation Earth system data record from DSCOVR EPIC observations: new parameters in version 2 VESDR product,"The NASA's Earth Polychromatic Imaging Camera (EPIC) onboard NOAA's Deep Space Climate Observatory (DSCOVR) mission was launched on February 11, 2015 to the Sun-Earth Lagrangian L1 point where it began to collect radiance data of the entire sunlit Earth every 65 to 110 min in June 2015. It provides imageries in near backscattering directions at ten ultraviolet to near infrared narrow spectral bands. The DSCOVR EPIC science product suite includes vegetation Earth System Data Record (VESDR). The first version of the product provided leaf area index (LAI) and diurnal courses of normalized difference vegetation index (NDVI), sunlit LAI (SLAI), fraction of incident photosynthetically active radiation (FPAR) and directional area scattering function (DASF). Five new parameters have been developed and added in Version 2 VESDR product: Earth Reflector Type Index (ERTI) and Canopy Scattering Coefficient (CSC) at 443 nm, 551 nm, 680 nm and 779 nm. The parameters are at 10 km regional sinusoidal grids and 65 to 110 minute temporal frequency generated from the upstream DSCOVR EPIC BRF product and available from the NASA Langley Atmospheric Science Data Center. This poster provides an overview of the EPIC VESDR research. This includes a description of the VESDR product, its initial quality assessment, showcasing the value of the product for monitoring changes of the equatorial forests and obtaining new parameters from on canopy structure from the VESDR parameters."
RANGA B MYNENI,"Vegetation hot spot signatures from synergy of EPIC/DSCOVR, MISR/Terra, MODIS and geostationary sensors to monitor changes and biophysical processes of global forests",
RANGA B MYNENI,"DSCOVR EPIC Vegetation Earth System Data Record (DSCOVR EPIC L2 VESDR),",
RANGA B MYNENI,Seasonal and long-term variations in leaf area of Congolese rainforest,"It is important to understand temporal and spatial variations in the structure and photosynthetic capacity of tropical rainforests in a world of changing climate, increased disturbances and human appropriation. The equatorial rainforests of Central Africa are the second largest and least disturbed of the biodiversly-rich and highly productive rainforests on Earth. Currently, there is a dearth of knowledge about the phenological behavior and long-term changes that these forests are experiencing. Thus, this study reports on leaf area seasonality and its time trend over the past two decades as assessed from multiple remotely sensed datasets. Seasonal variations of leaf area in Congolese forests derived from MODIS data co-vary with the bimodal precipitation pattern in this region, with higher values during the wet season. Independent observational evidence derived from MISR and EPIC sensors in the form of angular reflectance signatures further corroborate this seasonal behavior of leaf area. The bimodal patterns vary latitudinally within this large region. Two sub-seasonal cycles, each consisting of a dry and wet season, could be discerned clearly. These exhibit different sensitivities to changes in precipitation. Contrary to a previous published report, no widespread decline in leaf area was detected across the entire extent of the Congolese rainforests over the past two decades with the latest MODIS Collection 6 dataset. Long-term precipitation decline did occur in some localized areas, but these had minimal impacts on leaf area, as inferred from MODIS and MISR multi-angle observations."
RANGA B MYNENI,Post-drought decline of the Amazon carbon sink,"Amazon forests have experienced frequent and severe droughts in the past two decades. However, little is known about the large-scale legacy of droughts on carbon stocks and dynamics of forests. Using systematic sampling of forest structure measured by LiDAR waveforms from 2003 to 2008, here we show a significant loss of carbon over the entire Amazon basin at a rate of 0.3 ± 0.2 (95% CI) PgC yr−1 after the 2005 mega-drought, which continued persistently over the next 3 years (2005–2008). The changes in forest structure, captured by average LiDAR forest height and converted to above ground biomass carbon density, show an average loss of 2.35 ± 1.80 MgC ha−1 a year after (2006) in the epicenter of the drought. With more frequent droughts expected in future, forests of Amazon may lose their role as a robust sink of carbon, leading to a significant positive climate feedback and exacerbating warming trends."
RANGA B MYNENI,Evaluation of MODIS LAI/FPAR product Collection 6. Part 1: consistency and improvements,"As the latest version of Moderate Resolution Imaging Spectroradiometer (MODIS) Leaf Area Index (LAI) and Fraction of Photosynthetically Active Radiation (FPAR) products, Collection 6 (C6) has been distributed since August 2015. This collection is evaluated in this two-part series with the goal of assessing product accuracy, uncertainty and consistency with the previous version. In this first paper, we compare C6 (MOD15A2H) with Collection 5 (C5) to check for consistency and discuss the scale effects associated with changing spatial resolution between the two collections and benefits from improvements to algorithm inputs. Compared with C5, C6 benefits from two improved inputs: (1) L2G–lite surface reflectance at 500 m resolution in place of reflectance at 1 km resolution; and (2) new multi-year land-cover product at 500 m resolution in place of the 1 km static land-cover product. Global and seasonal comparison between C5 and C6 indicates good continuity and consistency for all biome types. Moreover, inter-annual LAI anomalies at the regional scale from C5 and C6 agree well. The proportion of main radiative transfer algorithm retrievals in C6 increased slightly in most biome types, notably including a 17% improvement in evergreen broadleaf forests. With same biome input, the mean RMSE of LAI and FPAR between C5 and C6 at global scale are 0.29 and 0.091, respectively, but biome type disagreement worsens the consistency (LAI: 0.39, FPAR: 0.102). By quantifying the impact of input changes, we find that the improvements of both land-cover and reflectance products improve LAI/FPAR products. Moreover, we find that spatial scale effects due to a resolution change from 1 km to 500 m do not cause any significant differences."
RANGA B MYNENI,"Vegetation hot spot signatures from synergy of DSCOVR EPIC, Terra MISR, MODIS and geostationary sensors","It has been widely recognized that the hotspot region in Bidirectional Reflectance Factors (BRF) of vegetated surfaces represents the most information-rich directions in the directional distribution of canopy reflected radiation. The hotspot effect is strongly correlated with canopy architectural parameters such as foliage size and shape, crown geometry and within-crown foliage arrangement, leaf area index and its sunlit fraction. Here we present a new methodology that synergistically incorporate features of Terra Multi-angle Imaging SpectroRadiometer (MISR) and Moderate Resolution Imaging Spectroradiometer (MODIS), Aqua MODIS, Earth Polychromatic Imaging Camera (EPIC) onboard the Deep Space Climate Observatory (DSCOVR), Advanced Baseline Imager (ABI) carried by the Geostationary Operational Environmental Satellites (GOES) R series and Advanced Himawari Imager (AHI) observation geometries and results in a new type of hot spot signatures that maximally sensitive to vegetation changes. We discuss a physical basis for the synergy of multi-sensor data. Five areas that include Amazonian forests (evergreen broadleaf forest), Mississippi forest (deciduous forest), Heihe River Basin (crops), Genhe forest (coniferous forest) and Australia central grassland were selected to generate time series of hot spot signatures of different land cover types for the period of concurrent Terra/Aqua/DSCOVR and geostationary observations. We demonstrate value of the hot spot signatures for monitoring changes and biophysical processes in vegetated land through analyses of variations in magnitude and shape of angular distribution of canopy reflected radiation and the rigorous use of radiative transfer theory."
RANGA B MYNENI,Improving the MODIS LAI compositing using prior time-series information,
RANGA B MYNENI,Revisit the performance of MODIS and VIIRS leaf area index products from the perspective of time-series stability,"As an essential vegetation structural parameter, leaf area index (LAI) is involved in many critical biochemical processes, such as photosynthesis, respiration, and precipitation interception. The MODerate resolution Imaging Spectroradiometer (MODIS) and Visible Infrared Imager Radiometer Suite (VIIRS) LAI sequence products have long supported various global climate, biogeochemistry, and energy flux research. These applications all rely on the accuracy of the product’s long time series. However, uncontrolled interferences (e.g., adverse observation conditions and sensor uncertainties) potentially introduce substantial uncertainties to time series in product applications. As one of the most sensitive areas in response to global climate change, the Tibet Plateau (TP) has been treated as a crucial testing ground for thousands of studies on vegetation. To ensure the credibility of the studies arising from MODIS/VIIRSLAI products, the temporal quality uncertainties of data need to be clarified. This article proposed a method to revisit the temporal stability of the MODIS (MOD and MYD) and VIIRS (VNP) LAI in the TP, expecting to provide useful information for better accounting for the uncertainties in this area. Results show that the MODIS and VIIRS LAI were relatively stable in time series and available to be used continuously, among which the temporal quality of the MODIS LAI was the most stable. Moreover, the MODIS and VIIRS LAI products performed similarly in both time-series stability and time-series anomaly distribution, magnitudes and fluctuations. The time-series stability evaluation strategy applied to the MODIS and VIIRS LAI can also be employed to other remote sensing products."
RANGA B MYNENI,Evaluation of the MODIS LAI/FPAR algorithm based on 3D-RTM simulations: a case study of grassland,"Uncertainty assessment of the moderate resolution imaging spectroradiometer (MODIS) leaf area index (LAI) and the fraction of photosynthetically active radiation absorbed by vegetation (FPAR) retrieval algorithm can provide a scientific basis for the usage and improvement of this widely-used product. Previous evaluations generally depended on the intercomparison with other datasets as well as direct validation using ground measurements, which mix the uncertainties from the model, inputs, and assessment method. In this study, we adopted the evaluation method based on three-dimensional radiative transfer model (3D RTM) simulations, which helps to separate model uncertainty and other factors. We used the well-validated 3D RTM LESS (large-scale remote sensing data and image simulation framework) for a grassland scene simulation and calculated bidirectional reflectance factors (BRFs) as inputs for the LAI/FPAR retrieval. The dependency between LAI/FPAR truth and model estimation serves as the algorithm uncertainty indicator. This paper analyzed the LAI/FPAR uncertainty caused by inherent model uncertainty, input uncertainty (BRF and biome classification), clumping effect, and scale dependency. We found that the uncertainties of different algorithm paths vary greatly (−6.61% and +84.85% bias for main and backup algorithm, respectively) and the “hotspot” geometry results in greatest retrieval uncertainty. For the input uncertainty, the BRF of the near-infrared (NIR) band has greater impacts than that of the red band, and the biome misclassification also leads to nonnegligible LAI/FPAR bias. Moreover, the clumping effect leads to a significant LAI underestimation (−0.846 and −0.525 LAI difference for two clumping types), but the scale dependency (pixel size ranges from 100 m to 1000 m) has little impact on LAI/FPAR uncertainty. Overall, this study provides a new perspective on the evaluation of LAI/FPAR retrieval algorithms."
RANGA B MYNENI,Interannual variability of carbon uptake of secondary forests in the Brazilian Amazon (2004‐2014),"Tropical secondary forests (SF) play an important role in the global carbon cycle as a major terrestrial carbon sink. Here, we use high-resolution TerraClass data set for tracking land use activities in the Brazilian Amazon from 2004–2014 to detect spatial patterns and carbon sequestration dynamics of secondary forests (SF). By integrating satellite lidar and radar observations, we found the SF area in the Brazilian Amazon increased from approximately 22 Mha (10^6 ha) in 2004 to 28 Mha in 2014. However, the expansion in area was also accompanied by a dynamic land use activity that resulted in about 50% recycling of SF area annually from frequent clearing and abandonment. Consequently, the average age of SF remained less than 10 years (age ~8.2 with one standard deviation of 3.2 spatially) over the period of the study. Estimation of changes of carbon stocks shows that SF accumulates approximately 8.5 Mg ha^−1 year^−1 aboveground biomass during the first 10 years after clearing and abandonment, 4.5 Mg ha^−1 year^−1 for the next 10 years followed by a more gradual increase of 3 Mg ha^−1 year^−1 from 20 to 30 years with much slower rate thereafter. The effective carbon uptake of SF in Brazilian Amazon was negligible (0.06 ± 0.03 PgC year^−1) during this period, but the interannual variability was significantly larger (±0.2 PgC year^−1). If the SF areas were left to grow without further clearing for 100 years, it would absorb about 0.14 PgC year^−1 from the atmosphere, partially compensating the emissions from current rate of deforestation in the Brazilian Amazon."
RANGA B MYNENI,Vegetation Earth system data record from DSCOVR EPIC observation: product status,"The NASA's Earth Polychromatic Imaging Camera (EPIC) onboard NOAA's Deep Space Climate Observatory (DSCOVR) mission was launched on February 11, 2015 to the Sun-Earth Lagrangian L1 point where it began to collect radiance data of the entire sunlit Earth every 65 to 110 min in June 2015. It provides imageries in near backscattering directions at ten ultraviolet to near infrared narrow spectral bands. The DSCOVR EPIC science product suite includes vegetation Earth System Data Record (VESDR) that provides leaf area index (LAI) and diurnal courses of normalized difference vegetation index (NDVI), sunlit LAI (SLAI), fraction of incident photosynthetically active radiation (FPAR) directional area scattering function (DASF) as well as recently added Earth Reflector Type Index (ERTI) and Canopy Scattering Coefficient (CSC). The parameters at 10 km regional sinusoidal grids and 65 to 110 minute temporal frequency generated from the upstream DSCOVR EPIC BRF product will be available from the NASA Langley Atmospheric Science Data Center. This poster provides an overview of the EPIC VESDR research. This includes a description of the algorithm and its performance, details of the product, its quality assessment and scientific exploration."
RANGA B MYNENI,Seasonal variation of Congo rainforests from DSCOVR/EPIC and MISR observations,"Knowledge of seasonal variation of tropical rainforests are essential for understanding its response to the climate change. The equatorial Congo rainforest, the second-largest on Earth, however is still lacking of systematic analyses of seasonal variation in forest greenness with strong observational evidences. This poster investigates the seasonality of the Congo rainforest with Deep Space Climate Observatory (DSCOVR) Earth Polychromatic Imaging Camera (EPIC) and Multi-angle Imaging SpectroRadiometer (MISR) datasets. The monthly mean near-infrared (NIR) bidirectional reflectance factor (BRF) from EPIC of 2016-2019 exhibits a bimodal pattern over the Congo forest, which is consistent with the seasonality of leaf area from MODIS LAI (Collection 6) and precipitation from TRMM. Analyses of NIR BRF angular signatures from MISR and EPIC further confirms that more green leaves appear during the wet season compared to the dry season. Variation in the canopy scattering coefficient (CSC) suggests a higher leaf absorption in wet season than in dry season, which is attributed to a higher concentration of chlorophyll and/or dry matter in leaves. This research also demonstrates the complementarity and consistency between DSCOVR/EPIC records and existing data from polar-orbiting satellites in tropical rainforest monitoring, and the CSC will be provided in the upcoming version of DSCOVR/EPIC Vegetation Earth System Data Record (VESDR) science product."
RANGA B MYNENI,Vegetation hot spot signatures from synergy of EPIC/DSCOVR and EOS/SUOMI sensors to monitor changes in global forests,
RANGA B MYNENI,Modelling vegetation angular signatures from DSCOVR/EPIC and MISR Observations,"The angular signatures of reflectance are rich sources of diagnostic information about vegetation canopies, because the geometric structure and foliage optics determine their magnitude and angular distribution. This poster presents angular signatures of Bidirectional Reflectance Factors (BRF) in different biome types for the period of concurrent DSCOVR/EPIC (Earth Polychromatic Imaging Camera onboard the Deep Space Climate Observatory) and MISR (Terra Multi-angle Imaging SpectroRadiometer) observations. We developed a BRF model, which could approximate DSCOVR/EPIC and MISR observations, through analyses of variations in magnitude and shape of angular distribution of canopy reflected radiation and the rigorous use of radiative transfer theory. In this model, the correlation coefficient, visible fraction of leaf area in the direction Ω from the sunlit areas of leaves, is an important parameter that allows us to extend conventional radiative transfer equation to media with finite dimensional scatters and consequently accurately discriminate between sunlit and shaded leaves. Our model was able to capture seasonal variations of reflectance in amazon rain forest, which resulted from changes in both leaf area and solar zenith angle."
RANGA B MYNENI,DSCOVR EPIC L2 version 2 vegetation Earth system data record: product status,
RANGA B MYNENI,Sensor-independent LAI/FPAR CDR: reconstructing a global sensor-independent climate data record of MODIS and VIIRS LAI/FPAR from 2000 to 2022,"Leaf area index (LAI) and fraction of photosynthetically active radiation (FPAR) are critical biophysical parameters for the characterization of terrestrial ecosystems. Long-term global LAI/FPAR products, such as the moderate resolution imaging spectroradiometer (MODIS) and the Visible Infrared Imaging Radiometer Suite (VIIRS), provide the fundamental dataset for accessing vegetation dynamics and studying climate change. However, existing global LAI/FPAR products suffer from several limitations, including spatial–temporal inconsistencies and accuracy issues. Considering these limitations, this study develops a sensor-independent (SI) LAI/FPAR climate data record (CDR) based on Terra-MODIS/Aqua-MODIS/VIIRS LAI/FPAR standard products. The SI LAI/FPAR CDR covers the period from 2000 to 2022, at spatial resolutions of 500 m/5 km/0.05∘, 8 d/bimonthly temporal frequencies and available in sinusoidal and WGS1984 projections. The methodology includes (i) comprehensive analyses of sensor-specific quality assessment variables to select high-quality retrievals, (ii) application of the spatial–temporal tensor (ST-tensor) completion model to extrapolate LAI and FPAR beyond areas with high-quality retrievals, (iii) generation of SI LAI/FPAR CDR in various projections and various spatial and temporal resolutions, and (iv) evaluation of the CDR by direct comparisons with ground data and indirectly through reproducing results of LAI/FPAR trends documented in the literature. This paper provides a comprehensive analysis of each step involved in the generation of the SI LAI/FPAR CDR, as well as evaluation of the ST-tensor completion model. Comparisons of SI LAI (FPAR) CDR with ground truth data suggest an RMSE of 0.84 LAI (0.15 FPAR) units with R2 of 0.72 (0.79), which outperform the standard Terra/Aqua/VIIRS LAI (FPAR) products. The SI LAI/FPAR CDR is characterized by a low time series stability (TSS) value, suggesting a more stable and less noisy dataset than sensor-dependent counterparts. Furthermore, the mean absolute error (MAE) of the CDR is also lower, suggesting that SI LAI/FPAR CDR is comparable in accuracy to high-quality retrievals. LAI/FPAR trend analyses based on the SI LAI/FPAR CDR agree with previous studies, which indirectly provides enhanced capabilities to utilize this CDR for studying vegetation dynamics and climate change. Overall, the integration of multiple satellite data sources and the use of advanced gap filling modeling techniques improve the accuracy of the SI LAI/FPAR CDR, ensuring the reliability of long-term vegetation studies, global carbon cycle modeling, and land policy development for informed decision-making and sustainable environmental management. The SI LAI/FPAR CDR is open access and available under a Creative Commons Attribution 4.0 License at https://doi.org/10.5281/zenodo.8076540 (Pu et al., 2023a)."
RANGA B MYNENI,Seasonal biological carryover dominates northern vegetation growth,"The state of ecosystems is influenced strongly by their past, and describing this carryover effect is important to accurately forecast their future behaviors. However, the strength and persistence of this carryover effect on ecosystem dynamics in comparison to that of simultaneous environmental drivers are still poorly understood. Here, we show that vegetation growth carryover (VGC), defined as the effect of present states of vegetation on subsequent growth, exerts strong positive impacts on seasonal vegetation growth over the Northern Hemisphere. In particular, this VGC of early growing-season vegetation growth is even stronger than past and co-occurring climate on determining peak-to-late season vegetation growth, and is the primary contributor to the recently observed annual greening trend. The effect of seasonal VGC persists into the subsequent year but not further. Current process-based ecosystem models greatly underestimate the VGC effect, and may therefore underestimate the CO2 sequestration potential of northern vegetation under future warming."
MALIKA JEFFRIES-EL,"Benzobisoxazole cruciforms: a tunable, cross-conjugated platform for the generation of deep blue OLED materials","Four new cross-conjugated small molecules based on a central benzo[1,2-d:4,5-d′]bisoxazole moiety possessing semi-independently tunable HOMO and LUMO levels were synthesized and the properties of these materials were evaluated experimentally and theoretically. The molecules were thermally stable with 5% weight loss occurring well above 350 °C. The cruciforms all exhibited blue emission in solution ranging from 433–450 nm. Host–guest OLEDs fabricated from various concentrations of these materials using the small molecule host 4,4′-bis(9-carbazolyl)-biphenyl (CBP) exhibited deep blue-emission with Commission Internationale de L'Eclairage (CIE) coordinates of (0.15 ≤ x ≤ 0.17, 0.05 ≤ y ≤ 0.11), and maximum luminance efficiencies as high as ∼2 cd A−1. These results demonstrate the potential of benzobisoxazole cruciforms as emitters for developing high-performance deep blue OLEDs."
MALIKA JEFFRIES-EL,"Evaluating the effect of heteroatoms on the photophysical properties of donor-acceptor conjugated polymers based on 2,6-di(thiophen-2-yl)benzo[1,2-b:4,5-b ']difuran: two-photon cross section and ultrafast time-resolved spectroscopy","We investigate the influence of the heteroatom on the electronic and photophysical properties of four conjugated polymers based on 3,7-didodecyl-2,6-di(thiophen-2-yl)benzo[1,2-b:4,5-b′]difuran (BDF) as the donor and 3,6-di(thiophen-2-yl)-1,4-diketopyrrolo[3,4-c]pyrrole (TDPP) or 3,6-di(2-furanyl)-1,4-diketopyrrolo[3,4-c]pyrrole (FDPP) as the acceptor. The polymers with a furan as the linker showed higher extinction coefficients than their thiophene counterparts. Ultrafast fluorescence decay showed that the exciton relaxation process is affected by the type of linker in these conjugated polymers. Theoretical calculations showed that the polymers with a furan as the linker are more planar than their thiophene analogues. Also, theoretical calculation showed that the polymers with a thiophene as the linker have larger transition dipole moments. The two-photon absorption cross sections (TPACS) of the polymers with a furan as the linker were larger than their thiophene polymer analogues. These results suggest that the polymers with a furan as the linker have higher charge transfer character than their thiophene polymers analogues. The photovoltaics performance for these polymers are correlated with their optical properties. These results suggest that furan-derivatives are good candidates for synthetic exploration for long-range energy transport materials in photovoltaic applications."
MALIKA JEFFRIES-EL,"Evaluating the role of molecular heredity in the optical and electronic properties of cross-conjugated benzo[1,2-d:4,5-d']bisoxazoles","A series of eight benzo[1,2-d:4,5-d']bisoxazole (BBOs) were synthesized using the heredity principle as a design motif, whereby we investigated which characteristics of the linear parents were inherited by their cross-conjugated children. Four linear parents bearing 4-tert-butylbenzene (P) or 1,3-bis(4-tert-butylphenyl)benzene (M) at either the 2,6- or 4,8-position on the BBO and four cross-conjugated children bearing various combinations of the two isoelectronic aryl substituents were evaluated. Due to the bulky nature of the M substituent compared to that of the P substituent, the influence of steric hindrance along the BBO axes was explored theoretically and experimentally. The optical and electronic properties of each molecule were investigated in the solution and solid state using density functional theory (DFT) and time-dependent DFT (TD-DFT) and characterized using ultraviolet photoelectron spectroscopy (UPS), ultraviolet-visible (UV-vis) spectroscopy, and photoluminescence (PL) spectroscopy. The well-correlated theoretical and experimental results showed that the selective tuning of the highest occupied molecular orbital (HOMO) and lowest unoccupied molecular orbital (LUMO) energy levels was possible through the strategic placement of substituents without impacting the H → L transition energy. Specifically, the theoretical results demonstrated that for the BBO children the HOMO and LUMO energy levels were inherited from the 4,8- and 2,6-parents, respectively. Each molecule was found to exhibit emission maxima ≤451 nm, making them ideal candidates for blue organic light-emitting diode (OLED) materials."
MALIKA JEFFRIES-EL,"Heteroatom and side chain effects on the optical and photophysical properties: ultrafast and nonlinear spectroscopy of new Naphtho[1,2-b:5,6-b ']difuran donor polymers","The photophysical and electronic properties of four novel conjugated donor polymers were investigated to understand the influence of heteroatoms (based on the first two member chalcogens) in the polymer backbone. The side chains were varied as well to evaluate the effect of polymer solubility on the photophysical properties. The donor–acceptor polymer structure is based on naptho[1,2-b:5,6-b′]difuran as the donor moiety, and either 3,6-di(furan-2-yl)-1,4-diketopyrrolo[3,4-c]pyrrole or 3,6-di(thiophen-2-yl)-1,4-diketopyrrolo[3,4-c]pyrrole as the acceptor moiety. Steady-state absorption studies showed that the polymers with the furan moiety in the backbone displayed a favorable tendency of capturing more solar photons when used in a photovoltaic device. This is observed experimentally by the higher extinction coefficient in the visible and near-infrared regions of these polymers relative to that of their thiophene counterparts. The excitonic lifetimes were monitored using ultrafast dynamics, and the results obtained show that the type of heteroatom π-linker used in the backbone affects the decay dynamics. Furthermore, the side chain also plays a role in determining the fluorescence decay time. Quantum chemical simulations were performed to describe the absorption energies and transition characters. Two-photon absorption cross sections (TPA-δ) were analyzed with the simulations, illustrating the planarity of the backbone in relation to its torsional angles. Because of the planarity in the molecular backbone, the polymer with the furan π-linker showed a higher TPA-δ relative to that of its thiophene counterpart. This suggests that the furan compound will display higher charge transfer (CT) tendencies in comparison to those of their thiophene analogues. The pump–probe transient absorption technique was employed to probe the nonemissive states (including the CT state) of the polymers, and unique activities were captured at 500 and 750 nm for all of the studied compounds. Target and global analyses were performed to understand the dynamics of each peak and deduce the number of components responsible for the transient behavior observed respectively. The results obtained suggest that the furan π-linker component of a donor and acceptor moiety in a conjugated polymer might be a more suitable candidate compared with its more popular chalcogenic counterpart, thiophene, for use as donor materials in bulk heterojunction photovoltaic devices."
MALIKA JEFFRIES-EL,Evaluating the impact of fluorination on the electro-optical properties of cross-conjugated benzobisoxazoles,"Six 2,4,6,8-tetrarylbenzo[1,2-d:4,5-d′]bisoxazoles (BBOs) were synthesized: three bearing phenyl substituents at the 2- and 6-positions and three bearing perfluorophenyl groups at those positions. The influence of perfluoro-aryl group substitution on the physical, optical, and electronic properties of 2,4,6,8-tetrarylbenzo[1,2-d:4,5-d′]bisoxazoles (BBO) was evaluated using both experimental and theoretical methods. The density functional theory (DFT) model was found to be well-matched to the experimental optical data, as evidenced by the UV–vis spectra. Both cyclic voltammetry (CV) and ultraviolet photoelectron spectroscopy (UPS) were used to determine the position of the HOMO with varying results. The values obtained by CV were deeper than those obtained via UPS and correlated well with the theoretical calculations. However, the UPS values were more consistent with the expected outcomes for a system with segregated frontier molecular orbitals (FMOs). The UPS results are also supported by the electrostatic potential maps, which indicate that the electron density within the LUMO and HOMO is nearly completely localized along the 2,6- or 4,8-axis, respectively. The summation of the results indicates that strongly electron-withdrawing groups can be used to selectively tune the LUMO level with minor perturbation of the HOMO, something that is challenging to accomplish in typical donor–acceptor systems."
VICTOR A COELHO,The long-baseline neutrino experiment: exploring fundamental symmetries of the universe,"The preponderance of matter over antimatter in the early Universe, the dynamics of the supernova bursts that produced the heavy elements necessary for life and whether protons eventually decay --- these mysteries at the forefront of particle physics and astrophysics are key to understanding the early evolution of our Universe, its current state and its eventual fate. The Long-Baseline Neutrino Experiment (LBNE) represents an extensively developed plan for a world-class experiment dedicated to addressing these questions. LBNE is conceived around three central components: (1) a new, high-intensity neutrino source generated from a megawatt-class proton accelerator at Fermi National Accelerator Laboratory, (2) a near neutrino detector just downstream of the source, and (3) a massive liquid argon time-projection chamber deployed as a far detector deep underground at the Sanford Underground Research Facility. This facility, located at the site of the former Homestake Mine in Lead, South Dakota, is approximately 1,300 km from the neutrino source at Fermilab -- a distance (baseline) that delivers optimal sensitivity to neutrino charge-parity symmetry violation and mass ordering effects. This ambitious yet cost-effective design incorporates scalability and flexibility and can accommodate a variety of upgrades and contributions. With its exceptional combination of experimental configuration, technical capabilities, and potential for transformative discoveries, LBNE promises to be a vital facility for the field of particle physics worldwide, providing physicists from around the globe with opportunities to collaborate in a twenty to thirty year program of exciting science. In this document we provide a comprehensive overview of LBNE's scientific objectives, its place in the landscape of neutrino physics worldwide, the technologies it will incorporate and the capabilities it will possess."
KECIA ALI,Half of faith: American Muslim marriage and divorce in the twenty-first century,"Half of Faith gathers a selection of resources on, and reflections and analyses of, Muslim marriage and divorce in twenty-first century America. In the United States as elsewhere, marriage is central to ongoing Muslim conversations about belonging, identity, and the good life. The articles collected here, written over the course of two decades, provide a window onto moments in American Muslim life and thought. Though far from comprehensive, topics covered include diversity in Islamic legal thought, marriage contracts, wedding customs, dower norms, divorce practices, and experiences of polygyny. Contributors engage—and disagree—with each other, and sometimes with their past selves. By bringing together and making more widely available existing publications alongside a few purpose-written essays, this reader aims to enrich current conversations and to help document scholarly debates and community activism."
KECIA ALI,A Jihad for justice : honoring the work and life of Amina Wadud,
KECIA ALI,Tying the knot: a Feminist/Womanist guide to Muslim marriage in America,"""Tying the Knot: A Feminist/Womanist Guide to Muslim Marriage in America"" is a follow up to the open-access reader ""Half of Faith: American Muslim Marriage and Divorce in the Twenty-First Century"" (OpenBU, 2021; https://hdl.handle.net/2144/42505). The all-new work published here is geared toward American Muslims thinking about and planning for getting married and being married, with all the promise and pitfalls that entails. We cover topics including premarital counseling, marriage contracts, finding an officiant, interreligious marriages, mut‘ah, LGBTQ marriage, and wisdom from married women and widows from African American Muslim communities. The contributors to this volume are American Muslim cis women and non-binary scholars from a range of backgrounds and with a range of perspectives. Our ethic is informed and thoughtful engagement with a range of perspectives. We have more questions than answers. Those questions have emerged from our years—in some cases, decades—of engagement with diverse Muslim communities and organizations. In the essays collected here, we draw on both our academic expertise, where relevant, and our practical experiences to help frame and consider issues that arise in what is a complicated and fraught as well as exciting and joyful life transition. We span the gamut from weddings to widowhood, treating the latter as something anyone embarking on marriage should consider."
KECIA ALI,The End of the World as We Know It: Climate Catastrophe in Nalini Singh’s Paranormal Romance Fiction,
RAUL I GARCIA,Cumulative Lead Exposure and Tooth Loss in Men: The Normative Aging Study,"BACKGROUND. Individuals previously exposed to lead remain at risk because of endogenous release of lead stored in their skeletal compartments. However, it is not known if long-term cumulative lead exposure is a risk factor for tooth loss. OBJECTIVES. We examined the association of bone lead concentrations with loss of natural teeth. METHODS. We examined 333 men enrolled in the Veterans Affairs Normative Aging Study. We used a validated K-shell X-ray fluorescence (KXRF) method to measure lead concentrations in the tibial midshaft and patella. A dentist recorded the number of teeth remaining, and tooth loss was categorized as 0, 1-8 or ≥ 9 missing teeth. We used proportional odds models to estimate the association of bone lead biomarkers with tooth loss, adjusting for age, smoking, diabetes, and other putative confounders. RESULTS. Participants with ≥ 9 missing teeth had significantly higher bone lead concentrations than those who had not experienced tooth loss. In multivariable-adjusted analyses, men in the highest tertile of tibia lead (> 23 μg/g) and patella lead (> 36 μg/g) had approximately three times the odds of having experienced an elevated degree of tooth loss (≥ 9 vs. 0-8 missing teeth or ≥ 1 vs. 0 missing teeth) as those in the lowest tertile [prevalence odds ratio (OR) = 3.03; 95% confidence interval (CI), 1.60-5.76 and OR = 2.41; 95% CI, 1.30-4.49, respectively]. Associations between bone lead biomarkers and tooth loss were similar in magnitude to the increased odds observed in participants who were current smokers. CONCLUSION. Long-term cumulative lead exposure is associated with increased odds of tooth loss."
RAUL I GARCIA,Risk of Tooth Loss After Cigarette Smoking Cessation,"INTRODUCTION. Little is known about the effect of cigarette smoking cessation on risk of tooth loss. We examined how risk of tooth loss changed with longer periods of smoking abstinence in a prospective study of oral health in men. METHODS. Research subjects were 789 men who participated in the Veterans Administration Dental Longitudinal Study from 1968 to 2004. Tooth status and smoking status were determined at examinations performed every 3 years, for a maximum follow-up time of 35 years. Risk of tooth loss subsequent to smoking cessation was assessed sequentially at 1-year intervals with multivariate proportional hazards regression models. Men who never smoked cigarettes, cigars, or pipes formed the reference group. Hazard ratios were adjusted for age, education, total pack-years of cigarette exposure, frequency of brushing, and use of floss. RESULTS. The hazard ratio for tooth loss was 2.1 (95% confidence interval [CI], 1.5-3.1) among men who smoked cigarettes during all or part of follow-up. Risk of tooth loss among men who quit smoking declined as time after smoking cessation increased, from 2.0 (95% CI, 1.4-2.9) after 1 year of abstinence to 1.0 (95% CI, 0.5-2.2) after 15 years of abstinence. The risk remained significantly elevated for the first 9 years of abstinence but eventually dropped to the level of men who never smoked after 13 or more years. CONCLUSION. These results indicate that smoking cessation is beneficial for tooth retention, but long-term abstinence is required to reduce the risk to the level of people who have never smoked."
LESLIE WILL KUO,17th IEEE Real-Time Systems Symposium: Work in Progress Sessions,"Dear Colleagues: ￼￼￼￼This year marks the beginning of a new tradition within the Real-Time Systems Symposium, that of holding special sessions for the presentation of new and on-going projects in real-time systems. The prime purpose of these Work In Progress (WIP) sessions is to provide researchers in Academia and Industry an opportunity to discuss their evolving ideas and gather feedback thereon from the real-time community at large. The idea of holding these sessions is timely, and I am pleased to report that this year RTSS'96 WIP received 22 submissions, of which 14 have been accepted for presentation during the symposium and for inclusion in RTSS'96 WIP proceedings. Many people worked hard to make the idea of holding the WIP sessions a reality. In particular, I would like to thank Sang Son for his hard work in accommodating the WIP sessions within the busy schedule of RTSS'96. Also, I would like to thank all members of the RTSS'96 Program Committee, Al Mok and Doug Locke in particular, for their encouragement and constructive feedback regarding the organization of these sessions. Finally, I would like to thank all those who submitted their work to RTSS'96 WIP and those from RTSS'96 program committee who helped review these submissions. I hope these sessions will prove beneficial, both to the WIP presenters and to RTSS'96 attendees. Azer Bestavros RTSS'96 WIP Chair December 1996."
LESLIE WILL KUO,First Sagittarius A* Event Horizon Telescope results. I. The shadow of the supermassive black hole in the center of the Milky Way,"We present the first Event Horizon Telescope (EHT) observations of Sagittarius A* (Sgr A*), the Galactic center source associated with a supermassive black hole. These observations were conducted in 2017 using a global interferometric array of eight telescopes operating at a wavelength of λ = 1.3 mm. The EHT data resolve a compact emission region with intrahour variability. A variety of imaging and modeling analyses all support an image that is dominated by a bright, thick ring with a diameter of 51.8 ± 2.3 μas (68% credible interval). The ring has modest azimuthal brightness asymmetry and a comparatively dim interior. Using a large suite of numerical simulations, we demonstrate that the EHT images of Sgr A* are consistent with the expected appearance of a Kerr black hole with mass ∼4 × 106 M ⊙, which is inferred to exist at this location based on previous infrared observations of individual stellar orbits, as well as maser proper-motion studies. Our model comparisons disfavor scenarios where the black hole is viewed at high inclination (i &gt; 50°), as well as nonspinning black holes and those with retrograde accretion disks. Our results provide direct evidence for the presence of a supermassive black hole at the center of the Milky Way, and for the first time we connect the predictions from dynamical measurements of stellar orbits on scales of 103–105 gravitational radii to event-horizon-scale images and variability. Furthermore, a comparison with the EHT results for the supermassive black hole M87* shows consistency with the predictions of general relativity spanning over three orders of magnitude in central mass."
STEVEN KOU,How does the introduction of hidden orders affect limit order markets?,
STEVEN KOU,Robo-advising: a dynamic mean-variance approach,"In contrast to traditional financial advising, robo-advising needs to elicit investors’ risk profile via several simple online questions and provide advice consistent with conventional investment wisdom, e.g., rich and young people should invest more in risky assets. To meet the two challenges, we propose to do the asset allocation part of robo-advising using a dynamic mean-variance criterion over the portfolio’s log returns. We obtain analytical and time-consistent optimal portfolio policies under jump-diffusion models and regime-switching models."
STEVEN KOU,Bitcoin mining and electricity consumption,"We propose a dynamic industry equilibrium model for Bitcoin electricity consumption in a general framework, including Bitcoin miners’ optimal entry and exit with technology innovation. By adopting average operating costs as an approximation to the true operating costs, we overcome the difficulty of strong path-dependency due to the interaction among entry, exit, and technology innovation. The model can capture both the upside and downside co-movements of miners’ computing power, electricity consumption, and mining revenue. Our model shows that the Bitcoin electricity consumption will not grow indefinitely, with the ratio of Bitcoin electricity consumption to the miners’ revenue fluctuating within a range."
STEVEN KOU,"Discussion on ""text selection""","This is a discussion on the paper ""Text Selection"" by Kelly et al. (2021)."
MATTHEW FOX,HIV viral load as an independent risk factor for tuberculosis in South Africa: collaborative analysis of cohort studies,"INTRODUCTION: Chronic immune activation due to ongoing HIV replication may lead to impaired immune responses against opportunistic infections such as tuberculosis (TB). We studied the role of HIV replication as a risk factor for incident TB after starting antiretroviral therapy (ART). METHODS: We included all HIV-positive adult patients (≥16 years) in care between 2000 and 2014 at three ART programmes in South Africa. Patients with previous TB were excluded. Missing CD4 cell counts and HIV-RNA viral loads at ART start (baseline) and during follow-up were imputed. We used parametric survival models to assess TB incidence (pulmonary and extrapulmonary) by CD4 cell and HIV-RNA levels, and estimated the rate ratios for TB by including age, sex, baseline viral loads, CD4 cell counts, and WHO clinical stage in the model. We also used Poisson general additive regression models with time-updated CD4 and HIV-RNA values, adjusting for age and sex. We evaluated the frequency of stavudine use and SDS by calendar year 2004-2014. Competing risk regression was used to assess the association between nucleoside reverse transcriptase inhibitor use and SDS in the first 24 months on ART. RESULTS: We included 44,260 patients with a median follow-up time of 2.7 years (interquartile range [IQR] 1.0–5.0); 3,819 incident TB cases were recorded (8.6%). At baseline, the median age was 34 years (IQR 28–41); 30,675 patients (69.3%) were female. The median CD4 cell count was 156 cells/µL (IQR 79–229) and the median HIV-RNA viral load 58,000 copies/mL (IQR 6,000–240,000). Overall TB incidence was 26.2/1,000 person-years (95% confidence interval [CI] 25.3–27.0). Compared to the lowest viral load category (0–999 copies/mL), the adjusted rate ratio for TB was 1.41 (95% CI 1.15–1.75, p < 0.001) in the highest group (>10,000 copies/mL). Time-updated analyses for CD4/HIV-RNA confirmed the association of viral load with the risk for TB. CONCLUSIONS: Our results indicate that ongoing HIV replication is an important risk factor for TB, regardless of CD4 cell counts, and underline the importance of early ART start and retention on ART. Keywords: tuberculosis, HIV, antiretroviral treatment, viral load, CD4 cell count, time-updated, incidence, opportunistic infection, prediction."
MATTHEW FOX,Health facility and skilled birth deliveries among poor women with Jamkesmas health insurance in Indonesia: a mixed-methods study,"BACKGROUND: The growing momentum for quality and affordable health care for all has given rise to the recent global universal health coverage (UHC) movement. As part of Indonesia’s strategy to achieve the goal of UHC, large investments have been made to increase health access for the poor, resulting in the implementation of various health insurance schemes targeted towards the poor and near-poor, including the Jamkesmas program. In the backdrop of Indonesia’s aspiration to reach UHC is the high rate of maternal mortality that disproportionally affects poor women. The objective of this study was to evaluate the association of health facility and skilled birth deliveries among poor women with and without Jamkesmas and explore perceived barriers to health insurance membership and maternal health service utilization. METHODS: We used a mixed-methods design. Utilizing data from the 2012 Indonesian Demographic and Health Survey (n = 45,607), secondary analysis using propensity score matching was performed on key outcomes of interest: health facility delivery (HFD) and skilled birth delivery (SBD). In-depth interviews (n = 51) were conducted in the provinces of Jakarta and Banten among poor women, midwives, and government representatives. Thematic framework analysis was performed on qualitative data to explore perceived barriers. RESULTS: In 2012, 63.0% of women did not have health insurance; 19.1% had Jamkesmas. Poor women with Jamkesmas were 19% (OR = 1.19 [1.03–1.37]) more likely to have HFD and 17% (OR = 1.17 [1.01–1.35]) more likely to have SBD compared to poor women without insurance. Qualitative interviews highlighted key issues, including: lack of proper documentation for health insurance registration; the preference of pregnant women to deliver in their parents’ village; the use of traditional birth attendants; distance to health facilities; shortage of qualified health providers; overcrowded health facilities; and lack of health facility accreditation. CONCLUSION: Poor women with Jamkesmas membership had a modest increase in HFD and SBD. These findings are consistent with economic theory that health insurance coverage can reduce financial barriers to care and increase service uptake. However, factors such as socio-cultural beliefs, accessibility, and quality of care are important elements that need to be addressed as part of the national UHC agenda to improve maternal health services in Indonesia."
MATTHEW FOX,Mobility and Clinic Switching Among Postpartum Women Considered Lost to HIV Care in South Africa.,"OBJECTIVE: Retention in HIV care, particularly among postpartum women, is a challenge to national antiretroviral therapy programs. Retention estimates may be underestimated because of unreported transfers. We explored mobility and clinic switching among patients considered lost to follow-up (LTFU). DESIGN: Observational cohort study. METHODS: Of 788 women initiating antiretroviral therapy during pregnancy at 6 public clinics in Johannesburg, South Africa, 300 (38.1%) were LTFU (no visit ≥3 months). We manually searched for these women in the South African National Health Laboratory Services database to assess continuity of HIV care. We used geographic information system tools to map mobility to new facilities. RESULTS: Over one-third (37.6%) of women showed evidence of continued HIV care after LTFU. Of these, 67.0% continued care in the same province as the origin clinic. Compared with those who traveled outside of the province for care, these same-province ""clinic shoppers"" stayed out-of-care longer {median 373 days [interquartile range (IQR): 175-790] vs. 175.5 days (IQR: 74-371)} and had a lower CD4 cell count on re-entry [median 327 cells/μL (IQR: 196-576) vs. 493 cells/μL (IQR: 213-557). When considering all women with additional evidence of care as engaged in care, cohort LTFU dropped from 38.1% to 25.0%. CONCLUSION: We found evidence of continued care after LTFU and identified local and national clinic mobility among postpartum women. Laboratory records do not show all clinic visits and manual matching may have been under- or overestimated. A national health database linked to a unique identifier is necessary to improve reporting and patient care among highly mobile populations."
MATTHEW FOX,The Framingham Heart Study 100K SNP Genome-Wide Association Study Resource: Overview of 17 Phenotype Working Group Reports,"BACKGROUND: The Framingham Heart Study (FHS), founded in 1948 to examine the epidemiology of cardiovascular disease, is among the most comprehensively characterized multi-generational studies in the world. Many collected phenotypes have substantial genetic contributors; yet most genetic determinants remain to be identified. Using single nucleotide polymorphisms (SNPs) from a 100K genome-wide scan, we examine the associations of common polymorphisms with phenotypic variation in this community-based cohort and provide a full-disclosure, web-based resource of results for future replication studies. METHODS: Adult participants (n = 1345) of the largest 310 pedigrees in the FHS, many biologically related, were genotyped with the 100K Affymetrix GeneChip. These genotypes were used to assess their contribution to 987 phenotypes collected in FHS over 56 years of follow up, including: cardiovascular risk factors and biomarkers; subclinical and clinical cardiovascular disease; cancer and longevity traits; and traits in pulmonary, sleep, neurology, renal, and bone domains. We conducted genome-wide variance components linkage and population-based and family-based association tests. RESULTS: The participants were white of European descent and from the FHS Original and Offspring Cohorts (examination 1 Offspring mean age 32 ± 9 years, 54% women). This overview summarizes the methods, selected findings and limitations of the results presented in the accompanying series of 17 manuscripts. The presented association results are based on 70,897 autosomal SNPs meeting the following criteria: minor allele frequency ≥ 10%, genotype call rate ≥ 80%, Hardy-Weinberg equilibrium p-value ≥ 0.001, and satisfying Mendelian consistency. Linkage analyses are based on 11,200 SNPs and short-tandem repeats. Results of phenotype-genotype linkages and associations for all autosomal SNPs are posted on the NCBI dbGaP website at. CONCLUSION: We have created a full-disclosure resource of results, posted on the dbGaP website, from a genome-wide association study in the FHS. Because we used three analytical approaches to examine the association and linkage of 987 phenotypes with thousands of SNPs, our results must be considered hypothesis-generating and need to be replicated. Results from the FHS 100K project with NCBI web posting provides a resource for investigators to identify high priority findings for replication."
MATTHEW FOX,Defining retention and attrition in pre-antiretroviral HIV care: proposals based on experience in Africa,"Fewer than 33% of those testing HIV-positive in sub-Saharan Africa are continuously retained in pre-antiretroviral therapy (ART) HIV care until ART initiation. Existing evidence is difficult to synthesize, however, due to unclear and inconsistent definitions of terms. We developed practical, standardized definitions for reporting retention for the three stages of pre-ART care: Stage 1, testing HIV-positive to initial ART eligibility assessment; Stage 2, initial assessment to ART eligibility; and Stage 3, ART eligibility to ART initiation. For each stage, negative outcomes include death, loss, or not being retained. Stage 1 retention is defined as the proportion of patients who complete initial ART eligibility assessment within 3 months of HIV testing, with reporting of cohort outcomes at 3 and 12 months after HIV testing. Patients who end Stage 1 eligible for ART move directly to Stage 3. Stage 2 retention is defined as the proportion of patients who either complete all possible ART eligibility re-assessments within 6 months of the site’s standard visit schedule or had an assessment within 1 year of the time reported to and were not ART eligible at the last assessment. Retention should be reported at 12-month intervals. Stage 3 retention is defined as the proportion of patients eligible for ART who initiate ART (i.e.ARVs dispensed) within 3 months of determining ART eligibility, with reporting at 3 months after eligibility and 3 monthly intervals thereafter. If pre-ART retention is to improve, consistent terminology is needed for collecting data, measuring and reporting outcomes, and comparing results across programs and countries. The definitions we propose offer a strategy for improving the consistency and comparability of future reports."
MATTHEW FOX,The Impact of AIDS on Government Service Delivery: The Case of the Zambia Wildlife Authority,"Background: The loss of working-aged adults to HIV/AIDS has been shown to increase the costs of labor to the private sector in Africa. There is little corresponding evidence for the public sector. This study evaluated the impact of AIDS on the capacity of a government agency, the Zambia Wildlife Authority (ZAWA), to patrol Zambia’s national parks. Methods: Data were collected from ZAWA on workforce characteristics, recent mortality, costs, and the number of days spent on patrol between 2003 and 2005 by a sample of 76 current patrol officers (reference subjects) and 11 patrol officers who died of AIDS or suspected AIDS (index subjects). An estimate was made of the impact of AIDS on service delivery capacity and labor costs and the potential net benefits of providing treatment. Results: Reference subjects spent an average of 197.4 days on patrol per year. After adjusting for age, years of service, and worksite, index subjects spent 62.8 days on patrol in their last year of service (68% decrease, p<0.0001), 96.8 days on patrol in their second to last year of service (51% decrease, p<0.0001), and 123.7 days on patrol in their third to last year of service (37% decrease, p<0.0001). For each employee who died, ZAWA lost an additional 111 person-days for management, funeral attendance, vacancy, and recruitment and training of a replacement, resulting in a total productivity loss per death of 2.0 person-years. Each AIDS-related death also imposed budgetary costs for care, benefits, recruitment, and training equivalent to 3.3 years’ annual compensation. In 2005, AIDS reduced service delivery capacity by 6.2% and increased labor costs by 9.7%. If antiretroviral therapy could be provided for $500/patient/year, net savings to ZAWA would approach $285,000/year. Conclusion: AIDS is constraining ZAWA’s ability to protect Zambia’s wildlife and parks. Impacts on this government agency are substantially larger than have been observed in the private sector. Provision of ART would result in net budgetary savings to ZAWA and greatly increase its service delivery capacity."
MATTHEW FOX,Patient Retention in Antiretroviral Therapy Programs in Sub-Saharan Africa: A Systematic Review,"Background Long-term retention of patients in Africa’s rapidly expanding antiretroviral therapy (ART) programs for HIV/AIDS is essential for these programs’ success but has received relatively little attention. In this paper we present a systematic review of patient retention in ART programs in sub-Saharan Africa. Methods and Findings We searched Medline, other literature databases, conference abstracts, publications archives, and the ‘‘gray literature’’ (project reports available online) between 2000 and 2007 for reports on the proportion of adult patients retained (i.e., remaining in care and on ART) after 6 mo or longer in sub-Saharan African, non-research ART programs, with and without donor support. Estimated retention rates at 6, 12, and 24 mo were calculated and plotted for each program. Retention was also estimated using Kaplan-Meier curves. In sensitivity analyses we considered best-case, worst-case, and midpoint scenarios for retention at 2 y; the best-case scenario assumed no further attrition beyond that reported, while the worst-case scenario assumed that attrition would continue in a linear fashion. We reviewed 32 publications reporting on 33 patient cohorts (74,192 patients, 13 countries). For all studies, the weighted average follow-up period reported was 9.9 mo, after which 77.5% of patients were retained. Loss to follow-up and death accounted for 56% and 40% of attrition, respectively. Weighted mean retention rates as reported were 79.1%, 75.0% and 61.6 % at 6, 12, and 24 mo, respectively. Of those reporting 24 mo of follow-up, the best program retained 85% of patients and the worst retained 46%. Attrition was higher in studies with shorter reporting periods, leading to monthly weighted mean attrition rates of 3.3%/mo, 1.9%/mo, and 1.6%/month for studies reporting to 6, 12, and 24 months, respectively, and suggesting that overall patient retention may be overestimated in the published reports. In sensitivity analyses, estimated retention rates ranged from 24% in the worse case to 77% in the best case at the end of 2 y, with a plausible midpoint scenario of 50%. Conclusions Since the inception of large-scale ART access early in this decade, ART programs in Africa have retained about 60% of their patients at the end of 2 y. Loss to follow-up is the major cause of attrition, followed by death. Better patient tracing procedures, better understanding of loss to follow-up, and earlier initiation of ART to reduce mortality are needed if retention is to be improved. Retention varies widely across programs, and programs that have achieved higher retention rates can serve as models for future improvements."
MATTHEW FOX,Imputing HIV treatment start dates from routine laboratory data in South Africa: a validation study.,"BACKGROUND: Poor clinical record keeping hinders health systems monitoring and patient care in many low resource settings. We develop and validate a novel method to impute dates of antiretroviral treatment (ART) initiation from routine laboratory data in South Africa's public sector HIV program. This method will enable monitoring of the national ART program using real-time laboratory data, avoiding the error potential of chart review. METHODS: We developed an algorithm to impute ART start dates based on the date of a patient's ""ART workup"", i.e. the laboratory tests used to determine treatment readiness in national guidelines, and the time from ART workup to initiation based on clinical protocols (21 days). To validate the algorithm, we analyzed data from two large clinical HIV cohorts: Hlabisa HIV Treatment and Care Programme in rural KwaZulu-Natal; and Right to Care Cohort in urban Gauteng. Both cohorts contain known ART initiation dates and laboratory results imported directly from the National Health Laboratory Service. We assessed median time from ART workup to ART initiation and calculated sensitivity (SE), specificity (SP), positive predictive value (PPV), and negative predictive value (NPV) of our imputed start date vs. the true start date within a 6 month window. Heterogeneity was assessed across individual clinics and over time. RESULTS: We analyzed data from over 80,000 HIV-positive adults. Among patients who had a workup and initiated ART, median time to initiation was 16 days (IQR 7,31) in Hlabisa and 21 (IQR 8,43) in RTC cohort. Among patients with known ART start dates, SE of the imputed start date was 83% in Hlabisa and 88% in RTC, indicating this method accurately predicts ART start dates for about 85% of all ART initiators. In Hlabisa, PPV was 95%, indicating that for patients with a lab workup, true start dates were predicted with high accuracy. SP (100%) and NPV (92%) were also very high. CONCLUSIONS: Routine laboratory data can be used to infer ART initiation dates in South Africa's public sector. Where care is provided based on protocols, laboratory data can be used to monitor health systems performance and improve accuracy and completeness of clinical records."
MATTHEW FOX,Retention on antiretroviral therapy in low- and middle-income countries: systematic review of papers and abstracts since 2008,
MATTHEW FOX,Effect of eliminating CD4-count thresholds on HIV treatment initiation in South Africa: An empirical modeling study,"BACKGROUND: The World Health Organization recommends initiating antiretroviral therapy (ART) regardless of CD4 count. We assessed the effect of ART eligibility on treatment uptake and simulated the impact of WHO’s recommendations in South Africa. METHODS: We conducted an empirical analysis of cohort data using a regression discontinuity design, and then used this model for policy simulation. We enrolled all patients (n = 19,279) diagnosed with HIV between August 2011 and December 2013 in the Hlabisa HIV Treatment and Care Programme in rural South Africa. Patients were ART-eligible with CD4<350 cells/mm3 or Stage III/IV illness. We estimated: (1) distribution of first CD4 counts in 2013; (2) probability of initiating ART ≤6 months of HIV diagnosis under existing criteria at each CD4 count; (3) probability of initiating ART by CD4 count if thresholds were eliminated; and (4) number of expected new initiators if South Africa eliminates thresholds. FINDINGS: In 2013, 38.9% of patients diagnosed had a CD4 count ≥500. 8.0% of these patients initiated even without eligible CD4 counts. If CD4 criteria were eliminated, we project that an additional 19.2% of patients with CD4 ≥500 would initiate ART; 72.8% would not initiate ART despite being eligible. Eliminating CD4 criteria would increase the number starting ART by 26.7%. If these numbers hold nationally, this would represent an additional 164,000 initiators per year, a 5.2% increase in patients receiving ART and 5.3% increase in programme costs. CONCLUSIONS: Removing CD4 criteria alone will modestly increase timely uptake of ART. However, our results suggest the majority of newly-eligible patients will not initiate. Improved testing, linkage, and initiation procedures are needed to achieve 90-90-90 targets."
MATTHEW FOX,The Framingham Heart Study 100K SNP Genome-Wide Association Study Resource: Overview of 17 Phenotype Working Group Reports,"BACKGROUND: The Framingham Heart Study (FHS), founded in 1948 to examine the epidemiology of cardiovascular disease, is among the most comprehensively characterized multi-generational studies in the world. Many collected phenotypes have substantial genetic contributors; yet most genetic determinants remain to be identified. Using single nucleotide polymorphisms (SNPs) from a 100K genome-wide scan, we examine the associations of common polymorphisms with phenotypic variation in this community-based cohort and provide a full-disclosure, web-based resource of results for future replication studies. METHODS: Adult participants (n = 1345) of the largest 310 pedigrees in the FHS, many biologically related, were genotyped with the 100K Affymetrix GeneChip. These genotypes were used to assess their contribution to 987 phenotypes collected in FHS over 56 years of follow up, including: cardiovascular risk factors and biomarkers; subclinical and clinical cardiovascular disease; cancer and longevity traits; and traits in pulmonary, sleep, neurology, renal, and bone domains. We conducted genome-wide variance components linkage and population-based and family-based association tests. RESULTS: The participants were white of European descent and from the FHS Original and Offspring Cohorts (examination 1 Offspring mean age 32 ± 9 years, 54% women). This overview summarizes the methods, selected findings and limitations of the results presented in the accompanying series of 17 manuscripts. The presented association results are based on 70,897 autosomal SNPs meeting the following criteria: minor allele frequency ≥ 10%, genotype call rate ≥ 80%, Hardy-Weinberg equilibrium p-value ≥ 0.001, and satisfying Mendelian consistency. Linkage analyses are based on 11,200 SNPs and short-tandem repeats. Results of phenotype-genotype linkages and associations for all autosomal SNPs are posted on the NCBI dbGaP website at. CONCLUSION: We have created a full-disclosure resource of results, posted on the dbGaP website, from a genome-wide association study in the FHS. Because we used three analytical approaches to examine the association and linkage of 987 phenotypes with thousands of SNPs, our results must be considered hypothesis-generating and need to be replicated. Results from the FHS 100K project with NCBI web posting provides a resource for investigators to identify high priority findings for replication."
MATTHEW FOX,Do the Socioeconomic Impacts of Antiretroviral Therapy Vary by Gender? A Longitudinal Study of Kenyan Agricultural Worker Employment Outcomes,"BACKGROUND. As access to antiretroviral therapy (ART) has grown in Africa, attention has turned to evaluating the socio-economic impacts of ART. One key issue is the extent to which improvements in health resulting from ART allows individuals to return to work and earn income. Improvements in health from ART may also be associated with reduced impaired presenteeism, which is the loss of productivity when an ill or disabled individual attends work but accomplishes less at his or her usual tasks or shifts to other, possibly less valuable, tasks. METHODS. Longitudinal data for this analysis come from company payroll records for 97 HIV-infected tea estate workers (the index group, 56 women, 41 men) and a comparison group of all workers assigned to the same work teams (n = 2485, 1691 men, 794 women) for a 37-month period covering two years before and one year after initiating ART. We used nearest neighbour matching methods to estimate the impacts of HIV/AIDS and ART on three monthly employment outcomes for tea estate workers in Kenya – days plucking tea, days assigned to non-plucking assignments, and kilograms harvested when plucking. RESULTS. The female index group worked 30% fewer days plucking tea monthly than the matched female comparison group during the final 9 months pre-ART. They also worked 87% more days on non-plucking assignments. While the monthly gap between the two groups narrowed after beginning ART, the female index group worked 30% fewer days plucking tea and about 100% more days on non-plucking tasks than the comparison group after one year on ART. The male index group was able to maintain a similar pattern of work as their comparison group except during the initial five months on therapy. CONCLUSION. Significant impaired presenteeism continued to exist among the female index group after one year on ART. Future research needs to explore further the socio-economic implications of HIV-infected female workers on ART being less productive than the general female workforce over sustained periods of time."
MATTHEW FOX,Patient Retention in Antiretroviral Therapy Programs in Sub-Saharan Africa: A Systematic Review,"BACKGROUND. Long-term retention of patients in Africa's rapidly expanding antiretroviral therapy (ART) programs for HIV/AIDS is essential for these programs' success but has received relatively little attention. In this paper we present a systematic review of patient retention in ART programs in sub-Saharan Africa. METHODS AND FINDINGS. We searched Medline, other literature databases, conference abstracts, publications archives, and the ""gray literature"" (project reports available online) between 2000 and 2007 for reports on the proportion of adult patients retained (i.e., remaining in care and on ART) after 6 mo or longer in sub-Saharan African, non-research ART programs, with and without donor support. Estimated retention rates at 6, 12, and 24 mo were calculated and plotted for each program. Retention was also estimated using Kaplan-Meier curves. In sensitivity analyses we considered best-case, worst-case, and midpoint scenarios for retention at 2 y; the best-case scenario assumed no further attrition beyond that reported, while the worst-case scenario assumed that attrition would continue in a linear fashion. We reviewed 32 publications reporting on 33 patient cohorts (74,192 patients, 13 countries). For all studies, the weighted average follow-up period reported was 9.9 mo, after which 77.5% of patients were retained. Loss to follow-up and death accounted for 56% and 40% of attrition, respectively. Weighted mean retention rates as reported were 79.1%, 75.0% and 61.6% at 6, 12, and 24 mo, respectively. Of those reporting 24 mo of follow-up, the best program retained 85% of patients and the worst retained 46%. Attrition was higher in studies with shorter reporting periods, leading to monthly weighted mean attrition rates of 3.3%/mo, 1.9%/mo, and 1.6%/month for studies reporting to 6, 12, and 24 months, respectively, and suggesting that overall patient retention may be overestimated in the published reports. In sensitivity analyses, estimated retention rates ranged from 24% in the worse case to 77% in the best case at the end of 2 y, with a plausible midpoint scenario of 50%. CONCLUSIONS. Since the inception of large-scale ART access early in this decade, ART programs in Africa have retained about 60% of their patients at the end of 2 y. Loss to follow-up is the major cause of attrition, followed by death. Better patient tracing procedures, better understanding of loss to follow-up, and earlier initiation of ART to reduce mortality are needed if retention is to be improved. Retention varies widely across programs, and programs that have achieved higher retention rates can serve as models for future improvements. Almost half of people entering African HIV treatment programs were lost to follow-up or died within two years, according to this systematic review by Sydney Rosen and colleagues."
MATTHEW FOX,"Barriers to Initiation of Antiretroviral Treatment in Rural and Urban Areas of Zambia: A Cross-Sectional Study of Cost, Stigma, and Perceptions about ART","BACKGROUND While the number of HIV-positive patients on antiretroviral therapy (ART) in resource-limited settings has increased dramatically, some patients eligible for treatment do not initiate ART even when it is available to them. Understanding why patients opt out of care, or are unable to opt in, is important to achieving the goal of universal access. METHODS We conducted a cross-sectional survey among 400 patients on ART (those who were able to access care) and 400 patients accessing home-based care (HBC), but who had not initiated ART (either they were not able to, or chose not to, access care) in two rural and two urban sites in Zambia to identify barriers to and facilitators of ART uptake. RESULTS HBC patients were 50% more likely to report that it would be very difficult to get to the ART clinic than those on ART (RR: 1.48; 95% CI: 1.21-1.82). Stigma was common in all areas, with 54% of HBC patients, but only 15% of ART patients, being afraid to go to the clinic (RR: 3.61; 95% CI: 3.12-4.18). Cost barriers differed by location: urban HBC patients were three times more likely to report needing to pay to travel to the clinic than those on ART (RR: 2.84; 95% CI: 2.02-3.98) and 10 times more likely to believe they would need to pay a fee at the clinic (RR: 9.50; 95% CI: 2.24-40.3). In rural areas, HBC subjects were more likely to report needing to pay non-transport costs to attend the clinic than those on ART (RR: 4.52; 95% CI: 1.91-10.7). HBC patients were twice as likely as ART patients to report not having enough food to take ART being a concern (27% vs. 13%, RR: 2.03; 95% CI: 1.71-2.41), regardless of location and gender. CONCLUSIONS Patients in home-based care for HIV/AIDS who never initiated ART perceived greater financial and logistical barriers to seeking HIV care and had more negative perceptions about the benefits of the treatment. Future efforts to expand access to antiretroviral care should consider ways to reduce these barriers in order to encourage more of those medically eligible for antiretrovirals to initiate care."
MATTHEW FOX,The impact of AIDS on government service delivery: the case of the Zambia Wildlife Authority,"BACKGROUND: The loss of working-aged adults to HIV/AIDS has been shown to increase the costs of labor to the private sector in Africa. There is little corresponding evidence for the public sector. This study evaluated the impact of AIDS on the capacity of a government agency, the Zambia Wildlife Authority (ZAWA), to patrol Zambia’s national parks. METHODS: Data were collected from ZAWA on workforce characteristics, recent mortality, costs, and the number of days spent on patrol between 2003 and 2005 by a sample of 76 current patrol officers (reference subjects) and 11 patrol officers who died of AIDS or suspected AIDS (index subjects). An estimate was made of the impact of AIDS on service delivery capacity and labor costs and the potential net benefits of providing treatment. RESULTS: Reference subjects spent an average of 197.4 days on patrol per year. After adjusting for age, years of service, and worksite, index subjects spent 62.8 days on patrol in their last year of service (68% decrease, p<0.0001), 96.8 days on patrol in their second to last year of service (51% decrease, p<0.0001), and 123.7 days on patrol in their third to last year of service (37% decrease, p<0.0001). For each employee who died, ZAWA lost an additional 111 person-days for management, funeral attendance, vacancy, and recruitment and training of a replacement, resulting in a total productivity loss per death of 2.0 person-years. Each AIDS-related death also imposed budgetary costs for care, benefits, recruitment, and training equivalent to 3.3 years’ annual compensation. In 2005, AIDS reduced service delivery capacity by 6.2% and increased labor costs by 9.7%. If antiretroviral therapy could be provided for $500/patient/year, net savings to ZAWA would approach $285,000/year. CONCLUSION: AIDS is constraining ZAWA’s ability to protect Zambia’s wildlife and parks. Impacts on this government agency are substantially larger than have been observed in the private sector. Provision of ART would result in net budgetary savings to ZAWA and greatly increase its service delivery capacity."
MATTHEW FOX,Prospects for beyond the standard model physics searches at the deep underground neutrino experiment: DUNE collaboration,"The Deep Underground Neutrino Experiment (DUNE) will be a powerful tool for a variety of physics topics. The high-intensity proton beams provide a large neutrino flux, sampled by a near detector system consisting of a combination of capable precision detectors, and by the massive far detector system located deep underground. This configuration sets up DUNE as a machine for discovery, as it enables opportunities not only to perform precision neutrino measurements that may uncover deviations from the present three-flavor mixing paradigm, but also to discover new particles and unveil new interactions and symmetries beyond those predicted in the Standard Model (SM). Of the many potential beyond the Standard Model (BSM) topics DUNE will probe, this paper presents a selection of studies quantifying DUNE's sensitivities to sterile neutrino mixing, heavy neutral leptons, non-standard interactions, CPT symmetry violation, Lorentz invariance violation, neutrino trident production, dark matter from both beam induced and cosmogenic sources, baryon number violation, and other new physics topics that complement those at high-energy colliders and significantly extend the present reach."
MATTHEW FOX,The long-baseline neutrino experiment: exploring fundamental symmetries of the universe,"The preponderance of matter over antimatter in the early Universe, the dynamics of the supernova bursts that produced the heavy elements necessary for life and whether protons eventually decay --- these mysteries at the forefront of particle physics and astrophysics are key to understanding the early evolution of our Universe, its current state and its eventual fate. The Long-Baseline Neutrino Experiment (LBNE) represents an extensively developed plan for a world-class experiment dedicated to addressing these questions. LBNE is conceived around three central components: (1) a new, high-intensity neutrino source generated from a megawatt-class proton accelerator at Fermi National Accelerator Laboratory, (2) a near neutrino detector just downstream of the source, and (3) a massive liquid argon time-projection chamber deployed as a far detector deep underground at the Sanford Underground Research Facility. This facility, located at the site of the former Homestake Mine in Lead, South Dakota, is approximately 1,300 km from the neutrino source at Fermilab -- a distance (baseline) that delivers optimal sensitivity to neutrino charge-parity symmetry violation and mass ordering effects. This ambitious yet cost-effective design incorporates scalability and flexibility and can accommodate a variety of upgrades and contributions. With its exceptional combination of experimental configuration, technical capabilities, and potential for transformative discoveries, LBNE promises to be a vital facility for the field of particle physics worldwide, providing physicists from around the globe with opportunities to collaborate in a twenty to thirty year program of exciting science. In this document we provide a comprehensive overview of LBNE's scientific objectives, its place in the landscape of neutrino physics worldwide, the technologies it will incorporate and the capabilities it will possess."
MATTHEW FOX,Scintillation light detection in the 6-m drift-length ProtoDUNE Dual Phase liquid argon TPC,"DUNE is a dual-site experiment for long-baseline neutrino oscillation studies, neutrino astrophysics and nucleon decay searches. ProtoDUNE Dual Phase (DP) is a 6  ×  6  ×  6 m 3 liquid argon time-projection-chamber (LArTPC) that recorded cosmic-muon data at the CERN Neutrino Platform in 2019-2020 as a prototype of the DUNE Far Detector. Charged particles propagating through the LArTPC produce ionization and scintillation light. The scintillation light signal in these detectors can provide the trigger for non-beam events. In addition, it adds precise timing capabilities and improves the calorimetry measurements. In ProtoDUNE-DP, scintillation and electroluminescence light produced by cosmic muons in the LArTPC is collected by photomultiplier tubes placed up to 7 m away from the ionizing track. In this paper, the ProtoDUNE-DP photon detection system performance is evaluated with a particular focus on the different wavelength shifters, such as PEN and TPB, and the use of Xe-doped LAr, considering its future use in giant LArTPCs. The scintillation light production and propagation processes are analyzed and a comparison of simulation to data is performed, improving understanding of the liquid argon properties."
MATTHEW FOX,Treatment outcomes and costs of providing antiretroviral therapy at a primary health clinic versus a hospital-based HIV clinic in South Africa,"BACKGROUND:In 2010 South Africa revised its HIV treatment guidelines to allow the initiation and management of patients on antiretroviral therapy (ART) by nurses, rather than solely doctors, under a program called NIMART (Nurse Initiated and Managed Antiretroviral Therapy). We compared the outcomes and costs of NIMART between the two major public sector HIV treatment delivery models in use in South Africa today, primary health clinics and hospital-based HIV clinics. METHODS AND FINDINGS:The study was conducted at one hospital-based outpatient HIV clinic and one primary health clinic (PHC) in Gauteng Province. A retrospective cohort of adult patients initiated on ART at the PHC was propensity-score matched to patients initiated at the hospital outpatient clinic. Each patient was assigned a 12-month outcome of alive and in care or died/lost to follow up. Costs were estimated from the provider perspective for the 12 months after ART initiation. The proportion of patients alive and in care at 12 months did not differ between the PHC (76.5%) and the hospital-based site (74.2%). The average annual cost per patient alive and in care at 12 months after ART initiation was significantly lower at the PHC (US$238) than at the hospital outpatient clinic (US$428). CONCLUSIONS: Initiating and managing ART patients at PHCs under NIMART is producing equally good outcomes as hospital-based HIV clinic care at much lower cost. Evolution of hospital-based clinics into referral facilities that serve complicated patients, while investing most program expansion resources into PHCs, may be a preferred strategy for achieving treatment coverage targets."
MATTHEW FOX,Simplified clinical algorithm for identifying patints eligible for immediate initiation of antiretroviral therapy for HIV (SLATE): protocol for a randomised evaluation,"INTRODUCTION: African countries are rapidly adopting guidelines to offer antiretroviral therapy (ART) to all HIV-infected individuals, regardless of CD4 count. For this policy of 'treat all' to succeed, millions of new patients must be initiated on ART as efficiently as possible. Studies have documented high losses of treatment-eligible patients from care before they receive their first dose of antiretrovirals (ARVs), due in part to a cumbersome, resource-intensive process for treatment initiation, requiring multiple clinic visits over a several-week period. METHODS AND ANALYSIS: The Simplified Algorithm for Treatment Eligibility (SLATE) study is an individually randomised evaluation of a simplified clinical algorithm for clinicians to reliably determine a patient's eligibility for immediate ART initiation without waiting for laboratory results or additional clinic visits. SLATE will enroll and randomize (1:1) 960 adult, HIV-positive patients who present for HIV testing or care and are not yet on ART in South Africa and Kenya. Patients randomized to the standard arm will receive routine, standard of care ART initiation from clinic staff. Patients randomized to the intervention arm will be administered a symptom report, medical history, brief physical exam and readiness assessment. Patients who have positive (satisfactory) results for all four components of SLATE will be dispensed ARVs immediately, at the same clinic visit. Patients who have any negative results will be referred for further clinical investigation, counseling, tests or other services prior to being dispensed ARVs. After the initial visit, follow-up will be by passive medical record review. The primary outcomes will be ART initiation ≤28 days and retention in care 8 months after study enrollment. ETHICS AND DISSEMINATION: Ethics approval has been provided by the Boston University Institutional Review Board, the University of the Witwatersrand Human Research Ethics Committee (Medical) and the KEMRI Scientific and Ethics Review Unit. Results will be published in peer-reviewed journals and made widely available through presentations and briefing documents."
MATTHEW FOX,Economic Outcomes of Patients Receiving Antiretroviral Therapy for HIV/AIDS in South Africa Are Sustained through Three Years on Treatment,"BACKGROUND. Although the medical outcomes of antiretroviral therapy (ART) for HIV/AIDS are well described, less is known about how ART affects patients' economic activities and quality of life, especially after the first year on ART. We assessed symptom prevalence, general health, ability to perform normal activities, and employment status among adult antiretroviral therapy patients in South Africa over three full years following ART initiation. METHODOLOGY/PRINCIPAL FINDINGS. A cohort of 855 adult pre-ART patients and patients on ART for <6 months was enrolled and interviewed an average of 4.4 times each during routine clinic visits for up to three years after treatment initiation using an instrument designed for the study. The probability of pain in the previous week fell from 74% before ART initiation to 32% after three years on ART, fatigue from 66% to 12%, nausea from 28% to 4%, and skin problems from 55% to 10%. The probability of not feeling well physically yesterday fell from 46% to 23%. Before starting ART, 39% of subjects reported not being able to perform their normal activities sometime during the previous week; after three years, this proportion fell to 10%. Employment rose from 27% to 42% of the cohort. Improvement in all outcomes was sustained over 3 years and for some outcomes increased in the second and third year. CONCLUSIONS/SIGNIFICANCE. Improvements in adult ART patients' symptom prevalence, general health, ability to perform normal activities, and employment status were large and were sustained through the first three years on treatment. These results suggest that some of the positive economic and social externalities anticipated as a result of large-scale treatment provision, such as increases in workforce participation and productivity and the ability of patients to carry on normal lives, may indeed be accruing."
MATTHEW FOX,"Timing of pregnancy, postpartum risk of virologic failure and loss to follow-up among HIV-positive women.","BACKGROUND: We assessed the association between the timing of pregnancy with the risk of postpartum virologic failure and loss from HIV care in South Africa. METHODS: The incidence of virologic failure (two consecutive viral load measurements of >1000 copies/ml) and loss to follow-up (>3 months late for a visit) during 24 months postpartum were assessed using Cox proportional hazards modelling. RESULTS: The rate of postpartum virologic failure was higher following an incident pregnancy on ART [adjusted hazard ratio 1.8, 95% confidence interval (CI): 1.1-2.7] than among women who initiated ART during pregnancy. This difference was sustained among women with CD4 cell count less than 350 cells/μl at delivery (adjusted hazard ratio 1.8, 95% CI: 1.1-3.0). Predictors of postpartum virologic failure were being viremic, longer time on ART, being 25 or less years old and low CD4 cell count and anaemia at delivery, as well as initiating ART on stavudine-containing or abacavir-containing regimen. There was no difference postpartum loss to follow-up rates between the incident pregnancies group (hazard ratio 0.9, 95% CI: 0.7-1.1) and those who initiated ART in pregnancy. CONCLUSION: The risk of virologic failure remains high among postpartum women, particularly those who conceive on ART. The results highlight the need to provide adequate support for HIV-positive women with fertility intention after ART initiation and to strengthen monitoring and retention efforts for postpartum women to sustain the benefits of ART."
MATTHEW FOX,The Importance of Clinic Attendance in the First Six Months on Antiretroviral Treatment: A Retrospective Analysis at a Large Public Sector HIV Clinic in South Africa,"BACKGROUND Adherence to care and treatment are essential for HIV-infected individuals to benefit from antiretroviral therapy (ART). We sought to quantify the effects on treatment outcomes of missing visits soon after initiating ART. METHODS We analyzed data from HIV-infected patients initiating ART at Themba Lethu Clinic, Johannesburg, South Africa, from April 2004 to August 2008. We used log-binomial regression to evaluate the relative risk of missing visits during the first six months of ART on immunological response and virologic suppression. Cox models were used to evaluate the relationship between missed visits and mortality and loss to follow up over 12 months. RESULTS Of 4476 patients, 65% missed no visits, while 26% missed one visit, 7% missed two and 1.6% missed three or more visits during the first six months on treatment. Patients who missed three or more medical or antiretroviral (ARV) visits had a two-fold increased risk of poor CD4 response by six months, while the risk of failing to achieve virologic suppression by six months increased two- to five-fold among patients who missed two and three or more medical or ARV visits. Adjusted Cox models showed that patients who missed two (HR 2.1; 95% CI: 1.0-4.3) and three or more (HR 4.7; 95% CI: 1.4-16.2) medical visits had an increased risk of death, while those who missed two ARV (HR 3.8; 95% CI: 2.5-5.8) or three or more medical (HR 3.0; 95% CI: 1.1-8.1) visits had an increased risk of loss to follow up. CONCLUSIONS Thirty-five percent of patients missed one or more visits in the first six months on treatment, increasing their risk of poorer outcomes. These patients could be targeted for additional adherence counselling to help improve ART outcomes."
MATTHEW FOX,"Cohort profile: the Right to Care Clinical HIV Cohort, South Africa","PURPOSE: The research objectives of the Right to Care Clinical HIV Cohort analyses are to: (1) monitor treatment outcomes (including death, loss to follow-up, viral suppression and CD4 count gain among others) for patients on antiretroviral therapy (ART); (2) evaluate the impact of changes in the national treatment guidelines around when to initiate ART on HIV treatment outcomes; (3) evaluate the impact of changes in the national treatment guidelines around what ART regimens to initiate on drug switches; (4) evaluate the cost and cost-effectiveness of HIV treatment delivery models; (5) evaluate the need for and outcomes on second-line and third-line ART; (6) evaluate the impact of comorbidity with non-communicable diseases on HIV treatment outcomes and (7) evaluate the impact of the switch to initiating all patients onto ART regardless of CD4 count. PARTICIPANTS: The Right to Care Clinical HIV Cohort is an open cohort of data from 10 clinics in two provinces within South Africa. All clinics include data from 2004 onwards. The cohort currently has data on over 115 000 patients initiated on HIV treatment and patients are followed up every 3–6 months for clinical and laboratory monitoring. FINDINGS TO DATE: Cohort data includes information on demographics, clinical visit, laboratory data, medication history and clinical diagnoses. The data have been used to identify rates and predictors of first-line failure, to identify predictors of mortality for patients on second-line (eg, low CD4 counts) and to show that adolescents and young adults are at increased risk of unsuppressed viral loads compared with adults. FUTURE PLANS: Future analyses will inform national models of HIV care and treatment to improve HIV care policy in South Africa."
MATTHEW FOX,Changing the South African national antiretroviral therapy guidelines: The role of cost modelling,"Background We were tasked by the South African Department of Health to assess the cost implications to the largest ART programme in the world of adopting sets of ART guidelines issued by the World Health Organization between 2010 to 2016. Methods Using data from large South African ART clinics (n = 24,244 patients), projections of patients in need of ART, and cost data from bottom-up cost analyses, we constructed a population-level health-state transition model with 6-monthly transitions between health states depending on patients’ age, CD4 cell count/ percentage, and, for adult first-line ART, time on treatment. Findings For each set of guidelines, the modelled increase in patient numbers as a result of prevalence and uptake was substantially more than the increase resulting from additional eligibility. Under each set of guidelines, the number of people on ART was projected to increase by 31-133% over the next seven years, and cost by 84-175%, while increased eligibility led to 1-26% more patients, and 1-17% higher cost. The projected increases in treatment cost due to the 2010 and the 2015 WHO guidelines could be offset in their entirety by the introduction of cost-saving measures such as opening the drug tenders for international competition and task-shifting. Under universal treatment, annual costs of the treatment programme will decrease for the first time from 2024 onwards. Conclusions Annual budgetary requirements for ART will continue to increase in South Africa until universal treatment is taken to full scale. Model results were instrumental in changing South African ART guidelines, more than tripling the population on treatment between 2009 and 2017, and reducing the per-patient cost of treatment by 64%."
KEITH MARZILLI ERICSON,Team formation and performance: evidence from healthcare referral networks,"We examine the teams that emerge when a primary care physician (PCP) refers patients to specialists. When PCPs concentrate their specialist referrals — for instance, sending their cardiology patients to fewer distinct cardiologists — this encourages repeat interactions between PCPs and specialists. Repeated interactions provide more opportunities and incentives to develop productive team relationships. Using data from the Massachusetts All Payer Claims Database, we construct a new measure of PCP team referral concentration and document that it varies widely across PCPs, even among PCPs in the same organization. Chronically ill patients treated by PCPs with 1 standard deviation higher team referral concentration have 4% lower health care utilization on average, with no discernible reduction in quality. We corroborate this finding using a national sample of Medicare claims, and show that it holds under various identification strategies that account for observed and unobserved patient and physician characteristics. The results suggest that repeated PCP-specialist interactions improve team performance."
DAVID STAROBINSKI,A game-theoretic analysis of shared/buy-in computing systems,"High performance computing clusters are increasingly operating under a shared/buy-in paradigm. Under this paradigm, users choose between two tiers of services: shared services and buy-in services. Shared services provide users with access to shared resources for free, while buy-in services allow users to purchase additional buy-in resources in order to shorten job completion time. An important feature of shared/buy-in computing systems consists of making unused buy-in resources available to all other users of the system. Such a feature has been shown to enhance the utilization of resources. Alongside, it creates strategic interactions among users, hence giving rise to a non-cooperative game at the system level. Specifically, each user is faced with the questions of whether to purchase buy-in resources, and if so, how much to pay for them. Under quite general conditions, we establish that a shared/buy-in computing game yields a unique Nash equilibrium, which can be computed in polynomial time. We provide an algorithm for this purpose, which can be implemented in a distributed manner. Moreover, by establishing a connection to the theory of aggregative games, we prove that the game converges to the Nash equilibrium through best response dynamics from any initial state. We justify the underlying game-theoretic assumptions of our model using real data from a computing cluster, and conduct numerical simulations to further explore convergence properties and the influence of system parameters on the Nash equilibrium. In particular, we point out potential unfairness and abuse issues and discuss solution venues."
DAVID STAROBINSKI,Priority-Based Synchronization of Distributed Data,"We consider the general problem of synchronizing the data on two devices using a minimum amount of communication, a core infrastructural requirement for a large variety of distributed systems. Our approach considers the interactive synchronization of prioritized data, where, for example, certain information is more time-sensitive than other information. We propose and analyze a new scheme for efficient priority-based synchronization, which promises benefits over conventional synchronization."
DAVID STAROBINSKI,Equilibrium and learning in queues with advance reservations.,"Consider a multi-class preemptive-resume M/D/1 queueing system that supports advance reservations (AR). In this system, strategic customers must decide whether to reserve a server in advance (thereby gaining higher priority) or avoid AR. Reserving a server in advance bears a cost. In this paper, we conduct a game-theoretic analysis of this system, characterizing the equilibrium strategies. Specifically, we show that the game has two types of equilibria. In one type, none of the customers makes reservation. In the other type, only customers that realize early enough that they will need service make reservations. We show that the types and number of equilibria depend on the parameters of the queue and on the reservation cost. Specifically, we prove that the equilibrium is unique if the server utilization is below 1/2. Otherwise, there may be multiple equilibria depending on the reservation cost. Next, we assume that the reservation cost is a fee set by the provider. In that case, we show that the revenue maximizing fee leads to a unique equilibrium if the utilization is below 2/3, but multiple equilibria if the utilization exceeds 2/3. Finally, we study a dynamic version of the game, where users learn and adapt their strategies based on observations of past actions or strategies of other users. Depending on the type of learning (i.e., action learning vs. strategy learning), we show that the game converges to an equilibrium in some cases, while it cycles in other cases."
DAVID STAROBINSKI,Game theoretic analysis of Citizens Broadband Radio Service,"The Citizens Broadband Radio Service (CBRS) is a spectrum sharing framework on the 3.5 GHz tier with three priority tiers: the incumbents, priority commercial users (PAL), and general commercial users (GAA). Thus, commercial users compete for resources within the second and third priority tiers. The interaction between commercial providers and customers is complicated by the presence of the incumbents, who impact the availability of spectrum but bypass the market entirely. In particular, PAL customers are themselves subject to preemption even with the priority purchase. In this paper, we propose a game-theoretic framework to shed light into the equilibrium outcomes and the impact of the incumbents into these. We determine that there exist several possible equilibrium regions, including one with a unique mixed equilibrium which is stable in the evolutionary stable strategy sense, and others featuring unstable mixed equilibria and stable pure equilibria. We show that for fixed parameters, the maximum possible revenue a provider can obtain is associated with a stable equilibrium and is thus guaranteed. However, changes in incumbent behavior can result in phase changes which have a sizable impact on the maximum potential revenue."
DAVID STAROBINSKI,A general security approach for soft-information decoding against smart bursty jammers,"Malicious attacks such as jamming can cause significant disruption or complete denial of service (DoS) to wireless communication protocols. Moreover, jamming devices are getting smarter, making them difficult to detect. Forward error correction, which adds redundancy to data, is commonly deployed to protect communications against the deleterious effects of channel noise. Soft-information error correction decoders obtain reliability information from the receiver to inform their decoding, but in the presence of a jammer such information is misleading and results in degraded error correction performance. As decoders assume noise occurs independently to each bit, a bursty jammer will lead to greater degradation in performance than a non-bursty one. Here we establish, however, that such temporal dependencies can aid inferences on which bits have been subjected to jamming, thus enabling counter-measures. In particular, we introduce a pre-decoding processing step that updates log-likelihood ratio (LLR) reliability information to reflect inferences in the presence of a jammer, enabling improved decoding performance for any soft detection decoder. The proposed method requires no alteration to the decoding algorithm. Simulation results show that the method correctly infers a significant proportion of jamming in any received frame. Results with one particular decoding algorithm, the recently introduced ORBGRAND, show that the proposed method reduces the block-error rate (BLER) by an order of magnitude for a selection of codes, and prevents complete DoS at the receiver."
DAVID STAROBINSKI,Advance reservation games,"Advance reservation (AR) services form a pillar of several branches of the economy, including transportation, lodging, dining, and, more recently, cloud computing. In this work, we use game theory to analyze a slotted AR system in which customers differ in their lead times. For each given time slot, the number of customers requesting service is a random variable following a general probability distribution. Based on statistical information, the customers decide whether or not to make an advance reservation of server resources in future slots for a fee. We prove that only two types of equilibria are possible: either none of the customers makes AR or only customers with lead time greater than some threshold make AR. Our analysis further shows that the fee that maximizes the provider’s profit may lead to other equilibria, one of which yields zero profit. In order to prevent ending up with no profit, the provider can elect to advertise a lower fee yielding a guaranteed but smaller profit. We refer to the ratio of the maximum possible profit to the maximum guaranteed profit as the price of conservatism. When the number of customers is a Poisson random variable, we prove that the price of conservatism is one in the single-server case, but can be arbitrarily high in a many-server system."
DAVID STAROBINSKI,Brief announcement: passive and active attacks on audience response systems using software defined radios,"Audience response systems, also known as clickers, are used at many academic institutions to offer active learning environments. Since these systems are used to administer graded assignments, and sometimes even exams, it is crucial to assess their security. Our work seeks to exploit and document potential vulnerabilities of clickers. For this purpose, we use software defined radios to perform jamming, sniffing and spoofing attacks on an audience response system in production, which provide different possible methods of cheating. The results of our study demonstrate that clickers are easily exploitable. We build a prototype and show that it is practically possible to covertly steal or forge answers of a peer or even an entire classroom, with high levels of confidence. Additionally, we find that the receivers software of the system lacks protection against unexpected answers, which allows our spoofer to submit any ASCII character and opens the receiver up to possible fuzzing attacks. As a result of this study, we discourage using clickers for high-stake assessments, unless they provide proper security protection.."
DAVID STAROBINSKI,Automated exposure notification for COVID-19,"In the current COVID-19 pandemic, various Automated Exposure Notification (AEN) systems have been proposed to help quickly identify potential contacts of infected individuals. All these systems try to leverage the current understanding of the following factors: transmission risk, technology to address risk modeling, system policies and privacy considerations. While AEN holds promise for mitigating the spread of COVID-19, using short-range communication channels (Bluetooth) in smartphones to detect close individual contacts may be inaccurate for modeling and informing transmission risk. This work finds that the current close contact definitions may be inadequate to reduce viral spread using AEN technology. Consequently, relying on distance measurements from Bluetooth Low-Energy may not be optimal for determining risks of exposure and protecting privacy. This paper's literature analysis suggests that AEN may perform better by using broadly accessible technologies to sense the respiratory activity, mask status, or environment of participants. Moreover, the paper remains cognizant that smartphone sensors can leak private information and thus recommends additional objectives for maintaining user privacy without compromising utility for population health. This literature review and analysis will simultaneously interest (i) health professionals who desire a fundamental understanding of the design and utility of AEN systems and (ii) technologists interested in understanding their epidemiological basis in the light of recent research. Ultimately, the two disparate communities need to understand each other to assess the value of AEN systems in mitigating viral spread, whether for the COVID-19 pandemic or for future ones."
DAVID STAROBINSKI,Live demonstration: cyber attack against an ingestible medical device,"Intelligent and compact healthcare systems are gaining interest, potentially changing medical monitoring and treatment procedures. Ingestible medical devices (IMD) inside a swallowable pill can transform unpleasant and immobile operations like endoscopy into a remote process. These devices raise a concern for security, where its absence can result in a lethal attack [1] . Some attacks have been shown on medical devices, such as insulin pumps and cardiac defibrillators [2] . A typical challenge for securing an IMD is its resource-constrained design. IMDs have to be small in size to make them swallowable, which limits the battery size. This obliges the device to run on ultra-low power, targeting hours of measurement and data transmission on a small battery. Considering that, it is generally not feasible to have calculation-intensive encryption or a separate cryptography core on the device. However, the security of an IMD is crucial as a breach can cause leakage of confidential data or a wrong diagnosis."
DAVID STAROBINSKI,Uncovering product vulnerabilities with threat knowledge graphs,"Threat modeling and security assessment rely on public information on products, vulnerabilities and weaknesses. So far, databases in these categories have rarely been analyzed in combination. Yet, doing so could help predict unreported vulnerabilities and identify common threat patterns. In this paper, we propose a methodology for producing and optimizing a knowledge graph that aggregates knowledge from common threat databases (CPE, CVE, and CWE). We apply the threat knowledge graph to predict associations between threat databases, specifically between products and vulnerabilities. We evaluate the prediction performance based on historical data, using precision, recall, and F1-score metrics. We demonstrate the ability of the threat knowledge graph to uncover many associations that are currently unknown but will be revealed in the future."
DAVID STAROBINSKI,Empirical comparison of block relay protocols,"Block relay protocols play a key role in the performance and security of public blockchains. As a result, several such protocols have been deployed in the context of Bitcoin and its variants (e.g., legacy, compact block relay and Graphene) in an attempt to reduce bandwidth utilization. However, the relative performance of these protocols in realistic networking conditions (e.g., with nodes churning - joining and leaving the network) is still not known. This paper aims to fill this key knowledge gap using an experimental testbed of twelve full nodes connected to the Bitcoin Cash blockchain. With the aid of novel logging tools, we contrast the performance of these three protocols, in realistic scenarios, with respect to communication, delay, and block decoding success. Our main findings are that Graphene generally performs the best when nodes remain connected, boasting an average propagation delay of 190 ms (i.e., 29% lower than compact block and 80% lower than the legacy protocol). However, when nodes churn at a high rate, compact blocks may perform better. Through a careful temporal analysis, we identify some root causes of the protocol inefficiencies, together with potential mitigation. We have made our measurement framework and experimental logs publicly available to the broader research community."
DAVID STAROBINSKI,Multi-protocol IoT network reconnaissance,"Network reconnaissance is a core security functionality, which can be used to detect hidden unauthorized devices or to identify missing devices. Currently, there is a lack of network reconnaissance tools capable of discovering Internet of Things (IoT) devices across multiple protocols. To bridge this gap, we introduce IoT-Scan, an extensible IoT network reconnaissance tool. IoT - Scan is based on software-defined radio (SDR) technology, which allows for a flexible implementation of radio protocols. We propose passive, active, multi-channel, and multi-protocol scanning algorithms to speed up the discovery of devices with IoT-Scan. We implement the scanning algorithms and compare their performance with four popular IoT protocols: Zigbee, Bluetooth LE, Z-Wave, and LoRa. Through experiments with dozens of IoT devices, we demonstrate that our implementation experiences minimal packet losses, and achieves performance near a theoretical benchmark."
DAVID STAROBINSKI,Optimizing freshness in IoT scans,"Motivated by IoT security monitoring applications, we consider the problem of a wireless monitor that must implement a multi-channel scanning policy to minimize the Age of Information (AoI) of received information. We model this problem as a Markov Decision Process (MDP). To address the curse of dimensionality, we propose various scanning policies of low computational complexity. We compare the performance of these policies against the optimal one in small instances, and further simulate them using time-series data obtained from real IoT device communication traces. We show that a policy, coined Greedy Expected Area (GEA), performs well in many scenarios."
CHRISTOS G CASSANDRAS,Final report: Workshop on: Integrating electric mobility systems with the grid infrastructure,"EXECUTIVE SUMMARY: This document is a report on the workshop entitled “Integrating Electric Mobility Systems with the Grid Infrastructure” which was held at Boston University on November 6-7 with the sponsorship of the Sloan Foundation. Its objective was to bring together researchers and technical leaders from academia, industry, and government in order to set a short and longterm research agenda regarding the future of mobility and the ability of electric utilities to meet the needs of a highway transportation system powered primarily by electricity. The report is a summary of their insights based on workshop presentations and discussions. The list of participants and detailed Workshop program are provided in Appendices 1 and 2. Public and private decisions made in the coming decade will direct profound changes in the way people and goods are moved and the ability of clean energy sources – primarily delivered in the form of electricity – to power these new systems. Decisions need to be made quickly because of rapid advances in technology, and the growing recognition that meeting climate goals requires rapid and dramatic action. The blunt fact is, however, that the pace of innovation, and the range of business models that can be built around these innovations, has grown at a rate that has outstripped our ability to clearly understand the choices that must be made or estimate the consequences of these choices. The group of people assembled for this Workshop are uniquely qualified to understand the options that are opening both in the future of mobility and the ability of electric utilities to meet the needs of a highway transportation system powered primarily by electricity. They were asked both to explain what is known about the choices we face and to define the research issues most urgently needed to help public and private decision-makers choose wisely. This report is a summary of their insights based on workshop presentations and discussions. New communication and data analysis tools have profoundly changed the definition of what is technologically possible. Cell phones have put powerful computers, communication devices, and position locators into the pockets and purses of most Americans making it possible for Uber, Lyft and other Transportation Network Companies to deliver on-demand mobility services. But these technologies, as well as technologies for pricing access to congested roads, also open many other possibilities for shared mobility services – both public and private – that could cut costs and travel time by reducing congestion. Options would be greatly expanded if fully autonomous vehicles become available. These new business models would also affect options for charging electric vehicles. It is unclear, however, how to optimize charging (minimizing congestion on the electric grid) without increasing congestion on the roads or creating significant problems for the power system that supports such charging capacity. With so much in flux, many uncertainties cloud our vision of the future. The way new mobility services will reshape the number, length of trips, and the choice of electric vehicle charging systems and constraints on charging, and many other important behavioral issues are critical to this future but remain largely unknown. The challenge at hand is to define plausible future structures of electric grids and mobility systems, and anticipate the direct and indirect impacts of the changes involved. These insights can provide tools essential for effective private ... [TRUNCATED]"
CHRISTOS G CASSANDRAS,Data-driven estimation of origin-destination demand and user cost functions for the optimization of transportation networks,
CHRISTOS G CASSANDRAS,Congestion-aware routing and rebalancing of autonomous mobility-on-demand systems in mixed traffic,"This paper studies congestion-aware route-planning policies for Autonomous Mobility-on-Demand (AMoD) systems, whereby a fleet of autonomous vehicles provides on demand mobility under mixed traffic conditions. Specifically, we first devise a network flow model to optimize the AMoD routing and rebalancing strategies in a congestion-aware fashion by accounting for the endogenous impact of AMoD flows on travel time. Second, we capture reactive exogenous traffic consisting of private vehicles selfishly adapting to the AMoD flows in a user centric fashion by leveraging an iterative approach. Finally, we showcase the effectiveness of our framework with a case-study considering the transportation sub-network in New York City. Our results suggest that for high levels of demand, pure AMoD travel can be detrimental due to the additional traffic stemming from its rebalancing flows, whilst the combination of AMoD with walking or micro mobility options can significantly improve the overall system performance."
CHRISTOS G CASSANDRAS,The price of anarchy in transportation networks by estimating user cost functions from actual traffic data,"We have considered a large-scale road network in Eastern Massachusetts. Using real traffic data in the form of spatial average speeds and the flow capacity for each road segment of the network, we converted the speed data to flow data and estimated the origin-destination flow demand matrices for the network. Assuming that the observed traffic data correspond to user (Wardrop) equilibria for different times-of-the-day and days-of-the-week, we formulated appropriate inverse problems to recover the per-road cost (congestion) functions determining user route selection for each month and time-of-day period. In addition, we analyzed the sensitivity of the total user latency cost to important parameters such as road capacities and minimum travel times. Finally, we formulated a system-optimum problem in order to find socially optimal flows for the network. We investigated the network performance, in terms of the total latency, under a user-optimal policy versus a system-optimal policy. The ratio of these two quantities is defined as the Price of Anarchy (POA) and quantifies the efficiency loss of selfish actions compared to socially optimal ones. Our findings contribute to efforts for a smarter and more efficient city."
CHRISTOS G CASSANDRAS,Generalized proximal policy optimization with sample reuse,"In real-world decision making tasks, it is critical for data-driven reinforcement learning methods to be both stable and sample efficient. On-policy methods typically generate reliable policy improvement throughout training, while off-policy methods make more efficient use of data through sample reuse. In this work, we combine the theoretically supported stability benefits of on-policy algorithms with the sample efficiency of off-policy algorithms. We develop policy improvement guarantees that are suitable for the off-policy setting, and connect these bounds to the clipping mechanism used in Proximal Policy Optimization. This motivates an off-policy version of the popular algorithm that we call Generalized Proximal Policy Optimization with Sample Reuse. We demonstrate both theoretically and empirically that our algorithm delivers improved performance by effectively balancing the competing goals of stability and sample efficiency."
CHRISTOS G CASSANDRAS,Event-driven receding horizon control for distributed persistent monitoring on graphs,"We consider the optimal multi-agent persistent monitoring problem defined on a set of nodes (targets) inter-connected through a fixed graph topology. The objective is to minimize a measure of mean overall node state uncertainty evaluated over a finite time interval by controlling the motion of a team of agents. Prior work has addressed this problem through on-line parametric controllers and gradient-based methods, often leading to low-performing local optima or through off-line computationally intensive centralized approaches. This paper proposes a computationally efficient event-driven receding horizon control approach providing a distributed on-line gradient-free solution to the persistent monitoring problem. A novel element in the controller, which also makes it parameter-free, is that it self-optimizes the planning horizon over which control actions are sequentially taken in event-driven fashion. Numerical results show significant improvements compared to state of the art distributed on-line parametric control solutions."
CHRISTOS G CASSANDRAS,Comparison of centralized and decentralized approaches in cooperative coverage problems with energy-constrained agents,"A multi-agent coverage problem is considered with energy-constrained agents. The objective of this paper is to compare the coverage performance between centralized and decentralized approaches. To this end, a near-optimal centralized coverage control method is developed under energy depletion and repletion constraints. The optimal coverage formation corresponds to the locations of agents where the coverage performance is maximized. The optimal charging formation corresponds to the locations of agents with one agent fixed at the charging station and the remaining agents maximizing the coverage performance. We control the behavior of this cooperative multi-agent system by switching between the optimal coverage formation and the optimal charging formation. Finally, the optimal dwell times at coverage locations, charging time, and agent trajectories are determined so as to maximize coverage over a given time interval. In particular, our controller guarantees that at any time there is at most one agent leaving the team for energy repletion."
CHRISTOS G CASSANDRAS,Distributed non-convex optimization of multi-agent systems using boosting functions to escape local optima,"We address the problem of multiple local optima arising in cooperative multi-agent optimization problems with non-convex objective functions. We propose a systematic approach to escape these local optima using boosting functions. These functions temporarily transform a gradient at a local optimum into a ""boosted"" non-zero gradient. Extending a prior centralized optimization approach, we develop a distributed framework for the use of boosted gradients and show that convergence of this distributed process can be attained by employing an optimal variable step size scheme for gradient-based algorithms. Numerical examples are included to show how the performance of a class of multi-agent optimization systems can be improved."
CHRISTOS G CASSANDRAS,Decentralized optimal merging control for connected and automated vehicles with safety constraint guarantees,"This paper addresses the optimal control of Connected and Automated Vehicles (CAVs) arriving from two roads at a Merging Point (MP) where the objective is to jointly minimize the travel time and energy consumption of each CAV. The optimal solution can be used as a reference for tracking control and it guarantees that a speed-dependent safety constraint is satisfied both at the MP and everywhere within a Control Zone (CZ) which precedes it. We analyze the case of no active constraints and prove that under certain conditions the safety and speed constraints remain inactive, thus significantly simplifying the determination of an explicit decentralized solution. When these conditions do not apply, a complete solution is still obtained which includes all possible constraints becoming active. Our analysis allows us to study the tradeoff between the two objective function components (travel time and energy within the CZ). Simulation examples are provided to compare the performance of the optimal controller to a baseline consisting of human-driven vehicles with results showing improvements in both metrics."
CHRISTOS G CASSANDRAS,Physiological and socioeconomic characteristics predict COVID-19 mortality and resource utilization in Brazil,"BACKGROUND: Given the severity and scope of the current COVID-19 pandemic, it is critical to determine predictive features of COVID-19 mortality and medical resource usage to effectively inform health, risk-based physical distancing, and work accommodation policies. Non-clinical sociodemographic features are important explanatory variables of COVID-19 outcomes, revealing existing disparities in large health care systems. METHODS AND FINDINGS: We use nation-wide multicenter data of COVID-19 patients in Brazil to predict mortality and ventilator usage. The dataset contains hospitalized patients who tested positive for COVID-19 and had either recovered or were deceased between March 1 and June 30, 2020. A total of 113,214 patients with 50,387 deceased, were included. Both interpretable (sparse versions of Logistic Regression and Support Vector Machines) and state-of-the-art non-interpretable (Gradient Boosted Decision Trees and Random Forest) classification methods are employed. Death from COVID-19 was strongly associated with demographics, socioeconomic factors, and comorbidities. Variables highly predictive of mortality included geographic location of the hospital (OR = 2.2 for Northeast region, OR = 2.1 for North region); renal (OR = 2.0) and liver (OR = 1.7) chronic disease; immunosuppression (OR = 1.7); obesity (OR = 1.7); neurological (OR = 1.6), cardiovascular (OR = 1.5), and hematologic (OR = 1.2) disease; diabetes (OR = 1.4); chronic pneumopathy (OR = 1.4); immunosuppression (OR = 1.3); respiratory symptoms, ranging from respiratory discomfort (OR = 1.4) and dyspnea (OR = 1.3) to oxygen saturation less than 95% (OR = 1.7); hospitalization in a public hospital (OR = 1.2); and self-reported patient illiteracy (OR = 1.1). Validation accuracies (AUC) for predicting mortality and ventilation need reach 79% and 70%, respectively, when using only pre-admission variables. Models that use post-admission disease progression information reach accuracies (AUC) of 86% and 87% for predicting mortality and ventilation use, respectively. CONCLUSIONS: The results highlight the predictive power of socioeconomic information in assessing COVID-19 mortality and medical resource allocation, and shed light on existing disparities in the Brazilian health care system during the COVID-19 pandemic."
CHRISTOS G CASSANDRAS,Optimal composition of heterogeneous multi-agent teams for coverage problems with performance bound guarantees,
CHRISTOS G CASSANDRAS,Joint pricing and rebalancing of autonomous mobility-on-demand systems,"This paper studies optimal pricing and rebalancing policies for Autonomous Mobility-on-Demand (AMoD) systems. We adopt a macroscopic planning perspective to tackle a profit maximization problem while ensuring that the system is load-balanced. We describe the system using a dynamic fluid model to show the existence and stability of an equilibrium (i.e., load balance) through pricing policies. We then develop an optimization framework that allows us to find optimal policies in terms of both pricing and rebalancing. We first maximize profit by only using pricing policies, then incorporate rebalancing, and finally we consider whether the solution is found sequentially or jointly. We apply each approach to a data-driven case study using real taxi data from New York City. Depending on which benchmarking solution we use, the joint problem (i.e., pricing and rebalancing) increases profits by 7% to 40%."
CHRISTOS G CASSANDRAS,Decentralized optimal control in multi-lane merging for connected and automated vehicles,
CHRISTOS G CASSANDRAS,Feasibility-guided learning for constrained optimal control problems,"Optimal control problems with constraints ensuring safety can be mapped onto a sequence of real time optimization problems through the use of Control Barrier Functions (CBFs) and Control Lyapunov Functions (CLFs). One of the main challenges in these approaches is ensuring the feasibility of the resulting quadratic programs (QPs) if the system is affine in controls. In this paper, we improve the feasibility robustness (i.e., feasibility maintenance in the presence of time-varying and unknown unsafe sets) through the definition of a High Order CBF (HOCBF); this is achieved by a proposed feasibility-guided learning approach using machine learning techniques. The effectiveness of the proposed feasibility-guided learning approach is demonstrated on a robot control problem."
CHRISTOS G CASSANDRAS,Multi-agent infinite horizon persistent monitoring of targets with uncertain states in multi-dimensional environments,
CHRISTOS G CASSANDRAS,Network anomaly detection: a survey and comparative analysis of stochastic and deterministic methods,"We present five methods to the problem of network anomaly detection. These methods cover most of the common techniques in the anomaly detection field, including Statistical Hypothesis Tests (SHT), Support Vector Machines (SVM) and clustering analysis. We evaluate all methods in a simulated network that consists of nominal data, three flow-level anomalies and one packet-level attack. Through analyzing the results, we point out the advantages and disadvantages of each method and conclude that combining the results of the individual methods can yield improved anomaly detection results."
KAREN E LASSER,"Development and assessment of a new framework for disease surveillance, prediction, and risk adjustment: the diagnostic items classification system","IMPORTANCE: Current disease risk-adjustment formulas in the US rely on diagnostic classification frameworks that predate the International Classification of Diseases, Tenth Revision, Clinical Modification (ICD-10-CM). OBJECTIVE: To develop an ICD-10-CM-based classification framework for predicting diverse health care payment, quality, and performance outcomes. DESIGN SETTING AND PARTICIPANTS: Physician teams mapped all ICD-10-CM diagnoses into 3 types of diagnostic items (DXIs): main effect DXIs that specify diseases; modifiers, such as laterality, timing, and acuity; and scaled variables, such as body mass index, gestational age, and birth weight. Every diagnosis was mapped to at least 1 DXI. Stepwise and weighted least-squares estimation predicted cost and utilization outcomes, and their performance was compared with models built on (1) the Agency for Healthcare Research and Quality Clinical Classifications Software Refined (CCSR) categories, and (2) the Health and Human Services Hierarchical Condition Categories (HHS-HCC) used in the Affordable Care Act Marketplace. Each model's performance was validated using R 2, mean absolute error, the Cumming prediction measure, and comparisons of actual to predicted outcomes by spending percentiles and by diagnostic frequency. The IBM MarketScan Commercial Claims and Encounters Database, 2016 to 2018, was used, which included privately insured, full- or partial-year eligible enrollees aged 0 to 64 years in plans with medical, drug, and mental health/substance use coverage. MAIN OUTCOMES AND MEASURES: Fourteen concurrent outcomes were predicted: overall and plan-paid health care spending (top-coded and not top-coded); enrollee out-of-pocket spending; hospital days and admissions; emergency department visits; and spending for 6 types of services. The primary outcome was annual health care spending top-coded at $250 000. RESULTS: A total of 65 901 460 person-years were split into 90% estimation/10% validation samples (n = 6 604 259). In all, 3223 DXIs were created: 2435 main effects, 772 modifiers, and 16 scaled items. Stepwise regressions predicting annual health care spending (mean [SD], $5821 [$17 653]) selected 76% of the main effect DXIs with no evidence of overfitting. Validated R 2 was 0.589 in the DXI model, 0.539 for CCSR, and 0.428 for HHS-HCC. Use of DXIs reduced underpayment for enrollees with rare (1-in-a-million) diagnoses by 83% relative to HHS-HCCs. CONCLUSIONS: In this diagnostic modeling study, the new DXI classification system showed improved predictions over existing diagnostic classification systems for all spending and utilization outcomes considered."
KAREN E LASSER,A Multilevel Intervention to Promote Colorectal Cancer Screening among Community Health Center Patients: Results of a Pilot Study,"BACKGROUND. Colorectal cancer screening rates are low among poor and disadvantaged patients. Patient navigation has been shown to increase breast and cervical cancer screening rates, but few studies have looked at the potential of patient navigation to increase colorectal cancer screening rates. METHODS. The objective was to determine the feasibility and effectiveness of a patient navigator-based intervention to increase colorectal cancer screening rates in community health centers. Patients at the intervention health center who had not been screened for colorectal cancer and were designated as ""appropriate for outreach"" by their primary care providers received a letter from their provider about the need to be screened and a brochure about colorectal cancer screening. Patient navigators then called patients to discuss screening and to assist patients in obtaining screening. Patients at a demographically similar control health center received usual care. RESULTS. Thirty-one percent of intervention patients were screened at six months, versus nine percent of control patients (p < .001). CONCLUSION. A patient navigator-based intervention, in combination with a letter from the patient's primary care provider, was associated with an increased rate of colorectal cancer screening at one health center as compared to a demographically similar control health center. Our study adds to an emerging literature supporting the use of patient navigators to increase colorectal cancer screening in diverse populations served by urban health centers."
PATRICK KINNEY,The Event Horizon general relativistic magnetohydrodynamic code comparison project,"Recent developments in compact object astrophysics, especially the discovery of merging neutron stars by LIGO, the imaging of the black hole in M87 by the Event Horizon Telescope, and high- precision astrometry of the Galactic Center at close to the event horizon scale by the GRAVITY experiment motivate the development of numerical source models that solve the equations of general relativistic magnetohydrodynamics (GRMHD). Here we compare GRMHD solutions for the evolution of a magnetized accretion flow where turbulence is promoted by the magnetorotational instability from a set of nine GRMHD codes: Athena++, BHAC, Cosmos++, ECHO, H-AMR, iharm3D, HARM-Noble, IllinoisGRMHD, and KORAL. Agreement among the codes improves as resolution increases, as measured by a consistently applied, specially developed set of code performance metrics. We conclude that the community of GRMHD codes is mature, capable, and consistent on these test problems."
PATRICK KINNEY,Carbon Free Boston: Technical Summary,"OVERVIEW: This technical summary is intended to argument the rest of the Carbon Free Boston technical reports that seek to achieve this goal of deep mitigation. This document provides below: a rationale for carbon neutrality, a high level description of Carbon Free Boston’s analytical approach; a summary of crosssector strategies; a high level analysis of air quality impacts; and, a brief analysis of off-road and street light emissions."
JACK A CLARK,Measuring Perceived Effects of Drinking an Extract of Basidiomycetes Agaricus Blazei Murill: A Survey of Japanese Consumers with Cancer,"BACKGROUND. To survey cancer patients who consume an extract of the Basidiomycetes Agaricus blazei Murill mushroom (Sen-Sei-Ro) to measure their self-assessment of its effects and to develop an instrument for use in future randomized trials. METHODS. We designed, translated and mailed a survey to 2,346 Japanese consumers of Sen-Sei-Ro self-designated as cancer patients. The survey assessed consumer demographics, cancer history, Sen-Sei-Ro consumption, and its perceived effects. We performed exploratory psychometric analyses to identify distinct, multi-item scales that could summarize perceptions of effects. RESULTS. We received completed questionnaires from 782 (33%) of the sampled Sen-Sei-Ro consumers with a cancer history. Respondents represented a broad range of cancer patients familiar with Sen-Sei-Ro. Nearly all had begun consumption after their cancer diagnosis. These consumers expressed consistently positive views, though not extremely so, with more benefit reported for more abstract benefits such as emotional and physical well-being than relief of specific symptoms. We identified two conceptually and empirically distinct and internally consistent summary scales measuring Sen-Sei-Ro consumers' perceptions of its effects, Relief of Symptoms and Functional Well-being (Cronbach's alpha: Relief of Symptoms, α = .74; Functional Well-Being, α = .91). CONCLUSION. Respondents to our survey of Sen-Sei-Ro consumers with cancer reported favorable perceived effects from its use. Our instrument, when further validated, may be a useful outcome in trials assessing this and other complementary and alternative medicine (CAM) substances in cancer patients."
JACK A CLARK,"Minerva-Australis. I. design, commissioning, and first photometric results","The Minerva-Australis telescope array is a facility dedicated to the follow-up, confirmation, characterization, and mass measurement of planets orbiting bright stars discovered by the Transiting Exoplanet Survey Satellite (TESS)—a category in which it is almost unique in the Southern Hemisphere. It is located at the University of Southern Queensland's Mount Kent Observatory near Toowoomba, Australia. Its flexible design enables multiple 0.7 m robotic telescopes to be used both in combination, and independently, for high-resolution spectroscopy and precision photometry of TESS transit planet candidates. Minerva-Australis also enables complementary studies of exoplanet spin–orbit alignments via Doppler observations of the Rossiter–McLaughlin effect, radial velocity searches for nontransiting planets, planet searches using transit timing variations, and ephemeris refinement for TESS planets. In this first paper, we describe the design, photometric instrumentation, software, and science goals of Minerva-Australis, and note key differences from its Northern Hemisphere counterpart, the Minerva array. We use recent transit observations of four planets, WASP-2b, WASP-44b, WASP-45b, and HD 189733b, to demonstrate the photometric capabilities of Minerva-Australis."
ALEXANDER V SERGIENKO,Topological qubits as carriers of quantum information in optics,"Winding number is a topologically significant quantity that has found valuable applications in various areas of mathematical physics. Here, topological qubits are shown capable of formation from winding number superpositions and so of being used in the communication of quantum information in linear optical systems, the most common realm for quantum communication. In particular, it is shown that winding number qubits appear in several aspects of such systems, including quantum electromagnetic states of spin, momentum, orbital angular momentum, polarization of beams of particles propagating in free-space, optical fiber, beam splitters, and optical multiports."
ALEXANDER V SERGIENKO,Experimental demonstration of a directionally-unbiased linear-optical multiport,"All existing optical quantum walk approaches are based on the use of beamsplitters and multiple paths to explore the multitude of unitary transformations of quantum amplitudes in a Hilbert space. The beamsplitter is naturally a directionally biased device: the photon cannot travel in reverse direction. This causes rapid increases in optical hardware resources required for complex quantum walk applications, since the number of options for the walking particle grows with each step. Here we present the experimental demonstration of a directionally-unbiased linear-optical multiport, which allows reversibility of photon direction. An amplitude-controllable probability distribution matrix for a unitary three-edge vertex is reconstructed with only linear-optical devices. Such directionally-unbiased multiports allow direct execution of quantum walks over a multitude of complex graphs and in tensor networks. This approach would enable simulation of complex Hamiltonians of physical systems and quantum walk applications in a more efficient and compact setup, substantially reducing the required hardware resources."
ALEXANDER V SERGIENKO,Joint entanglement of topology and polarization enables error-protected quantum registers,"Linear-optical systems can implement photonic quantum walks that simulate systems with nontrivial topological properties. Here, such photonic walks are used to jointly entangle polarization and winding number. This joint entanglement allows information processing tasks to be performed with interactive access to a wide variety of topological features. Topological considerations are used to suppress errors, with polarization allowing easy measurement and manipulation of qubits. We provide three examples of this approach: production of two-photon systems with entangled winding number (including topological analogs of Bell states), a topologically error-protected optical memory register, and production of entangled topologically-protected boundary states. In particular it is shown that a pair of quantum memory registers, entangled in polarization and winding number, with topologically-assisted error suppression can be made with qubits stored in superpositions of winding numbers; as a result, information processing with winding number-based qubits is a viable possibility."
ALEXANDER V SERGIENKO,Entangled-coherent-state quantum key distribution with entanglement witnessing,"An entanglement witness approach to quantum coherent state key distribution and a system for its practical implementation are described. In this approach, eavesdropping can be detected by a change in sign of either of two witness functions, an entanglement witness S or an eavesdropping witness W. The effects of loss and eavesdropping on system operation are evaluated as a function of distance. Although the eavesdropping witness W does not directly witness entanglement for the system, its behavior remains related to that of the true entanglement witness S. Furthermore, W is easier to implement experimentally than S. W crosses the axis at a finite distance, in a manner reminiscent of entanglement sudden death. The distance at which this occurs changes measurably when an eavesdropper is present. The distance dependance of the two witnesses due to amplitude reduction and due to increased variance resulting from both ordinary propagation losses and possible eavesdropping activity is provided. Finally, the information content and secure key rate of a continuous variable protocol using this witness approach are given."
ALEXANDER V SERGIENKO,"Entanglement, mixedness, and spin-flip symmetry in multiple-qubit systems","A relationship between a recently introduced multipartite entanglement measure, state mixedness, and spin-flip symmetry is established for any finite number of qubits. It is also shown that, for those classes of states invariant under the spin-flip transformation, there is a complementarity relation between multipartite entanglement and mixedness. A number of example classes of multiple-qubit systems are studied in light of this relationship."
ALEXANDER V SERGIENKO,Lossless broadband adiabatic polarizing beam splitting in a plasmonic system,"The intriguing analogy between quantum physics and optics has inspired the design of unconventional integrated photonics devices. In this paper, we numerically demonstrate a broadband integrated polarization beam splitter (PBS) by implementing the stimulated Raman adiabatic passage (STIRAP) technique in a three-waveguide plasmonic system. Our proposed PBS exhibits >250 nm TM bandwidth with <-40 dB extinction and >150 nm TE bandwidth with <-20 dB extinction, covering the entire S-, C- and L-band and partly E-band. Moreover, near-lossless light transfer is achieved in our system despite the incorporation of a plasmonic hybrid waveguide because of the unique loss mitigating feature of the STIRAP scheme. Through this approach, various broadband integrated devices that were previously impossible can be realized, which will allow innovation in integrated optics."
ALEXANDER V SERGIENKO,Directionally-unbiased unitary optical devices in discrete-time quantum walks,"The optical beam splitter is a widely-used device in photonics-based quantum information processing. Specifically, linear optical networks demand large numbers of beam splitters for unitary matrix realization. This requirement comes from the beam splitter property that a photon cannot go back out of the input ports, which we call “directionally-biased”. Because of this property, higher dimensional information processing tasks suffer from rapid device resource growth when beam splitters are used in a feed-forward manner. Directionally-unbiased linear-optical devices have been introduced recently to eliminate the directional bias, greatly reducing the numbers of required beam splitters when implementing complicated tasks. Analysis of some originally directional optical devices and basic principles of their conversion into directionally-unbiased systems form the base of this paper. Photonic quantum walk implementations are investigated as a main application of the use of directionally-unbiased systems. Several quantum walk procedures executed on graph networks constructed using directionally-unbiased nodes are discussed. A significant savings in hardware and other required resources when compared with traditional directionally-biased beam-splitter-based optical networks is demonstrated."
ALEXANDER V SERGIENKO,Quantum simulation of discrete-time Hamiltonians using directionally unbiased linear optical multiports,"Recently, a generalization of the standard optical multiport was proposed [Phys. Rev. A 93, 043845 (2016)]. These directionally unbiased multiports allow photons to reverse direction and exit backwards from the input port, providing a realistic linear optical scattering vertex for quantum walks on arbitrary graph structures. Here, it is shown that arrays of these multiports allow the simulation of a range of discrete-time Hamiltonian systems. Examples are described, including a case where both spatial and internal degrees of freedom are simulated. Because input ports also double as output ports, there is substantial savings of resources compared to feed-forward networks carrying out the same functions. The simulation is implemented in a scalable manner using only linear optics, and can be generalized to higher dimensional systems in a straightforward fashion, thus offering a concrete experimentally achievable implementation of graphical models of discrete-time quantum systems."
ALEXANDER V SERGIENKO,Quantum simulation of topologically protected states using directionally unbiased linear-optical multiports,"It is shown that quantum walks on one-dimensional arrays of special linear-optical units allow the simulation of discrete-time Hamiltonian systems with distinct topological phases. In particular, a slightly modified version of the Su-Schrieffer-Heeger (SSH) system can be simulated, which exhibits states of nonzero winding number and has topologically protected boundary states. In the large-system limit this approach uses quadratically fewer resources to carry out quantum simulations than previous linear-optical approaches and can be readily generalized to higher-dimensional systems. The basic optical units that implement this simulation consist of combinations of optical multiports that allow photons to reverse direction."
ALEXANDER V SERGIENKO,Electro-optically modulated polarization mode conversion in lithium niobate ridge waveguides,"Lithium niobate on insulator (LNOI) waveguides, as an emerging technology, have proven to offer a promising platform for integrated optics, due to their strong optical confinement comparable to silicon on insulator (SOI) waveguides, while possessing the versatile properties of lithium niobate, such as high electro-optic coefficients. In this paper, we show that mode hybridization, a phenomenon widely found in vertically asymmetric waveguides, can be efficiently modulated in an LNOI ridge waveguide by electro-optic effect, leading to a polarization mode converter with 97% efficiency. Moreover, the proposed device does not require tapering or periodic poling, thereby greatly simplifying the fabrication process. It can also be actively switched by external fields. Such a platform facilitates technological progress of photonic circuits and sensors."
ALEXANDER V SERGIENKO,Discrimination and synthesis of recursive quantum states in high-dimensional Hilbert spaces,"We propose an interferometric method for statistically discriminating between nonorthogonal states in high-dimensional Hilbert spaces for use in quantum information processing. The method is illustrated for the case of photon orbital angular momentum (OAM) states. These states belong to pairs of bases that are mutually unbiased on a sequence of two-dimensional subspaces of the full Hilbert space, but the vectors within the same basis are not necessarily orthogonal to each other. Over multiple trials, this method allows distinguishing OAM eigenstates from superpositions of multiple such eigenstates. Variations of the same method are then shown to be capable of preparing and detecting arbitrary linear combinations of states in Hilbert space. One further variation allows the construction of chains of states obeying recurrence relations on the Hilbert space itself, opening a new range of possibilities for more abstract information-coding algorithms to be carried out experimentally in a simple manner. Among other applications, we show that this approach provides a simplified means of switching between pairs of high-dimensional mutually unbiased OAM bases."
ROBERT J POLLACK,Slopes of modular forms and the ghost conjecture,"We formulate a conjecture on slopes of overconvergent p-adic cusp forms of any p-adic weight in the Gamma_0(N)-regular case. This conjecture unifies a conjecture of Buzzard on classical slopes and more recent conjectures on slopes “at the boundary of weight space""."
TASSO J KAPER,The dynamics of hybrid metabolic-genetic oscillators,"The synthetic construction of intracellular circuits is frequently hindered by a poor knowledge of appropriate kinetics and precise rate parameters. Here, we use generalized modeling (GM) to study the dynamical behavior of topological models of a family of hybrid metabolic-genetic circuits known as ""metabolators."" Under mild assumptions on the kinetics, we use GM to analytically prove that all explicit kinetic models which are topologically analogous to one such circuit, the ""core metabolator,"" cannot undergo Hopf bifurcations. Then, we examine more detailed models of the metabolator. Inspired by the experimental observation of a Hopf bifurcation in a synthetically constructed circuit related to the core metabolator, we apply GM to identify the critical components of the synthetically constructed metabolator which must be reintroduced in order to recover the Hopf bifurcation. Next, we study the dynamics of a re-wired version of the core metabolator, dubbed the ""reverse"" metabolator, and show that it exhibits a substantially richer set of dynamical behaviors, including both local and global oscillations. Prompted by the observation of relaxation oscillations in the reverse metabolator, we study the role that a separation of genetic and metabolic time scales may play in its dynamics, and find that widely separated time scales promote stability in the circuit. Our results illustrate a generic pipeline for vetting the potential success of a circuit design, simply by studying the dynamics of the corresponding generalized model."
TASSO J KAPER,Modeling the dynamics of glacial cycles,"This article is concerned with the dynamics of glacial cycles observed in the geological record of the Pleistocene Epoch. It focuses on a conceptual model proposed by Maasch and Saltzman [J. Geophys. Res.,95, D2 (1990), pp. 1955-1963], which is based on physical arguments and emphasizes the role of atmospheric CO2 in the generation and persistence of periodic orbits (limit cycles). The model consists of three ordinary differential equations with four parameters for the anomalies of the total global ice mass, the atmospheric CO2 concentration, and the volume of the North Atlantic Deep Water (NADW). In this article, it is shown that a simplified two-dimensional symmetric version displays many of the essential features of the full model, including equilibrium states, limit cycles, their basic bifurcations, and a Bogdanov-Takens point that serves as an organizing center for the local and global dynamics. Also, symmetry breaking splits the Bogdanov-Takens point into two, with different local dynamics in their neighborhoods."
KATYA RAVID,Hematopoietic gene promoters subjected to a group-combinatorial study of DNA samples: identification of a megakaryocytic selective DNA signature,"Identification of common sub-sequences for a group of functionally related DNA sequences can shed light on the role of such elements in cell-specific gene expression. In the megakaryocytic lineage, no one single unique transcription factor was described as linage specific, raising the possibility that a cluster of gene promoter sequences presents a unique signature. Here, the megakaryocytic gene promoter group, which consists of both human and mouse 5' non-coding regions, served as a case study. A methodology for group-combinatorial search has been implemented as a customized software platform. It extracts the longest common sequences for a group of related DNA sequences and allows for single gaps of varying length, as well as double- and multiple-gap sequences. The results point to common DNA sequences in a group of genes that is selectively expressed in megakaryocytes, and which does not appear in a large group of control, random and specific sequences. This suggests a role for a combination of these sequences in cell-specific gene expression in the megakaryocytic lineage. The data also point to an intrinsic cross-species difference in the organization of 5' non-coding sequences within the mammalian genomes. This methodology may be used for the identification of regulatory sequences in other lineages."
KATYA RAVID,Activation of Adenosine A2B Receptors Enhances Ciliary Beat Frequency in Mouse Lateral Ventricle Ependymal Cells,"BACKGROUND: Ependymal cells form a protective monolayer between the brain parenchyma and cerebrospinal fluid (CSF). They possess motile cilia important for directing the flow of CSF through the ventricular system. While ciliary beat frequency in airway epithelia has been extensively studied, fewer reports have looked at the mechanisms involved in regulating ciliary beat frequency in ependyma. Prior studies have demonstrated that ependymal cells express at least one purinergic receptor (P2X7). An understanding of the full range of purinergic receptors expressed by ependymal cells, however, is not yet complete. The objective of this study was to identify purinergic receptors which may be involved in regulating ciliary beat frequency in lateral ventricle ependymal cells. METHODS: High-speed video analysis of ciliary movement in the presence and absence of purinergic agents was performed using differential interference contrast microscopy in slices of mouse brain (total number of animals = 67). Receptor identification by this pharmacological approach was corroborated by immunocytochemistry, calcium imaging experiments, and the use of two separate lines of knockout mice. RESULTS: Ciliary beat frequency was enhanced by application of a commonly used P2X7 agonist. Subsequent experiments, however, demonstrated that this enhancement was observed in both P2X7+/+ and P2X7-/- mice and was reduced by pre-incubation with an ecto-5'-nucleotidase inhibitor. This suggested that enhancement was primarily due to a metabolic breakdown product acting on another purinergic receptor subtype. Further studies revealed that ciliary beat frequency enhancement was also induced by adenosine receptor agonists, and pharmacological studies revealed that ciliary beat frequency enhancement was primarily due to A2B receptor activation. A2B expression by ependymal cells was subsequently confirmed using A2B-/-/β-galactosidase reporter gene knock-in mice. CONCLUSION: This study demonstrates that A2B receptor activation enhances ciliary beat frequency in lateral ventricle ependymal cells. Ependymal cell ciliary beat frequency regulation may play an important role in cerebral fluid balance and cerebrospinal fluid dynamics."
KATYA RAVID,The Reno-Vascular A2B Adenosine Receptor Protects the Kidney from Ischemia,"BACKGROUND. Acute renal failure from ischemia significantly contributes to morbidity and mortality in clinical settings, and strategies to improve renal resistance to ischemia are urgently needed. Here, we identified a novel pathway of renal protection from ischemia using ischemic preconditioning (IP). METHODS AND FINDINGS. For this purpose, we utilized a recently developed model of renal ischemia and IP via a hanging weight system that allows repeated and atraumatic occlusion of the renal artery in mice, followed by measurements of specific parameters or renal functions. Studies in gene-targeted mice for each individual adenosine receptor (AR) confirmed renal protection by IP in A1−/−, A2A−/−, or A3AR−/− mice. In contrast, protection from ischemia was abolished in A2BAR−/− mice. This protection was associated with corresponding changes in tissue inflammation and nitric oxide production. In accordance, the A2BAR-antagonist PSB1115 blocked renal protection by IP, while treatment with the selective A2BAR-agonist BAY 60–6583 dramatically improved renal function and histology following ischemia alone. Using an A2BAR-reporter model, we found exclusive expression of A2BARs within the reno-vasculature. Studies using A2BAR bone-marrow chimera conferred kidney protection selectively to renal A2BARs. CONCLUSIONS. These results identify the A2BAR as a novel therapeutic target for providing potent protection from renal ischemia. Using gene-targeted mice, Holger Eltzschig and colleagues identify the A2B adenosine receptor as a novel therapeutic target for providing protection from renal ischemia."
KATYA RAVID,Hematopoietic Gene Promoters Subjected to a Group-Combinatorial Study of DNA Samples: Identification of a Megakaryocytic Selective DNA Signature,"Identification of common sub-sequences for a group of functionally related DNA sequences can shed light on the role of such elements in cell-specific gene expression. In the megakaryocytic lineage, no one single unique transcription factor was described as linage specific, raising the possibility that a cluster of gene promoter sequences presents a unique signature. Here, the megakaryocytic gene promoter group, which consists of both human and mouse 5′ non-coding regions, served as a case study. A methodology for group-combinatorial search has been implemented as a customized software platform. It extracts the longest common sequences for a group of related DNA sequences and allows for single gaps of varying length, as well as double- and multiple-gap sequences. The results point to common DNA sequences in a group of genes that is selectively expressed in megakaryocytes, and which does not appear in a large group of control, random and specific sequences. This suggests a role for a combination of these sequences in cell-specific gene expression in the megakaryocytic lineage. The data also point to an intrinsic cross-species difference in the organization of 5′ non-coding sequences within the mammalian genomes. This methodology may be used for the identification of regulatory sequences in other lineages."
KATYA RAVID,The Reno-Vascular A2B Adenosine Receptor Protects the Kidney from Ischemia,"BACKGROUND. Acute renal failure from ischemia significantly contributes to morbidity and mortality in clinical settings, and strategies to improve renal resistance to ischemia are urgently needed. Here, we identified a novel pathway of renal protection from ischemia using ischemic preconditioning (IP). METHODS AND FINDINGS. For this purpose, we utilized a recently developed model of renal ischemia and IP via a hanging weight system that allows repeated and atraumatic occlusion of the renal artery in mice, followed by measurements of specific parameters or renal functions. Studies in gene-targeted mice for each individual adenosine receptor (AR) confirmed renal protection by IP in A1−/−, A2A−/−, or A3AR−/− mice. In contrast, protection from ischemia was abolished in A2BAR−/− mice. This protection was associated with corresponding changes in tissue inflammation and nitric oxide production. In accordance, the A2BAR-antagonist PSB1115 blocked renal protection by IP, while treatment with the selective A2BAR-agonist BAY 60–6583 dramatically improved renal function and histology following ischemia alone. Using an A2BAR-reporter model, we found exclusive expression of A2BARs within the reno-vasculature. Studies using A2BAR bone-marrow chimera conferred kidney protection selectively to renal A2BARs. CONCLUSIONS. These results identify the A2BAR as a novel therapeutic target for providing potent protection from renal ischemia. Using gene-targeted mice, Holger Eltzschig and colleagues identify the A2B adenosine receptor as a novel therapeutic target for providing protection from renal ischemia."
JULIET FLOYD,Wittgenstein on ethics: working through Lebensformen,"In his Tractatus Logico-Philosophicus, Wittgenstein conveyed the idea that ethics cannot be located in an object or self-standing subject matter of propositional discourse, true or false. At the same time, he took his work to have an eminently ethical purpose, and his attitude was not that of the emotivist. The trajectory of this conception of the normativity of philosophy as it developed in his subsequent thought is traced. It is explained that and how the notion of a ‘form of life’ (Lebensform) emerged only in his later thought, in 1937, earmarking a significant step forward in his philosophical method. We argue that the concept of Lebensform represents a way of domesticating logic itself, the very idea of a claim or reason, supplementing the idea of a ‘language game’, which it deepens. Lebensform is contrasted with the phenomenologists’ Lebenswelt through a reading of the notions of ‘I’, ‘world’ and ‘self’ as they were treated in the Tractatus, The Blue and Brown Books and Philosophical Investigations. Finally, the notion of Lebensform is shown to have replaced the notion of culture (Kultur) in Philosophical Investigations. Wittgenstein’s spring 1937 ‘domestication’ of the nature of logic is shown to have been fully consonant with the idea that he was influenced by his reading Alan Turing’s 1936/1937 paper, ‘On computable numbers, with an application to the Entscheidungsproblem’."
JULIET FLOYD,Parikh and Wittgenstein,"A survey of Parikh’s philosophical appropriations of Wittgensteinian themes, placed into historical context against the backdrop of Turing’s famous paper, “On computable numbers, with an application to the Entscheidungsproblem” (Turing in Proc Lond Math Soc 2(42): 230–265, 1936/1937) and its connections with Wittgenstein and the foundations of mathematics. Characterizing Parikh’s contributions to the interaction between logic and philosophy at its foundations, we argue that his work gives the lie to recent presentations of Wittgenstein’s so-called metaphilosophy (e.g., Horwich in Wittgenstein’s metaphilosophy. Oxford University Press, Oxford, 2012) as a kind of “dead end” quietism. From early work on the idea of a feasibility in arithmetic (Parikh in J Symb Log 36(3):494–508, 1971) and vagueness (Parikh in Logic, language and method. Reidel, Boston, pp 241–261, 1983) to his more recent program in social software (Parikh in Advances in modal logic, vol 2. CSLI Publications, Stanford, pp 381–400, 2001a), Parikh’s work encompasses and touches upon many foundational issues in epistemology, philosophy of logic, philosophy of language, and value theory. But it expresses a unified philosophical point of view. In his most recent work, questions about public and private languages, opportunity spaces, strategic voting, non-monotonic inference and knowledge in literature provide a remarkable series of suggestions about how to present issues of fundamental importance in theoretical computer science as serious philosophical issues."
JULIET FLOYD,"Introductory philosophical reflections, ""Germs, Genes and GMO's: Has the Power of Social Media Disrupted Scientific Understanding"", BU Conference, Division of Emerging Media","Social media have dramatically increased the public’s voice in debates over the social consequences of scientific developments. Millions of heretofore uninvolved citizens have an opportunity to weigh-in on often complex and uncertain scientific questions in ways that are meaningful to themselves and their peers, and that affect the trajectory of debate as well as governmental support and regulations. To what extent has this development changed levels of social media users’ empowerment and participation, and how has it affected scientists who feel its effects? Has the quality of debate and policy outcomes improved? Are there unrecognized personal and professional costs? Do social media enhance or detract from public understanding and the advancement of knowledge? By examining three specific areas—genetic testing by individuals, user behaviors that affect antibiotic resistance and public policy concerning genetically modified organisms (GMOs)—speakers will offer views on these questions. Come hear Piper Below (U of TX Health Science Center at Houston), Kevin Folta (U of Florida), Dominique Broussard (U of Wisconsin), Annie Waldherr (Freie Universität Berlin) and others scientists and scholars share their insights concerning these questions."
JULIET FLOYD,"Chains of life: Turing, Lebensform, and the emergence of Wittgenstein’s later style",
JULIET FLOYD,Teaching and learning with Wittgenstein and Turing: sailing the seas of social media,"Results of the Boston University Mellon Sawyer seminar 2016–2019 (www.mellophilemerge.com) reveal that social and philosophical drives are increasingly central to our uses of technology, including AI. This raises critical challenges for democracy, especially in a hyper‐connected world where social media shapes human conduct in ways we are only beginning to appreciate. A history of the mutual impact of Turing and Wittgenstein on one another points to the contemporary foundational significance of our artful capacity to embed everyday words in forms of life. Wittgenstein's mature focus on forms of life, interlocutory drift, and rule‐following, with its play between the ‘I’ and the ‘we’, was an informed critical response to Turing's idea of a ‘Turing machine’, his analysis of the very idea of taking a ‘step’ in a formal system. Wittgenstein's characterisations of our drive to evade a responsibility in speech, especially by appealing to ‘machines’ or ‘algorithms’ as pure mathematical objects, are invaluable warnings for us. The enduring importance of mutually‐attuned ‘phraseology’ to education may be formulated as a humanistic challenge to the very ideas of ‘computational foundations’ and ‘Big Data’ in our hyper‐connected world."
JULIET FLOYD,Teaching and learning with Wittgenstein and Turing: sailing the seas of social media,"A history of the mutual impact of Turing and Wittgenstein on one another points to the contemporary foundational significance of our artful capacity to embed everyday words in forms of life. Wittgenstein’s mature focus on forms of life, interlocutory drift, and rule-following, with its play between the ‘I’ and the ‘we’, was an informed critical response to Turing’s idea of a “Turing machine”, his analysis of the very idea of taking a “step” in a formal system. Wittgenstein’s characterizations of our drive to evade a responsibility in speech, especially by appealing to “machines” or “algorithms” as pure mathematical objects, are invaluable warnings for us. The enduring importance of mutually-attuned “phraseology” to education may be formulated as a humanistic challenge to the very ideas of “computational foundations” and “Big Data” in our hyper-connected world. This raises critical challenges for democracy, especially in a world where social media shapes human-to-human conduct in ways we are only beginning to appreciate."
JULIET FLOYD,Positive pragmatic pluralism,
JULIET FLOYD,"""The True"" in Journalism","“The True” is a central norm of journalism that cannot be reduced away to something else: opinion, consensus, social force, power, or demographic identity. There are no “alternative” facts, though there are of course alternative interpretations of facts. The evolution of social media platforms have made collective pursuit of the ideal of “the True” more difficult and complex in our time. Just as in the 15th century the printing press led to the dissemination of forms of scepticism, so today, as the manufacture of doubt is disseminated by algorithms, distrust of journalists is widespread and increasing. Following Frege, we argue that journalists should nevertheless pursue the norm of “the True” as a central, irreducible and irreplaceable norm. We survey debates over “the True” from Frege through the pragmatists, social constructivists, deconstructionists, deflationists. In the end, we side with those who refuse to eliminate “the True”: Frege, Wittgenstein, Quine, Putnam and Diamond. “The True” should not be reduced to something it is not. Nevertheless we should be realistic about the struggle involved in unfolding and revealing it. The struggle for appropriate representation, acknowledgement of “the True”, is a central target of journalism, as it is also in everyday life."
JULIET FLOYD,Wittgenstein and Turing on ‘cultural searches’ and ‘common sense’,"Video of keynote speaker, for this NEH sponsored conference. Thanks to Jon Burmeister, organizer, funded by NEH. See https://workandleisure.org"
JULIET FLOYD,"""Ultimate"" facts? Zalabardo on the metaphysics of truth",A Comment on a Forthcoming article by José Zalabardo on the Tractatus Picture Theory's origins in Wittgenstein's reactions to Russell's Multiple Relation theory of Judgment and Truth. For a special issue of the Australasian Philosophical Review.
ANDREW J HENDERSON,High-resolution and -precision correlation of dark and light layers in the Quaternary hemipelagic sediments of the Japan Sea recovered during IODP Expedition 346,"The Quaternary hemipelagic sediments of the Japan Sea are characterized by centimeter- to decimeter-scale alternation of dark and light clay to silty clay, which are bio-siliceous and/or bio-calcareous to a various degree. Each of the dark and light layers are considered as deposited synchronously throughout the deeper (> 500 m) part of the sea. However, attempts for correlation and age estimation of individual layers are limited to the upper few tens of meters. In addition, the exact timing of the depositional onset of these dark and light layers and its synchronicity throughout the deeper part of the sea have not been explored previously, although the onset timing was roughly estimated as ~ 1.5 Ma based on the result of Ocean Drilling Program legs 127/128. Consequently, it is not certain exactly when their deposition started, whether deposition of dark and light layers was synchronous and whether they are correlatable also in the earlier part of their depositional history. The Quaternary hemipelagic sediments of the Japan Sea were drilled at seven sites during Integrated Ocean Drilling Program Expedition 346 in 2013. Alternation of dark and light layers was recovered at six sites whose water depths are > ~ 900 m, and continuous composite columns were constructed at each site. Here, we report our effort to correlate individual dark layers and estimate their ages based on a newly constructed age model at Site U1424 using the best available paleomagnetic datum and marker tephras. The age model is further tuned to LR04 δ18O curve using gamma ray attenuation density (GRA) since it reflects diatom contents that are higher during interglacial high-stands. The constructed age model for Site U1424 is projected to other sites using correlation of dark layers to form a high-resolution and high-precision paleo-observatory network that allows to reconstruct changes in material fluxes with high spatio-temporal resolutions."
ANDREW J HENDERSON,First radial velocity results from the MINiature Exoplanet Radial Velocity Array (MINERVA),"The MINiature Exoplanet Radial Velocity Array (MINERVA) is a dedicated observatory of four 0.7 m robotic telescopes fiber-fed to a KiwiSpec spectrograph. The MINERVA mission is to discover super-Earths in the habitable zones of nearby stars. This can be accomplished with MINERVA's unique combination of high precision and high cadence over long time periods. In this work, we detail changes to the MINERVA facility that have occurred since our previous paper. We then describe MINERVA's robotic control software, the process by which we perform 1D spectral extraction, and our forward modeling Doppler pipeline. In the process of improving our forward modeling procedure, we found that our spectrograph's intrinsic instrumental profile is stable for at least nine months. Because of that, we characterized our instrumental profile with a time-independent, cubic spline function based on the profile in the cross dispersion direction, with which we achieved a radial velocity precision similar to using a conventional ""sum-of-Gaussians"" instrumental profile: 1.8 m s−1 over 1.5 months on the RV standard star HD 122064. Therefore, we conclude that the instrumental profile need not be perfectly accurate as long as it is stable. In addition, we observed 51 Peg and our results are consistent with the literature, confirming our spectrograph and Doppler pipeline are producing accurate and precise radial velocities."
ANDREW J HENDERSON,Single cell transcriptomics reveals opioid usage evokes widespread suppression of antiviral gene program,"Chronic opioid usage not only causes addiction behavior through the central nervous system, but also modulates the peripheral immune system. However, how opioid impacts the immune system is still barely characterized systematically. In order to understand the immune modulatory effect of opioids in an unbiased way, here we perform single-cell RNA sequencing (scRNA-seq) of peripheral blood mononuclear cells from opioid-dependent individuals and controls to show that chronic opioid usage evokes widespread suppression of antiviral gene program in naive monocytes, as well as in multiple immune cell types upon stimulation with the pathogen component lipopolysaccharide. Furthermore, scRNA-seq reveals the same phenomenon after a short in vitro morphine treatment. These findings indicate that both acute and chronic opioid exposure may be harmful to our immune system by suppressing the antiviral gene program. Our results suggest that further characterization of the immune modulatory effects of opioid is critical to ensure the safety of clinical opioids."
DAVID CAMPBELL,Effect of mediated interactions on a Hubbard chain in mixed-dimensional fermionic cold atoms,"Cold atom experiments can now realize mixtures where different components move in different spatial dimensions. We investigate a fermion mixture where one species is constrained to move along a one-dimensional lattice embedded in a two-dimensional lattice populated by another species of fermions, and where all bare interactions are contact interactions. By focusing on the one-dimensional fermions, we map this problem onto a model of fermions with nonlocal interactions on a chain. The effective interaction is mediated by the two-dimensional fermions and is both attractive and retarded, the form of which can be varied by changing the density of the two-dimensional fermions. By using the functional renormalization group in the weak-coupling limit and ignoring the retardation effect, we show that the one-dimensional fermions can be controlled to be in various density-wave, or spin-singlet or spin-triplet superconducting phases."
DAVID CAMPBELL,Searching for the Casimir Energy,"In this article, we present a nano-electromechanical system (NEMS) designed to detect changes in the Casimir Energy. The Casimir effect is a result of the appearance of quantum fluctuations in the electromagnetic vacuum. Previous experiments have used nano- or micro- scale parallel plate capacitors to detect the Casimir force by measuring the small attractive force these fluctuations exert between the two surfaces. In this new set of experiments, we aim to directly detect shifts in the Casimir energy in the vacuum due to the presence of metallic parallel plates, one of which is a superconductor. A change in the Casimir energy of this configuration is predicted to shift the superconducting transition temperature (T_c) because of an interaction between it and the superconducting condensation energy. The experiment we discuss consists of taking a superconducting film, carefully measuring its transition temperature, bringing a conducting plate close to the film, creating a Casimir cavity, and then measuring the transition temperature again. The expected shifts will be small, comparable to the normal shifts one sees in cycling superconducting films to cryogenic temperatures and so using a NEMS resonator and doing this in situ is the only practical way to obtain accurate, reproducible data. Using a thin Pb film and opposing Au surface, we observe no shift in T_c greater than 12 𝜇K down to a minimum spacing of approximately 70 nm."
DAVID CAMPBELL,Editorial: The pre-history of Chaos—An Interdisciplinary Journal of Nonlinear Science,
DAVID CAMPBELL,Piecewise Linear Models for the Quasiperiodic Transition to Chaos,"We formulate and study analytically and computationally two families of piecewise linear degree one circle maps. These families offer the rare advantage of being non-trivial but essentially solvable models for the phenomenon of mode-locking and the quasi-periodic transition to chaos. For instance, for these families, we obtain complete solutions to several questions still largely unanswered for families of smooth circle maps. Our main results describe (1) the sets of maps in these families having some prescribed rotation interval; (2) the boundaries between zero and positive topological entropy and between zero length and non-zero length rotation interval; and (3) the structure and bifurcations of the attractors in one of these families. We discuss the interpretation of these maps as low-order spline approximations to the classic ``sine-circle'' map and examine more generally the implications of our results for the case of smooth circle maps. We also mention a possible connection to recent experiments on models of a driven Josephson junction."
DAVID CAMPBELL,Quantum Monte Carlo in the Interaction Representation --- Application to a Spin-Peierls Model,"A quantum Monte Carlo algorithm is constructed starting from the standard perturbation expansion in the interaction representation. The resulting configuration space is strongly related to that of the Stochastic Series Expansion (SSE) method, which is based on a direct power series expansion of exp(-beta*H). Sampling procedures previously developed for the SSE method can therefore be used also in the interaction representation formulation. The new method is first tested on the S=1/2 Heisenberg chain. Then, as an application to a model of great current interest, a Heisenberg chain including phonon degrees of freedom is studied. Einstein phonons are coupled to the spins via a linear modulation of the nearest-neighbor exchange. The simulation algorithm is implemented in the phonon occupation number basis, without Hilbert space truncations, and is exact. Results are presented for the magnetic properties of the system in a wide temperature regime, including the T-->0 limit where the chain undergoes a spin-Peierls transition. Some aspects of the phonon dynamics are also discussed. The results suggest that the effects of dynamic phonons in spin-Peierls compounds such as GeCuO3 and NaV2O5 must be included in order to obtain a correct quantitative description of their magnetic properties, both above and below the dimerization temperature."
DAVID CAMPBELL,Possible Exotic Phases in the One-Dimensional Extended Hubbard Model,"We investigate numerically the ground state phase diagram of the one-dimensional extended Hubbard model, including an on--site interaction U and a nearest--neighbor interaction V. We focus on the ground state phases of the model in the V >> U region, where previous studies have suggested the possibility of dominant superconducting pairing fluctuations before the system phase separates at a critical value V=V_PS. Using quantum Monte Carlo methods on lattices much larger than in previous Lanczos diagonalization studies, we determine the boundary of phase separation, the Luttinger Liquid correlation exponent K_rho, and other correlation functions in this region. We find that phase separation occurs for V significantly smaller than previously reported. In addition, for negative U, we find that a uniform state re-enters from phase separation as the electron density is increased towards half filling. For V < V_PS, our results show that superconducting fluctuations are not dominant. The system behaves asymptotically as a Luttinger Liquid with K_rho < 1, but we also find strong low-energy (but gapped) charge-density fluctuations at a momentum not expected for a standard Luttinger Liquid."
DAVID CAMPBELL,Fractional and unquantized dc voltage generation in THz-driven semiconductor superlattices,"We consider the spontaneous creation of a dc voltage across a strongly coupled semiconductor superlattice subjected to THz radiation. We show that the dc voltage may be approximately proportional either to an integer or to a half- integer multiple of the frequency of the applied ac field, depending on the ratio of the characteristic scattering rates of conducting electrons. For the case of an ac field frequency less than the characteristic scattering rates, we demonstrate the generation of an unquantized dc voltage."
DAVID CAMPBELL,Dissipative Chaos in Semiconductor Superlattices,"We consider the motion of ballistic electrons in a miniband of a semiconductor superlattice (SSL) under the influence of an external, time-periodic electric field. We use the semi-classical balance-equation approach which incorporates elastic and inelastic scattering (as dissipation) and the self-consistent field generated by the electron motion. The coupling of electrons in the miniband to the self-consistent field produces a cooperative nonlinear oscillatory mode which, when interacting with the oscillatory external field and the intrinsic Bloch-type oscillatory mode, can lead to complicated dynamics, including dissipative chaos. For a range of values of the dissipation parameters we determine the regions in the amplitude-frequency plane of the external field in which chaos can occur. Our results suggest that for terahertz external fields of the amplitudes achieved by present-day free electron lasers, chaos may be observable in SSLs. We clarify the nature of this novel nonlinear dynamics in the superlattice-external field system by exploring analogies to the Dicke model of an ensemble of two-level atoms coupled with a resonant cavity field and to Josephson junctions."
DAVID CAMPBELL,Dynamical Instabilities and Deterministic Chaos in Ballistic Electron Motion in Semiconductor Superlattices,We consider the motion of ballistic electrons within a superlattice miniband under the influence of an alternating electric field. We show that the interaction of electrons with the self-consistent electromagnetic field generated by the electron current may lead to the transition from regular to chaotic dynamics. We estimate the conditions for the experimental observation of this deterministic chaos and discuss the similarities of the superlattice system with the other condensed matter and quantum optical systems.
DAVID CAMPBELL,Spontaneous DC Current Generation in a Resistively Shunted Semiconductor Superlattice Driven by a TeraHertz Field,"We study a resistively shunted semiconductor superlattice subject to a high-frequency electric field. Using a balance equation approach that incorporates the influence of the electric circuit, we determine numerically a range of amplitude and frequency of the ac field for which a dc bias and current are generated spontaneously and show that this region is likely accessible to current experiments. Our simulations reveal that the Bloch frequency corresponding to the spontaneous dc bias is approximately an integer multiple of the ac field frequency."
DAVID CAMPBELL,Forward and inverse design of kirigami via supervised autoencoder,"Machine learning (ML) methods have recently been used as forward solvers to predict the mechanical properties of composite materials. Here, we use a supervised autoencoder (SAE) to perform the inverse design of graphene kirigami, where predicting the ultimate stress or strain under tensile loading is known to be difficult due to nonlinear effects arising from the out-of-plane buckling. Unlike the standard autoencoder, our SAE is able not only to reconstruct cut configurations but also to predict the mechanical properties of graphene kirigami and classify the kirigami with either parallel or orthogonal cuts. By interpolating in the latent space of kirigami structures, the SAE is able to generate designs that mix parallel and orthogonal cuts, despite being trained independently on parallel or orthogonal cuts. Our method allows us to both identify alternate designs and predict, with reasonable accuracy, their mechanical properties, which is crucial for expanding the search space for materials design."
DAVID CAMPBELL,Norman Julius Zabusky OBITUARY,"Norman Julius Zabusky, who laid the foundations for several critical advancements in nonlinear science and experimental mathematics, died of idiopathic pulmonary fibrosis on 5 February 2018 in Beersheba, Israel. He also made fundamental contributions to computational fluid dynamics and advocated the importance of visualization in science."
DAVID CAMPBELL,Breakdown and behavior of Higher-Order-Fermi-Pasta-Ulam-Tsignou recurrences,"We numerically investigate the existence and stability of higher-order recurrences (HoRs), including super-recurrences, super-super-recurrences, etc., in the α and β Fermi-Pasta-Ulam-Tsingou (FPUT) lattices for initial conditions in the fundamental normal mode. Our results represent a considerable extension of the pioneering work of Tuck and Menzel on super-recurrences. For fixed lattice sizes, we observe and study apparent singularities in the periods of these HoRs, speculated to be caused by nonlinear resonances. Interestingly, these singularities depend very sensitively on the initial energy and the respective nonlinear parameters. Furthermore, we compare the mechanisms by which the super-recurrences in the two models breakdown as the initial energy and respective nonlinear parameters are increased. The breakdown of super-recurrences in the β-FPUT lattice is associated with the destruction of the so-called metastable state and thus with relaxation towards equilibrium. For the α-FPUT lattice, we find this is not the case and show that the super-recurrences break down while the lattice is still metastable and far from equilibrium. We close with comments on the generality of our results for different lattice sizes."
DAVID CAMPBELL,Dynamics of entanglement in a dissipative Bose-Hubbard dimer,"We study the connection between the semiclassical phase space of the Bose-Hubbard dimer and inherently quantum phenomena in this model, such as entanglement and dissipation-induced coherence. Near the semiclassical self-trapping fixed points, the dynamics of Einstein-Podolski-Rosen (EPR) entanglement and condensate fraction consists of beats among just three eigenstates. Since persistent EPR entangled states arise only in the neighborhood of these fixed points, our analysis explains essentially all of the entanglement dynamics in the system. We derive accurate analytical approximations by expanding about the strong-coupling limit; surprisingly, their realm of validity is nearly the entire parameter space for which the self-trapping fixed points exist. Finally, we show significant enhancement of entanglement can be produced by applying localized dissipation."
DAVID CAMPBELL,Symmetry-breaking and chaos in electron transport in semiconductor superlattices,"We study the motion of electrons in a single miniband of a semiconductor superlattice driven by THz electric field polarized along the growth direction. We work in the semiclassical balance-equation model, including different elastic and inelastic scattering rates, and incorporating the self-consistent electric field generated by electron motion. We explore regions of complex dynamics, which can include chaotic behaviour and symmetry-breaking. We estimate the magnitudes of dc current and dc voltage that spontaneously appear in regions of broken-symmetry for parameters characteristic of modern semiconductor superlattices. This work complements PRL 80(1998)2669 [ cond-mat/9709026 ]."
DAVID CAMPBELL,"Effects of dietary protein and fiber at breakfast on appetite, ad libitum energy intake at lunch, and neural responses to visual food stimuli in overweight adults","Increasing either protein or fiber at mealtimes has relatively modest effects on ingestive behavior. Whether protein and fiber have additive or interactive effects on ingestive behavior is not known. Fifteen overweight adults (5 female, 10 male; BMI: 27.1 ± 0.2 kg/m²; aged 26 ± 1 year) consumed four breakfast meals in a randomized crossover manner (normal protein (12 g) + normal fiber (2 g), normal protein (12 g) + high fiber (8 g), high protein (25 g) + normal fiber (2 g), high protein (25 g) + high fiber (8 g)). The amount of protein and fiber consumed at breakfast did not influence postprandial appetite or ad libitum energy intake at lunch. In the fasting-state, visual food stimuli elicited significant responses in the bilateral insula and amygdala and left orbitofrontal cortex. Contrary to our hypotheses, postprandial right insula responses were lower after consuming normal protein vs. high protein breakfasts. Postprandial responses in other a priori brain regions were not significantly influenced by protein or fiber intake at breakfast. In conclusion, these data do not support increasing dietary protein and fiber at breakfast as effective strategies for modulating neural reward processing and acute ingestive behavior in overweight adults."
DAVID CAMPBELL,Adiabatic eigenstate deformations as a sensitive probe for quantum chaos,"In the past decades, it was recognized that quantum chaos, which is essential for the emergence of statistical mechanics and thermodynamics, manifests itself in the effective description of the eigenstates of chaotic Hamiltonians through random matrix ensembles and the eigenstate thermalization hypothesis. Standard measures of chaos in quantum many-body systems are level statistics and the spectral form factor. In this work, we show that the norm of the adiabatic gauge potential, the generator of adiabatic deformations between eigenstates, serves as a much more sensitive measure of quantum chaos. We are able to detect transitions from integrable to chaotic behavior at perturbation strengths orders of magnitude smaller than those required for standard measures. Using this alternative probe in two generic classes of spin chains, we show that the chaotic threshold decreases exponentially with system size and that one can immediately detect integrability-breaking (chaotic) perturbations by analyzing infinitesimal perturbations even at the integrable point. In some cases, small integrability breaking is shown to lead to anomalously slow relaxation of the system, exponentially long in system size."
DAVID CAMPBELL,Floquet-engineered quantum state manipulation in a noisy qubit,"Adiabatic evolution is a common strategy for manipulating quantum states and has been employed in diverse fields such as quantum simulation, computation and annealing. However, adiabatic evolution is inherently slow and therefore susceptible to decoherence. Existing methods for speeding up adiabatic evolution require complex many-body operators or are difficult to construct for multi-level systems. Using the tools of Floquet engineering, we design a scheme for high-fidelity quantum state manipulation, utilizing only the interactions available in the original Hamiltonian. We apply this approach to a qubit and experimentally demonstrate its performance with the electronic spin of a Nitrogen-vacancy center in diamond. Our Floquet-engineered protocol achieves state preparation fidelity of $0.994 \pm 0.004$, on the same level as the conventional fast-forward protocol, but is more robust to external noise acting on the qubit. Floquet engineering provides a powerful platform for high-fidelity quantum state manipulation in complex and noisy quantum systems."
DAVID CAMPBELL,Bond-order-wave phase and quantum phase transitions in the one-dimensional extended Hubbard model,"We use a stochastic series-expansion quantum Monte Carlo method to study the phase diagram of the one-dimensional extended Hubbard model at half-filling for small to intermediate values of the on-site U and nearest-neighbor V repulsions. We confirm the existence of a novel, long-range-ordered bond-order-wave (BOW) phase recently predicted by Nakamura [J. Phys. Soc. Jpn. 68, 3123 (1999)] in a small region of the parameter space between the familiar charge-density-wave (CDW) state for V≳U/2 and the state with dominant spin-density-wave (SDW) fluctuations for V≲U/2. We discuss the nature of the transitions among these states and evaluate some of the critical exponents. Further, we determine accurately the position of the multicritical point, (Um,Vm)=(4.7±0.1,2.51±0.04) (in energy units where the hopping integral is normalized to unity), above which the two continuous SDW-BOW-CDW transitions are replaced by one discontinuous (first-order) direct SDW-CDW transition. We also discuss the evolution of the CDW and BOW states upon hole doping. We find that in both cases the ground state is a Luther-Emery liquid, i.e., the spin gap remains but the charge gap existing at half-filling is immediately closed upon doping. The charge and bond-order correlations decay with distance r as r−ᴷρ, where Kρ is approximately 0.5 for the parameters we have considered. We also discuss advantages of using parallel tempering (or exchange Monte Carlo)—an extended ensemble method that we here combine with quantum Monte Carlo—in studies of quantum phase transitions."
DAVID CAMPBELL,Kirigami actuators,"Thin elastic sheets bend easily and, if they are patterned with cuts, can deform in sophisticated ways. Here we show that carefully tuning the location and arrangement of cuts within thin sheets enables the design of mechanical actuators that scale down to atomically-thin 2D materials. We first show that by understanding the mechanics of a single non-propagating crack in a sheet, we can generate four fundamental forms of linear actuation: roll, pitch, yaw, and lift. Our analytical model shows that these deformations are only weakly dependent on thickness, which we confirm with experiments on centimeter-scale objects and molecular dynamics simulations of graphene and MoS₂ nanoscale sheets. We show how the interactions between non-propagating cracks can enable either lift or rotation, and we use a combination of experiments, theory, continuum computational analysis, and molecular dynamics simulations to provide mechanistic insights into the geometric and topological design of kirigami actuators."
DAVID CAMPBELL,Spin-Peierls transition in the Heisenberg chain with finite-frequency phonons,"We study the spin-Peierls transition in a Heisenberg spin chain coupled to optical bond phonons. Quantum Monte Carlo results for systems with up to N=256 spins show unambiguously that the transition occurs only when the spin-phonon coupling α exceeds a critical value α_c. Using sum rules, we show that the phonon spectral function has divergent (for N→∞) weight extending to zero frequency for α<α_c. The phonon correlations decay with distance r as 1/r. This behavior is characteristic for all 0<α<α_c and the q=π phonon does not soften (to zero frequency) at α=α_c."
DAVID CAMPBELL,"Comment on ""Ground-state phase diagram of a half-filled one-dimensional extended Hubbard model""","In Phys. Rev. Lett. 89, 236401 (2002), Jeckelmann argued that the recently discovered bond-order-wave (BOW) phase of the 1D extended Hubbard model does not have a finite extent in the (U,V) plane, but exists only on a segment of a first-order SDW-CDW phase boundary. We here present quantum Monte Carlo result of higher precision and for larger system sizes than previously and reconfirm that the BOW phase does exist a finite distance away from the phase boundary, which hence is a BOW-CDW transition curve."
DAVID CAMPBELL,Atomistic simulations of tension-induced large deformation and stretchability in graphene kirigami,"Graphene's exceptional mechanical properties, including its highest-known stiffness (1 TPa) and strength (100 GPa), have been exploited for various structural applications. However, graphene is also known to be quite brittle, with experimentally measured tensile fracture strains that do not exceed a few percent. In this work, we introduce the notion of graphene kirigami, where concepts that have been used almost exclusively for macroscale structures are applied to dramatically enhance the stretchability of both zigzag and armchair graphene. Specifically, we show using classical molecular-dynamics simulations that the yield and fracture strains of graphene can be enhanced by about a factor of 3 using kirigami as compared to standard monolayer graphene. Finally, we demonstrate that this enhanced ductility in graphene may open up interesting opportunities in coupling to graphene's electronic behavior."
DAVID CAMPBELL,Functional renormalization group analysis of the half-filled one-dimensional extended Hubbard model,"We study the phase diagram of the half-filled one-dimensional extended Hubbard model at weak coupling using a novel functional renormalization group (FRG) approach. The FRG method includes in a systematic manner the effects of the scattering processes involving electrons away from the Fermi points. Our results confirm the existence of a finite region of bond charge density wave, also known as a ""bond order wave"" near U=2V and clarify why earlier g-ology calculations have not found this phase. We argue that this is an example in which formally irrelevant corrections change the topology of the phase diagram. Whenever marginal terms lead to an accidental symmetry, this generalized FRG method may be crucial to characterize the phase diagram accurately."
DAVID CAMPBELL,Ground state phases of the half-filled one-dimensional extended Hubbard model,"Using quantum Monte Carlo simulations, results of a strong-coupling expansion, and Luttinger liquid theory, we determine quantitatively the ground state phase diagram of the one-dimensional extended Hubbard model with on-site and nearest-neighbor repulsions U and V. We show that spin frustration stabilizes a bond-ordered (dimerized) state for U≈V/2 up to U/t≈9, where t is the nearest-neighbor hopping. The transition from the dimerized state to the staggered charge-density-wave state for large V/U is continuous for U≲5.5 and first order for higher U."
DAVID CAMPBELL,Renormalization-group approach to superconductivity: from weak to strong electron-phonon coupling,"We present the numerical solution of the renormalization group (RG) equations derived in Tsai, Castro Neto, Shankar, et al. [Phys. Rev. B to appear], for the problem of superconductivity in the presence of both electron–electron and electron–phonon coupling at zero temperature. We study the instability of a Fermi liquid to a superconductor and the RG flow of the couplings in the presence of retardation effects and the crossover from weak to strong coupling. We show that our numerical results provide an ansatz for the analytic solution to the problem in the asymptotic limits of weak and strong coupling."
DAVID CAMPBELL,Feedforward control algorithms for MEMS galvos and scanners,
DAVID CAMPBELL,Genome-wide characterization of microRNA and gene expression patterns in smoking-related lung disease,"Smoking-related lung diseases such as chronic obstructive pulmonary disease (COPD) and lung cancer are significant public health concerns world-wide. High throughput genomic technologies have opened up a new realm of understanding into the complexities of human disease by providing a means by which we can gain considerable amounts of information about a sample. In my research, I examine genome-wide gene expression via microarrays and microRNA expression via small RNA-sequencing (small RNA-Seq) to gain insights into lung disease pathogenesis, assess novel strategies for identifying therapeutics, and develop biomarkers for earlier diagnosis of disease. First, I revealed mechanisms of emphysema progression within individuals by leveraging a unique dataset that contains multiple lung-tissue samples per patient collected from regions with different levels of emphysematous destruction. Pathways involved in immune response and tissue remodeling were enriched among gene expression profiles associated with increasing regional emphysema severity. Using the Connectivity Map, a compound was discovered capable of reversing the gene-expression signature of increasing emphysema severity which can serve as a lead in therapeutic development for COPD. [TRUNCATED]"
DAVID CAMPBELL,Mitchell Feigenbaum: his life and legacy,
DAVID CAMPBELL,Counterdiabatic driving in the classical β-Fermi-Pasta-Ulam-Tsingou chain,"Shortcuts to adiabaticity (STAs) have been used to make rapid changes to a system while eliminating or minimizing excitations in the system's state. In quantum systems, these shortcuts allow us to minimize inefficiencies and heating in experiments and quantum computing protocols, but the theory of STAs can also be generalized to classical systems. We focus on one such STA, approximate counterdiabatic (ACD) driving, and numerically compare its performance in two classical systems: a quartic anharmonic oscillator and the β Fermi-Pasta-Ulam-Tsingou lattice. In particular, we modify an existing variational technique to optimize the approximate driving and then develop classical figures of merit to quantify the performance of the driving. We find that relatively simple forms for the ACD driving can dramatically suppress excitations regardless of system size. ACD driving in classical nonlinear oscillators could have many applications, from minimizing heating in bosonic gases to finding optimal local dressing protocols in interacting field theories."
DAVID CAMPBELL,Zeptometer metrology using the Casimir effect,"In this paper, we discuss using the Casimir force in conjunction with a MEMS parametric amplifier to construct a quantum displacement amplifier. Such a mechanical amplifier converts DC displacements into much larger AC oscillations via the quantum gain of the system which, in some cases, can be a factor of a million or more. This would allow one to build chip scale metrology systems with zeptometer positional resolution. This approach leverages quantum fluctuations to build a device with a sensitivity that can’t be obtained with classical systems."
DAVID CAMPBELL,Analysis of a Casimir-driven parametric amplifier with resilience to Casimir pull-in for MEMS single-point magnetic gradiometry,"The Casimir force, a quantum mechanical effect, has been observed in several microelectromechanical system (MEMS) platforms. Due to its extreme sensitivity to the separation of two objects, the Casimir force has been proposed as an excellent avenue for quantum metrology. Practical application, however, is challenging due to attractive forces leading to stiction and device failure, called Casimir pull-in. In this work, we design and simulate a Casimir-driven metrology platform, where a time-delay-based parametric amplification technique is developed to achieve a steady-state and avoid pull-in. We apply the design to the detection of weak, low-frequency, gradient magnetic fields similar to those emanating from ionic currents in the heart and brain. Simulation parameters are selected from recent experimental platforms developed for Casimir metrology and magnetic gradiometry, both on MEMS platforms. While a MEMS offers many advantages to such an application, the detected signal must typically be at the resonant frequency of the device, with diminished sensitivity in the low frequency regime of biomagnetic fields. Using a Casimir-driven parametric amplifier, we report a 10,000-fold improvement in the best-case resolution of MEMS single-point gradiometers, with a maximum sensitivity of 6 Hz/(pT/cm) at 1 Hz. Further development of the proposed design has the potential to revolutionize metrology and may specifically enable the unshielded monitoring of biomagnetic fields in ambient conditions."
DAVID CAMPBELL,"Strain-induced gauge and Rashba fields in ferroelectric Rashba lead chalcogenide PbX monolayers (X = S, Se, Te)","One of the exciting features of two-dimensional (2D) materials is their electronic and optical tunability through strain engineering. Previously, we found a class of 2D ferroelectric Rashba semiconductors PbX (X=S, Se, Te) with tunable spin-orbital properties. In this work, based on our previous tight-binding (TB) results, we derive an effective low-energy Hamiltonian around the symmetry points that captures the effects of strain on the electronic properties of PbX. We find that strains induce gauge fields which shift the Rashba point and modify the Rashba parameter. This effect is equivalent to the application of in-plane magnetic fields. The out-of-plane strain, which is proportional to the electric polarization, is also shown to modify the Rashba parameter. Overall, our theory connects strain and spin splitting in ferroelectric Rashba materials, which will be important to understand the strain-induced variations in local Rashba parameters that will occur in practical applications."
DAVID CAMPBELL,Two-dimensional square buckled Rashba lead chalcogenides,"We propose the lead sulphide (PbS) monolayer as a two-dimensional semiconductor with a large Rashba-like spin-orbit effect controlled by the out-of-plane buckling. The buckled PbS conduction band is found to possess Rashba-like dispersion and spin texture at the M and Γ points, with large effective Rashba parameters of λ∼5 eV Å and λ∼1 eV Å, respectively. Using a tight-binding formalism, we show that the Rashba effect originates from the very large spin-orbit interaction and the hopping term that mixes the in-plane and out-of-plane p orbitals of Pb and S atoms. The latter, which depends on the buckling angle, can be controlled by applying strain to vary the spin texture as well as the Rashba parameter at Γ and M. Our density functional theory results together with tight-binding formalism provide a unifying framework for designing Rashba monolayers and for manipulating their spin properties."
DAVID CAMPBELL,Accelerated search and design of stretchable graphene kirigami using machine learning,"Making kirigami-inspired cuts into a sheet has been shown to be an effective way of designing stretchable materials with metamorphic properties where the 2D shape can transform into complex 3D shapes. However, finding the optimal solutions is not straightforward as the number of possible cutting patterns grows exponentially with system size. Here, we report on how machine learning (ML) can be used to approximate the target properties, such as yield stress and yield strain, as a function of cutting pattern. Our approach enables the rapid discovery of kirigami designs that yield extreme stretchability as verified by molecular dynamics (MD) simulations. We find that convolutional neural networks, commonly used for classification in vision tasks, can be applied for regression to achieve an accuracy close to the precision of the MD simulations. This approach can then be used to search for optimal designs that maximize elastic stretchability with only 1000 training samples in a large design space of ∼4×106 candidate designs. This example demonstrates the power and potential of ML in finding optimal kirigami designs at a fraction of iterations that would be required of a purely MD or experiment-based approach, where no prior knowledge of the governing physics is known or available."
DAVID CAMPBELL,Graphene kirigami as a platform for stretchable and tunable quantum dot arrays,"The quantum transport properties of a graphene kirigami similar to those studied in recent experiments are calculated in the regime of elastic, reversible deformations. Our results show that, at low electronic densities, the conductance profile of such structures replicates that of a system of coupled quantum dots, characterized by a sequence of minibands and stopgaps. The conductance and I-V curves have different characteristics in the distinct stages of deformation that characterize the elongation of these structures. Notably, the effective coupling between localized states is strongly reduced in the small elongation stage but revived at large elongations that allow the reestablishment of resonant tunneling across the kirigami. This provides an interesting example of interplay between geometry, strain, spatial confinement, and electronic transport. The alternating miniband and stopgap structure in the transmission leads to I-V characteristics with negative differential conductance in well defined energy/doping ranges. These effects should be stable in a realistic scenario that includes edge roughness and Coulomb interactions, as these are expected to further promote localization of states at low energies in narrow segments of graphene nanostructures."
DAVID CAMPBELL,Highly stretchable MoS2 kirigami,"We report the results of classical molecular dynamics simulations focused on studying the mechanical properties of MoS2 kirigami. Several different kirigami structures were studied based upon two simple non-dimensional parameters, which are related to the density of cuts, as well as the ratio of the overlapping cut length to the nanoribbon length. Our key findings are significant enhancements in tensile yield (by a factor of four) and fracture strains (by a factor of six) as compared to pristine MoS2 nanoribbons. These results, in conjunction with recent results on graphene, suggest that the kirigami approach may be generally useful for enhancing the ductility of two-dimensional nanomaterials."
DAVID CAMPBELL,Transport properties of pristine few-layer black phosphorus by van der Waals passivation in an inert atmosphere,"Ultrathin black phosphorus is a two-dimensional semiconductor with a sizeable band gap. Its excellent electronic properties make it attractive for applications in transistor, logic and optoelectronic devices. However, it is also the first widely investigated two-dimensional material to undergo degradation upon exposure to ambient air. Therefore a passivation method is required to study the intrinsic material properties, understand how oxidation affects the physical properties and enable applications of phosphorene. Here we demonstrate that atomically thin graphene and hexagonal boron nitride can be used for passivation of ultrathin black phosphorus. We report that few-layer pristine black phosphorus channels passivated in an inert gas environment, without any prior exposure to air, exhibit greatly improved n-type charge transport resulting in symmetric electron and hole transconductance characteristics."
DAVID CAMPBELL,The mesoscopic magnetron as an open quantum system,"Motivated by the emergence of materials with mean free paths on the order of microns, we propose a novel class of solid state radiation sources based on reimplementing classical vacuum tube designs in semiconductors. Using materials with small effective masses, these devices should be able to access the terahertz range. We analyze the DC and AC operation of the simplest such device, the cylindrical diode magnetron, using effective quantum models. By treating the magnetron as an open quantum system, we show that it continues to operate as a radiation source even if its diameter is only a few tens of magnetic lengths."
DAVID CAMPBELL,A study of the effect of the father's absence upon family life during a boy's early years.,
DAVID CAMPBELL,Effects of finite-range interactions on the one-electron spectral properties of one-dimensional metals: application to Bi/InSb(001),"We study the one-electron spectral properties of one-dimensional interacting electron systems in which the interactions have finite range. We employ a mobile quantum impurity scheme that describes the interactions of the fractionalized excitations at energies above the standard Tomonga-Luttinger liquid limit and show that the phase shifts induced by the impurity describe universal properties of the one-particle spectral function. We find the explicit forms in terms of these phase shifts for the momentum dependent exponents that control the behavior of the spectral function near and at the (k,ω)-plane singularities where most of the spectral weight is located. The universality arises because the line shape near the singularities is independent of the short-distance part of the interaction potentials. For the class of potentials considered here, the charge fractionalized particles have screened Coulomb interactions that decay with a power-law exponent l>5. We apply the theory to the angle-resolved photo-electron spectroscopy (ARPES) in the highly one-dimensional bismuth-induced anisotropic structure on indium antimonide Bi/InSb(001). Our theoretical predictions agree quantitatively with both (i) the experimental value found in Bi/InSb(001) for the exponent α that controls the suppression of the density of states at very small excitation energy ω and (ii) the location in the (k,ω) plane of the experimentally observed high-energy peaks in the ARPES momentum and energy distributions. We conclude with a discussion of experimental properties beyond the range of our present theoretical framework and further open questions regarding the one-electron spectral properties of Bi/InSb(001)."
DAVID CAMPBELL,Effects of finite-range interactions on the one-electron spectral properties of TTF-TCNQ,"The electronic dispersions of the quasi-one-dimensional organic conductor TTF-TCNQ are studied by angle-resolved photoelectron spectroscopy (ARPES) with higher angular resolution and accordingly smaller step width than in previous studies. Our experimental results suggest that a refinement of the single-band 1D Hubbard model that includes finite-range interactions is needed to explain these photoemission data. To account for the effects of these finite-range interactions we employ a mobile quantum impurity scheme that describes the scattering of fractionalized particles at energies above the standard Tomonaga-Luttinger liquid limit. Our theoretical predictions agree quantitatively with the location in the (k,ω) plane of the experimentally observed ARPES structures at these higher energies. The nonperturbative microscopic mechanisms that control the spectral properties are found to simplify in terms of the exotic scattering of the charge fractionalized particles. We find that the scattering occurs in the unitary limit of (minus) infinite scattering length, which limit occurs within neutron-neutron interactions in shells of neutron stars and in the scattering of ultracold atoms but not in perturbative electronic condensed-matter systems. Our results provide important physical information on the exotic processes involved in the finite-range electron interactions that control the high-energy spectral properties of TTF-TCNQ. Our results also apply to a wider class of 1D and quasi-1D materials and systems that are of theoretical and potential technological interest."
DAVID CAMPBELL,The β Fermi-Pasta-Ulam-Tsingou recurrence problem,"We perform a thorough investigation of the first Fermi-Pasta-Ulam-Tsingou (FPUT) recurrence in the β-FPUT chain for both positive and negative β. We show numerically that the rescaled FPUT recurrence time Tr=tr/(N+1)3 depends, for large N, only on the parameter S≡Eβ(N+1). Our numerics also reveal that for small |S|, Tr is linear in S with positive slope for both positive and negative β. For large |S|, Tr is proportional to |S|-1/2 for both positive and negative β but with different multiplicative constants. We numerically study the continuum limit and find that the recurrence time closely follows the |S|-1/2 scaling and can be interpreted in terms of solitons, as in the case of the KdV equation for the α chain. The difference in the multiplicative factors between positive and negative β arises from soliton-kink interactions that exist only in the negative β case. We complement our numerical results with analytical considerations in the nearly linear regime (small |S|) and in the highly nonlinear regime (large |S|). For the former, we extend previous results using a shifted-frequency perturbation theory and find a closed form for Tr that depends only on S. In the latter regime, we show that Tr∝|S|-1/2 is predicted by the soliton theory in the continuum limit. We then investigate the existence of the FPUT recurrences and show that their disappearance surprisingly depends only on Eβ for large N, not S. Finally, we end by discussing the striking differences in the amount of energy mixing between positive and negative β and offer some remarks on the thermodynamic limit."
DAVID CAMPBELL,Dynamical glass in weakly nonintegrable Klein-Gordon chains,"Integrable many-body systems are characterized by a complete set of preserved actions. Close to an integrable limit, a nonintegrable perturbation creates a coupling network in action space which can be short or long ranged. We analyze the dynamics of observables which become the conserved actions in the integrable limit. We compute distributions of their finite time averages and obtain the ergodization time scale T_{E} on which these distributions converge to δ distributions. We relate T_{E} to the statistics of fluctuation times of the observables, which acquire fat-tailed distributions with standard deviations σ_{τ}^{+} dominating the means μ_{τ}^{+} and establish that T_{E}∼(σ_{τ}^{+})^{2}/μ_{τ}^{+}. The Lyapunov time T_{Λ} (the inverse of the largest Lyapunov exponent) is then compared to the above time scales. We use a simple Klein-Gordon chain to emulate long- and short-range coupling networks by tuning its energy density. For long-range coupling networks T_{Λ}≈σ_{τ}^{+}, which indicates that the Lyapunov time sets the ergodization time, with chaos quickly diffusing through the coupling network. For short-range coupling networks we observe a dynamical glass, where T_{E} grows dramatically by many orders of magnitude and greatly exceeds the Lyapunov time, which satisfies T_{Λ}≲μ_{τ}^{+}. This effect arises from the formation of highly fragmented inhomogeneous distributions of chaotic groups of actions, separated by growing volumes of nonchaotic regions. These structures persist up to the ergodization time T_{E}."
DAVID CAMPBELL,A system for probing Casimir energy corrections to the condensation energy,"In this article, we present a nanoelectromechanical system (NEMS) designed to detect changes in the Casimir energy. The Casimir effect is a result of the appearance of quantum fluctuations in an electromagnetic vacuum. Previous experiments have used nano- or microscale parallel plate capacitors to detect the Casimir force by measuring the small attractive force these fluctuations exert between the two surfaces. In this new set of experiments, we aim to directly detect the shifts in the Casimir energy in a vacuum due to the presence of the metallic parallel plates, one of which is a superconductor. A change in the Casimir energy of this configuration is predicted to shift the superconducting transition temperature (Tc) because of the interaction between it and the superconducting condensation energy. In our experiment, we take a superconducting film, carefully measure its transition temperature, bring a conducting plate close to the film, create a Casimir cavity, and then measure the transition temperature again. The expected shifts are smaller than the normal shifts one sees in cycling superconducting films to cryogenic temperatures, so using a NEMS resonator in situ is the only practical way to obtain accurate, reproducible data. Using a thin Pb film and opposing Au surface, we observe no shift in Tc >12 µK down to a minimum spacing of ~70 nm at zero applied magnetic field."
DAVID CAMPBELL,NFIA Haploinsufficiency Is Associated with a CNS Malformation Syndrome and Urinary Tract Defects,"Complex central nervous system (CNS) malformations frequently coexist with other developmental abnormalities, but whether the associated defects share a common genetic basis is often unclear. We describe five individuals who share phenotypically related CNS malformations and in some cases urinary tract defects, and also haploinsufficiency for the NFIA transcription factor gene due to chromosomal translocation or deletion. Two individuals have balanced translocations that disrupt NFIA. A third individual and two half-siblings in an unrelated family have interstitial microdeletions that include NFIA. All five individuals exhibit similar CNS malformations consisting of a thin, hypoplastic, or absent corpus callosum, and hydrocephalus or ventriculomegaly. The majority of these individuals also exhibit Chiari type I malformation, tethered spinal cord, and urinary tract defects that include vesicoureteral reflux. Other genes are also broken or deleted in all five individuals, and may contribute to the phenotype. However, the only common genetic defect is NFIA haploinsufficiency. In addition, previous analyses of Nfia−/− knockout mice indicate that Nfia deficiency also results in hydrocephalus and agenesis of the corpus callosum. Further investigation of the mouse Nfia+/− and Nfia−/− phenotypes now reveals that, at reduced penetrance, Nfia is also required in a dosage-sensitive manner for ureteral and renal development. Nfia is expressed in the developing ureter and metanephric mesenchyme, and Nfia+/− and Nfia−/− mice exhibit abnormalities of the ureteropelvic and ureterovesical junctions, as well as bifid and megaureter. Collectively, the mouse Nfia mutant phenotype and the common features among these five human cases indicate that NFIA haploinsufficiency contributes to a novel human CNS malformation syndrome that can also include ureteral and renal defects. Author Summary Central nervous system (CNS) and urinary tract abnormalities are common human malformations, but their variability and genetic complexity make it difficult to identify the responsible genes. Analysis of human chromosomal abnormalities associated with such disorders offers one approach to this problem. In five individuals described herein, a novel human syndrome that involves both CNS and urinary tract defects is associated with chromosomal disruption or deletion of NFIA, encoding a member of the Nuclear Factor I (NFI) family of transcription factors. This syndrome includes brain abnormalities (abnormal corpus callosum, hydrocephalus, ventriculomegaly, and Chiari type I malformation), spinal abnormalities (tethered spinal cord), and urinary tract abnormalities (vesicoureteral reflux). Nfia disruption in mice was already known to cause hydrocephalus and abnormal corpus callosum, and is now shown to exhibit renal defects and disturbed ureteral development. Other genes besides NFIA are also disrupted or deleted and may contribute to the observed phenotype. However, loss of one copy of NFIA is the only genetic defect common to all five patients. The authors thus provide evidence that genetic loss of NFIA contributes to a distinct CNS malformation syndrome with urinary tract defects of variable penetrance."
DAVID CAMPBELL,Prelude and fugue and related forms in German organ music from 1640-1717,
DAVID CAMPBELL,Quantum Monte Carlo in the interaction representation: Application to a spin-Peierls model,"A quantum Monte Carlo algorithm is constructed starting from the standard perturbation expansion in the interaction representation. The resulting configuration space is strongly related to that of the stochastic series expansion (SSE) method, which is based on a direct power-series expansion of exp(−βH). Sampling procedures previously developed for the SSE method can therefore be used also in the interaction representation formulation. The method is tested on the S=1/2 Heisenberg chain. Then, as an application to a model of great current interest, a Heisenberg chain including phonon degrees of freedom is studied. Einstein phonons are coupled to the spins via a linear modulation of the nearest-neighbor exchange. The simulation algorithm is implemented in the phonon occupation-number basis, without Hilbert space truncations, and is exact. Results are presented for the magnetic properties of the system in a wide temperature regime, including the T→0 limit where the chain undergoes a spin-Peierls transition. Some aspects of the phonon dynamics are also discussed. The results suggest that the effects of dynamic phonons in spin-Peierls compounds such as GeCuO₃ and α′−NaV₂O₅ must be included in order to obtain a correct quantitative description of their magnetic properties, both above and below the dimerization temperature."
DAVID CAMPBELL,Tunneling in the self-trapped regime of a two-well Bose-Einstein condensate,"Starting from a mean-field model of the Bose-Einstein condensate dimer, we reintroduce classically forbidden tunneling through a Bohr-Sommerfeld quantization approach. We find closed-form approximations to the tunneling frequency more accurate than those previously obtained using different techniques. We discuss the central role that tunneling in the self-trapped regime plays in a quantitatively accurate model of a dissipative dimer leaking atoms to the environment. Finally, we describe the prospects of experimental observation of tunneling in the self-trapped regime, both with and without dissipation."
DAVID CAMPBELL,Behavior and breakdown of higher-order Fermi-Pasta-Ulam-Tsingou recurrences,"We numerically investigate the existence and stability of higher-order recurrences (HoRs), including super-recurrences, super-super-recurrences, etc., in the α and β Fermi-Pasta-Ulam-Tsingou (FPUT) lattices for initial conditions in the fundamental normal mode. Our results represent a considerable extension of the pioneering work of Tuck and Menzel on super-recurrences. For fixed lattice sizes, we observe and study apparent singularities in the periods of these HoRs, speculated to be caused by nonlinear resonances. Interestingly, these singularities depend very sensitively on the initial energy and the respective nonlinear parameters. Furthermore, we compare the mechanisms by which the super-recurrences in the two models breakdown as the initial energy and respective nonlinear parameters are increased. The breakdown of super-recurrences in the β-FPUT lattice is associated with the destruction of the so-called metastable state and thus with relaxation towards equilibrium. For the α -FPUT lattice, we find this is not the case and show that the super-recurrences break down while the lattice is still metastable and far from equilibrium. We close with comments on the generality of our results for different lattice sizes."
DAVID CAMPBELL,Vanishing spin stiffness in the spin-1/2 Heisenberg chain for any nonzero temperature,"Whether at the zero spin density m = 0 and finite temperatures T > 0 the spin stiffness of the spin- 1 / 2 X X X chain is finite or vanishes remains an unsolved and controversial issue, as different approaches yield contradictory results. Here we explicitly compute the stiffness at m = 0 and find strong evidence that it vanishes. In particular, we derive an upper bound on the stiffness within a canonical ensemble at any fixed value of spin density m that is proportional to m 2 L in the thermodynamic limit of chain length L → ∞, for any finite, nonzero temperature, which implies the absence of ballistic transport for T > 0 for m = 0. Although our method relies in part on the thermodynamic Bethe ansatz (TBA), it does not evaluate the stiffness through the second derivative of the TBA energy eigenvalues relative to a uniform vector potential. Moreover, we provide strong evidence that in the thermodynamic limit the upper bounds on the spin current and stiffness used in our derivation remain valid under string deviations. Our results also provide strong evidence that in the thermodynamic limit the TBA method used by X. Zotos [Phys. Rev. Lett. 82, 1764 (1999)] leads to the exact stiffness values at finite temperature T > 0 for models whose stiffness is finite at T = 0, similar to the spin stiffness of the spin- 1 / 2 Heisenberg chain but unlike the charge stiffness of the half-filled 1D Hubbard model."
DAVID CAMPBELL,Singularities of the dynamical structure factors of the spin-1/2 XXX chain at finite magnetic field,"We study the longitudinal and transverse spin dynamical structure factors of the spin-1/2 XXX chain at finite magnetic field h, focusing in particular on the singularities at excitation energies in the vicinity of the lower thresholds. While the static properties of the model can be studied within a Fermi-liquid like description in terms of pseudoparticles, our derivation of the dynamical properties relies on the introduction of a form of the 'pseudofermion dynamical theory' (PDT) of the 1D Hubbard model suitably modified for the spin-only XXX chain and other models with two pseudoparticle Fermi points. Specifically, we derive the exact momentum and spin-density dependences of the exponents ${{\zeta}^{\tau}}(k)$ controlling the singularities for both the longitudinal $\left(\tau =l\right)$ and transverse $\left(\tau =t\right)$ dynamical structure factors for the whole momentum range $k\in ]0,\pi[$ , in the thermodynamic limit. This requires the numerical solution of the integral equations that define the phase shifts in these exponents expressions. We discuss the relation to neutron scattering and suggest new experiments on spin-chain compounds using a carefully oriented crystal to test our predictions."
DAVID CAMPBELL,Pseudoparticle description of the 1D Hubbard model electronic transport properties,We extend the pseudoparticle transport description of the Hubbard chain to all energy scales. In particular we compute the mean value of the electric current transported by any Bethe ansatz state and the transport masses of the charge carriers. We present numerical results for the optical conductivity of the model at half-filling for values of U/t = 3 and 4. We show that these are in good agreement with the pseudoparticle description of the finite-energy transitions involving new pseudoparticle energy bands.
DAVID CAMPBELL,Corrigendum: singularities of the dynamical structure factors of the spin 1/2 XXX chain at finite magnetic field (2015 J. Phys.: Condens. Matter 27 406001),
DAVID CAMPBELL,Possible exotic phases in the one-dimensional extended Hubbard model,"We investigate numerically the ground state phase diagram of the one-dimensional extended Hubbard model, including an on-site interaction U and a nearest-neighbor interaction V. We focus on the ground state phases of the model in the V≫U region, where previous studies have suggested the possibility of dominant superconducting pairing fluctuations before the system phase separates at a critical value V=V𝘱𝒔. Using quantum Monte Carlo methods on lattices much larger than in previous Lanczös diagonalization studies, we determine the boundary of phase separation, the Luttinger-liquid correlation exponent Kρ, and other correlation functions in this region. We find that phase separation occurs for V significantly smaller than previously reported. In addition, for negative U, we find that a uniform state reenters from phase separation as the electron density is increased towards half filling. For V<V𝘱𝑠, our results show that superconducting fluctuations are not dominant. The system behaves asymptotically as a Luttinger liquid with Kρ<1, but we also find strong low-energy (but gapped) charge-density fluctuations at a momentum not expected for a standard Luttinger liquid."
DAVID CAMPBELL,XXVI IUPAP Conference on Computational Physics (CCP2014),"The 26th IUPAP Conference on Computational Physics, CCP2014, was held in Boston, Massachusetts, during August 11-14, 2014. Almost 400 participants from 38 countries convened at the George Sherman Union at Boston University for four days of plenary and parallel sessions spanning a broad range of topics in computational physics and related areas. The first meeting in the series that developed into the annual Conference on Computational Physics (CCP) was held in 1989, also on the campus of Boston University and chaired by our colleague Claudio Rebbi. The express purpose of that meeting was to discuss the progress, opportunities and challenges of common interest to physicists engaged in computational research. The conference having returned to the site of its inception, it is interesting to recect on the development of the field during the intervening years. Though 25 years is a short time for mankind, computational physics has taken giant leaps during these years, not only because of the enormous increases in computer power but especially because of the development of new methods and algorithms, and the growing awareness of the opportunities the new technologies and methods can offer. Computational physics now represents a ''third leg'' of research alongside analytical theory and experiments in almost all subfields of physics, and because of this there is also increasing specialization within the community of computational physicists. It is therefore a challenge to organize a meeting such as CCP, which must have suffcient depth in different areas to hold the interest of experts while at the same time being broad and accessible. Still, at a time when computational research continues to gain in importance, the CCP series is critical in the way it fosters cross-fertilization among fields, with many participants specifically attending in order to get exposure to new methods in fields outside their own. As organizers and editors of these Proceedings, we are very pleased with the high quality of the papers provided by the participants. These articles represent a good cross-section of what was presented at the meeting, and it is our hope that they will not only be useful individually for their specific scientific content but will also represent a historical snapshot of the state of computational physics that they represent collectively. The remainder of this Preface contains lists detailing the organizational structure of CCP2014, endorsers and sponsors of the meeting, plenary and invited talks, and a presentation of the 2014 IUPAP C20 Young Scientist Prize. We would like to take the opportunity to again thank all those who contributed to the success of CCP214, as organizers, sponsors, presenters, exhibitors, and participants. Anders Sandvik, David Campbell, David Coker, Ying Tang"
DAVID CAMPBELL,Bulk brain tissue cell-type deconvolution with bias correction for single-nuclei RNA sequencing data using DeTREM,"BACKGROUND: Quantifying cell-type abundance in bulk tissue RNA-sequencing enables researchers to better understand complex systems. Newer deconvolution methodologies, such as MuSiC, use cell-type signatures derived from single-cell RNA-sequencing (scRNA-seq) data to make these calculations. Single-nuclei RNA-sequencing (snRNA-seq) reference data can be used instead of scRNA-seq data for tissues such as human brain where single-cell data are difficult to obtain, but accuracy suffers due to sequencing differences between the technologies. RESULTS: We propose a modification to MuSiC entitled 'DeTREM' which compensates for sequencing differences between the cell-type signature and bulk RNA-seq datasets in order to better predict cell-type fractions. We show DeTREM to be more accurate than MuSiC in simulated and real human brain bulk RNA-sequencing datasets with various cell-type abundance estimates. We also compare DeTREM to SCDC and CIBERSORTx, two recent deconvolution methods that use scRNA-seq cell-type signatures. We find that they perform well in simulated data but produce less accurate results than DeTREM when used to deconvolute human brain data. CONCLUSION: DeTREM improves the deconvolution accuracy of MuSiC and outperforms other deconvolution methods when applied to snRNA-seq data. DeTREM enables accurate cell-type deconvolution in situations where scRNA-seq data are not available. This modification improves characterization cell-type specific effects in brain tissue and identification of cell-type abundance differences under various conditions."
DAVID CAMPBELL,Dynamical symmetry breaking through AI: the dimer self-trapping transition,"The nonlinear dimer obtained through the nonlinear Schrödinger equation has been a workhorse for the discovery the role nonlinearity plays in strongly interacting systems. While the analysis of the stationary states demonstrates the onset of a symmetry broken state for some degree of nonlinearity, the full dynamics maps the system into an effective [Formula: see text] model. In this later context, the self-trapping transition is an initial condition-dependent transfer of a classical particle over a barrier set by the nonlinear term. This transition that has been investigated analytically and mathematically is expressed through the hyperbolic limit of Jacobian elliptic functions. The aim of this work is to recapture this transition through the use of methods of Artificial Intelligence (AI). Specifically, we used a physics motivated machine learning model that is shown to be able to capture the original dynamic self-trapping transition and its dependence on initial conditions. Exploitation of this result in the case of the nondegenerate nonlinear dimer gives additional information on the more general dynamics and helps delineate linear from nonlinear localization. This work shows how AI methods may be embedded in physics and provide useful tools for discovery."
DAVID CAMPBELL,Memory effects in monolayer group-IV monochalcoginides,"Group-IV monochalcogenides are a family of two-dimensional puckered materials with an orthorhombic structure that is comprised of polar layers. In this article, we use first principles calculations to show the multistability of monolayer SnS and GeSe, two prototype materials where the direction of the puckering can be switched by application of tensile stress or electric field. Furthermore, the two inequivalent valleys in momentum space, which are dictated by the puckering orientation, can be excited selectively using linearly polarized light, and this provides an additional tool to identify the polarization direction. Our findings suggest that SnS and GeSe monolayers may have observable ferroelectricity and multistability, with potential applications in information storage."
DAVID CAMPBELL,Intermittent many-body dynamics at equilibrium,"The equilibrium value of an observable defines a manifold in the phase space of an ergodic and equipartitioned many-body system. A typical trajectory pierces that manifold infinitely often as time goes to infinity. We use these piercings to measure both the relaxation time of the lowest frequency eigenmode of the Fermi-Pasta-Ulam chain, as well as the fluctuations of the subsequent dynamics in equilibrium. The dynamics in equilibrium is characterized by a power-law distribution of excursion times far off equilibrium, with diverging variance. Long excursions arise from sticky dynamics close to q-breathers localized in normal mode space. Measuring the exponent allows one to predict the transition into nonergodic dynamics. We generalize our method to Klein-Gordon lattices where the sticky dynamics is due to discrete breathers localized in real space."
DAVID CAMPBELL,Rashba-like dispersion in buckled square lattices,The band structure of a general class of buckled square lattice materials is investigated using ab initio calculations along with tight-binding modeling. We show that buckling and spin-orbit interaction give rise to a large Rashba-like splitting in the absence of an external electric field. The generality and the robustness of the effect make this class of materials promising candidates for spintronic applications.
DAVID CAMPBELL,"The Mutational signature comprehensive analysis toolkit (musicatk) for the discovery, prediction, and exploration of mutational signatures","Mutational signatures are patterns of somatic alterations in the genome caused by carcinogenic exposures or aberrant cellular processes. To provide a comprehensive workflow for preprocessing, analysis, and visualization of mutational signatures, we created the Mutational Signature Comprehensive Analysis Toolkit (musicatk) package. musicatk enables users to select different schemas for counting mutation types and to easily combine count tables from different schemas. Multiple distinct methods are available to deconvolute signatures and exposures or to predict exposures in individual samples given a pre-existing set of signatures. Additional exploratory features include the ability to compare signatures to the Catalogue Of Somatic Mutations In Cancer (COSMIC) database, embed tumors in two dimensions with uniform manifold approximation and projection, cluster tumors into subgroups based on exposure frequencies, identify differentially active exposures between tumor subgroups, and plot exposure distributions across user-defined annotations such as tumor type. Overall, musicatk will enable users to gain novel insights into the patterns of mutational signatures observed in cancer cohorts. SIGNIFICANCE: The musicatk package empowers researchers to characterize mutational signatures and tumor heterogeneity with a comprehensive set of preprocessing utilities, discovery and prediction tools, and multiple functions for downstream analysis and visualization."
DAVID CAMPBELL,Celda: a Bayesian model to perform co-clustering of genes into modules and cells into subpopulations using single-cell RNA-seq data,"Single-cell RNA-seq (scRNA-seq) has emerged as a powerful technique to quantify gene expression in individual cells and to elucidate the molecular and cellular building blocks of complex tissues. We developed a novel Bayesian hierarchical model called Cellular Latent Dirichlet Allocation (Celda) to perform co-clustering of genes into transcriptional modules and cells into subpopulations. Celda can quantify the probabilistic contribution of each gene to each module, each module to each cell population and each cell population to each sample. In a peripheral blood mononuclear cell dataset, Celda identified a subpopulation of proliferating T cells and a plasma cell which were missed by two other common single-cell workflows. Celda also identified transcriptional modules that could be used to characterize unique and shared biological programs across cell types. Finally, Celda outperformed other approaches for clustering genes into modules on simulated data. Celda presents a novel method for characterizing transcriptional programs and cellular heterogeneity in scRNA-seq data."
DAVID CAMPBELL,Characterization and decontamination of background noise in droplet-based single-cell protein expression data with DecontPro,"Assays such as CITE-seq can measure the abundance of cell surface proteins on individual cells using antibody derived tags (ADTs). However, many ADTs have high levels of background noise that can obfuscate down-stream analyses. In an exploratory analysis of PBMC datasets, we find that some droplets that were originally called 'empty' due to low levels of RNA contained high levels of ADTs and likely corresponded to neutrophils. We identified a novel type of artifact in the empty droplets called a 'spongelet' which has medium levels of ADT expression and is distinct from ambient noise. ADT expression levels in the spongelets correlate to ADT expression levels in the background peak of true cells in several datasets suggesting that they can contribute to background noise along with ambient ADTs. We then developed DecontPro, a novel Bayesian hierarchical model that can decontaminate ADT data by estimating and removing contamination from these sources. DecontPro outperforms other decontamination tools in removing aberrantly expressed ADTs while retaining native ADTs and in improving clustering specificity. Overall, these results suggest that identification of empty drops should be performed separately for RNA and ADT data and that DecontPro can be incorporated into CITE-seq workflows to improve the quality of downstream analyses."
DAVID CAMPBELL,"Comprehensive generation, visualization, and reporting of quality control metrics for single-cell RNA sequencing data","Single-cell RNA sequencing (scRNA-seq) can be used to gain insights into cellular heterogeneity within complex tissues. However, various technical artifacts can be present in scRNA-seq data and should be assessed before performing downstream analyses. While several tools have been developed to perform individual quality control (QC) tasks, they are scattered in different packages across several programming environments. Here, to streamline the process of generating and visualizing QC metrics for scRNA-seq data, we built the SCTK-QC pipeline within the singleCellTK R package. The SCTK-QC workflow can import data from several single-cell platforms and preprocessing tools and includes steps for empty droplet detection, generation of standard QC metrics, prediction of doublets, and estimation of ambient RNA. It can run on the command line, within the R console, on the cloud platform or with an interactive graphical user interface. Overall, the SCTK-QC pipeline streamlines and standardizes the process of performing QC for scRNA-seq data."
DAVID CAMPBELL,Decontamination of ambient RNA in single-cell RNA-seq with DecontX,"Droplet-based microfluidic devices have become widely used to perform single-cell RNA sequencing (scRNA-seq). However, ambient RNA present in the cell suspension can be aberrantly counted along with a cell's native mRNA and result in cross-contamination of transcripts between different cell populations. DecontX is a novel Bayesian method to estimate and remove contamination in individual cells. DecontX accurately predicts contamination levels in a mouse-human mixture dataset and removes aberrant expression of marker genes in PBMC datasets. We also compare the contamination levels between four different scRNA-seq protocols. Overall, DecontX can be incorporated into scRNA-seq workflows to improve downstream analyses."
DAVID CAMPBELL,Interactive single cell RNA-Seq analysis with Single Cell Toolkit (SCTK),"I will present the Single Cell Toolkit (SCTK), an R package and interactive single cell RNA-sequencing (scRNA-Seq) analysis package that provides the first complete workflow for scRNA-Seq data analysis and visualization using a set of R functions and an interactive web interface. Users can perform analysis with modules for filtering raw results, clustering, batch correction, differential expression, pathway enrichment, and scRNA-Seq study design. The toolkit supports command line or pipeline data processing, and results can be loaded into the GUI for additional exploration and downstream analysis. We demonstrate the effectiveness of the SCTK on multiple scRNA-seq examples, including data from mucosal-associated invariant T cells, induced pluripotent stem cells, and breast cancer tumor cells. While other scRNA-Seq analysis tools exist, the SCTK is the first fully interactive analysis toolkit for scRNA-Seq data available within the R language."
DAVID CAMPBELL,Frequency dependent functional renormalization group for interacting fermionic systems,"We derive an expansion of the functional renormalization (fRG) equations that treats the frequency and momentum dependencies of the vertices in a systematic manner. The scheme extends the channel-decomposed fRG equations to the frequency domain and reformulates them as a series of linear integral equations in the particle-particle, particle-hole and particle-hole exchange channels. We show that the linearity of the equations offers numerous computational advantages and leads to converged, stable solutions for a variety of Hamiltonians. As the expansion is in the coupling between channels, the truncations that are necessary to making the scheme computationally viable still lead to equations that treat contributions from all channels equally. As a first benchmark we apply the two-loop fRG equations to the single impurity Anderson model. We consider the sources of error within the fRG, the computational cost associated with each, and how the choice of regulator affects the flow of the fRG. We then use the optimal truncation scheme to study the Extended Hubbard Hamiltonian in one and two dimensions. We find that in many cases of interest the fRG flow converges to a stable vertex and self-energy from which we can extract the various correlation functions and susceptibilities of interest."
DAVID CAMPBELL,Dynamical glass in weakly non-integrable many-body systems,"Integrable many-body systems are characterized by a complete set of preserved actions. Close to an integrable limit, a {\it nonintegrable} perturbation creates a coupling network in action space which can be short- or long-ranged. We analyze the dynamics of observables which turn into the conserved actions in the integrable limit. We compute distributions of their finite-time averages and obtain the ergodization time scale TE on which these distributions converge to δ-distributions. We relate TE∼(σ+τ)2/μ+τ to the statistics of fluctuation times of the observables, which acquire fat-tailed distributions with standard deviations σ+τ dominating the means μ+τ. The Lyapunov time TΛ (the inverse of the largest Lyapunov exponent) is then compared to the above time scales. We use a simple Klein-Gordon chain to emulate long- and short-range coupling networks by tuning its energy density. For long-range coupling networks TΛ≈σ+τ, which indicates that the Lyapunov time sets the ergodization time, with chaos quickly diffusing through the coupling network. For short-range coupling networks we observe a {\it dynamical glass}, where TE grows dramatically by many orders of magnitude and greatly exceeds the Lyapunov time, which TΛ≲μ+τ. This is due to the formation of a highly fragmented inhomogeneous distributions of chaotic groups of actions, separated by growing volumes of non-chaotic regions. These structures persist up to the ergodization time TE."
DAVID CAMPBELL,Critical entanglement for the half-fIlled extended Hubbard model,"We study the ground state of the one-dimensional extended Hubbard model at half filling using the entanglement entropy calculated by density matrix renormalization-group techniques. We apply curve fitting and scaling methods to accurately identify a second-order critical point as well as a Berezinskii-Kosterlitz-Thouless critical point. Using open boundary conditions and medium-sized lattices with very small truncation errors, we are able to achieve similar accuracy to that of previous authors. We also report observations of finite-size and boundary effects that can be remedied with careful pinning."
DAVID CAMPBELL,"Phi-four solitary waves in a parabolic potentia: existence, stability, and collisional dynamics","We explore a φ4 model with an added external parabolic potential term. This term dramatically alters the spectral properties of the system. We identify single and multiple kink solutions and examine their stability features; importantly, all of the stationary structures turn out to be unstable. We complement these with a dynamical study of the evolution of a single kink in the trap, as well as of the scattering of kink and anti-kink solutions of the model. We observe that some of the key characteristics of kink-antikink collisions, such as the critical velocity and the multi-bounce windows, are sensitively dependent on the trap strength parameter, as well as the initial displacement of the kink and antikink."
DAVID CAMPBELL,Mapping the complexity of higher education in the developing world,"On October 27 and 28, 2009, a workshop of experts on higher education in developing countries was convened by the Boston University Frederick S. Pardee Center for the Study of the Longer-Range Future. The meeting was supported by a grant from the National Academies Keck Futures Initiative with additional support from the Pardee Center and the Office of the Boston University Provost. The meeting brought together experts in economics, public policy, education, development, university management, and quantitative modeling who had rich experiences across the developing world. These experts offered a variety of conceptual tools with which to look at the particular complexities associated with higher education in developing countries. The meeting was convened by the authors of this paper. This policy brief builds upon and reflects on the discussion at this meeting, but is not a meeting report, per se."
DAVID CAMPBELL,Mitchell Jay Feigenbaum,"Mitchell Jay Feigenbaum, a theoretical physicist whose inquiring mind and intense focus enabled him to contribute broadly across many fields, died of a heart attack on 30 June 2019 in New York City. Mitchell’s most celebrated work was his discovery of the universality of the period-doubling transition to chaos and the associated Feigenbaum constant, δ = 4.6692016 …. His predictions were confirmed experimentally in Albert Libchaber’s convection experiments in 1979. For their work, the two men shared the 1986 Wolf Prize in Physics."
DAVID CAMPBELL,David Pines,"David Pines, a preeminent theoretical physicist and a convener of numerous academic efforts, died of pancreatic cancer at his home in Urbana, Illinois, on 3 May 2018. Over his long and illustrious career, David made major contributions to condensed-matter physics, nuclear physics, and astrophysics and created lasting national and international institutions."
DAVID CAMPBELL,The history of the Center for Nonlinear Studies,I discussed the background of the development of the CNLS at LANL.
DAVID CAMPBELL,"The future of human nature: a symposium on the promises and challenges of the revolutions in genomics and computer science, April 10, 11, and 12, 2003","This conference focused on scientific and technological advances in genetics, computer science, and their convergence during the next 35 to 250 years. In particular, it focused on directed evolution, the futures it allows, the shape of society in those futures, and the robustness of human nature against technological change at the level of individuals, groups, and societies. It is taken as a premise that biotechnology and computer science will mature and will reinforce one another. During the period of interest, human cloning, germ-line genetic engineering, and an array of reproductive technologies will become feasible and safe. Early in this period, we can reasonably expect the processing power of a laptop computer to exceed the collective processing power of every human brain on the planet; later in the period human/machine interfaces will begin to emerge. Whether such technologies will take hold is not known. But if they do, human evolution is likely to proceed at a greatly accelerated rate; human nature as we know it may change markedly, if it does not disappear altogether, and new intelligent species may well be created."
EMELIA J BENJAMIN,The Framingham Heart Study 100K SNP Genome-Wide Association Study Resource: Overview of 17 Phenotype Working Group Reports,"BACKGROUND: The Framingham Heart Study (FHS), founded in 1948 to examine the epidemiology of cardiovascular disease, is among the most comprehensively characterized multi-generational studies in the world. Many collected phenotypes have substantial genetic contributors; yet most genetic determinants remain to be identified. Using single nucleotide polymorphisms (SNPs) from a 100K genome-wide scan, we examine the associations of common polymorphisms with phenotypic variation in this community-based cohort and provide a full-disclosure, web-based resource of results for future replication studies. METHODS: Adult participants (n = 1345) of the largest 310 pedigrees in the FHS, many biologically related, were genotyped with the 100K Affymetrix GeneChip. These genotypes were used to assess their contribution to 987 phenotypes collected in FHS over 56 years of follow up, including: cardiovascular risk factors and biomarkers; subclinical and clinical cardiovascular disease; cancer and longevity traits; and traits in pulmonary, sleep, neurology, renal, and bone domains. We conducted genome-wide variance components linkage and population-based and family-based association tests. RESULTS: The participants were white of European descent and from the FHS Original and Offspring Cohorts (examination 1 Offspring mean age 32 ± 9 years, 54% women). This overview summarizes the methods, selected findings and limitations of the results presented in the accompanying series of 17 manuscripts. The presented association results are based on 70,897 autosomal SNPs meeting the following criteria: minor allele frequency ≥ 10%, genotype call rate ≥ 80%, Hardy-Weinberg equilibrium p-value ≥ 0.001, and satisfying Mendelian consistency. Linkage analyses are based on 11,200 SNPs and short-tandem repeats. Results of phenotype-genotype linkages and associations for all autosomal SNPs are posted on the NCBI dbGaP website at. CONCLUSION: We have created a full-disclosure resource of results, posted on the dbGaP website, from a genome-wide association study in the FHS. Because we used three analytical approaches to examine the association and linkage of 987 phenotypes with thousands of SNPs, our results must be considered hypothesis-generating and need to be replicated. Results from the FHS 100K project with NCBI web posting provides a resource for investigators to identify high priority findings for replication."
EMELIA J BENJAMIN,"Genome-Wide Association of Echocardiographic Dimensions, Brachial Artery Endothelial Function and Treadmill Exercise Responses in the Framingham Heart Study","BACKGROUND: Echocardiographic left ventricular (LV) measurements, exercise responses to standardized treadmill test (ETT) and brachial artery (BA) vascular function are heritable traits that are associated with cardiovascular disease risk. We conducted a genome-wide association study (GWAS) in the community-based Framingham Heart Study. METHODS: We estimated multivariable-adjusted residuals for quantitative echocardiography, ETT and BA function traits. Echocardiography residuals were averaged across 4 examinations and included LV mass, diastolic and systolic dimensions, wall thickness, fractional shortening, left atrial and aortic root size. ETT measures (single exam) included systolic blood pressure and heart rate responses during exercise stage 2, and at 3 minutes post-exercise. BA measures (single exam) included vessel diameter, flow-mediated dilation (FMD), and baseline and hyperemic flow responses. Generalized estimating equations (GEE), family-based association tests (FBAT) and variance-components linkage were used to relate multivariable-adjusted trait residuals to 70,987 SNPs (Human 100K GeneChip, Affymetrix) restricted to autosomal SNPs with minor allele frequency ≥0.10, genotype call rate ≥0.80, and Hardy-Weinberg equilibrium p ≥ 0.001. RESULTS: We summarize results from 17 traits in up to 1238 related middle-aged to elderly men and women. Results of all association and linkage analyses are web-posted at . We confirmed modest-to-strong heritabilities (estimates 0.30–0.52) for several Echo, ETT and BA function traits. Overall, p < 10-5 in either GEE or FBAT models were observed for 21 SNPs (nine for echocardiography, eleven for ETT and one for BA function). The top SNPs associated were (GEE results): LV diastolic dimension, rs1379659 (SLIT2, p = 1.17*10-7); LV systolic dimension, rs10504543 (KCNB2, p = 5.18*10-6); LV mass, rs10498091 (p = 5.68*10-6); Left atrial size, rs1935881 (FAM5C, p = 6.56*10-6); exercise heart rate, rs6847149 (NOLA1, p = 2.74*10-6); exercise systolic blood pressure, rs2553268 (WRN, p = 6.3*10-6); BA baseline flow, rs3814219 (OBFC1, 9.48*10-7), and FMD, rs4148686 (CFTR, p = 1.13*10-5). Several SNPs are reasonable biological candidates, with some being related to multiple traits suggesting pleiotropy. The peak LOD score was for LV mass (4.38; chromosome 5); the 1.5 LOD support interval included NRG2. CONCLUSION: In hypothesis-generating GWAS of echocardiography, ETT and BA vascular function in a moderate-sized community-based sample, we identified several SNPs that are candidates for replication attempts and we provide a web-based GWAS resource for the research community."
EMELIA J BENJAMIN,Genome-Wide Association with Select Biomarker Traits in the Framingham Heart Study,"BACKGROUND: Systemic biomarkers provide insights into disease pathogenesis, diagnosis, and risk stratification. Many systemic biomarker concentrations are heritable phenotypes. Genome-wide association studies (GWAS) provide mechanisms to investigate the genetic contributions to biomarker variability unconstrained by current knowledge of physiological relations. METHODS: We examined the association of Affymetrix 100K GeneChip single nucleotide polymorphisms (SNPs) to 22 systemic biomarker concentrations in 4 biological domains: inflammation/oxidative stress; natriuretic peptides; liver function; and vitamins. Related members of the Framingham Offspring cohort (n = 1012; mean age 59 ± 10 years, 51% women) had both phenotype and genotype data (minimum-maximum per phenotype n = 507–1008). We used Generalized Estimating Equations (GEE), Family Based Association Tests (FBAT) and variance components linkage to relate SNPs to multivariable-adjusted biomarker residuals. Autosomal SNPs (n = 70,987) meeting the following criteria were studied: minor allele frequency ≥ 10%, call rate ≥ 80% and Hardy-Weinberg equilibrium p ≥ 0.001. RESULTS: With GEE, 58 SNPs had p < 10-6: the top SNPs were rs2494250 (p = 1.00*10-14) and rs4128725 (p = 3.68*10-12) for monocyte chemoattractant protein-1 (MCP1), and rs2794520 (p = 2.83*10-8) and rs2808629 (p = 3.19*10-8) for C-reactive protein (CRP) averaged from 3 examinations (over about 20 years). With FBAT, 11 SNPs had p < 10-6: the top SNPs were the same for MCP1 (rs4128725, p = 3.28*10-8, and rs2494250, p = 3.55*10-8), and also included B-type natriuretic peptide (rs437021, p = 1.01*10-6) and Vitamin K percent undercarboxylated osteocalcin (rs2052028, p = 1.07*10-6). The peak LOD (logarithm of the odds) scores were for MCP1 (4.38, chromosome 1) and CRP (3.28, chromosome 1; previously described) concentrations; of note the 1.5 support interval included the MCP1 and CRP SNPs reported above (GEE model). Previous candidate SNP associations with circulating CRP concentrations were replicated at p < 0.05; the SNPs rs2794520 and rs2808629 are in linkage disequilibrium with previously reported SNPs. GEE, FBAT and linkage results are posted at . CONCLUSION: The Framingham GWAS represents a resource to describe potentially novel genetic influences on systemic biomarker variability. The newly described associations will need to be replicated in other studies."
EMELIA J BENJAMIN,Genetic Correlates of Longevity and Selected Age-Related Phenotypes: A Genome-Wide Association Study in the Framingham Study,"BACKGROUND: Family studies and heritability estimates provide evidence for a genetic contribution to variation in the human life span. METHODS: We conducted a genome wide association study (Affymetrix 100K SNP GeneChip) for longevity-related traits in a community-based sample. We report on 5 longevity and aging traits in up to 1345 Framingham Study participants from 330 families. Multivariable-adjusted residuals were computed using appropriate models (Cox proportional hazards, logistic, or linear regression) and the residuals from these models were used to test for association with qualifying SNPs (70, 987 autosomal SNPs with genotypic call rate ≥80%, minor allele frequency ≥10%, Hardy-Weinberg test p ≥ 0.001). RESULTS: In family-based association test (FBAT) models, 8 SNPs in two regions approximately 500 kb apart on chromosome 1 (physical positions 73,091,610 and 73, 527,652) were associated with age at death (p-value < 10-5). The two sets of SNPs were in high linkage disequilibrium (minimum r2 = 0.58). The top 30 SNPs for generalized estimating equation (GEE) tests of association with age at death included rs10507486 (p = 0.0001) and rs4943794 (p = 0.0002), SNPs intronic to FOXO1A, a gene implicated in lifespan extension in animal models. FBAT models identified 7 SNPs and GEE models identified 9 SNPs associated with both age at death and morbidity-free survival at age 65 including rs2374983 near PON1. In the analysis of selected candidate genes, SNP associations (FBAT or GEE p-value < 0.01) were identified for age at death in or near the following genes: FOXO1A, GAPDH, KL, LEPR, PON1, PSEN1, SOD2, and WRN. Top ranked SNP associations in the GEE model for age at natural menopause included rs6910534 (p = 0.00003) near FOXO3a and rs3751591 (p = 0.00006) in CYP19A1. Results of all longevity phenotype-genotype associations for all autosomal SNPs are web posted at . CONCLUSION. Longevity and aging traits are associated with SNPs on the Affymetrix 100K GeneChip. None of the associations achieved genome-wide significance. These data generate hypotheses and serve as a resource for replication as more genes and biologic pathways are proposed as contributing to longevity and healthy aging."
EMELIA J BENJAMIN,Framingham Heart Study 100K Project: Genome-Wide Associations for Cardiovascular Disease Outcomes,"BACKGROUND: Cardiovascular disease (CVD) and its most common manifestations – including coronary heart disease (CHD), stroke, heart failure (HF), and atrial fibrillation (AF) – are major causes of morbidity and mortality. In many industrialized countries, cardiovascular disease (CVD) claims more lives each year than any other disease. Heart disease and stroke are the first and third leading causes of death in the United States. Prior investigations have reported several single gene variants associated with CHD, stroke, HF, and AF. We report a community-based genome-wide association study of major CVD outcomes. METHODS: In 1345 Framingham Heart Study participants from the largest 310 pedigrees (54% women, mean age 33 years at entry), we analyzed associations of 70,987 qualifying SNPs (Affymetrix 100K GeneChip) to four major CVD outcomes: major atherosclerotic CVD (n = 142; myocardial infarction, stroke, CHD death), major CHD (n = 118; myocardial infarction, CHD death), AF (n = 151), and HF (n = 73). Participants free of the condition at entry were included in proportional hazards models. We analyzed model-based deviance residuals using generalized estimating equations to test associations between SNP genotypes and traits in additive genetic models restricted to autosomal SNPs with minor allele frequency ≥0.10, genotype call rate ≥0.80, and Hardy-Weinberg equilibrium p-value ≥ 0.001. RESULTS: Six associations yielded p < 10-5. The lowest p-values for each CVD trait were as follows: major CVD, rs499818, p = 6.6 × 10-6; major CHD, rs2549513, p = 9.7 × 10-6; AF, rs958546, p = 4.8 × 10-6; HF: rs740363, p = 8.8 × 10-6. Of note, we found associations of a 13 Kb region on chromosome 9p21 with major CVD (p 1.7 – 1.9 × 10-5) and major CHD (p 2.5 – 3.5 × 10-4) that confirm associations with CHD in two recently reported genome-wide association studies. Also, rs10501920 in CNTN5 was associated with AF (p = 9.4 × 10-6) and HF (p = 1.2 × 10-4). Complete results for these phenotypes can be found at the dbgap website. CONCLUSION: No association attained genome-wide significance, but several intriguing findings emerged. Notably, we replicated associations of chromosome 9p21 with major CVD. Additional studies are needed to validate these results. Finding genetic variants associated with CVD may point to novel disease pathways and identify potential targeted preventive therapies."
EMELIA J BENJAMIN,"Diabetes, Gender, and Left Ventricular Structure in African-Americans: The Atherosclerosis Risk in Communities Study","BACKGROUND: Cardiovascular risk associated with diabetes may be partially attributed to left ventricular structural abnormalities. However, the relations between left ventricular structure and diabetes have not been extensively studied in African-Americans. METHODS: We studied 514 male and 965 female African-Americans 51 to 70 years old, in whom echocardiographic left ventricular mass measurements were collected for the ARIC Study. In these, we investigated the independent association of diabetes with left ventricular structural abnormalities. RESULTS: Diabetes, hypertension and obesity prevalences were 22%, 57% and 45%, respectively. Unindexed left ventricular mass was higher with diabetes in both men (238.3 ± 79.4 g vs. 213.7 ± 58.6 g; p < 0.001) and women (206.4 ± 61.5 g vs. 176.9 ± 50.1 g; p < 0.001), respectively. Prevalence of height-indexed left ventricular hypertrophy was higher in women while increased relative wall thickness was similar in men and women. Those with diabetes had higher prevalences of height-indexed left ventricular hypertrophy (52% vs. 32%; p < 0.001), and of increased relative wall thickness (73% vs. 64%; p = 0.002). Gender-adjusted associations of diabetes with left ventricular hypertrophy (OR = 2.29 95%CI:1.79–2.94) were attenuated after multiple adjustments in logistic regression (OR = 1.50 95%CI:1.12–2.00). Diabetes was associated with higher left ventricle diameter (OR = 2.13 95%CI:1.28–3.53) only in men and with higher wall thickness (OR = 1.89 95%CI:1.34–2.66) only in women. Attenuations in diabetes associations were frequently seen after adjustment for obesity indices. CONCLUSION: In African-Americans, diabetes is associated with left ventricular hypertrophy and, with different patterns of left ventricular structural abnormalities between genders. Attenuation seen in adjusted associations suggests that the higher frequency of structural abnormalities seen in diabetes may be due to factors other than hyperglycemia."
EMELIA J BENJAMIN,Genome-Wide Association Study for Renal Traits in the Framingham Heart and Atherosclerosis Risk in Communities Studies,"BACKGROUND: The Framingham Heart Study (FHS) recently obtained initial results from the first genome-wide association scan for renal traits. The study of 70,987 single nucleotide polymorphisms (SNPs) in 1,010 FHS participants provides a list of SNPs showing the strongest associations with renal traits which need to be verified in independent study samples. METHODS: Sixteen SNPs were selected for replication based on the most promising associations with chronic kidney disease (CKD), estimated glomerular filtration rate (eGFR), and serum cystatin C in FHS. These SNPs were genotyped in 15,747 participants of the Atherosclerosis in Communities (ARIC) Study and evaluated for association using multivariable adjusted regression analyses. Primary outcomes in ARIC were CKD and eGFR. Secondary prospective analyses were conducted for association with kidney disease progression using multivariable adjusted Cox proportional hazards regression. The definition of the outcomes, all covariates, and the use of an additive genetic model was consistent with the original analyses in FHS. RESULTS: The intronic SNP rs6495446 in the gene MTHFS was significantly associated with CKD among white ARIC participants at visit 4: the odds ratio per each C allele was 1.24 (95% CI 1.09–1.41, p = 0.001). Borderline significant associations of rs6495446 were observed with CKD at study visit 1 (p = 0.024), eGFR at study visits 1 (p = 0.073) and 4 (lower mean eGFR per C allele by 0.6 ml/min/1.73 m2, p = 0.043) and kidney disease progression (hazard ratio 1.13 per each C allele, 95% CI 1.00–1.26, p = 0.041). Another SNP, rs3779748 in EYA1, was significantly associated with CKD at ARIC visit 1 (odds ratio per each T allele 1.22, p = 0.01), but only with eGFR and cystatin C in FHS. CONCLUSION: This genome-wide association study provides unbiased information implicating MTHFS as a candidate gene for kidney disease. Our findings highlight the importance of replication to identify common SNPs associated with renal traits."
EMELIA J BENJAMIN,The Relation of C - Reactive Protein to Chronic Kidney Disease in African Americans: The Jackson Heart Study,"BACKGROUND: African Americans have an increased incidence and worse prognosis with chronic kidney disease (CKD - estimated glomerular filtration rate [eGFR] <60 ml/min/1.73 m2) than their counterparts of European-descent. Inflammation has been related to renal disease in non-Hispanic whites, but there are limited data on the role of inflammation in renal dysfunction in African Americans in the community. METHODS: We examined the cross-sectional relation of log transformed C-reactive protein (CRP) to renal function (eGFR by Modification of Diet and Renal Disease equation) in African American participants of the community-based Jackson Heart Study's first examination (2000 to 2004). We conducted multivariable linear regression relating CRP to eGFR adjusting for age, sex, body mass index, systolic and diastolic blood pressure, diabetes, total/HDL cholesterol, triglycerides, smoking, antihypertensive therapy, lipid lowering therapy, hormone replacement therapy, and prevalent cardiovascular disease events. In a secondary analysis we assessed the association of CRP with albuminuria (defined as albumin-to-creatinine ratio > 30 mg/g). RESULTS: Participants (n = 4320, 63.2% women) had a mean age ± SD of 54.0 ± 12.8 years. The prevalence of CKD was 5.2% (n = 228 cases). In multivariable regression, CRP concentrations were higher in those with CKD compared to those without CKD (mean CRP 3.2 ± 1.1 mg/L vs. 2.4 ± 1.0 mg/L, respectively p < 0.0001). CRP was significantly associated with albuminuria in sex and age adjusted model however not in the multivariable adjusted model (p > 0.05). CONCLUSION: CRP was associated with CKD however not albuminuria in multivariable-adjusted analyses. The study of inflammation in the progression of renal disease in African Americans merits further investigation."
EMELIA J BENJAMIN,"Plasma Leptin Levels and Incidence of Heart Failure, Cardiovascular Disease, and Total Mortality in Elderly Individuals","OBJECTIVE: Obesity predisposes individuals to congestive heart failure (CHF) and cardiovascular disease (CVD). Leptin regulates energy homeostasis, is elevated in obesity, and influences ventricular and vascular remodeling. We tested the hypothesis that leptin levels are associated with greater risk of CHF, CVD, and mortality in elderly individuals. RESEARCH DESIGN AND METHODS: We evaluated 818 elderly (mean age 79 years, 62% women) Framingham Study participants attending a routine examination at which plasma leptin was assayed. RESULTS: Leptin levels were higher in women and strongly correlated with BMI (P < 0.0001). On follow-up (mean 8.0 years), 129 (of 775 free of CHF) participants developed CHF, 187 (of 532 free of CVD) experienced a first CVD event, and 391 individuals died. In multivariable Cox regression models adjusting for established risk factors, log-leptin was positively associated with incidence of CHF and CVD (hazard ratio [HR] per SD increment 1.26 [95% CI 1.03–1.55] and 1.28 [1.09–1.50], respectively). Additional adjustment for BMI nullified the association with CHF (0.97 [0.75–1.24]) but only modestly attenuated the relation to CVD incidence (1.23 [1.00–1.51], P = 0.052). We observed a nonlinear, U-shaped relation between log-leptin and mortality (P = 0.005 for quadratic term) with greater risk of death evident at both low and high leptin levels. CONCLUSIONS: In our moderate-sized community-based elderly sample, higher circulating leptin levels were associated with a greater risk of CHF and CVD, but leptin did not provide incremental prognostic information beyond BMI. Additional investigations are warranted to elucidate the U-shaped relation of leptin to mortality."
EMELIA J BENJAMIN,Framingham Heart Study 100K Project: Genome-Wide Associations for Blood Pressure and Arterial Stiffness,"BACKGROUND: About one quarter of adults are hypertensive and high blood pressure carries increased risk for heart disease, stroke, kidney disease and death. Increased arterial stiffness is a key factor in the pathogenesis of systolic hypertension and cardiovascular disease. Substantial heritability of blood-pressure (BP) and arterial-stiffness suggests important genetic contributions. METHODS: In Framingham Heart Study families, we analyzed genome-wide SNP (Affymetrix 100K GeneChip) associations with systolic (SBP) and diastolic (DBP) BP at a single examination in 1971–1975 (n = 1260), at a recent examination in 1998–2001 (n = 1233), and long-term averaged SBP and DBP from 1971–2001 (n = 1327, mean age 52 years, 54% women) and with arterial stiffness measured by arterial tonometry (carotid-femoral and carotid-brachial pulse wave velocity, forward and reflected pressure wave amplitude, and mean arterial pressure; 1998–2001, n = 644). In primary analyses we used generalized estimating equations in models for an additive genetic effect to test associations between SNPs and phenotypes of interest using multivariable-adjusted residuals. A total of 70,987 autosomal SNPs with minor allele frequency ≥ 0.10, genotype call rate ≥ 0.80, and Hardy-Weinberg equilibrium p ≥ 0.001 were analyzed. We also tested for association of 69 SNPs in six renin-angiotensin-aldosterone pathway genes with BP and arterial stiffness phenotypes as part of a candidate gene search. RESULTS: In the primary analyses, none of the associations attained genome-wide significance. For the six BP phenotypes, seven SNPs yielded p values < 10-5. The lowest p-values for SBP and DBP respectively were rs10493340 (p = 1.7 × 10-6) and rs1963982 (p = 3.3 × 10-6). For the five tonometry phenotypes, five SNPs had p values < 10-5; lowest p-values were for reflected wave (rs6063312, p = 2.1 × 10-6) and carotid-brachial pulse wave velocity (rs770189, p = 2.5 × 10-6) in MEF2C, a regulator of cardiac morphogenesis. We found only weak association of SNPs in the renin-angiotensin-aldosterone pathway with BP or arterial stiffness. CONCLUSION: These results of genome-wide association testing for blood pressure and arterial stiffness phenotypes in an unselected community-based sample of adults may aid in the identification of the genetic basis of hypertension and arterial disease, help identify high risk individuals, and guide novel therapies for hypertension. Additional studies are needed to replicate any associations identified in these analyses."
EMELIA J BENJAMIN,Phenotype-Genotype Association Grid: A Convenient Method for Summarizing Multiple Association Analyses,"BACKGROUND: High-throughput genotyping generates vast amounts of data for analysis; results can be difficult to summarize succinctly. A single project may involve genotyping many genes with multiple variants per gene and analyzing each variant in relation to numerous phenotypes, using several genetic models and population subgroups. Hundreds of statistical tests may be performed for a single SNP, thereby complicating interpretation of results and inhibiting identification of patterns of association. RESULTS: To facilitate visual display and summary of large numbers of association tests of genetic loci with multiple phenotypes, we developed a Phenotype-Genotype Association (PGA) grid display. A database-backed web server was used to create PGA grids from phenotypic and genotypic data (sample sizes, means and standard errors, P-value for association). HTML pages were generated using Tcl scripts on an AOLserver platform, using an Oracle database, and the ArsDigita Community System web toolkit. The grids are interactive and permit display of summary data for individual cells by a mouse click (i.e. least squares means for a given SNP and phenotype, specified genetic model and study sample). PGA grids can be used to visually summarize results of individual SNP associations, gene-environment associations, or haplotype associations. CONCLUSION: The PGA grid, which permits interactive exploration of large numbers of association test results, can serve as an easily adapted common and useful display format for large-scale genetic studies. Doing so would reduce the problem of publication bias, and would simplify the task of summarizing large-scale association studies."
EMELIA J BENJAMIN,The Framingham Heart Study 100K SNP Genome-Wide Association Study Resource: Overview of 17 Phenotype Working Group Reports,"BACKGROUND: The Framingham Heart Study (FHS), founded in 1948 to examine the epidemiology of cardiovascular disease, is among the most comprehensively characterized multi-generational studies in the world. Many collected phenotypes have substantial genetic contributors; yet most genetic determinants remain to be identified. Using single nucleotide polymorphisms (SNPs) from a 100K genome-wide scan, we examine the associations of common polymorphisms with phenotypic variation in this community-based cohort and provide a full-disclosure, web-based resource of results for future replication studies. METHODS: Adult participants (n = 1345) of the largest 310 pedigrees in the FHS, many biologically related, were genotyped with the 100K Affymetrix GeneChip. These genotypes were used to assess their contribution to 987 phenotypes collected in FHS over 56 years of follow up, including: cardiovascular risk factors and biomarkers; subclinical and clinical cardiovascular disease; cancer and longevity traits; and traits in pulmonary, sleep, neurology, renal, and bone domains. We conducted genome-wide variance components linkage and population-based and family-based association tests. RESULTS: The participants were white of European descent and from the FHS Original and Offspring Cohorts (examination 1 Offspring mean age 32 ± 9 years, 54% women). This overview summarizes the methods, selected findings and limitations of the results presented in the accompanying series of 17 manuscripts. The presented association results are based on 70,897 autosomal SNPs meeting the following criteria: minor allele frequency ≥ 10%, genotype call rate ≥ 80%, Hardy-Weinberg equilibrium p-value ≥ 0.001, and satisfying Mendelian consistency. Linkage analyses are based on 11,200 SNPs and short-tandem repeats. Results of phenotype-genotype linkages and associations for all autosomal SNPs are posted on the NCBI dbGaP website at. CONCLUSION: We have created a full-disclosure resource of results, posted on the dbGaP website, from a genome-wide association study in the FHS. Because we used three analytical approaches to examine the association and linkage of 987 phenotypes with thousands of SNPs, our results must be considered hypothesis-generating and need to be replicated. Results from the FHS 100K project with NCBI web posting provides a resource for investigators to identify high priority findings for replication."
EMELIA J BENJAMIN,Comparison of M-mode echocardiographic left ventricular mass measured using digital and strip chart readings: the atherosclerosis risk in communities (ARIC) study,"BACKGROUND: Epidemiological and clinical studies frequently use echocardiography to measure LV wall thicknesses and chamber dimension for estimating quantitative measures of LV mass. While echocardiographic M-mode LV images have traditionally been measured using hand-held calipers and strip-chart paper tracings, digitized M-mode LV image measurements made directly on the computer screen using electronic calipers have become standard practice. We sought to determine if systematic differences in LV mass occur between the two methods by comparing LV mass measured from simultaneous M-mode strip chart recordings and digitized recordings. METHODS: The Atherosclerosis Risk in Communities study applied the latter method. To determine if systematic differences in LV mass occur between the two methods, LV mass was measured from simultaneous M-mode strip chart recordings and digitized recordings. RESULTS: We found no difference in LV mass (p > .25) and a strong correlation in LV mass between the two methods (r = 0.97). Neither age, sex, nor hypertension status affected the correlation of LV mass between the two methods. CONCLUSIONS: We conclude that digital estimates of LV mass provide unbiased estimates comparable to the strip-chart method."
EMELIA J BENJAMIN,Framingham Heart Study 100K project: genome-wide associations for cardiovascular disease outcomes,"BACKGROUND:Cardiovascular disease (CVD) and its most common manifestations - including coronary heart disease (CHD), stroke, heart failure (HF), and atrial fibrillation (AF) - are major causes of morbidity and mortality. In many industrialized countries, cardiovascular disease (CVD) claims more lives each year than any other disease. Heart disease and stroke are the first and third leading causes of death in the United States. Prior investigations have reported several single gene variants associated with CHD, stroke, HF, and AF. We report a community-based genome-wide association study of major CVD outcomes.METHODS:In 1345 Framingham Heart Study participants from the largest 310 pedigrees (54% women, mean age 33 years at entry), we analyzed associations of 70,987 qualifying SNPs (Affymetrix 100K GeneChip) to four major CVD outcomes: major atherosclerotic CVD (n = 142; myocardial infarction, stroke, CHD death), major CHD (n = 118; myocardial infarction, CHD death), AF (n = 151), and HF (n = 73). Participants free of the condition at entry were included in proportional hazards models. We analyzed model-based deviance residuals using generalized estimating equations to test associations between SNP genotypes and traits in additive genetic models restricted to autosomal SNPs with minor allele frequency [greater than or equal to]0.10, genotype call rate [greater than or equal to]0.80, and Hardy-Weinberg equilibrium p-value [greater than or equal to] 0.001.RESULTS:Six associations yielded p <10-5. The lowest p-values for each CVD trait were as follows: major CVD, rs499818, p = 6.6 x 10-6; major CHD, rs2549513, p = 9.7 x 10-6; AF, rs958546, p = 4.8 x 10-6; HF: rs740363, p = 8.8 x 10-6. Of note, we found associations of a 13 Kb region on chromosome 9p21 with major CVD (p 1.7 - 1.9 x 10-5) and major CHD (p 2.5 - 3.5 x 10-4) that confirm associations with CHD in two recently reported genome-wide association studies. Also, rs10501920 in CNTN5 was associated with AF (p = 9.4 x 10-6) and HF (p = 1.2 x 10-4). Complete results for these phenotypes can be found at the dbgap website http://www.ncbi.nlm.nih.gov/projects/gap/cgi-bin/study.cgi?id=phs000007.CONCLUSION:No association attained genome-wide significance, but several intriguing findings emerged. Notably, we replicated associations of chromosome 9p21 with major CVD. Additional studies are needed to validate these results. Finding genetic variants associated with CVD may point to novel disease pathways and identify potential targeted preventive therapies."
EMELIA J BENJAMIN,The Framingham Heart Study 100K SNP genome-wide association study resource: overview of 17 phenotype working group reports,"BACKGROUND:The Framingham Heart Study (FHS), founded in 1948 to examine the epidemiology of cardiovascular disease, is among the most comprehensively characterized multi-generational studies in the world. Many collected phenotypes have substantial genetic contributors; yet most genetic determinants remain to be identified. Using single nucleotide polymorphisms (SNPs) from a 100K genome-wide scan, we examine the associations of common polymorphisms with phenotypic variation in this community-based cohort and provide a full-disclosure, web-based resource of results for future replication studies.METHODS:Adult participants (n = 1345) of the largest 310 pedigrees in the FHS, many biologically related, were genotyped with the 100K Affymetrix GeneChip. These genotypes were used to assess their contribution to 987 phenotypes collected in FHS over 56 years of follow up, including: cardiovascular risk factors and biomarkers; subclinical and clinical cardiovascular disease; cancer and longevity traits; and traits in pulmonary, sleep, neurology, renal, and bone domains. We conducted genome-wide variance components linkage and population-based and family-based association tests.RESULTS:The participants were white of European descent and from the FHS Original and Offspring Cohorts (examination 1 Offspring mean age 32 +/- 9 years, 54% women). This overview summarizes the methods, selected findings and limitations of the results presented in the accompanying series of 17 manuscripts. The presented association results are based on 70,897 autosomal SNPs meeting the following criteria: minor allele frequency [greater than or equal to] 10%, genotype call rate [greater than or equal to] 80%, Hardy-Weinberg equilibrium p-value [greater than or equal to] 0.001, and satisfying Mendelian consistency. Linkage analyses are based on 11,200 SNPs and short-tandem repeats. Results of phenotype-genotype linkages and associations for all autosomal SNPs are posted on the NCBI dbGaP website at http://www.ncbi.nlm.nih.gov/projects/gap/cgi-bin/study.cgi?id=phs000007.CONCLUSION:We have created a full-disclosure resource of results, posted on the dbGaP website, from a genome-wide association study in the FHS. Because we used three analytical approaches to examine the association and linkage of 987 phenotypes with thousands of SNPs, our results must be considered hypothesis-generating and need to be replicated. Results from the FHS 100K project with NCBI web posting provides a resource for investigators to identify high priority findings for replication."
EMELIA J BENJAMIN,Genetic correlates of longevity and selected age-related phenotypes: a genome-wide association study in the Framingham Study,"BACKGROUND: Family studies and heritability estimates provide evidence for a genetic contribution to variation in the human life span. METHODS:We conducted a genome wide association study (Affymetrix 100K SNP GeneChip) for longevity-related traits in a community-based sample. We report on 5 longevity and aging traits in up to 1345 Framingham Study participants from 330 families. Multivariable-adjusted residuals were computed using appropriate models (Cox proportional hazards, logistic, or linear regression) and the residuals from these models were used to test for association with qualifying SNPs (70, 987 autosomal SNPs with genotypic call rate [greater than or equal to]80%, minor allele frequency [greater than or equal to]10%, Hardy-Weinberg test p [greater than or equal to] 0.001).RESULTS:In family-based association test (FBAT) models, 8 SNPs in two regions approximately 500 kb apart on chromosome 1 (physical positions 73,091,610 and 73, 527,652) were associated with age at death (p-value < 10-5). The two sets of SNPs were in high linkage disequilibrium (minimum r2 = 0.58). The top 30 SNPs for generalized estimating equation (GEE) tests of association with age at death included rs10507486 (p = 0.0001) and rs4943794 (p = 0.0002), SNPs intronic to FOXO1A, a gene implicated in lifespan extension in animal models. FBAT models identified 7 SNPs and GEE models identified 9 SNPs associated with both age at death and morbidity-free survival at age 65 including rs2374983 near PON1. In the analysis of selected candidate genes, SNP associations (FBAT or GEE p-value < 0.01) were identified for age at death in or near the following genes: FOXO1A, GAPDH, KL, LEPR, PON1, PSEN1, SOD2, and WRN. Top ranked SNP associations in the GEE model for age at natural menopause included rs6910534 (p = 0.00003) near FOXO3a and rs3751591 (p = 0.00006) in CYP19A1. Results of all longevity phenotype-genotype associations for all autosomal SNPs are web posted at http://www.ncbi.nlm.nih.gov/projects/gap/cgi-bin/study.cgi?id=phs000007. CONCLUSION: Longevity and aging traits are associated with SNPs on the Affymetrix 100K GeneChip. None of the associations achieved genome-wide significance. These data generate hypotheses and serve as a resource for replication as more genes and biologic pathways are proposed as contributing to longevity and healthy aging."
ROBERT M CAREY,Magnetic-field measurement and analysis for the Muon g−2 Experiment at Fermilab,"The Fermi National Accelerator Laboratory (FNAL) Muon g−2 Experiment has measured the anomalous precession frequency aμ≡(gμ−2)/2 of the muon to a combined precision of 0.46 parts per million with data collected during its first physics run in 2018. This paper documents the measurement of the magnetic field in the muon storage ring. The magnetic field is monitored by systems and calibrated in terms of the equivalent proton spin precession frequency in a spherical water sample at 34.7∘C. The measured field is weighted by the muon distribution resulting in ˜ω′p, the denominator in the ratio ωa/˜ω′p that together with known fundamental constants yields aμ. The reported uncertainty on ˜ω′p for the Run-1 data set is 114 ppb consisting of uncertainty contributions from frequency extraction, calibration, mapping, tracking, and averaging of 56 ppb, and contributions from fast transient fields of 99 ppb."
ROBERT M CAREY,"BMQ : Boston medical quarterly: v. 15, no. 1-4",
ROBERT M CAREY,Calorimetry for low-energy electrons using charge and light in liquid argon,"Precise calorimetric reconstruction of 5–50 MeV electrons in liquid argon time projection chambers (LArTPCs) will enable the study of astrophysical neutrinos in DUNE and could enhance the physics reach of oscillation analyses. Liquid argon scintillation light has the potential to improve energy reconstruction for low-energy electrons over charge-based measurements alone. Here we demonstrate light-augmented calorimetry for low-energy electrons in a single-phase LArTPC using a sample of Michel electrons from decays of stopping cosmic muons in the LArIAT experiment at Fermilab. Michel electron energy spectra are reconstructed using both a traditional charge-based approach as well as a more holistic approach that incorporates both charge and light. A maximum-likelihood fitter, using LArIAT’s well-tuned simulation, is developed for combining these quantities to achieve optimal energy resolution. A sample of isolated electrons is simulated to better determine the energy resolution expected for astrophysical electron-neutrino charged-current interaction final states. In LArIAT, which has very low wire noise and an average light yield of 18  pe/MeV, an energy resolution of σ/E≃9.3%/√E⊕1.3% is achieved. Samples are then generated with varying wire noise levels and light yields to gauge the impact of light-augmented calorimetry in larger LArTPCs. At a charge-readout signal-to-noise of S/N≃30, for example, the energy resolution for electrons below 40 MeV is improved by ≈10%, ≈20%, and ≈40% over charge-only calorimetry for average light yields of 10  pe/MeV, 20  pe/MeV, and 100  pe/MeV, respectively."
ROBERT M CAREY,"BMQ : Boston medical quarterly: v. 12, no. 1-4",
ROBERT M CAREY,Beam dynamics corrections to the Run-1 measurement of the muon anomalous magnetic moment at Fermilab,"This paper presents the beam dynamics systematic corrections and their uncertainties for the Run-1 dataset of the Fermilab Muon g−2 Experiment. Two corrections to the measured muon precession frequency ωma are associated with well-known effects owing to the use of electrostatic quadrupole (ESQ) vertical focusing in the storage ring. An average vertically oriented motional magnetic field is felt by relativistic muons passing transversely through the radial electric field components created by the ESQ system. The correction depends on the stored momentum distribution and the tunes of the ring, which has relatively weak vertical focusing. Vertical betatron motions imply that the muons do not orbit the ring in a plane exactly orthogonal to the vertical magnetic field direction. A correction is necessary to account for an average pitch angle associated with their trajectories. A third small correction is necessary, because muons that escape the ring during the storage time are slightly biased in initial spin phase compared to the parent distribution. Finally, because two high-voltage resistors in the ESQ network had longer than designed RC time constants, the vertical and horizontal centroids and envelopes of the stored muon beam drifted slightly, but coherently, during each storage ring fill. This led to the discovery of an important phase-acceptance relationship that requires a correction. The sum of the corrections to ωma is 0.50±0.09  ppm; the uncertainty is small compared to the 0.43 ppm statistical precision of ωma."
ROBERT M CAREY,Measurement of the anomalous precession frequency of the muon in the Fermilab Muon g − 2 Experiment,"The Muon g−2 Experiment at Fermi National Accelerator Laboratory (FNAL) has measured the muon anomalous precession frequency ωma to an uncertainty of 434 parts per billion (ppb), statistical, and 56 ppb, systematic, with data collected in four storage ring configurations during its first physics run in 2018. When combined with a precision measurement of the magnetic field of the experiment’s muon storage ring, the precession frequency measurement determines a muon magnetic anomaly of aμ(FNAL)=116592040(54)×10−11 (0.46 ppm). This article describes the multiple techniques employed in the reconstruction, analysis, and fitting of the data to measure the precession frequency. It also presents the averaging of the results from the 11 separate determinations of ωma, and the systematic uncertainties on the result."
ROBERT M CAREY,Measurement of the positive muon anomalous magnetic moment to 0.46 ppm,"We present the first results of the Fermilab National Accelerator Laboratory (FNAL) Muon g-2 Experiment for the positive muon magnetic anomaly a_{μ}≡(g_{μ}-2)/2. The anomaly is determined from the precision measurements of two angular frequencies. Intensity variation of high-energy positrons from muon decays directly encodes the difference frequency ω_{a} between the spin-precession and cyclotron frequencies for polarized muons in a magnetic storage ring. The storage ring magnetic field is measured using nuclear magnetic resonance probes calibrated in terms of the equivalent proton spin precession frequency ω[over ˜]_{p}^{'} in a spherical water sample at 34.7 °C. The ratio ω_{a}/ω[over ˜]_{p}^{'}, together with known fundamental constants, determines a_{μ}(FNAL)=116 592 040(54)×10^{-11} (0.46 ppm). The result is 3.3 standard deviations greater than the standard model prediction and is in excellent agreement with the previous Brookhaven National Laboratory (BNL) E821 measurement. After combination with previous measurements of both μ^{+} and μ^{-}, the new experimental average of a_{μ}(Exp)=116 592 061(41)×10^{-11} (0.35 ppm) increases the tension between experiment and theory to 4.2 standard deviations."
ROBERT M CAREY,"BMQ : Boston medical quarterly: v. 6, no. 1-4",
MEGAN SANDEL,"The Urban Environment and Childhood Asthma (URECA) Birth Cohort Study: Design, Methods, and Study Population","BACKGROUND. The incidence and morbidity of wheezing illnesses and childhood asthma is especially high in poor urban areas. This paper describes the study design, methods, and population of the Urban Environment and Childhood Asthma (URECA) study, which was established to investigate the immunologic causes of asthma among inner-city children. METHODS AND RESULTS. URECA is an observational prospective study that enrolled pregnant women in central urban areas of Baltimore, Boston, New York City, and St. Louis and is following their offspring from birth through age 7 years. The birth cohort consists of 560 inner-city children who have at least one parent with an allergic disease or asthma, and all families live in areas in which at least 20% of the population has incomes below the poverty line. In addition, 49 inner-city children with no parental history of allergies or asthma were enrolled. The primary hypothesis is that specific urban exposures in early life promote a unique pattern of immune development (impaired antiviral and increased Th2 responses) that increases the risk of recurrent wheezing and allergic sensitization in early childhood, and of asthma by age 7 years. To track immune development, cytokine responses of blood mononuclear cells stimulated ex vivo are measured at birth and then annually. Environmental assessments include allergen and endotoxin levels in house dust, pre- and postnatal maternal stress, and indoor air nicotine and nitrogen dioxide. Nasal mucous samples are collected from the children during respiratory illnesses and analyzed for respiratory viruses. The complex interactions between environmental exposures and immune development will be assessed with respect to recurrent wheeze at age 3 years and asthma at age 7 years. CONCLUSION. The overall goal of the URECA study is to develop a better understanding of how specific urban exposures affect immune development to promote wheezing illnesses and asthma."
CATHERINE E COSTELLO,CD1c Bypasses Lysosomes to Present a Lipopeptide Antigen with 12 Amino Acids,"The recent discovery of dideoxymycobactin (DDM) as a ligand for CD1a demonstrates how a nonribosomal lipopeptide antigen is presented to T cells. DDM contains an unusual acylation motif and a peptide sequence present only in mycobacteria, but its discovery raises the possibility that ribosomally produced viral or mammalian proteins that commonly undergo lipidation might also function as antigens. To test this, we measured T cell responses to synthetic acylpeptides that mimic lipoproteins produced by cells and viruses. CD1c presented an N-acyl glycine dodecamer peptide (lipo-12) to human T cells, and the response was specific for the acyl linkage as well as the peptide length and sequence. Thus, CD1c represents the second member of the CD1 family to present lipopeptides. lipo-12 was efficiently recognized when presented by intact cells, and unlike DDM, it was inactivated by proteases and augmented by protease inhibitors. Although lysosomes often promote antigen presentation by CD1, rerouting CD1c to lysosomes by mutating CD1 tail sequences caused reduction in lipo-12 presentation. Thus, although certain antigens require antigen processing in lysosomes, others are destroyed there, providing a hypothesis for the evolutionary conservation of large CD1 families containing isoforms that survey early endosomal pathways."
CATHERINE E COSTELLO,Dissecting the mechanism of the nonheme iron endoperoxidase FtmOx1 using substrate analogues,"FtmOx1 is a nonheme iron (NHFe) endoperoxidase, catalyzing three disparate reactions, endoperoxidation, alcohol dehydrogenation, and dealkylation, under in vitro conditions; the diversity complicates its mechanistic studies. In this study, we use two substrate analogues to simplify the FtmOx1-catalyzed reaction to either a dealkylation or an alcohol dehydrogenation reaction for structure-function relationship analysis to address two key FtmOx1 mechanistic questions: (1) Y224 flipping in the proposed COX-like model vs α-ketoglutarate (αKG) rotation proposed in the CarC-like mechanistic model and (2) the involvement of a Y224 radical (COX-like model) or a Y68 radical (CarC-like model) in FtmOx1-catalysis. When 13-oxo-fumitremorgin B (7) is used as the substrate, FtmOx1-catalysis changes from the endoperoxidation to a hydroxylation reaction and leads to dealkylation. In addition, consistent with the dealkylation side-reaction in the COX-like model prediction, the X-ray structure of the FtmOx1•CoII•αKG•7 ternary complex reveals a flip of Y224 to an alternative conformation relative to the FtmOx1•FeII•αKG binary complex. Verruculogen (2) was used as a second substrate analogue to study the alcohol dehydrogenation reaction to examine the involvement of the Y224 radical or Y68 radical in FtmOx1-catalysis, and again, the results from the verruculogen reaction are more consistent with the COX-like model."
CATHERINE E COSTELLO,Giardia Cyst Wall Protein 1 Is a Lectin That Binds to Curled Fibrils of the GalNAc Homopolymer,"The infectious and diagnostic stage of Giardia lamblia (also known as G. intestinalis or G. duodenalis) is the cyst. The Giardia cyst wall contains fibrils of a unique β-1,3-linked N-acetylgalactosamine (GalNAc) homopolymer and at least three cyst wall proteins (CWPs) composed of Leu-rich repeats (CWPLRR) and a C-terminal conserved Cys-rich region (CWPCRR). Our goals were to dissect the structure of the cyst wall and determine how it is disrupted during excystation. The intact Giardia cyst wall is thin (~400 nm), easily fractured by sonication, and impermeable to small molecules. Curled fibrils of the GalNAc homopolymer are restricted to a narrow plane and are coated with linear arrays of oval-shaped protein complex. In contrast, cyst walls of Giardia treated with hot alkali to deproteinate fibrils of the GalNAc homopolymer are thick (~1.2 µm), resistant to sonication, and permeable. The deproteinated GalNAc homopolymer, which forms a loose lattice of curled fibrils, is bound by native CWP1 and CWP2, as well as by maltose-binding protein (MBP)-fusions containing the full-length CWP1 or CWP1LRR. In contrast, neither MBP alone nor MBP fused to CWP1CRR bind to the GalNAc homopolymer. Recombinant CWP1 binds to the GalNAc homopolymer within secretory vesicles of Giardia encysting in vitro. Fibrils of the GalNAc homopolymer are exposed during excystation or by treatment of heat-killed cysts with chymotrypsin, while deproteinated fibrils of the GalNAc homopolymer are degraded by extracts of Giardia cysts but not trophozoites. These results show the Leu-rich repeat domain of CWP1 is a lectin that binds to curled fibrils of the GalNAc homopolymer. During excystation, host and Giardia proteases appear to degrade bound CWPs, exposing fibrils of the GalNAc homopolymer that are digested by a stage-specific glycohydrolase. Author SummaryWhile the walls of plants and fungi contain numerous sugar homopolymers (cellulose, chitin, and β-1,3-glucans) and dozens of proteins, the cyst wall of Giardia is relatively simple. The Giardia wall contains a unique homopolymer of β-1,3-linked N-acetylgalactosamine (GalNAc) and at least three cyst wall proteins (CWPs), each of which is composed of Leu-rich repeats and a C-terminal Cys-rich region. The three major discoveries here are: 1) Fibrils of the GalNAc homopolymer are curled and form a lattice that is compressed into a narrow plane by bound protein in intact cyst walls. 2) Leu-rich repeats of CWP1 form a novel lectin domain that is specific for fibrils of the GalNAc homopolymer, which can be isolated by methods used to deproteinate fungal walls. 3) A cyst-specific glycohydrolase is able to degrade deproteinated fibrils of the GalNAc homopolymer. We incorporate these findings into a new curled fiber and lectin model of the intact Giardia cyst wall and a protease and glycohydrolase model of excystation."
CATHERINE E COSTELLO,Retraction note: endoperoxide formation by an α-ketoglutarate-dependent mononuclear non-haem iron enzyme,
DOUGLAS I KATZ,Clinicopathological evaluation of chronic traumatic encephalopathy in players of American football,"IMPORTANCE: Players of American football may be at increased risk of long-term neurological conditions, particularly chronic traumatic encephalopathy (CTE). OBJECTIVE: To determine the neuropathological and clinical features of deceased football players with CTE. DESIGN, SETTING, AND PARTICIPANTS: Case series of 202 football players whose brains were donated for research. Neuropathological evaluations and retrospective telephone clinical assessments (including head trauma history) with informants were performed blinded. Online questionnaires ascertained athletic and military history. EXPOSURES: Participation in American football at any level of play. MAIN OUTCOMES AND MEASURES: Neuropathological diagnoses of neurodegenerative diseases, including CTE, based on defined diagnostic criteria; CTE neuropathological severity (stages I to IV or dichotomized into mild [stages I and II] and severe [stages III and IV]); informant-reported athletic history and, for players who died in 2014 or later, clinical presentation, including behavior, mood, and cognitive symptoms and dementia. RESULTS: Among 202 deceased former football players (median age at death, 66 years [interquartile range, 47-76 years]), CTE was neuropathologically diagnosed in 177 players (87%; median age at death, 67 years [interquartile range, 52-77 years]; mean years of football participation, 15.1 [SD, 5.2]), including 0 of 2 pre–high school, 3 of 14 high school (21%), 48 of 53 college (91%), 9 of 14 semiprofessional (64%), 7 of 8 Canadian Football League (88%), and 110 of 111 National Football League (99%) players. Neuropathological severity of CTE was distributed across the highest level of play, with all 3 former high school players having mild pathology and the majority of former college (27 [56%]), semiprofessional (5 [56%]), and professional (101 [86%]) players having severe pathology. Among 27 participants with mild CTE pathology, 26 (96%) had behavioral or mood symptoms or both, 23 (85%) had cognitive symptoms, and 9 (33%) had signs of dementia. Among 84 participants with severe CTE pathology, 75 (89%) had behavioral or mood symptoms or both, 80 (95%) had cognitive symptoms, and 71 (85%) had signs of dementia. CONCLUSIONS AND RELEVANCE: In a convenience sample of deceased football players who donated their brains for research, a high proportion had neuropathological evidence of CTE, suggesting that CTE may be related to prior participation in football."
DOUGLAS I KATZ,Regional Brain Morphometry Predicts Memory Rehabilitation Outcome After Traumatic Brain Injury,"Cognitive deficits following traumatic brain injury (TBI) commonly include difficulties with memory, attention, and executive dysfunction. These deficits are amenable to cognitive rehabilitation, but optimally selecting rehabilitation programs for individual patients remains a challenge. Recent methods for quantifying regional brain morphometry allow for automated quantification of tissue volumes in numerous distinct brain structures. We hypothesized that such quantitative structural information could help identify individuals more or less likely to benefit from memory rehabilitation. Fifty individuals with TBI of all severities who reported having memory difficulties first underwent structural MRI scanning. They then participated in a 12 session memory rehabilitation program emphasizing internal memory strategies (I-MEMS). Primary outcome measures (HVLT, RBMT) were collected at the time of the MRI scan, immediately following therapy, and again at 1-month post-therapy. Regional brain volumes were used to predict outcome, adjusting for standard predictors (e.g., injury severity, age, education, pretest scores). We identified several brain regions that provided significant predictions of rehabilitation outcome, including the volume of the hippocampus, the lateral prefrontal cortex, the thalamus, and several subregions of the cingulate cortex. The prediction range of regional brain volumes were in some cases nearly equal in magnitude to prediction ranges provided by pretest scores on the outcome variable. We conclude that specific cerebral networks including these regions may contribute to learning during I-MEMS rehabilitation, and suggest that morphometric measures may provide substantial predictive value for rehabilitation outcome in other cognitive interventions as well."
DOUGLAS I KATZ,Effectiveness of an inpatient movement disorders program for patients with atypical parkinsonism,"This paper investigated the effectiveness of an inpatient movement disorders program for patients with atypical parkinsonism, who typically respond poorly to pharmacologic intervention and are challenging to rehabilitate as outpatients. Ninety-one patients with atypical parkinsonism participated in an inpatient movement disorders program. Patients received physical, occupational, and speech therapy for 3 hours/day, 5 to 7 days/week, and pharmacologic adjustments based on daily observation and data. Differences between admission and discharge scores were analyzed for the functional independence measure (FIM), timed up and go test (TUG), two-minute walk test (TMW), Berg balance scale (BBS) and finger tapping test (FT), and all showed significant improvement on discharge (P > .001). Clinically significant improvements in total FIM score were evident in 74% of the patients. Results were similar for ten patients whose medications were not adjusted. Patients with atypical parkinsonism benefit from an inpatient interdisciplinary movement disorders program to improve functional status."
ERAN TROMER,Formalizing human ingenuity: a quantitative framework for copyright law's substantial similarity,"A central notion in U.S. copyright law is judging the substantial similarity between an original and an (allegedly) derived work. Capturing this notion has proven elusive, and the many approaches offered by case law and legal scholarship are often ill-defined, contradictory, or internally-inconsistent. This work suggests that key parts of the substantial-similarity puzzle are amendable to modeling inspired by theoretical computer science. Our proposed framework quantitatively evaluates how much ""novelty"" is needed to produce the derived work with access to the original work, versus reproducing it without access to the copyrighted elements of the original work. ""Novelty"" is captured by a computational notion of description length, in the spirit of Kolmogorov-Levin complexity, which is robust to mechanical transformations and availability of contextual information. This results in an actionable framework that could be used by courts as an aid for deciding substantial similarity. We evaluate it on several pivotal cases in copyright law and observe that the results are consistent with the rulings, and are philosophically aligned with the abstraction-filtration-comparison test of Altai."
REBECCA PERKINS,Cost-effectiveness analysis of the 2019 American Society for colposcopy and cervical pathology risk-based management consensus guidelines for the management of abnormal cervical cancer screening tests and cancer precursors,"BACKGROUND: The guidelines for managing abnormal cervical cancer screening tests changed from a results-based approach in 2012 to a risk-based approach in 2019. OBJECTIVE: We estimated the cost-effectiveness of the 2019 management guidelines and the changes in resource utilization moving from 2012 to 2019 guidelines. STUDY DESIGN: We utilized a previously published model of cervical cancer natural history and screening to estimate and compare the lifetime costs and the number of screens, colposcopies, precancer treatments, cancer cases, and cancer deaths associated with the 2012 vs 2019 management guidelines. We assessed these guidelines under the scenarios of observed screening practice and perfect screening adherence to 3-year cytology starting at age 21, with a switch to either 3-year or 5-year cytology plus human papillomavirus cotesting at age 30. In addition, we estimated the lifetime costs and life years to determine the cost-effectiveness of shifting to the 2019 management guidelines. RESULTS: Under the assumptions of both observed screening practice and perfect screening adherence with a strategy of 3-year cytology at ages 21 to 29 and switching to 3-year cotesting at age 30, the management of the screening tests according to the 2019 guidelines was less costly and more effective than the 2012 guidelines. For 3-year cytology screening at ages 21 to 29 and switching to 5-year cotesting at age 30, the 2019 guidelines were more cost-effective at a willingness-to-pay threshold of $100,000 per life year gained. Across all scenarios, the 2019 management guidelines were associated with fewer colposcopies and cancer deaths. CONCLUSION: Our model-based analysis suggests that the 2019 guidelines are more effective overall and also more cost-effective than the 2012 guidelines, supporting the principle of “equal management of equal risks.”"
SEAN P MULLEN,De novo transcriptome assembly of the clown anemonefish (Amphiprion percula): a new resource to study the evolution of fish color,"A fundamental question of evolutionary biology is, why are some animals conspicuously colored? This question may be addressed from both a proximate (genetic and ontogenetic) and ultimate (adaptive value and evolutionary origins) perspective, and integrating these perspectives can provide further insights. Over the last few decades we have made great advances in understanding the causes of conspicuous coloration in terrestrial systems, e.g., birds and butterflies, but we still know relatively little about the causes of conspicuous, “poster” coloration in coral reef fishes. Of all coral reef fishes, the clownfish Amphiprion percula, is perhaps the most conspicuously colored, possessing a bright orange body with three iridescent white bars bordered with pitch black. Here, we review what is known about the proximate and ultimate causes of the conspicuous coloration of clownfishes Amphiprion sp.: coloration has a heritable genetic basis; coloration is influenced by development and environment; coloration has multiple plausible signaling functions; there is a phylogenetic component to coloration. Subsequently, to provide new insights into the genetic mechanisms and potential functions of A. percula coloration we (i) generate the first de novo transcriptome for this species, (ii) conduct differential gene expression analyses across different colored epidermal tissues, and (iii) conduct gene ontology (GO) enrichment analyses to characterize function of these differentially expressed genes. BUSCO indicated that transcriptome assembly was successful and many genes were found to be differentially expressed between epidermal tissues of different colors. In orange tissue, relative to white and black, many GO terms associated with muscle were over-represented. In white tissue, relative to orange and black tissue, there were very few over- or under-represented GO terms. In black tissue, relative to orange and white, many GO terms related to immune function were over-represented, supporting the hypothesis that black (melanin) coloration may serve a protective function. Overall, this study presents the assembly of the A. percula transcriptome, and represents a first step in an integrative investigation of the proximate and ultimate causes of conspicuous coloration of this iconic coral reef fish."
SEAN P MULLEN,Hybridization reveals the evolving genomic architecture of speciation,
SEAN P MULLEN,Butterfly genome reveals promiscuous exchange of mimicry adaptations among species,"The evolutionary importance of hybridization and introgression has long been debated1. Hybrids are usually rare and unfit, but even infrequent hybridization can aid adaptation by transferring beneficial traits between species. Here we use genomic tools to investigate introgression in Heliconius, a rapidly radiating genus of neotropical butterflies widely used in studies of ecology, behaviour, mimicry and speciation2,3,4,5. We sequenced the genome of Heliconius melpomene and compared it with other taxa to investigate chromosomal evolution in Lepidoptera and gene flow among multiple Heliconius species and races. Among 12,669 predicted genes, biologically important expansions of families of chemosensory and Hox genes are particularly noteworthy. Chromosomal organization has remained broadly conserved since the Cretaceous period, when butterflies split from the Bombyx (silkmoth) lineage. Using genomic resequencing, we show hybrid exchange of genes between three co-mimics, Heliconius melpomene, Heliconius timareta and Heliconius elevatus, especially at two genomic regions that control mimicry pattern. We infer that closely related Heliconius species exchange protective colour-pattern genes promiscuously, implying that hybridization has an important role in adaptive radiation."
SEAN P MULLEN,Transitions from single- to multi-locus processes during speciation with gene flow,"During speciation-with-gene-flow, a transition from single-locus to multi-locus processes can occur, as strong coupling of multiple loci creates a barrier to gene flow. Testing predictions about such transitions with empirical data requires building upon past theoretical work and the continued development of quantitative approaches. We simulated genomes under several evolutionary scenarios of gene flow and divergent selection, extending previous work with the additions of neutral sites and coupling statistics. We used these simulations to investigate, in a preliminary way, if and how selected and neutral sites differ in the conditions they require for transitions during speciation. For the parameter combinations we explored, as the per-locus strength of selection grew and/or migration decreased, it became easier for selected sites to show divergence—and thus to rise in linkage disequilibrium (LD) with each other as a statistical consequence—farther in advance of the conditions under which neutral sites could diverge. Indeed, even very low rates of effective gene flow were sufficient to prevent differentiation at neutral sites. However, once strong enough, coupling among selected sites eventually reduced gene flow at neutral sites as well. To explore whether similar transitions might be detectable in empirical data, we used published genome resequencing data from three taxa of Heliconius butterflies. We found that fixation index ( FST ) outliers and allele-frequency outliers exhibited stronger patterns of within-deme LD than the genomic background, as expected. The statistical characteristics of within-deme LD—likely indicative of the strength of coupling of barrier loci—varied between chromosomes and taxonomic comparisons. Qualitatively, the patterns we observed in the empirical data and in our simulations suggest that selection drives rapid genome-wide transitions to multi-locus coupling, illustrating how divergence and gene flow interact along the speciation continuum."
SEAN P MULLEN,Ancient homology underlies adaptive mimetic diversity across butterflies,"Convergent evolution provides a rare, natural experiment with which to test the predictability of adaptation at the molecular level. Little is known about the molecular basis of convergence over macro-evolutionary timescales. Here we use a combination of positional cloning, population genomic resequencing, association mapping and developmental data to demonstrate that positionally orthologous nucleotide variants in the upstream region of the same gene, WntA, are responsible for parallel mimetic variation in two butterfly lineages that diverged >65 million years ago. Furthermore, characterization of spatial patterns of WntA expression during development suggests that alternative regulatory mechanisms underlie wing pattern variation in each system. Taken together, our results reveal a strikingly predictable molecular basis for phenotypic convergence over deep evolutionary time."
DAVID T. FELSON,Leg-Length Inequality Is not Associated with Greater Trochanteric Pain Syndrome,"INTRODUCTION. Greater trochanteric pain syndrome (GTPS) is a common condition, the pathogenesis of which is incompletely understood. Although leg-length inequality has been suggested as a potential risk factor for GTPS, this widely held assumption has not been tested. METHODS. A cross-sectional analysis of greater trochanteric tenderness to palpation was performed in subjects with complaints of hip pain and no signs of hip osteoarthritis or generalized myofascial tenderness. Subjects were recruited from one clinical center of the Multicenter Osteoarthritis Study, a multicenter population-based study of community-dwelling adults aged 50 to 79 years. Diagnosis of GTPS was based on a standardized physical examination performed by trained examiners, and technicians measured leg length on full-limb anteroposterior radiographs. RESULTS. A total of 1,482 subjects were eligible for analysis of GTPS and leg length. Subjects' mean ± standard deviation age was 62.4 ± 8.2 years, and 59.8% were female. A total of 372 lower limbs from 271 subjects met the definition for having GTPS. Leg-length inequality (difference ≥ 1 cm) was present in 37 subjects with GTPS and in 163 subjects without GTPS (P = 0.86). Using a variety of definitions of leg-length inequality, including categorical and continuous measures, there was no association of this parameter with the occurrence of GTPS (for example, for ≥ 1 cm leg-length inequality, odds ratio = 1.17 (95% confidence interval = 0.79 to 1.73)). In adjusted analyses, female sex was significantly associated with the presence of GTPS, with an adjusted odds ratio of 3.04 (95% confidence interval = 2.07 to 4.47). CONCLUSION. The present study found no evidence to support an association between leg-length inequality and greater trochanteric pain syndrome."
DAVID T. FELSON,Can Health Care Databases Be Used to Identify Incident Cases of Osteonecrosis?,"INTRODUCTION. Osteonecrosis (ON) is a rare disease associated with alcohol and glucocorticoid use. Identifying additional risk factors is difficult as the number of cases at any single center is small. We investigated whether data available in large health care databases can be used to identify incident ON cases. METHODS. Using data from the Boston Veterans Affairs Healthcare system, we identified potential cases of ON. These records, including available radiographs and reports, were reviewed. Using published criteria, we evaluated whether the subjects had confirmed ON (radiographs/reports met criteria), incident ON (onset of symptoms within 6 months of first code), or prevalent ON (onset more than 6 months prior to first code or onset could not be determined). We tested different definitions for incident ON using information derived from administrative data. These were compared to the 'gold standard' (record review) and positive predictive values (PPVs) were derived. Since PPVs for incident cases were low, we found the number of incident cases expected for every 1,000 potential cases identified, using the definitions as an initial screening tool to reduce the number of medical records that required examination. RESULTS. We identified 87 potential cases. No case of jaw ON was identified. Only 15 (17%) incident cases of ON were identified. PPVs never exceed 50% for incident ON. However, if we used the definition '(at least 1 inpatient ON code) and (no prior codes for osteoarthritis)' as an initial screen, then for every 1,000 records, we would need to review only 150 to find 69 incident cases. CONCLUSIONS. Though the precise PPVs we found may not be generalizable to other databases, we believe that administrative data alone should not be used to identify incident cases of ON without confirming the diagnosis through a review of medical records. By applying the above definition, the number of records requiring review can be markedly reduced. This method can be used to find cases for valid case-control studies of risk factors for ON."
DAVID T. FELSON,A Functional Difficulty and Functional Pain Instrument for Hip and Knee Osteoarthritis,"INTRODUCTION. The objectives of this study were to develop a functional outcome instrument for hip and knee osteoarthritis research (OA-FUNCTION-CAT) using item response theory (IRT) and computer adaptive test (CAT) methods and to assess its psychometric performance compared to the current standard in the field. METHODS. We conducted an extensive literature review, focus groups, and cognitive testing to guide the construction of an item bank consisting of 125 functional activities commonly affected by hip and knee osteoarthritis. We recruited a convenience sample of 328 adults with confirmed hip and/or knee osteoarthritis. Subjects reported their degree of functional difficulty and functional pain in performing each activity in the item bank and completed the Western Ontario and McMaster Universities Osteoarthritis Index (WOMAC). Confirmatory factor analyses were conducted to assess scale uni-dimensionality, and IRT methods were used to calibrate the items and examine the fit of the data. We assessed the performance of OA-FUNCTION-CATs of different lengths relative to the full item bank and WOMAC using CAT simulation analyses. RESULTS. Confirmatory factor analyses revealed distinct functional difficulty and functional pain domains. Descriptive statistics for scores from 5-, 10-, and 15-item CATs were similar to those for the full item bank. The 10-item OA-FUNCTION-CAT scales demonstrated a high degree of accuracy compared with the item bank (r = 0.96 and 0.89, respectively). Compared to the WOMAC, both scales covered a broader score range and demonstrated a higher degree of precision at the ceiling and reliability across the range of scores. CONCLUSIONS. The OA-FUNCTION-CAT provided superior reliability throughout the score range and improved breadth and precision at the ceiling compared with the WOMAC. Further research is needed to assess whether these improvements carry over into superior ability to measure change."
DAVID T. FELSON,Biochemical Markers of Bone Turnover and Their Association with Bone Marrow Lesions,"INTRODUCTION. Our objective was to determine whether markers of bone resorption and formation could serve as markers for the presence of bone marrow lesions (BMLs). METHODS. We conducted an analysis of data from the Boston Osteoarthritis of the Knee Study (BOKS). Knee magnetic resonance images were scored for BMLs using a semiquantitative grading scheme. In addition, a subset of persons with BMLs underwent quantitative volume measurement of their BML, using a proprietary software method. Within the BOKS population, 80 people with BMLs and 80 without BMLs were selected for the purposes of this case-control study. Bone biomarkers assayed included type I collagen N-telopeptide (NTx) corrected for urinary creatinine, bone-specific alkaline phosphatase, and osteocalcin. The same methods were used and applied to a nested case-control sample from the Framingham study, in which BMD assessments allowed evaluation of this as a covariate. Logistic regression models were fit using BML as the outcome and biomarkers, age, sex, and body mass index as predictors. An receiver operating characteristic curve was generated for each model and the area under the curve assessed. RESULTS. A total of 151 subjects from BOKS with knee OA were assessed. The mean (standard deviation) age was 67 (9) years and 60% were male. Sixty-nine per cent had maximum BML score above 0, and 48% had maximum BML score above 1. The only model that reached statistical significance used maximum score of BML above 0 as the outcome. Ln-NTx (Ln is the natural log) exhibited a significant association with BMLs, with the odds of a BML being present increasing by 1.4-fold (95% confidence interval = 1.0-fold to 2.0-fold) per 1 standard deviation increase in the LnNTx, and with a small partial R2 of 3.05. We also evaluated 144 participants in the Framingham Osteoarthritis Study, whose mean age was 68 years and body mass index was 29 kg/m2, and of whom 40% were male. Of these participants 55% had a maximum BML score above 0. The relationship between NTx and maximum score of BML above 0 revealed a significant association, with an odds ratio fo 1.7 (95% confidence interval = 1.1 to 2.7) after adjusting for age, sex, and body mass index. CONCLUSIONS. Serum NTx was weakly associated with the presence of BMLs in both study samples. This relationship was not strong and we would not advocate the use of NTx as a marker of the presence of BMLs."
DAVID T. FELSON,A Surplus of Positive Trials: Weighing Biases and Reconsidering Equipoise,"In this issue, Fries and Krishnan raise provocative new ideas to explain the surfeit of positive industry sponsored trials evaluating new drugs. They suggest that these trials were designed after so much preliminary work that they were bound to be positive (design bias) and that this violates clinical equipoise, which they characterize as an antiquated concept that should be replaced by a focus on subject autonomy in decision making and expected value for all treatments in a trial. We contend that publication bias, more than design bias, could account for the remarkably high prevalence of positive presented trials. Furthermore, even if all new drugs were efficacious, given the likelihood of type 2 errors, not all trials would be positive. We also suggest that clinical equipoise is a nuanced concept dependent on the existence of controversy about the relative value of two treatments being compared. If there were no controversy, then trials would be both unnecessary and unethical. The proposed idea of positive expected value is intriguing, but in the real world such clearly determinable values do not exist. Neither is it clear how investigators and sponsors, who are invested in the success of a proposed therapy, would (or whether they should) develop such a formula."
DAVID T. FELSON,Cartilage Markers and Their Association with Cartilage Loss on Magnetic Resonance Imaging in Knee Osteoarthritis: The Boston Osteoarthritis Knee Study,"We used data from a longitudinal observation study to determine whether markers of cartilage turnover could serve as predictors of cartilage loss on magnetic resonance imaging (MRI). We conducted a study of data from the Boston Osteoarthritis of the Knee Study (BOKS), a completed natural history study of knee osteoarthritis (OA). All subjects in the study met American College of Rheumatology criteria for knee OA. Baseline and follow-up knee magnetic resonance images were scored for cartilage loss by means of the WORMS (Whole Organ Magnetic Resonance Imaging Score) semiquantitative grading scheme. Within the BOKS population, 80 subjects who experienced cartilage loss and 80 subjects who did not were selected for the purposes of this nested case control study. We assessed the baseline levels of cartilage degradation and synthesis products by means of assays for type I and II cleavage by collagenases (Col2:3/4Cshort or C1,2C), type II cleavage only with Col2:3/4Clongmono (C2C), type II synthesis (C-propeptide), the C-telopeptide of type II (Col2CTx), aggrecan 846 epitope, and cartilage oligomeric matrix protein (COMP). We performed a logistic regression to examine the relation of levels of each biomarker to the risk of cartilage loss in any knee. All analyses were adjusted for gender, age, and body mass index (BMI); results stratified by gender gave similar results. One hundred thirty-seven patients with symptomatic knee OA were assessed. At baseline, the mean (standard deviation) age was 67 (9) years and 54% were male. Seventy-six percent of the subjects had radiographic tibiofemoral OA (Kellgren & Lawrence grade of greater than or equal to 2) and the remainder had patellofemoral OA. With the exception of COMP, none of the other biomarkers was a statistically significant predictor of cartilage loss. For a 1-unit increase in COMP, the odds of cartilage loss increased 6.09 times (95% confidence interval [CI] 1.34 to 27.67). After the analysis of COMP was adjusted for age, gender, and BMI, the risk for cartilage loss was 6.35 (95% CI 1.36 to 29.65). Among subjects with symptomatic knee OA, a single measurement of increased COMP predicted subsequent cartilage loss on MRI. The other biochemical markers of cartilage synthesis and degradation do not facilitate prediction of cartilage loss. With the exception of COMP, if changes in cartilage turnover in patients with symptomatic knee OA are associated with cartilage loss, they do not appear to affect systemic biomarker levels."
DAVID T. FELSON,The Association between Patellar Alignment on Magnetic Resonance Imaging and Radiographic Manifestations of Knee Osteoarthritis,"The aim of our study was to evaluate the association between patellar alignment by using magnetic resonance imaging images and radiographic manifestations of patello-femoral osteoarthritis (OA). Subjects were recruited to participate in a natural history study of symptomatic knee OA. We examined the relation of patellar alignment in the sagittal plane (patellar length ratio (PLR)) and the transverse plane (sulcus angle (SA), lateral patellar tilt angle (LPTA), and bisect offset (BO)) to radiographic features of patello-femoral OA, namely joint space narrowing and patellar osteophytes, using a proportional odds logistic regression model while adjusting for age, sex, and bone mass index (BMI). The study sample consisted of 126 males (average age 68.0 years, BMI 31.2) and 87 females (average age 64.7 years, BMI 31.6), 75% of whom had tibiofemoral OA (a Kellgren-Lawrence score of 2 or more). PLR showed a statistically significant association with joint space narrowing and osteophytosis in the lateral compartment. SA showed significant association with medial joint space narrowing and with lateral and medial patellar osteophytosis. LPTA and BO showed significant association with both radiographic indices of the lateral compartment. Clear linear trends were found in association between PLR, LPTA and BO, and with outcomes associated with lateral patello-femoral OA. SA, LPTA, and BO showed linear trends of association with medial joint space narrowing. Results of our study clearly suggest the association between indices of patellar alignment and such features of patello-femoral OA as osteophytosis and joint space narrowing. Additional studies will be required to establish the normal and abnormal ranges of patellar alignment indices and their longitudinal relation to patello-femoral OA."
DAVID T. FELSON,Developments in the Clinical Understanding of Osteoarthritis,"With the recognition that osteoarthritis is a disease of the whole joint, attention has focused increasingly on features in the joint environment which cause ongoing joint damage and are likely sources of pain. This article reviews current ways of assessing osteoarthritis progression and what factors potentiate it, structural abnormalities that probably produce pain, new understandings of the genetics of osteoarthritis, and evaluations of new and old treatments."
LADORA THOMPSON,Denervation-induced activation of the standard proteasome and immunoproteasome,"The standard 26S proteasome is responsible for the majority of myofibrillar protein degradation leading to muscle atrophy. The immunoproteasome is an inducible form of the proteasome. While its function has been linked to conditions of atrophy, its contribution to muscle proteolysis remains unclear. Therefore, the purpose of this study was to determine if the immunoproteasome plays a role in skeletal muscle atrophy induced by denervation. Adult male C57BL/6 wild type (WT) and immunoproteasome knockout lmp7-/-/mecl-1-/- (L7M1) mice underwent tibial nerve transection on the left hindlimb for either 7 or 14 days, while control mice did not undergo surgery. Proteasome activity (caspase-, chymotrypsin-, and trypsin- like), protein content of standard proteasome (β1, β5 and β2) and immunoproteasome (LMP2, LMP7 and MECL-1) catalytic subunits were determined in the gastrocnemius muscle. Denervation induced significant atrophy and was accompanied by increased activities and protein content of the catalytic subunits in both WT and L7M1 mice. Although denervation resulted in a similar degree of muscle atrophy between strains, the mice lacking two immunoproteasome subunits showed a differential response in the extent and duration of proteasome features, including activities and content of the β1, β5 and LMP2 catalytic subunits. The results indicate that immunoproteasome deficiency alters the proteasome's composition and activities. However, the immunoproteasome does not appear to be essential for muscle atrophy induced by denervation."
LADORA THOMPSON,Short-term ONX-0914 administration: performance and muscle phenotype in Mdx mice,"Duchenne muscular dystrophy (DMD) is a severe muscle-wasting disease. Although the lack of dystrophin protein is the primary defect responsible for the development of DMD, secondary disease complications such as persistent inflammation contribute greatly to the pathogenesis and the time-dependent progression of muscle destruction. The immunoproteasome is a potential therapeutic target for conditions or diseases mechanistically linked to inflammation. In this study, we explored the possible effects of ONX-0914 administration, an inhibitor specific for the immunoproteasome subunit LMP7 (ß5i), on motor performance, muscular pathology and protein degradation in 7-week old MDX mice, an age when the dystrophic muscles show extensive degeneration and regeneration. ONX-0914 (10 mg/kg) was injected subcutaneously on Day 2, 4, and 6. The mice were evaluated for physical performance (walking speed and strength) on Day 1 and 8. We show that this short-term treatment of ONX-0914 in MDX mice did not alter strength nor walking speed. The physical performance findings were consistent with no change in muscle inflammatory infiltration, percentage of central nuclei and proteasome content. Taken together, muscle structure and function in the young adult MDX mouse model are not altered with ONX-0914 treatment, indicating the administration of ONX-0914 during this critical time period does not exhibit any detrimental effects and may be an effective treatment of secondary complications of muscular dystrophy after further investigations."
LADORA THOMPSON,"Frailty: past, present, and future?","The prevalence of frailty across the world in older adults is increasing dramatically and having frailty places a person at increased risk for many adverse health outcomes, including impaired mobility, falls, hospitalizations, and mortality. Globally, the concept of frailty is gaining attention and the scientific field has made great strides in identifying and conceptually defining frailty through consensus conferences, in advancing the overall science of frailty by drawing on basic science discoveries including concepts surrounding the hallmarks of aging, resilience, and intrinsic capacities, and in identifying the many challenges faced by professionals within diverse clinical settings. Currently, it is thought that frailty is preventable, thus the identification of a person's degree of frailty is vital. Identification of frailty is achievable through widely used frailty screening tools, which are valid, reliable, and easy to use. Following the identification of a person's degree of frailty, targeted intervention strategies, such as physical activity programs must be implemented. In this perspective, we provide a historical perspective of the frailty field since the last quarter of the 20th century to present. We identify the proposed underlying pathophysiology of multiple physiological systems, including compromised homeostasis and resilience. Next, we outline the available screening tools for frailty with a physical performance assessment and highlight specific benefits of physical activity. Lastly, we discuss current scientific evidence supporting the physical activity recommendations for the aging population and for older adults with frailty. The goal is to emphasize early detection of frailty and stress the value of physical activity."
LADORA THOMPSON,C57BL/6 life span study: age-related declines in muscle power production and contractile velocity,"Quantification of key outcome measures in animal models of aging is an important step preceding intervention testing. One such measurement, skeletal muscle power generation (force * velocity), is critical for dynamic movement. Prior research focused on maximum power (P max), which occurs around 30-40 % of maximum load. However, movement occurs over the entire load range. Thus, the primary purpose of this study was to determine the effect of age on power generation during concentric contractions in the extensor digitorum longus (EDL) and soleus muscles over the load range from 10 to 90 % of peak isometric tetanic force (P 0). Adult, old, and elderly male C57BL/6 mice were examined for contractile function (6-7 months old, 100 % survival; ~24 months, 75 %; and ~28 months, <50 %, respectively). Mice at other ages (5-32 months) were also tested for regression modeling. We hypothesized and found that power decreased with age not only at P max but also over the load range. Importantly, we found greater age-associated deficits in both power and velocity when the muscles were contracting concentrically against heavy loads (>50 % P 0). The shape of the force-velocity curve also changed with age (a/P 0 increased). In addition, there were prolonged contraction times to maximum force and shifts in the distribution of the myosin light and heavy chain isoforms in the EDL. The results demonstrate that age-associated difficulty in movement during challenging tasks is likely due, in addition to overall reduced force output, to an accelerated deterioration of power production and contractile velocity under heavily loaded conditions."
LADORA THOMPSON,Age-induced oxidative stress: how does it influence skeletal muscle quantity and quality?,"With advancing age, skeletal muscle function declines as a result of strength loss. These strength deficits are largely due to reductions in muscle size (i.e., quantity) and its intrinsic force-producing capacity (i.e., quality). Age-induced reductions in skeletal muscle quantity and quality can be the consequence of several factors, including accumulation of reactive oxygen and nitrogen species (ROS/RNS), also known as oxidative stress. Therefore, the purpose of this mini-review is to highlight the published literature that has demonstrated links between aging, oxidative stress, and skeletal muscle quantity or quality. In particular, we focused on how oxidative stress has the potential to reduce muscle quantity by shifting protein balance in a deficit, and muscle quality by impairing activation at the neuromuscular junction, excitation-contraction (EC) coupling at the ryanodine receptor (RyR), and cross-bridge cycling within the myofibrillar apparatus. Of these, muscle weakness due to EC coupling failure mediated by RyR dysfunction via oxidation and/or nitrosylation appears to be the strongest candidate based on the publications reviewed. However, it is clear that age-associated oxidative stress has the ability to alter strength through several mechanisms and at various locations of the muscle fiber."
LADORA THOMPSON,Distinct patterns of fiber type adaptation in rat hindlimb muscles 4 weeks After hemorrhagic stroke,"OBJECTIVE: The aim of this study was to evaluate adaptations in soleus and tibialis anterior muscles in a rat model 4 wks after hemorrhagic stroke. DESIGN: Young adult Sprague Dawley rats were randomly assigned to two groups: stroke and control, with eight soleus and eight tibialis anterior muscles per group. Hemorrhagic stroke was induced in the right caudoputamen of the stroke rats. Control rats had no intervention. Neurologic status was evaluated in both groups before stroke and 4 wks after stroke. Muscles were harvested after poststroke neurologic testing. Muscle fiber types and cross-sectional areas were determined in soleus and tibialis anterior using immunohistochemical labeling for myosin heavy chain. RESULTS: No generalized fiber atrophy was found in any of the muscles. Fiber types shifted from faster to slower in the tibialis anterior of the stroke group, but no fiber type shifts occurred in the soleus muscles of stroke animals. CONCLUSIONS: Because slower myosin heavy chain fiber types are associated with weaker contractile force and slower contractile speed, this faster to slower fiber type shift in tibialis anterior muscles may contribute to weaker and slower muscle contraction in this muscle after stroke. This finding may indicate potential therapeutic benefit from treatments known to influence fiber type plasticity."
LADORA THOMPSON,Skeletal muscle denervation investigations: selecting an experimental control wisely,"Unilateral denervation is widely used for studies investigating mechanisms of muscle atrophy. The ""contralateral-innervated muscle"" is a commonly used experimental control in denervation studies. It is not clear whether denervation unilaterally alters the proteolytic system in the contralateral-innervated muscles. Therefore, the objectives of this rapid report are 1) to determine whether unilateral denervation has an effect on the proteolytic system in contralateral-innervated control muscles and 2) to identify the changes in proteasome properties in denervated muscles after 7- and 14-day tibial nerve transection with either the contralateral-innervated muscles or intact muscles from nonsurgical mice used as the experimental control. In the contralateral-innervated muscles after 7 and 14 days of nerve transection, the proteasome activities and content are significantly increased compared with muscles from nonsurgical mice. When the nonsurgical mice are used as the experimental control, a robust increase in proteasome properties is found in the denervated muscles. This robust increase in proteasome properties is eliminated when the contralateral-innervated muscles are the experimental control. In conclusion, there is a crossover effect from unilateral denervation on proteolytic parameters. As a result, the crossover effect on contralateral-innervated muscles must be considered when an experimental control is selected in a denervation study."
LADORA THOMPSON,Denervation-induced activation of the ubiquitin-proteasome system reduces skeletal muscle quantity not quality,"It is well known that the ubiquitin-proteasome system is activated in response to skeletal muscle wasting and functions to degrade contractile proteins. The loss of these proteins inevitably reduces skeletal muscle size (i.e., quantity). However, it is currently unknown whether activation of this pathway also affects function by impairing the muscle's intrinsic ability to produce force (i.e., quality). Therefore, the purpose of this study was twofold, (1) document how the ubiquitin-proteasome system responds to denervation and (2) identify the physiological consequences of these changes. To induce soleus muscle atrophy, C57BL6 mice underwent tibial nerve transection of the left hindlimb for 7 or 14 days (n = 6-8 per group). At these time points, content of several proteins within the ubiquitin-proteasome system were determined via Western blot, while ex vivo whole muscle contractility was specifically analyzed at day 14. Denervation temporarily increased several key proteins within the ubiquitin-proteasome system, including the E3 ligase MuRF1 and the proteasome subunits 19S, α7 and β5. These changes were accompanied by reductions in absolute peak force and power, which were offset when expressed relative to physiological cross-sectional area. Contrary to peak force, absolute and relative forces at submaximal stimulation frequencies were significantly greater following 14 days of denervation. Taken together, these data represent two keys findings. First, activation of the ubiquitin-proteasome system is associated with reductions in skeletal muscle quantity rather than quality. Second, shortly after denervation, it appears the muscle remodels to compensate for the loss of neural activity via changes in Ca2+ handling."
LADORA THOMPSON,"Assessing onset, prevalence and survival in mice using a frailty phenotype","Little is known whether frailty assessments in mice are capable of distinguishing important characteristics of the frailty syndrome. The goals of this study were to identify the onset and the prevalence of frailty across the lifespan and to determine if a frailty phenotype predicts mortality. Body weight, walking speed, strength, endurance and physical activity were assessed in male C57BL/6 mice every three months starting at 14 months of age. Mice that fell in the bottom 20% for walking speed, strength, endurance and physical activity, and in the top 20% for body weight were considered to have a positive frailty marker. The onset of frailty occurred at 17 months, and represented only 3.5% of the mouse cohort. The percentage of frail mice increased with age until basically every mouse was identified as frail. Frail, pre-frail, and non-frail mice had mean survival ages of 27, 29 and 34 months, respectively. In closing, frail mice lack resilience; in that, multiple tissue/organ systems may deteriorate at an accelerated rate and ultimately lead to early mortality when compared to non-frail mice. Identifying the onset and prevalence of frailty, in addition to predicting mortality, has potential to yield information about several aging processes."
LADORA THOMPSON,Downhill exercise alters immunoproteasome content in mouse skeletal muscle,"Content of the immunoproteasome, the inducible form of the standard proteasome, increases in atrophic muscle suggesting it may be associated with skeletal muscle remodeling. However, it remains unknown if the immunoproteasome responds to stressful situations that do not promote large perturbations in skeletal muscle proteolysis. The purpose of this study was to determine how an acute bout of muscular stress influences immunoproteasome content. To accomplish this, wildtype (WT) and immunoproteasome knockout lmp7-/-/mecl1-/-(L7M1) mice were run downhill on a motorized treadmill. Soleus muscles were excised 1 and 3 days post-exercise and compared to unexercised muscle(control). Ex vivophysiology, histology and biochemical analyses were used to assess the effects of immunoproteasome knockout and unaccustomed exercise. Besides L7M1 muscle being LMP7/MECL1deficient, no other major biochemical, histological or functional differences were observed between the control muscles. In both strains, the downhill run shifted the force-frequency curve to the right and reduced twitch force, however did not alter tetanic force or inflammatory markers. In the days post-exercise, several of the proteasome 's catalytic subunits were upregulated. Specifically, WT muscle increased LMP7 while L7M1 muscle instead increased ≤ 5. These findings indicate that running mice downhill results in subtle contractile characteristics that correspond to skeletal muscle injury, yet does not appear to induce a significant inflammatory response. Interestingly, this minor stress activated the production of specific immunoproteasome subunits; that if knocked out, were replaced by components of the standard proteasome. These data suggest that the immunoproteasome may be involved in maintaining cellular homeostasis."
LADORA THOMPSON,Identifying characteristics of frailty in female mice using a phenotype assessment tool,"Preclinical studies are important in identifying the underlying mechanisms contributing to frailty. Frailty studies have mainly focused on male rodents with little directed at female rodents. Therefore, the purposes of this study were to identify the onset and prevalence of frailty across the life span in female mice, and to determine if frailty predicts mortality. Female C57BL/6 (n = 27) mice starting at 17 months of age were assessed across the life span using a frailty phenotype, which included body weight, walking speed, strength, endurance, and physical activity. The onset of frailty occurred at approximately 17 months (1/27 mice), with the prevalence of frailty increasing thereafter. At 17 months, 11.1% of the mice were pre-frail and by 26 months peaked at 36.9%. The percentage of frail mice progressively increased up to 66.7% at 32 months. Non-frail mice lived to 29 months whereas frail/pre-frail mice lived only to 26 months (p = .04). In closing, using a mouse frailty phenotype, we are able to identify that the prevalence of frailty in female mice increases across the life span and accurately predicts mortality. Together, this frailty phenotype has the potential to yield information about the underlying mechanisms contributing to frailty."
LADORA THOMPSON,"Lipotoxicity, aging, and muscle contractility: does fiber type matter?","Sarcopenia is a universal characteristic of the aging process and is often accompanied by increases in whole-body adiposity. These changes in body composition have important clinical implications, given that loss of muscle and gain of fat mass are both significantly and independently associated with declining physical performance as well as an increased risk for disability, hospitalizations, and mortality in older individuals. This increased fat mass is not exclusively stored in adipose depots but may become deposited in non-adipose tissues, such as skeletal muscle, when the oxidative capacity of the adipose tissue itself is exceeded. The redistributed adipose tissue is thought to exert detrimental local effects on the muscle environment given the close proximity. Thus, sarcopenia observed with aging may be better defined in the context of loss of muscle quality rather than loss of muscle quantity per se. In this perspective, we briefly review the age-related physiological changes in cellularity, secretory profiles, and inflammatory status of adipose tissue which drive lipotoxicity (spillover) of skeletal muscle and then provide evidence of how this may affect specific fiber type contractility. We focus on biological contributors (cellular machinery) to contractility for which there is some evidence of vulnerability to lipid stress distinguishing between fiber types."
LADORA THOMPSON,Sex-specific components of frailty in C57BL/6 mice.,"Many age-related biochemical, physiological and behavioral changes are known to be sex-specific. However, how sex influences frailty status and mortality risk in frail rodents has yet to be established. The purpose of this study was therefore to characterize sex differences in frail mice across the lifespan. Male (n=29) and female (n=27) mice starting at 17 months of age were assessed using a frailty phenotype adjusted according to sex, which included body weight, walking speed, strength, endurance and physical activity. Regardless of sex, frail mice were phenotypically dysfunctional compared to age-matched non-frail mice, while non-frail females generally possessed a higher body fat percentage and were more physically active than non-frail males (p≤0.05). The prevalence of frailty was greater in female mice at 26 months of age (p=0.05), but if normalized to mean lifespan, no sex differences remained. No differences were detected in the rate of death or mean lifespan between frail male and female mice (p≥0.12). In closing, these data indicate that sexual differences exist in aging C57BL/6 mice and if the frailty criteria are adjusted according to sex, the prevalence of frailty increases across age with frail mice dying early in life, regardless of sex."
LADORA THOMPSON,Increasing myosin light chain 3f (MLC3f) protects against a decline in contractile velocity,"Disuse induces adaptations in skeletal muscle, which lead to muscle deterioration. Hindlimb-unloading (HU) is a well-established model to investigate cellular mechanisms responsible for disuse-induced skeletal muscle dysfunction. In myosin heavy chain (MHC) type IIB fibers HU induces a reduction in contraction speed (Vo) and a reduction in the relative myosin light chain 3f (MLC3f) protein content compared with myosin light chain 1f (MLC1f) protein. This study tested the hypothesis that increasing the relative MLC3f protein content via rAd-MLC3f vector delivery would attenuate the HU-induced decline in Vo in single MHC type IIB fibers. Fischer-344 rats were randomly assigned to one of three groups: control, HU for 7 days, and HU for 7 days plus rAd-MLC3f. The semimembranosus muscles were injected with rAd-MLC3f (3.75 x 1011-5 x 1011 ifu/ml) at four days after the initiation of HU. In single MHC type IIB fibers the relative MLC3f content decreased by 25% (12.00±0.60% to 9.06±0.66%) and Vo was reduced by 29% (3.22±0.14fl/s vs. 2.27±0.08fl/s) with HU compared to the control group. The rAd-MLC3f injection resulted in an increase in the relative MLC3f content (12.26±1.19%) and a concomitant increase in Vo (2.90±0.15fl/s) of MHC type IIB fibers. A positive relationship was observed between the percent of MLC3f content and Vo. Maximal isometric force and specific tension were reduced with HU by 49% (741.45±44.24μN to 379.09±23.77μN) and 33% (97.58±4.25kN/m2 to 65.05±2.71kN/m2), respectively compared to the control group. The rAd-MLC3f injection did not change the HU-induced decline in force or specific tension. Collectively, these results indicate that rAd-MLC3f injection rescues hindlimb unloading-induced decline in Vo in MHC type IIB single muscle fibers."
LADORA THOMPSON,Novel individualized power training protocol preserves physical function in adult and older mice,"Sarcopenia, the age-related loss of muscle mass and strength, contributes to frailty, functional decline, and reduced quality of life in older adults. Exercise is a recognized therapy for sarcopenia and muscle dysfunction, though not a cure. Muscle power declines at an increased rate compared to force, and force output declines earlier than mass. Thus, there is a need for research of exercise focusing on improving power output and functionality in older adults. Our primary purpose was proof-of-concept that a novel individualized power exercise modality would induce positive adaptations in adult mice, before the exercise program was applied to an aged cohort. We hypothesized that after following our protocol, both adult and older mice would show improved function, though there would be evidence of anabolic resistance in the older mice. Male C57BL/6 mice (12 months of age at study conclusion) were randomized into control (n = 9) and exercise (n = 6) groups. The trained group used progressive resistance (with a weighted harness) and intensity (~ 4-10 rpm) on a custom motorized running wheel. The mice trained similarly to a human workout regimen (4-5 sets/session, 3 sessions/week, for 12 weeks). We determined significant (p < 0.05) positive adaptations post-intervention, including: neuromuscular function (rotarod), strength/endurance (inverted cling grip test), training physiology (force/power output per session), muscle size (soleus mass), and power/velocity of contraction (in vitro physiology). Secondly, we trained a cohort of older male mice (28 months old at conclusion): control (n = 12) and exercised (n = 8). While the older exercised mice did preserve function and gain benefits, they also demonstrated evidence of anabolic resistance."
MICHAEL ZANK,Judaism and the west: From Hermann Cohen to Joseph Soloveitchik,
MICHAEL ZANK,Torah als Staatsrecht? Epochen einer theologisch-politischen Idee,"Bevor ich mich im Folgenden von Spinoza ausgehend der Frage des 'mischpat ivri' zuwende und die Frage stelle, welche Rolle die Idee der Torah als Staatsrecht im modernen Israel spielt, möchte ich kurz in Erinnerung rufen, was wir historisch über die Torah im Alten Israel und aus dem antiken bzw. spätantiken Judentum wissen. Ich beginne also bei Spinoza, gehe dann kurz in die tiefere Vergangenheit und wende mich dann der zionistischen und israelischen Beschäftigung mit dem hebräischen Recht zu. Der Bezug auf die Torah als Staatsrecht bewegt sich sowohl im Fall der Staatstheorie der Frühen Neuzeit als auch im Fall des 'mischpat ivri' auf der dreifachen Grenze von Religionsquelle, Gelehrtentradition und Politik bzw. Entwürfen der Gesellschaftsgestaltung. Ohne das hier weiter theoretisch reflektieren zu können, gehören diese Beobachtungen meines Erachtens in den Bereich der hier verhandelten Fragen um das Verhältnis von Religion und Kulturwissenschaft."
MICHAEL ZANK,"Strauss, Schmitt y Peterson. Contornos comparativos del ""problema teológico-político""","Spanish translation of “Strauss, Schmitt, and Peterson: Comparative Contours of the ‘Theological Political Predicament’” in German-Jewish Thought Between Religion and Politics. FS Mendes Flohr, ed. Martina Urban and Christian Wiese (Berlin: Walter de Gruyter, 2012), pp. 317-333"
MICHAEL ZANK,Hebräischer Humanismus und Zionism: Martin Buber zur jüdisch-arabischen Frage,"Keynote lecture for a conference on the centenary of Martin Buber's ""I and Thou"""
MICHAEL ZANK,The Jerusalem Basic Law (1980/2000) and the Jerusalem Embassy Act (1990/95): A comparative investigation of Israeli and US legislation on the status of Jerusalem,"This essay, written from a religious studies perspective, compares two pieces of largely symbolic legislation, the Israeli 1980 Jerusalem Basic Law and the US 1995 Jerusalem Embassy Act, situating them in their respective historical contexts and raising questions about the dynamic of legislative acts that exceed the intention of both those who introduced these bills and the legislators who passed them into law. I argue that these laws indicate the power of broadly-shared public sentiments in modern politics and policy-making, a power that has the potential of overwhelming more pragmatic and cautious approaches to public law."
MICHAEL ZANK,A putative (private) life of Hannah Arendt. Bio-portraiture as performance in the work of Miriam Shenitzer,"The paper uses tropes culled from several of Hannah Arendt's works, as well as Rebecca Schneider's performance-theoretical considerations on ""reenactment"", to analyze the work of artist Miriam Shenitzer, specifically a show of drawings, captions, and objects called ""A Putative Life of Hannah Arendt."" The essay probes this ""putative life"" as construed from the artist's own memory fragments (including the memories of others that have become the artist's own), as well as from faux-artifacts that constitute a ""collection"" (à la Benjamin) without claim to representing an actual past. With access to history denied and a heritage claimed ""without testament,"" the artist opens a space ""between past and future,"" a moment of contemplation on the borders between private and public lives."
RENEE SPENCER,It takes a village to break up a match: a systemic analysis of formal youth mentoring relationship endings,"BACKGROUND Although early closure of formal youth mentoring relationships has recently begun to receive some attention, more information about factors that contribute to premature endings, and how those factors interact, is needed so that empirically-based program practices can be developed and disseminated to prevent such endings and to ensure that youth reap the benefits mentoring can provide. OBJECTIVE This qualitative interview study applies a systemic model of youth mentoring relationships (Keller in J Prim Prev 26:169–188, 2005a) to the study of mentoring relationship endings in community-based mentoring matches to understand why these matches ended. METHOD Mentors, parents/guardians and program staff associated with 36 mentoring matches that had ended were interviewed about their experiences of these relationships and their understanding of why they had ended. Thematic analysis of the interview transcripts and mentoring program case notes for each match followed by systemic modeling of the relationships yielded three major findings. RESULTS A strong mentor–youth relationship is necessary but not sufficient for match longevity. The mentor–youth relationship, even when relatively strong, is unlikely to withstand disruptions in other relationships in the system. Agency contextual factors, such as program practices and policies and staffing patterns, have a critical role to play in sustaining mentoring matches, as they directly influence all of the relationships in the mentoring system. CONCLUSION These findings highlight the importance of considering not just the mentoring dyad but also the parent/guardian and program context when trying to prevent match closures. They also point to several program practices that may support longer mentoring relationships."
LUCY HUTYRA,"Cities, traffic, and CO2: A multidecadal assessment of trends, drivers, and scaling relationships","Emissions of CO2 from road vehicles were 1.57 billion metric tons in 2012, accounting for 28% of US fossil fuel CO2 emissions, but the spatial distributions of these emissions are highly uncertain. We develop a new emissions inventory, the Database of Road Transportation Emissions (DARTE), which estimates CO2 emitted by US road transport at a resolution of 1 km annually for 1980-2012. DARTE reveals that urban areas are responsible for 80% of on-road emissions growth since 1980 and for 63% of total 2012 emissions. We observe nonlinearities between CO2 emissions and population density at broad spatial/temporal scales, with total on-road CO2 increasing nonlinearly with population density, rapidly up to 1,650 persons per square kilometer and slowly thereafter. Per capita emissions decline as density rises, but at markedly varying rates depending on existing densities. We make use of DARTE's bottom-up construction to highlight the biases associated with the common practice of using population as a linear proxy for disaggregating national- or state-scale emissions. Comparing DARTE with existing downscaled inventories, we find biases of 100% or more in the spatial distribution of urban and rural emissions, largely driven by mismatches between inventory downscaling proxies and the actual spatial patterns of vehicle activity at urban scales. Given cities' dual importance as sources of CO2 and an emerging nexus of climate mitigation initiatives, high-resolution estimates such as DARTE are critical both for accurately quantifying surface carbon fluxes and for verifying the effectiveness of emissions mitigation efforts at urban scales."
LUCY HUTYRA,"On the use of ‘cool roofs’ to reduce residential heat exposure disparities in Boston, MA","A “cool roofs” program targeted to the hottest, most vulnerable neighborhoods in Boston has the potential to significantly reduce urban heat islands and heat exposure disparities. Boston’s hottest neighborhoods have the highest proportion of flat black roofs, such as those on our famous triple deckers, which absorb rather than reflect heat. Because of the proportion of this type of roof and housing stock in Boston, a targeted program to whiten or lighten residential rooftops would have a measurable impact on reducing extreme heat, improving thermal comfort, and reducing energy use in summer. A similar program has recently been piloted in Louisville, KY, offering lessons for potential implementation in Boston. While Boston’s recent Heat Resilience Plan (City of Boston 2022) already highlights the need for a cool roof program, the focus is on commercial or city-owned property such as schools, and the intervention calls for grants to nonprofits rather than integration with Boston’s existing residential programs. Boston has an opportunity to invest in a more focused program targeting the hottest, most vulnerable residential blocks."
JEROME MERTZ,Multi-plane 3D optical voltage imaging using high-speed multi-Z confocal microscopy,
JEROME MERTZ,Neuronal imaging with ultrahigh dynamic range multiphoton microscopy,"Multiphoton microscopes are hampered by limited dynamic range, preventing weak sample features from being detected in the presence of strong features, or preventing the capture of unpredictable bursts in sample strength. We present a digital electronic add-on technique that vastly improves the dynamic range of a multiphoton microscope while limiting potential photodamage. The add-on provides real-time negative feedback to regulate the laser power delivered to the sample, and a log representation of the sample strength to accommodate ultrahigh dynamic range without loss of information. No microscope hardware modifications are required, making the technique readily compatible with commercial instruments. Benefits are shown in both structural and in-vivo functional mouse brain imaging applications."
JEROME MERTZ,Video-rate volumetric neuronal imaging using 3D targeted illumination,"Fast volumetric microscopy is required to monitor large-scale neural ensembles with high spatio-temporal resolution. Widefield fluorescence microscopy can image large 2D fields of view at high resolution and speed while remaining simple and costeffective. A focal sweep add-on can further extend the capacity of widefield microscopy by enabling extended-depth-of-field (EDOF) imaging, but suffers from an inability to reject out-of-focus fluorescence background. Here, by using a digital micromirror device to target only in-focus sample features, we perform EDOF imaging with greatly enhanced contrast and signal-to-noise ratio, while reducing the light dosage delivered to the sample. Image quality is further improved by the application of a robust deconvolution algorithm. We demonstrate the advantages of our technique for in vivo calcium imaging in the mouse brain."
JEROME MERTZ,Development of a beam propagation method to simulate the point spread function degradation in scattering media,"Scattering is one of the main issues that limit the imaging depth in deep tissue optical imaging. To characterize the role of scattering, we have developed a forward model based on the beam propagation method and established the link between the macroscopic optical properties of the media and the statistical parameters of the phase masks applied to the wavefront. Using this model, we have analyzed the degradation of the point-spread function of the illumination beam in the transition regime from ballistic to diffusive light transport. Our method provides a wave-optic simulation toolkit to analyze the effects of scattering on image quality degradation in scanning microscopy. Our open-source implementation is available at https://github.com/BUNPC/Beam-Propagation-Method."
JEROME MERTZ,Practical Implementation of Log-Scale Active Illumination Microscopy,"Active illumination microscopy (AIM) is a method of redistributing dynamic range in a scanning microscope using real-time feedback to control illumination power on a sub-pixel time scale. We describe and demonstrate a fully integrated instrument that performs both feedback and image reconstruction. The image is reconstructed on a logarithmic scale to accommodate the dynamic range benefits of AIM in a single output channel. A theoretical and computational analysis of the influence of noise on active illumination feedback is presented, along with imaging examples illustrating the benefits of AIM. While AIM is applicable to any type of scanning microscope, we apply it here specifically to two-photon microscopy."
JEROME MERTZ,Reverberation microscopy systems and methods,"A method for obtaining one or more images of a sample using a microscope includes dividing , using a reverberation cavity , a first one of a plurality of laser pulses into a plurality of sequential sub-pulses, each of the plurality of sequential sub-pulses having a power that is less than a previous one of the plurality of sequential sub-pulses, directing, using the one or more lenses of the microscope , the plurality of sequential sub-pulses onto a portion of the sample to generate a plurality of signals, each of the plurality of signals being associated with a different depth within the sample, and detecting the plurality of signals from the sample to generate one or more images of at least a portion of the sample."
JEROME MERTZ,Simultaneous multiplane imaging with reverberation multiphoton microscopy,"Multiphoton microscopy (MPM) has gained enormous popularity over the years for its capacity to provide high resolution images from deep within scattering samples1. However, MPM is generally based on single-point laser-focus scanning, which is intrinsically slow. While imaging speeds as fast as video rate have become routine for 2D planar imaging, such speeds have so far been unattainable for 3D volumetric imaging without severely compromising microscope performance. We demonstrate here 3D volumetric (multiplane) imaging at the same speed as 2D planar (single plane) imaging, with minimal compromise in performance. Specifically, multiple planes are acquired by near-instantaneous axial scanning while maintaining 3D micron-scale resolution. Our technique, called reverberation MPM, is well adapted for large-scale imaging in scattering media with low repetition-rate lasers, and can be implemented with conventional MPM as a simple add-on."
JEROME MERTZ,Large-scale deep tissue voltage imaging with targeted illumination confocal microscopy,"Voltage imaging with cellular specificity has been made possible by the tremendous advances in genetically encoded voltage indicators (GEVIs). However, the kilohertz rates required for voltage imaging lead to weak signals. Moreover, out-of-focus fluorescence and tissue scattering produce background that both undermines signal-to-noise ratio (SNR) and induces crosstalk between cells, making reliable in vivo imaging in densely labeled tissue highly challenging. We describe a microscope that combines the distinct advantages of targeted illumination and confocal gating, while also maximizing signal detection efficiency. The resulting benefits in SNR and crosstalk reduction are quantified experimentally and theoretically. Our microscope provides a versatile solution for enabling high-fidelity in vivo voltage imaging at large scales and penetration depths, which we demonstrate across a wide range of imaging conditions and different GEVI classes."
KATHLEEN M KANTAK,Methylphenidate treatment beyond adolescence maintains increased cocaine self-administration in the spontaneously hypertensive rat model of attention deficit/hyperactivity disorder,"Past research with the spontaneously hypertensive rat (SHR) model of attention deficit/hyperactivity disorder showed that adolescent methylphenidate treatment enhanced cocaine abuse risk in SHR during adulthood. The acquisition of cocaine self-administration was faster, and cocaine dose-response functions were shifted upward under fixed-ratio and progressive ratio schedules compared to adult SHR that received adolescent vehicle treatment or to control strains that received adolescent methylphenidate treatment. The current study determined if extending treatment beyond adolescence would ameliorate long-term consequences of adolescent methylphenidate treatment on cocaine abuse risk in adult SHR. Treatments (vehicle or 1.5mg/kg/day oral methylphenidate) began on postnatal day 28. Groups of male SHR were treated with vehicle during adolescence and adulthood, with methylphenidate during adolescence and vehicle during adulthood, or with methylphenidate during adolescence and adulthood. The group receiving adolescent-only methylphenidate was switched to vehicle on P56. Cocaine self-administration began on postnatal day 77, and groups receiving methylphenidate during adolescence and adulthood were treated either 1-h before or 1-h after daily sessions. At baseline under a fixed-ratio 1 schedule, cocaine self-administration (2h sessions; 0.3mg/kg unit dose) did not differ among the four treatment groups. Under a progressive ratio schedule (4.5h maximum session length; 0.01-1.0mg/kg unit doses), breakpoints for self-administered cocaine in SHR receiving the adult methylphenidate treatment 1-h pre-session were not different from the vehicle control group. However, compared to the vehicle control group, breakpoints for self-administered cocaine at the 0.3 and 1.0mg/kg unit doses were greater in adult SHR that received adolescent-only methylphenidate or received methylphenidate that was continued into adulthood and administered 1-h post-session. These findings suggest that extending methylphenidate treatment beyond adolescence does not ameliorate explicitly the long-term consequences of adolescent methylphenidate treatment. Pre-session methylphenidate may mask temporarily the detection of an increase in cocaine self-administration following chronic methylphenidate treatment."
KATHLEEN M KANTAK,Blockade of alpha 2-adrenergic receptors in prelimbic cortex: impact on cocaine self-administration in adult spontaneously hypertensive rats following adolescent atomoxetine treatment,"RATIONALE: Research with the spontaneously hypertensive rat (SHR) model of attention deficit/hyperactivity disorder demonstrated that chronic methylphenidate treatment during adolescence increased cocaine self-administration established during adulthood under a progressive ratio (PR) schedule. Compared to vehicle, chronic atomoxetine treatment during adolescence failed to increase cocaine self-administration under a PR schedule in adult SHR. OBJECTIVES: We determined if enhanced noradrenergic transmission at α2-adrenergic receptors within prefrontal cortex contributes to this neutral effect of adolescent atomoxetine treatment in adult SHR. METHODS: Following treatment from postnatal days 28–55 with atomoxetine (0.3 mg/kg) or vehicle, adult male SHR and control rats from Wistar-Kyoto (WKY) and Wistar (WIS) strains were trained to self-administer 0.3 mg/kg cocaine. Self-administration performance was evaluated under a PR schedule of cocaine delivery following infusion of the α2-adrenergic receptor antagonist idazoxan (0 and 10–56 μg/side) directly into prelimbic cortex. RESULTS: Adult SHR attained higher PR break points and had greater numbers of active lever responses and infusions than WKY and WIS. Idazoxan dose-dependently increased PR break points and active lever responses in SHR following adolescent atomoxetine vs. vehicle treatment. Behavioral changes were negligible after idazoxan pretreatment in SHR following adolescent vehicle or in WKY and WIS following adolescent atomoxetine or vehicle. CONCLUSIONS: α2-Adrenergic receptor blockade in prelimbic cortex of SHR masked the expected neutral effect of adolescent atomoxetine on adult cocaine self-administration behavior. Moreover, greater efficacy of acute idazoxan challenge in adult SHR after adolescent atomoxetine relative to vehicle is consistent with the idea that chronic atomoxetine may downregulate presynaptic α2A-adrenergic autoreceptors in SHR."
KATHLEEN M KANTAK,"Environmental enrichment facilitates cocaine-cue extinction, deters reacquisition of cocaine self-administration and alters AMPAR GluA1 expression and phosphorylation","This study investigated the combination of environmental enrichment (EE) with cocaine‐cue extinction training on reacquisition of cocaine self‐administration. Rats were trained under a second‐order schedule for which responses were maintained by cocaine injections and cocaine‐paired stimuli. During three weekly extinction sessions, saline was substituted for cocaine but cocaine‐paired stimuli were presented. Rats received 4‐h periods of EE at strategic time points during extinction training, or received NoEE. Additional control rats received EE or NoEE without extinction training. One week later, reacquisition of cocaine self‐administration was evaluated for 15 sessions, and then GluA1 expression, a cellular substrate for learning and memory, was measured in selected brain regions. EE provided both 24 h before and immediately after extinction training facilitated extinction learning and deterred reacquisition of cocaine self‐administration for up to 13 sessions. Each intervention by itself (EE alone or extinction alone) was ineffective, as was EE scheduled at individual time points (EE 4 h or 24 h before, or EE immediately or 6 h after, each extinction training session). Under these conditions, rats rapidly reacquired baseline rates of cocaine self‐administration. Cocaine self‐administration alone decreased total GluA1 and/or pSer845GluA1 expression in basolateral amygdala and nucleus accumbens. Extinction training, with or without EE, opposed these changes and also increased total GluA1 in ventromedial prefrontal cortex and dorsal hippocampus. EE alone increased pSer845GluA1 and EE combined with extinction training decreased pSer845GluA1 in ventromedial prefrontal cortex. EE might be a useful adjunct to extinction therapy by enabling neuroplasticity that deters relapse to cocaine self‐administration."
KATHLEEN M KANTAK,Adolescent D-amphetamine treatment in a rodent model of ADHD: pro-cognitive effects in adolescence without an impact on cocaine cue reactivity in adulthood,"Attention-deficit/hyperactivity disorder (ADHD) is comorbid with cocaine abuse. Whereas initiating ADHD medication in childhood does not alter later cocaine abuse risk, initiating medication during adolescence may increase risk. Preclinical work in the Spontaneously Hypertensive Rat (SHR) model of ADHD found that adolescent methylphenidate increased cocaine self-administration in adulthood, suggesting a need to identify alternatively efficacious medications for teens with ADHD. We examined effects of adolescent d-amphetamine treatment on strategy set shifting performance during adolescence and on cocaine self-administration and reinstatement of cocaine-seeking behavior (cue reactivity) during adulthood in male SHR, Wistar-Kyoto (inbred control), and Wistar (outbred control) rats. During the set shift phase, adolescent SHR needed more trials and had a longer latency to reach criterion, made more regressive errors and trial omissions, and exhibited slower and more variable lever press reaction times. d-Amphetamine improved performance only in SHR by increasing choice accuracy and decreasing errors and latency to criterion. In adulthood, SHR self-administered more cocaine, made more cocaine-seeking responses, and took longer to extinguish lever responding than control strains. Adolescent d-amphetamine did not alter cocaine self-administration in adult rats of any strain, but reduced cocaine seeking during the first of seven reinstatement test sessions in adult SHR. These findings highlight utility of SHR in modeling cognitive dysfunction and comorbid cocaine abuse in ADHD. Unlike methylphenidate, d-amphetamine improved several aspects of flexible learning in adolescent SHR and did not increase cocaine intake or cue reactivity in adult SHR. Thus, adolescent d-amphetamine was superior to methylphenidate in this ADHD model."
KATHLEEN M KANTAK,Necessity for research directed at stimulant type and treatment-onset age to access the impact of medication on drug abuse vulnerability in teenagers with ADHD,"Controversy continues regarding increased vulnerability for addiction to cocaine and other drugs of abuse in adulthood following the use of stimulant medications for the treatment of Attention Deficit Hyperactivity Disorder (ADHD). The results of recent research utilizing an animal model of ADHD strongly advocate for a closer look at this important issue in clinical populations, particularly where treatment is initiated in adolescence, and with certain ADHD medications."
KATHLEEN M KANTAK,Adolescent D-amphetamine treatment in a rodent model of ADHD: pro-cognitive effects during adolescence and cocaine abuse risk during adulthood,"Attention-deficit/hyperactivity disorder (ADHD) is comorbid with cocaine abuse. Whereas initiating ADHD medication in childhood does not alter later cocaine abuse risk, initiating medication during adolescence may increase risk. Preclinical work in the Spontaneously Hypertensive Rat (SHR) model of ADHD found that adolescent methylphenidate increased cocaine self-administration in adulthood, suggesting a need to identify alternatively efficacious medications for teens with ADHD. We examined effects of adolescent d-amphetamine treatment on strategy set shifting performance during adolescence and on cocaine self-administration and reinstatement of cocaine-seeking behavior (cue reactivity) during adulthood in male SHR, Wistar- Kyoto (inbred control), and Wistar (outbred control) rats. During the set shift phase, adolescent SHR needed more trials and had a longer latency to reach criterion, made more regressive errors and trial omissions, and exhibited slower and more variable lever press reaction times. d- Amphetamine improved performance only in SHR by increasing choice accuracy and decreasing errors and latency to criterion. In adulthood, SHR self-administered more cocaine, made more cocaine-seeking responses, and took longer to extinguish lever responding than control strains. Adolescent d-amphetamine did not alter cocaine self-administration in adult rats of any strain, but reduced cocaine seeking during the first of seven reinstatement test sessions in adult SHR. These findings highlight utility of SHR in modeling cognitive dysfunction and comorbid cocaine abuse in ADHD. Unlike methylphenidate, d-amphetamine improved several aspects of flexible learning in adolescent SHR and did not increase cocaine intake or cue reactivity in adult SHR. Thus, adolescent d-amphetamine was superior to methylphenidate in this ADHD model."
KATHLEEN M KANTAK,Predicting substance use disorder using long-term ADHD medication records in Truven,"About 20% of individuals with attention deficit hyperactivity disorder are first diagnosed during adolescence. While preclinical experiments suggest that adolescent-onset exposure to attention deficit hyperactivity disorder medication is an important factor in the development of substance use disorder phenotypes in adulthood, the long-term impact of attention deficit hyperactivity disorder medication initiated during adolescence has been largely unexplored in humans. Our analysis of 11,624 adolescent enrollees with attention deficit hyperactivity disorder in the Truven database indicates that temporal medication features, rather than stationary features, are the most important factors on the health consequences related to substance use disorder and attention deficit hyperactivity disorder medication initiation during adolescence."
KATHLEEN M KANTAK,Effect of methylphenidate treatment during adolescence on norepinephrine transporter function in orbitofrontal cortex in a rat model of attention deficit hyperactivity disorder,"Attention deficit hyperactivity disorder (ADHD) is associated with hypofunctional medial prefrontal cortex (mPFC) and orbitofrontal cortex (OFC). Methylphenidate (MPH) remediates ADHD, in part, by inhibiting the norepinephrine transporter (NET). MPH also reduces ADHD-like symptoms in spontaneously hypertensive rats (SHRs), a model of ADHD. However, effects of chronic MPH treatment on NET function in mPFC and OFC in SHR have not been reported. In the current study, long-term effects of repeated treatment with a therapeutically relevant oral dose of MPH during adolescence on NET function in subregions of mPFC (cingulate gyrus, prelimbic cortex and infralimbic cortex) and in the OFC of adult SHR, Wistar-Kyoto (WKY, inbred control) and Wistar (WIS, outbred control) rats were determined using in vivo voltammetry. Following local ejection of norepinephrine (NE), uptake rate was determined as peak amplitude (Amax)× first-order rate constant (k-1). In mPFC subregions, no strain or treatment effects were found in NE uptake rate. In OFC, NE uptake rate in vehicle-treated adult SHR was greater than in adult WKY and WIS administered vehicle. MPH treatment during adolescence normalized NE uptake rate in OFC in SHR. Thus, the current study implicates increased NET function in OFC as an underlying mechanism for reduced noradrenergic transmission in OFC, and consequently, the behavioral deficits associated with ADHD. MPH treatment during adolescence normalized NET function in OFC in adulthood, suggesting that the therapeutic action of MPH persists long after treatment cessation and may contribute to lasting reductions in deficits associated with ADHD."
KATHLEEN M KANTAK,Adolescent D-amphetamine treatment in a rodent model of attention deficit/hyperactivity disorder: impact on cocaine abuse vulnerability in adulthood,"RATIONALE: Stimulant medications for attention-deficit/hyperactivity disorder (ADHD) in adolescents remain controversial with respect to later development of cocaine abuse. Past research demonstrated that adolescent methylphenidate treatment increased several aspects of cocaine self-administration during adulthood using the spontaneously hypertensive rat (SHR) model of ADHD. Presently, we determined effects of the alternate stimulant medication, d-amphetamine, on cocaine self-administration. OBJECTIVES: We tested the hypothesis that adolescent d-amphetamine would not increase cocaine self-administration in adult SHR, given that d-amphetamine has a different mechanism of action than methylphenidate. METHODS: A pharmacologically relevant dose of d-amphetamine (0.5 mg/kg) or vehicle was administered throughout adolescence to SHR and two control strains, Wistar-Kyoto (WKY) and Wistar (WIS). Three aspects of cocaine abuse vulnerability were assessed in adulthood after discontinuing adolescent treatments: acquisition rate and dose-related responding under fixed (FR) and progressive (PR) ratio schedules. RESULTS: Adult SHR acquired cocaine self-administration faster and self-administered more cocaine across multiple doses compared to WKY and WIS under FR and PR schedules, indicating that SHR is a reliable animal model of comorbid ADHD and cocaine abuse. Relative to vehicle, SHR and WIS with adolescent d-amphetamine treatment self-administered less cocaine upon reaching acquisition criteria, and WIS additionally acquired cocaine self-administration more slowly and had downward shifts in FR and PR cocaine dose-response curves. WKY with adolescent d-amphetamine treatment acquired cocaine self-administration more quickly relative to vehicle. CONCLUSIONS: In contrast to methylphenidate, adolescent d-amphetamine did not augment cocaine self-administration in SHR. Adolescent d-amphetamine treatment actually protected against cocaine abuse vulnerability in adult SHR and WIS."
KATHLEEN M KANTAK,Integrating data science into the translational science research spectrum: a substance use disorder case study,"The availability of large healthcare datasets offers the opportunity for researchers to navigate the traditional clinical and translational science research stages in a nonlinear manner. In particular, data scientists can harness the power of large healthcare datasets to bridge from preclinical discoveries (T0) directly to assessing population-level health impact (T4). A successful bridge from T0 to T4 does not bypass the other stages entirely; rather, effective team science makes a direct progression from T0 to T4 impactful by incorporating the perspectives of researchers from every stage of the clinical and translational science research spectrum. In this exemplar, we demonstrate how effective team science overcame challenges and, ultimately, ensured success when a diverse team of researchers worked together, using healthcare big data to test population-level substance use disorder (SUD) hypotheses generated from preclinical rodent studies. This project, called Advancing Substance use disorder Knowledge using Big Data (ASK Big Data), highlights the critical roles that data science expertise and effective team science play in quickly translating preclinical research into public health impact."
KATHLEEN M KANTAK,Assessment of binge-like eating of unsweetened vs. sweetened chow pellets in BALB/c substrains,"Binge eating disorder (BED) is defined as chronic episodes of consuming large amounts of food in less than 2 h. Binge eating disorder poses a serious public health problem, as it increases the risk of obesity, type II diabetes, and heart disease. Binge eating is a highly heritable trait; however, its genetic basis remains largely unexplored. We employed a mouse model for binge eating that focused on identifying heritable differences between inbred substrains in acute and escalated intake of sucrose-sweetened palatable food vs. unsweetened chow pellets in a limited, intermittent access paradigm. In the present study, we examined two genetically similar substrains of BALB/c mice for escalation in food consumption, incubation of craving after a no-food training period, and compulsive-like food consumption in an aversive context. BALB/cJ and BALB/cByJ mice showed comparable levels of acute and escalated consumption of palatable food across training trials. Surprisingly, BALB/cByJ mice also showed binge-like eating of the unsweetened chow pellets similar to the escalation in palatable food intake of both substrains. Finally, we replicated the well-documented decrease in anxiety-like behavior in BALB/cByJ mice in the light-dark conflict test that likely contributed to greater palatable food intake than BALB/cJ in the light arena. To summarize, BALB/cByJ mice show binge-like eating in the presence and absence of sucrose. Possible explanations for the lack of selectivity in binge-like eating across diets (e.g., novelty preference, taste) are discussed."
KATHLEEN M KANTAK,Spontaneously hypertensive rat substrains show differences in model traits for addiction risk and cocaine self-administration: implications for a novel rat reduced complexity cross,"Forward genetic mapping of F2 crosses between closely related substrains of inbred rodents - referred to as a reduced complexity cross (RCC) - is a relatively new strategy for accelerating the pace of gene discovery for complex traits, such as drug addiction. RCCs to date were generated in mice, but rats are thought to be optimal for addiction genetic studies. Based on past literature, one inbred Spontaneously Hypertensive Rat substrain, SHR/NCrl, is predicted to exhibit a distinct behavioral profile as it relates to cocaine self-administration traits relative to another substrain, SHR/NHsd. Direct substrain comparisons are a necessary first step before implementing an RCC. We evaluated model traits for cocaine addiction risk and cocaine self-administration behaviors using a longitudinal within-subjects design. Impulsive-like and compulsive-like traits were greater in SHR/NCrl than SHR/NHsd, as were reactivity to sucrose reward, sensitivity to acute psychostimulant effects of cocaine, and cocaine use studied under fixed-ratio and tandem schedules of cocaine self-administration. Compulsive-like behavior correlated with the acute psychostimulant effects of cocaine, which in turn correlated with cocaine taking under the tandem schedule. Compulsive-like behavior also was the best predictor of cocaine seeking responses. Heritability estimates indicated that 22 %-40 % of the variances for the above phenotypes can be explained by additive genetic factors, providing sufficient genetic variance to conduct genetic mapping in F2 crosses of SHR/NCrl and SHR/NHsd. These results provide compelling support for using an RCC approach in SHR substrains to uncover candidate genes and variants that are of relevance to cocaine use disorders."
KATHLEEN M KANTAK,"Corrigendum to ""Adolescent-onset vs. adult-onset cocaine use: impact on cognitive functioning in animal models and opportunities for translation"" [Pharmacol. Biochem. Behav. Volume 196, September (2020) 172994]",
MICHAEL DIETZE,Alternative stable states of the forest mycobiome are maintained through positive feedbacks,"Most trees on Earth form a symbiosis with either arbuscular mycorrhizal or ectomycorrhizal fungi. By forming common mycorrhizal networks, actively modifying the soil environment and other ecological mechanisms, these contrasting symbioses may generate positive feedbacks that favour their own mycorrhizal strategy (that is, the con-mycorrhizal strategy) at the expense of the alternative strategy. Positive con-mycorrhizal feedbacks set the stage for alternative stable states of forests and their fungi, where the presence of different forest mycorrhizal strategies is determined not only by external environmental conditions but also mycorrhiza-mediated feedbacks embedded within the forest ecosystem. Here, we test this hypothesis using thousands of US forest inventory sites to show that arbuscular and ectomycorrhizal tree recruitment and survival exhibit positive con-mycorrhizal density dependence. Data-driven simulations show that these positive feedbacks are sufficient in magnitude to generate and maintain alternative stable states of the forest mycobiome. Given the links between forest mycorrhizal strategy and carbon sequestration potential, the presence of mycorrhizal-mediated alternative stable states affects how we forecast forest composition, carbon sequestration and terrestrial climate feedbacks."
MICHAEL DIETZE,Adding tree rings to North America's National Forest Inventories: an essential tool to guide drawdown of atmospheric CO2,"Tree-ring time series provide long-term, annually resolved information on the growth of trees. When sampled in a systematic context, tree-ring data can be scaled to estimate the forest carbon capture and storage of landscapes, biomes, and-ultimately-the globe. A systematic effort to sample tree rings in national forest inventories would yield unprecedented temporal and spatial resolution of forest carbon dynamics and help resolve key scientific uncertainties, which we highlight in terms of evidence for forest greening (enhanced growth) versus browning (reduced growth, increased mortality). We describe jump-starting a tree-ring collection across the continent of North America, given the commitments of Canada, the United States, and Mexico to visit forest inventory plots, along with existing legacy collections. Failing to do so would be a missed opportunity to help chart an evidence-based path toward meeting national commitments to reduce net greenhouse gas emissions, urgently needed for climate stabilization and repair."
MICHAEL DIETZE,A novel model–data fusion approach to terrestrial carbon cycle reanalysis across the contiguous U.S using SIPNET and PEcAn state data assimilation system v. 1.7.2,"The ability to monitor, understand, and predict the dynamics of the terrestrial carbon cycle requires the capacity to robustly and coherently synthesize multiple streams of information that each provide partial information about different pools and fluxes. In this study, we introduce a new terrestrial carbon cycle data assimilation system, built on the PEcAn modeldata eco-informatics system, and its application for the development of a proof-of-concept carbon ""reanalysis"" product that harmonizes carbon 5 pools (leaf, wood, soil) and fluxes (GPP, Ra, Rh, NEE) across the contiguous United States from 1986- 2019. We first calibrated this system against plant trait and flux tower Net Ecosystem Exchange (NEE) using a novel emulated hierarchical Bayesian approach. Next, we extended the Tobit-Wishart Ensemble Filter (TWEnF) State Data Assimilation (SDA) framework, a generalization of the common Ensemble Kalman Filter which accounts for censored data and provides a fully Bayesian estimate of model process error, to a regional-scale system with a calibrated localization. Combined with additional 10 workflows for propagating parameter, initial condition, and driver uncertainty, this represents the most complete and robust uncertainty accounting available for terrestrial carbon models. Our initial reanalysis was run on an irregular grid of   500 points selected using a stratified sampling method to efficiently capture environmental heterogeneity. Remotely sensed observations of aboveground biomass (Landsat LandTrendr) and LAI (MODIS MOD15) were sequentially assimilated into the SIPNET model. Reanalysis soil carbon, which was indirectly constrained based on modeled covariances, showed general agreement 15 with SoilGrids, an independent soil carbon data product. Reanalysis NEE, which was constrained based on posterior ensemble weights, also showed good agreement with eddy flux tower NEE and reduced RMSE compared to the calibrated forecast. Ultimately, PEcAn’s carbon cycle reanalysis provides a scalable framework for harmonizing multiple data constraints and providing a uniform synthetic platform for carbon monitoring, reporting, and verification (MRV) and accelerating terrestrial carbon cycle research."
MICHAEL DIETZE,The terrestrial biosphere model farm,
MICHAEL DIETZE,A community convention for ecological forecasting: output files and metadata,"This document summarizes the open community standards developed by the Ecological Forecasting Initiative (EFI) for the common formatting and archiving of ecological forecasts and the metadata associated with these forecasts. Such open standards are intended to promote interoperability and facilitate forecast adoption, distribution, validation, and synthesis. For output files EFI has adopted a three-tiered approach reflecting trade-offs in forecast data volume and technical expertise. The preferred output file format is netCDF following the Climate and Forecast Convention for dimensions and variable naming, including an ensemble dimension where appropriate. The second-tier option is a semi-long CSV format, with state variables as columns and each row representing a unique issue date time, prediction date time, location, ensemble member, etc. The third-tier option is similar to option 2, but each row represents a specific summary statistic (mean, upper/lower CI) rather than individual ensemble members. For metadata, EFI expands upon the Ecological Metadata Language (EML), using additional Metadata tags to store information designed to facilitate cross-forecast synthesis (e.g. uncertainty propagation, data assimilation, model complexity) and setting a subset of base EML tags (e.g. temporal resolution, output variables) to be required. To facilitate community adoption we also provides a R package containing a number of vignettes on how to both write and read in the EFI standard, as well as a metadata validator tool."
MICHAEL DIETZE,Using near-term forecasts and uncertainty partitioning to improve predictions of low-frequency cyanobacterial events,"Near-term ecological forecasts provide resource managers advance notice of changes in ecosystem services, such as fisheries stocks, timber yields, or water and air quality. Importantly, ecological forecasts can identify where uncertainty enters the forecasting system, which is necessary to refine and improve forecast skill and guide interpretation of forecast results. Uncertainty partitioning identifies the relative contributions to total forecast variance (uncertainty) introduced by different sources, including specification of the model structure, errors in driver data, and estimation of initial state conditions. Uncertainty partitioning could be particularly useful in improving forecasts of high-density cyanobacterial events, which are difficult to predict and present a persistent challenge for lake managers. Cyanobacteria can produce toxic or unsightly surface scums and advance warning of these events could help managers mitigate water quality issues. Here, we calibrate fourteen Bayesian state-space models to evaluate different hypotheses about cyanobacterial growth using data from eight summers of weekly cyanobacteria density samples in an oligotrophic (low nutrient) lake that experiences sporadic surface scums of the toxin-producing cyanobacterium, Gloeotrichia echinulata. We identify dominant sources of uncertainty for near-term (one-week to four-week) forecasts of G. echinulata densities over two years. Water temperature was an important predictor in calibration and at the four-week forecast horizon. However, no environmental covariates improved over a simple autoregressive (AR) model at the one-week horizon. Even the best fit models exhibited large variance in forecasted cyanobacterial densities and often did not capture rare peak density occurrences, indicating that significant explanatory variables in calibration are not always effective for near-term forecasting of low-frequency events. Uncertainty partitioning revealed that model process specification and initial conditions uncertainty dominated forecasts at both time horizons. These findings suggest that observed densities result from both growth and movement of G. echinulata, and that imperfect observations as well as spatial misalignment of environmental data and cyanobacteria observations affect forecast skill. Future research efforts should prioritize long-term studies to refine process understanding and increased sampling frequency and replication to better define initial conditions. Our results emphasize the importance of ecological forecasting principles and uncertainty partitioning to refine and understand predictive capacity across ecosystems."
MICHAEL DIETZE,Targeting extreme events: complementing near-term ecological forecasting with rapid experiments and regional surveys,"Ecologists are improving predictive capability using near-term ecological forecasts, in which predictions are made iteratively and publically to increase transparency, rate of learning, and maximize utility. Ongoing ecological forecasting efforts focus mostly on long-termdatasets of continuous variables, such as CO2 fluxes, ormore abrupt variables, such as phenological events or algal blooms. Generally lacking from these forecasting efforts is the integration of short-term, opportunistic data concurrent with developing climate extremes such as drought.We posit that incorporating targeted experiments and regional surveys, implemented rapidly during developing extreme events, into current forecasting efforts will ultimately enhance our ability to forecast ecological responses to climate extremes, which are projected to increase in both frequency and intensity. We highlight a project, “chasing tree die-off,” in which we coupled an experiment with regional-scale observational field surveys during a developing severe drought to test and improve forecasts of tree die-off. General insights to consider in incorporating this approach include: (1) tracking developing climate extremes in near-real time to efficiently ramp up measurements rapidly and, if feasible, initiate an experiment quickly—including funding and site selection challenges; (2) accepting uncertainty in projected extreme climatic events and adjusting sampling design over-time as needed, especially given the spatially heterogeneous nature of many ecological disturbances; and (3) producing timely and iterative output. In summary, targeted experiments and regional surveys implemented rapidly during developing extreme climatic events offer promise to efficiently (both financially and logistically) improve our ability to forecast ecological responses to climate extremes."
MICHAEL DIETZE,Imaging X-ray polarimetry explorer: prelaunch,"Launched on 2021 December 9, the Imaging X-ray Polarimetry Explorer (IXPE) is a NASA Small Explorer Mission in collaboration with the Italian Space Agency (ASI). The mission will open a new window of investigation—imaging x-ray polarimetry. The observatory features three identical telescopes, each consisting of a mirror module assembly with a polarization-sensitive imaging x-ray detector at the focus. A coilable boom, deployed on orbit, provides the necessary 4-m focal length. The observatory utilizes a three-axis-stabilized spacecraft, which provides services such as power, attitude determination and control, commanding, and telemetry to the ground. During its 2-year baseline mission, IXPE will conduct precise polarimetry for samples of multiple categories of x-ray sources, with follow-on observations of selected targets."
MICHAEL DIETZE,Forecasting a bright future for ecology,
MICHAEL DIETZE,Evaluation of 11 terrestrial carbon–nitrogen cycle models against observations from two temperate Free‐Air CO2 Enrichment studies,"We analysed the responses of 11 ecosystem models to elevated atmospheric [CO2] (eCO2) at two temperate forest ecosystems (Duke and Oak Ridge National Laboratory (ORNL) Free‐Air CO2 Enrichment (FACE) experiments) to test alternative representations of carbon (C)–nitrogen (N) cycle processes. We decomposed the model responses into component processes affecting the response to eCO2 and confronted these with observations from the FACE experiments. Most of the models reproduced the observed initial enhancement of net primary production (NPP) at both sites, but none was able to simulate both the sustained 10‐yr enhancement at Duke and the declining response at ORNL: models generally showed signs of progressive N limitation as a result of lower than observed plant N uptake. Nonetheless, many models showed qualitative agreement with observed component processes. The results suggest that improved representation of above‐ground–below‐ground interactions and better constraints on plant stoichiometry are important for a predictive understanding of eCO2 effects. Improved accuracy of soil organic matter inventories is pivotal to reduce uncertainty in the observed C–N budgets. The two FACE experiments are insufficient to fully constrain terrestrial responses to eCO2, given the complexity of factors leading to the observed diverging trends, and the consequential inability of the models to explain these trends. Nevertheless, the ecosystem models were able to capture important features of the experiments, lending some support to their projections."
MICHAEL DIETZE,Priorities for synthesis research in ecology and environmental science,
MICHAEL DIETZE,Determinants of predictability in multi-decadal forest community and carbon dynamics,
MICHAEL DIETZE,Ecological forecasting—21st century science for 21st century management,"Natural resource managers are coping with rapid changes in both environmental conditions and ecosystems. Enabled by recent advances in data collection and assimilation, short-term ecological forecasting may be a powerful tool to help resource managers anticipate impending near-term changes in ecosystem conditions or dynamics. Managers may use the information in forecasts to minimize the adverse effects of ecological stressors and optimize the effectiveness of management actions. To explore the potential for ecological forecasting to enhance natural resource management, the U.S. Geological Survey (USGS) convened a workshop titled ""Building Capacity for Applied Short-Term Ecological Forecasting"" on May 29—31, 2019, with participants from several Federal agencies, including the Bureau of Land Management, the U.S. Fish and Wildlife Service, the National Park Service, and the National Oceanic and Atmospheric Administration as well as all mission areas within the USGS. Participants broadly agreed that short-term ecological forecasting—on the order of days to years into the future—has tremendous potential to improve the quality and timeliness of information available to guide resource management decisions. Participants considered how ecological forecasting could directly affect their agency missions and specified numerous critical tools for addressing natural resource management concerns in the 21st century that could be enhanced by ecological forecasting. Given this breadth of possible applications for forecast products, participants developed a repeatable framework for evaluating potential value of a forecast product for enhancing resource management. Applying that process to a large list of forecast ideas that were developed in a brainstorming session, participants identified a small set of promising forecast products that illustrate the value of ecological forecasting for informing resource management. Workshop outcomes also include insights about important likely obstacles and next steps. In particular, reliable production and delivery of operational ecological forecasts will require a sustained commitment by research agencies, in partnership with resource management agencies, to maintain and improve forecasting tools and capabilities."
MICHAEL DIETZE,Cutting out the middleman: calibrating and validating a dynamic vegetation model (ED2-PROSPECT5) using remotely sensed surface reflectance,"Ecosystem models are often calibrated and/or validated against derived remote sensing data products, such as MODIS leaf area index. However, these data products are generally based on their own models, whose assumptions may not be compatible with those of the ecosystem model in question, and whose uncertainties are usually not well quantified. Here, we develop an alternative approach whereby we modify an ecosystem model to predict full-range, high spectral resolution surface reflectance, which can then be compared directly against airborne and satellite data. Specifically, we coupled the two-stream representation of canopy radiative transfer in the Ecosystem Demography model (ED2) with a leaf radiative transfer model (PROSPECT 5) and a simple soil reflectance model. We then calibrated this model against reflectance observations from the NASA Airborne VIsible/InfraRed Imaging Spectrometer (AVIRIS) and survey data from 54 temperate forest plots in the northeastern United States. The calibration successfully constrained the posterior distributions of model parameters related to leaf biochemistry and morphology and canopy structure for five plant functional types. The calibrated model was able to accurately reproduce surface reflectance and leaf area index for sites with highly varied forest composition and structure, using a single common set of parameters across all sites. We conclude that having dynamic vegetation models directly predict surface reflectance is a promising avenue for model calibration and validation using remote sensing data."
MICHAEL DIETZE,Improving the monitoring of deciduous broadleaf phenology using the Geostationary Operational Environmental Satellite (GOES) 16 and 17,"Monitoring leaf phenology allows for tracking the progression of climate change and seasonal variations in a variety of organismal and ecosystem processes. Networks of finite-scale remote sensing, such as the PhenoCam Network, provide valuable information on phenological state at high temporal resolution, but have limited coverage. To more broadly remotely sense phenology, satellite-based data that has lower temporal resolution has primarily been used (e.g., 16-day MODIS NDVI 10 product). Recent versions of the Geostationary Operational Environmental Satellites (GOES-16 and -17) allow the monitoring of NDVI at temporal scales comparable to that of PhenoCam throughout most of the western hemisphere. Here we examine the current capacity of this new data to measure the phenology of deciduous broadleaf forests for the first two full calendar years of data (2018 and 2019) by fitting double-logistic Bayesian models and comparing the start, middle, and end of season transition dates to those obtained from PhenoCam and MODIS 16-day NDVI and EVI products. Compared to the MODIS 15 indices, GOES was more correlated with PhenoCam at the start and middle of spring, but had a larger bias (3.35 ± 0.03 days later than PhenoCam) at the end of spring. Satellite-based autumn transition dates were mostly uncorrelated with those of PhenoCam. PhenoCam data produced significantly more certain (all p-values £ 0.013) estimates of all transition dates than any of the satellite sources did. GOES transition date uncertainties were significantly smaller than those of MODIS EVI for all transition dates (all p-values £ 0.026), but were only smaller (based on p-value < 0.05) than those from MODIS NDVI for 20 the beginning and middle of spring estimates. GOES will improve the monitoring of phenology at large spatial coverages and is able to provide real-time indicators of phenological change even for spring transitions that might occur within the 16-day resolution of these MODIS products."
MICHAEL DIETZE,Addressing data integration challenges to link ecological processes across scales,"Data integration is a statistical modeling approach that incorporates multiple data sources within a unified analytical framework. Macrosystems ecology – the study of ecological phenomena at broad scales, including interactions across scales – increasingly employs data integration techniques to expand the spatiotemporal scope of research and inferences, increase the precision of parameter estimates, and account for multiple sources of uncertainty in estimates of multiscale processes. We highlight four common analytical challenges to data integration in macrosystems ecology research: data scale mismatches, unbalanced data, sampling biases, and model development and assessment. We explain each problem, discuss current approaches to address the issue, and describe potential areas of research to overcome these hurdles. Use of data integration techniques has increased rapidly in recent years, and given the inferential value of such approaches, we expect continued development and wider application across ecological disciplines, especially in macrosystems ecology."
MICHAEL DIETZE,Training macrosystems scientists requires both interpersonal and technical skills,"Macrosystems science strives to integrate patterns and processes that span regional to continental scales. The scope of such research often necessitates the involvement of large interdisciplinary and/or multi-institutional teams composed of scientists across a range of career stages, a diversity that requires researchers to hone both technical and interpersonal skills. We surveyed participants in macrosystems projects funded by the US National Science Foundation to assess the perceived importance of different skills needed in their research, as well as the types of training they received. Survey results revealed a mismatch between the skills participants perceive as important and the training they received, particularly for interpersonal and management skills. We highlight lessons learned from macrosystems training case studies, explore avenues for further improvement of undergraduate and graduate education, and discuss other training opportunities for macrosystems scientists. Given the trend toward interdisciplinary research beyond the macrosystems community, these insights are broadly applicable for scientists involved in diverse, collaborative projects."
MICHAEL DIETZE,Predicting spring phenology in deciduous broadleaf forests: NEON phenology forecasting community challenge,
MICHAEL DIETZE,The power of forecasts to advance ecological theory,"Ecological forecasting provides a powerful set of methods for predicting short‐ and long‐term change in living systems. Forecasts are now widely produced, enabling proactive management for many applied ecological problems. However, despite numerous calls for an increased emphasis on prediction in ecology, the potential for forecasting to accelerate ecological theory development remains underrealized. Here, we provide a conceptual framework describing how ecological forecasts can energize and advance ecological theory. We emphasize the many opportunities for future progress in this area through increased forecast development, comparison and synthesis. Our framework describes how a forecasting approach can shed new light on existing ecological theories while also allowing researchers to address novel questions. Through rigorous and repeated testing of hypotheses, forecasting can help to refine theories and understand their generality across systems. Meanwhile, synthesizing across forecasts allows for the development of novel theory about the relative predictability of ecological variables across forecast horizons and scales. We envision a future where forecasting is integrated as part of the toolset used in fundamental ecology. By outlining the relevance of forecasting methods to ecological theory, we aim to decrease barriers to entry and broaden the community of researchers using forecasting for fundamental ecological insight."
MICHAEL DIETZE,A community convention for ecological forecasting: output files and metadata version 1.0,"This paper summarizes the open community conventions developed by the Ecological Forecasting Initiative (EFI) for the common formatting and archiving of ecological forecasts and the metadata associated with these forecasts. Such open standards are intended to promote interoperability and facilitate forecast communication, distribution, validation, and synthesis. For output files, we first describe the convention conceptually in terms of global attributes, forecast dimensions, forecasted variables, and ancillary indicator variables. We then illustrate the application of this convention to the two file formats that are currently preferred by the EFI, netCDF (network common data form), and comma‐separated values (CSV), but note that the convention is extensible to future formats. For metadata, EFI's convention identifies a subset of conventional metadata variables that are required (e.g., temporal resolution and output variables) but focuses on developing a framework for storing information about forecast uncertainty propagation, data assimilation, and model complexity, which aims to facilitate cross‐forecast synthesis. The initial application of this convention expands upon the Ecological Metadata Language (EML), a commonly used metadata standard in ecology. To facilitate community adoption, we also provide a Github repository containing a metadata validator tool and several vignettes in R and Python on how to both write and read in the EFI standard. Lastly, we provide guidance on forecast archiving, making an important distinction between short‐term dissemination and long‐term forecast archiving, while also touching on the archiving of code and workflows. Overall, the EFI convention is a living document that can continue to evolve over time through an open community process."
MICHAEL DIETZE,A reporting format for leaf-level gas exchange data and metadata,"Leaf-level gas exchange data support the mechanistic understanding of plant fluxes of carbon and water. These fluxes inform our understanding of ecosystem function, are an important constraint on parameterization of terrestrial biosphere models, are necessary to understand the response of plants to global environmental change, and are integral to efforts to improve crop production. Collection of these data using gas analyzers can be both technically challenging and time consuming, and individual studies generally focus on a small range of species, restricted time periods, or limited geographic regions. The high value of these data is exemplified by the many publications that reuse and synthesize gas exchange data, however the lack of metadata and data reporting conventions make full and efficient use of these data difficult. Here we propose a reporting format for leaf-level gas exchange data and metadata to provide guidance to data contributors on how to store data in repositories to maximize their discoverability, facilitate their efficient reuse, and add value to individual datasets. For data users, the reporting format will better allow data repositories to optimize data search and extraction, and more readily integrate similar data into harmonized synthesis products. The reporting format specifies data table variable naming and unit conventions, as well as metadata characterizing experimental conditions and protocols. For common data types that were the focus of this initial version of the reporting format, i.e., survey measurements, dark respiration, carbon dioxide and light response curves, and parameters derived from those measurements, we took a further step of defining required additional data and metadata that would maximize the potential reuse of those data types. To aid data contributors and the development of data ingest tools by data repositories we provided a translation table comparing the outputs of common gas exchange instruments. Extensive consultation with data collectors, data users, instrument manufacturers, and data scientists was undertaken in order to ensure that the reporting format met community needs. The reporting format presented here is intended to form a foundation for future development that will incorporate additional data types and variables as gas exchange systems and measurement approaches advance in the future. The reporting format is published in the U.S. Department of Energy's ESS-DIVE data repository, with documentation and future development efforts being maintained in a version control system."
PAUL WITHERS,The Dependence of Peak Electron Density on Solar Irradiance in the Ionosphere of Mars,
PAUL WITHERS,Pioneer Venus Orbiter Radio Occultation Profiles,
ROBERT TSAI,The color of cancer: margin guidance for oral cancer resection using elastic scattering spectroscopy,"OBJECTIVES/HYPOTHESIS: To evaluate the usefulness of elastic scattering spectroscopy (ESS) as a diagnostic adjunct to frozen section analysis in patients with diagnosed squamous cell carcinoma of the oral cavity. STUDY DESIGN: Prospective analytic study. METHODS: Subjects for this single institution, institutional review board-approved study were recruited from among patients undergoing surgical resection for squamous cell cancer of the oral cavity. A portable ESS device with a contact fiberoptic probe was used to obtain spectral signals. Four to 10 spectral readings were obtained on each subject from various sites including gross tumor and normal-appearing mucosa in the surgical margin. Each reading was correlated with the histopathologic findings of biopsies taken from the exact location of the spectral readings. A diagnostic algorithm based on multidimensional pattern recognition/machine learning was developed. Sensitivity and specificity, error rate, and area under the curve were used as performance metrics for tests involving classification between disease and nondisease classes. RESULTS: Thirty-four (34) subjects were enrolled in the study. One hundred seventy-six spectral data point/biopsy specimen pairs were available for analysis. ESS distinguished normal from abnormal tissue, with a sensitivity ranging from 84% to 100% and specificity ranging from 71% to 89%, depending on how the cutoff between normal and abnormal tissue was defined (i.e., mild, moderate, or severe dysplasia). There were statistically significant differences in malignancy scores between histologically normal tissue and invasive cancer and between noninflamed tissue and inflamed tissue. CONCLUSIONS: This is the first study to evaluate the effectiveness of ESS in guiding mucosal resection margins in oral cavity cancer. ESS provides fast, real-time assessment of tissue without the need for pathology expertise. ESS appears to be effective in distinguishing between normal mucosa and invasive cancer and between ""normal"" tissue (histologically normal and mild dysplasia) and ""abnormal"" tissue (severe dysplasia and carcinoma in situ) that might require further margin resection. Further studies, however, are needed with a larger sample size to validate these findings and to determine the effectiveness of ESS in distinguishing visibly and histologically normal tissue from visibly normal but histologically abnormal tissue. LEVEL OF EVIDENCE: NA Laryngoscope, 127:S1-S9, 2017."
ROBERT TSAI,Identification and reconstruction of low-energy electrons in the ProtoDUNE-SP detector,
ROBERT TSAI,The long-baseline neutrino experiment: exploring fundamental symmetries of the universe,"The preponderance of matter over antimatter in the early Universe, the dynamics of the supernova bursts that produced the heavy elements necessary for life and whether protons eventually decay --- these mysteries at the forefront of particle physics and astrophysics are key to understanding the early evolution of our Universe, its current state and its eventual fate. The Long-Baseline Neutrino Experiment (LBNE) represents an extensively developed plan for a world-class experiment dedicated to addressing these questions. LBNE is conceived around three central components: (1) a new, high-intensity neutrino source generated from a megawatt-class proton accelerator at Fermi National Accelerator Laboratory, (2) a near neutrino detector just downstream of the source, and (3) a massive liquid argon time-projection chamber deployed as a far detector deep underground at the Sanford Underground Research Facility. This facility, located at the site of the former Homestake Mine in Lead, South Dakota, is approximately 1,300 km from the neutrino source at Fermilab -- a distance (baseline) that delivers optimal sensitivity to neutrino charge-parity symmetry violation and mass ordering effects. This ambitious yet cost-effective design incorporates scalability and flexibility and can accommodate a variety of upgrades and contributions. With its exceptional combination of experimental configuration, technical capabilities, and potential for transformative discoveries, LBNE promises to be a vital facility for the field of particle physics worldwide, providing physicists from around the globe with opportunities to collaborate in a twenty to thirty year program of exciting science. In this document we provide a comprehensive overview of LBNE's scientific objectives, its place in the landscape of neutrino physics worldwide, the technologies it will incorporate and the capabilities it will possess."
ROBERT TSAI,Scintillation light detection in the 6-m drift-length ProtoDUNE Dual Phase liquid argon TPC,"DUNE is a dual-site experiment for long-baseline neutrino oscillation studies, neutrino astrophysics and nucleon decay searches. ProtoDUNE Dual Phase (DP) is a 6  ×  6  ×  6 m 3 liquid argon time-projection-chamber (LArTPC) that recorded cosmic-muon data at the CERN Neutrino Platform in 2019-2020 as a prototype of the DUNE Far Detector. Charged particles propagating through the LArTPC produce ionization and scintillation light. The scintillation light signal in these detectors can provide the trigger for non-beam events. In addition, it adds precise timing capabilities and improves the calorimetry measurements. In ProtoDUNE-DP, scintillation and electroluminescence light produced by cosmic muons in the LArTPC is collected by photomultiplier tubes placed up to 7 m away from the ionizing track. In this paper, the ProtoDUNE-DP photon detection system performance is evaluated with a particular focus on the different wavelength shifters, such as PEN and TPB, and the use of Xe-doped LAr, considering its future use in giant LArTPCs. The scintillation light production and propagation processes are analyzed and a comparison of simulation to data is performed, improving understanding of the liquid argon properties."
CHARLES P DELISI,COMBREX: A Project to Accelerate the Functional Annotation of Prokaryotic Genomes,COMBREX (http://combrex.bu.edu) is a project to increase the speed of the functional annotation of new bacterial and archaeal genomes. It consists of a database of functional predictions produced by computational biologists and a mechanism for experimental biochemists to bid for the validation of those predictions. Small grants are available to support successful bids.
CHARLES P DELISI,Portraits of Breast Cancer Progression,"BACKGROUND. Clustering analysis of microarray data is often criticized for giving ambiguous results because of sensitivity to data perturbation or clustering techniques used. In this paper, we describe a new method based on principal component analysis and ensemble consensus clustering that avoids these problems. RESULTS. We illustrate the method on a public microarray dataset from 36 breast cancer patients of whom 31 were diagnosed with at least two of three pathological stages of disease (atypical ductal hyperplasia (ADH), ductal carcinoma in situ (DCIS) and invasive ductal carcinoma (IDC). Our method identifies an optimum set of genes and divides the samples into stable clusters which correlate with clinical classification into Luminal, Basal-like and Her2+ subtypes. Our analysis reveals a hierarchical portrait of breast cancer progression and identifies genes and pathways for each stage, grade and subtype. An intriguing observation is that the disease phenotype is distinguishable in ADH and progresses along distinct pathways for each subtype. The genetic signature for disease heterogeneity across subtypes is greater than the heterogeneity of progression from DCIS to IDC within a subtype, suggesting that the disease subtypes have distinct progression pathways. Our method identifies six disease subtype and one normal clusters. The first split separates the normal samples from the cancer samples. Next, the cancer cluster splits into low grade (pathological grades 1 and 2) and high grade (pathological grades 2 and 3) while the normal cluster is unchanged. Further, the low grade cluster splits into two subclusters and the high grade cluster into four. The final six disease clusters are mapped into one Luminal A, three Luminal B, one Basal-like and one Her2+. CONCLUSION. We confirm that the cancer phenotype can be identified in early stage because the genes altered in this stage progressively alter further as the disease progresses through DCIS into IDC. We identify six subtypes of disease which have distinct genetic signatures and remain separated in the clustering hierarchy. Our findings suggest that the heterogeneity of disease across subtypes is higher than the heterogeneity of the disease progression within a subtype, indicating that the subtypes are in fact distinct diseases."
CHARLES P DELISI,COMBREX: a project to accelerate the functional annotation of prokaryotic genomes,COMBREX (http://combrex.bu.edu) is a project to increase the speed of the functional annotation of new bacterial and archaeal genomes. It consists of a database of functional predictions produced by computational biologists and a mechanism for experimental biochemists to bid for the validation of those predictions. Small grants are available to support successful bids.
MERAV OPHER,The Solar-wind with Hydrogen Ion Exchange and Large-scale Dynamics (SHIELD) model: a self-consistent kinetic-MHD model of the outer heliosphere,"Neutral hydrogen has been shown to greatly impact the plasma flow in the heliopshere and the location of the heliospheric boundaries. We present the re- sults of the Solar-wind with Hydrogen Ion Exchange and Large-scale Dynamics (SHIELD) model, a new, self-consistent, kinetic-MHD model of the outer helio- sphere within the Space Weather Modeling Framework. The charge-exchange mean free path is on order of the size of the heliosphere; therefore, the neutral atoms cannot be described as a fluid. The SHIELD model couples the MHD so- lution for a single plasma fluid to the kinetic solution from for neutral hydrogen atoms streaming through the system. The kinetic code is based on the Adaptive Mesh Particle Simulator (AMPS), a Monte Carlo method for solving the Boltz- mann equation. The SHIELD model accurately predicts the increased filtration of interstellar neutrals into the heliosphere. In order to verify the correct imple- mentation within the model, we compare the results of the SHIELD model to other, well-established kinetic-MHD models. The SHIELD model matches the neutral hydrogen solution of these studies as well as the shift in all heliospheric boundaries closer to the Sun in comparison the the multi-fluid treatment of the neutral hydrogen atoms. Overall the SHIELD model shows excellent agreement to these models and is a significant improvement to the fluid treatment of inter- stellar hydrogen."
MERAV OPHER,The formation of magnetic depletions and flux annihilation due to reconnection in the heliosheath,"The misalignment of the solar rotation axis and the magnetic axis of the Sun produces a periodic reversal of the Parker spiral magnetic field and the sectored solar wind. The compression of the sectors is expected to lead to reconnection in the heliosheath (HS). We present particle-in-cell simulations of the sectored HS that reflect the plasma environment along the Voyager 1 and 2 trajectories, specifically including unequal positive and negative azimuthal magnetic flux as seen in the Voyager data. Reconnection proceeds on individual current sheets until islands on adjacent current layers merge. At late time, bands of the dominant flux survive, separated by bands of deep magnetic field depletion. The ambient plasma pressure supports the strong magnetic pressure variation so that pressure is anticorrelated with magnetic field strength. There is little variation in the magnetic field direction across the boundaries of the magnetic depressions. At irregular intervals within the magnetic depressions are long-lived pairs of magnetic islands where the magnetic field direction reverses so that spacecraft data would reveal sharp magnetic field depressions with only occasional crossings with jumps in magnetic field direction. This is typical of the magnetic field data from the Voyager spacecraft. Voyager 2 data reveal that fluctuations in the density and magnetic field strength are anticorrelated in the sector zone, as expected from reconnection, but not in unipolar regions. The consequence of the annihilation of subdominant flux is a sharp reduction in the number of sectors and a loss in magnetic flux, as documented from the Voyager 1 magnetic field and flow data."
MERAV OPHER,Coronal disturbances and their effects on the dynamics of the heliosphere,"The Sun blows out the solar wind which propagates into the interplanetary medium and forms the heliosphere about 100 AU across. The solar activity causes various types of time-dependent phenomena in the solar wind from long-lived corotating interaction regions to shorter on duration but more extreme events like coronal mass ejections. As these structures propagate outward from the Sun, they evolve and interact with each other and the ambient solar wind. Voyager 1 and 2 provided first unique in-situ measurements of these structures in the outer heliosphere. In particular, Voyager observations in the heliosheath, the outermost region of the heliosphere, showed highly variable plasma flows indicating effects of solar variations extending from the Sun to the heliosphere boundaries. Most surprisingly, Voyager 1 data shows shocks and pressure waves beyond the heliosphere in the interstellar medium. Important questions for the future Interstellar Probe mission are (1) how do the heliosphere boundaries respond to solar variations? (2) how do disturbances evolve in the heliosheath? and (3) how far does the Sun influence extend into the interstellar medium? This talk will review observations and recent modeling efforts demonstrating highly variable and dynamic nature of the global heliosphere in response to disturbances originated in the Sun's atmosphere."
MERAV OPHER,The structure of the large-scale heliosphere as seen by current models,"This review summarizes the current state of research aiming at a description of the global heliosphere using both analytical and numerical modeling efforts, particularly in view of the overall plasma/neutral flow and magnetic field structure, and its relation to energetic neutral atoms. Being part of a larger volume on current heliospheric research, it also lays out a number of key concepts and describes several classic, though still relevant early works on the topic. Regarding numerical simulations, emphasis is put on magnetohydrodynamic (MHD), multi-fluid, kinetic-MHD, and hybrid modeling frameworks. Finally, open issues relating to the physical relevance of so-called ""croissant"" models of the heliosphere, as well as the general (dis)agreement of model predictions with observations are highlighted and critically discussed."
MERAV OPHER,Voyager 2 solar plasma and magnetic field spectral analysis for intermediate data sparsity,"The Voyager probes are the furthest, still active, spacecraft ever launched from Earth. During their 38 year trip, they have collected data regarding solar wind properties (such as the plasma velocity and magnetic field intensity). Unfortunately, a complete time evolution of the measured physical quantities is not available. The time series contains many gaps which increase in frequency and duration at larger distances. The aim of this work is to perform a spectral and statistical analysis of the solar wind plasma velocity and magnetic field using Voyager 2 data measured in 1979, when the gap density is between the 30% and 50%. For these gap densities, we show the spectra of gapped signals inherit the characteristics of the data gaps. In particular, the algebraic decay of the intermediate frequency range is underestimated and discrete peaks result not from the underlaying data but from the gap sequence. This analysis is achieved using five different data treatment techniques coming from the multidisciplinary context: averages on linearly interpolated subsets, correlation without data interpolation, correlation of linearly interpolated data, maximum likelihood data reconstruction, and compressed sensing spectral estimation. With five frequency decades, the spectra we obtained have the largest frequency range ever computed at five astronomical units from the Sun; spectral exponents have been determined for all the components of the velocity and magnetic field fluctuations. Void analysis is also useful in recovering other spectral properties such as micro and integral scales."
MERAV OPHER,A small and round heliosphere suggested by MHD modeling of pick-up ions,"As the Sun moves through the surrounding partially-ionized medium, neutral hydrogen atoms penetrate the heliosphere, and through charge-exchange with the supersonic solar wind, create a population of hot pick-up ions (PUIs). Until recently, the consensus was that the shape of the heliosphere, is comet-like. The Termination Shock crossing by Voyager 2 (V2) demonstrated that the heliosheath (HS) (the region of shocked solar wind) pressure is dominated by PUIs; however, their impact on the global structure of the heliosphere has not been explored. Here we use a novel magnetohydrodynamic model that treats the PUIs as a separate fluid from the thermal component of the solar wind. The depletion of PUIs, due to charge exchange with the neutral H atoms of the ISM in the HS, cools the heliosphere, “deflating” it and leading to a narrower HS and a smaller and rounder shape, confirming the shape suggested by Cassini observations. The new model reproduces both the properties of the PUIs, based on the New Horizon, and solar wind ions, based on the V2 spacecraft observations, as well as the solar like magnetic field data outside the heliosphere at Voyager 1(V1) and V2."
MERAV OPHER,The twist of the draped interstellar magnetic field ahead of the heliopause: a magnetic reconnection driven rotational discontinuity,"Based on the difference between the orientation of the interstellar B ISM and the solar magnetic fields, there was an expectation that the magnetic field direction would rotate dramatically across the heliopause (HP). However, the Voyager 1 spacecraft measured very little rotation across the HP. Previously, we showed that the B ISM twists as it approaches the HP and acquires a strong T component (east–west). Here, we establish that reconnection in the eastern flank of the heliosphere is responsible for the twist. On the eastern flank the solar magnetic field has twisted into the positive N direction and reconnects with the southward pointing component of the B ISM. Reconnection drives a rotational discontinuity (RD) that twists the B ISM into the −T direction and propagates upstream in the interstellar medium toward the nose. The consequence is that the N component of B ISM is reduced in a finite width band upstream of the HP. Voyager 1 currently measures angles ($\delta ={\sin }^{-1}({B}_{N}/B)$) close to solar values. We present MHD simulations to support this scenario, suppressing reconnection in the nose region while allowing it in the flanks, consistent with recent ideas about reconnection suppression from diamagnetic drifts. The jump in plasma β (the plasma to magnetic pressure) across the nose of HP is much greater than in the flanks because the heliosheath β is greater there than in the flanks. Large-scale reconnection is therefore suppressed in the nose but not at the flanks. Simulation data suggest that B ISM will return to its pristine value 10–15 au past the HP."
MERAV OPHER,"Globally distributed energetic neutral atom maps for the ""croissant"" heliosphere","A recent study by Opher et al. suggested the heliosphere has a ""croissant"" shape, where the heliosheath plasma is confined by the toroidal solar magnetic field. The ""croissant"" heliosphere is in contrast to the classically accepted view of a comet-like tail. We investigate the effect of the ""croissant"" heliosphere model on energetic neutral atom (ENA) maps. Regardless of the existence of a split tail, the confinement of the heliosheath plasma should appear in ENA maps. ENA maps from the Interstellar Boundary Explorer (IBEX) have shown two high latitude lobes with excess ENA flux at higher energies in the tail of the heliosphere. These lobes could be a signature of the confinement of the heliosheath plasma, while some have argued they are caused by the fast/slow solar wind profile. Here we present ENA maps of the ""croissant"" heliosphere, focusing on understanding the effect of the heliosheath plasma collimation by the solar magnetic field while using a uniform solar wind. We incorporate pick-up ions (PUIs) into our model based on Malama et al. and Zank et al. We use the neutral solution from our MHD model to determine the angular variation of the PUIs, and include the extinction of PUIs in the heliosheath. In the presence of a uniform solar wind, we find that the collimation in the ""croissant"" heliosphere does manifest itself into two high latitude lobes of increased ENA flux in the downwind direction."
MERAV OPHER,A predicted small and round heliosphere,"The shape of the solar wind bubble within the interstellar medium, the so-called heliosphere, has been explored over six decades(1-7). As the Sun moves through the surrounding partially-ionized medium, neutral hydrogen atoms penetrate the heliosphere, and through charge-exchange with the supersonic solar wind, create a population of hot pick-up ions (PUIs). The Voyager 2 (V2) data demonstrated that the heliosheath pressure is dominated by PUIs. Here we use a novel magnetohydrodynamic model that treats the PUIs as a separate fluid from the thermal component of the solar wind. Unlike previous models (8-10), the new model reproduces the properties of the PUIs and solar wind ions based on the New Horizon(11) and V2(12) spacecraft observations. The model significantly changes the energy flow in the outer heliosphere, leading to a smaller and rounder shape than previously predicted, in agreement with energetic neutral atom observations by the Cassini spacecraft."
MERAV OPHER,Major scientific challenges and opportunities in understanding magnetic reconnection and related explosive phenomena throughout the universe,"This is a group white paper of 100 authors (each with explicit permission via email) from 51 institutions on the topic of magnetic reconnection which is relevant to 6 thematic areas. Grand challenges and research opportunities are described in observations, numerical modeling and laboratory experiments in the upcoming decade."
DANIEL O DAHLSTROM,Scheler on shame: a critical review,"This paper presents a critical review of Scheler’s analysis of shame's structure, dynamic, and affectivity, and his explanation of phenomena of shame. This first part of the paper examines Scheler’s accounts of shame’s basic condition, the law ultimately governing its origin, and its basic dynamic. The second part of the paper turns to his general descriptions of what we feel when we feel shame and his analyses of two distinct forms of shame. The conclusion attempts to draw these aspects of his account of shame together to illustrate why, according to Scheler, we feel shame. Throughout the paper, some basic criticisms of Scheler’s account are advanced. At the same time the paper attempts to demonstrate the virtues of his highly differentiated descriptions of experiences of shame and his attempt to weave these descriptions together into a general theory."
DANIEL O DAHLSTROM,"Truth, knowledge, and “the pretensions of idealism”: a critical commentary on the First Part of Mendelssohn’s Morning Hours","Whereas research on Moses Mendelssohn’s Morning Hours has largely focused on the proofs for the existence of God and the elaboration of a purified pantheism in the Second Part of the text, scholars have paid far less attention to the First Part where Mendelssohn details his mature epistemology and conceptions of truth. In an attempt to contribute to remedying this situation, the present article critically examines his account, in the First Part, of different types of truth, different types of knowledge, and the case against idealism. The examination stresses potential but overlooked strengths of his account (e. g., a conception of immediate knowledge that is both far broader than the sensory field and distinctive for having change as its object), questions of ambiguity if not inconsistency in his concepts of existence and substance, and the potential import of these questions for the role he assigns to common sense."
DANIEL O DAHLSTROM,Naïve and sentimental character: Schiller’s poetic phenomenology,"[Excerpt] ""Poets are, by definition, “the preservers of nature,” but when they can no longer completely be so, they serve as its witnesses” and “avengers.” In the former case, they are natural; in the latter, they seek the lost nature. In the former case, they imitate what is actual; in the latter, they portray something ideal. Every poet is accordingly “either naïve or sentimental.” Even in the present day, Schiller insists, “nature is the only flame that nourishes the poetic spirit,” a spirit that gathers all its power from nature and speaks to it alone even in the case of “artificial” human beings, caught in the grip of culture (NSD, 196/432, 200f/436f). In this way Schiller distinguishes between two basic kinds of poetry and poetic genius grounded in different relationships to nature. Indeed, the development in Schiller’s thinking from the Letters on the Aesthetic Education of Man to On Naïve and Sentimental Poetry is marked by the way that nature replaces reason as the center of gravity. Each form of poetry possesses a distinctive and constitutive moral dimension that is sustained by their respective relationships to nature."""
DANIEL O DAHLSTROM,Scheler on shame,"This paper presents a critical review of Scheler’s analysis of shame's structure, dynamic, and affectivity, and his explanation of phenomena of shame. This first part of the paper examines Scheler’s accounts of shame’s basic condition, the law ultimately governing its origin, and its basic dynamic. The second part of the paper turns to his general descriptions of what we feel when we feel shame and his analyses of two distinct forms of shame. The conclusion attempts to draw these aspects of his account of shame together to illustrate why, according to Scheler, we feel shame. Throughout the paper, some basic criticisms of Scheler’s account are advanced. At the same time the paper attempts to demonstrate the virtues of his highly differentiated descriptions of experiences of shame and his attempt to weave these descriptions together into a general theory."
DANIEL O DAHLSTROM,Reason within the limits of religion alone: Hamann’s onto-Christology,
DANIEL O DAHLSTROM,Heidegger's early phenomenology,"This paper attempts to shed some light on Heidegger’s early conception of phenomenology in light of its conscious departure from Husserl’s conception of phenomenology. The period in question extends from Heidegger’s first Freiburg lectures in 1919 to his return to Freiburg from Marburg in the fall of 1928. After flagging some prima facie differences between their phenomenological projects during these years, I suggest how Heidegger adapts into his phenomenology four basic aspects of Husserl’s phenomenology (the phenomenological reduction, formalization, and the performative and constitutive aspects of the analysis). In conclusion I call attention to a fundamental, arguably irreconcilable difference between their phenomenologies."
DANIEL O DAHLSTROM,Missing in action: affectivity in being and time,"Despite the importance that Heidegger assigns to affectivity structurally in Being and Time, accounts of the relevant sorts of affectivity are frequently and, in some cases, perhaps even egregiously missing from existential analyses that form the centerpiece of the work. The aim of this paper is to demonstrate as much. After recounting the considerable insights of Heidegger’s general account of disposedness and affectivity and the fundamental status he assigns to them, the focus of the paper turns to the secondary status often accorded them in the first half of Being and Time and the seemingly crucial absence of an adequate account of the affective dimension of authentic existence, in the second half of the work. After making the argument that, according to Heidegger’s own criterion, the adequate rootedness of the existential analysis demands a more robust account of the affective character of existing authentically, the paper concludes with an open question about the mood of undertaking the existential analysis itself."
DANIEL O DAHLSTROM,Im-position: Heidegger’s analysis of the essence of modern technology,"[Excerpt] ""One of Heidegger’s initial moves in “The Question Concerning Technology” is to call attention to the difference between Technik and its essence. When we are looking for the essence of a tree, he notes, we are not looking for something that is itself a tree. So, too, the essence of Technik is “completely and utterly nothing Technisches.” Initially, Heidegger offers no argument for this claim (pragmatists might dispute it), but it has a certain prima facie plausibility. What a bit of know-how is—the essence of Technik as a technique or mechanical art—is arguably not itself necessarily a bit of know-how, any more than what technical equipment is—the essence of Technik as equipment—is itself a piece of equipment."""
DANIEL O DAHLSTROM,The given and a proximity to art: Heidegger's early dialectical conception of phenomenology,
DANIEL O DAHLSTROM,Heideggerian ruminations on being and presence,"As Aristotle puts it, ‘being’ (used interchangeably with ‘existence’ here) is said in many ways, including many opposing ways. Potentialities exist precisely as potentialities for specific actualities but the potentialities and the respective actualities for which they are potentialities are not identical to one another, even though they are determinable only in terms of one another (e.g., the acorn and the mature oak, the glass before and after shattering). In this sense being exceeds the exclusive disjunction of potentialities and their respective actualities."
FRANK H GUENTHER,Engaging the articulators enhances perception of concordant visible speech movements,"PURPOSE This study aimed to test whether (and how) somatosensory feedback signals from the vocal tract affect concurrent unimodal visual speech perception. METHOD Participants discriminated pairs of silent visual utterances of vowels under 3 experimental conditions: (a) normal (baseline) and while holding either (b) a bite block or (c) a lip tube in their mouths. To test the specificity of somatosensory-visual interactions during perception, we assessed discrimination of vowel contrasts optically distinguished based on their mandibular (English /ɛ/-/æ/) or labial (English /u/-French /u/) postures. In addition, we assessed perception of each contrast using dynamically articulating videos and static (single-frame) images of each gesture (at vowel midpoint). RESULTS Engaging the jaw selectively facilitated perception of the dynamic gestures optically distinct in terms of jaw height, whereas engaging the lips selectively facilitated perception of the dynamic gestures optically distinct in terms of their degree of lip compression and protrusion. Thus, participants perceived visible speech movements in relation to the configuration and shape of their own vocal tract (and possibly their ability to produce covert vowel production-like movements). In contrast, engaging the articulators had no effect when the speaking faces did not move, suggesting that the somatosensory inputs affected perception of time-varying kinematic information rather than changes in target (movement end point) mouth shapes. CONCLUSIONS These findings suggest that orofacial somatosensory inputs associated with speech production prime premotor and somatosensory brain regions involved in the sensorimotor control of speech, thereby facilitating perception of concordant visible speech movements. SUPPLEMENTAL MATERIAL https://doi.org/10.23641/asha.9911846"
FRANK H GUENTHER,Involvement of the cortico-basal ganglia-thalamocortical loop in developmental stuttering,"Stuttering is a complex neurodevelopmental disorder that has to date eluded a clear explication of its pathophysiological bases. In this review, we utilize the Directions Into Velocities of Articulators (DIVA) neurocomputational modeling framework to mechanistically interpret relevant findings from the behavioral and neurological literatures on stuttering. Within this theoretical framework, we propose that the primary impairment underlying stuttering behavior is malfunction in the cortico-basal ganglia-thalamocortical (hereafter, cortico-BG) loop that is responsible for initiating speech motor programs. This theoretical perspective predicts three possible loci of impaired neural processing within the cortico-BG loop that could lead to stuttering behaviors: impairment within the basal ganglia proper; impairment of axonal projections between cerebral cortex, basal ganglia, and thalamus; and impairment in cortical processing. These theoretical perspectives are presented in detail, followed by a review of empirical data that make reference to these three possibilities. We also highlight any differences that are present in the literature based on examining adults versus children, which give important insights into potential core deficits associated with stuttering versus compensatory changes that occur in the brain as a result of having stuttered for many years in the case of adults who stutter. We conclude with outstanding questions in the field and promising areas for future studies that have the potential to further advance mechanistic understanding of neural deficits underlying persistent developmental stuttering."
FRANK H GUENTHER,Articulating: the neural mechanisms of speech production,"Speech production is a highly complex sensorimotor task involving tightly coordinated processing across large expanses of the cerebral cortex. Historically, the study of the neural underpinnings of speech suffered from the lack of an animal model. The development of non-invasive structural and functional neuroimaging techniques in the late 20th century has dramatically improved our understanding of the speech network. Techniques for measuring regional cerebral blood flow have illuminated the neural regions involved in various aspects of speech, including feedforward and feedback control mechanisms. In parallel, we have designed, experimentally tested, and refined a neural network model detailing the neural computations performed by specific neuroanatomical regions during speech. Computer simulations of the model account for a wide range of experimental findings, including data on articulatory kinematics and brain activity during normal and perturbed speech. Furthermore, the model is being used to investigate a wide range of communication disorders."
FRANK H GUENTHER,Surface electromyographic control of a novel phonemic interface for speech synthesis,"Many individuals with minimal movement capabilities use AAC to communicate. These individuals require both an interface with which to construct a message (e.g., a grid of letters) and an input modality with which to select targets. This study evaluated the interaction of two such systems: (a) an input modality using surface electromyography (sEMG) of spared facial musculature, and (b) an onscreen interface from which users select phonemic targets. These systems were evaluated in two experiments: (a) participants without motor impairments used the systems during a series of eight training sessions, and (b) one individual who uses AAC used the systems for two sessions. Both the phonemic interface and the electromyographic cursor show promise for future AAC applications."
FRANK H GUENTHER,Weak responses to auditory feedback perturbation during articulation in persons who stutter: evidence for abnormal auditory-motor transformation,"Previous empirical observations have led researchers to propose that auditory feedback (the auditory perception of self-produced sounds when speaking) functions abnormally in the speech motor systems of persons who stutter (PWS). Researchers have theorized that an important neural basis of stuttering is the aberrant integration of auditory information into incipient speech motor commands. Because of the circumstantial support for these hypotheses and the differences and contradictions between them, there is a need for carefully designed experiments that directly examine auditory-motor integration during speech production in PWS. In the current study, we used real-time manipulation of auditory feedback to directly investigate whether the speech motor system of PWS utilizes auditory feedback abnormally during articulation and to characterize potential deficits of this auditory-motor integration. Twenty-one PWS and 18 fluent control participants were recruited. Using a short-latency formant-perturbation system, we examined participants' compensatory responses to unanticipated perturbation of auditory feedback of the first formant frequency during the production of the monophthong [ε]. The PWS showed compensatory responses that were qualitatively similar to the controls' and had close-to-normal latencies (∼150 ms), but the magnitudes of their responses were substantially and significantly smaller than those of the control participants (by 47% on average, p<0.05). Measurements of auditory acuity indicate that the weaker-than-normal compensatory responses in PWS were not attributable to a deficit in low-level auditory processing. These findings are consistent with the hypothesis that stuttering is associated with functional defects in the inverse models responsible for the transformation from the domain of auditory targets and auditory error information into the domain of speech motor commands."
FRANK H GUENTHER,Auditory-motor adaptation is reduced in adults who stutter but not in children who stutter,"Previous studies have shown that adults who stutter produce smaller corrective motor responses to compensate for unexpected auditory perturbations in comparison to adults who do not stutter, suggesting that stuttering may be associated with deficits in integration of auditory feedback for online speech monitoring. In this study, we examined whether stuttering is also associated with deficiencies in integrating and using discrepancies between expect ed and received auditory feedback to adaptively update motor programs for accurate speech production. Using a sensorimotor adaptation paradigm, we measured adaptive speech responses to auditory formant frequency perturbations in adults and children who stutter and their matched nonstuttering controls. We found that the magnitude of the speech adaptive response for children who stutter did not differ from that of fluent children. However, the adaptation magnitude of adults who stutter in response to formant perturbation was significantly smaller than the adaptation magnitude of adults who do not stutter. Together these results indicate that stuttering is associated with deficits in integrating discrepancies between predicted and received auditory feedback to calibrate the speech production system in adults but not children. This auditory-motor integration deficit thus appears to be a compensatory effect that develops over years of stuttering."
FRANK H GUENTHER,A wireless brain-machine interface for real-time speech synthesis,"BACKGROUND: Brain-machine interfaces (BMIs) involving electrodes implanted into the human cerebral cortex have recently been developed in an attempt to restore function to profoundly paralyzed individuals. Current BMIs for restoring communication can provide important capabilities via a typing process, but unfortunately they are only capable of slow communication rates. In the current study we use a novel approach to speech restoration in which we decode continuous auditory parameters for a real-time speech synthesizer from neuronal activity in motor cortex during attempted speech. METHODOLOGY/PRINCIPAL FINDINGS: Neural signals recorded by a Neurotrophic Electrode implanted in a speech-related region of the left precentral gyrus of a human volunteer suffering from locked-in syndrome, characterized by near-total paralysis with spared cognition, were transmitted wirelessly across the scalp and used to drive a speech synthesizer. A Kalman filter-based decoder translated the neural signals generated during attempted speech into continuous parameters for controlling a synthesizer that provided immediate (within 50 ms) auditory feedback of the decoded sound. Accuracy of the volunteer's vowel productions with the synthesizer improved quickly with practice, with a 25% improvement in average hit rate (from 45% to 70%) and 46% decrease in average endpoint error from the first to the last block of a three-vowel task. CONCLUSIONS/SIGNIFICANCE: Our results support the feasibility of neural prostheses that may have the potential to provide near-conversational synthetic speech output for individuals with severely impaired speech motor control. They also provide an initial glimpse into the functional properties of neurons in speech motor cortical areas."
FRANK H GUENTHER,Auditory-motor adaptation is reduced in adults who stutter but not in children who stutter,"Previous studies have shown that adults who stutter produce smaller corrective motor responses to compensate for unexpected auditory perturbations in comparison to adults who do not stutter, suggesting that stuttering may be associated with deficits in integration of auditory feedback for online speech monitoring. In this study, we examined whether stuttering is also associated with deficiencies in integrating and using discrepancies between expected and received auditory feedback to adaptively update motor programs for accurate speech production. Using a sensorimotor adaptation paradigm, we measured adaptive speech responses to auditory formant frequency perturbations in adults and children who stutter and their matched nonstuttering controls. We found that the magnitude of the speech adaptive response for children who stutter did not differ from that of fluent children. However, the adaptation magnitude of adults who stutter in response to auditory perturbation was significantly smaller than the adaptation magnitude of adults who do not stutter. Together these results indicate that stuttering is associated with deficits in integrating discrepancies between predicted and received auditory feedback to calibrate the speech production system in adults but not children. This auditory-motor integration deficit thus appears to be a compensatory effect that develops over years of stuttering."
FRANK H GUENTHER,Decreased cerebellar-orbitofrontal connectivity correlates with stuttering severity: whole-brain functional and structural connectivity associations with persistent developmental stuttering,"Persistent developmental stuttering is characterized by speech production disfluency and affects 1% of adults. The degree of impairment varies widely across individuals and the neural mechanisms underlying the disorder and this variability remain poorly understood. Here we elucidate compensatory mechanisms related to this variability in impairment using whole-brain functional and white matter connectivity analyses in persistent developmental stuttering. We found that people who stutter had stronger functional connectivity between cerebellum and thalamus than people with fluent speech, while stutterers with the least severe symptoms had greater functional connectivity between left cerebellum and left orbitofrontal cortex (OFC). Additionally, people who stutter had decreased functional and white matter connectivity among the perisylvian auditory, motor, and speech planning regions compared to typical speakers, but greater functional connectivity between the right basal ganglia and bilateral temporal auditory regions. Structurally, disfluency ratings were negatively correlated with white matter connections to left perisylvian regions and to the brain stem. Overall, we found increased connectivity among subcortical and reward network structures in people who stutter compared to controls. These connections were negatively correlated with stuttering severity, suggesting the involvement of cerebellum and OFC may underlie successful compensatory mechanisms by more fluent stutterers."
FRANK H GUENTHER,Changes in the McGurk Effect Across Phonetic Contexts,"To investigate the process underlying audiovisual speech perception, the McGurk illusion was examined across a range of phonetic contexts. Two major changes were found. First, the frequency of illusory /g/ fusion percepts increased relative to the frequency of illusory /d/ fusion percepts as vowel context was shifted from /i/ to /a/ to /u/. This trend could not be explained by biases present in perception of the unimodal visual stimuli. However, the change found in the McGurk fusion effect across vowel environments did correspond systematically with changes in second format frequency patterns across contexts. Second, the order of consonants in illusory combination percepts was found to depend on syllable type. This may be due to differences occuring across syllable contexts in the timecourses of inputs from the two modalities as delaying the auditory track of a vowel-consonant stimulus resulted in a change in the order of consonants perceived. Taken together, these results suggest that the speech perception system either fuses audiovisual inputs into a visually compatible percept with a similar second formant pattern to that of the acoustic stimulus or interleaves the information from different modalities, at a phonemic or subphonemic level, based on their relative arrival times."
FRANK H GUENTHER,A Self-Organizing Neural Network for Learning a Body-Centered Invariant Representation of 3-D Target Position,"This paper describes a self-organizing neural network that rapidly learns a body-centered representation of 3-D target positions. This representation remains invariant under head and eye movements, and is a key component of sensory-motor systems for producing motor equivalent reaches to targets (Bullock, Grossberg, and Guenther, 1993)."
FRANK H GUENTHER,Articulatory Tradeoffs Reduce Acoustic Variability During American English /r/ Production,"Acoustic and articulatory recordings reveal that speakers utilize systematic articulatory tradeoffs to maintain acoustic stability when producing the phoneme /r/. Distinct articulator configurations used to produce /r/ in various phonetic contexts show systematic tradeoffs between the cross-sectional areas of different vocal tract sections. Analysis of acoustic and articulatory variabilities reveals that these tradeoffs act to reduce acoustic variability, thus allowing large contextual variations in vocal tract shape; these contextual variations in turn apparently reduce the amount of articulatory movement required. These findings contrast with the widely held view that speaking involves a canonical vocal tract shape target for each phoneme."
FRANK H GUENTHER,A Self-Organizing Neural Model for Motor Equivalent Phoneme Production,"This paper describes a model of speech production called DIVA that highlights issues of self-organization and motor equivalent production of phonological units. The model uses a circular reaction strategy to learn two mappings between three levels of representation. Data on the plasticity of phonemic perceptual boundaries motivates a learned mapping between phoneme representations and vocal tract variables. A second mapping between vocal tract variables and articulator movements is also learned. To achieve the flexible control made possible by the redundancy of this mapping, desired directions in vocal tract configuration space are mapped into articulator velocity commands. Because each vocal tract direction cell learns to activate several articulator velocities during babbling, the model provides a natural account of the formation of coordinative structures. Model simulations show automatic compensation for unexpected constraints despite no previous experience or learning under these constraints."
FRANK H GUENTHER,"Neural Representations for Sensory-Motor Control, III: Learning a Body-Centered Representation of 3-D Target Position","A neural model is described of how the brain may autonomously learn a body-centered representation of 3-D target position by combining information about retinal target position, eye position, and head position in real time. Such a body-centered spatial representation enables accurate movement commands to the limbs to be generated despite changes in the spatial relationships between the eyes, head, body, and limbs through time. The model learns a vector representation--otherwise known as a parcellated distributed representation--of target vergence with respect to the two eyes, and of the horizontal and vertical spherical angles of the target with respect to a cyclopean egocenter. Such a vergence-spherical representation has been reported in the caudal midbrain and medulla of the frog, as well as in psychophysical movement studies in humans. A head-centered vergence-spherical representation of foveated target position can be generated by two stages of opponent processing that combine corollary discharges of outflow movement signals to the two eyes. Sums and differences of opponent signals define angular and vergence coordinates, respectively. The head-centered representation interacts with a binocular visual representation of non-foveated target position to learn a visuomotor representation of both foveated and non-foveated target position that is capable of commanding yoked eye movementes. This head-centered vector representation also interacts with representations of neck movement commands to learn a body-centered estimate of target position that is capable of commanding coordinated arm movements. Learning occurs during head movements made while gaze remains fixed on a foveated target. An initial estimate is stored and a VOR-mediated gating signal prevents the stored estimate from being reset during a gaze-maintaining head movement. As the head moves, new estimates arc compared with the stored estimate to compute difference vectors which act as error signals that drive the learning process, as well as control the on-line merging of multimodal information."
FRANK H GUENTHER,Responses to intensity-shifted auditory feedback during running speech,"PURPOSE: Responses to intensity perturbation during running speech were measured to understand whether prosodic features are controlled in an independent or integrated manner. METHOD: Nineteen English-speaking healthy adults (age range = 21-41 years) produced 480 sentences in which emphatic stress was placed on either the 1st or 2nd word. One participant group received an upward intensity perturbation during stressed word production, and the other group received a downward intensity perturbation. Compensations for perturbation were evaluated by comparing differences in participants' stressed and unstressed peak fundamental frequency (F0), peak intensity, and word duration during perturbed versus baseline trials. RESULTS: Significant increases in stressed-unstressed peak intensities were observed during the ramp and perturbation phases of the experiment in the downward group only. Compensations for F0 and duration did not reach significance for either group. CONCLUSIONS: Consistent with previous work, speakers appear sensitive to auditory perturbations that affect a desired linguistic goal. In contrast to previous work on F0 perturbation that supported an integrated-channel model of prosodic control, the current work only found evidence for intensity-specific compensation. This discrepancy may suggest different F0 and intensity control mechanisms, threshold-dependent prosodic modulation, or a combined control scheme."
FRANK H GUENTHER,Reliability of fMRI data during speech production tasks across scanning sessions,
FRANK H GUENTHER,A neural network-based exploratory learning and motor planning system for co-robots,"Collaborative robots, or co-robots, are semi-autonomous robotic agents designed to work alongside humans in shared workspaces. To be effective, co-robots require the ability to respond and adapt to dynamic scenarios encountered in natural environments. One way to achieve this is through exploratory learning, or ""learning by doing,"" an unsupervised method in which co-robots are able to build an internal model for motor planning and coordination based on real-time sensory inputs. In this paper, we present an adaptive neural network-based system for co-robot control that employs exploratory learning to achieve the coordinated motor planning needed to navigate toward, reach for, and grasp distant objects. To validate this system we used the 11-degrees-of-freedom RoPro Calliope mobile robot. Through motor babbling of its wheels and arm, the Calliope learned how to relate visual and proprioceptive information to achieve hand-eye-body coordination. By continually evaluating sensory inputs and externally provided goal directives, the Calliope was then able to autonomously select the appropriate wheel and joint velocities needed to perform its assigned task, such as following a moving target or retrieving an indicated object."
FRANK H GUENTHER,Feedforward and feedback control in apraxia of speech: effects of noise masking on vowel production,"PURPOSE: This study was designed to test two hypotheses about apraxia of speech (AOS) derived from the Directions Into Velocities of Articulators (DIVA) model (Guenther et al., 2006): the feedforward system deficit hypothesis and the feedback system deficit hypothesis. METHOD: The authors used noise masking to minimize auditory feedback during speech. Six speakers with AOS and aphasia, 4 with aphasia without AOS, and 2 groups of speakers without impairment (younger and older adults) participated. Acoustic measures of vowel contrast, variability, and duration were analyzed. RESULTS: Younger, but not older, speakers without impairment showed significantly reduced vowel contrast with noise masking. Relative to older controls, the AOS group showed longer vowel durations overall (regardless of masking condition) and a greater reduction in vowel contrast under masking conditions. There were no significant differences in variability. Three of the 6 speakers with AOS demonstrated the group pattern. Speakers with aphasia without AOS did not differ from controls in contrast, duration, or variability. CONCLUSION: The greater reduction in vowel contrast with masking noise for the AOS group is consistent with the feedforward system deficit hypothesis but not with the feedback system deficit hypothesis; however, effects were small and not present in all individual speakers with AOS. Theoretical implications and alternative interpretations of these findings are discussed."
FRANK H GUENTHER,Modelling speech motor programming and apraxia of speech in the DIVA/GODIVA neurocomputational framework,"BACKGROUND: The Directions Into Velocities of Articulators (DIVA) model and its partner, the Gradient Order DIVA (GODIVA) model, provide neurobiologically grounded, computational accounts of speech motor control and motor sequencing, with applications for the study and treatment of neurological motor speech disorders. AIMS: In this review, we provide an overview of the DIVA and GODIVA models and how they explain the interface between phonological and motor planning systems to build on previous models and provide a mechanistic accounting of apraxia of speech (AOS), a disorder of speech motor programming. MAIN CONTRIBUTION: Combined, the DIVA and GODIVA models account for both the segmental and suprasegmental features that define AOS via damage to (i) a speech sound map, hypothesized to reside in the left ventral premotor cortex, (ii) a phonological content buffer hypothesized to reside in the left posterior inferior frontal sulcus, and/or (iii) the axonal projections between these regions. This account is in line with a large body of behavioural work, and it unifies several prior theoretical accounts of AOS. CONCLUSIONS: The DIVA and GODIVA models provide an integrated framework for the generation and testing of both behavioural and neuroimaging hypotheses about the underlying neural mechanisms responsible for motor programming in typical speakers and in speakers with AOS."
FRANK H GUENTHER,Plug-and-play supervisory control using muscle and brain signals for real-time gesture and error detection,"Effective human supervision of robots can be key for ensuring correct robot operation in a variety of potentially safety-critical scenarios. This paper takes a step towards fast and reliable human intervention in supervisory control tasks by combining two streams of human biosignals: muscle and brain activity acquired via EMG and EEG, respectively. It presents continuous classification of left and right hand-gestures using muscle signals, time-locked classification of error-related potentials using brain signals (unconsciously produced when observing an error), and a framework that combines these pipelines to detect and correct robot mistakes during multiple-choice tasks. The resulting hybrid system is evaluated in a “plug-and-play” fashion with 7 untrained subjects supervising an autonomous robot performing a target selection task. Offline analysis further explores the EMG classification performance, and investigates methods to select subsets of training data that may facilitate generalizable plug-and-play classifiers."
FRANK H GUENTHER,A Neural Network Model of Speech Acquisition and Motor Equivalent Speech Production,"This article describes a neural network model that addresses the acquisition of speaking skills by infants and subsequent motor equivalent production of speech sounds. The model learns two mappings during a babbling phase. A phonetic-to-orosensory mapping specifies a vocal tract target for each speech sound; these targets take the form of convex regions in orosensory coordinates defining the shape of the vocal tract. The babbling process wherein these convex region targets are formed explains how an infant can learn phoneme-specific and language-specific limits on acceptable variability of articulator movements. The model also learns an orosensory-to-articulatory mapping wherein cells coding desired movement directions in orosensory space learn articulator movements that achieve these orosensory movement directions. The resulting mapping provides a natural explanation for the formation of coordinative structures. This mapping also makes efficient use of redundancy in the articulator system, thereby providing the model with motor equivalent capabilities. Simulations verify the model's ability to compensate for constraints or perturbations applied to the articulators automatically and without new learning and to explain contextual variability seen in human speech production."
FRANK H GUENTHER,A Self-Organizing Neural Network Architecture for Navigation Using Optic Flow,"This paper describes a self-organizing neural network architecture that transforms optic now information into representations of heading, scene depth, and moving object locations. These representations are used to reactively navigate in simulations involving obstacle avoidance and pursuit of a moving target. The network's weights are trained during an action-perception cycle in which self-generated eye and body movements produce optic flow information, thus allowing the network to tunc itself without requiring explicit knowledge of sensor geometry. The confounding effect of eye movement during translation is suppressed by learning the relationship between eye movement outflow commands and the optic flow signals that they induce. The remaining optic flow field is due only to observer translation and independent motion of objects in the scene. A self-organizing feature map categorizes normalized translational flow patterns, thereby creating a map of cells that code heading directions. Heading information is then recombined with translational flow patterns in two different ways to form maps of scene depth and moving object locations. All learning processes take place concurrently and require no external ""teachers."" Simulations of the network verify its performance using both noise-free and noisy optic flow information."
FRANK H GUENTHER,An Investigation of the Effects of Categorization and Discrimination Training on Auditory Perceptual Space,"Psychophysical phenomena such as categorical perception and the perceptual magnet effect indicate that our auditory perceptual spaces are warped for some stimuli. This paper investigates the effects of two different kinds of training on auditory perceptual space. It is first shown that categorization training, in which subjects learn to identify stimuli within a particular frequency range as members of the same category, can lead to a decrease in sensitivity to stimuli in that category. This phenomenon is an example of acquired similarity and apparently has not been previously demonstrated for a category-relevant dimension. Discrimination training with the same set of stimuli was shown to have the opposite effect: subjects became more sensitive to differences in the stimuli presented during training. Further experiments investigated some of the conditions that are necessary to generate the acquired similarity found in the first experiment. The results of these experiments are used to evaluate two neural network models of the perceptual magnet effect. These models, in combination with our experimental results, are used to generate an experimentally testable hypothesis concerning changes in the brain's auditory maps under different training conditions."
FRANK H GUENTHER,The Perceptual Magnet Effect as an Emergent Property of Neural Map Formation,"The perceptual magnet effect is one of the earliest known language-specific phenomena arising in infant speech development. The effect is characterized by a warping of perceptual space near phonemic category centers. Previous explanations have been formulated within the theoretical framework of cognitive psychology. The model proposed in this paper builds on research from both psychology and neuroscience in working toward a more complete account of the effect. The model embodies two principal hypotheses supported by considerable experimental and theoretical research from the neuroscience literature: (1) sensory experience guides language-specific development of an auditory neural map, and (2) a population vector can predict psychological phenomena based on map cell activities. These hypotheses are realized in a self-organizing neural network model. The magnet effect arises in the model from language-specific nonuniformities in the distribution of map cell firing preferences. Numerical simulations verify that the model captures the known general characteristics of the magnet effect and provides accurate fits to specific psychophysical data."
FRANK H GUENTHER,Acoustic Space Movement Planning in a Neural Model of Motor Equivalent Vowel Production,"Recent evidence suggests that speakers utilize an acoustic-like reference frame for the planning of speech movements. DIVA, a computational model of speech acquisition and motor equivalent speech production, has previously been shown to provide explanations for a wide range of speech production data using a constriction-based reference frame for movement planning. This paper extends the previous work by investigating an acoustic-like planning frame in the DIVA modeling framework. During a babbling phase, the model self-organizes targets in the planning space for each of ten vowels and learns a mapping from desired movement directions in this planning space into appropriate articulator velocities. Simulation results verify that after babbling the model is capable of producing easily recognizable vowel sounds using an acoustic planning space consisting of the formants F1 and F2. The model successfully reaches all vowel targets from any initial vocal tract configuration, even in the presence of constraints such as a blocked jaw."
FRANK H GUENTHER,Intraspeaker Comparisons of Acoustic and Articulatory Variability in American English /r/ Productions,"The purpose of this report is to test the hypothesis that speakers utilize an acoustic, rather than articulatory, planning space for speech production. It has been well-documented that many speakers of American English use different tongue configurations to produce /r/ in different phonetic contexts. The acoustic planning hypothesis suggests that although the /r/ configuration varies widely in different contexts, the primary acoustic cue for /r/, a dip in the F3 trajectory, will be less variable due to tradeoffs in articulatory variability, or trading relations, that help maintain a relatively constant F3 trajectory across phonetic contexts. Acoustic data and EMMA articulatory data from seven speakers producing /r/ in different phonetic contexts were analyzed. Visual inspection of the EMMA data at the point of F3 minimum revealed that each speaker appeared to use at least two of three trading relation strategies that would be expected to reduce F3 variability. Articulatory covariance measures confirmed that all seven speakers utilized a trading relation between tongue back height and tongue back horizontal position, six speakers utilized a trading relation between tongue tip height and tongue back height, and the speaker who did not use this latter strategy instead utilized a trading relation between tongue tip height and tongue back horizontal position. Estimates of F3 variability with and without the articulatory covariances indicated that F3 would be much higher for all speakers if the articulatory covariances were not utilized. These conclusions were further supported by a comparison of measured F3 variability to F3 variabilities estimated from the pellet data with and without articulatory covariances. In all subjects, the actual F3 variance was significantly lower than the F3 variance estimated without articulatory covariances, further supporting the conclusion that the articulatory trading relations were being used to reduce F3 variability. Together, these results strongly suggest that the neural control mechanisms underlying speech production make elegant use of trading relations between articulators to maintain a relatively invariant acoustic trace for /r/ across phonetic contexts."
FRANK H GUENTHER,A Self-Organizing Neural Model of Motor Equivalent Reaching and Tool Use by a Multijoint Arm,"This paper describes a self-organizing neural model for eye-hand coordination. Called the DIRECT model, it embodies a solution of the classical motor equivalence problem. Motor equivalence computations allow humans and other animals to flexibly employ an arm with more degrees of freedom than the space in which it moves to carry out spatially defined tasks under conditions that may require novel joint configurations. During a motor babbling phase, the model endogenously generates movement commands that activate the correlated visual, spatial, and motor information that are used to learn its internal coordinate transformations. After learning occurs, the model is capable of controlling reaching movements of the arm to prescribed spatial targets using many different combinations of joints. When allowed visual feedback, the model can automatically perform, without additional learning, reaches with tools of variable lengths, with clamped joints, with distortions of visual input by a prism, and with unexpected perturbations. These compensatory computations occur within a single accurate reaching movement. No corrective movements are needed. Blind reaches using internal feedback have also been simulated. The model achieves its competence by transforming visual information about target position and end effector position in 3-D space into a body-centered spatial representation of the direction in 3-D space that the end effector must move to contact the target. The spatial direction vector is adaptively transformed into a motor direction vector, which represents the joint rotations that move the end effector in the desired spatial direction from the present arm configuration. Properties of the model are compared with psychophysical data on human reaching movements, neurophysiological data on the tuning curves of neurons in the monkey motor cortex, and alternative models of movement control."
FRANK H GUENTHER,Chunking of phonological units in speech sequencing,"Efficient speech communication requires rapid, fluent production of phoneme sequences. To achieve this, our brains store frequently occurring subsequences as cohesive ""chunks"" that reduce phonological working memory load and improve motor performance. The current study used a motor-sequence learning paradigm in which the generalization of two performance gains (utterance duration and errors) from practicing novel phoneme sequences was used to infer the nature of these speech chunks. We found that performance improvements in duration from practicing syllables with non-native consonant clusters largely generalized to new syllables that contained those clusters. Practicing the whole syllable, however, resulted in larger performance gains in error rates compared to practicing just the consonant clusters. Collectively, these findings are consistent with theories of speech production that posit the consonant cluster as a fundamental unit of phonological working memory and speech sequencing as well as those positing the syllable as a fundamental unit of motor programming."
FRANK H GUENTHER,Reliability of single-subject neural activation patterns in speech production tasks,"Speech neuroimaging research targeting individual speakers could help elucidate differences that may be crucial to understanding speech disorders. However, this research necessitates reliable brain activation across multiple speech production sessions. In the present study, we evaluated the reliability of speech-related brain activity measured by functional magnetic resonance imaging data from twenty neuro-typical subjects who participated in two experiments involving reading aloud simple speech stimuli. Using traditional methods like the Dice and intraclass correlation coefficients, we found that most individuals displayed moderate to high reliability. We also found that a novel machine-learning subject classifier could identify these individuals by their speech activation patterns with 97% accuracy from among a dataset of seventy-five subjects. These results suggest that single-subject speech research would yield valid results and that investigations into the reliability of speech activation in people with speech disorders are warranted."
FRANK H GUENTHER,Neural substrates of verbal repetition deficits in primary progressive aphasia.,"In this cross-sectional study, we examined the relationship between cortical thickness and performance on several verbal repetition tasks in a cohort of patients with primary progressive aphasia in order to test predictions generated by theoretical accounts of phonological working memory that predict phonological content buffers in left posterior inferior frontal sulcus and supramarginal gyrus. Cortical surfaces were reconstructed from magnetic resonance imaging scans from 42 participants diagnosed with primary progressive aphasia. Cortical thickness was measured in a set of anatomical regions spanning the entire cerebral cortex. Correlation analyses were performed between cortical thickness and average score across three phonological working memory-related tasks: the Repetition sub-test from the Western Aphasia Battery, a forward digit span task, and a backward digit span task. Significant correlations were found between average working memory score across tasks and cortical thickness in left supramarginal gyrus and left posterior inferior frontal sulcus, in support of prior theoretical accounts of phonological working memory. Exploratory whole-brain correlation analyses performed for each of the three behavioural tasks individually revealed a distinct set of positively correlated regions for each task. Comparison of cortical thickness measures from different primary progressive aphasia sub-types to cortical thickness in age-matched controls further revealed unique patterns of atrophy in the different subtypes."
FRANK H GUENTHER,Sensorimotor adaptation of voice fundamental frequency in Parkinson's disease,"OBJECTIVE: This study examined adaptive responses to auditory perturbation of fundamental frequency (fo) in speakers with Parkinson's disease (PD) and control speakers. METHOD: Sixteen speakers with PD and nineteen control speakers produced sustained vowels while they received perturbed auditory feedback (i.e., fo shifted upward or downward). Speakers' pitch acuity was quantified using a just-noticeable-difference (JND) paradigm. Twelve listeners provided estimates of the speech intelligibility for speakers with PD. RESULTS: Fifteen responses from each speaker group for each shift direction were included in analyses. While control speakers generally showed consistent adaptive responses opposing the perturbation, speakers with PD showed no compensation on average, with individual PD speakers showing highly variable responses. In the PD group, the degree of compensation was not significantly correlated with age, disease progression, pitch acuity, or intelligibility. CONCLUSIONS: These findings indicate reduced adaptation to sustained fo perturbation and higher variability in PD compared to control participants. No significant differences were seen in pitch acuity between groups, suggesting that the fo adaptation deficit in PD is not the result of purely perceptual mechanisms. SIGNIFICANCE: These results suggest there is an impairment in vocal motor control in PD. Building on these results, contributions can be made to developing targeted voice treatments for PD."
FRANK H GUENTHER,Contributions of auditory and somatosensory feedback to vocal motor control,"PURPOSE: To better define the contributions of somatosensory and auditory feedback in vocal motor control, a laryngeal perturbation experiment was conducted with and without masking of auditory feedback. METHOD: Eighteen native speakers of English produced a sustained vowel while their larynx was physically and externally displaced on a subset of trials. For the condition with auditory masking, speech-shaped noise was played via earphones at 90 dB SPL. Responses to the laryngeal perturbation were compared to responses by the same participants to an auditory perturbation experiment that involved a 100-cent downward shift in fundamental frequency (fo). Responses were also examined in relation to a measure of auditory acuity. RESULTS: Compensatory responses to the laryngeal perturbation were observed with and without auditory masking. The level of compensation was greatest in the laryngeal perturbation condition without auditory masking, followed by the condition with auditory masking; the level of compensation was smallest in the auditory perturbation experiment. No relationship was found between the degree of compensation to auditory versus laryngeal perturbations, and the variation in responses in both perturbation experiments was not related to auditory acuity. CONCLUSIONS: The findings indicate that somatosensory and auditory feedback control mechanisms work together to compensate for laryngeal perturbations, resulting in the greatest degree of compensation when both sources of feedback are available. In contrast, these two control mechanisms work in competition in response to auditory perturbations, resulting in an overall smaller degree of compensation. Supplemental Material https://doi.org/10.23641/asha.12559628."
FRANK H GUENTHER,Classification of intended phoneme production from chronic intracortical microelectrode recordings in speech-motor cortex,"We conducted a neurophysiological study of attempted speech production in a paralyzed human volunteer using chronic microelectrode recordings. The volunteer suffers from locked-in syndrome leaving him in a state of near-total paralysis, though he maintains good cognition and sensation. In this study, we investigated the feasibility of supervised classification techniques for prediction of intended phoneme production in the absence of any overt movements including speech. Such classification or decoding ability has the potential to greatly improve the quality-of-life of many people who are otherwise unable to speak by providing a direct communicative link to the general community. We examined the performance of three classifiers on a multi-class discrimination problem in which the items were 38 American English phonemes including monophthong and diphthong vowels and consonants. The three classifiers differed in performance, but averaged between 16 and 21% overall accuracy (chance-level is 1/38 or 2.6%). Further, the distribution of phonemes classified statistically above chance was non-uniform though 20 of 38 phonemes were classified with statistical significance for all three classifiers. These preliminary results suggest supervised classification techniques are capable of performing large scale multi-class discrimination for attempted speech production and may provide the basis for future communication prostheses."
FRANK H GUENTHER,Decoding of intended saccade direction in an oculomotor brain-computer interface,"OBJECTIVE: To date, invasive brain-computer interface (BCI) research has largely focused on replacing lost limb functions using signals from the hand/arm areas of motor cortex. However, the oculomotor system may be better suited to BCI applications involving rapid serial selection from spatial targets, such as choosing from a set of possible words displayed on a computer screen in an augmentative and alternative communication (AAC) application. Here we aimed to demonstrate the feasibility of a BCI utilizing the oculomotor system. APPROACH: We developed a chronic intracortical BCI in monkeys to decode intended saccadic eye movement direction using activity from multiple frontal cortical areas. MAIN RESULTS: Intended saccade direction could be decoded in real time with high accuracy, particularly at contralateral locations. Accurate decoding was evident even at the beginning of the BCI session; no extensive BCI experience was necessary. High-frequency (80-500 Hz) local field potential magnitude provided the best performance, even over spiking activity, thus simplifying future BCI applications. Most of the information came from the frontal and supplementary eye fields, with relatively little contribution from dorsolateral prefrontal cortex. SIGNIFICANCE: Our results support the feasibility of high-accuracy intracortical oculomotor BCIs that require little or no practice to operate and may be ideally suited for 'point and click' computer operation as used in most current AAC systems."
FRANK H GUENTHER,Anomalous morphology in left hemisphere motor and premotor cortex of children who stutter,"Stuttering is a neurodevelopmental disorder that affects the smooth flow of speech production. Stuttering onset occurs during a dynamic period of development when children first start learning to formulate sentences. Although most children grow out of stuttering naturally, ∼1% of all children develop persistent stuttering that can lead to significant psychosocial consequences throughout one’s life. To date, few studies have examined neural bases of stuttering in children who stutter, and even fewer have examined the basis for natural recovery versus persistence of stuttering. Here we report the first study to conduct surface-based analysis of the brain morphometric measures in children who stutter. We used FreeSurfer to extract cortical size and shape measures from structural MRI scans collected from the initial year of a longitudinal study involving 70 children (36 stuttering, 34 controls) in the 3–10-year range. The stuttering group was further divided into two groups: persistent and recovered, based on their later longitudinal visits that allowed determination of their eventual clinical outcome. A region of interest analysis that focused on the left hemisphere speech network and a whole-brain exploratory analysis were conducted to examine group differences and group × age interaction effects. We found that the persistent group could be differentiated from the control and recovered groups by reduced cortical thickness in left motor and lateral premotor cortical regions. The recovered group showed an age-related decrease in local gyrification in the left medial premotor cortex (supplementary motor area and and pre-supplementary motor area). These results provide strong evidence of a primary deficit in the left hemisphere speech network, specifically involving lateral premotor cortex and primary motor cortex, in persistent developmental stuttering. Results further point to a possible compensatory mechanism involving left medial premotor cortex in those who recover from childhood stuttering."
FRANK H GUENTHER,The neural correlates of speech motor sequence learning,"Speech is perhaps the most sophisticated example of a species-wide movement capability in the animal kingdom, requiring split-second sequencing of approximately 100 muscles in the respiratory, laryngeal, and oral movement systems. Despite the unique role speech plays in human interaction and the debilitating impact of its disruption, little is known about the neural mechanisms underlying speech motor learning. Here, we studied the behavioral and neural correlates of learning new speech motor sequences. Participants repeatedly produced novel, meaningless syllables comprising illegal consonant clusters (e.g., GVAZF) over 2 days of practice. Following practice, participants produced the sequences with fewer errors and shorter durations, indicative of motor learning. Using fMRI, we compared brain activity during production of the learned illegal sequences and novel illegal sequences. Greater activity was noted during production of novel sequences in brain regions linked to non-speech motor sequence learning, including the BG and pre-SMA. Activity during novel sequence production was also greater in brain regions associated with learning and maintaining speech motor programs, including lateral premotor cortex, frontal operculum, and posterior superior temporal cortex. Measures of learning success correlated positively with activity in left frontal operculum and white matter integrity under left posterior superior temporal sulcus. These findings indicate speech motor sequence learning relies not only on brain areas involved generally in motor sequencing learning but also those associated with feedback-based speech motor learning. Furthermore, learning success is modulated by the integrity of structural connectivity between these motor and sensory brain regions."
FRANK H GUENTHER,Exploring auditory-motor interactions in normal and disordered speech,"Auditory feedback plays an important role in speech motor learning and in the online correction of speech movements. Speakers can detect and correct auditory feedback errors at the segmental and suprasegmental levels during ongoing speech. The frontal brain regions that contribute to these corrective movements have also been shown to be more active during speech in persons who stutter (PWS) compared to fluent speakers. Further, various types of altered auditory feedback can temporarily improve the fluency of PWS, suggesting that atypical auditory-motor interactions during speech may contribute to stuttering disfluencies. To investigate this possibility, we have developed and improved Audapter, a software that enables configurable dynamic perturbation of the spatial and temporal content of the speech auditory signal in real time. Using Audapter, we have measured the compensatory responses of PWS to static and dynamic perturbations of the formant content of auditory feedback and compared these responses with those from matched fluent controls. Our findings indicate deficient utilization of auditory feedback by PWS for short-latency online control of the spatial and temporal parameters of articulation during vowel production and during running speech. These findings provide further evidence that stuttering is associated with aberrant auditory-motor integration during speech."
FRANK H GUENTHER,Sensory feedback control in speech: Neural circuits and individual differences,"Speech production involves a combination of feedforward and sensory feedback-based control mechanisms. The latter have been characterized in experiments involving real-time perturbations during speech. Unexpected perturbations of auditory feedback result in corrective motor responses with a minimum latency of approximately 100 ms after perturbation onset. These responses have been shown for both pitch and formant frequency perturbations, and the responsible neural circuitry includes portions of the superior temporal gyrus and ventral premotor cortex (vPMC). Corrective responses to somatosensory perturbations (such as a downward force applied to the jaw) occur approximately 50 ms after perturbation onset and are mediated by ventral somatosensory cortex and vPMC. The degree to which an individual weights auditory versus somatosensory feedback varies substantially. Such differences rely in part on differences in sensory acuity, e.g., a speaker with relatively poor hearing is likely to rely more heavily on somatosensory feedback control mechanisms than auditory feedback control mechanisms. Additionally, reliance on feedback control may be modulated to compensate for impairments in the feedforward system for speech, e.g., adults who stutter show a reduced reliance on auditory feedback control compared to fluent speakers, perhaps because auditory feedback can have a deleterious effect on speech initiation in stuttering."
FRANK H GUENTHER,Diffusion imaging of cerebral white matter in persons who stutter: evidence for network-level anomalies,"Deficits in brain white matter have been a main focus of recent neuroimaging studies on stuttering. However, no prior study has examined brain connectivity on the global level of the cerebral cortex in persons who stutter (PWS). In the current study, we analyzed the results from probabilistic tractography between regions comprising the cortical speech network. An anatomical parcellation scheme was used to define 28 speech production-related ROIs in each hemisphere. We used network-based statistic (NBS) and graph theory to analyze the connectivity patterns obtained from tractography. At the network-level, the probabilistic corticocortical connectivity from the PWS group were significantly weaker than that from persons with fluent speech (PFS). NBS analysis revealed significant components in the bilateral speech networks with negative correlations with stuttering severity. To facilitate comparison with previous studies, we also performed tract-based spatial statistics (TBSS) and regional fractional anisotropy (FA) averaging. Results from tractography, TBSS and regional FA averaging jointly highlight the importance of several regions in the left peri-Rolandic sensorimotor and premotor areas, most notably the left ventral premotor cortex (vPMC) and middle primary motor cortex, in the neuroanatomical basis of stuttering."
FRANK H GUENTHER,A Wireless Brain-Machine Interface for Real-Time Speech Synthesis,"BACKGROUND. Brain-machine interfaces (BMIs) involving electrodes implanted into the human cerebral cortex have recently been developed in an attempt to restore function to profoundly paralyzed individuals. Current BMIs for restoring communication can provide important capabilities via a typing process, but unfortunately they are only capable of slow communication rates. In the current study we use a novel approach to speech restoration in which we decode continuous auditory parameters for a real-time speech synthesizer from neuronal activity in motor cortex during attempted speech. METHODOLOGY/PRINCIPAL FINDINGS. Neural signals recorded by a Neurotrophic Electrode implanted in a speech-related region of the left precentral gyrus of a human volunteer suffering from locked-in syndrome, characterized by near-total paralysis with spared cognition, were transmitted wirelessly across the scalp and used to drive a speech synthesizer. A Kalman filter-based decoder translated the neural signals generated during attempted speech into continuous parameters for controlling a synthesizer that provided immediate (within 50 ms) auditory feedback of the decoded sound. Accuracy of the volunteer's vowel productions with the synthesizer improved quickly with practice, with a 25% improvement in average hit rate (from 45% to 70%) and 46% decrease in average endpoint error from the first to the last block of a three-vowel task. CONCLUSIONS/SIGNIFICANCE. Our results support the feasibility of neural prostheses that may have the potential to provide near-conversational synthetic speech output for individuals with severely impaired speech motor control. They also provide an initial glimpse into the functional properties of neurons in speech motor cortical areas."
FRANK H GUENTHER,Reliability of single-subject neural activation patterns in speech production tasks,"Traditional group fMRI (functional magnetic resonance imaging) analyses are not designed to detect individual differences that may be crucial to better understanding speech disorders. Single-subject research could therefore provide a richer characterization of the neural substrates of speech production in development and disease. Before this line of research can be tackled, however, it is necessary to evaluate whether healthy individuals exhibit reproducible brain activation across multiple sessions during speech production tasks. In the present study, we evaluated the reliability and discriminability of cortical functional magnetic resonance imaging data from twenty neurotypical subjects who participated in two experiments involving reading aloud mono- or bisyllabic speech stimuli. Using traditional methods like the Dice and intraclass correlation coefficients, we found that most individuals displayed moderate to high reliability, with exceptions likely due to increased head motion in the scanner. Further, this level of reliability for speech production was not directly correlated with reliable patterns in the underlying average blood oxygenation level dependent signal across the brain. Finally, we found that a novel machine-learning subject classifier could identify these individuals by their speech activation patterns with 97% accuracy from among a dataset of seventy-five subjects. These results suggest that single-subject speech research would yield valid results and that investigations into the reliability of speech activation in people with speech disorders are warranted."
FRANK H GUENTHER,Sensorimotor adaptation to auditory perturbation of speech is facilitated by noninvasive brain stimulation,"Repeated exposure to disparity between the motor plan and auditory feedback during speech production results in a proportionate change in the motor system’s response called auditory-motor adaptation. Artificially raising F1 in auditory feedback results in a concomitant decrease in F1 during speech production. Transcranial direct current stimulation (tDCS) can be used to alter neuronal excitability in focal areas of the brain. The present experiment explored the effect of noninvasive brain stimulation applied to the speech premotor cortex on the timing and magnitude of adaptation responses to artificially raised F1 in auditory feedback. Participants (N = 18) completed a speaking task in which they read target words aloud. Participants' speech was processed to raise F1 by 30% and played back to them over headphones in real time. A within-subjects design compared acoustics of participants’ speech while receiving anodal (active) tDCS stimulation versus sham (control) stimulation. Participants' speech showed an increasing magnitude of adaptation of F1 over time during anodal stimulation compared to sham. These results indicate that tDCS can affect behavioral response during auditory-motor adaptation, which may have translational implications for sensorimotor training in speech disorders."
FRANK H GUENTHER,An investigation of compensation and adaptation to auditory perturbations in individuals with acquired apraxia of speech,"Two auditory perturbation experiments were used to investigate the integrity of neural circuits responsible for speech sensorimotor adaptation in acquired apraxia of speech (AOS). This has implications for understanding the nature of AOS as well as normal speech motor control. Two experiments were conducted. In Experiment 1, compensatory responses to unpredictable fundamental frequency (F₀) perturbations during vocalization were investigated in healthy older adults and adults with acquired AOS plus aphasia. F₀ perturbation involved upward and downward 100-cent shifts versus no shift, in equal proportion, during 2 s vocalizations of the vowel /a/. In Experiment 2, adaptive responses to sustained first formant (F₁) perturbations during speech were investigated in healthy older adults, adults with AOS and adults with aphasia only (APH). The F₁ protocol involved production of the vowel /ε/ in four consonant-vowel words of Australian English (pear, bear, care, dare), and one control word with a different vowel (paw). An unperturbed Baseline phase was followed by a gradual Ramp to a 30% upward F₁ shift stimulating a compensatory response, a Hold phase where the perturbation was repeatedly presented with alternating blocks of masking trials to probe adaptation, and an End phase with masking trials only to measure persistence of any adaptation. AOS participants showed normal compensation to unexpected F₀ perturbations, indicating that auditory feedback control of low-level, non-segmental parameters is intact. Furthermore, individuals with AOS displayed an adaptive response to sustained F₁ perturbations, but age-matched controls and APH participants did not. These findings suggest that older healthy adults may have less plastic motor programs that resist modification based on sensory feedback, whereas individuals with AOS have less well-established and more malleable motor programs due to damage from stroke."
FRANK H GUENTHER,"Assessing dynamics, spatial scale, and uncertainty in task-related brain network analyses","The brain is a complex network of interconnected elements, whose interactions evolve dynamically in time to cooperatively perform specific functions. A common technique to probe these interactions involves multi-sensor recordings of brain activity during a repeated task. Many techniques exist to characterize the resulting task-related activity, including establishing functional networks, which represent the statistical associations between brain areas. Although functional network inference is commonly employed to analyze neural time series data, techniques to assess the uncertainty—both in the functional network edges and the corresponding aggregate measures of network topology—are lacking. To address this, we describe a statistically principled approach for computing uncertainty in functional networks and aggregate network measures in task-related data. The approach is based on a resampling procedure that utilizes the trial structure common in experimental recordings. We show in simulations that this approach successfully identifies functional networks and associated measures of confidence emergent during a task in a variety of scenarios, including dynamically evolving networks. In addition, we describe a principled technique for establishing functional networks based on predetermined regions of interest using canonical correlation. Doing so provides additional robustness to the functional network inference. Finally, we illustrate the use of these methods on example invasive brain voltage recordings collected during an overt speech task. The general strategy described here—appropriate for static and dynamic network inference and different statistical measures of coupling—permits the evaluation of confidence in network measures in a variety of settings common to neuroscience."
MICHAEL P LAVALLEY,"Design of the WHIP-PD study: a phase II, twelve-month, dual-site, randomized controlled trial evaluating the effects of a cognitive-behavioral approach for promoting enhanced walking activity using mobile health technology in people with Parkinson-disease","BACKGROUND: Parkinson disease (PD) is a debilitating and chronic neurodegenerative disease resulting in ambulation difficulties. Natural walking activity often declines early in disease progression despite the relative stability of motor impairments. In this study, we propose a paradigm shift with a ""connected behavioral approach"" that targets real-world walking using cognitive-behavioral training and mobile health (mHealth) technology. METHODS/DESIGN: The Walking and mHealth to Increase Participation in Parkinson Disease (WHIP-PD) study is a twelve-month, dual site, two-arm, randomized controlled trial recruiting 148 participants with early to mid-stage PD. Participants will be randomly assigned to connected behavioral or active control conditions. Both conditions will include a customized program of goal-oriented walking, walking-enhancing strengthening exercises, and eight in-person visits with a physical therapist. Participants in the connected behavioral condition also will (1) receive cognitive-behavioral training to promote self-efficacy for routine walking behavior and (2) use a mHealth software application to manage their program and communicate remotely with their physical therapist. Active control participants will receive no cognitive-behavioral training and manage their program on paper. Evaluations will occur at baseline, three-, six-, and twelve-months and include walking assessments, self-efficacy questionnaires, and seven days of activity monitoring. Primary outcomes will include the change between baseline and twelve months in overall amount of walking activity (mean number of steps per day) and amount of moderate intensity walking activity (mean number of minutes per day in which > 100 steps were accumulated). Secondary outcomes will include change in walking capacity as measured by the six-minute walk test and ten-meter walk test. We also will examine if self-efficacy mediates change in amount of walking activity and if change in amount of walking activity mediates change in walking capacity. DISCUSSION: We expect this study to show the connected behavioral approach will be more effective than the active control condition in increasing the amount and intensity of real-world walking activity and improving walking capacity. Determining effective physical activity interventions for persons with PD is important for preserving mobility and essential for maintaining quality of life. Clinical trials registration NCT03517371, May 7, 2018. TRIAL REGISTRATION: ClinicalTrials.gov: NCT03517371. Date of registration: May 7, 2018. Protocol version: Original."
NATHAN PHILLIPS,Seeing the invisible: from imagined to virtual urban landscapes,"Urban ecosystems consist of infrastructure features working together to provide services for inhabitants. Infrastructure functions akin to an ecosystem, having dynamic relationships and interdependencies. However, with age, urban infrastructure can deteriorate and stop functioning. Additional pressures on infrastructure include urbanizing populations and a changing climate that exposes vulnerabilities. To manage the urban infrastructure ecosystem in a modernizing world, urban planners need to integrate a coordinated management plan for these co-located and dependent infrastructure features. To implement such a management practice, an improved method for communicating how these infrastructure features interact is needed. This study aims to define urban infrastructure as a system, identify the systematic barriers preventing implementation of a more coordinated management model, and develop a virtual reality tool to provide visualization of the spatial system dynamics of urban infrastructure. Data was collected from a stakeholder workshop that highlighted a lack of appreciation for the system dynamics of urban infrastructure. An urban ecology VR model was created to highlight the interconnectedness of infrastructure features. VR proved to be useful for communicating spatial information to urban stakeholders about the complexities of infrastructure ecology and the interactions between infrastructure features."
NATHAN PHILLIPS,An enhanced procedure for urban mobile methane leak detection,"Leaked methane from natural gas distribution pipelines is a significant human and environmental health problem in urban areas. To assess this risk, urban mobile methane leak surveys were conducted, using innovative methodology, on the streets of Hartford, Danbury, and New London, Connecticut, in March 2019. The Hartford survey was done to determine if results from a 2016 survey (Keyes et al., 2019) were persistent, and surveys in additional towns were done to determine if similar findings could be made using an identical approach. Results show that Hartford continues to be problematic, with approximately 3.4 leaks per road mile observed in 2016 and 4.3 leaks per mile estimated in 2019, similar to that previously found in Boston, Massachusetts (Phillips et al., 2013). A preliminary estimate of methane leaks in Hartford is 0.86 metric tonnes per day (or 313 metric tonnes per year), equivalent to 42,840 cubic feet per day of natural gas, and a daily gas consumption of approximately 214 U.S. households. Moreover, the surveys and analyses done for Danbury and New London also reveal problematic leaks, particularly for Danbury with an estimated 3.6 leaks per mile. Although road miles covered in New London were more limited, the survey revealed leak-prone areas, albeit with a range of methane readings lower than those in Hartford and Danbury. Data collection methods for all studies is first reported here and are readily transferable to similar urban settings. This work demonstrates the actionable value that can be gained from data-driven evaluations of urban pipeline performance, and if supplemented with a map of leak-prone pipe geo-location, and information on pipeline operating pressures, will provide a spatial database facilitating proactive repair and replacement of leak-prone urban pipes, a considerable improvement compared to reactive mitigation of human-reported leaks. While this work pertains to the selected urban towns in the Northeast, it exemplifies issues and opportunities nationwide in the United States."
NATHAN PHILLIPS,Characterizing urban landscapes using fuzzy sets,"Characterizing urban landscapes is important given the present and future projections of global population that favor urban growth. The definition of “urban” on a thematic map has proven to be problematic since urban areas are heterogeneous in terms of land use and land cover. Further, certain urban classes are inherently imprecise due to the difficulty in integrating various social and environmental inputs into a precise definition. Social components often include demographic patterns, transportation, building type and density while ecological components include soils, elevation, hydrology, climate, vegetation and tree cover. In this paper, we adopt a coupled human and natural system (CHANS) integrated scientific framework for characterizing urban landscapes. We implement the framework by adopting a fuzzy sets concept of “urban characterization” since fuzzy sets relate to classes of object with imprecise boundaries in which membership is a matter of degree. For dynamic mapping applications, user-defined classification schemes involving rules combining different social and ecological inputs can lead to a degree of quantification in class labeling varying from “highly urban” to “least urban”. A socio-economic perspective of urban may include threshold values for population and road network density while a more ecological perspective of urban may utilize the ratio of natural versus built area and percent forest cover. Threshold values are defined to derive the fuzzy rules of membership, in each case, and various combinations of rules offer a greater flexibility to characterize the many facets of the urban landscape. We illustrate the flexibility and utility of this fuzzy inference approach called the Fuzzy Urban Index for the Boston Metro region with five inputs and eighteen rules. The resulting classification map shows levels of fuzzy membership ranging from highly urban to least urban or rural in the Boston study region. We validate our approach using two experts assessing accuracy of the resulting fuzzy urban map. We discuss how our approach can be applied in other urban contexts with newly emerging descriptors of urban sustainability, urban ecology and urban metabolism."
NATHAN PHILLIPS,The BosWash infrastructure biome and energy system succession,"The BosWash corridor is a megalopolis, or large urbanized region composed of interconnected transportation, infrastructure, physiography, and sociopolitical systems. Previous work has not considered the BosWash corridor as an integrated, holistic ecosystem. Building on the emerging field of infrastructure ecology, the region is conceptualized here as an infrastructure biome, and this concept is applied to the region’s energy transition to a post-fossil fueled heating sector, in analogy to ecosystem succession. In this conception, infrastructure systems are analogous to focal species. A case study for an energy succession from an aging natural gas infrastructure to a carbon-free heating sector is presented, in order to demonstrate the utility of the infrastructure biome framework to address climate and energy challenges facing BosWash communities. Natural gas is a dominant energy source that emits carbon dioxide when burned and methane when leaked along the process chain; therefore, a transition to electricity is widely seen as necessary toward reducing greenhouse gas emissions. Utilizing an infrastructure biome framework for energy policy, a regional gas transition plan akin to the Regional Greenhouse Gas Initiative is generated to harmonize natural gas transition within the BosWash infrastructure biome and resolve conflict arising from a siloed approach to infrastructure management at individual city and state levels. This work generates and utilizes the novel infrastructure biome concept to prescribe a regional energy policy for an element of infrastructure that has not previously been explored at the regional scale—natural gas."
NATHAN PHILLIPS,Post-drought decline of the Amazon carbon sink,"Amazon forests have experienced frequent and severe droughts in the past two decades. However, little is known about the large-scale legacy of droughts on carbon stocks and dynamics of forests. Using systematic sampling of forest structure measured by LiDAR waveforms from 2003 to 2008, here we show a significant loss of carbon over the entire Amazon basin at a rate of 0.3 ± 0.2 (95% CI) PgC yr−1 after the 2005 mega-drought, which continued persistently over the next 3 years (2005–2008). The changes in forest structure, captured by average LiDAR forest height and converted to above ground biomass carbon density, show an average loss of 2.35 ± 1.80 MgC ha−1 a year after (2006) in the epicenter of the drought. With more frequent droughts expected in future, forests of Amazon may lose their role as a robust sink of carbon, leading to a significant positive climate feedback and exacerbating warming trends."
NATHAN PHILLIPS,Transient response of sap flow to wind speed,"Transient responses of sap flow to step changes in wind speed were experimentally investigated in a wind tunnel. AGranier-type sap flow sensor was calibrated and tested in a cylindrical tube for analysis of its transient time response.Then the sensor was used to measure the transient response of a well-watered Pachira macrocarpa plant to wind speed variations. The transient response of sap flow was described using the resistance–capacitance model. The steady sap flow rate increased as the wind speed increased at low wind speeds. Once the wind speed exceeded8.0 m s-1, the steady sap flow rate did not increase further. The transpiration rate, measured gravimetrically, showed a similar trend. The response of nocturnal sap flow to wind speed variation was also measured and compared with the results in the daytime. Under the same wind speed, the steady sap flow rate was smaller than that in the daytime,indicating differences between diurnal and nocturnal hydraulic function, and incomplete stomatal closure at night. In addition, it was found that the temporal response of the Granier sensor is fast enough to resolve the transient behaviour of water flux in plant tissue."
NATHAN PHILLIPS,Effects of warming temperatures on winning times in the Boston Marathon,"It is not known whether global warming will affect winning times in endurance events, and counterbalance improvements in race performances that have occurred over the past century. We examined a time series (1933–2004) from the Boston Marathon to test for an effect of warming on winning times by men and women. We found that warmer temperatures and headwinds on the day of the race slow winning times. However, 1.6°C warming in annual temperatures in Boston between 1933 and 2004 did not consistently slow winning times because of high variability in temperatures on race day. Starting times for the race changed to earlier in the day beginning in 2006, making it difficult to anticipate effects of future warming on winning times. However, our models indicate that if race starting times had not changed and average race day temperatures had warmed by 0.058°C/yr, a high-end estimate, we would have had a 95% chance of detecting a consistent slowing of winning marathon times by 2100. If average race day temperatures had warmed by 0.028°C/yr, a mid-range estimate, we would have had a 64% chance of detecting a consistent slowing of winning times by 2100."
NATHAN PHILLIPS,Pre-filed testimony in support of the ten persons group by Nathan G. Phillips,"I have two interrelated technical concerns with the Enbridge Model used by MassDEP to grant the air permit for the proposed Weymouth, which invalidate the air permit. I state these immediately below and elaborate on them thereafter. Rural Designation Ignores Coastal Site. Enbridge mischaracterized the site as “rural” when in fact it is a coastal, shoreline site embedded in an urban coastal community. This means the model cannot assess key meteorological phenomena important for pollution dispersion. Using an incorrect site characterization - even if surface meteorological measurements were made in a reasonably comparable location (Logan Airport compared to 50 Bridge Street, Weymouth) - means that the model cannot represent coastal/shoreline advection and incorrectly assumes that surface winds are uniform across a uniform surface rather than exhibiting sharp spatial gradients in surface energy balance and resulting atmospheric stability, winds, and air mixing associated with the water-land boundary. Shoreline Boundary Layer Development and Thermal Inversions Ignored. Since the Enbridge model is incapable of capturing shoreline effects it cannot assess the potential of pollution trapping through under-developed thermal internal boundary layers that may blanket residential areas. Moreover, MassDEP made no data collection or model validation across seasons, crucially ignoring winter coastal temperature inversions and resulting pollution trapping. Thermal and radiative inversions occur typically over vertical length scales of 150 meters, whereas the paired surface and upper air temperature measurements (from Gray, Maine, 185 miles away) used in the Enbridge Model are intended to and can only capture mesoscale effects, and cannot resolve crucial shoreline inversion events. The applicant’s consultant does not state what altitude it used for “upper air” measurements (www.mass.gov/files/documents/2018/06/11/algonquin-modeling.pdf) but according to EPA guidance these are typically several kilometers. The Enbridge Model mistakenly effectively assumes a fully-developed boundary layer condition and is thus unable to produce conditions that produce shoreline-induced looping or downwelling fumigating plumes that can expose residents to intermittently high concentrations of pollutants."
NATHAN PHILLIPS,"Strategic electrification in Washington, D.C.: neighborhood case studies of transition from gas to electric-based building heating",
NATHAN PHILLIPS,Home is where the pipeline ends: characterization of volatile organic compounds present in natural gas at the point of the residential end user,"The presence of volatile organic compounds (VOCs) in unprocessed natural gas (NG) is well documented; however, the degree to which VOCs are present in NG at the point of end use is largely uncharacterized. We collected 234 whole NG samples across 69 unique residential locations across the Greater Boston metropolitan area, Massachusetts. NG samples were measured for methane (CH4), ethane (C2H6), and nonmethane VOC (NMVOC) content (including tentatively identified compounds) using commercially available USEPA analytical methods. Results revealed 296 unique NMVOC constituents in end use NG, of which 21 (or approximately 7%) were designated as hazardous air pollutants. Benzene (bootstrapped mean = 164 ppbv; SD = 16; 95% CI: 134-196) was detected in 95% of samples along with hexane (98% detection), toluene (94%), heptane (94%), and cyclohexane (89%), contributing to a mean total concentration of NMVOCs in distribution-grade NG of 6.0 ppmv (95% CI: 5.5-6.6). While total VOCs exhibited significant spatial variability, over twice as much temporal variability was observed, with a wintertime NG benzene concentration nearly eight-fold greater than summertime. By using previous NG leakage data, we estimated that 120-356 kg/yr of annual NG benzene emissions throughout Greater Boston are not currently accounted for in emissions inventories, along with an unaccounted-for indoor portion. NG-odorant content (tert-butyl mercaptan and isopropyl mercaptan) was used to estimate that a mean NG-CH4 concentration of 21.3 ppmv (95% CI: 16.7-25.9) could persist undetected in ambient air given known odor detection thresholds. This implies that indoor NG leakage may be an underappreciated source of both CH4 and associated VOCs."
NATHAN PHILLIPS,"Carbon, indoor air, energy and financial benefits of coupled ventilation upgrade and enhanced rooftop garden installation: an interdisciplinary climate mitigation approach",
NATHAN PHILLIPS,A simple method to measure methane emissions from indoor gas leaks,"From wellhead to burner tip, each component of the natural gas process chain has come under increased scrutiny for the presence and magnitude of methane leaks, because of the large global warming potential of methane. Top-down measures of methane emissions in urban areas are significantly greater than bottom-up estimates. Recent research suggests this disparity might in part be explained by gas leaks from one of the least understood parts of the process chain: behind the gas meter in homes and buildings. However, little research has been performed in this area and few methods and data sets exist to measure or estimate them. We develop and test a simple and widely deployable closed chamber method that can be used for quantifying indoor methane emissions with an order-of-magnitude precision which allows for screening of indoor large volume (""super-emitting"") leaks. We also perform test applications of the method finding indoor leaks in 90% of the 20 Greater Boston buildings studied and indoor methane emissions between 0.02-0.51 ft3 CH4 day-1 (0.4-10.3 g CH4 day-1) with a mean of 0.14 ft3 CH4 day-1 (2.8 g CH4 day-1). Our method provides a relatively simple way to scale up indoor methane emissions data collection. Increased data may reduce uncertainty in bottom-up inventories, and can be used to find super-emitting indoor emissions which may better explain the disparity between top-down and bottom-up post-meter emissions estimates."
NATHAN PHILLIPS,Enhancing crop growth in rooftop farms by repurposing CO2 from human respiration inside buildings,"Integrating cities with the surrounding environment by incorporating green spaces in creative ways would help counter climate change. We propose a rooftop farm system called BIG GRO where air enriched with carbon dioxide (CO2) produced through respiration from indoor spaces is applied through existing ventilation systems to produce a fertilization effect and increased plant growth. CO2 measurements were taken inside 20 classrooms and at two exhaust vents on a rooftop at Boston University in Boston, MA. Exhausted air was directed toward spinach and corn and plant biomass and leaf number were analyzed. High concentrations of CO2 persisted inside classrooms and at rooftop exhaust vents in correlation with expected human occupancy. CO2 levels averaged 1,070 and 830 parts per million (ppm), reaching a maximum of 4,470 and 1,300 ppm CO2 indoors and at exhaust vents, respectively. The biomass of spinach grown next to exhaust air increased fourfold compared to plants grown next to a control fan applying atmospheric air. High wind speed from fans decreased growth by approximately twofold. The biomass of corn, a C4 plant, experienced a two to threefold increase, indicating that alternative environmental factors, such as temperature, likely contribute to growth enhancement. Enhancing growth in rooftop farms using indoor air would help increase yield and help crops survive harsh conditions, which would make their installation in cities more feasible."
NATHAN PHILLIPS,Event Horizon Telescope imaging of the archetypal blazar 3C 279 at an extreme 20 microarcsecond resolution,"3C 279 is an archetypal blazar with a prominent radio jet that show broadband flux density variability across the entire electromagnetic spectrum. We use an ultra-high angular resolution technique – global Very Long Baseline Interferometry (VLBI) at 1.3 mm (230 GHz) – to resolve the innermost jet of 3C 279 in order to study its fine-scale morphology close to the jet base where highly variable γ-ray emission is thought to originate, according to various models. The source was observed during four days in April 2017 with the Event Horizon Telescope at 230 GHz, including the phased Atacama Large Millimeter/submillimeter Array (ALMA), at an angular resolution of ∼20 μas (at a redshift of z = 0.536 this corresponds to ∼0.13 pc  ∼ 1700 Schwarzschild radii with a black hole mass M<jats:sub>BH</jats:sub> = 8 × 10^8 M_⊙). Imaging and model-fitting techniques were applied to the data to parameterize the fine-scale source structure and its variation. We find a multicomponent inner jet morphology with the northernmost component elongated perpendicular to the direction of the jet, as imaged at longer wavelengths. The elongated nuclear structure is consistent on all four observing days and across different imaging methods and model-fitting techniques, and therefore appears robust. Owing to its compactness and brightness, we associate the northern nuclear structure as the VLBI “core”. This morphology can be interpreted as either a broad resolved jet base or a spatially bent jet. We also find significant day-to-day variations in the closure phases, which appear most pronounced on the triangles with the longest baselines. Our analysis shows that this variation is related to a systematic change of the source structure. Two inner jet components move non-radially at apparent speeds of ∼15 c and ∼20 c (∼1.3 and ∼1.7 μas day^−1, respectively), which more strongly supports the scenario of traveling shocks or instabilities in a bent, possibly rotating jet. The observed apparent speeds are also coincident with the 3C 279 large-scale jet kinematics observed at longer (cm) wavelengths, suggesting no significant jet acceleration between the 1.3 mm core and the outer jet. The intrinsic brightness temperature of the jet components are ≲10^10 K, a magnitude or more lower than typical values seen at ≥7 mm wavelengths. The low brightness temperature and morphological complexity suggest that the core region of 3C 279 becomes optically thin at short (mm) wavelengths."
NATHAN PHILLIPS,First M87 Event Horizon Telescope results. II. Array and instrumentation,"The Event Horizon Telescope (EHT) is a very long baseline interferometry (VLBI) array that comprises millimeter- and submillimeter-wavelength telescopes separated by distances comparable to the diameter of the Earth. At a nominal operating wavelength of ~1.3 mm, EHT angular resolution (λ/D) is ~25 μas, which is sufficient to resolve nearby supermassive black hole candidates on spatial and temporal scales that correspond to their event horizons. With this capability, the EHT scientific goals are to probe general relativistic effects in the strong-field regime and to study accretion and relativistic jet formation near the black hole boundary. In this Letter we describe the system design of the EHT, detail the technology and instrumentation that enable observations, and provide measures of its performance. Meeting the EHT science objectives has required several key developments that have facilitated the robust extension of the VLBI technique to EHT observing wavelengths and the production of instrumentation that can be deployed on a heterogeneous array of existing telescopes and facilities. To meet sensitivity requirements, high-bandwidth digital systems were developed that process data at rates of 64 gigabit s−1, exceeding those of currently operating cm-wavelength VLBI arrays by more than an order of magnitude. Associated improvements include the development of phasing systems at array facilities, new receiver installation at several sites, and the deployment of hydrogen maser frequency standards to ensure coherent data capture across the array. These efforts led to the coordination and execution of the first Global EHT observations in 2017 April, and to event-horizon-scale imaging of the supermassive black hole candidate in M87."
NATHAN PHILLIPS,First M87 Event Horizon Telescope results. I. The shadow of the supermassive black hole,"When surrounded by a transparent emission region, black holes are expected to reveal a dark shadow caused by gravitational light bending and photon capture at the event horizon. To image and study this phenomenon, we have assembled the Event Horizon Telescope, a global very long baseline interferometry array observing at a wavelength of 1.3 mm. This allows us to reconstruct event-horizon-scale images of the supermassive black hole candidate in the center of the giant elliptical galaxy M87. We have resolved the central compact radio source as an asymmetric bright emission ring with a diameter of 42 ± 3 μas, which is circular and encompasses a central depression in brightness with a flux ratio gsim10:1. The emission ring is recovered using different calibration and imaging schemes, with its diameter and width remaining stable over four different observations carried out in different days. Overall, the observed image is consistent with expectations for the shadow of a Kerr black hole as predicted by general relativity. The asymmetry in brightness in the ring can be explained in terms of relativistic beaming of the emission from a plasma rotating close to the speed of light around a black hole. We compare our images to an extensive library of ray-traced general-relativistic magnetohydrodynamic simulations of black holes and derive a central mass of M = (6.5 ± 0.7) × 10^9 M ⊙. Our radio-wave observations thus provide powerful evidence for the presence of supermassive black holes in centers of galaxies and as the central engines of active galactic nuclei. They also present a new tool to explore gravity in its most extreme limit and on a mass scale that was so far not accessible."
DAVID SHERR,Direct Assessment of Cumulative Aryl Hydrocarbon Receptor Agonist Activity in Sera from Experimentally Exposed Mice and Environmentally Exposed Humans,"BACKGROUND. Aryl hydrocarbon receptor (AhR) ligands adversely affect many biological processes. However, assessment of the significance of human exposures is hampered by an incomplete understanding of how complex mixtures affect AhR activation/inactivation. OBJECTIVES. These studies used biological readouts to provide a broader context for estimating human risk than that obtained with serum extraction and gas chromatography/mass spectroscopy (GC/MS)-based assays alone. METHODS. AhR agonist activity was quantified in sera from dioxin-treated mice, commercial human sources, and polychlorinated biphenyl (PCB)-exposed Faroe Islanders using an AhR-driven reporter cell line. To validate relationships between serum AhR agonist levels and biological outcomes, AhR agonist activity in mouse sera correlated with toxic end points. AhR agonist activity in unmanipulated (""neat"") human sera was compared with these biologically relevant doses and with GC/MS-assayed PCB levels. RESULTS. Mouse serum AhR agonist activity correlated with injected dioxin dose, thymic atrophy, and heptomegaly, validating the use of neat serum to assess AhR agonist activity. AhR agonist activity in sera from Faroe Islanders varied widely, was associated with the frequency of recent pilot whale dinners, but did not correlate with levels of PCBs quantified by GC/MS. Surprisingly, significant ""baseline"" AhR activity was found in commercial human sera. CONCLUSIONS. An AhR reporter assay revealed cumulative levels of AhR activation potential in neat serum, whereas extraction may preclude detection of important non-dioxin-like biological activity. Significant levels of AhR agonist activity in commercial sera and in Faroe Islander sera, compared with that from experimentally exposed mice, suggest human exposures that are biologically relevant in both populations."
DAVID SHERR,Disruption of Human Plasma Cell Differentiation by an Environmental Polycyclic Aromatic Hydrocarbon: A Mechanistic Immunotoxicological Study,"BACKGROUND: The AhR is a ligand-activated transcription factor that mediates immunosuppression induced by environmental PAH and HAH. Recently, a critical role for the AhR in development of T cells involved in autoimmunity (Th17 and Treg) has been demonstrated, supporting the hypothesis that the AhR plays a key role in immune regulation both in the presence and absence of environmental ligands. Despite these results with T cells systems, little is known of the role that the AhR plays in B cell development. We have demonstrated that B cell activation with CD40 ligand, a stimulus that models adaptive immunity, induces AhR expression in primary human B cells, suggesting that activation may increase human B cell sensitivity to AhR ligands and that the AhR may play a role in B cell development. METHODS: To test these possibilities, we developed an in vitro system in which activated human B cells expressing high AhR levels are induced to differentiate into plasma cells. Consequently, the effects of benzo [a]pyrene, a prototypic environmental AhR ligand, on plasma cell differentiation could be investigated and this chemical could be exploited essentially as drug probe to implicate the role of the AhR in plasma cell development. RESULTS: A previously unattainable level of B cell differentiation into plasma cells (up to 45% conversion) was observed. Benzo [a]pyrene significantly suppressed that differentiation. γ-Irradiation after an initial proliferation phase induced by CD40 ligand and immediately prior to initiation of the differentiation phase blocked cell growth but did not affect cell viability or plasma cell differentiation. B [a]P suppressed differentiation whether or not cell growth was inhibited by γ-irradiation. CONCLUSIONS: 1) Extensive proliferation is not required during the differentiation phase per se for CD40L-activated human B cells to undergo plasma cell differentiation, and 2) an environmental PAH blocks both proliferation and differentiation of AhR expressing B cells. The results uncover a new mechanism by which environmentally ubiquitous PAHs may negatively impact human B cell-mediated immunity."
DAVID SHERR,"Growth of a Human Mammary Tumor Cell Line Is Blocked by Galangin, a Naturally Occurring Bioflavonoid, and Is Accompanied by Down-Regulation of Cyclins D3, E, and A","INTRODUCTION. This study was designed to determine if and how a non-toxic, naturally occurring bioflavonoid, galangin, affects proliferation of human mammary tumor cells. Our previous studies demonstrated that, in other cell types, galangin is a potent inhibitor of the aryl hydrocarbon receptor (AhR), an environmental carcinogen-responsive transcription factor implicated in mammary tumor initiation and growth control. Because some current breast cancer therapeutics are ineffective in estrogen receptor (ER) negative tumors and since the AhR may be involved in breast cancer proliferation, the effects of galangin on the proliferation of an ER-, AhRhigh line, Hs578T, were studied. METHODS. AhR expression and function in the presence or absence of galangin, a second AhR inhibitor, α-naphthoflavone (α-NF), an AhR agonist, indole-3-carbinol, and a transfected AhR repressor-encoding plasmid (FhAhRR) were studied in Hs578T cells by western blotting for nuclear (for instance, constitutively activated) AhR and by transfection of an AhR-driven reporter construct, pGudLuc. The effects of these agents on cell proliferation were studied by 3H-thymidine incorporation and by flow cytometry. The effects on cyclins implicated in mammary tumorigenesis were evaluated by western blotting. RESULTS. Hs578T cells were shown to express high levels of constitutively active AhR. Constitutive and environmental chemical-induced AhR activity was profoundly suppressed by galangin as was cell proliferation. However, the failure of α-NF or FhAhRR transfection to block proliferation indicated that galangin-mediated AhR inhibition was either insufficient or unrelated to its ability to significantly block cell proliferation at therapeutically relevant doses (IC50 = 11 μM). Galangin inhibited transition of cells from the G0/G1 to the S phases of cell growth, likely through the nearly total elimination of cyclin D3. Expression of cyclins A and E was also suppressed. CONCLUSION. Galangin is a strong inhibitor of Hs578T cell proliferation that likely mediates this effect through a relatively unique mechanism, suppression of cyclin D3, and not through the AhR. The results suggest that this non-toxic bioflavonoid may be useful as a chemotherapeutic, particularly in combination with agents that target other components of the tumor cell cycle and in situations where estrogen receptor-specific therapeutics are ineffective."
DAVID SHERR,NFIA Haploinsufficiency Is Associated with a CNS Malformation Syndrome and Urinary Tract Defects,"Complex central nervous system (CNS) malformations frequently coexist with other developmental abnormalities, but whether the associated defects share a common genetic basis is often unclear. We describe five individuals who share phenotypically related CNS malformations and in some cases urinary tract defects, and also haploinsufficiency for the NFIA transcription factor gene due to chromosomal translocation or deletion. Two individuals have balanced translocations that disrupt NFIA. A third individual and two half-siblings in an unrelated family have interstitial microdeletions that include NFIA. All five individuals exhibit similar CNS malformations consisting of a thin, hypoplastic, or absent corpus callosum, and hydrocephalus or ventriculomegaly. The majority of these individuals also exhibit Chiari type I malformation, tethered spinal cord, and urinary tract defects that include vesicoureteral reflux. Other genes are also broken or deleted in all five individuals, and may contribute to the phenotype. However, the only common genetic defect is NFIA haploinsufficiency. In addition, previous analyses of Nfia−/− knockout mice indicate that Nfia deficiency also results in hydrocephalus and agenesis of the corpus callosum. Further investigation of the mouse Nfia+/− and Nfia−/− phenotypes now reveals that, at reduced penetrance, Nfia is also required in a dosage-sensitive manner for ureteral and renal development. Nfia is expressed in the developing ureter and metanephric mesenchyme, and Nfia+/− and Nfia−/− mice exhibit abnormalities of the ureteropelvic and ureterovesical junctions, as well as bifid and megaureter. Collectively, the mouse Nfia mutant phenotype and the common features among these five human cases indicate that NFIA haploinsufficiency contributes to a novel human CNS malformation syndrome that can also include ureteral and renal defects. Author Summary Central nervous system (CNS) and urinary tract abnormalities are common human malformations, but their variability and genetic complexity make it difficult to identify the responsible genes. Analysis of human chromosomal abnormalities associated with such disorders offers one approach to this problem. In five individuals described herein, a novel human syndrome that involves both CNS and urinary tract defects is associated with chromosomal disruption or deletion of NFIA, encoding a member of the Nuclear Factor I (NFI) family of transcription factors. This syndrome includes brain abnormalities (abnormal corpus callosum, hydrocephalus, ventriculomegaly, and Chiari type I malformation), spinal abnormalities (tethered spinal cord), and urinary tract abnormalities (vesicoureteral reflux). Nfia disruption in mice was already known to cause hydrocephalus and abnormal corpus callosum, and is now shown to exhibit renal defects and disturbed ureteral development. Other genes besides NFIA are also disrupted or deleted and may contribute to the observed phenotype. However, loss of one copy of NFIA is the only genetic defect common to all five patients. The authors thus provide evidence that genetic loss of NFIA contributes to a distinct CNS malformation syndrome with urinary tract defects of variable penetrance."
DAVID SHERR,Aryl hydrocarbon receptor (AhR) agonists suppress Interleukin-6 expression by bone marrow stromal cells: an immunotoxicology study,"BACKGROUND: Bone marrow stromal cells produce cytokines required for the normal growth and development of all eight hematopoietic cell lineages. Aberrant cytokine production by stromal cells contributes to blood cell dyscrasias. Consequently, factors that alter stromal cell cytokine production may significantly compromise the development of normal blood cells. We have shown that environmental chemicals, such as aromatic hydrocarbon receptor (AhR) agonists, suppress B lymphopoiesis by modulating bone marrow stromal cell function. Here, we extend these studies to evaluate the potential for two prototypic AhR agonists, 7,12-dimethylbenz [a]anthracene (DMBA) and 2,3,7,8-tetrachlorodibenzo-p-dioxin (TCDD), to alter stromal cell cytokine responses. METHODS: Bone marrow stromal cells were treated with AhR agonists and bacterial lipopolysaccharide (LPS) to mimic innate inflammatory cytokine responses and to study the effects of AhR ligands on those responses. Steady state cytokine RNA levels were screened by RNAse protection assays (RPA) and quantified by real-time PCR. Cytokine (IL-6) protein production was measured by ELISA. NF-κB EMSAs were used to study IL-6 transcriptional regulation. RESULTS: RPAs indicated that AhR+ bone marrow stromal cells consistently up-regulated genes encoding IL-6 and LIF in response to LPS, presumably through activation of Toll-like receptor 4. Pre-treatment with low doses of DMBA or TCDD selectively abrogated IL-6 gene induction but had no effect on LIF mRNA. Real-time-PCR indicated a significant inhibition of IL-6 mRNA by AhR ligands within 1 hour of LPS challenge which was reflected in a profound down-regulation of IL-6 protein induction, with DMBA and TCDD suppressing IL-6 levels as much as 65% and 88%, respectively. This potent inhibitory effect persisted for at least 72 hours. EMSAs measuring NF-κB binding to IL-6 promoter sequences, an event known to induce IL-6 transcription, indicated a significant decrease in the LPS-mediated induction of DNA-binding RelA/p50 and c-Rel/p50 heterodimers in the presence of DMBA. CONCLUSIONS: Common environmental AhR agonists can suppress the response to bacterial lipopolysaccharide, a model for innate inflammatory responses, through down-regulation of IL-6, a cytokine critical to the growth of several hematopoietic cell subsets, including early B cells. This suppression occurs at least at the level of IL-6 gene transcription and may be regulated by NF-κB."
ANDREW W TAYLOR,"BMQ : Boston medical quarterly: v. 15, no. 1-4",
ANDREW W TAYLOR,"Caribbean Corals in Crisis: Record Thermal Stress, Bleaching, and Mortality in 2005","BACKGROUND. The rising temperature of the world's oceans has become a major threat to coral reefs globally as the severity and frequency of mass coral bleaching and mortality events increase. In 2005, high ocean temperatures in the tropical Atlantic and Caribbean resulted in the most severe bleaching event ever recorded in the basin. METHODOLOGY/PRINCIPAL FINDINGS. Satellite-based tools provided warnings for coral reef managers and scientists, guiding both the timing and location of researchers' field observations as anomalously warm conditions developed and spread across the greater Caribbean region from June to October 2005. Field surveys of bleaching and mortality exceeded prior efforts in detail and extent, and provided a new standard for documenting the effects of bleaching and for testing nowcast and forecast products. Collaborators from 22 countries undertook the most comprehensive documentation of basin-scale bleaching to date and found that over 80% of corals bleached and over 40% died at many sites. The most severe bleaching coincided with waters nearest a western Atlantic warm pool that was centered off the northern end of the Lesser Antilles. CONCLUSIONS/SIGNIFICANCE. Thermal stress during the 2005 event exceeded any observed from the Caribbean in the prior 20 years, and regionally-averaged temperatures were the warmest in over 150 years. Comparison of satellite data against field surveys demonstrated a significant predictive relationship between accumulated heat stress (measured using NOAA Coral Reef Watch's Degree Heating Weeks) and bleaching intensity. This severe, widespread bleaching and mortality will undoubtedly have long-term consequences for reef ecosystems and suggests a troubled future for tropical marine ecosystems under a warming climate."
ANDREW W TAYLOR,The eighteenth data release of the Sloan Digital Sky Surveys: targeting and first spectra from SDSS-V,"The eighteenth data release (DR18) of the Sloan Digital Sky Survey (SDSS) is the first one for SDSS-V, the fifth generation of the survey. SDSS-V comprises three primary scientific programs or “Mappers”: the Milky Way Mapper (MWM), the Black Hole Mapper (BHM), and the Local Volume Mapper. This data release contains extensive targeting information for the two multiobject spectroscopy programs (MWM and BHM), including input catalogs and selection functions for their numerous scientific objectives. We describe the production of the targeting databases and their calibration and scientifically focused components. DR18 also includes ∼25,000 new SDSS spectra and supplemental information for X-ray sources identified by eROSITA in its eFEDS field. We present updates to some of the SDSS software pipelines and preview changes anticipated for DR19. We also describe three value-added catalogs (VACs) based on SDSS-IV data that have been published since DR17, and one VAC based on the SDSS-V data in the eFEDS field."
ANDREW W TAYLOR,The long-baseline neutrino experiment: exploring fundamental symmetries of the universe,"The preponderance of matter over antimatter in the early Universe, the dynamics of the supernova bursts that produced the heavy elements necessary for life and whether protons eventually decay --- these mysteries at the forefront of particle physics and astrophysics are key to understanding the early evolution of our Universe, its current state and its eventual fate. The Long-Baseline Neutrino Experiment (LBNE) represents an extensively developed plan for a world-class experiment dedicated to addressing these questions. LBNE is conceived around three central components: (1) a new, high-intensity neutrino source generated from a megawatt-class proton accelerator at Fermi National Accelerator Laboratory, (2) a near neutrino detector just downstream of the source, and (3) a massive liquid argon time-projection chamber deployed as a far detector deep underground at the Sanford Underground Research Facility. This facility, located at the site of the former Homestake Mine in Lead, South Dakota, is approximately 1,300 km from the neutrino source at Fermilab -- a distance (baseline) that delivers optimal sensitivity to neutrino charge-parity symmetry violation and mass ordering effects. This ambitious yet cost-effective design incorporates scalability and flexibility and can accommodate a variety of upgrades and contributions. With its exceptional combination of experimental configuration, technical capabilities, and potential for transformative discoveries, LBNE promises to be a vital facility for the field of particle physics worldwide, providing physicists from around the globe with opportunities to collaborate in a twenty to thirty year program of exciting science. In this document we provide a comprehensive overview of LBNE's scientific objectives, its place in the landscape of neutrino physics worldwide, the technologies it will incorporate and the capabilities it will possess."
ANDREW W TAYLOR,Broadband multi-wavelength properties of M87 during the 2017 Event Horizon Telescope campaign,"In 2017, the Event Horizon Telescope (EHT) Collaboration succeeded in capturing the first direct image of the center of the M87 galaxy. The asymmetric ring morphology and size are consistent with theoretical expectations for a weakly accreting supermassive black hole of mass ∼6.5 × 109 M ⊙. The EHTC also partnered with several international facilities in space and on the ground, to arrange an extensive, quasi-simultaneous multi-wavelength campaign. This Letter presents the results and analysis of this campaign, as well as the multi-wavelength data as a legacy data repository. We captured M87 in a historically low state, and the core flux dominates over HST-1 at high energies, making it possible to combine core flux constraints with the more spatially precise very long baseline interferometry data. We present the most complete simultaneous multi-wavelength spectrum of the active nucleus to date, and discuss the complexity and caveats of combining data from different spatial scales into one broadband spectrum. We apply two heuristic, isotropic leptonic single-zone models to provide insight into the basic source properties, but conclude that a structured jet is necessary to explain M87’s spectrum. We can exclude that the simultaneous γ-ray emission is produced via inverse Compton emission in the same region producing the EHT mm-band emission, and further conclude that the γ-rays can only be produced in the inner jets (inward of HST-1) if there are strongly particle-dominated regions. Direct synchrotron emission from accelerated protons and secondaries cannot yet be excluded."
MARC E LENBURG,Smoking-Induced Gene Expression Changes in the Bronchial Airway Are Reflected in Nasal and Buccal Epithelium,"BACKGROUND: Cigarette smoking is a leading cause of preventable death and a significant cause of lung cancer and chronic obstructive pulmonary disease. Prior studies have demonstrated that smoking creates a field of molecular injury throughout the airway epithelium exposed to cigarette smoke. We have previously characterized gene expression in the bronchial epithelium of never smokers and identified the gene expression changes that occur in the mainstem bronchus in response to smoking. In this study, we explored relationships in whole-genome gene expression between extrathorcic (buccal and nasal) and intrathoracic (bronchial) epithelium in healthy current and never smokers. RESULTS: Using genes that have been previously defined as being expressed in the bronchial airway of never smokers (the ""normal airway transcriptome""), we found that bronchial and nasal epithelium from non-smokers were most similar in gene expression whencompared to other epithelial and nonepithelial tissues, with several antioxidant, detoxification, and structural genes being highly expressed in both the bronchus and nose. Principle component analysis of previously defined smoking-induced genes from the bronchus suggested that smoking had a similar effect on gene expression in nasal epithelium. Gene set enrichment analysis demonstrated that this set of genes was also highly enriched among the genes most altered by smoking in both nasal and buccal epithelial samples. The expression of several detoxification genes was commonly altered by smoking in all three respiratory epithelial tissues, suggesting a common airway-wide response to tobacco exposure. CONCLUSION: Our findings support a relationship between gene expression in extra- and intrathoracic airway epithelial cells and extend the concept of a smoking-induced field of injury to epithelial cells that line the mouth and nose. This relationship could potentially be utilized to develop a non-invasive biomarker for tobacco exposure as well as a non-invasive screening or diagnostic tool providing information about individual susceptibility to smoking-induced lung diseases."
MARC E LENBURG,Reversible and Permanent Effects of Tobacco Smoke Exposure on Airway Epithelial Gene Expression,"Oligonucleotide microarray analysis revealed 175 genes that are differentially expressed in large airway epithelial cells of people who currently smoke compared with those who never smoked, with 28 classified as irreversible, 6 as slowly reversible, and 139 as rapidly reversible. BACKGROUND. Tobacco use remains the leading preventable cause of death in the US. The risk of dying from smoking-related diseases remains elevated for former smokers years after quitting. The identification of irreversible effects of tobacco smoke on airway gene expression may provide insights into the causes of this elevated risk. RESULTS. Using oligonucleotide microarrays, we measured gene expression in large airway epithelial cells obtained via bronchoscopy from never, current, and former smokers (n = 104). Linear models identified 175 genes differentially expressed between current and never smokers, and classified these as irreversible (n = 28), slowly reversible (n = 6), or rapidly reversible (n = 139) based on their expression in former smokers. A greater percentage of irreversible and slowly reversible genes were down-regulated by smoking, suggesting possible mechanisms for persistent changes, such as allelic loss at 16q13. Similarities with airway epithelium gene expression changes caused by other environmental exposures suggest that common mechanisms are involved in the response to tobacco smoke. Finally, using irreversible genes, we built a biomarker of ever exposure to tobacco smoke capable of classifying an independent set of former and current smokers with 81% and 100% accuracy, respectively. CONCLUSION. We have categorized smoking-related changes in airway gene expression by their degree of reversibility upon smoking cessation. Our findings provide insights into the mechanisms leading to reversible and persistent effects of tobacco smoke that may explain former smokers increased risk for developing tobacco-induced lung disease and provide novel targets for chemoprophylaxis. Airway gene expression may also serve as a sensitive biomarker to identify individuals with past exposure to tobacco smoke."
MARC E LENBURG,The expression level of HJURP has an independent prognostic impact and predicts the sensitivity to radiotherapy in breast cancer,"INTRODUCTION. HJURP (Holliday Junction Recognition Protein) is a newly discovered gene reported to function at centromeres and to interact with CENPA. However its role in tumor development remains largely unknown. The goal of this study was to investigate the clinical significance of HJURP in breast cancer and its correlation with radiotherapeutic outcome. METHODS. We measured HJURP expression level in human breast cancer cell lines and primary breast cancers by Western blot and/or by Affymetrix Microarray; and determined its associations with clinical variables using standard statistical methods. Validation was performed with the use of published microarray data. We assessed cell growth and apoptosis of breast cancer cells after radiation using high-content image analysis. RESULTS. HJURP was expressed at higher level in breast cancer than in normal breast tissue. HJURP mRNA levels were significantly associated with estrogen receptor (ER), progesterone receptor (PR), Scarff-Bloom-Richardson (SBR) grade, age and Ki67 proliferation indices, but not with pathologic stage, ERBB2, tumor size, or lymph node status. Higher HJURP mRNA levels significantly decreased disease-free and overall survival. HJURP mRNA levels predicted the prognosis better than Ki67 proliferation indices. In a multivariate Cox proportional-hazard regression, including clinical variables as covariates, HJURP mRNA levels remained an independent prognostic factor for disease-free and overall survival. In addition HJURP mRNA levels were an independent prognostic factor over molecular subtypes (normal like, luminal, Erbb2 and basal). Poor clinical outcomes among patients with high HJURP expression were validated in five additional breast cancer cohorts. Furthermore, the patients with high HJURP levels were much more sensitive to radiotherapy. In vitro studies in breast cancer cell lines showed that cells with high HJURP levels were more sensitive to radiation treatment and had a higher rate of apoptosis than those with low levels. Knock down of HJURP in human breast cancer cells using shRNA reduced the sensitivity to radiation treatment. HJURP mRNA levels were significantly correlated with CENPA mRNA levels. CONCLUSIONS. HJURP mRNA level is a prognostic factor for disease-free and overall survival in patients with breast cancer and is a predictive biomarker for sensitivity to radiotherapy."
MARC E LENBURG,Comparison of Proteomic and Transcriptomic Profiles in the Bronchial Airway Epithelium of Current and Never Smokers,"BACKGROUND. Although prior studies have demonstrated a smoking-induced field of molecular injury throughout the lung and airway, the impact of smoking on the airway epithelial proteome and its relationship to smoking-related changes in the airway transcriptome are unclear. METHODOLOGY/PRINCIPAL FINDINGS. Airway epithelial cells were obtained from never (n=5) and current (n=5) smokers by brushing the mainstem bronchus. Proteins were separated by one dimensional polyacrylamide gel electrophoresis (1D-PAGE). After in-gel digestion, tryptic peptides were processed via liquid chromatography/ tandem mass spectrometry (LC-MS/MS) and proteins identified. RNA from the same samples was hybridized to HG-U133A microarrays. Protein detection was compared to RNA expression in the current study and a previously published airway dataset. The functional properties of many of the 197 proteins detected in a majority of never smokers were similar to those observed in the never smoker airway transcriptome. LC-MS/MS identified 23 proteins that differed between never and current smokers. Western blotting confirmed the smoking-related changes of PLUNC, P4HB1, and uteroglobin protein levels. Many of the proteins differentially detected between never and current smokers were also altered at the level of gene expression in this cohort and the prior airway transcriptome study. There was a strong association between protein detection and expression of its corresponding transcript within the same sample, with 86% of the proteins detected by LC-MS/MS having a detectable corresponding probeset by microarray in the same sample. Forty-one proteins identified by LC-MS/MS lacked detectable expression of a corresponding transcript and were detected in =5% of airway samples from a previously published dataset. CONCLUSIONS/SIGNIFICANCE. 1D-PAGE coupled with LC-MS/MS effectively profiled the airway epithelium proteome and identified proteins expressed at different levels as a result of cigarette smoke exposure. While there was a strong correlation between protein and transcript detection within the same sample, we also identified proteins whose corresponding transcripts were not detected by microarray. This noninvasive approach to proteomic profiling of airway epithelium may provide additional insights into the field of injury induced by tobacco exposure."
MARC E LENBURG,Previously Unidentified Changes in Renal Cell Carcinoma Gene Expression Identified by Parametric Analysis of Microarray Data,"BACKGROUND. Renal cell carcinoma is a common malignancy that often presents as a metastatic-disease for which there are no effective treatments. To gain insights into the mechanism of renal cell carcinogenesis, a number of genome-wide expression profiling studies have been performed. Surprisingly, there is very poor agreement among these studies as to which genes are differentially regulated. To better understand this lack of agreement we profiled renal cell tumor gene expression using genome-wide microarrays (45,000 probe sets) and compare our analysis to previous microarray studies. METHODS. We hybridized total RNA isolated from renal cell tumors and adjacent normal tissue to Affymetrix U133A and U133B arrays. We removed samples with technical defects and removed probesets that failed to exhibit sequence-specific hybridization in any of the samples. We detected differential gene expression in the resulting dataset with parametric methods and identified keywords that are overrepresented in the differentially expressed genes with the Fisher-exact test. RESULTS. We identify 1,234 genes that are more than three-fold changed in renal tumors by t-test, 800 of which have not been previously reported to be altered in renal cell tumors. Of the only 37 genes that have been identified as being differentially expressed in three or more of five previous microarray studies of renal tumor gene expression, our analysis finds 33 of these genes (89%). A key to the sensitivity and power of our analysis is filtering out defective samples and genes that are not reliably detected. CONCLUSIONS. The widespread use of sample-wise voting schemes for detecting differential expression that do not control for false positives likely account for the poor overlap among previous studies. Among the many genes we identified using parametric methods that were not previously reported as being differentially expressed in renal cell tumors are several oncogenes and tumor suppressor genes that likely play important roles in renal cell carcinogenesis. This highlights the need for rigorous statistical approaches in microarray studies."
ADAM B SELIGMAN,Symposium: the achievement of David Martin,This brief introduction notes some of the salient aspects of David Martin’s career and thought. It further presents and frames the following eight essays in this symposium devoted to different aspects of David Martin’s work.
ADAM B SELIGMAN,David Martin and the sociology of hope,"David Martin’s work has always bridged many worlds: the sacred and the secular, the world of power politics and of religious visions, of individual and society, and of poetry and rational analysis. His trenchant and uncompromising analyses of human social formations and their ideational concomitants have nevertheless provided many with a vision of that hope which must sustain scholarly analysis if it is not to become tedious and moribund. His sensitivity to tradition, to ritual, to received knowledge and the debt we owe to the past – even while appreciating the frisson of the radically new (as in his studies of Pentacostalism) – have made him one of only a small handful of scholars who could address the broad range of human religious expression and its implications for life in the world. This paper explores some of these themes in terms of what we understand as the overwhelming sense of hope that is a permanent feature of David’s scholarly contributions."
JAMES A FELDMAN,Microfluidic chip for molecular amplification of influenza A RNA in human respiratory specimens,"A rapid, low cost, accurate point-of-care (POC) device to detect influenza virus is needed for effective treatment and control of both seasonal and pandemic strains. We developed a single-use microfluidic chip that integrates solid phase extraction (SPE) and molecular amplification via a reverse transcription polymerase chain reaction (RT-PCR) to amplify influenza virus type A RNA. We demonstrated the ability of the chip to amplify influenza A RNA in human nasopharyngeal aspirate (NPA) and nasopharyngeal swab (NPS) specimens collected at two clinical sites from 2008–2010. The microfluidic test was dramatically more sensitive than two currently used rapid immunoassays and had high specificity that was essentially equivalent to the rapid assays and direct fluorescent antigen (DFA) testing. We report 96% (CI 89%,99%) sensitivity and 100% (CI 95%,100%) specificity compared to conventional (bench top) RT-PCR based on the testing of n = 146 specimens (positive predictive value = 100%(CI 94%,100%) and negative predictive value = 96%(CI 88%,98%)). These results compare well with DFA performed on samples taken during the same time period (98% (CI 91%,100%) sensitivity and 96%(CI 86%,99%) specificity compared to our gold standard testing). Rapid immunoassay tests on samples taken during the enrollment period were less reliable (49%(CI 38%,61%) sensitivity and 98%(CI 98%,100%) specificity). The microfluidic test extracted and amplified influenza A RNA directly from clinical specimens with viral loads down to 103 copies/ml in 3 h or less. The new test represents a major improvement over viral culture in terms of turn around time, over rapid immunoassay tests in terms of sensitivity, and over bench top RT-PCR and DFA in terms of ease of use and portability."
JAMES A FELDMAN,"Centerscope: v. 6, no. 1-6",
JAMES A FELDMAN,"Scope: v. 2, no. 1-6",
JAMES A FELDMAN,"Self-Reported Safety Belt Use among Emergency Department Patients in Boston, Massachusetts","BACKGROUND. Safety belt use is 80% nationally, yet only 63% in Massachusetts. Safety belt use among potentially at-risk groups in Boston is unknown. We sought to assess the prevalence and correlates of belt non-use among emergency department (ED) patients in Boston. METHODS. A cross-sectional survey with systematic sampling was conducted on non-urgent ED patients age ≥18. A closed-ended survey was administered by interview. Safety belt use was defined via two methods: a single-item and a multiple-item measure of safety belt use. Each was scored using a 5-point frequency scale. Responses were used to categorize safety belt use as 'always' or less than 'always'. Outcome for multivariate logistic regression analysis was safety belt use less than 'always'. RESULTS. Of 478 patients approached, 381 (80%) participated. Participants were 48% female, 48% African-American, 40% White, median age 39. Among participants, 250 (66%) had been in a car crash; 234 (61%) had a valid driver's license, and 42 (11%) had been ticketed for belt non-use. Using two different survey measures, a single-item and a multiple-item measure, safety belt use 'always' was 51% and 36% respectively. According to separate regression models, factors associated with belt non-use included male gender, alcohol consumption >5 drinks in one episode, riding with others that drink and drive, ever receiving a citation for belt non-use, believing that safety belt use is 'uncomfortable', and that 'I just forget', while 'It's my usual habit' was protective. CONCLUSION. ED patients at an urban hospital in Boston have considerably lower self-reported safety belt use than state or national estimates. An ED-based intervention to increase safety belt use among this hard-to-reach population warrants consideration."
JAMES A FELDMAN,Strategies to promote language inclusion at 17 CTSA hubs,"The prioritization of English language in clinical research is a barrier to translational science. We explored promising practices to advance the inclusion of people who speak languages other than English in research conducted within and supported by NIH Clinical Translational Science Award (CTSA) hubs. Key informant interviews were conducted with representatives (n = 24) from CTSA hubs (n = 17). Purposive sampling was used to identify CTSA hubs focused on language inclusion. Hubs electing to participate were interviewed via Zoom. Thematic analysis was performed to analyze interview transcripts. We report on strategies employed by hubs to advance linguistic inclusion and influence institutional change that were identified. Strategies ranged from translations, development of culturally relevant materials and consultations to policies and procedural changes and workforce initiatives. An existing framework was adapted to conceptualize hub strategies. Language justice is paramount to bringing more effective treatments to all people more quickly. Inclusion will require institutional transformation and CTSA hubs are well positioned to catalyze change."
KEVIN GALLAGHER,Safeguarding United States’ trade and investment treaties for financial stability,This policy brief discusses new evidence in the economics profession showing that capital controls are important macro-prudential measures that nations should have in their toolkit to prevent and mitigate financial crises. United States trade and investment treaties do not reflect this emerging consensus on capital controls. It is essential to rectify this problem as the United States finalizes its new moves forward on negotiations for a Trans-Pacific Partnership Agreement (TPP) and a bi-lateral investment treaty (BIT) with China.
KEVIN GALLAGHER,2017 China-Latin America economic bulletin,
KEVIN GALLAGHER,Repositioning Chinese development finance in Latin America: opportunities for green finance,"China is one of the largest creditors of Latin American and the Caribbean and has loaned the region more than $125 billion since 2005. However, the composition of China’s financing in the region has been concentrated in commodity related sectors that are currently on the decline. This policy brief notes the extent to which Chinese finance is concentrated in new green economy sectors, and finds that China is not taking full opportunity of the potential in this sector. Moreover, as the global commodity boom has declined, much of China’s investments in the region have been exposed to significant risk, including prominent environmental and social risks. Despite great strides whereby the Chinese government has established a series of guidelines on greening overseas investment over the last few years, China’s development banks and companies are lacking the policies and staffing to identify and fully mitigate such risks. This policy brief reviews the green profile of Chinese development finance in LAC and analyzes environment related risks and policies for Chinese overseas investment. It also outlines the opportunities of green finance in LAC and how blending instruments can mobilize green financial flows that are beneficial for both China and LAC."
KEVIN GALLAGHER,Regulating capital flows in emerging markets: The IMF and the global financial crisis,"In the wake of the financial crisis the International Monetary Fund (IMF) began to publicly express support for what have traditionally been referred to as ‘capital controls’. This paper empirically examines the extent to which the change in IMF discourse on these matters has resulted in significant changes in actual IMF policy advice. By creating and analyzing a database of IMF Article IV reports, we examine whether the financial crisis had an independent impact on IMF support for capital controls. We find that the IMF’s level of support for capital controls has increased as a result of the crisis and as the vulnerabilities associated with capital flows accentuate."
KEVIN GALLAGHER,2016 China-Latin America economic bulletin,
KEVIN GALLAGHER,The IMF's new view on financial globalization: a critical assessment,"In December 2012, the International Monetary Fund (IMF) issued a new “institutional view” on capital account liberalization and the management of capital flows between countries. In this Issues in Brief, Kevin P. Gallagher, one of the co-chairs of the Pardee Center Task Force on Regulating Global Capital Flows for Long-Run Development, offers his assessment of the IMF’s new position. The IMF’s “institutional view” historically tempers the IMF’s advocacy of capital account liberalization and even endorses the regulation of cross-border finance in some circumstances. What is more, the IMF points out that many trade and investment treaties do not provide the appropriate level of policy space to regulate cross-border finance when needed. This is truly landmark, given that the IMF attempted to legally mandate worldwide capital account liberalization in the 1990s. The turnaround is largely a function of the persistence of emerging market and developing country members of the Fund, in addition to some innovative economists on the IMF staff. Unfortunately however, those voices did not fully prevail. The IMF view still urges capital account liberalization as a long-run goal for all nations, only sanctions regulating cross-border finance under limited circumstances, and puts too much of the burden for regulation on emerging market countries, rather than the industrialized world that is often the source of this finance. The brief reiterates the “rules of thumb” put forward by the Pardee Center Task Force in 2011 that should be considered when devising capital account regulations applicable to developing countries."
KEVIN GALLAGHER,"Development that works, March 31, 2011","The theme and the title of the conference—”Development That Works”—stemmed from the conference organizers’ desire to explore, from a groundlevel perspective, what programs, policies, and practices have been shown—or appear to have the potential—to achieve sustained, long-term advances in development in various parts of the world. The intent was not to simply showcase “success stories,” but rather to explore the larger concepts and opportunities that have resulted in development that is meaningful and sustainable over time. The presentations and discussions focused on critical assessments of why and how some programs take hold, and what can be learned from them. From the influence of global economic structures to innovative private sector programs and the need to evaluate development programs at the “granular” level, the expert panelists provided well-informed and often provocative perspectives on what is and isn’t working in development programs today, and what could work better in the future."
KEVIN GALLAGHER,"Trade in the balance: reconciling trade and climate policy: report of the Working Group on Trade, Investment, and Climate Policy","This report outlines the general tensions between the trade and investment regime and climate policy, and outlines a framework toward making trade and investment rules more climate friendly. Members of the working group have contributed short pieces addressing a range of issues related to the intersection of trade and climate policy. The first two are by natural scientists. Anthony Janetos discusses the need to address the effects of international trade on efforts to limit the increase in global annual temperature to no more than 2oC over preindustrial levels. James J. Corbett examines the failure of the Trans Pacific Partnership (TPP) and the Transatlantic Trade and Investment Partnership (TTIP) to adequately address the environmental implications of shipping and maritime transport. The next two pieces are by economists who examine economic aspects of the trade-climate linkage. Irene Monasterolo and Marco Raberto discuss the potential impacts of including fossil fuel subsidies reduction under the TTIP. Frank Ackerman explores the economic costs of efforts to promote convergence of regulatory standards between the United States and the European Union under the TTIP. The following two contributions are by legal scholars. Brooke Güven and Lise Johnson explore the potential for international investment treaties to redirect investment flows to support climate change mitigation and adaptation, particularly with regard to China and India. Matt Porterfield provides an overview of the ways in which both existing and proposed trade and investment agreements could have either “climate positive” or “climate negative” effects on mitigation policies. The final article is by Tao Hu, a former WTO trade and environment expert advisor for China and currently at the World Wildlife Fund, arguing that the definition of environmental goods and services’ under the WTO negotiations needs to be expanded to better incorporate climate change."
KEVIN GALLAGHER,Latin America 2060: consolidation or crisis?,"Latin America has produced vigorous ideas throughout its history, expressed in narratives about its struggles and successes, or its weaknesses and failures. Together, these have shaped a multi-faceted vision of the region and its peoples. Some of its expositors, finding the story to be neither complete nor precise, work toward reformulations, some quite radical. Such generation of knowledge in different fields seems destined to yield a variety of distinct outcomes, at least in part because some of the emerging social and cultural movements are not yet very well structured. This Task Force Report project seeks to harness ideas about the region’s future into a coherent and policy useful discourse. A Workshop and a Task Force meeting was held at Boston University on November 18-19, 2010. A select group of invited experts – a mix of academic scholars and practitioners – were asked to turn their ideas into short ‘Think Pieces’ essays. Each Think Piece focuses on a specific topical issue for the region as a whole, instead of looking only at particular countries. These Think Piece essays are compiled and edited by the Task Force coordinator and published by the Pardee Center as a Task Force Report."
KEVIN GALLAGHER,The future of North American trade policy: lessons from NAFTA,"This Task Force Report written by an international group of trade policy experts calls for significant reforms to address adverse economic, environmental, labor and societal impacts created by the 1994 North American Free Trade Agreement (NAFTA). The report is intended to contribute to the discussion and decisions stemming from ongoing reviews of proposed reforms to NAFTA as well as to help shape future trade agreements. It offers detailed proposals on topics including services, manufacturing, agriculture, investment, intellectual property, labor, environment, and migration. Fifteen years after NAFTA was enacted, there is widespread agreement that the trade treaty among the United States, Canada and Mexico has fallen short of its stated goals. While proponents credit the agreement with stimulating the flow of goods, services, and investment among the North American countries, critics in all three countries argue that this has not brought improvements in the standards of living of most people. Rather than triggering a convergence across the three nations, NAFTA has accentuated the economic and regulatory asymmetries that had existed among the three countries. [TRUNCATED]"
KEVIN GALLAGHER,Fueling growth and financing risk: The benefits and risks of China’s development finance in the global energy sector,This paper is organized in four parts. Part one presents an overview and estimates of China’s emerging development finance architecture. Part two exhibits our estimates of the extent to which China’s development banks are financing energy projects in developing countries in comparative perspective. Part three identifies some of the risks associated with China’s overseas energy investments. Part four summarizes our findings and provides suggestions for further research and policy.
KEVIN GALLAGHER,Exporting national champions: China’s OFDI finance in comparative perspective,"Scholars have compared China’s liberalization, inward FDI attraction, and export promotion policies to those of its ""Asian Miracle"" predecessors to assess China as a 'developmental state.' We build on that literature by drawing a new but similar comparison: the extent to which Chinese development banks have financed the globalization of China’s ‘national champion’ firms. We focus on the role of state finance in promoting China’s outward foreign direct investment (OFDI) in comparative perspective. In order to answer this research question, we created a database of Chinese finance for OFDI and compared our results to the existing literature on previous developmental states. We estimate the total value of China’s OFDI finance from 2002‐2012 at $140 billion. As a percentage of total OFDI, China’s lending is roughly three times 55% higher than Japan, the previous global leader in OFDI finance. Like Japan and South Korea at earlier developmental stages, China’s lending also goes overwhelmingly toward natural resource acquisition, though to a much greater degree. Unlike Japan or Korea, we find that China’s market entry has more to do with developing project expertise and supporting exports than it does with tariff‐hopping or outsourcing industries that are fading on the mainland. We identify two major reasons for China’s high (31%) ratio of OFDI lending to total OFDI. First, China has a greater incentive to give OFDI loans than Japan or Korea ever did because its borrowers are state‐owned so it can more easily dictate how they use the money. Second, China has a greater capacity to give OFDI loans because it has significantly higher savings and foreign exchange reserves than Japan and Korea, both today and especially during equivalent developmental stages."
KEVIN GALLAGHER,Housing price volatility and the capital account in China,"China experienced significant volatility in its housing market from 2005‐2013. Economists analyzing the determinants of volatility in these markets find that the bubble was largely been driven by factors specific to the Chinese economy and Chinese economic policy. In this paper, we examine the extent to which a) short-­‐ term capital flows may have further impacted the prices and volatility in the Chinese housing market and b) whether China’s 2006 Capital Account Regulations (CARs) on foreign purchases of Chinese real estate were effective in reducing the level and volatility of prices in China’s housing markets. According to our OLS baseline model, we find that short-­‐term capital flows from abroad had a modest impact on price increases in the Chinese housing market, but a more significant impact on increasing market volatility. In terms of Chinese 2006 CAR, the measures did not appear to have impact on reducing housing prices, but had a strong impact on reducing volatility in Chinese housing market. The results from a supplementary quantile regression analysis show that hot money magnified the impacts of capital flows on housing prices during upward surges in the housing price. In terms of market volatility, our quantile regressions suggest that the more volatile the housing market became, the larger the impact short-­‐term capital flows had on accentuating such volatility. Furthermore, we find that the 2006 CARs continued to have a strong impact on reducing volatility in the Chinese housing market during the period under study."
KEVIN GALLAGHER,Global financial reform and trade rules: the need for reconciliation,"In the wake of the global financial crisis, many economists and policymakers are advocating the use of regulations to control the cross-border flows of capital. However, such capital account regulations (known as CARs) often are limited or prohibited by commonly-used provisions in trade and investment treaties. This policy brief describes the outcomes of a “compatibility review” between the ability to implement capital account regulations and standard provisions of the global trading system. It argues that changes should be made so that the two systems are more compatible, providing countries – especially developing countries – with the policy space to employ CARs to stabilize their economies and stave off boom-and-bust cycles and still participate in bi-lateral and multi-lateral trade and investment treaties."
KEVIN GALLAGHER,Capital account regulations for stability and development: a new approach,"In the wake of the financial crisis numerous emerging market and developing countries have been deploying what have traditionally been referred to as ‘capital controls’ to curb excessive speculation on their currencies and domestic assets. In response to those efforts, French President Nicolas Sarkozy called on the International Monetary Fund to develop a set of guidelines for the use of capital controls. The goal is for the President to present such guidelines at the G-20 Summit in Cannes this year. The IMF has published a preliminary set of guidelines to that end. This policy brief provides a critical review of those guidelines and offers an alternative protocol for a development friendly-approach to capital account regulation. In this policy brief, the co-conveners of the Pardee Center Task Force on Managing Capital Flows for Long-Run Development argue that capital account regulations (CARs) should be viewed as an essential tool in the macroeconomic policy toolkit. Based on discussions that occurred at the Task Force meeting in September 2011, the authors present an alternative set of guidelines for how and when CARs should be employed, and call for international financial institutions and international trade agreements to ensure that policy space remains available to allow developing countries to employ CARs when deemed necessary for financial stability and economic development."
KEVIN GALLAGHER,Mapping China’s foreign direct investment in the global energy sector,"As China’s current account surplus has grown, and the nation has liberalized its capital account, Chinese overseas foreign direct investment has increased significantly. In 2016, Chinese overseas foreign investment flows were more than $1.3 trillion, and outward investment from Hong Kong was $1.5 trillion, combining to $2.8 trillion in total—up a factor of 10 since 2005 and second only second to the United States in total outbound foreign direct investment. This paper examines the extent to which Chinese overseas foreign investment is significantly different from the United States and other OECD foreign investors in general, and with a specific focus on the electricity energy sector. After creating a unique spatial database that allows us to analyze both greenfield and merger & acquisition flows into the electricity generation sector, we examine the extent to which Chinese foreign investment differs from the other major players in electricity by energy source, level of technology, and level of pollution. We examine the geographical concentration of China’s FDI investment in Asia and Europe. We present initial results of our origin and destination analysis. China’s FDI trends have tremendous policy implications, both in geopolitical terms and for international trade and investment promotion policies."
KEVIN GALLAGHER,Fueling global energy finance: the emergence of China in global energy investment,"Global financial investments in energy production and consumption are significant since all aspects of a country’s economic activity and development require energy resources. In this paper, we assess the investment trends in the global energy sector during, before, and after the financial crisis of 2008 using two data sources: (1) The Dealogic database providing cross-border mergers and acquisitions (M&As); and (2) The “fDi Intelligence fDi Markets” database providing Greenfield (GF) foreign direct investments (FDIs). We highlight the changing role of China and compare its M&A and GF FDI activities to those of the United States, Germany, UK, Japan, and others during this period. We analyze the investments along each segment of the energy supply chain of these countries to highlight the geographical origin and destination, sectoral distribution, and cross-border M&As and GF FDI activities. Our paper shows that while energy accounts for nearly 25% of all GF FDI, it only accounts for 4.82% of total M&A FDI activity in the period 1996–2016. China’s outbound FDI in the energy sector started its ascent around the time of the global recession and accelerated in the post-recession phase. In the energy sector, China’s outbound cross-border M&As are similar to the USA or UK, located mostly in the developed countries of the West, while their outbound GF investments are spread across many countries around the world. Also, China’s outbound energy M&As are concentrated in certain segments of the energy supply chain (extraction, and electricity generation) while their GF FDI covers other segments (electricity generation and power/pipeline transmission) of the energy supply chain."
KEVIN GALLAGHER,The globalization of Chinese energy companies,
KEVIN GALLAGHER,Financial stability and the Trans-Pacific Partnership: lessons from Chile and Malaysia,"There is growing recognition that nations may need to deploy cross-border financial regulations to prevent and mitigate financial crises. Indeed, in December of 2012 the International Monetary Fund (IMF) agreed on a new ‘institutional view’ that notes how the IMF will begin to recommend that nations deploy cross-border financial regulations going forward. However, many nations have become party to global, regional, and bi-lateral trade and investment treaties that may restrict their ability to effectively deploy such regulations. This paper examines the cases of two countries currently in negotiations for a Trans-Pacific Partnership Agreement (TPP): Chile and Malaysia. The paper examines the extent to which each nation has deployed cross-border financial regulations in the past, and the extent to which they have negotiated the policy space for such regulations in its previous trade and investment treaties. Finally, it analyzes the extent to which such measures would be permitted if the TPP’s investment provisions looked like the model bi-lateral investment treaty of the United States. We find that, with some important exceptions, both countries have successfully deployed crossborder financial regulations and have carved out the ability to do so under a sample of representative trading commitments. However, such policy space would be jeopardized if the TPP conformed to the US model rather than arrangements that each country has been able to broker in other arenas."
KEVIN GALLAGHER,2013 China-Latin America economic bulletin,
KEVIN GALLAGHER,Capital openness and income inequality: smooth sailing or troubled waters?,"The 2008 Financial Crisis and subsequent financial turbulence has triggered economists and policymakers to revisit the extent to which capital account liberalization is optimal for all countries at all levels of development. While that literature has largely concluded that capital account liberalization may have detrimental effects on growth and accentuate financial instability in emerging markets, relatively little literature has examined the impacts of capital account liberalization on inequality—a subject that has also been under intense study over the past decade as well. In this paper, we attempt to build upon and bridge these two literatures to examine the extent to which capital account liberalization is associated with income inequality in emerging market and developing countries. We confirm earlier studies that show there is such a relationship between increased capital account openness and increases in inequality, and that capital account regulations are associated with less inequality—at least for emerging market economies. We expand on these findings to learn that there are differential impacts of capital account liberalization on inequality during booms and busts, being financial development a key factor. During normal times we find that there are positive impacts on income inequality, whereas during busts capital account liberalization appears to exacerbate inequality, calling for active policies."
KEVIN GALLAGHER,Infrastructure for sustainable development: the role of national development banks,"Development banks are increasingly becoming relied upon to help finance sustainable infrastructure in the 21st century. Much of the emphasis has been on the role of the existing multi-lateral development banks (MDBs), but lesser attention has been paid to the role of national development banks (NDBs). To help fill this gap, Boston University’s Global Economic Governance initiative (GEGI) and the Brookings Institution’s Global Economy and Development program convened a Task Force on Development Banks and Sustainable Development to examine the extent to which development banks are becoming catalysts for achieving a climate friendly and more socially inclusive world economy."
KEVIN GALLAGHER,China and the future of Latin American industrialization,"The rise of China has created an unprecedented demand for Latin American and Caribbean exports, which has helped boost the region’s growth for almost a decade. But ultimately, such export growth may not be sustainable. Perhaps even worse, Chinese manufactured goods are more competitive than those from Latin America in both home and world markets. These twin trends may jeopardize prospects for long-term growth in the region. Based on research for his most recent book, economist and trade expert Kevin Gallagher discusses how China’s rise to prominence on the world trade scene has affected Latin America and what Latin America might learn from China’s ascendency to improve the long-term outlook for its own economic future."
KEVIN GALLAGHER,21st century trade agreements: implications for long-run development policy,"This paper examines the extent to which the emerging world trading regime leaves nations the “policy space” to deploy effective policy for long-run diversification and development and the extent to which there is a convergence of such policy space under global and regional trade regimes. We examine the economic theory of trade and long-run growth and underscore the fact that traditional theories lose luster in the presence of the need for long-run dynamic comparative advantages and when market failures are rife. We then review a “toolbox” of policies that have been deployed by developed and developing countries past and present to kick-start diversity and development with the hope of achieving longrun growth. Next, we examine the extent to which rules under the World Trade Organization (WTO), trade agreements between the European Union (EU) and developing countries, trade agreements between the United States (US) and developing countries, and those among developing countries (South-South, or S-S, agreements) allow for the use of such policies. We demonstrate that there is a great divergence among trade regimes over this question. While S-S agreements provide ample policy space for industrial development, the WTO and EU agreements largely represent the middle of the spectrum in terms of constraining policy space choices. On the far end, opposite S-S agreements, US agreements place considerably more constraints by binding parties both broadly and deeply in their trade commitments. Rachel Denae Thrasher holds a master’s degree in International Relations and a law degree, both from Boston University, and she is a Research Fellow at the Frederick S. Pardee Center for the Study of the Longer-Range Future. Her recent research has focused on policy issues related to regional trade agreements, multilateral environmental agreements (MEAs) and on global forests governance. Kevin P. Gallagher is an Assistant Professor in the Department of International Relations and Research Fellow at the Frederick S. Pardee Center for the Study of the Longer-Range Future, both at Boston University. He is also a fellow at the Global Development and Environment Institute at Tufts University. He has written extensively on trade and global development. Also see related publication The Future of the WTO, by Kevin Gallagher."
KEVIN GALLAGHER,The future of the WTO,"This policy brief reviews the current debates about the future of the World Trade Organization (WTO) and looks at why current discussions on international trade and development are stalled and also on what the implication of this stalemate might be on the longer-term future of the WTO, and of trade and development in general."
KEVIN GALLAGHER,Capital account liberalization in China: a cautionary tale,"This policy brief synthesizes some of the main themes and policy recommendations discussed at a February 2014 workshop of the Pardee Task Force for Regulating Capital Flows at Boston University, and presented in this report, though the specific recommendations discussed in this brief are our own. The main message is that China would do well to draw lessons from both the economics literature and country experiences with capital account liberalization. Such an approach would guide China to adopt a carefully sequenced and cautionary approach to capital account liberalization."
KEVIN GALLAGHER,Investment provisions in trade and investment treaties: the need for reform,Nations of the world are currently negotiating a variety of significant trade and investment treaties that cover upwards of eighty percent of the world economy. The Trans-Pacific Partnership (TPP) would further integrate a number of Pacific-Rim nations; the Trans-Atlantic Trade and Investment Partnership (TTIP) would be a treaty between the United States and European countries. The United States and others are also negotiating major bilateral investment treaties (BITs) with China and India.
KEVIN GALLAGHER,Greening development finance in the Americas,
KEVIN GALLAGHER,Regulating capital flows in emerging markets: the IMF and the Global Financial Crisis,"In the wake of the financial crisis the International Monetary Fund (IMF) began to publicly express support for what have traditionally been referred to as ‘capital controls’. In addition to public statements, the IMF underwent a systematic re-­‐ evaluation of Fund policy on the matter, and published an official view on the economics of capital flows. In this view the IMF concluded that capital account liberalization is not always the most optimal policy and that there are situations where capital controls—rebranded as ‘capital flow management measures’—are appropriate. This paper empirically examines the extent to which the change in IMF discourse on these matters has resulted in significant changes in IMF policy advice. To answer this question we create a database of IMF Article IV reports and examine whether the financial crisis had an independent impact on IMF support for capital controls. We find that the IMF’s level of support for capital controls has increased as a result of the crisis and as the vulnerabilities associated with capital flows accentuate."
KEVIN GALLAGHER,China in Latin America: lessons for South-South cooperation and sustainable development,
KEVIN GALLAGHER,2015 China-Latin America economic bulletin,
KEVIN GALLAGHER,The Trans-Pacific Partnership and regulating capital flows: recommendations for strengthening proposed safeguards in the leaked TPP investment chapter,The leaked text of the Trans-Pacific Partnership (TPP) Agreement’s investment chapter reveals that negotiators are giving serious consideration to a safeguard intended to allow nations to regulate capital flows. It is critical that the safeguard be drafted in such a way that governments have sufficient policy flexibility to prevent and mitigate financial instability.
KEVIN GALLAGHER,Financialization and the resource curse: the challenge of exchange rate management in Brazil,"A stable and competitive exchange rate is imperative for efforts to diversify an economy in an open economy setting. However, there are an increasing number of exogenous economic and political factors in Brazil and other emerging market economies that accentuate the difficulties of shifting toward a more developmentalist economic policy. Nevertheless, over the past decade or more Brazil has developed a broad array of tools that enable the country to address the exogenously determined factors related to exchange rate instability. These tools have been a modest success at best, but lay the groundwork for what may be the necessary economic policies and political conditions for a more comprehensive program to achieve stability‐led diversified growth in Brazil."
KEVIN GALLAGHER,The deforestation and biodiversity risks of power plant projects in Southeast Asia: a big data spatial analytical framework,"Ecosystem destruction and biodiversity loss are now widespread, extremely rapid, and among the top global anthropogenic risks both in terms of likelihood and overall impact. Thorough impact evaluation of these environmental abuses—essential for conservation and future project planning—requires good analysis of local ecological and environmental data in addition to social and economic impacts. We characterized the deforestation and biodiversity impacts of energy investments in Southeast Asia using multiple geospatial data sources related to forest cover and loss data from 2000 to 2018, other landcover data, and the location, type, and characteristics of energy investments. This study paid particular attention to different types of power plants and financing sources. We identified critical buffer zones and forest structures impacted by these projects in accordance with IUCN criteria and spatial ecology. The paper introduces a novel, replicable analytical framework that goes beyond earlier studies in which all forests are treated as equivalent. It characterizes forests based on spatial morphological structures such as core forest, edges, islands, and bridges, allowing for a more nuanced understanding of deforestation and its impacts on biodiversity. Preliminary findings suggest that projects financed by Chinese development banks pose different risks compared to non-Chinese financing. The study also reveals significant differences in biodiversity impacts based on the type of energy source, be it coal or hydro. The study offers critical insights into the trade-offs between energy development and biodiversity conservation. It provides actionable metrics and strategies for policymakers, conservationists, and development banks to prioritize forest and habitat preservation in Southeast Asia and globally."
KEVIN GALLAGHER,The long-baseline neutrino experiment: exploring fundamental symmetries of the universe,"The preponderance of matter over antimatter in the early Universe, the dynamics of the supernova bursts that produced the heavy elements necessary for life and whether protons eventually decay --- these mysteries at the forefront of particle physics and astrophysics are key to understanding the early evolution of our Universe, its current state and its eventual fate. The Long-Baseline Neutrino Experiment (LBNE) represents an extensively developed plan for a world-class experiment dedicated to addressing these questions. LBNE is conceived around three central components: (1) a new, high-intensity neutrino source generated from a megawatt-class proton accelerator at Fermi National Accelerator Laboratory, (2) a near neutrino detector just downstream of the source, and (3) a massive liquid argon time-projection chamber deployed as a far detector deep underground at the Sanford Underground Research Facility. This facility, located at the site of the former Homestake Mine in Lead, South Dakota, is approximately 1,300 km from the neutrino source at Fermilab -- a distance (baseline) that delivers optimal sensitivity to neutrino charge-parity symmetry violation and mass ordering effects. This ambitious yet cost-effective design incorporates scalability and flexibility and can accommodate a variety of upgrades and contributions. With its exceptional combination of experimental configuration, technical capabilities, and potential for transformative discoveries, LBNE promises to be a vital facility for the field of particle physics worldwide, providing physicists from around the globe with opportunities to collaborate in a twenty to thirty year program of exciting science. In this document we provide a comprehensive overview of LBNE's scientific objectives, its place in the landscape of neutrino physics worldwide, the technologies it will incorporate and the capabilities it will possess."
VERNON SCOTT SOLBERG,Empowering women in finance through developing girls' financial literacy skills in the United States,"This study examines the effectiveness of a financial literacy program, Invest in Girls (IIG), in promoting financial capability among high school girls. Using a quasi-experimental separate-samples pretest-posttest design and a longitudinal qualitative study, the study aims to assess the program efficacy and investigate the perspectives of the female students on its impact on their knowledge, behavior, and future goals and aspirations. The results indicated that the participants had significantly higher confidence for engaging in financial literacy after the program. The findings from the longitudinal study also suggested that that the program was influencing the students in positive ways, increasing their financial capability and leading them to consider wide occupational pathways available in finance. Given the lack of female leaders in the world of finance, the IIG program aims to address gender disparity in financial knowledge and highlight the importance of building financial literacy skills among girls."
VERNON SCOTT SOLBERG,Embedding life design in future readiness efforts to promote collective impact and economically sustainable communities: conceptual frameworks and case example,"This is the first of two sequential papers describing the design and first-year implementation of a collaborative participatory action research effort between Sociedad Latina, a youth serving organization in Boston, Massachusetts, and Boston University. The collaboration aimed to develop and deliver a combined STEM and career development set of lessons for middle school Latinx youth. In the first paper, life design and the U.N. Sustainable Development Goals are described in relation to the rationale and the design of the career development intervention strategy that aims to help middle school youth discover the ways that learning advanced-STEM skills expand future decent work opportunities both within STEM and outside STEM, ultimately leading to an outcome of well-being and sustainable communities. In addition to providing evidence of career development intervention strategies, a qualitative analysis of the collaboration is described. The second paper will discuss two additional frameworks that guided the design and implementation of our work. As an example of translational research, the paper will provide larger national and regional contexts by describing system level career development interventions underway using Bronfenbrenner’s bioecological and person–process–context–time frameworks."
FERNANDO ZAPATERO,Uncertainty and dispersion of opinions,
FERNANDO ZAPATERO,The intensity of keeping up with the Joneses: evidence from neighbour effects in car purchases,"We show that status-driven behaviour is largely determined by how connected a community is. Using a unique dataset on car purchases in Southern California, we show that social influence intensifies in suburban communities in which neighbours are likely to know each other well. The effect of connected communities cannot be fully explained by word of mouth, as it spills over across different makes, and is particularly apparent in higher price segments. We argue that, in connected communities, the signalling of income or wealth through the public display of consumption has a substantial effect on the behaviour of neighbours."
FERNANDO ZAPATERO,Rolling the skewed die: economic foundations of the demand for skewness,
FERNANDO ZAPATERO,Short squeeze uncertainty and skewness,"A growing body of literature argues that skewness-seeking is an optimal investment strategy under plausible conditions. Yet, the difficulty of estimating skewness leads many investors in this category to look for proxies, some of which have also been documented in previous work. In this paper we show that indicators of possible short squeezes provide another such proxy and motivate skewness-seeking investors to buy call options –preferred to the stock for this purpose, as also shown in the literature. The baseline analysis uses a novel measure of possible –yet low probability– short squeezes in the near term, and the main result is corroborated using standard information related to short interest. As in similar instances, investors are willing to pay a premium for the upside potential. This type of investment strategy has attracted much attention recently, but we document that it has been used for decades."
FERNANDO ZAPATERO,Demand for lotteries: the choice between stocks and options,"The literature has demonstrated that stocks with skewness-like haracteristics – lotteryness– are the target of a type of investors willing to pay a premium to achieve exposure to skewness. We show that only stocks without options written on them have the potential to attract skewness-seeking investors; furthermore, out-of-the-money options are the dominant security with lottery characteristics for skewness investors. Finally, we find evidence that suggests that skewness-seeking in out-of-the money options is driven mainly by retail investors, while average negative returns in at-the-money options seem to be linked to the covered-call strategies of mutual funds."
FERNANDO ZAPATERO,"Disagreement, information quality and asset prices","We solve analytically a pure exchange economy with a continuum of agents, disagreement, time-varying information quality, and reference-dependent preferences. The general equilibrium model yields stationary dynamics and explains the equity premium, the stock price volatility, and empirical relations between forecast dispersion and various properties of asset prices. We quantify the implications of the model, which shows that the usual asset pricing channels of disagreement in the literature are not quantitatively important, while information quality in the presence of disagreement generates significant excess volatility and contributes substantially to explaining the equity premium."
FERNANDO ZAPATERO,Does competition between stars increase output? Evidence from financial analyst forecasts,"Top sportsmen often refer to competition against other top sportsmen as a motivation to exert more effort. We examine whether a similar pattern exists among another group of top professionals – star analysts. Our evidence suggests that star analysts concentrate their efforts and generate substantially more accurate earnings forecasts in multi-star stocks, in which they cross paths with other stars. We further show that the higher accuracy in multi-star stocks is not driven by other changes in the information environment of the firms. Our results suggest that competition among stars has substantial effects on the largest firms in financial markets."
FERNANDO ZAPATERO,Carrot and stick: a risk-sharing rationale for fulcrum fees in active fund management,
FERNANDO ZAPATERO,Clinging onto the cliff: a model of financial misconduct,"We propose a novel model of financial misconduct. In line with empirical evidence thereon, our model interprets white-collar crime as gambles with skewed payoffs, as opposed to Becker's analysis of criminal activity that postulates positive expected payoffs associated to crime. In our model, criminal motives arise as optimal responses to a ""tunnel vision"" that engross firm managers, whereby the intense pressure to attain the focused goal triggers strong demand for negatively skewed bets in the form of crime. The key mechanism is consistent with the notion of a ""slippery slope to crime"" that is finding growing support in the literature as well as in practitioner accounts. Comparative static analyses on the model reveal several empirical implications {for example, a ""pecking order of crime"" indicating that serious infringements will only follow the depletion of the more preferred (and possibly prevalent) option of milder incursions of law, e.g., minor violations of financial reporting standards - many of which find empirical support in the literature."
FERNANDO ZAPATERO,Mapping the “Valley of Death”: managing selection and technology advancement in NASA's small business innovation research program,"In this paper, we determine the risk mitigation process inherent in managing a portfolio of technologies diverse in both their readiness for infusion and the nature of the performing organization, focusing on the so-called “valley of death” in which the technology's principles have been proven but prototypes have yet to be developed. Using the Technology Readiness Levels (TRLs) of projects funded by the National Aeronautics and Space Administration Small Business Innovation Research program, a two-stage competitive process, we find that the result of selection of the first round is a tendency toward larger companies. In the second round of funding, technology maturity is a stronger determinant of selection and company headcount is no longer a statistically significant driver. This combination allows the program to manage risk and deliver real technical advancement from even the smallest companies. We find that technologies typically advance from TRL 2, concept formulation, at the program's outset to roughly TRL 5, component validation, at the program's conclusion; these outcomes precede economic benefits from the subsidy. These findings illuminate a mechanism to address risk as well as demonstrating the technical outcomes of a managed early-stage technology program."
FERNANDO ZAPATERO,Asset pricing implications of the mismatch between performance window and benchmark duration,"Short performance windows shrink fund managers' investment horizons well below value investors' long-term investment mandates. We unravel that the frictions tied to the asset management industry are responsible for the recent empirical  ndings show- ing that the risk premium, volatility, and Sharpe ratio on short-term dividend strips are higher than long-term dividend strips |  ndings that are at odds with the lead- ing equilibrium asset pricing models. The interplay between fund managers' relative performance objective and short-term performance window is the primary equilibrium channel, which remains robust to various extensions. Our continuous-time setup admits closed-form expressions and is supported by additional empirical evidence."
FERNANDO ZAPATERO,Uncertainty and dispersion of opinions,
CHRISTOPHER RICKS,"Bostonia: v. 61, no. 1-2, 4",
CHRISTOPHER RICKS,"Bostonia: 1997-1998, no. 1-4",
CHRISTOPHER RICKS,Characterization of the atmosphere of the hot Jupiter HAT-P-32Ab and the M-dwarf companion HAT-P-32B,"We report secondary eclipse photometry of the hot Jupiter HAT-P-32Ab, taken with Hale/WIRC in H and KS bands and with Spitzer/IRAC at 3.6 and 4.5 μm. We carried out adaptive optics imaging of the planet host star HAT-P-32A and its companion HAT-P-32B in the near-IR and the visible. We clearly resolve the two stars from each other and find a separation of 2.′′923 ± 0.′′004 and a position angle 110.◦64 ± 0.◦12. We measure the flux ratios of the binary in g′r′i′z′ and H & KS bands, and determine Teff = 3565 ± 82 K for the companion star, corresponding to an M1.5 dwarf. We use PHOENIX stellar atmosphere models to correct the dilution of the secondary eclipse depths of the hot Jupiter due to the presence of the M1.5 companion. We also improve the secondary eclipse photometry by accounting for the non-classical, flux-dependent nonlinearity of the WIRC IR detector in the H band. We measure planet-to-star flux ratios of 0.090 ± 0.033%, 0.178 ± 0.057%, 0.364 ± 0.016%, and 0.438 ± 0.020% in the H, KS, 3.6 and 4.5 μm bands, respectively. We compare these with planetary atmospheric models, and find they prefer an atmosphere with a temperature inversion and inefficient heat redistribution. However, we also find that the data are equally well-described by a blackbody model for the planet with Tp = 2042 ± 50 K. Finally, we measure a secondary eclipse timing offset of 0.3 ± 1.3 min from the predicted mid-eclipse time, which constrains e = 0.0072+0.0700 −0.0064 when combined with RV data and is more consistent with a circular orbit."
CHRISTOPHER RICKS,"Bostonia: 2002-2003, no.1-4",
BENJAMIN LINAS,Priorities for synthesis research in ecology and environmental science,
BENJAMIN LINAS,HIV pre-exposure prophylaxis and buprenorphine at a drug detoxification center during the opioid epidemic: opportunities and challenges,"Human immunodeficiency virus (HIV) pre-exposure prophylaxis (PrEP) and buprenorphine decrease HIV acquisition. Between November, 2016 - July, 2017, we surveyed persons (N=200) at a drug detoxification center to assess their interest in PrEP and in buprenorphine, and to examine factors associated with such interests. Participants had a mean [SD] age of 39 [10] years. Over the previous 6 months, 58% (117/200) injected drugs and 50% (85/171) had condomless sex. Only 22% (26/117) of persons who injected drugs were aware of PrEP, yet 74% (86/116) and 72% (84/116) were interested in oral or injectable PrEP, respectively. Thirty-eight percent (47/125) of persons not receiving buprenorphine or methadone expressed interest in buprenorphine. After multivariable adjustment, Latinx ethnicity was associated with interest in PrEP (aOR: 3.80; 95% CI, 1.37-10.53), while male gender (aOR: 2.76; 95% CI, 1.21-6.34) was associated with interest in buprenorphine. Opportunities exist to implement PrEP and buprenorphine within drug detoxification centers."
