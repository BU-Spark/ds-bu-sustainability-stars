Author,Title,Abstract
ADIL NAJAM,"Africa 2060: good news from Africa, April 16, 2010","This report provides commentary reflecting upon and information pertaining to the substance of the conference. An introductory overview looks at the major issues discussed at the event, which are placed within the larger literature on Africa’s future. Four short essays prepared by Boston University graduate students provide readers with more specific reflections and highlights of each conference session and the main issues discussed by panelists. The final section presents analyses of key trends and projections related to societal, economic, and governance issues for Africa and a commentary on what this information tells us about the drivers that will determine the continent’s future."
ADIL NAJAM,"Development that works, March 31, 2011","The theme and the title of the conference—”Development That Works”—stemmed from the conference organizers’ desire to explore, from a groundlevel perspective, what programs, policies, and practices have been shown—or appear to have the potential—to achieve sustained, long-term advances in development in various parts of the world. The intent was not to simply showcase “success stories,” but rather to explore the larger concepts and opportunities that have resulted in development that is meaningful and sustainable over time. The presentations and discussions focused on critical assessments of why and how some programs take hold, and what can be learned from them. From the influence of global economic structures to innovative private sector programs and the need to evaluate development programs at the “granular” level, the expert panelists provided well-informed and often provocative perspectives on what is and isn’t working in development programs today, and what could work better in the future."
ADIL NAJAM,Beyond Rio+20: governance for a green economy,"As an intellectual contribution to the preparations for the 2012 United Nations Conference on Sustainable Development (UNCSD, a.k.a. Rio +20), the Boston University Frederick S. Pardee Center for the Study of the Longer-Range Future convened a task force of experts to discuss the role of institutions in the actualization of a green economy in the context of sustainable development. A stellar group of experts from academia, government and civil society convened at the Pardee Center and were asked to outline ideas about what the world has learned about institutions for sustainable development from the past, and what we can propose about the governance challenges and opportunities for the continuous development of a green economy in the future. The Task Force members were encouraged to think big and think bold. They were asked to be innovative in their ideas, and maybe even a little irreverent and provocative. They were charged specifically NOT to come to consensus about specific recommendations, but to present a variety and diversity of views. This report presents their thoughts and ideas."
ADIL NAJAM,Connecting the dots: information visualization and text analysis of the Searchlight Project newsletters,"This report is the product of the Pardee Center’s work on the Searchlight:Visualization and Analysis of Trend Data project sponsored by the Rockefeller Foundation. Part of a larger effort to analyze and disseminate on-the-ground information about important societal trends as reported in a large number of regional newsletters developed in Asia, Africa and the Americas specifically for the Foundation, the Pardee Center developed sophisticated methods to systematically review, categorize, analyze, visualize, and draw conclusions from the information in the newsletters."
ADIL NAJAM,Rio+20: another world summit?,"This issue explores the possibility of holding a Sustainable World Summit in 2012 and three possible options to support such an event . The paper concludes, “The world may not choose any of our three options as the grand purpose for a 2012 World Sustainability Summit. But whatever goal is chosen for a summit needs to be not only grand, but truly meaningful. If it is, then future generations will remember that event like we remember Stockholm and Rio. If it is not, then another summit is not what the world needs right now.”"
ADIL NAJAM,Rio+20: accountability and implementation as key goals,"Over the past two decades, the Global Environmental Governance (GEG) system has grown and evolved, making much progress in incorporating sustainable development as the central goal of environmental governance, and delivering scores of new international institutions, legal instruments, declarations and financial mechanisms. However, the GEG system lacks the crucial components of accountability and implementation as part of its core operating system. The authors argue that the upcoming Rio + 20 meeting provides the perfect opportunity to help bring about these much needed changes to strengthen the GEG and help achieve its ultimate goals. The authors propose a set of four accountability-enabling mechanisms: 1. Improved metrics and reporting mechanisms. 2. Transparency mechanisms. 3. Compliance mechanisms. 4. Capacity building. The authors also propose a set of four enabling institutional arrangements: 1. Compendium of best (and worst) practices. 2. Registry of commitments. 3. Renewed focus for CSD. 4. A global “Aarhus” instrument."
ADIL NAJAM,Global environmental governance: the challenge of accountability,This issue argues that accountability – or lack thereof- is a fundamental challenge in confronting improved global environmental governance (GEG) and that success must be measured not simply by the vitality of the negotiation process but by the robustness of implementation. States as well as institutions must be judged not by their statements of good intentions but by measurable implementation of their commitments and achievement of goals. The authors provide five reasons for GEG’s culture of unaccountability and seven related ideas for GEG reform.
ADIL NAJAM,"Economic development, human development, and the pursuit of happiness, April 1, 2, and 3, 2004","The conference asks the questions, how can we make sure that the benefits of economic growth flow into health, education, welfare, and other aspects of human development; and what is the relationship between human development and economic development? Speakers and participants discuss the role that culture, legal and political institutions, the UN Developmental Goals, the level of decision-making, and ethics, play in development."
ADIL NAJAM,"Looking ahead: forecasting and planning for the longer-range future, April 1, 2, and 3, 2005","The conference allowed for many highly esteemed scholars and professionals from a broad range of fields to come together to discuss strategies designed for the 21st century and beyond. The speakers and discussants covered a broad range of subjects including: long-term policy analysis, forecasting for business and investment, the National Intelligence Council Global Trends 2020 report, Europe’s transition from the Marshal plan to the EU, forecasting global transitions, foreign policy planning, and forecasting for defense."
ADIL NAJAM,Mapping the complexity of higher education in the developing world,"On October 27 and 28, 2009, a workshop of experts on higher education in developing countries was convened by the Boston University Frederick S. Pardee Center for the Study of the Longer-Range Future. The meeting was supported by a grant from the National Academies Keck Futures Initiative with additional support from the Pardee Center and the Office of the Boston University Provost. The meeting brought together experts in economics, public policy, education, development, university management, and quantitative modeling who had rich experiences across the developing world. These experts offered a variety of conceptual tools with which to look at the particular complexities associated with higher education in developing countries. The meeting was convened by the authors of this paper. This policy brief builds upon and reflects on the discussion at this meeting, but is not a meeting report, per se."
DEBORAH A FRANK,"The medical student: v. 15, no. 1-8.",
TIMOTHY S SIMCOE,The economics of reproducibility in preclinical research,"Low reproducibility rates within life science research undermine cumulative knowledge production and contribute to both delays and costs of therapeutic drug development. An analysis of past studies indicates that the cumulative (total) prevalence of irreproducible preclinical research exceeds 50%, resulting in approximately US$28,000,000,000 (US$28B)/year spent on preclinical research that is not reproducible—in the United States alone. We outline a framework for solutions and a plan for long-term improvements in reproducibility rates that will help to accelerate the discovery of life-saving therapies and cures."
DUNCAN M FITZGERALD,Following the sand grains,"When longshore transport systems encounter tidal inlets, complex mechanisms are involved in bypassing sand to downdrift barriers. Here, this process is examined at Plum Island Sound and Essex Inlets, Massachusetts, USA. One major finding from this study is that sand is transferred along the coast—especially at tidal inlets—by parcels, in discrete steps, and over decadal-scale periods. The southerly orientation of the main-ebb channel at Plum Island Sound, coupled with the landward migration of bars from the ebb delta to the central portion of the downdrift Castle Neck barrier island, have formed a beach protuberance. During the constructional phase, sand is sequestered at the protuberance and the spit-end of the barrier becomes sediment starved, leading to shoreline retreat and a broadening of the spit platform at the mouth to Essex Bay (downdrift side of Castle Neck). Storm-induced sand transport from erosion of the spit and across the spit platform is washed into Essex Bay, filling channels and enlarging flood deltas. This study illustrates the pathways and processes of sand transfer along the shoreline of a barrier-island/tidal-inlet system and provides an important example of the processes that future hydrodynamic and sediment-transport modeling should strive to replicate."
DUNCAN M FITZGERALD,"Causal relationships among sea level rise, marsh crab activity, and salt marsh geomorphology",
DAVID ATKINSON,Reproductive inequality in humans and other mammals,"To address claims of human exceptionalism, we determine where humans fit within the greater mammalian distribution of reproductive inequality. We show that humans exhibit lower reproductive skew (i.e., inequality in the number of surviving offspring) among males and smaller sex differences in reproductive skew than most other mammals, while nevertheless falling within the mammalian range. Additionally, female reproductive skew is higher in polygynous human populations than in polygynous nonhumans mammals on average. This patterning of skew can be attributed in part to the prevalence of monogamy in humans compared to the predominance of polygyny in nonhuman mammals, to the limited degree of polygyny in the human societies that practice it, and to the importance of unequally held rival resources to women's fitness. The muted reproductive inequality observed in humans appears to be linked to several unusual characteristics of our species-including high levels of cooperation among males, high dependence on unequally held rival resources, complementarities between maternal and paternal investment, as well as social and legal institutions that enforce monogamous norms."
ADAM LERNER,Loss of AND-34/BCAR3 Expression in Mice Results in Rupture of the Adult Lens,"PURPOSE. AND-34/BCAR3 (Breast Cancer Anti-Estrogen Resistance 3) associates with the focal adhesion adaptor protein, p130CAS/BCAR1. Expression of AND-34 regulates epithelial cell growth pattern, motility, and growth factor dependence. We sought to establish the effects of the loss of AND-34 expression in a mammalian organism. METHODS. AND-34−/− mice were generated by homologous recombination. Histopathology, in situ hybridization, and western blotting were performed on murine tissues. RESULTS. Western analyses confirmed total loss of expression in AND-34−/− splenic lymphocytes. Mice lacking AND-34 are fertile and have normal longevity. While AND-34 is widely expressed in wild type mice, histologic analysis of multiple organs in AND-34−/− mice is unremarkable and analyses of lymphocyte development show no overt changes. A small percentage of AND-34−/− mice show distinctive small white eye lesions resulting from the migration of ruptured cortical lens tissue into the anterior chamber. Following initial vacuolization and liquefaction of the lens cortex first observed at postnatal day three, posterior lens rupture occurs in all AND-34−/− mice, beginning as early as three weeks and seen in all mice at three months. Western blot analysis and in situ hybridization confirmed the presence of AND-34 RNA and protein in lens epithelial cells, particularly at the lens equator. Prior data link AND-34 expression to the activation of Akt signaling. While Akt Ser 473 phosphorylation was readily detectable in AND-34+/+ lens epithelial cells, it was markedly reduced in the AND-34−/− lens epithelium. Basal levels of p130Cas phosphorylation were higher in AND-34+/+ than in AND-34−/− lens epithelium. CONCLUSIONS. These results demonstrate the loss of AND-34 dysregulates focal adhesion complex signaling in lens epithelial cells and suggest that AND-34-mediated signaling is required for maintenance of the structural integrity of the adult ocular lens."
ANATOLI POLKOVNIKOV,Variational Schrieffer-Wolff transformations for quantum many-body dynamics,"Building on recent results for adiabatic gauge potentials, we propose a variational approach for computing the generator of Schrieffer-Wolff transformations. These transformations consist of block diagonalizing a Hamiltonian through a unitary rotation, which leads to effective dynamics in a computationally tractable reduced Hilbert space. The generators of these rotations are computed variationally and thus go beyond standard perturbative methods, with error controlled by the locality of the variational ansatz. The method is demonstrated on two models. First, in the attractive Fermi-Hubbard model with onsite disorder, we find indications of a lack of observable many-body localization in the thermodynamic limit due to the inevitable mixture of different spinon sectors. Second, in the low-energy sector of the XY spin model with a broken U(1) symmetry, we analyze ground-state response functions by combining the variational Schrieffer-Wolf transformation with the truncated spectrum approach."
ANATOLI POLKOVNIKOV,Floquet-engineering counterdiabatic protocols in quantum many-body systems,"Counterdiabatic (CD) driving presents a way of generating adiabatic dynamics at an arbitrary pace, where excitations due to nonadiabaticity are exactly compensated by adding an auxiliary driving term to the Hamiltonian. While this CD term is theoretically known and given by the adiabatic gauge potential, obtaining and implementing this potential in many-body systems is a formidable task, requiring knowledge of the spectral properties of the instantaneous Hamiltonians and control of highly nonlocal multibody interactions. We show how an approximate gauge potential can be systematically built up as a series of nested commutators, remaining well defined in the thermodynamic limit. Furthermore, the resulting CD driving protocols can be realized up to arbitrary order without leaving the available control space using tools from periodically driven (Floquet) systems. This is illustrated on few- and many-body quantum systems, where the resulting Floquet protocols significantly suppress dissipation and provide a drastic increase in fidelity."
ANATOLI POLKOVNIKOV,Quantum hydrodynamics in spin chains with phase space methods,"Connecting short time microscopic dynamics with long time hydrodynamics in strongly correlated quantum systems is one of the outstanding questions. In particular, it is very difficult to determine various hydrodynamic coefficients like the diffusion constant or viscosity starting from a microscopic model: exact quantum simulations are limited to either small system sizes or to short times, which are insufficient to reach asymptotic behavior. In this Letter, we show that these difficulties, at least for a particular model, can be circumvented by using the cluster truncated Wigner approximation (CTWA), which maps quantum Hamiltonian dynamics into classical Hamiltonian dynamics in auxiliary high-dimensional phase space. We apply CTWA to a XXZ next-nearest-neighbor spin 1/2 chain and find behavior consisting of short time spin relaxation which gradually crosses over to emergent diffusive behavior at long times. For a random initial state we show that CTWA correctly reproduces the whole spin spectral function. Necessary in this construction is sampling from properly fluctuating initial conditions: the Dirac mean-field (variational) ansatz, which neglects such fluctuations, leads to incorrect predictions."
ANATOLI POLKOVNIKOV,Gaussian ensemble for quantum integrable dynamics,"We propose a Gaussian ensemble as a description of the long-time dynamics of isolated quantum integrable systems. Our approach extends the Generalized Gibbs Ensemble (GGE) by incorporating fluctuations of integrals of motion. It is asymptotically exact in the classical limit irrespective of the system size and, under appropriate conditions, in the thermodynamic limit irrespective of the value of Planck’s constant. Moreover, it captures quantum corrections near the classical limit, finite size corrections near the thermodynamic limit, and is valid in the presence of non-local interactions. The Gaussian ensemble bridges the gap between classical integrable systems, where a generalized microcanonical ensemble is exact even for few degrees of freedom, and GGE, which requires thermodynamic limit. We illustrate our results with examples of increasing complexity."
ANATOLI POLKOVNIKOV,Geometry and non-adiabatic response in quantum and classical systems,"In these lecture notes, partly based on a course taught at the Karpacz Winter School in March 2014, we explore the close connections between non-adiabatic response of a system with respect to macroscopic parameters and the geometry of quantum and classical states. We center our discussion around adiabatic gauge potentials, which are the generators of unitary basis transformations in quantum systems and generators of special canonical transformations in classical systems. In quantum systems, eigenstate expectation values of these potentials are the Berry connections and the covariance matrix of these gauge potentials is the geometric tensor, whose antisymmetric part defines the Berry curvature and whose symmetric part is the Fubini-Study metric tensor. In classical systems one simply replaces the eigenstate expectation value by an average over the micro-canonical shell. For complicated interacting systems, we show that a variational principle may be used to derive approximate gauge potentials. We then express the non-adiabatic response of the physical observables of the system through these gauge potentials, specifically demonstrating the close connection of the geometric tensor to the notions of Lorentz force and renormalized mass. We highlight applications of this formalism to deriving counter-diabatic (dissipationless) driving protocols in various systems, as well as to finding equations of motion for slow macroscopic parameters coupled to fast microscopic degrees of freedom that go beyond macroscopic Hamiltonian dynamics. Finally, we illustrate these ideas with a number of simple examples and highlight a few more complicated ones drawn from recent literature."
ANATOLI POLKOVNIKOV,Integrable Floquet dynamics,"We discuss several classes of integrable Floquet systems, i.e. systems which do not exhibit chaotic behavior even under a time dependent perturbation. The first class is associated with finite-dimensional Lie groups and infinite-dimensional generalization thereof. The second class is related to the row transfer matrices of the 2D statistical mechanics models. The third class of models, called here ""boost models"", is constructed as a periodic interchange of two Hamiltonians - one is the integrable lattice model Hamiltonian, while the second is the boost operator. The latter for known cases coincides with the entanglement Hamiltonian and is closely related to the corner transfer matrix of the corresponding 2D statistical models. We present several explicit examples. As an interesting application of the boost models we discuss a possibility of generating periodically oscillating states with the period different from that of the driving field. In particular, one can realize an oscillating state by performing a static quench to a boost operator. We term this state a ""Quantum Boost Clock"". All analyzed setups can be readily realized experimentally, for example in cod atoms."
ANATOLI POLKOVNIKOV,Adiabatic eigenstate deformations as a sensitive probe for quantum chaos,"In the past decades, it was recognized that quantum chaos, which is essential for the emergence of statistical mechanics and thermodynamics, manifests itself in the effective description of the eigenstates of chaotic Hamiltonians through random matrix ensembles and the eigenstate thermalization hypothesis. Standard measures of chaos in quantum many-body systems are level statistics and the spectral form factor. In this work, we show that the norm of the adiabatic gauge potential, the generator of adiabatic deformations between eigenstates, serves as a much more sensitive measure of quantum chaos. We are able to detect transitions from integrable to chaotic behavior at perturbation strengths orders of magnitude smaller than those required for standard measures. Using this alternative probe in two generic classes of spin chains, we show that the chaotic threshold decreases exponentially with system size and that one can immediately detect integrability-breaking (chaotic) perturbations by analyzing infinitesimal perturbations even at the integrable point. In some cases, small integrability breaking is shown to lead to anomalously slow relaxation of the system, exponentially long in system size."
ANATOLI POLKOVNIKOV,Minimizing irreversible losses in quantum systems by local counterdiabatic driving,"Counterdiabatic driving protocols have been proposed [Demirplak M, Rice SA (2003) J Chem Phys A 107:9937-9945; Berry M (2009) J Phys A Math Theor 42:365303] as a means to make fast changes in the Hamiltonian without exciting transitions. Such driving in principle allows one to realize arbitrarily fast annealing protocols or implement fast dissipationless driving, circumventing standard adiabatic limitations requiring infinitesimally slow rates. These ideas were tested and used both experimentally and theoretically in small systems, but in larger chaotic systems, it is known that exact counterdiabatic protocols do not exist. In this work, we develop a simple variational approach allowing one to find the best possible counterdiabatic protocols given physical constraints, like locality. These protocols are easy to derive and implement both experimentally and numerically. We show that, using these approximate protocols, one can drastically suppress heating and increase fidelity of quantum annealing protocols in complex many-particle systems. In the fast limit, these protocols provide an effective dual description of adiabatic dynamics, where the coupling constant plays the role of time and the counterdiabatic term plays the role of the Hamiltonian."
ANATOLI POLKOVNIKOV,Quantum eigenstates from classical Gibbs distributions,"We discuss how the language of wave functions (state vectors) and associated non-commuting Hermitian operators naturally emerges from classical mechanics by applying the inverse Wigner-Weyl transform to the phase space probability distribution and observables. In this language, the Schr\""odinger equation follows from the Liouville equation, with ℏ now a free parameter. Classical stationary distributions can be represented as sums over stationary states with discrete (quantized) energies, where these states directly correspond to quantum eigenstates. Interestingly, it is now classical mechanics which allows for apparent negative probabilities to occupy eigenstates, dual to the negative probabilities in Wigner's quasiprobability distribution. These negative probabilities are shown to disappear when allowing sufficient uncertainty in the classical distributions. We show that this correspondence is particularly pronounced for canonical Gibbs ensembles, where classical eigenstates satisfy an integral eigenvalue equation that reduces to the Schr\""odinger equation in a saddle-point approximation controlled by the inverse temperature. We illustrate this correspondence by showing that some paradigmatic examples such as tunneling, band structures, Berry phases, Landau levels, level statistics and quantum eigenstates in chaotic potentials can be reproduced to a surprising precision from a classical Gibbs ensemble, without any reference to quantum mechanics and with all parameters (including ℏ) on the order of unity."
ANATOLI POLKOVNIKOV,Adiabatic landscape and optimal paths in ergodic systems,"Whether one is interested in quantum state preparation or in the design of efficient heat engines, adiabatic (reversible) transformations play a pivotal role in minimizing computational complexity and energy losses. Understanding the structure of these transformations and identifying the systems for which such transformations can be performed efficiently and quickly is therefore of primary importance. In this paper we focus on finding optimal paths in the space of couplings controlling the system's Hamiltonian. More specifically, starting from a local Hamiltonian we analyze directions in the space of couplings along which adiabatic transformations can be accurately generated by local operators, which are both realizable in experiments and easy to simulate numerically. We consider a nonintegrable 1D Ising model parametrized by two independent couplings, corresponding to longitudinal and transverse magnetic fields. We find regions in the space of couplings characterized by a very strong anisotropy of the variational adiabatic gauge potential (AGP), generating the adiabatic transformations, which allows us to define optimal adiabatic paths. We find that these paths generally terminate at singular points characterized by extensive degeneracies in the energy spectrum, splitting the parameter space into adiabatically disconnected regions. The anisotropy follows from singularities in the AGP, and we identify special robust weakly thermalizing and nonabsorbing many-body “dark” states which are annihilated by the singular part of the AGP and show that their existence extends deep into the ergodic regime."
ANATOLI POLKOVNIKOV,Floquet-engineered quantum state manipulation in a noisy qubit,"Adiabatic evolution is a common strategy for manipulating quantum states and has been employed in diverse fields such as quantum simulation, computation and annealing. However, adiabatic evolution is inherently slow and therefore susceptible to decoherence. Existing methods for speeding up adiabatic evolution require complex many-body operators or are difficult to construct for multi-level systems. Using the tools of Floquet engineering, we design a scheme for high-fidelity quantum state manipulation, utilizing only the interactions available in the original Hamiltonian. We apply this approach to a qubit and experimentally demonstrate its performance with the electronic spin of a Nitrogen-vacancy center in diamond. Our Floquet-engineered protocol achieves state preparation fidelity of $0.994 \pm 0.004$, on the same level as the conventional fast-forward protocol, but is more robust to external noise acting on the qubit. Floquet engineering provides a powerful platform for high-fidelity quantum state manipulation in complex and noisy quantum systems."
ANATOLI POLKOVNIKOV,Enabling adiabatic passages between disjoint regions in parameter space through topological transitions,"We explore topological transitions in parameter space in order to enable adiabatic passages between regions adiabatically disconnected within a given parameter manifold. To this end, we study the Hamiltonian of two coupled qubits interacting with external magnetic fields, and make use of the analogy between the Berry curvature and magnetic fields in parameter space, with spectrum degeneracies associated to magnetic charges. Symmetry-breaking terms induce sharp topological transitions on these charge distributions, and we show how one can exploit this effect to bypass crossing degeneracies. We also investigate the curl of the Berry curvature, an interesting but as of yet not fully explored object, which together with its divergence uniquely defines this field. Finally, we suggest a simple method for measuring the Berry curvature, thereby showing how one can experimentally verify our results."
ANATOLI POLKOVNIKOV,Cluster truncated Wigner approximation in strongly interacting systems,"We present a general method by which linear quantum Hamiltonian dynamics with exponentially many degrees of freedom is replaced by approximate classical nonlinear dynamics with the number of degrees of freedom (phase space dimensionality) scaling polynomially in the system size. This method is based on generalization of the truncated Wigner approximation (TWA) to a higher dimensional phase space, where phase space variables are associated with a complete set of quantum operators spanning finite size clusters. The method becomes asymptotically exact with increasing cluster size. The crucial feature of TWA is fluctuating initial conditions, which we approximate by a Gaussian distribution. We show that such fluctuations dramatically increase accuracy of TWA over traditional cluster mean-field approximations. In this way we can treat on equal footing quantum and thermal fluctuations as well as compute entanglement and various equal and non-equal time correlation functions. The main limitation of the method is exponential scaling of the phase space dimensionality with the cluster size, which can be significantly reduced by using the language of Schwinger bosons and can likely be further reduced by truncating the local Hilbert space variables. We demonstrate the power of this method analyzing dynamics in various spin chains with and without disorder and show that we can capture such phenomena as long time hydrodynamic relaxation, many-body localization and the ballistic spread of entanglement."
ANATOLI POLKOVNIKOV,Replica resummation of the Baker-Campbell-Hausdorff series,"We developed a novel perturbative expansion based on the replica trick for the Floquet Hamiltonian governing the dynamics of periodically kicked systems where the kick strength is the small parameter. The expansion is formally equivalent to an infinite resummation of the Baker-Campbell-Hausdorff series in the un-driven (non-perturbed) Hamiltonian, while considering terms up to a finite order in the kick strength. As an application of the replica expansion, we analyze an Ising spin 1/2 chain periodically kicked with magnetic field of strength h, which has both longitudinal and transverse components. We demonstrate that even away from the regime of high frequency driving, the heating rate is nonperturbative in the kick strength bounded from above by a stretched exponential: e^-consth^-1/2. This guarantees existence of a very long pre-thermal regime, where the dynamics is governed by the Floquet Hamiltonian obtained from the replica expansion."
ANATOLI POLKOVNIKOV,Dynamical stability of a many-body Kapitza pendulum,"We consider a many-body generalization of the Kapitza pendulum: the periodically-driven sine–Gordon model. We show that this interacting system is dynamically stable to periodic drives with finite frequency and amplitude. This finding is in contrast to the common belief that periodically-driven unbounded interacting systems should always tend to an absorbing infinite-temperature state. The transition to an unstable absorbing state is described by a change in the sign of the kinetic term in the Floquet Hamiltonian and controlled by the short-wavelength degrees of freedom. We investigate the stability phase diagram through an analytic high-frequency expansion, a self-consistent variational approach, and a numeric semiclassical calculation. Classical and quantum experiments are proposed to verify the validity of our results."
ANATOLI POLKOVNIKOV,Thermalization in small quantum systems,"Chaos and ergodicity are the cornerstones of statistical physics and thermodynamics. While classically even small systems like a particle in a two-dimensional cavity, can exhibit chaotic behavior and thereby relax to a microcanonical ensemble, quantum systems formally can not. Recent theoretical breakthroughs and, in particular, the eigenstate thermalization hypothesis (ETH) however indicate that quantum systems can also thermalize. In fact ETH provided us with a framework connecting microscopic models and macroscopic phenomena, based on the notion of highly entangled quantum states. Such thermalization was beautifully demonstrated experimentally by A. Kaufman et. al. who studied relaxation dynamics of a small lattice system of interacting bosonic particles. By directly measuring the entanglement entropy of subsystems, as well as other observables, they showed that after the initial transient time the system locally relaxes to a thermal ensemble while globally maintaining a zero-entropy pure state."
ANATOLI POLKOVNIKOV,From quantum chaos and eigenstate thermalization to statistical mechanics and thermodynamics,"This review gives a pedagogical introduction to the eigenstate thermalization hypothesis (ETH), its basis, and its implications to statistical mechanics and thermodynamics. In the first part, ETH is introduced as a natural extension of ideas from quantum chaos and random matrix theory (RMT). To this end, we present a brief overview of classical and quantum chaos, as well as RMT and some of its most important predictions. The latter include the statistics of energy levels, eigenstate components, and matrix elements of observables. Building on these, we introduce the ETH and show that it allows one to describe thermalization in isolated chaotic systems without invoking the notion of an external bath. We examine numerical evidence of eigenstate thermalization from studies of many-body lattice systems. We also introduce the concept of a quench as a means of taking isolated systems out of equilibrium, and discuss results of numerical experiments on quantum quenches. The second part of the review explores the implications of quantum chaos and ETH to thermodynamics. Basic thermodynamic relations are derived, including the second law of thermodynamics, the fundamental thermodynamic relation, fluctuation theorems, the fluctuation–dissipation relation, and the Einstein and Onsager relations. In particular, it is shown that quantum chaos allows one to prove these relations for individual Hamiltonian eigenstates and thus extend them to arbitrary stationary statistical ensembles. In some cases, it is possible to extend their regimes of applicability beyond the standard thermal equilibrium domain. We then show how one can use these relations to obtain nontrivial universal energy distributions in continuously driven systems. At the end of the review, we briefly discuss the relaxation dynamics and description after relaxation of integrable quantum systems, for which ETH is violated. We present results from numerical experiments and analytical studies of quantum quenches at integrability. We introduce the concept of the generalized Gibbs ensemble and discuss its connection with ideas of prethermalization in weakly interacting systems."
ANATOLI POLKOVNIKOV,Negative mass corrections in a dissipative stochastic environment,"We study the dynamics of a macroscopic object interacting with a dissipative stochastic environment using an adiabatic perturbation theory. The perturbation theory reproduces known expressions for the friction coefficient and, surprisingly, gives an additional negative mass correction. The effect of the negative mass correction is illustrated by studying a harmonic oscillator interacting with a dissipative stochastic environment. While it is well known that the friction coefficient causes a reduction of the oscillation frequency, we show that the negative mass correction can lead to its enhancement. By studying an exactly solvable model of a magnet coupled to a spin environment evolving under standard non-conserving dynamics we show that the effect is present even beyond the validity of the adiabatic perturbation theory."
ANATOLI POLKOVNIKOV,The second law of thermodynamics under unitary evolution and external operations,"The von Neumann entropy cannot represent the thermodynamic entropy of equilibrium pure states in isolated quantum systems. The diagonal entropy, which is the Shannon entropy in the energy eigenbasis at each instant of time, is a natural generalization of the von Neumann entropy and applicable to equilibrium pure states. We show that the diagonal entropy is consistent with the second law of thermodynamics upon arbitrary external unitary operations. In terms of the diagonal entropy, thermodynamic irreversibility follows from the facts that quantum trajectories under unitary evolution are restricted by the Hamiltonian dynamics and that the external operation is performed without reference to the microscopic state of the system."
ANATOLI POLKOVNIKOV,Quantum versus classical annealing: insights from scaling theory and results for spin glasses on 3-regular graphs,"We discuss an Ising spin glass where each S=1/2 spin is coupled antiferromagnetically to three other spins (3-regular graphs). Inducing quantum fluctuations by a time-dependent transverse field, we use out-of-equilibrium quantum Monte Carlo simulations to study dynamic scaling at the quantum glass transition. Comparing the dynamic exponent and other critical exponents with those of the classical (temperature-driven) transition, we conclude that quantum annealing is less efficient than classical simulated annealing in bringing the system into the glass phase. Quantum computing based on the quantum annealing paradigm is therefore inferior to classical simulated annealing for this class of problems. We also comment on previous simulations where a parameter is changed with the simulation time, which is very different from the true Hamiltonian dynamics simulated here."
ANATOLI POLKOVNIKOV,Universal dynamic scaling in three-dimensional Ising spin glasses,"We use a nonequilibrium Monte Carlo simulation method and dynamical scaling to study the phase transition in three-dimensional Ising spin glasses. The transition point is repeatedly approached at finite velocity v (temperature change versus time) in Monte Carlo simulations starting at a high temperature. This approach has the advantage that the equilibrium limit does not have to be strictly reached for a scaling analysis to yield critical exponents. For the dynamic exponent we obtain z=5.85(9) for bimodal couplings distribution and z=6.00(10) for the Gaussian case. Assuming universal dynamic scaling, we combine the two results and obtain z=5.93±0.07 for generic 3D Ising spin glasses."
ANATOLI POLKOVNIKOV,Slow quenches in a quantum Ising chain: dynamical phase transitions and topology,"We study the slow quenching dynamics (characterized by an inverse rate τ−1) of a one-dimensional transverse Ising chain with nearest neighbor ferromagentic interactions across the quantum critical point (QCP) and analyze the Loschmidt overlap measured using the subsequent temporal evolution of the final wave function (reached at the end of the quenching) with the final time-independent Hamiltonian. Studying the Fisher zeros of the corresponding generalized “partition function,” we probe nonanalyticities manifested in the rate function of the return probability known as dynamical phase transitions (DPTs). In contrast to the sudden quenching case, we show that DPTs survive in the subsequent temporal evolution following the quenching across two critical points of the model for a sufficiently slow rate; furthermore, an interesting “lobe” structure of Fisher zeros emerge. We have also made a connection to topological aspects studying the dynamical topological order parameter [νD(t)] as a function of time (t) measured from the instant when the quenching is complete. Remarkably, the time evolution of νD(t) exhibits drastically different behavior following quenches across a single QCP and two QCPs. In the former case, νD(t) increases stepwise by unity at every DPT (i.e., ΔνD=1). In the latter case, on the other hand, νD(t) essentially oscillates between 0 and 1 (i.e., successive DPTs occur with ΔνD=1 and ΔνD=−1, respectively), except for instants where it shows a sudden jump by a factor of unity when two successive DPTs carry a topological charge of the same sign."
ANATOLI POLKOVNIKOV,Dynamic trapping near a quantum critical point,"The study of dynamics in closed quantum systems has been revitalized by the emergence of experimental systems that are well-isolated from their environment. In this paper, we consider the closed-system dynamics of an archetypal model: spins driven across a second-order quantum critical point, which are traditionally described by the Kibble-Zurek mechanism. Imbuing the driving field with Newtonian dynamics, we find that the full closed system exhibits a robust new phenomenon—dynamic critical trapping—in which the system is self-trapped near the critical point due to efficient absorption of field kinetic energy by heating the quantum spins. We quantify limits in which this phenomenon can be observed and generalize these results by developing a Kibble-Zurek scaling theory that incorporates the dynamic field. Our findings can potentially be interesting in the context of early universe physics, where the role of the driving field is played by the inflaton or a modulus field."
ANATOLI POLKOVNIKOV,Geodesic paths for quantum many-body systems,"We propose a method to obtain optimal protocols for adiabatic ground-state preparation near the adiabatic limit, extending earlier ideas from [D. A. Sivak and G. E. Crooks, Phys. Rev. Lett. 108, 190602 (2012)] to quantum non-dissipative systems. The space of controllable parameters of isolated quantum many-body systems is endowed with a Riemannian quantum metric structure, which can be exploited when such systems are driven adiabatically. Here, we use this metric structure to construct optimal protocols in order to accomplish the task of adiabatic ground-state preparation in a fixed amount of time. Such optimal protocols are shown to be geodesics on the parameter manifold, maximizing the local fidelity. Physically, such protocols minimize the average energy fluctuations along the path. Our findings are illustrated on the Landau-Zener model and the anisotropic XY spin chain. In both cases we show that geodesic protocols drastically improve the final fidelity. Moreover, this happens even if one crosses a critical point, where the adiabatic perturbation theory fails."
ANATOLI POLKOVNIKOV,Counterdiabatic driving in the classical β-Fermi-Pasta-Ulam-Tsingou chain,"Shortcuts to adiabaticity (STAs) have been used to make rapid changes to a system while eliminating or minimizing excitations in the system's state. In quantum systems, these shortcuts allow us to minimize inefficiencies and heating in experiments and quantum computing protocols, but the theory of STAs can also be generalized to classical systems. We focus on one such STA, approximate counterdiabatic (ACD) driving, and numerically compare its performance in two classical systems: a quartic anharmonic oscillator and the β Fermi-Pasta-Ulam-Tsingou lattice. In particular, we modify an existing variational technique to optimize the approximate driving and then develop classical figures of merit to quantify the performance of the driving. We find that relatively simple forms for the ACD driving can dramatically suppress excitations regardless of system size. ACD driving in classical nonlinear oscillators could have many applications, from minimizing heating in bosonic gases to finding optimal local dressing protocols in interacting field theories."
ANATOLI POLKOVNIKOV,Electron-electron relaxation effect on Auger recombination in direct-band semiconductors,"Influence of electron-electron relaxation processes on Auger recombination rate in direct band semiconductors is investigated. Comparison between carrier-carrier and carrier-phonon relaxation processes is provided. It is shown that relaxation processes are essential if the free path length of carriers does not exceed a certain critical value, which exponentially increases with temperature. For illustration of obtained results a typical InGaAsP compound is used."
ANATOLI POLKOVNIKOV,How to study correlation functions in fluctuating Bose liquids using interference experiments,"Interference experiments with independent condensates provide a powerful tool for analyzing correlation functions. Scaling of the average fringe contrast with the system size is determined by the two-point correlation function and can be used to study the Luttinger liquid liquid behavior in one-dimensional systems and to observe the Kosterlitz-Thouless transition in two-dimensional quasicondensates. Additionally, higher moments of the fringe contrast can be used to determine the higher order correlation functions. In this article we focus on interference experiments with one-dimensional Bose liquids and show that methods of conformal field theory can be applied to calculate the full quantum distribution function of the fringe contrast."
ANATOLI POLKOVNIKOV,Universality in the onset of quantum chaos in many-body systems,"We show that the onset of quantum chaos at infinite temperature in two many-body one-dimensional lattice models, the perturbed spin-1/2 XXZ and Anderson models, is characterized by universal behavior. Specifically, we show that the onset of quantum chaos is marked by maxima of the typical fidelity susceptibilities that scale with the square of the inverse average level spacing, saturating their upper bound, and that the strength of the integrability- or localization-breaking perturbation at these maxima decreases with increasing system size. We also show that the spectral function below the “Thouless” energy (in the quantum-chaotic regime) diverges when approaching those maxima. Our results suggest that, in the thermodynamic limit, arbitrarily small integrability- or localization-breaking perturbations result in quantum chaos in the many-body quantum systems studied here."
ANATOLI POLKOVNIKOV,Fermi's golden rule for heating in strongly driven Floquet systems,"We study heating dynamics in isolated quantum many-body systems driven periodically at high frequency and large amplitude. Combining the high-frequency expansion for the Floquet Hamiltonian with Fermi's golden rule (FGR), we develop a master equation termed the Floquet FGR. Unlike the conventional one, the Floquet FGR correctly describes heating dynamics, including the prethermalization regime, even for strong drives, under which the Floquet Hamiltonian is significantly dressed, and nontrivial Floquet engineering is present. The Floquet FGR depends on system size only weakly, enabling us to analyze the thermodynamic limit with small-system calculations. Our results also indicate that, during heating, the system approximately stays in the thermal state for the Floquet Hamiltonian with a gradually rising temperature."
ANATOLI POLKOVNIKOV,Ergodic dynamics and thermalization in an isolated quantum system,"Statistical mechanics is founded on the assumption that all accessible configurations of a system are equally likely. This requires dynamics that explore all states over time, known as ergodic dynamics. In isolated quantum systems, however, the occurrence of ergodic behaviour has remained an outstanding question^1,2,3,4. Here, we demonstrate ergodic dynamics in a small quantum system consisting of only three superconducting qubits. The qubits undergo a sequence of rotations and interactions and we measure the evolution of the density matrix. Maps of the entanglement entropy show that the full system can act like a reservoir for individual qubits, increasing their entropy through entanglement. Surprisingly, these maps bear a strong resemblance to the phase space dynamics in the classical limit; classically, chaotic motion coincides with higher entanglement entropy. We further show that in regions of high entropy the full multi-qubit system undergoes ergodic dynamics. Our work illustrates how controllable quantum systems can investigate fundamental questions in non-equilibrium thermodynamics."
ANATOLI POLKOVNIKOV,Quantum corrections to the dynamics of interacting bosons: beyond the truncated Wigner approximation,"We develop a consistent perturbation theory in quantum fluctuations around the classical evolution of a system of interacting bosons. The zero-order approximation gives the classical Gross-Pitaevskii equations. In the next order we recover the truncated Wigner approximation, where the evolution is still classical but the initial conditions are distributed according to the Wigner transform of the initial density matrix. Further corrections can be characterized as quantum scattering events, which appear in the form of a nonlinear response of the observable to an infinitesimal displacement of the field along its classical evolution. At the end of the paper we give a few numerical examples to test the formalism."
ANATOLI POLKOVNIKOV,Kondo effect in d-wave superconductors,"We present theoretical investigation of a single magnetic impurity in a d-wave superconductor using the large-N limit. It is shown that Kondo screening occurs only in the presence of particle-hole asymmetry. We find analytical expressions for the Kondo temperature, magnetic susceptibility, and scattering matrix near the phase transition. The results are generalized for the density of states vanishing with an arbitrary exponent. Also we briefly study the modifications of the theory for the case of a nonmagnetic impurity which induces a staggered spin configuration on nearby copper atoms."
ANATOLI POLKOVNIKOV,Nonequilibrium Gross-Pitaevskii dynamics of boson lattice models,"Motivated by recent experiments on trapped ultracold bosonic atoms in an optical lattice potential, we consider the nonequilibrium dynamic properties of such bosonic systems for a number of experimentally relevant situations. When the number of bosons per lattice site is large, there is a wide parameter regime where the effective boson interactions are strong, but the ground state remains a superfluid (and not a Mott insulator): we describe the conditions under which the dynamics in this regime can be described by a discrete Gross-Pitaevskii equation. We describe the evolution of the phase coherence after the system is initially prepared in a Mott insulating state, and then allowed to evolve after a sudden change in parameters places it in a regime with a superfluid ground state. We also consider initial conditions with a “π phase” imprint on a superfluid ground state (i.e., the initial phases of neighboring wells differ by π), and discuss the subsequent appearance of the density wave order and “Schrödinger cat,” i.e., macroscopic quantum interference, states."
ANATOLI POLKOVNIKOV,Spin collective mode and quasiparticle contributions to STM spectra of d-wave superconductors with pinning,"We present additional details of the scanning tunneling microscopy spectra predicted by the model of pinning of dynamic spin collective modes in d-wave superconductor proposed by Polkovnikov et al. [Phys. Rev. B 65 (2002) 220509]. Along with modulations at the twice the wavevector of the spin collective mode, the local density of states (LDOS) displays features linked to the spectrum of the Bogoliubov quasiparticles. The former is expected to depend more strongly on an applied magnetic field or the doping concentration. The spin collective mode and the quasiparticles are distinct, co-existing, low energy excitations of the d-wave superconductor (strongly coupled only in some sectors of the Brillouin zone), and should not be viewed as mutually exclusive sources of LDOS modulation."
ANATOLI POLKOVNIKOV,Universal dynamics near quantum critical points,"We give an overview of the scaling of density of quasi-particles and excess energy (heat) for nearly adiabatic dynamics near quantum critical points (QCPs). In particular we discuss both sudden quenches of small amplitude and slow sweeps across the QCP. We show close connection between universal scaling of these quantities with the scaling behavior of the fidelity susceptibility and its generalizations. In particular we argue that the Kibble-Zurek scaling can be easily understood using this concept. We discuss how these scalings can be derived within the adiabatic perturbation theory and how using this approach slow and fast quenches can be treated within the same framework. We also describe modifications of these scalings for finite temperature quenches and emphasize the important role of statistics of low-energy excitations. In the end we mention some connections between adiabatic dynamics near critical points with dynamics associated with space-time singularities in the metrics, which naturally emerges in such areas as cosmology and string theory."
ANATOLI POLKOVNIKOV,Auger recombination in semiconductor quantum wells,"The principal mechanisms of Auger recombination of nonequilibrium carriers in semiconductor heterostructures with quantum wells (QW’s) are investigated. It is shown that there exist three fundamentally different Auger recombination mechanisms of (i) thresholdless, (ii) quasithreshold, and (iii) threshold types. The rate of the thresholdless Auger process depends on temperature only slightly. The threshold energy of the quasithreshold process essentially varies with QW width and is close to zero for narrow QW’s. It is shown that the thresholdless and the quasithreshold Auger processes dominate in narrow QW’s, while the threshold and the quasithreshold processes prevail in wide QW’s. The limiting case of a three-dimensional (3D) Auger process is reached for infinitely wide QW’s. The critical QW width is found at which the quasithreshold and threshold Auger processes merge into a single 3D Auger process. Also studied is phonon-assisted Auger recombination in QW’s. It is shown that for narrow QW’s the act of phonon emission becomes resonant, which in turn increases substantially the coefficient of phonon-assisted Auger recombination."
ANATOLI POLKOVNIKOV,Pinning of dynamic spin-density-wave fluctuations in cuprate superconductors,"We present a theory of the pinning of dynamic spin-density-wave (SDW) fluctuations in a d-wave superconductor by local imperfections that preserve spin-rotation invariance, such as impurities or vortex cores. The pinning leads to static spatial modulations in spin-singlet observables, while the SDW correlations remain dynamic: these are the “Friedel oscillations” of a spin-gap antiferromagnet. We connect the spectrum of these modulations as observed by scanning tunnelling microscopy to the dynamic spin structure factor measured by inelastic neutron scattering."
ANATOLI POLKOVNIKOV,Impurity in a d-wave superconductor: Kondo effect and STM spectra,"We present a theory for recent STM studies of Zn impurities in the superconductor Bi₂Sr₂CaCu₂O₈+𝛿, using insights from NMR experiments which show that there is a net S=1/2 moment on the Cu ions near the Zn. We argue that the Kondo spin dynamics of this moment is the origin of the low bias peak in the differential conductance, rather than a resonance in a purely potential scattering model. The spatial and energy dependence of the STM spectra of our model can also fit the experiments."
ANATOLI POLKOVNIKOV,Adiabatic perturbation theory: from Landau-Zener problem to quenching through a quantum critical point,"Dynamics in closed systems recently attracted a lot of theoretical interest largely following experimental developments in cold atom systems (see e.g., [1] for a review). Several spectacular experiments already explored different aspects of non-equilibrium dynamics in interacting many-particle systems [2–8]. Recent theoretical works in this context focused on various topics, for instance: connection of dynamics and thermodynamics [9–11 M. Rigol, unpublished], dynamics following a sudden quench in low dimensional systems [11–23, L. Mathey and A. Polkovnikov, unpublished; A. Iucci and M.A. Cazalilla,unpublished], adiabatic dynamics near quantum critical points [24–37, D. Chowdhury et al., unpublished; K. Sengupta and D. Sen, unpublished; A.P. Itin and P. Törmä, unpublished; F. Pollmann et al., unpublished] and others. Though there is still very limited understanding of the generic aspects of non-equilibrium quantum dynamics, it has been recognized that such issues as integrability, dimensionality, universality (near critical points) can be explored to understand the non-equilibrium behavior of many-particle systems in various specific situations."
ANATOLI POLKOVNIKOV,Evolution of the macroscopically entangled states in optical lattices,We consider dynamics of boson condensates in finite optical lattices under a slow external perturbation which brings the system to the unstable equilibrium. It is shown that quantum fluctuations drive the condensate into the maximally entangled state. We argue that the truncated Wigner approximation being a natural generalization of the Gross-Pitaevskii classical equations of motion is adequate to correctly describe the time evolution including both collapse and revival of the condensate.
ANATOLI POLKOVNIKOV,Reflection of light and heavy holes from a linear potential barrier,"In this paper, we study reflection of holes in direct-band semiconductors from a linear potential barrier. It is shown that the light-hole–heavy-hole transformation matrix depends only on the dimensionless product of the light-hole longitudinal momentum and the characteristic length determined by the slope of the potential and it does not depend on the ratio of light- and heavy-hole masses, provided this ratio is small. This coefficient is shown to vanish both in the limit of small and large longitudinal momenta, however the phase of a reflected hole is different in these limits. An approximate analytical expression for the light-hole–heavy-hole transformation coefficient is found."
ANATOLI POLKOVNIKOV,Dynamical obstruction to localization in a disordered spin chain,"We analyze a one-dimensional XXZ spin chain in a disordered magnetic field. As the main probes of the system's behavior, we use the sensitivity of eigenstates to adiabatic transformations, as expressed through the fidelity susceptibility, in conjunction with the low-frequency asymptotes of the spectral function. We identify a region of maximal chaos—with exponentially enhanced susceptibility—which separates the many-body localized phase from the diffusive ergodic phase. This regime is characterized by slow transport, and we argue that the presence of such slow dynamics highly constrains any possible localization transition in the thermodynamic limit. Rather, the results are more consistent with absence of the localized phase."
ANATOLI POLKOVNIKOV,Shortcuts to dynamic polarization,"Dynamic polarization protocols aim to hyperpolarize a spin bath by transferring spin polarization from a well-controlled qubit such as a quantum dot or a color defect. Building on techniques from shortcuts to adiabaticity, we design fast and efficient dynamic polarization protocols in central spin models that apply to dipolarly interacting systems. The protocols maximize the transfer of polarization via bright states at a nearby integrable point, exploit the integrability-breaking terms to reduce the statistical weight on dark states that do not transfer polarization, and realize experimentally accessible local counterdiabatic driving through Floquet engineering. A master equation treatment suggests that the protocol duration scales linearly with the number of bath spins with a prefactor that can be orders of magnitude smaller than that of unassisted protocols. This work opens pathways to cool spin baths and extend qubit coherence times for applications in quantum information processing and metrology."
ANATOLI POLKOVNIKOV,Observing dynamical quantum phase transitions through quasilocal string operators,"We analyze signatures of the dynamical quantum phase transitions in physical observables. In particular, we show that both the expectation value and various out of time order correlation functions of the finite length product or string operators develop cusp singularities following quench protocols, which become sharper and sharper as the string length increases. We illustrated our ideas analyzing both integrable and nonintegrable one-dimensional Ising models showing that these transitions are robust both to the details of the model and to the choice of the initial state."
ANATOLI POLKOVNIKOV,Counterdiabatic optimized local driving,"Adiabatic protocols are employed across a variety of quantum technologies, from implementing state preparation and individual operations that are building blocks of larger devices, to higher-level protocols in quantum annealing and adiabatic quantum computation. The problem of speeding up these processes has garnered a large amount of interest, resulting in a menagerie of approaches, most notably quantum optimal control and shortcuts to adiabaticity. The two approaches are complementary: optimal control manipulates control fields to steer the dynamics in the minimum allowed time, while shortcuts to adiabaticity aims to retain the adiabatic condition upon speed-up. We outline a new method that combines the two methodologies and takes advantage of the strengths of each. The new technique improves upon approximate local counterdiabatic driving with the addition of time-dependent control fields. We refer to this new method as counterdiabatic optimized local driving (COLD) and we show that it can result in a substantial improvement when applied to annealing protocols, state preparation schemes, entanglement generation, and population transfer on a lattice. We also demonstrate a new approach to the optimization of control fields that does not require access to the wave function or the computation of system dynamics. COLD can be enhanced with existing advanced optimal control methods and we explore this using the chopped randomized basis method and gradient ascent pulse engineering."
ANATOLI POLKOVNIKOV,Quantum chaos through eigenstate deformations,"In the past decades, it was recognized that quantum chaos, which is essential for the emergence of statistical mechanics and thermodynamics, manifests itself in the effective description of the eigenstates of chaotic Hamiltonians through random matrix ensembles and the eigenstate thermalization hypothesis. Standard measures of chaos in quantum many-body systems are level statistics and the spectral form factor. In this work, we show that the norm of the adiabatic gauge potential, the generator of adiabatic deformations between eigenstates, serves as a much more sensitive measure of quantum chaos. We are able to detect transitions from integrable to chaotic behavior at perturbation strengths orders of magnitude smaller than those required for standard measures. Using this alternative probe in two generic classes of spin chains, we show that the chaotic threshold decreases exponentially with system size and that one can immediately detect integrability-breaking (chaotic) perturbations by analyzing infinitesimal perturbations even at the integrable point. In some cases, small integrability breaking is shown to lead to anomalously slow relaxation of the system, exponentially long in system size."
ANATOLI POLKOVNIKOV,Semiclassical echo dynamics in the Sachdev-Ye-Kitaev model,"The existence of a quantum butterfly effect in the form of exponential sensitivity to small perturbations has been under debate for a long time. Lately, this question gained increased interest due to the proposal to probe chaotic dynamics and scrambling using out-of-time-order correlators. In this work we study echo dynamics in the Sachdev-Ye-Kitaev model under effective time reversal in a semiclassical approach. We demonstrate that small imperfections introduced in the time-reversal procedure result in an exponential divergence from the perfect echo, which allows to identify a Lyapunov exponent λ_L. In particular, we find that λ_L is twice the Lyapunov exponent of the semiclassical equations of motion. This behavior is attributed to the growth of an out-of-time-order double commutator that resembles an out-of-time-order correlator."
ANATOLI POLKOVNIKOV,Swift heat transfer by fast-forward driving in open quantum systems,"Typically, time-dependent thermodynamic protocols need to run asymptotically slowly in order to avoid dissipative losses. By adapting ideas from counterdiabatic driving and Floquet engineering to open systems, we develop fast-forward protocols for swiftly thermalizing a system oscillator locally coupled to an optical phonon bath. These protocols control the system frequency and the system-bath coupling to induce a resonant state exchange between the system and the bath. We apply the fast-forward protocols to realize a fast approximate Otto engine operating at high power near the Carnot efficiency. Our results suggest design principles for swift cooling protocols in coupled many-body systems."
ANATOLI POLKOVNIKOV,Universal energy fluctuations in thermally isolated driven systems,"When an isolated system is brought in contact with a heat bath, its final energy is random and follows the Gibbs distribution—this finding is a cornerstone of statistical physics. The system’s energy can also be changed by performing non-adiabatic work using a cyclic process. Almost nothing is known about the resulting energy distribution in this set-up, which is in particular relevant to recent experimental progress in cold atoms, ion traps, superconducting qubits and other systems. Here we show that when the non-adiabatic process consists of many repeated cyclic processes, the resulting energy distribution is universal and different from the Gibbs ensemble. We predict the existence of two qualitatively different regimes with a continuous second-order-like transition between them. We illustrate our approach by performing explicit calculations for both interacting and non-interacting systems."
ANATOLI POLKOVNIKOV,Microscopic diagonal entropy and its connection to basic thermodynamic relations,"We define a diagonal entropy (d-entropy) for an arbitrary Hamiltonian system as Sd = − 𝛴𝑛 𝘗𝑛𝑛 l𝑛 𝘗𝘯𝘯 with the sum taken over the basis of instantaneous energy states. In equilibrium this entropy coincides with the conventional von Neumann entropy Sn = −Trρ ln ρ. However, in contrast to Sn, the d-entropy is not conserved in time in closed Hamiltonian systems. If the system is initially in stationary state then in accord with the second law of thermodynamics the d-entropy can only increase or stay the same. We also show that the d-entropy can be expressed through the energy distribution function and thus it is measurable, at least in principle. Under very generic assumptions of the locality of the Hamiltonian and non-integrability the d-entropy becomes a unique function of the average energy in large systems and automatically satisfies the fundamental thermodynamic relation. This relation reduces to the first law of thermodynamics for quasi-static processes. The d-entropy is also automatically conserved for adiabatic processes. We illustrate our results with explicit examples and show that Sd behaves consistently with expectations from thermodynamics."
ANATOLI POLKOVNIKOV,Semiclassical approach to dynamics of interacting fermions,"Understanding the behaviour of interacting fermions is of fundamental interest in many fields ranging from condensed matter to high energy physics. Developing numerically efficient and accurate simulation methods is an indispensable part of this. Already in equilibrium, fermions are notoriously hard to handle due to the sign problem. Out of equilibrium, an important outstanding problem is the efficient numerical simulation of the dynamics of these systems. In this work we develop a new semiclassical phase-space approach (a.k.a. the truncated Wigner approximation) for simulating the dynamics of interacting fermions in arbitrary dimensions. As fermions are essentially non-classical objects, a phase-space is constructed out of all fermionic bilinears. Classical phase-space is thus comprised of highly non-local (hidden) variables representing these bilinears, and the cost of the method is that it scales quadratic rather than linear with system size. We demonstrate the strength of the method by comparing the results to the exact quantum dynamics of fermion expansion in the Hubbard model and quantum thermalization in the Sachdev–Ye–Kitaev (SYK) model for small systems, where the semiclassics nearly perfectly reproduces correct results. We furthermore analyse fermion expansion in a larger, intractable by exact methods, 2D Hubbard model, which is directly relevant to recent cold atom experiments."
ANATOLI POLKOVNIKOV,Reinforcement learning in different phases of quantum control,"The ability to prepare a physical system in a desired quantum state is central to many areas of physics such as nuclear magnetic resonance, cold atoms, and quantum computing. Yet, preparing states quickly and with high fidelity remains a formidable challenge. In this work, we implement cutting-edge reinforcement learning (RL) techniques and show that their performance is comparable to optimal control methods in the task of finding short, high-fidelity driving protocol from an initial to a target state in nonintegrable many-body quantum systems of interacting qubits. RL methods learn about the underlying physical system solely through a single scalar reward (the fidelity of the resulting state) calculated from numerical simulations of the physical system. We further show that quantum-state manipulation viewed as an optimization problem exhibits a spin-glass-like phase transition in the space of protocols as a function of the protocol duration. Our RL-aided approach helps identify variational protocols with nearly optimal fidelity, even in the glassy phase, where optimal state manipulation is exponentially hard. This study highlights the potential usefulness of RL for applications in out-of-equilibrium quantum physics."
ANATOLI POLKOVNIKOV,Broken symmetry in a correlated quantum control landscape,"We analyze the physics of optimal protocols to prepare a target state with high fidelity in a symmetrically coupled two-qubit system. By varying the protocol duration, we find a discontinuous phase transition, which is characterized by a spontaneous breaking of a Z2 symmetry in the functional form of the optimal protocol, and occurs below the quantum speed limit. We study in detail this phase and demonstrate that even though high-fidelity protocols come degenerate with respect to their fidelity, they lead to final states of different entanglement entropy shared between the qubits. Consequently, while globally both optimal protocols are equally far away from the target state, one is locally closer than the other. An approximate variational mean-field theory which captures the physics of the different phases is developed."
ANATOLI POLKOVNIKOV,Magnetic field tuning of charge and spin order in the cuprate superconductors,"Recent neutron scattering, nuclear magnetic resonance, and scanning tunneling microscopy experiments have yielded valuable new information on the interplay between charge and spin density wave order and superconductivity in the cuprate superconductors, by using a perpendicular magnetic field to tune the ground state properties. We compare the results of these experiments with the predictions of a theory which assumed that the ordinary superconductor was proximate to a quantum transition to a superconductor with co-existing spin/charge density wave order."
ANATOLI POLKOVNIKOV,Decay of superfluid currents in a moving system of strongly interacting bosons,"We analyze the stability and decay of supercurrents of strongly interacting bosons on optical lattices. At the mean-field level, the system undergoes an irreversible dynamic phase transition, whereby the current decays beyond a critical phase gradient that depends on the interaction strength. At commensurate filling the transition line smoothly interpolates between the classical modulational instability of weakly interacting bosons and the equilibrium Mott transition at zero current. Below the mean-field instability, the current can decay due to quantum and thermal phase slips. We derive asymptotic expressions of the decay rate near the critical current. In a three-dimensional optical lattice this leads to very weak broadening of the transition. In one and two dimensions the broadening leads to significant current decay well below the mean-field critical current. We show that the temperature scale below which quantum phase slips dominate the decay of supercurrents is easily within experimental reach."
ANATOLI POLKOVNIKOV,Persistent dark states in anisotropic central spin models,"Long-lived dark states, in which an experimentally accessible qubit is not in thermal equilibrium with a surrounding spin bath, are pervasive in solid-state systems. We explain the ubiquity of dark states in a large class of inhomogeneous central spin models using the proximity to integrable lines with exact dark eigenstates. At numerically accessible sizes, dark states persist as eigenstates at large deviations from integrability, and the qubit retains memory of its initial polarization at long times. Although the eigenstates of the system are chaotic, exhibiting exponential sensitivity to small perturbations, they do not satisfy the eigenstate thermalization hypothesis. Rather, we predict long relaxation times that increase exponentially with system size. We propose that this intermediate chaotic but non-ergodic regime characterizes mesoscopic quantum dot and diamond defect systems, as we see no numerical tendency towards conventional thermalization with a finite relaxation time."
ANATOLI POLKOVNIKOV,Semiclassical dynamics of a disordered two-dimensional Hubbard model with long-range interactions,
ANATOLI POLKOVNIKOV,Universal high-frequency behavior of periodically driven systems: from dynamical stabilization to Floquet engineering,"We give a general overview of the high-frequency regime in periodically driven systems and identify three distinct classes of driving protocols in which the infinite-frequency Floquet Hamiltonian is not equal to the time-averaged Hamiltonian. These classes cover systems, such as the Kapitza pendulum, the Harper–Hofstadter model of neutral atoms in a magnetic field, the Haldane Floquet Chern insulator and others. In all setups considered, we discuss both the infinite-frequency limit and the leading finite-frequency corrections to the Floquet Hamiltonian. We provide a short overview of Floquet theory focusing on the gauge structure associated with the choice of stroboscopic frame and the differences between stroboscopic and non-stroboscopic dynamics. In the latter case, one has to work with dressed operators representing observables and a dressed density matrix. We also comment on the application of Floquet Theory to systems described by static Hamiltonians with well-separated energy scales and, in particular, discuss parallels between the inverse-frequency expansion and the Schrieffer–Wolff transformation extending the latter to driven systems."
ANATOLI POLKOVNIKOV,Geometric speed limit of accessible many-body state preparation,"We analyze state preparation within a restricted space of local control parameters between adiabatically connected states of control Hamiltonians. We formulate a conjecture that the time integral of energy fluctuations over the protocol duration is bounded from below by the geodesic length set by the quantum geometric tensor. The conjecture implies a geometric lower bound for the quantum speed limit (QSL). We prove the conjecture for arbitrary, sufficiently slow protocols using adiabatic perturbation theory and show that the bound is saturated by geodesic protocols, which keep the energy variance constant along the trajectory. Our conjecture implies that any optimal unit-fidelity protocol, even those that drive the system far from equilibrium, are fundamentally constrained by the quantum geometry of adiabatic evolution. When the control space includes all possible couplings, spanning the full Hilbert space, we recover the well-known Mandelstam-Tamm bound. However, using only accessible local controls to anneal in complex models such as glasses or to target individual excited states in quantum chaotic systems, the geometric bound for the quantum speed limit can be exponentially large in the system size due to a diverging geodesic length. We validate our conjecture both analytically by constructing counter-diabatic and fast-forward protocols for a three-level system, and numerically in nonintegrable spin chains and a nonlocal SYK model."
ANATOLI POLKOVNIKOV,Heating and many-body resonances in a periodically driven two-band system,"We study the dynamics and stability in a strongly interacting resonantly driven two-band model. Using exact numerical simulations, we find a stable regime at large driving frequencies where the time evolution is governed by a local Floquet Hamiltonian that is approximately conserved out to very long times. For slow driving, on the other hand, the system becomes unstable and heats up to infinite temperature. While thermalization is relatively fast in these two regimes (but to different “temperatures”), in the crossover between them we find slow nonthermalizing time evolution: temporal fluctuations become strong and temporal correlations long lived. Microscopically, we trace back the origin of this nonthermalizing time evolution to the properties of rare Floquet many-body resonances, whose proliferation at lower driving frequency removes the approximate energy conservation, and thus produces thermalization to infinite temperature."
ANATOLI POLKOVNIKOV,Asymptotic prethermalization in periodically driven classical spin chains,"We reveal a novel continuous dynamical heating transition between a prethermal and an infinite-temperature phase in a clean, chaotic periodically-driven classical spin chain. The transition time is a steep exponential function of the driving frequency, showing that the exponentially long-lived prethermal plateau, originally observed in quantum Floquet systems, survives the classical limit. Despite the inapplicability of Floquet's theorem to nonlinear systems, we present strong evidence that the physics of the prethermal phase is described well by the inverse-frequency expansion, even though its stability and robustness are related to drive-induced coherence not captured by the expansion. Our results pave the way to transfer the ideas of Floquet engineering to classical many-body systems, and are directly relevant for cold atom experiments in the superfluid regime."
ANATOLI POLKOVNIKOV,Schrieffer-Wolff Transformation for periodically driven systems: strongly correlated systems with artificial gauge fields,"We generalize the Schrieffer-Wolff transformation to periodically driven systems using Floquet theory. The method is applied to the periodically driven, strongly interacting Fermi-Hubbard model, for which we identify two regimes resulting in different effective low-energy Hamiltonians. In the nonresonant regime, we realize an interacting spin model coupled to a static gauge field with a nonzero flux per plaquette. In the resonant regime, where the Hubbard interaction is a multiple of the driving frequency, we derive an effective Hamiltonian featuring doublon association and dissociation processes. The ground state of this Hamiltonian undergoes a phase transition between an ordered phase and a gapless Luttinger liquid phase. One can tune the system between different phases by changing the amplitude of the periodic drive."
ANATOLI POLKOVNIKOV,Adiabatic perturbation theory and geometry of periodically-driven systems,"We give a systematic review of the adiabatic theorem and the leading non-adiabatic corrections in periodically-driven (Floquet) systems. These corrections have a two-fold origin: (i) conventional ones originating from the gradually changing Floquet Hamiltonian and (ii) corrections originating from changing the micro-motion operator. These corrections conspire to give a Hall-type linear response for non-stroboscopic (time-averaged) observables allowing one to measure the Berry curvature and the Chern number related to the Floquet Hamiltonian, thus extending these concepts to periodically-driven many-body systems. The non-zero Floquet Chern number allows one to realize a Thouless energy pump, where one can adiabatically add energy to the system in discrete units of the driving frequency. We discuss the validity of Floquet Adiabatic Perturbation Theory (FAPT) using five different models covering linear and non-linear few and many-particle systems. We argue that in interacting systems, even in the stable high-frequency regimes, FAPT breaks down at ultra slow ramp rates due to avoided crossings of photon resonances, not captured by the inverse-frequency expansion, leading to a counter-intuitive stronger heating at slower ramp rates. Nevertheless, large windows in the ramp rate are shown to exist for which the physics of interacting driven systems is well captured by FAPT."
ANATOLI POLKOVNIKOV,Emergent conservation laws and nonthermal states in the mixed-field Ising model,
ANATOLI POLKOVNIKOV,Semiclassical bounds on the dynamics of two-dimensional interacting disordered fermions,
ANATOLI POLKOVNIKOV,Quantum diffusion in spin chains with phase space methods,
ANATOLI POLKOVNIKOV,Quantum echo dynamics in the Sherrington-Kirkpatrick model,"Understanding the footprints of chaos in quantum-many-body systems has been under debate for a long time. In this work, we study the echo dynamics of the Sherrington-Kirkpatrick (SK) model with transverse field under effective time reversal. We investigate numerically its quantum and semiclassical dynamics. We explore how chaotic many-body quantum physics can lead to exponential divergence of the echo of observables and we show that it is a result of three requirements: i) the collective nature of the observable, ii) a properly chosen initial state and iii) the existence of a well-defined chaotic semi-classical (large-N) limit. Under these conditions, the echo grows exponentially up to the Ehrenfest time, which scales logarithmically with the number of spins N. In this regime, the echo is well described by the semiclassical (truncated Wigner) approximation. We also discuss a short-range version of the SK model, where the Ehrenfest time does not depend on N and the quantum echo shows only polynomial growth. Our findings provide new insights on scrambling and echo dynamics and how to observe it experimentally."
ANATOLI POLKOVNIKOV,Integrability and quench dynamics in the spin-1 central spin XX model,"Central spin models provide an idealized description of interactions between a central degree of freedom and a mesoscopic environment of surrounding spins. We show that the family of models with a spin-1 at the center and XX interactions of arbitrary strength with surrounding spins is integrable. Specifically, we derive an extensive set of conserved quantities and obtain the exact eigenstates using the Bethe ansatz. As in the homogenous limit, the states divide into two exponentially large classes: bright states, in which the spin-1 is entangled with its surroundings, and dark states, in which it is not. On resonance, the bright states further break up into two classes depending on their weight on states with central spin polarization zero. These classes are probed in quench dynamics wherein they prevent the central spin from reaching thermal equilibrium. In the single spin-flip sector we explicitly construct the bright states and show that the central spin exhibits oscillatory dynamics as a consequence of the semilocalization of these eigenstates. We relate the integrability to the closely related class of integrable Richardson-Gaudin models, and conjecture that the spin-s central spin XX model is integrable for any s."
ANATOLI POLKOVNIKOV,Probing chaos in the spherical p-spin glass model,"We study the dynamics of a quantum p-spin glass model starting from initial states defined in microcanonical shells, in a classical regime. We compute different chaos estimators, such as the Lyapunov exponent and the Kolmogorov-Sinai entropy, and find a marked maximum as a function of the energy of the initial state. By studying the relaxation dynamics and the properties of the energy landscape we show that the maximal chaos emerges in correspondence with the fastest spin relaxation and the maximum complexity, thus suggesting a qualitative picture where chaos emerges as the trajectories are scattered over the exponentially many saddles of the underlying landscape. We also observe hints of ergodicity breaking at low energies, indicated by the correlation function and a maximum of the fidelity susceptibility."
ANATOLI POLKOVNIKOV,Late-time critical behavior of local stringlike observables under quantum quenches,
XUE HAN,Automatic cell segmentation by adaptive thresholding (ACSAT) for large-scale calcium imaging datasets,"Advances in calcium imaging have made it possible to record from an increasingly larger number of neurons simultaneously. Neuroscientists can now routinely image hundreds to thousands of individual neurons. An emerging technical challenge that parallels the advancement in imaging a large number of individual neurons is the processing of correspondingly large datasets. One important step is the identification of individual neurons. Traditional methods rely mainly on manual or semimanual inspection, which cannot be scaled for processing large datasets. To address this challenge, we focused on developing an automated segmentation method, which we refer to as automated cell segmentation by adaptive thresholding (ACSAT). ACSAT works with a time-collapsed image and includes an iterative procedure that automatically calculates global and local threshold values during successive iterations based on the distribution of image pixel intensities. Thus, the algorithm is capable of handling variations in morphological details and in fluorescence intensities in different calcium imaging datasets. In this paper, we demonstrate the utility of ACSAT by testing it on 500 simulated datasets, two wide-field hippocampus datasets, a wide-field striatum dataset, a wide-field cell culture dataset, and a two-photon hippocampus dataset. For the simulated datasets with truth, ACSAT achieved >80% recall and precision when the signal-to-noise ratio was no less than ∼24 dB."
XUE HAN,Optogenetic activation of accessory olfactory bulb input to the forebrain differentially modulates investigation of opposite versus same-sex urinary chemosignals and stimulates mating in male mice,"Surgical or genetic disruption of vomeronasal organ (VNO)-accessory olfactory bulb (AOB) function previously eliminated the ability of male mice to processes pheromones that elicit territorial behavior and aggression. By contrast, neither disruption significantly affected mating behaviors, although VNO lesions reduced males' investigation of nonvolatile female pheromones. We explored the contribution of VNO-AOB pheromonal processing to male courtship using optogenetic activation of AOB projections to the forebrain. Protocadherin-Cre male transgenic mice received bilateral AOB infections with channelrhodopsin2 (ChR2) viral vectors, and an optical fiber was implanted above the AOB. In olfactory choice tests, males preferred estrous female urine (EFU) over water; however, this preference was eliminated when diluted (5%) EFU was substituted for 100% EFU. Optogenetic AOB activation concurrent with nasal contact significantly augmented males' investigation compared to 5% EFU alone. Conversely, concurrent optogenetic AOB activation significantly reduced males' nasal investigation of diluted urine from gonadally intact males (5% IMU) compared to 5% IMU alone. These divergent effects of AOB optogenetic activation were lost when males were prevented from making direct nasal contact. Optogenetic AOB stimulation also failed to augment males' nasal investigation of deionized water or of food odors. Finally, during mating tests, optogenetic AOB stimulation delivered for 30 s when the male was in physical contact with an estrous female significantly facilitated the occurrence of penile intromission. Our results suggest that VNO-AOB signaling differentially modifies males' motivation to seek out female vs male urinary pheromones while augmenting males' sexual arousal leading to intromission and improved reproductive performance."
XUE HAN,Optogenetics and deep brain stimulation neurotechnologies,"Brain neural network is composed of densely packed, intricately wired neurons whose activity patterns ultimately give rise to every behavior, thought, or emotion that we experience. Over the past decade, a novel neurotechnique, optogenetics that combines light and genetic methods to control or monitor neural activity patterns, has proven to be revolutionary in understanding the functional role of specific neural circuits. We here briefly describe recent advance in optogenetics and compare optogenetics with deep brain stimulation technology that holds the promise for treating many neurological and psychiatric disorders."
XUE HAN,Video-rate volumetric neuronal imaging using 3D targeted illumination,"Fast volumetric microscopy is required to monitor large-scale neural ensembles with high spatio-temporal resolution. Widefield fluorescence microscopy can image large 2D fields of view at high resolution and speed while remaining simple and costeffective. A focal sweep add-on can further extend the capacity of widefield microscopy by enabling extended-depth-of-field (EDOF) imaging, but suffers from an inability to reject out-of-focus fluorescence background. Here, by using a digital micromirror device to target only in-focus sample features, we perform EDOF imaging with greatly enhanced contrast and signal-to-noise ratio, while reducing the light dosage delivered to the sample. Image quality is further improved by the application of a robust deconvolution algorithm. We demonstrate the advantages of our technique for in vivo calcium imaging in the mouse brain."
XUE HAN,Unique contributions of parvalbumin and cholinergic interneurons in organizing striatal networks during movement,"Striatal pavalbumin (PV) and cholinergic (CHI) interneurons are poised to play major roles in behavior by coordinating the networks of medium spiny cells that relay motor output. However, the small numbers and scattered distribution of these cells has made it difficult to directly assess their contribution to activity in networks of MSNs during behavior. Here, we build upon recent improvements in single cell calcium imaging combined with optogenetics to test the capacity of PVs and CHIs to affect MSN activity and behavior in mice engaged in voluntarily locomotion. We find that PVs and CHIs have unique effects on MSN activity and dissociable roles in supporting movement. PV cells facilitate movement by refining the activation of MSN networks responsible for movement execution. CHIs, in contrast, synchronize activity within MSN networks to signal the end of a movement bout. These results provide new insights into the striatal network activity that supports movement."
XUE HAN,"Striatal cholinergic receptor activation causes a rapid, selective and state-dependent rise in cortico-striatal β activity",
XUE HAN,The eighteenth data release of the Sloan Digital Sky Surveys: targeting and first spectra from SDSS-V,"The eighteenth data release (DR18) of the Sloan Digital Sky Survey (SDSS) is the first one for SDSS-V, the fifth generation of the survey. SDSS-V comprises three primary scientific programs or “Mappers”: the Milky Way Mapper (MWM), the Black Hole Mapper (BHM), and the Local Volume Mapper. This data release contains extensive targeting information for the two multiobject spectroscopy programs (MWM and BHM), including input catalogs and selection functions for their numerous scientific objectives. We describe the production of the targeting databases and their calibration and scientifically focused components. DR18 also includes ∼25,000 new SDSS spectra and supplemental information for X-ray sources identified by eROSITA in its eFEDS field. We present updates to some of the SDSS software pipelines and preview changes anticipated for DR19. We also describe three value-added catalogs (VACs) based on SDSS-IV data that have been published since DR17, and one VAC based on the SDSS-V data in the eFEDS field."
XUE HAN,Striatal cholinergic interneurons generate beta and gamma oscillations in the corticostriatal circuit and produce motor deficits,"Cortico-basal ganglia-thalamic (CBT) neural circuits are critical modulators of cognitive and motor function. When compromised, these circuits contribute to neurological and psychiatric disorders, such as Parkinson's disease (PD). In PD, motor deficits correlate with the emergence of exaggerated beta frequency (15-30 Hz) oscillations throughout the CBT network. However, little is known about how specific cell types within individual CBT brain regions support the generation, propagation, and interaction of oscillatory dynamics throughout the CBT circuit or how specific oscillatory dynamics are related to motor function. Here, we investigated the role of striatal cholinergic interneurons (SChIs) in generating beta and gamma oscillations in cortical-striatal circuits and in influencing movement behavior. We found that selective stimulation of SChIs via optogenetics in normal mice robustly and reversibly amplified beta and gamma oscillations that are supported by distinct mechanisms within striatal-cortical circuits. Whereas beta oscillations are supported robustly in the striatum and all layers of primary motor cortex (M1) through a muscarinic-receptor mediated mechanism, gamma oscillations are largely restricted to the striatum and the deeper layers of M1. Finally, SChI activation led to parkinsonian-like motor deficits in otherwise normal mice. These results highlight the important role of striatal cholinergic interneurons in supporting oscillations in the CBT network that are closely related to movement and parkinsonian motor symptoms."
XUE HAN,Twelve-crystal prototype of Li₂MoO₄ scintillating bolometers for CUPID and CROSS experiments,"An array of twelve 0.28 kg lithium molybdate (LMO) low-temperature bolometers equipped with 16 bolometric Ge light detectors, aiming at optimization of detector structure for CROSS and CUPID double-beta decay experiments, was constructed and tested in a low-background pulse-tube-based cryostat at the Canfranc underground laboratory in Spain. Performance of the scintillating bolometers was studied depending on the size of phonon NTD-Ge sensors glued to both LMO and Ge absorbers, shape of the Ge light detectors (circular vs. square, from two suppliers), in different light collection conditions (with and without reflector, with aluminum coated LMO crystal surface). The scintillating bolometer array was operated over 8 months in the low-background conditions that allowed to probe a very low, μBq/kg, level of the LMO crystals radioactive contamination by ^228Th and ^226Ra."
XUE HAN,Large-scale deep tissue voltage imaging with targeted illumination confocal microscopy,"Voltage imaging with cellular specificity has been made possible by the tremendous advances in genetically encoded voltage indicators (GEVIs). However, the kilohertz rates required for voltage imaging lead to weak signals. Moreover, out-of-focus fluorescence and tissue scattering produce background that both undermines signal-to-noise ratio (SNR) and induces crosstalk between cells, making reliable in vivo imaging in densely labeled tissue highly challenging. We describe a microscope that combines the distinct advantages of targeted illumination and confocal gating, while also maximizing signal detection efficiency. The resulting benefits in SNR and crosstalk reduction are quantified experimentally and theoretically. Our microscope provides a versatile solution for enabling high-fidelity in vivo voltage imaging at large scales and penetration depths, which we demonstrate across a wide range of imaging conditions and different GEVI classes."
DAVID J WAXMAN,Delicate balances in cancer chemotherapy: modeling immune recruitment and emergence of systemic drug resistance,"Metronomic chemotherapy can drastically enhance immunogenic tumor cell death. However, the mechanisms responsible are still incompletely understood. Here, we develop a mathematical model to elucidate the underlying complex interactions between tumor growth, immune system activation, and therapy-mediated immunogenic cell death. Our model is conceptually simple, yet it provides a surprisingly excellent fit to empirical data obtained from a GL261 SCID mouse glioma model treated with cyclophosphamide on a metronomic schedule. The model includes terms representing immune recruitment as well as the emergence of drug resistance during prolonged metronomic treatments. Strikingly, a single fixed set of parameters, adjusted neither for individuals nor for drug schedule, recapitulates experimental data across various drug regimens remarkably well, including treatments administered at intervals ranging from 6 to 12 days. Additionally, the model predicts peak immune activation times, rediscovering experimental data that had not been used in parameter fitting or in model construction. Notably, the validated model suggests that immunostimulatory and immunosuppressive intermediates are responsible for the observed phenomena of resistance and immune cell recruitment, and thus for variation of responses with respect to different schedules of drug administration."
DAVID J WAXMAN,Spatial frequency domain imaging for monitoring immune-mediated chemotherapy treatment response and resistance in a murine breast cancer model,"Spatial Frequency Domain Imaging (SFDI) can provide longitudinal, label-free, and widefield hemodynamic and scattering measurements of murine tumors in vivo. Our previous work has shown that the reduced scattering coefficient (μ's) at 800 nm, as well as the wavelength dependence of scattering, both have prognostic value in tracking apoptosis and proliferation during treatment with anti-cancer therapies. However, there is limited work in validating these optical biomarkers in clinically relevant tumor models that manifest specific treatment resistance mechanisms that mimic the clinical setting. It was recently demonstrated that metronomic dosing of cyclophosphamide induces a strong anti-tumor immune response and tumor volume reduction in the E0771 murine breast cancer model. This immune activation mechanism can be blocked with an IFNAR-1 antibody, leading to treatment resistance. Here we present a longitudinal study utilizing SFDI to monitor this paired responsive-resistant model for up to 30 days of drug treatment. Mice receiving the immune modulatory metronomic cyclophosphamide schedule had a significant increase in tumor optical scattering compared to mice receiving cyclophosphamide in combination with the IFNAR-1 antibody (9% increase vs 10% decrease on day 5 of treatment, p < 0.001). The magnitude of these differences increased throughout the duration of treatment. Additionally, scattering changes on day 4 of treatment could discriminate responsive versus resistant tumors with an accuracy of 78%, while tumor volume had an accuracy of only 52%. These results validate optical scattering as a promising prognostic biomarker that can discriminate between treatment responsive and resistant tumor models."
DAVID J WAXMAN,PC3 prostate tumor-initiating cells with molecular profile FAM65Bhigh/MFI2low/LEF1low increase tumor angiogenesis,"BACKGROUND Cancer stem-like cells are proposed to sustain solid tumors by virtue of their capacity for self-renewal and differentiation to cells that comprise the bulk of the tumor, and have been identified for a variety of cancers based on characteristic clonal morphologies and patterns of marker gene expression. METHODS Single cell cloning and spheroid culture studies were used to identify a population of cancer stem-like cells in the androgen-independent human prostate cancer cell line PC3. RESULTS We demonstrate that, under standard culture conditions, ~10% of PC3 cells form holoclones with cancer stem cell characteristics. These holoclones display high self-renewal capability in spheroid formation assays under low attachment and serum-free culture conditions, retain their holoclone morphology when passaged at high cell density, exhibit moderate drug resistance, and show high tumorigenicity in scid immunodeficient mice. PC3 holoclones readily form spheres, and PC3-derived spheres yield a high percentage of holoclones, further supporting their cancer stem cell-like nature. We identified one gene, FAM65B, whose expression is consistently up regulated in PC3 holoclones compared to paraclones, the major cell morphology in the parental PC3 cell population, and two genes, MFI2 and LEF1, that are consistently down regulated. This molecular profile, FAM65Bhigh/MFI2low/LEF1low, also characterizes spheres generated from parental PC3 cells. The PC3 holoclones did not show significant enriched expression of the putative prostate cancer stem cell markers CD44 and integrin α2β1. PC3 tumors seeded with holoclones showed dramatic down regulation of FAM65B and dramatic up regulation of MFI2 and LEF1, and unexpectedly, a marked increase in tumor vascularity compared to parental PC3 tumors, suggesting a role of cancer stem cells in tumor angiogenesis. CONCLUSIONS These findings support the proposal that PC3 tumors are sustained by a small number of tumor-initiating cells with stem-like characteristics, including strong self-renewal and pro-angiogenic capability and marked by the expression pattern FAM65Bhigh/MFI2low/LEF1low. These markers may serve as targets for therapies designed to eliminate cancer stem cell populations associated with aggressive, androgen-independent prostate tumors such as PC3."
DAVID J WAXMAN,Impact of methoxyacetic acid on mouse Leydig cell gene expression,"BACKGROUND. Methoxyacetic acid (MAA) is the active metabolite of the widely used industrial chemical ethylene glycol monomethyl ether, which is associated with various developmental and reproductive toxicities, including neural toxicity, blood and immune disorders, limb degeneration and testicular toxicity. Testicular toxicity is caused by degeneration of germ cells in association with changes in gene expression in both germ cells and Sertoli cells of the testis. This study investigates the impact of MAA on gene expression in testicular Leydig cells, which play a critical role in germ cell survival and male reproductive function. METHODS. Cultured mouse TM3 Leydig cells were treated with MAA for 3, 8, and 24 h and changes in gene expression were monitored by genome-wide transcriptional profiling. RESULTS. A total of 3,912 MAA-responsive genes were identified. Ingenuity Pathway analysis identified reproductive system disease, inflammatory disease and connective tissue disorder as the top biological functions affected by MAA. The MAA-responsive genes were classified into 1,366 early responders, 1,387 mid-responders, and 1,138 late responders, based on the time required for MAA to elicit a response. Analysis of enriched functional clusters for each subgroup identified 106 MAA early response genes involved in transcription regulation, including 32 genes associated with developmental processes. 60 DNA-binding proteins responded to MAA rapidly but transiently, and may contribute to the downstream effects of MAA seen for many mid and late response genes. Genes within the phosphatidylinositol/phospholipase C/calcium signaling pathway, whose activity is required for potentiation of nuclear receptor signaling by MAA, were also enriched in the set of early MAA response genes. In contrast, many of the genes responding to MAA at later time points encode membrane proteins that contribute to cell adhesion and membrane signaling. CONCLUSIONS. These findings on the progressive changes in gene expression induced by MAA in a cultured Leydig cell model may help elucidate signaling pathways that lead to the testicular pathophysiological responses induced by MAA exposure and may identify useful biomarkers of MAA toxicity."
DAVID J WAXMAN,Adenoviral Delivery of Pan-Caspase Inhibitor P35 Enhances Bystander Killing by P450 Gene-Directed Enzyme Prodrug Therapy Using Cyclophosphamide+,"BACKGROUND. Cytochrome P450-based suicide gene therapy for cancer using prodrugs such as cyclophosphamide (CPA) increases anti-tumor activity, both directly and via a bystander killing mechanism. Bystander cell killing is essential for the clinical success of this treatment strategy, given the difficulty of achieving 100% efficient gene delivery in vivo using current technologies. Previous studies have shown that the pan-caspase inhibitor p35 significantly increases CPA-induced bystander killing by tumor cells that stably express P450 enzyme CYP2B6 (Schwartz et al, (2002) Cancer Res. 62: 6928-37). METHODS. To further develop this approach, we constructed and characterized a replication-defective adenovirus, Adeno-2B6/p35, which expresses p35 in combination with CYP2B6 and its electron transfer partner, P450 reductase. RESULTS. The expression of p35 in Adeno-2B6/p35-infected tumor cells inhibited caspase activation, delaying the death of the CYP2B6 ""factory"" cells that produce active CPA metabolites, and increased bystander tumor cell killing compared to that achieved in the absence of p35. Tumor cells infected with Adeno-2B6/p35 were readily killed by cisplatin and doxorubicin, indicating that p35 expression is not associated with acquisition of general drug resistance. Finally, p35 did not inhibit viral release when the replication-competent adenovirus ONYX-017 was used as a helper virus to facilitate co-replication and spread of Adeno-2B6/p35 and further increase CPA-induced bystander cell killing. CONCLUSIONS. The introduction of p35 into gene therapeutic regimens constitutes an effective approach to increase bystander killing by cytochrome P450 gene therapy. This strategy may also be used to enhance other bystander cytotoxic therapies, including those involving the production of tumor cell toxic protein products."
DAVID J WAXMAN,Plasma growth hormone pulses induce male-biased pulsatile chromatin opening and epigenetic regulation in adult mouse liver,
CHRISTOPHER MARTIN,Clinicopathological evaluation of chronic traumatic encephalopathy in players of American football,"IMPORTANCE: Players of American football may be at increased risk of long-term neurological conditions, particularly chronic traumatic encephalopathy (CTE). OBJECTIVE: To determine the neuropathological and clinical features of deceased football players with CTE. DESIGN, SETTING, AND PARTICIPANTS: Case series of 202 football players whose brains were donated for research. Neuropathological evaluations and retrospective telephone clinical assessments (including head trauma history) with informants were performed blinded. Online questionnaires ascertained athletic and military history. EXPOSURES: Participation in American football at any level of play. MAIN OUTCOMES AND MEASURES: Neuropathological diagnoses of neurodegenerative diseases, including CTE, based on defined diagnostic criteria; CTE neuropathological severity (stages I to IV or dichotomized into mild [stages I and II] and severe [stages III and IV]); informant-reported athletic history and, for players who died in 2014 or later, clinical presentation, including behavior, mood, and cognitive symptoms and dementia. RESULTS: Among 202 deceased former football players (median age at death, 66 years [interquartile range, 47-76 years]), CTE was neuropathologically diagnosed in 177 players (87%; median age at death, 67 years [interquartile range, 52-77 years]; mean years of football participation, 15.1 [SD, 5.2]), including 0 of 2 pre–high school, 3 of 14 high school (21%), 48 of 53 college (91%), 9 of 14 semiprofessional (64%), 7 of 8 Canadian Football League (88%), and 110 of 111 National Football League (99%) players. Neuropathological severity of CTE was distributed across the highest level of play, with all 3 former high school players having mild pathology and the majority of former college (27 [56%]), semiprofessional (5 [56%]), and professional (101 [86%]) players having severe pathology. Among 27 participants with mild CTE pathology, 26 (96%) had behavioral or mood symptoms or both, 23 (85%) had cognitive symptoms, and 9 (33%) had signs of dementia. Among 84 participants with severe CTE pathology, 75 (89%) had behavioral or mood symptoms or both, 80 (95%) had cognitive symptoms, and 71 (85%) had signs of dementia. CONCLUSIONS AND RELEVANCE: In a convenience sample of deceased football players who donated their brains for research, a high proportion had neuropathological evidence of CTE, suggesting that CTE may be related to prior participation in football."
CHRISTOPHER MARTIN,The Framingham Heart Study 100K SNP Genome-Wide Association Study Resource: Overview of 17 Phenotype Working Group Reports,"BACKGROUND: The Framingham Heart Study (FHS), founded in 1948 to examine the epidemiology of cardiovascular disease, is among the most comprehensively characterized multi-generational studies in the world. Many collected phenotypes have substantial genetic contributors; yet most genetic determinants remain to be identified. Using single nucleotide polymorphisms (SNPs) from a 100K genome-wide scan, we examine the associations of common polymorphisms with phenotypic variation in this community-based cohort and provide a full-disclosure, web-based resource of results for future replication studies. METHODS: Adult participants (n = 1345) of the largest 310 pedigrees in the FHS, many biologically related, were genotyped with the 100K Affymetrix GeneChip. These genotypes were used to assess their contribution to 987 phenotypes collected in FHS over 56 years of follow up, including: cardiovascular risk factors and biomarkers; subclinical and clinical cardiovascular disease; cancer and longevity traits; and traits in pulmonary, sleep, neurology, renal, and bone domains. We conducted genome-wide variance components linkage and population-based and family-based association tests. RESULTS: The participants were white of European descent and from the FHS Original and Offspring Cohorts (examination 1 Offspring mean age 32 ± 9 years, 54% women). This overview summarizes the methods, selected findings and limitations of the results presented in the accompanying series of 17 manuscripts. The presented association results are based on 70,897 autosomal SNPs meeting the following criteria: minor allele frequency ≥ 10%, genotype call rate ≥ 80%, Hardy-Weinberg equilibrium p-value ≥ 0.001, and satisfying Mendelian consistency. Linkage analyses are based on 11,200 SNPs and short-tandem repeats. Results of phenotype-genotype linkages and associations for all autosomal SNPs are posted on the NCBI dbGaP website at. CONCLUSION: We have created a full-disclosure resource of results, posted on the dbGaP website, from a genome-wide association study in the FHS. Because we used three analytical approaches to examine the association and linkage of 987 phenotypes with thousands of SNPs, our results must be considered hypothesis-generating and need to be replicated. Results from the FHS 100K project with NCBI web posting provides a resource for investigators to identify high priority findings for replication."
CHRISTOPHER MARTIN,"Genome-Wide Association of Echocardiographic Dimensions, Brachial Artery Endothelial Function and Treadmill Exercise Responses in the Framingham Heart Study","BACKGROUND: Echocardiographic left ventricular (LV) measurements, exercise responses to standardized treadmill test (ETT) and brachial artery (BA) vascular function are heritable traits that are associated with cardiovascular disease risk. We conducted a genome-wide association study (GWAS) in the community-based Framingham Heart Study. METHODS: We estimated multivariable-adjusted residuals for quantitative echocardiography, ETT and BA function traits. Echocardiography residuals were averaged across 4 examinations and included LV mass, diastolic and systolic dimensions, wall thickness, fractional shortening, left atrial and aortic root size. ETT measures (single exam) included systolic blood pressure and heart rate responses during exercise stage 2, and at 3 minutes post-exercise. BA measures (single exam) included vessel diameter, flow-mediated dilation (FMD), and baseline and hyperemic flow responses. Generalized estimating equations (GEE), family-based association tests (FBAT) and variance-components linkage were used to relate multivariable-adjusted trait residuals to 70,987 SNPs (Human 100K GeneChip, Affymetrix) restricted to autosomal SNPs with minor allele frequency ≥0.10, genotype call rate ≥0.80, and Hardy-Weinberg equilibrium p ≥ 0.001. RESULTS: We summarize results from 17 traits in up to 1238 related middle-aged to elderly men and women. Results of all association and linkage analyses are web-posted at . We confirmed modest-to-strong heritabilities (estimates 0.30–0.52) for several Echo, ETT and BA function traits. Overall, p < 10-5 in either GEE or FBAT models were observed for 21 SNPs (nine for echocardiography, eleven for ETT and one for BA function). The top SNPs associated were (GEE results): LV diastolic dimension, rs1379659 (SLIT2, p = 1.17*10-7); LV systolic dimension, rs10504543 (KCNB2, p = 5.18*10-6); LV mass, rs10498091 (p = 5.68*10-6); Left atrial size, rs1935881 (FAM5C, p = 6.56*10-6); exercise heart rate, rs6847149 (NOLA1, p = 2.74*10-6); exercise systolic blood pressure, rs2553268 (WRN, p = 6.3*10-6); BA baseline flow, rs3814219 (OBFC1, 9.48*10-7), and FMD, rs4148686 (CFTR, p = 1.13*10-5). Several SNPs are reasonable biological candidates, with some being related to multiple traits suggesting pleiotropy. The peak LOD score was for LV mass (4.38; chromosome 5); the 1.5 LOD support interval included NRG2. CONCLUSION: In hypothesis-generating GWAS of echocardiography, ETT and BA vascular function in a moderate-sized community-based sample, we identified several SNPs that are candidates for replication attempts and we provide a web-based GWAS resource for the research community."
CHRISTOPHER MARTIN,Framingham Heart Study 100K Project: Genome-Wide Associations for Cardiovascular Disease Outcomes,"BACKGROUND: Cardiovascular disease (CVD) and its most common manifestations – including coronary heart disease (CHD), stroke, heart failure (HF), and atrial fibrillation (AF) – are major causes of morbidity and mortality. In many industrialized countries, cardiovascular disease (CVD) claims more lives each year than any other disease. Heart disease and stroke are the first and third leading causes of death in the United States. Prior investigations have reported several single gene variants associated with CHD, stroke, HF, and AF. We report a community-based genome-wide association study of major CVD outcomes. METHODS: In 1345 Framingham Heart Study participants from the largest 310 pedigrees (54% women, mean age 33 years at entry), we analyzed associations of 70,987 qualifying SNPs (Affymetrix 100K GeneChip) to four major CVD outcomes: major atherosclerotic CVD (n = 142; myocardial infarction, stroke, CHD death), major CHD (n = 118; myocardial infarction, CHD death), AF (n = 151), and HF (n = 73). Participants free of the condition at entry were included in proportional hazards models. We analyzed model-based deviance residuals using generalized estimating equations to test associations between SNP genotypes and traits in additive genetic models restricted to autosomal SNPs with minor allele frequency ≥0.10, genotype call rate ≥0.80, and Hardy-Weinberg equilibrium p-value ≥ 0.001. RESULTS: Six associations yielded p < 10-5. The lowest p-values for each CVD trait were as follows: major CVD, rs499818, p = 6.6 × 10-6; major CHD, rs2549513, p = 9.7 × 10-6; AF, rs958546, p = 4.8 × 10-6; HF: rs740363, p = 8.8 × 10-6. Of note, we found associations of a 13 Kb region on chromosome 9p21 with major CVD (p 1.7 – 1.9 × 10-5) and major CHD (p 2.5 – 3.5 × 10-4) that confirm associations with CHD in two recently reported genome-wide association studies. Also, rs10501920 in CNTN5 was associated with AF (p = 9.4 × 10-6) and HF (p = 1.2 × 10-4). Complete results for these phenotypes can be found at the dbgap website. CONCLUSION: No association attained genome-wide significance, but several intriguing findings emerged. Notably, we replicated associations of chromosome 9p21 with major CVD. Additional studies are needed to validate these results. Finding genetic variants associated with CVD may point to novel disease pathways and identify potential targeted preventive therapies."
CHRISTOPHER MARTIN,"Adiposity, Cardiometabolic Risk, and Vitamin D Status: The Framingham Heart Study","OBJECTIVE: Because vitamin D deficiency is associated with a variety of chronic diseases, understanding the characteristics that promote vitamin D deficiency in otherwise healthy adults could have important clinical implications. Few studies relating vitamin D deficiency to obesity have included direct measures of adiposity. Furthermore, the degree to which vitamin D is associated with metabolic traits after adjusting for adiposity measures is unclear. RESEARCH DESIGN AND METHODS: We investigated the relations of serum 25-hydroxyvitamin D (25[OH]D) concentrations with indexes of cardiometabolic risk in 3,890 nondiabetic individuals; 1,882 had subcutaneous adipose tissue (SAT) and visceral adipose tissue (VAT) volumes measured by multidetector computed tomography (CT). RESULTS: In multivariable-adjusted regression models, 25(OH)D was inversely associated with winter season, waist circumference, and serum insulin (P < 0.005 for all). In models further adjusted for CT measures, 25(OH)D was inversely related to SAT (−1.1 ng/ml per SD increment in SAT, P = 0.016) and VAT (−2.3 ng/ml per SD, P < 0.0001). The association of 25(OH)D with insulin resistance measures became nonsignificant after adjustment for VAT. Higher adiposity volumes were correlated with lower 25(OH)D across different categories of BMI, including in lean individuals (BMI <25 kg/m2). The prevalence of vitamin D deficiency (25[OH]D <20 ng/ml) was threefold higher in those with high SAT and high VAT than in those with low SAT and low VAT (P < 0.0001). CONCLUSIONS: Vitamin D status is strongly associated with variation in subcutaneous and especially visceral adiposity. The mechanisms by which adiposity promotes vitamin D deficiency warrant further study."
CHRISTOPHER MARTIN,Genome-wide association study of electrocardiographic and heart rate variability traits: the Framingham Heart Study,"BACKGROUND: Heritable electrocardiographic (ECG) and heart rate variability (HRV) measures, reflecting pacemaking, conduction, repolarization and autonomic function in the heart have been associated with risks for cardiac arrhythmias. Whereas several rare monogenic conditions with extreme phenotypes have been noted, few common genetic factors contributing to interindividual variability in ECG and HRV measures have been identified. We report the results of a community-based genomewide association study of six ECG and HRV intermediate traits. METHODS: Genotyping using Affymetrix 100K GeneChip was conducted on 1345 related Framingham Heart Study Original and Offspring cohort participants. We analyzed 1175 Original and Offspring participants with ECG data (mean age 52 years, 52% women) and 548 Offspring participants with HRV data (mean age 48 years, 51% women), in relation to 70,987 SNPs with minor allele frequency ≥ 0.10, call rate ≥ 80%, Hardy-Weinberg p-value ≥ 0.001. We used generalized estimating equations to test association of SNP alleles with multivariable-adjusted residuals for QT, RR, and PR intervals, the ratio of low frequency to high frequency power (LF/HFP), total power (TP) and the standard deviation of normal RR intervals (SDNN). RESULTS: Associations at p < 10-3 were found for 117 (QT), 105 (RR), 111 (PR), 102 (LF/HF), 121 (TP), and 102 (SDNN) SNPs. Several common variants in NOS1AP (4 SNPs with p-values < 10-3; lowest p-value, rs6683968, p = 1 × 10-4) were associated with adjusted QT residuals, consistent with our previously reported finding for NOS1AP in an unrelated sample of FHS Offspring and other cohorts. All results are publicly available at NCBI's dbGaP at. CONCLUSION: In the community-based Framingham Heart Study none of the ECG and HRV results individually attained genomewide significance. However, the presence of bona fide QT-associated SNPs among the top 117 results for QT duration supports the importance of efforts to validate top results from the reported scans. Finding genetic variants associated with ECG and HRV quantitative traits may identify novel genes and pathways implicated in arrhythmogenesis and allow for improved recognition of individuals at high risk for arrhythmias in the general population."
CHRISTOPHER MARTIN,Awake mouse imaging: from two-photon microscopy to blood oxygen level-dependent functional magnetic resonance imaging,"BACKGROUND: Functional magnetic resonance imaging (fMRI) in awake behaving mice is well positioned to bridge the detailed cellular-level view of brain activity, which has become available owing to recent advances in microscopic optical imaging and genetics, to the macroscopic scale of human noninvasive observables. However, though microscopic (e.g., two-photon imaging) studies in behaving mice have become a reality in many laboratories, awake mouse fMRI remains a challenge. Owing to variability in behavior among animals, performing all types of measurements within the same subject is highly desirable and can lead to higher scientific rigor. METHODS: We demonstrated blood oxygenation level-dependent fMRI in awake mice implanted with long-term cranial windows that allowed optical access for microscopic imaging modalities and optogenetic stimulation. We started with two-photon imaging of single-vessel diameter changes (n = 1). Next, we implemented intrinsic optical imaging of blood oxygenation and flow combined with laser speckle imaging of blood flow obtaining a mesoscopic picture of the hemodynamic response (n = 16). Then we obtained corresponding blood oxygenation level-dependent fMRI data (n = 5). All measurements could be performed in the same mice in response to identical sensory and optogenetic stimuli. RESULTS: The cranial window did not deteriorate the quality of fMRI and allowed alternation between imaging modalities in each subject. CONCLUSIONS: This report provides a proof of feasibility for multiscale imaging approaches in awake mice. In the future, this protocol could be extended to include complex cognitive behaviors translatable to humans, such as sensory discrimination or attention."
CHRISTOPHER MARTIN,Cell type specificity of neurovascular coupling in cerebral cortex,"Identification of the cellular players and molecular messengers that communicate neuronal activity to the vasculature driving cerebral hemodynamics is important for (1) the basic understanding of cerebrovascular regulation and (2) interpretation of functional Magnetic Resonance Imaging (fMRI) signals. Using a combination of optogenetic stimulation and 2-photon imaging in mice, we demonstrate that selective activation of cortical excitation and inhibition elicits distinct vascular responses and identify the vasoconstrictive mechanism as Neuropeptide Y (NPY) acting on Y1 receptors. The latter implies that task-related negative Blood Oxygenation Level Dependent (BOLD) fMRI signals in the cerebral cortex under normal physiological conditions may be mainly driven by the NPY-positive inhibitory neurons. Further, the NPY-Y1 pathway may offer a potential therapeutic target in cerebrovascular disease."
CHRISTOPHER MARTIN,Eukaryotic translation initiation factor 4AI: a potential novel target in neuroblastoma,"Neuroblastoma (NB) is the most common extracranial pediatric solid tumor. Children suffering from high-risk and/or metastatic NB often show no response to therapy, and new therapeutic approaches are urgently needed. Malignant tumor development has been shown to be driven by the dysregulation of eukaryotic initiation factors (eIFs) at the translation initiation. Especially the activity of the heterotrimeric eIF4F complex is often altered in malignant cells, since it is the direct connection to key oncogenic signaling pathways such as the PI3K/AKT/mTOR-pathway. A large body of literature exists that demonstrates targeting the translational machinery as a promising anti-neoplastic approach. The objective of this study was to determine whether eIF4F complex members are aberrantly expressed in NB and whether targeting parts of the complex may be a therapeutic strategy against NB. We show that eIF4AI is overexpressed in NB patient tissue using immunohistochemistry, immunoblotting, and RT-qPCR. NB cell lines exhibit decreased viability, increased apoptosis rates as well as changes in cell cycle distribution when treated with the synthetic rocaglate CR-1-31-B, which clamps eIF4A and eIF4F onto mRNA, resulting in a translational block. Additionally, this study reveals that CR-1-31-B is effective against NB cell lines at low nanomolar doses (≤20 nM), which have been shown to not affect non-malignant cells in previous studies. Thus, our study provides information of the expression status on eIF4AI in NB and offers initial promising insight into targeting translation initiation as an anti-tumorigenic approach for NB."
CHRISTOPHER MARTIN,Imaging X-ray polarimetry explorer: prelaunch,"Launched on 2021 December 9, the Imaging X-ray Polarimetry Explorer (IXPE) is a NASA Small Explorer Mission in collaboration with the Italian Space Agency (ASI). The mission will open a new window of investigation—imaging x-ray polarimetry. The observatory features three identical telescopes, each consisting of a mirror module assembly with a polarization-sensitive imaging x-ray detector at the focus. A coilable boom, deployed on orbit, provides the necessary 4-m focal length. The observatory utilizes a three-axis-stabilized spacecraft, which provides services such as power, attitude determination and control, commanding, and telemetry to the ground. During its 2-year baseline mission, IXPE will conduct precise polarimetry for samples of multiple categories of x-ray sources, with follow-on observations of selected targets."
CHRISTOPHER MARTIN,Framingham Heart Study 100K Project: Genome-Wide Associations for Blood Pressure and Arterial Stiffness,"BACKGROUND: About one quarter of adults are hypertensive and high blood pressure carries increased risk for heart disease, stroke, kidney disease and death. Increased arterial stiffness is a key factor in the pathogenesis of systolic hypertension and cardiovascular disease. Substantial heritability of blood-pressure (BP) and arterial-stiffness suggests important genetic contributions. METHODS: In Framingham Heart Study families, we analyzed genome-wide SNP (Affymetrix 100K GeneChip) associations with systolic (SBP) and diastolic (DBP) BP at a single examination in 1971–1975 (n = 1260), at a recent examination in 1998–2001 (n = 1233), and long-term averaged SBP and DBP from 1971–2001 (n = 1327, mean age 52 years, 54% women) and with arterial stiffness measured by arterial tonometry (carotid-femoral and carotid-brachial pulse wave velocity, forward and reflected pressure wave amplitude, and mean arterial pressure; 1998–2001, n = 644). In primary analyses we used generalized estimating equations in models for an additive genetic effect to test associations between SNPs and phenotypes of interest using multivariable-adjusted residuals. A total of 70,987 autosomal SNPs with minor allele frequency ≥ 0.10, genotype call rate ≥ 0.80, and Hardy-Weinberg equilibrium p ≥ 0.001 were analyzed. We also tested for association of 69 SNPs in six renin-angiotensin-aldosterone pathway genes with BP and arterial stiffness phenotypes as part of a candidate gene search. RESULTS: In the primary analyses, none of the associations attained genome-wide significance. For the six BP phenotypes, seven SNPs yielded p values < 10-5. The lowest p-values for SBP and DBP respectively were rs10493340 (p = 1.7 × 10-6) and rs1963982 (p = 3.3 × 10-6). For the five tonometry phenotypes, five SNPs had p values < 10-5; lowest p-values were for reflected wave (rs6063312, p = 2.1 × 10-6) and carotid-brachial pulse wave velocity (rs770189, p = 2.5 × 10-6) in MEF2C, a regulator of cardiac morphogenesis. We found only weak association of SNPs in the renin-angiotensin-aldosterone pathway with BP or arterial stiffness. CONCLUSION: These results of genome-wide association testing for blood pressure and arterial stiffness phenotypes in an unselected community-based sample of adults may aid in the identification of the genetic basis of hypertension and arterial disease, help identify high risk individuals, and guide novel therapies for hypertension. Additional studies are needed to replicate any associations identified in these analyses."
CHRISTOPHER MARTIN,Phenotype-Genotype Association Grid: A Convenient Method for Summarizing Multiple Association Analyses,"BACKGROUND: High-throughput genotyping generates vast amounts of data for analysis; results can be difficult to summarize succinctly. A single project may involve genotyping many genes with multiple variants per gene and analyzing each variant in relation to numerous phenotypes, using several genetic models and population subgroups. Hundreds of statistical tests may be performed for a single SNP, thereby complicating interpretation of results and inhibiting identification of patterns of association. RESULTS: To facilitate visual display and summary of large numbers of association tests of genetic loci with multiple phenotypes, we developed a Phenotype-Genotype Association (PGA) grid display. A database-backed web server was used to create PGA grids from phenotypic and genotypic data (sample sizes, means and standard errors, P-value for association). HTML pages were generated using Tcl scripts on an AOLserver platform, using an Oracle database, and the ArsDigita Community System web toolkit. The grids are interactive and permit display of summary data for individual cells by a mouse click (i.e. least squares means for a given SNP and phenotype, specified genetic model and study sample). PGA grids can be used to visually summarize results of individual SNP associations, gene-environment associations, or haplotype associations. CONCLUSION: The PGA grid, which permits interactive exploration of large numbers of association test results, can serve as an easily adapted common and useful display format for large-scale genetic studies. Doing so would reduce the problem of publication bias, and would simplify the task of summarizing large-scale association studies."
CHRISTOPHER MARTIN,Transmission of Staphylococcus aureus from humans to green monkeys in The Gambia as revealed by whole-genome sequencing,"Staphylococcus aureus is an important pathogen of humans and animals. We genome sequenced 90 S. aureus isolates from The Gambia: 46 isolates from invasive disease in humans, 13 human carriage isolates, and 31 monkey carriage isolates. We inferred multiple anthroponotic transmissions of S. aureus from humans to green monkeys (Chlorocebus sabaeus) in The Gambia over different time scales. We report a novel monkey-associated clade of S. aureus that emerged from a human-to-monkey switch estimated to have occurred 2,700 years ago. Adaptation of this lineage to the monkey host is accompanied by the loss of phage-carrying genes that are known to play an important role in human colonization. We also report recent anthroponotic transmission of the well-characterized human lineages sequence type 6 (ST6) and ST15 to monkeys, probably because of steadily increasing encroachment of humans into the monkeys' habitat. Although we have found no evidence of transmission of S. aureus from monkeys to humans, as the two species come into ever-closer contact, there might be an increased risk of additional interspecies exchanges of potential pathogens. IMPORTANCE: The population structures of Staphylococcus aureus in humans and monkeys in sub-Saharan Africa have been previously described using multilocus sequence typing (MLST). However, these data lack the power to accurately infer details regarding the origin and maintenance of new adaptive lineages. Here, we describe the use of whole-genome sequencing to detect transmission of S. aureus between humans and nonhuman primates and to document the genetic changes accompanying host adaptation. We note that human-to-monkey switches tend to be more common than the reverse and that a novel monkey-associated clade is likely to have emerged from such a switch approximately 2,700 years ago. Moreover, analysis of the accessory genome provides important clues as to the genetic changes underpinning host adaptation and, in particular, shows that human-to-monkey switches tend to be associated with the loss of genes known to confer adaptation to the human host."
CHRISTOPHER MARTIN,Optical calibration of the SNO+ detector in the water phase with deployed sources,"SNO+ is a large-scale liquid scintillator experiment with the primary goal of searching for neutrinoless double beta decay, and is located approximately 2 km underground in SNOLAB, Sudbury, Canada. The detector acquired data for two years as a pure water Cherenkov detector, starting in May 2017. During this period, the optical properties of the detector were measured in situ using a deployed light diffusing sphere, with the goal of improving the detector model and the energy response systematic uncertainties. The measured parameters included the water attenuation coefficients, effective attenuation coefficients for the acrylic vessel, and the angular response of the photomultiplier tubes and their surrounding light concentrators, all across different wavelengths. The calibrated detector model was validated using a deployed tagged gamma source, which showed a 0.6% variation in energy scale across the primary target volume."
CHRISTOPHER MARTIN,Cre/lox-assisted non-invasive in vivo tracking of specific cell populations by positron emission tomography,"Many pathophysiological processes are associated with proliferation, migration or death of distinct cell populations. Monitoring specific cell types and their progeny in a non-invasive, longitudinal and quantitative manner is still challenging. Here we show a novel cell-tracking system that combines Cre/lox-assisted cell fate mapping with a thymidine kinase (sr39tk) reporter gene for cell detection by positron emission tomography (PET). We generate Rosa26-mT/sr39tk PET reporter mice and induce sr39tk expression in platelets, T lymphocytes or cardiomyocytes. As proof of concept, we demonstrate that our mouse model permits longitudinal PET imaging and quantification of T-cell homing during inflammation and cardiomyocyte viability after myocardial infarction. Moreover, Rosa26-mT/sr39tk mice are useful for whole-body characterization of transgenic Cre mice and to detect previously unknown Cre activity. We anticipate that the Cre-switchable PET reporter mice will be broadly applicable for non-invasive long-term tracking of selected cell populations in vivo.Non-invasive cell tracking is a powerful method to visualize cells in vivo under physiological and pathophysiological conditions. Here Thunemann et al. generate a mouse model for in vivo tracking and quantification of specific cell types by combining a PET reporter gene with Cre-dependent activation that can be exploited for any cell population for which a Cre mouse line is available."
CHRISTOPHER MARTIN,The Framingham Heart Study 100K SNP Genome-Wide Association Study Resource: Overview of 17 Phenotype Working Group Reports,"BACKGROUND: The Framingham Heart Study (FHS), founded in 1948 to examine the epidemiology of cardiovascular disease, is among the most comprehensively characterized multi-generational studies in the world. Many collected phenotypes have substantial genetic contributors; yet most genetic determinants remain to be identified. Using single nucleotide polymorphisms (SNPs) from a 100K genome-wide scan, we examine the associations of common polymorphisms with phenotypic variation in this community-based cohort and provide a full-disclosure, web-based resource of results for future replication studies. METHODS: Adult participants (n = 1345) of the largest 310 pedigrees in the FHS, many biologically related, were genotyped with the 100K Affymetrix GeneChip. These genotypes were used to assess their contribution to 987 phenotypes collected in FHS over 56 years of follow up, including: cardiovascular risk factors and biomarkers; subclinical and clinical cardiovascular disease; cancer and longevity traits; and traits in pulmonary, sleep, neurology, renal, and bone domains. We conducted genome-wide variance components linkage and population-based and family-based association tests. RESULTS: The participants were white of European descent and from the FHS Original and Offspring Cohorts (examination 1 Offspring mean age 32 ± 9 years, 54% women). This overview summarizes the methods, selected findings and limitations of the results presented in the accompanying series of 17 manuscripts. The presented association results are based on 70,897 autosomal SNPs meeting the following criteria: minor allele frequency ≥ 10%, genotype call rate ≥ 80%, Hardy-Weinberg equilibrium p-value ≥ 0.001, and satisfying Mendelian consistency. Linkage analyses are based on 11,200 SNPs and short-tandem repeats. Results of phenotype-genotype linkages and associations for all autosomal SNPs are posted on the NCBI dbGaP website at. CONCLUSION: We have created a full-disclosure resource of results, posted on the dbGaP website, from a genome-wide association study in the FHS. Because we used three analytical approaches to examine the association and linkage of 987 phenotypes with thousands of SNPs, our results must be considered hypothesis-generating and need to be replicated. Results from the FHS 100K project with NCBI web posting provides a resource for investigators to identify high priority findings for replication."
CHRISTOPHER MARTIN,Genetic Analysis Workshop 15: Gene Expression Analysis and Approaches to Detecting Multiple Functional Loci,
CHRISTOPHER MARTIN,Efficient non-degenerate two-photon excitation for fluorescence microscopy,"Non-degenerate two-photon excitation (ND-TPE) has been explored in two-photon excitation microscopy. However, a systematic study of the efficiency of ND-TPE to guide the selection of fluorophore excitation wavelengths is missing. We measured the relative non-degenerate two-photon absorption cross-section (ND-TPACS) of several commonly used fluorophores (two fluorescent proteins and three small-molecule dyes) and generated 2-dimensional ND-TPACS spectra. We observed that the shape of a ND-TPACS spectrum follows that of the corresponding degenerate two-photon absorption cross-section (D-TPACS) spectrum, but is higher in magnitude. We found that the observed enhancements are higher than theoretical predictions."
CHRISTOPHER MARTIN,Carbon Free Boston: Transportation Technical Report,"OVERVIEW: Transportation connects Boston’s workers, residents and tourists to their livelihoods, health care, education, recreation, culture, and other aspects of life quality. In cities, transit access is a critical factor determining upward mobility. Yet many urban transportation systems, including Boston’s, underserve some populations along one or more of those dimensions. Boston has the opportunity and means to expand mobility access to all residents, and at the same time reduce GHG emissions from transportation. This requires the transformation of the automobile-centric system that is fueled predominantly by gasoline and diesel fuel. The near elimination of fossil fuels—combined with more transit, walking, and biking—will curtail air pollution and crashes, and dramatically reduce the public health impact of transportation. The City embarks on this transition from a position of strength. Boston is consistently ranked as one of the most walkable and bikeable cities in the nation, and one in three commuters already take public transportation. There are three general strategies to reaching a carbon-neutral transportation system: • Shift trips out of automobiles to transit, biking, and walking;1 • Reduce automobile trips via land use planning that encourages denser development and affordable housing in transit-rich neighborhoods; • Shift most automobiles, trucks, buses, and trains to zero-GHG electricity. Even with Boston’s strong transit foundation, a carbon-neutral transportation system requires a wholesale change in Boston’s transportation culture. Success depends on the intelligent adoption of new technologies, influencing behavior with strong, equitable, and clearly articulated planning and investment, and effective collaboration with state and regional partners."
CHRISTOPHER MARTIN,Ancient hybridization and strong adaptation to viruses across African vervet monkey populations,"Vervet monkeys are among the most widely distributed nonhuman primates, show considerable phenotypic diversity, and have long been an important biomedical model for a variety of human diseases and in vaccine research. Using whole-genome sequencing data from 163 vervets sampled from across Africa and the Caribbean, we find high diversity within and between taxa and clear evidence that taxonomic divergence was reticulate rather than following a simple branching pattern. A scan for diversifying selection across taxa identifies strong and highly polygenic selection signals affecting viral processes. Furthermore, selection scores are elevated in genes whose human orthologs interact with HIV and in genes that show a response to experimental simian immunodeficiency virus (SIV) infection in vervet monkeys but not in rhesus macaques, suggesting that part of the signal reflects taxon-specific adaptation to SIV."
CHRISTOPHER MARTIN,Multiple Independent Loci at Chromosome 15q25.1 Affect Smoking Quantity: a Meta-Analysis and Comparison with Lung Cancer and COPD,"Recently, genetic association findings for nicotine dependence, smoking behavior, and smoking-related diseases converged to implicate the chromosome 15q25.1 region, which includes the CHRNA5-CHRNA3-CHRNB4 cholinergic nicotinic receptor subunit genes. In particular, association with the nonsynonymous CHRNA5 SNP rs16969968 and correlates has been replicated in several independent studies. Extensive genotyping of this region has suggested additional statistically distinct signals for nicotine dependence, tagged by rs578776 and rs588765. One goal of the Consortium for the Genetic Analysis of Smoking Phenotypes (CGASP) is to elucidate the associations among these markers and dichotomous smoking quantity (heavy versus light smoking), lung cancer, and chronic obstructive pulmonary disease (COPD). We performed a meta-analysis across 34 datasets of European-ancestry subjects, including 38,617 smokers who were assessed for cigarettes-per-day, 7,700 lung cancer cases and 5,914 lung-cancer-free controls (all smokers), and 2,614 COPD cases and 3,568 COPD-free controls (all smokers). We demonstrate statistically independent associations of rs16969968 and rs588765 with smoking (mutually adjusted p-values<10−35 and >10−8 respectively). Because the risk alleles at these loci are negatively correlated, their association with smoking is stronger in the joint model than when each SNP is analyzed alone. Rs578776 also demonstrates association with smoking after adjustment for rs16969968 (p<10−6). In models adjusting for cigarettes-per-day, we confirm the association between rs16969968 and lung cancer (p<10−20) and observe a nominally significant association with COPD (p = 0.01); the other loci are not significantly associated with either lung cancer or COPD after adjusting for rs16969968. This study provides strong evidence that multiple statistically distinct loci in this region affect smoking behavior. This study is also the first report of association between rs588765 (and correlates) and smoking that achieves genome-wide significance; these SNPs have previously been associated with mRNA levels of CHRNA5 in brain and lung tissue. Author Summary Nicotine binds to cholinergic nicotinic receptors, which are composed of a variety of subunits. Genetic studies for smoking behavior and smoking-related diseases have implicated a genomic region that encodes the alpha5, alpha3, and beta4 subunits. We examined genetic data across this region for over 38,000 smokers, a subset of which had been assessed for lung cancer or chronic obstructive pulmonary disease. We demonstrate strong evidence that there are at least two statistically independent loci in this region that affect risk for heavy smoking. One of these loci represents a change in the protein structure of the alpha5 subunit. This work is also the first to report strong evidence of association between smoking and a group of genetic variants that are of biological interest because of their links to expression of the alpha5 cholinergic nicotinic receptor subunit gene. These advances in understanding the genetic influences on smoking behavior are important because of the profound public health burdens caused by smoking and nicotine addiction."
CHRISTOPHER MARTIN,Selective Disruption of the Cerebral Neocortex in Alzheimer's Disease,"BACKGROUND. Alzheimer's disease (AD) and its transitional state mild cognitive impairment (MCI) are characterized by amyloid plaque and tau neurofibrillary tangle (NFT) deposition within the cerebral neocortex and neuronal loss within the hippocampal formation. However, the precise relationship between pathologic changes in neocortical regions and hippocampal atrophy is largely unknown. METHODOLOGY/PRINCIPAL FINDINGS. In this study, combining structural MRI scans and automated image analysis tools with reduced cerebrospinal fluid (CSF) Aß levels, a surrogate for intra-cranial amyloid plaques and elevated CSF phosphorylated tau (p-tau) levels, a surrogate for neocortical NFTs, we examined the relationship between the presence of Alzheimer's pathology, gray matter thickness of select neocortical regions, and hippocampal volume in cognitively normal older participants and individuals with MCI and AD (n=724). Amongst all 3 groups, only select heteromodal cortical regions significantly correlated with hippocampal volume. Amongst MCI and AD individuals, gray matter thickness of the entorhinal cortex and inferior temporal gyrus significantly predicted longitudinal hippocampal volume loss in both amyloid positive and p-tau positive individuals. Amongst cognitively normal older adults, thinning only within the medial portion of the orbital frontal cortex significantly differentiated amyloid positive from amyloid negative individuals whereas thinning only within the entorhinal cortex significantly discriminated p-tau positive from p-tau negative individuals. CONCLUSIONS/SIGNIFICANCE. Cortical Aß and tau pathology affects gray matter thinning within select neocortical regions and potentially contributes to downstream hippocampal degeneration. Neocortical Alzheimer's pathology is evident even amongst older asymptomatic individuals suggesting the existence of a preclinical phase of dementia."
CHRISTOPHER MARTIN,High-resolution and -precision correlation of dark and light layers in the Quaternary hemipelagic sediments of the Japan Sea recovered during IODP Expedition 346,"The Quaternary hemipelagic sediments of the Japan Sea are characterized by centimeter- to decimeter-scale alternation of dark and light clay to silty clay, which are bio-siliceous and/or bio-calcareous to a various degree. Each of the dark and light layers are considered as deposited synchronously throughout the deeper (> 500 m) part of the sea. However, attempts for correlation and age estimation of individual layers are limited to the upper few tens of meters. In addition, the exact timing of the depositional onset of these dark and light layers and its synchronicity throughout the deeper part of the sea have not been explored previously, although the onset timing was roughly estimated as ~ 1.5 Ma based on the result of Ocean Drilling Program legs 127/128. Consequently, it is not certain exactly when their deposition started, whether deposition of dark and light layers was synchronous and whether they are correlatable also in the earlier part of their depositional history. The Quaternary hemipelagic sediments of the Japan Sea were drilled at seven sites during Integrated Ocean Drilling Program Expedition 346 in 2013. Alternation of dark and light layers was recovered at six sites whose water depths are > ~ 900 m, and continuous composite columns were constructed at each site. Here, we report our effort to correlate individual dark layers and estimate their ages based on a newly constructed age model at Site U1424 using the best available paleomagnetic datum and marker tephras. The age model is further tuned to LR04 δ18O curve using gamma ray attenuation density (GRA) since it reflects diatom contents that are higher during interglacial high-stands. The constructed age model for Site U1424 is projected to other sites using correlation of dark layers to form a high-resolution and high-precision paleo-observatory network that allows to reconstruct changes in material fluxes with high spatio-temporal resolutions."
CHRISTOPHER MARTIN,The SNO+ experiment,
CHRISTOPHER MARTIN,Prospects for beyond the standard model physics searches at the deep underground neutrino experiment: DUNE collaboration,"The Deep Underground Neutrino Experiment (DUNE) will be a powerful tool for a variety of physics topics. The high-intensity proton beams provide a large neutrino flux, sampled by a near detector system consisting of a combination of capable precision detectors, and by the massive far detector system located deep underground. This configuration sets up DUNE as a machine for discovery, as it enables opportunities not only to perform precision neutrino measurements that may uncover deviations from the present three-flavor mixing paradigm, but also to discover new particles and unveil new interactions and symmetries beyond those predicted in the Standard Model (SM). Of the many potential beyond the Standard Model (BSM) topics DUNE will probe, this paper presents a selection of studies quantifying DUNE's sensitivities to sterile neutrino mixing, heavy neutral leptons, non-standard interactions, CPT symmetry violation, Lorentz invariance violation, neutrino trident production, dark matter from both beam induced and cosmogenic sources, baryon number violation, and other new physics topics that complement those at high-energy colliders and significantly extend the present reach."
CHRISTOPHER MARTIN,Identification and reconstruction of low-energy electrons in the ProtoDUNE-SP detector,
CHRISTOPHER MARTIN,The long-baseline neutrino experiment: exploring fundamental symmetries of the universe,"The preponderance of matter over antimatter in the early Universe, the dynamics of the supernova bursts that produced the heavy elements necessary for life and whether protons eventually decay --- these mysteries at the forefront of particle physics and astrophysics are key to understanding the early evolution of our Universe, its current state and its eventual fate. The Long-Baseline Neutrino Experiment (LBNE) represents an extensively developed plan for a world-class experiment dedicated to addressing these questions. LBNE is conceived around three central components: (1) a new, high-intensity neutrino source generated from a megawatt-class proton accelerator at Fermi National Accelerator Laboratory, (2) a near neutrino detector just downstream of the source, and (3) a massive liquid argon time-projection chamber deployed as a far detector deep underground at the Sanford Underground Research Facility. This facility, located at the site of the former Homestake Mine in Lead, South Dakota, is approximately 1,300 km from the neutrino source at Fermilab -- a distance (baseline) that delivers optimal sensitivity to neutrino charge-parity symmetry violation and mass ordering effects. This ambitious yet cost-effective design incorporates scalability and flexibility and can accommodate a variety of upgrades and contributions. With its exceptional combination of experimental configuration, technical capabilities, and potential for transformative discoveries, LBNE promises to be a vital facility for the field of particle physics worldwide, providing physicists from around the globe with opportunities to collaborate in a twenty to thirty year program of exciting science. In this document we provide a comprehensive overview of LBNE's scientific objectives, its place in the landscape of neutrino physics worldwide, the technologies it will incorporate and the capabilities it will possess."
CHRISTOPHER MARTIN,SmartFuse: reconfigurable smart switches to accelerate fused collectives in HPC applications,"Communication switches have sometimes been augmented to process collectives, e.g., in the IBM BlueGene and Mellanox SHArP switches. In this work, we find that there is a great acceleration opportunity through the further augmentation of switches to accelerate more complex functions that combine communication with computation. We consider three types of such functions. The first is fully-fused collectives built by fusing multiple existing collectives like Allreduce with Alltoall. The second is semi-fused collectives built by combining a collective with another computation. The third are higher-order collectives built by combining multiple computations and communications, such as to perform matrix-matrix multiply (PGEMM). In this work, we propose a framework called SmartFuse to accelerate fused collective functions. The core of SmartFuse is a reconfigurable smart switch to support these operations. The semi/fully fused collectives are implemented with a CGRA-like architecture, while higher-order collectives are implemented with a more specialized computational unit that can also schedule communication. Supporting our framework is software to evaluate and translate relevant parts of the input program, compile them into a control data flow graph, and then map this graph to the switch hardware. The proposed framework, once deployed, has the strong potential to accelerate existing HPC applications transparently by encapsulation within an MPI implementation. Experimental results show that this approach improves the performance of the PGEMM kernel, miniFE, and AMG by, on average, 94%, 15%, and 13%, respectively."
CHRISTOPHER MARTIN,Tevatron-for-LHC report: preparations for discoveries,"This is the ""TeV4LHC"" report of the ""Physics Landscapes"" Working Group, focused on facilitating the start-up of physics explorations at the LHC by using the experience gained at the Tevatron. We present experimental and theoretical results that can be employed to probe various scenarios for physics beyond the Standard Model."
CHRISTOPHER MARTIN,Scintillation light detection in the 6-m drift-length ProtoDUNE Dual Phase liquid argon TPC,"DUNE is a dual-site experiment for long-baseline neutrino oscillation studies, neutrino astrophysics and nucleon decay searches. ProtoDUNE Dual Phase (DP) is a 6  ×  6  ×  6 m 3 liquid argon time-projection-chamber (LArTPC) that recorded cosmic-muon data at the CERN Neutrino Platform in 2019-2020 as a prototype of the DUNE Far Detector. Charged particles propagating through the LArTPC produce ionization and scintillation light. The scintillation light signal in these detectors can provide the trigger for non-beam events. In addition, it adds precise timing capabilities and improves the calorimetry measurements. In ProtoDUNE-DP, scintillation and electroluminescence light produced by cosmic muons in the LArTPC is collected by photomultiplier tubes placed up to 7 m away from the ionizing track. In this paper, the ProtoDUNE-DP photon detection system performance is evaluated with a particular focus on the different wavelength shifters, such as PEN and TPB, and the use of Xe-doped LAr, considering its future use in giant LArTPCs. The scintillation light production and propagation processes are analyzed and a comparison of simulation to data is performed, improving understanding of the liquid argon properties."
CHRISTOPHER MARTIN,The genome of the vervet ( Chlorocebus aethiops sabaeus ),"We describe a genome reference of the African green monkey or vervet (Chlorocebus aethiops). This member of the Old World monkey (OWM) superfamily is uniquely valuable for genetic investigations of simian immunodeficiency virus (SIV), for which it is the most abundant natural host species, and of a wide range of health-related phenotypes assessed in Caribbean vervets (C. a. sabaeus), whose numbers have expanded dramatically since Europeans introduced small numbers of their ancestors from West Africa during the colonial era. We use the reference to characterize the genomic relationship between vervets and other primates, the intra-generic phylogeny of vervet subspecies, and genome-wide structural variations of a pedigreed C. a. sabaeus population. Through comparative analyseswith human and rhesus macaque, we characterize at high resolution the unique chromosomal fission events that differentiate the vervets and their close relatives from most other catarrhine primates, in whom karyotype is highly conserved. We also provide a summary of transposable elements and contrast these with the rhesus macaque and human. Analysis of sequenced genomes representing each of the main vervet subspecies supports previously hypothesized relationships between these populations, which range across most of sub-Saharan Africa, while uncovering high levels of genetic diversity within each. Sequence-based analyses of major histocompatibility complex (MHC) polymorphisms reveal extremely low diversity in Caribbean C. a. sabaeus vervets, compared to vervets from putatively ancestral West African regions. In the C. a. sabaeus research population, we discover the first structural variations that are, in some cases, predicted to have a deleterious effect; future studies will determine the phenotypic impact of these variations."
CHRISTOPHER MARTIN,Event Horizon Telescope imaging of the archetypal blazar 3C 279 at an extreme 20 microarcsecond resolution,"3C 279 is an archetypal blazar with a prominent radio jet that show broadband flux density variability across the entire electromagnetic spectrum. We use an ultra-high angular resolution technique – global Very Long Baseline Interferometry (VLBI) at 1.3 mm (230 GHz) – to resolve the innermost jet of 3C 279 in order to study its fine-scale morphology close to the jet base where highly variable γ-ray emission is thought to originate, according to various models. The source was observed during four days in April 2017 with the Event Horizon Telescope at 230 GHz, including the phased Atacama Large Millimeter/submillimeter Array (ALMA), at an angular resolution of ∼20 μas (at a redshift of z = 0.536 this corresponds to ∼0.13 pc  ∼ 1700 Schwarzschild radii with a black hole mass M<jats:sub>BH</jats:sub> = 8 × 10^8 M_⊙). Imaging and model-fitting techniques were applied to the data to parameterize the fine-scale source structure and its variation. We find a multicomponent inner jet morphology with the northernmost component elongated perpendicular to the direction of the jet, as imaged at longer wavelengths. The elongated nuclear structure is consistent on all four observing days and across different imaging methods and model-fitting techniques, and therefore appears robust. Owing to its compactness and brightness, we associate the northern nuclear structure as the VLBI “core”. This morphology can be interpreted as either a broad resolved jet base or a spatially bent jet. We also find significant day-to-day variations in the closure phases, which appear most pronounced on the triangles with the longest baselines. Our analysis shows that this variation is related to a systematic change of the source structure. Two inner jet components move non-radially at apparent speeds of ∼15 c and ∼20 c (∼1.3 and ∼1.7 μas day^−1, respectively), which more strongly supports the scenario of traveling shocks or instabilities in a bent, possibly rotating jet. The observed apparent speeds are also coincident with the 3C 279 large-scale jet kinematics observed at longer (cm) wavelengths, suggesting no significant jet acceleration between the 1.3 mm core and the outer jet. The intrinsic brightness temperature of the jet components are ≲10^10 K, a magnitude or more lower than typical values seen at ≥7 mm wavelengths. The low brightness temperature and morphological complexity suggest that the core region of 3C 279 becomes optically thin at short (mm) wavelengths."
CHRISTOPHER MARTIN,First M87 Event Horizon Telescope results. II. Array and instrumentation,"The Event Horizon Telescope (EHT) is a very long baseline interferometry (VLBI) array that comprises millimeter- and submillimeter-wavelength telescopes separated by distances comparable to the diameter of the Earth. At a nominal operating wavelength of ~1.3 mm, EHT angular resolution (λ/D) is ~25 μas, which is sufficient to resolve nearby supermassive black hole candidates on spatial and temporal scales that correspond to their event horizons. With this capability, the EHT scientific goals are to probe general relativistic effects in the strong-field regime and to study accretion and relativistic jet formation near the black hole boundary. In this Letter we describe the system design of the EHT, detail the technology and instrumentation that enable observations, and provide measures of its performance. Meeting the EHT science objectives has required several key developments that have facilitated the robust extension of the VLBI technique to EHT observing wavelengths and the production of instrumentation that can be deployed on a heterogeneous array of existing telescopes and facilities. To meet sensitivity requirements, high-bandwidth digital systems were developed that process data at rates of 64 gigabit s−1, exceeding those of currently operating cm-wavelength VLBI arrays by more than an order of magnitude. Associated improvements include the development of phasing systems at array facilities, new receiver installation at several sites, and the deployment of hydrogen maser frequency standards to ensure coherent data capture across the array. These efforts led to the coordination and execution of the first Global EHT observations in 2017 April, and to event-horizon-scale imaging of the supermassive black hole candidate in M87."
CHRISTOPHER MARTIN,First M87 Event Horizon Telescope results. I. The shadow of the supermassive black hole,"When surrounded by a transparent emission region, black holes are expected to reveal a dark shadow caused by gravitational light bending and photon capture at the event horizon. To image and study this phenomenon, we have assembled the Event Horizon Telescope, a global very long baseline interferometry array observing at a wavelength of 1.3 mm. This allows us to reconstruct event-horizon-scale images of the supermassive black hole candidate in the center of the giant elliptical galaxy M87. We have resolved the central compact radio source as an asymmetric bright emission ring with a diameter of 42 ± 3 μas, which is circular and encompasses a central depression in brightness with a flux ratio gsim10:1. The emission ring is recovered using different calibration and imaging schemes, with its diameter and width remaining stable over four different observations carried out in different days. Overall, the observed image is consistent with expectations for the shadow of a Kerr black hole as predicted by general relativity. The asymmetry in brightness in the ring can be explained in terms of relativistic beaming of the emission from a plasma rotating close to the speed of light around a black hole. We compare our images to an extensive library of ray-traced general-relativistic magnetohydrodynamic simulations of black holes and derive a central mass of M = (6.5 ± 0.7) × 10^9 M ⊙. Our radio-wave observations thus provide powerful evidence for the presence of supermassive black holes in centers of galaxies and as the central engines of active galactic nuclei. They also present a new tool to explore gravity in its most extreme limit and on a mass scale that was so far not accessible."
KIM SICHEL,From Icon to Irony: German and American Industrial Photography,
KIM SICHEL,Mapping the West: Nineteenth-Century American Landscape Photographs from the Boston Public Library,
SURYARAM GUMMULURU,Femtosecond photonic viral inactivation probed using solid-state nanopores,"We report on detection of virus inactivation using femtosecond laser radiation by measuring the conductance of a solid state nanopore designed for detecting single particles. Conventional methods of assaying for viral inactivation based on plaque forming assays require 24–48 h for bacterial growth. Nanopore conductance measurements provide information on morphological changes at a single virion level.We show that analysis of a time series of nanopore conductance can quantify the detection of inactivation, requiring only a few minutes from collection to analysis. Morphological changes were verified by dynamic light scattering. Statistical analysis maximizing the information entropy provides a measure of the log reduction value. This work provides a rapid method for assaying viral inactivation with femtosecond lasers using solid-state nanopores."
SURYARAM GUMMULURU,Quantifying Lipid Contents in Enveloped Virus Particles with Plasmonic Nanoparticles,"Phosphatidylserine (PS) and monosialotetrahexosylganglioside (GM1) are examples of two host-derived lipids in the membrane of enveloped virus particles that are known to contribute to virus attachment, uptake, and ultimately dissemination. A quantitative characterization of their contribution to the functionality of the virus requires information about their relative concentrations in the viral membrane. Here, a gold nanoparticle (NP) binding assay for probing relative PS and GM1 lipid concentrations in the outer leaflet of different HIV-1 and Ebola virus-like particles (VLPs) using sample sizes of less than 3 × 106 particles is introduced. The assay evaluates both scattering intensity and resonance wavelength, and determines relative NP densities through plasmon coupling as a measure for the target lipid concentrations in the NP-labeled VLP membrane. A correlation of the optical observables with absolute lipid contents is achieved by calibration of the plasmon coupling-based methodology with unilamellar liposomes of known PS or GM1 concentration. The performed studies reveal significant differences in the membrane of VLPs that assemble at different intracellular sites and pave the way to an optical quantification of lipid concentration in virus particles at physiological titers."
SURYARAM GUMMULURU,PPARγ and LXR Signaling Inhibit Dendritic Cell-Mediated HIV-1 Capture and trans-Infection,"Dendritic cells (DCs) contribute to human immunodeficiency virus type 1 (HIV-1) transmission and dissemination by capturing and transporting infectious virus from the mucosa to draining lymph nodes, and transferring these virus particles to CD4+ T cells with high efficiency. Toll-like receptor (TLR)-induced maturation of DCs enhances their ability to mediate trans-infection of T cells and their ability to migrate from the site of infection. Because TLR-induced maturation can be inhibited by nuclear receptor (NR) signaling, we hypothesized that ligand-activated NRs could repress DC-mediated HIV-1 transmission and dissemination. Here, we show that ligands for peroxisome proliferator-activated receptor gamma (PPARγ) and liver X receptor (LXR) prevented proinflammatory cytokine production by DCs and inhibited DC migration in response to the chemokine CCL21 by preventing the TLR-induced upregulation of CCR7. Importantly, PPARγ and LXR signaling inhibited both immature and mature DC-mediated trans-infection by preventing the capture of HIV-1 by DCs independent of the viral envelope glycoprotein. PPARγ and LXR signaling induced cholesterol efflux from DCs and led to a decrease in DC-associated cholesterol, which has previously been shown to be required for DC capture of HIV-1. Finally, both cholesterol repletion and the targeted knockdown of the cholesterol transport protein ATP-binding cassette A1 (ABCA1) restored the ability of NR ligand treated cells to capture HIV-1 and transfer it to T cells. Our results suggest that PPARγ and LXR signaling up-regulate ABCA1-mediated cholesterol efflux from DCs and that this accounts for the decreased ability of DCs to capture HIV-1. The ability of NR ligands to repress DC mediated trans-infection, inflammation, and DC migration underscores their potential therapeutic value in inhibiting HIV-1 mucosal transmission. Author SummaryHeterosexual transmission is the primary mode of HIV transmission worldwide. In the absence of an effective vaccine, there is an increasing demand for the development of effective microbicides that block HIV sexual transmission. Dendritic cells (DCs) play a critical role in HIV transmission by efficiently binding virus particles, migrating to lymph nodes, and transmitting them to CD4+ T cells, a process called trans-infection. In addition, DCs secrete proinflammatory cytokines that create a favorable environment for virus replication. DC maturation by pathogen-encoded TLR ligands or proinflammatory cytokines dramatically increases their capacity to capture HIV, migrate to lymphoid tissue, and trans-infect T cells. Here, we report that signaling through the nuclear receptors PPARγ and LXR prevents DC maturation and proinflammatory cytokine production, as well as migration. In addition, PPARγ and LXR signaling prevents efficient DC capture and transfer of infectious HIV by increasing ABCA1-mediated cholesterol efflux. Our studies suggest that PPARγ and LXR may be targets for drugs that can inhibit specific aspects of HIV mucosal transmission, namely inflammation, migration, and virus capture and transfer. These findings provide a rationale for considering PPARγ and LXR agonists as potential combination therapies with conventional anti-viral microbicides that target other aspects of mucosal HIV transmission."
ROBERT G KING,Magnetic-field measurement and analysis for the Muon g−2 Experiment at Fermilab,"The Fermi National Accelerator Laboratory (FNAL) Muon g−2 Experiment has measured the anomalous precession frequency aμ≡(gμ−2)/2 of the muon to a combined precision of 0.46 parts per million with data collected during its first physics run in 2018. This paper documents the measurement of the magnetic field in the muon storage ring. The magnetic field is monitored by systems and calibrated in terms of the equivalent proton spin precession frequency in a spherical water sample at 34.7∘C. The measured field is weighted by the muon distribution resulting in ˜ω′p, the denominator in the ratio ωa/˜ω′p that together with known fundamental constants yields aμ. The reported uncertainty on ˜ω′p for the Run-1 data set is 114 ppb consisting of uncertainty contributions from frequency extraction, calibration, mapping, tracking, and averaging of 56 ppb, and contributions from fast transient fields of 99 ppb."
ROBERT G KING,Beam dynamics corrections to the Run-1 measurement of the muon anomalous magnetic moment at Fermilab,"This paper presents the beam dynamics systematic corrections and their uncertainties for the Run-1 dataset of the Fermilab Muon g−2 Experiment. Two corrections to the measured muon precession frequency ωma are associated with well-known effects owing to the use of electrostatic quadrupole (ESQ) vertical focusing in the storage ring. An average vertically oriented motional magnetic field is felt by relativistic muons passing transversely through the radial electric field components created by the ESQ system. The correction depends on the stored momentum distribution and the tunes of the ring, which has relatively weak vertical focusing. Vertical betatron motions imply that the muons do not orbit the ring in a plane exactly orthogonal to the vertical magnetic field direction. A correction is necessary to account for an average pitch angle associated with their trajectories. A third small correction is necessary, because muons that escape the ring during the storage time are slightly biased in initial spin phase compared to the parent distribution. Finally, because two high-voltage resistors in the ESQ network had longer than designed RC time constants, the vertical and horizontal centroids and envelopes of the stored muon beam drifted slightly, but coherently, during each storage ring fill. This led to the discovery of an important phase-acceptance relationship that requires a correction. The sum of the corrections to ωma is 0.50±0.09  ppm; the uncertainty is small compared to the 0.43 ppm statistical precision of ωma."
ROBERT G KING,Measurement of the anomalous precession frequency of the muon in the Fermilab Muon g − 2 Experiment,"The Muon g−2 Experiment at Fermi National Accelerator Laboratory (FNAL) has measured the muon anomalous precession frequency ωma to an uncertainty of 434 parts per billion (ppb), statistical, and 56 ppb, systematic, with data collected in four storage ring configurations during its first physics run in 2018. When combined with a precision measurement of the magnetic field of the experiment’s muon storage ring, the precession frequency measurement determines a muon magnetic anomaly of aμ(FNAL)=116592040(54)×10−11 (0.46 ppm). This article describes the multiple techniques employed in the reconstruction, analysis, and fitting of the data to measure the precession frequency. It also presents the averaging of the results from the 11 separate determinations of ωma, and the systematic uncertainties on the result."
ROBERT G KING,Identification and reconstruction of low-energy electrons in the ProtoDUNE-SP detector,
ROBERT G KING,Measurement of the positive muon anomalous magnetic moment to 0.46 ppm,"We present the first results of the Fermilab National Accelerator Laboratory (FNAL) Muon g-2 Experiment for the positive muon magnetic anomaly a_{μ}≡(g_{μ}-2)/2. The anomaly is determined from the precision measurements of two angular frequencies. Intensity variation of high-energy positrons from muon decays directly encodes the difference frequency ω_{a} between the spin-precession and cyclotron frequencies for polarized muons in a magnetic storage ring. The storage ring magnetic field is measured using nuclear magnetic resonance probes calibrated in terms of the equivalent proton spin precession frequency ω[over ˜]_{p}^{'} in a spherical water sample at 34.7 °C. The ratio ω_{a}/ω[over ˜]_{p}^{'}, together with known fundamental constants, determines a_{μ}(FNAL)=116 592 040(54)×10^{-11} (0.46 ppm). The result is 3.3 standard deviations greater than the standard model prediction and is in excellent agreement with the previous Brookhaven National Laboratory (BNL) E821 measurement. After combination with previous measurements of both μ^{+} and μ^{-}, the new experimental average of a_{μ}(Exp)=116 592 061(41)×10^{-11} (0.35 ppm) increases the tension between experiment and theory to 4.2 standard deviations."
ROBERT G KING,Scintillation light detection in the 6-m drift-length ProtoDUNE Dual Phase liquid argon TPC,"DUNE is a dual-site experiment for long-baseline neutrino oscillation studies, neutrino astrophysics and nucleon decay searches. ProtoDUNE Dual Phase (DP) is a 6  ×  6  ×  6 m 3 liquid argon time-projection-chamber (LArTPC) that recorded cosmic-muon data at the CERN Neutrino Platform in 2019-2020 as a prototype of the DUNE Far Detector. Charged particles propagating through the LArTPC produce ionization and scintillation light. The scintillation light signal in these detectors can provide the trigger for non-beam events. In addition, it adds precise timing capabilities and improves the calorimetry measurements. In ProtoDUNE-DP, scintillation and electroluminescence light produced by cosmic muons in the LArTPC is collected by photomultiplier tubes placed up to 7 m away from the ionizing track. In this paper, the ProtoDUNE-DP photon detection system performance is evaluated with a particular focus on the different wavelength shifters, such as PEN and TPB, and the use of Xe-doped LAr, considering its future use in giant LArTPCs. The scintillation light production and propagation processes are analyzed and a comparison of simulation to data is performed, improving understanding of the liquid argon properties."
MARY CATHERINE O'CONNOR,A study to ascertain what effect the study of homophones has on spelling achievement,
MARY CATHERINE O'CONNOR,Examining elementary teachers’ puzzles: a cross-disciplinary analysis,"We present a cross-disciplinary analysis of the puzzles and tensions elementary teachers experience as they conduct classroom discussion. We describe two teachers’ framings and sense-making about the puzzle of how (much) to steer discussion in light of instructional goals, considering similarities and differences across teachers and disciplines. This work is part of a project to understand how elementary teachers learn to conduct classroom discussions in ways that support deep disciplinary learning and seek to disrupt settled expectations of disciplines, children, and teaching (Bang, Warren, Rosebery, & Medin, 2012). We assume that systems of oppression permeate teaching and learning, for example, through curriculum structures, how subject matter is constituted, and privileged ways of speaking and acting (Bang et al, 2012; Esmonde & Booker, 2016). This poster shares how we have sought to understand the puzzles and tensions that elementary teachers experience as they conduct classroom discussion. We focus on puzzles because they provide windows into teacher sense-making and they may reveal opportunities to work with teachers around their own concerns at the intersection of disciplines, classroom discourse, and power. When teachers frame and try to make sense of puzzles and tensions, they draw upon practices, curriculum materials, and categories for labeling students (Hall & Horn, 2012) that inevitably reflect the dominant ideologies of society, school disciplines, and disciplinary knowing (Louie, 2020). We are interested in understanding how teachers' puzzles and tensions might be similar and different across school disciplines. While elementary teachers typically work with one group of children across content areas, researchers have tended to approach studying and supporting teachers’ practice from the perspective of a particular discipline (e.g., mathematics). We seek to understand how the puzzles and tensions that emerge for teachers might be shaped by school disciplines, and how they can serve to make visible the contradictions and dominant ideologies of larger systems."
MARY CATHERINE O'CONNOR,"An analysis of the general content, grade placement, and philosophy of state courses of study in arithmetic for grades one to six.",
JOSEPH P MIZGERD,Epithelial cell–derived secreted and transmembrane 1a signals to activated neutrophils during pneumococcal pneumonia,"Airway epithelial cell responses are critical to the outcome of lung infection. In this study, we aimed to identify unique contributions of epithelial cells during lung infection. To differentiate genes induced selectively in epithelial cells during pneumonia, we compared genome-wide expression profiles from three sorted cell populations: epithelial cells from uninfected mouse lungs, epithelial cells from mouse lungs with pneumococcal pneumonia, and nonepithelial cells from those same infected lungs. Of 1,166 transcripts that were more abundant in epithelial cells from infected lungs compared with nonepithelial cells from the same lungs or from epithelial cells of uninfected lungs, 32 genes were identified as highly expressed secreted products. Especially strong signals included two related secreted and transmembrane (Sectm) 1 genes, Sectm1a and Sectm1b. Refinement of sorting strategies suggested that both Sectm1 products were induced predominantly in conducting airway epithelial cells. Sectm1 was induced during the early stages of pneumococcal pneumonia, and mutation of NF-kB RelA in epithelial cells did not diminish its expression. Instead, type I IFN signaling was necessary and sufficient for Sectm1 induction in lung epithelial cells, mediated by signal transducer and activator of transcription 1. For target cells, Sectm1a bound to myeloid cells preferentially, in particular Ly6GbrightCD11bbright neutrophils in the infected lung. In contrast, Sectm1a did not bind to neutrophils from uninfected lungs. Sectm1a increased expression of the neutrophil-attracting chemokine CXCL2 by neutrophils from the infected lung. We propose that Sectm1a is an epithelial product that sustains a positive feedback loop amplifying neutrophilic inflammation during pneumococcal pneumonia."
DAVID W KAUFMAN,"Caribbean Corals in Crisis: Record Thermal Stress, Bleaching, and Mortality in 2005","BACKGROUND. The rising temperature of the world's oceans has become a major threat to coral reefs globally as the severity and frequency of mass coral bleaching and mortality events increase. In 2005, high ocean temperatures in the tropical Atlantic and Caribbean resulted in the most severe bleaching event ever recorded in the basin. METHODOLOGY/PRINCIPAL FINDINGS. Satellite-based tools provided warnings for coral reef managers and scientists, guiding both the timing and location of researchers' field observations as anomalously warm conditions developed and spread across the greater Caribbean region from June to October 2005. Field surveys of bleaching and mortality exceeded prior efforts in detail and extent, and provided a new standard for documenting the effects of bleaching and for testing nowcast and forecast products. Collaborators from 22 countries undertook the most comprehensive documentation of basin-scale bleaching to date and found that over 80% of corals bleached and over 40% died at many sites. The most severe bleaching coincided with waters nearest a western Atlantic warm pool that was centered off the northern end of the Lesser Antilles. CONCLUSIONS/SIGNIFICANCE. Thermal stress during the 2005 event exceeded any observed from the Caribbean in the prior 20 years, and regionally-averaged temperatures were the warmest in over 150 years. Comparison of satellite data against field surveys demonstrated a significant predictive relationship between accumulated heat stress (measured using NOAA Coral Reef Watch's Degree Heating Weeks) and bleaching intensity. This severe, widespread bleaching and mortality will undoubtedly have long-term consequences for reef ecosystems and suggests a troubled future for tropical marine ecosystems under a warming climate."
DAVID E LEVIN,Identification of Positive Regulators of the Yeast Fps1 Glycerol Channel,"The yeast Fps1 protein is an aquaglyceroporin that functions as the major facilitator of glycerol transport in response to changes in extracellular osmolarity. Although the High Osmolarity Glycerol pathway is thought to have a function in at least basal control of Fps1 activity, its mode of regulation is not understood. We describe the identification of a pair of positive regulators of the Fps1 glycerol channel, Rgc1 (Ypr115w) and Rgc2 (Ask10). An rgc1/2Δ mutant experiences cell wall stress that results from osmotic pressure associated with hyper-accumulation of glycerol. Accumulation of glycerol in the rgc1/2Δ mutant results from a defect in Fps1 activity as evidenced by suppression of the defect through Fps1 overexpression, failure to release glycerol upon hypo-osmotic shock, and resistance to arsenite, a toxic metalloid that enters the cell through Fps1. Regulation of Fps1 by Rgc1/2 appears to be indirect; however, evidence is presented supporting the view that Rgc1/2 regulate Fps1 channel activity, rather than its expression, folding, or localization. Rgc2 was phosphorylated in response to stresses that lead to regulation of Fps1. This stress-induced phosphorylation was partially dependent on the Hog1 MAPK. Hog1 was also required for basal phosphorylation of Rgc2, suggesting a mechanism by which Hog1 may regulate Fps1 indirectly. Author Summary When challenged by changes in extracellular osmolarity, many fungal species regulate their intracellular glycerol concentration to modulate their internal osmotic pressure. Maintenance of osmotic homeostasis prevents either cellular collapse under hyper-osmotic stress or cell rupture under hypo-osmotic stress. In baker's yeast, the Fps1 glycerol channel functions as the main vent for glycerol. Proper regulation of Fps1 is critical to the maintenance of osmotic homeostasis. In this study, we identify a pair of proteins (Rgc1 and Rgc2) that function as positive regulators of Fps1 activity. Their absence results in hyper-accumulation of glycerol and consequent cell lysis due to impaired Fps1 channel activity. Additionally, we found that these glycerol channel regulators function between the Hog1 (High Osmolarity Glycerol response) signaling kinase and Fps1, defining a signaling pathway for control of glycerol efflux. Because members of the Rgc1/2 family are found among pathogenic fungal species, but not in humans, they represent potentially attractive targets for antifungal drug development."
DAVID E LEVIN,"Big Gods and big science: further reflections on theory, data, and analysis",
DAVID E LEVIN,"Three red suns in the sky: A transiting, terrestrial planet in a triple M-dwarf system at 6.9 pc","We present the discovery from Transiting Exoplanet Survey Satellite (TESS) data of LTT 1445Ab. At a distance of 6.9 pc, it is the second nearest transiting exoplanet system found to date, and the closest one known for which the primary is an M dwarf. The host stellar system consists of three mid-to-late M dwarfs in a hierarchical configuration, which are blended in one TESS pixel. We use MEarth data and results from the Science Processing Operations Center data validation report to determine that the planet transits the primary star in the system. The planet has a radius of ${1.38}_{-0.12}^{+0.13}$ ${R}_{\oplus }$, an orbital period of ${5.35882}_{-0.00031}^{+0.00030}$ days, and an equilibrium temperature of ${433}_{-27}^{+28}$ K. With radial velocities from the High Accuracy Radial Velocity Planet Searcher, we place a 3σ upper mass limit of 8.4 ${M}_{\oplus }$ on the planet. LTT 1445Ab provides one of the best opportunities to date for the spectroscopic study of the atmosphere of a terrestrial world. We also present a detailed characterization of the host stellar system. We use high-resolution spectroscopy and imaging to rule out the presence of any other close stellar or brown dwarf companions. Nineteen years of photometric monitoring of A and BC indicate a moderate amount of variability, in agreement with that observed in the TESS light-curve data. We derive a preliminary astrometric orbit for the BC pair that reveals an edge-on and eccentric configuration. The presence of a transiting planet in this system hints that the entire system may be co-planar, implying that the system may have formed from the early fragmentation of an individual protostellar core."
DAVID E LEVIN,The first habitable-zone Earth-sized planet from TESS. II. Spitzer confirms TOI-700 d,"We present Spitzer 4.5 μm observations of the transit of TOI-700 d, a habitable-zone Earth-sized planet in a multiplanet system transiting a nearby M-dwarf star (TIC 150428135, 2MASS J06282325–6534456). TOI-700 d has a radius of 1.144_-0.061^+0.062R_⨁ and orbits within its host star's conservative habitable zone with a period of 37.42 days (T eq ~ 269 K). TOI-700 also hosts two small inner planets (R b = 1.037_-0.064^+0.065R_⨁ and R c = 2.65_-0.15^+0.16R_⨁) with periods of 9.98 and 16.05 days, respectively. Our Spitzer observations confirm the Transiting Exoplanet Survey Satellite (TESS) detection of TOI-700 d and remove any remaining doubt that it is a genuine planet. We analyze the Spitzer light curve combined with the 11 sectors of TESS observations and a transit of TOI-700 c from the LCOGT network to determine the full system parameters. Although studying the atmosphere of TOI-700 d is not likely feasible with upcoming facilities, it may be possible to measure the mass of TOI-700 d using state-of-the-art radial velocity (RV) instruments (expected RV semiamplitude of ~70 cm s^−1)."
MICHAEL MCCLEAN,Clinicopathological evaluation of chronic traumatic encephalopathy in players of American football,"IMPORTANCE: Players of American football may be at increased risk of long-term neurological conditions, particularly chronic traumatic encephalopathy (CTE). OBJECTIVE: To determine the neuropathological and clinical features of deceased football players with CTE. DESIGN, SETTING, AND PARTICIPANTS: Case series of 202 football players whose brains were donated for research. Neuropathological evaluations and retrospective telephone clinical assessments (including head trauma history) with informants were performed blinded. Online questionnaires ascertained athletic and military history. EXPOSURES: Participation in American football at any level of play. MAIN OUTCOMES AND MEASURES: Neuropathological diagnoses of neurodegenerative diseases, including CTE, based on defined diagnostic criteria; CTE neuropathological severity (stages I to IV or dichotomized into mild [stages I and II] and severe [stages III and IV]); informant-reported athletic history and, for players who died in 2014 or later, clinical presentation, including behavior, mood, and cognitive symptoms and dementia. RESULTS: Among 202 deceased former football players (median age at death, 66 years [interquartile range, 47-76 years]), CTE was neuropathologically diagnosed in 177 players (87%; median age at death, 67 years [interquartile range, 52-77 years]; mean years of football participation, 15.1 [SD, 5.2]), including 0 of 2 pre–high school, 3 of 14 high school (21%), 48 of 53 college (91%), 9 of 14 semiprofessional (64%), 7 of 8 Canadian Football League (88%), and 110 of 111 National Football League (99%) players. Neuropathological severity of CTE was distributed across the highest level of play, with all 3 former high school players having mild pathology and the majority of former college (27 [56%]), semiprofessional (5 [56%]), and professional (101 [86%]) players having severe pathology. Among 27 participants with mild CTE pathology, 26 (96%) had behavioral or mood symptoms or both, 23 (85%) had cognitive symptoms, and 9 (33%) had signs of dementia. Among 84 participants with severe CTE pathology, 75 (89%) had behavioral or mood symptoms or both, 80 (95%) had cognitive symptoms, and 71 (85%) had signs of dementia. CONCLUSIONS AND RELEVANCE: In a convenience sample of deceased football players who donated their brains for research, a high proportion had neuropathological evidence of CTE, suggesting that CTE may be related to prior participation in football."
MICHAEL MCCLEAN,Diet Contributes Significantly to the Body Burden of PBDEs in the General U.S. Population,"BACKGROUND. Exposure of the U.S. population to polybrominated diphenyl ethers (PBDEs) is thought to be via exposure to dust and diet. However, little work has been done to empirically link body burdens of these compounds to either route of exposure. OBJECTIVES. The primary goal of this research was to evaluate the dietary contribution to PBDE body burdens in the United States by linking serum levels to food intake. METHODS. We used two dietary instruments-a 24-hr food recall (24FR) and a 1-year food frequency questionnaire (FFQ)-to examine food intake among participants of the 2003-2004 National Health and Nutrition Examination Survey. We regressed serum concentrations of five PBDEs (BDE congeners 28, 47, 99, 100, and 153) and their sum (ΣPBDE) against diet variables while adjusting for age, sex, race/ethnicity, income, and body mass index. RESULTS. ΣPBDE serum concentrations among vegetarians were 23% (p = 0.006) and 27% (p = 0.009) lower than among omnivores for 24FR and 1-year FFQ, respectively. Serum levels of five PBDE congeners were associated with consumption of poultry fat: Low, medium, and high intake corresponded to geometric mean ΣPBDE concentrations of 40.6, 41.9, and 48.3 ng/g lipid, respectively (p = 0.0005). We observed similar trends for red meat fat, which were statistically significant for BDE-100 and BDE-153. No association was observed between serum PBDEs and consumption of dairy or fish. Results were similar for both dietary instruments but were more robust using 24FR. CONCLUSIONS. Intake of contaminated poultry and red meat contributes significantly to PBDE body burdens in the United States."
MICHAEL MCCLEAN,Participant experiences in a breastmilk biomonitoring study: a qualitative assessment,"BACKGROUND: Biomonitoring studies can provide information about individual and population-wide exposure. However they must be designed in a way that protects the rights and welfare of participants. This descriptive qualitative study was conducted as a follow-up to a breastmilk biomonitoring study. The primary objectives were to assess participants' experiences in the study, including the report-back of individual body burden results, and to determine if participation in the study negatively affected breastfeeding rates or duration. METHODS: Participants of the Greater Boston PBDE Breastmilk Biomonitoring Study were contacted and asked about their experiences in the study: the impact of study recruitment materials on attitudes towards breastfeeding; if participants had wanted individual biomonitoring results; if the protocol by which individual results were distributed met participants' needs; and the impact of individual results on attitudes towards breastfeeding. RESULTS: No participants reported reducing the duration of breastfeeding because of the biomonitoring study, but some responses suggested that breastmilk biomonitoring studies have the potential to raise anxieties about breastfeeding. Almost all participants wished to obtain individual results. Although several reported some concern about individual body burden, none reported reducing the duration of breastfeeding because of biomonitoring results. The study literature and report-back method were found to mitigate potential negative impacts. CONCLUSION: Biomonitoring study design, including clear communication about the benefits of breastfeeding and the manner in which individual results are distributed, can prevent negative impacts of biomonitoring on breastfeeding. Adoption of more specific standards for biomonitoring studies and continued study of risk communication issues related to biomonitoring will help protect participants from harm."
MICHAEL MCCLEAN,Genetic and Epigenetic Somatic Alterations in Head and Neck Squamous Cell Carcinomas Are Globally Coordinated but Not Locally Targeted,"BACKGROUND. Solid tumors, including head and neck squamous cell carcinomas (HNSCC), arise as a result of genetic and epigenetic alterations in a sustained stress environment. Little work has been done that simultaneously examines the spectrum of both types of changes in human tumors on a genome-wide scale and results so far have been limited and mixed. Since it has been hypothesized that epigenetic alterations may act by providing the second carcinogenic hit in gene silencing, we sought to identify genome-wide DNA copy number alterations and CpG dinucleotide methylation events and examine the global/local relationships between these types of alterations in HNSCC. METHODOLOGY/PRINCIPAL FINDINGS. We have extended a prior analysis of 1,413 cancer-associated loci for epigenetic changes in HNSCC by integrating DNA copy number alterations, measured at 500,000 polymorphic loci, in a case series of 19 primary HNSCC tumors. We have previously demonstrated that local copy number does not bias methylation measurements in this array platform. Importantly, we found that the global pattern of copy number alterations in these tumors was significantly associated with tumor methylation profiles (p<0.002). However at the local level, gene promoter regions did not exhibit a correlation between copy number and methylation (lowest q=0.3), and the spectrum of genes affected by each type of alteration was unique. CONCLUSION/SIGNIFICANCE. This work, using a novel and robust statistical approach demonstrates that, although a ""second hit"" mechanism is not likely the predominant mode of action for epigenetic dysregulation in cancer, the patterns of methylation events are associated with the patterns of allele loss. Our work further highlights the utility of integrative genomics approaches in exploring the driving somatic alterations in solid tumors."
MICHAEL MCCLEAN,Heat stress and heat strain among outdoor workers in El Salvador and Nicaragua,"BACKGROUND: There is growing attention on occupational heat stress in Central America, as workers in this region are affected by a unique form of chronic kidney disease. Previous studies have examined wet bulb globe temperatures and estimated metabolic rates to assess heat stress, but there are limited data characterizing heat strain among these workers. OBJECTIVE: The aims were to characterize heat stress and heat strain and examine whether job task, break duration, hydration practices, and kidney function were associated with heat strain. METHODS: We used data from the MesoAmerican Nephropathy Occupational Study, a cohort of 569 outdoor workers in El Salvador and Nicaragua who underwent workplace exposure monitoring, including continuous measurement of core body temperature (Tc), heart rate (HR), physical activity, and wet bulb globe temperature (WBGT), over the course of three days in January 2018 - May 2018. Participants represented five industries: sugarcane, corn, plantain, brickmaking, and construction. RESULTS: Median WBGTs were relatively high (>27 °C) at most sites, particularly when work shifts spanned the afternoon hours (e.g., 29.2 °C among plantain workers). Sugarcane workers, especially cane cutters in both countries and Nicaraguan agrichemical applicators, had the highest estimated metabolic rates (medians: 299-318 kcal/hr). Most workers spent little time on break (<10% of the shift), as determined by physical activity data. Overall, sugarcane workers-particularly those in Nicaragua-experienced the highest Tc and HR values. However, a few workers in other industries reached high Tc (>39 °C) as well. Impaired kidney function (estimated glomerular filtration rate <90 mL/min/1.73 m2) was associated with higher Tc and HR values, even after adjustment. SIGNIFICANCE: This is the largest study to-date examining heat stress and strain among outdoor workers in Central America. Workers at sugar companies regularly experienced Tc > 38°C (76.9% of monitored person-days at Nicaraguan companies; 46.5% at Salvadoran companies). Workers with impaired kidney function had higher measures of Tc and HR. IMPACT STATEMENT: This study examined levels of occupational heat stress and heat strain experienced among outdoor workers in five industries in El Salvador and Nicaragua. We characterized heat stress using wet bulb globe temperatures and estimated metabolic rate and heat strain using core body temperature and heart rate. Sugarcane workers, particularly cane cutters and Nicaraguan agrichemical applicators, performed more strenuous work and experienced greater levels of heat strain. Impaired kidney function was associated with higher heart rates and core body temperatures."
MAURICE LEE,"BMQ : Boston medical quarterly: v. 7, no. 1-4",
MAURICE LEE,"Genre, science, and ‘Hans Pfaall'","This essay moves beyond questions of source study and reception to show how “The Unparalleled Adventure of One Hans Pfaall” mixes genres of satire and realism. In doing so, Poe not only participates in the early development of science fiction but also explores emerging relationships between scientific and literary discourses during the nineteenth-century print revolution, which made it difficult to distinguish between fictions and facts. “Hans Pfaall” selfconsciously dramatizes through stylistic turbulence how knowledge is generically produced within unruly media ecologies and is thus epistemologically unstable. In this sense, the story— not in spite but precisely because of its generic and aesthetic inconsistencies—can be regarded less as an unsuccessful hoax and more as a narrative about the dynamics of writing and reading fiction under conditions of doubt."
MAURICE LEE,"""The Office of Literature""",
MAURICE LEE,"“Globalism, transparency, comprehensiveness”",
DANIEL SEGRE,Inferring microbial co-occurrence networks from amplicon data: a systematic evaluation,"Microbes commonly organize into communities consisting of hundreds of species involved in complex interactions with each other. 16S ribosomal RNA (16S rRNA) amplicon profiling provides snapshots that reveal the phylogenies and abundance profiles of these microbial communities. These snapshots, when collected from multiple samples, can reveal the co-occurrence of microbes, providing a glimpse into the network of associations in these communities. However, the inference of networks from 16S data involves numerous steps, each requiring specific tools and parameter choices. Moreover, the extent to which these steps affect the final network is still unclear. In this study, we perform a meticulous analysis of each step of a pipeline that can convert 16S sequencing data into a network of microbial associations. Through this process, we map how different choices of algorithms and parameters affect the co-occurrence network and identify the steps that contribute substantially to the variance. We further determine the tools and parameters that generate robust co-occurrence networks and develop consensus network algorithms based on benchmarks with mock and synthetic data sets. The Microbial Co-occurrence Network Explorer, or MiCoNE (available at https://github.com/segrelab/MiCoNE) follows these default tools and parameters and can help explore the outcome of these combinations of choices on the inferred networks. We envisage that this pipeline could be used for integrating multiple data sets and generating comparative analyses and consensus networks that can guide our understanding of microbial community assembly in different biomes. IMPORTANCE Mapping the interrelationships between different species in a microbial community is important for understanding and controlling their structure and function. The surge in the high-throughput sequencing of microbial communities has led to the creation of thousands of data sets containing information about microbial abundances. These abundances can be transformed into co-occurrence networks, providing a glimpse into the associations within microbiomes. However, processing these data sets to obtain co-occurrence information relies on several complex steps, each of which involves numerous choices of tools and corresponding parameters. These multiple options pose questions about the robustness and uniqueness of the inferred networks. In this study, we address this workflow and provide a systematic analysis of how these choices of tools affect the final network and guidelines on appropriate tool selection for a particular data set. We also develop a consensus network algorithm that helps generate more robust co-occurrence networks based on benchmark synthetic data sets."
DANIEL SEGRE,Species interactions differ in their genetic robustness,"Conflict and cooperation between bacterial species drive the composition and function of microbial communities. Stability of these emergent properties will be influenced by the degree to which species' interactions are robust to genetic perturbations. We use genome-scale metabolic modeling to computationally analyze the impact of genetic changes when Escherichia coli and Salmonella enterica compete, or cooperate. We systematically knocked out in silico each reaction in the metabolic network of E. coli to construct all 2583 mutant stoichiometric models. Then, using a recently developed multi-scale computational framework, we simulated the growth of each mutant E. coli in the presence of S. enterica. The type of interaction between species was set by modulating the initial metabolites present in the environment. We found that the community was most robust to genetic perturbations when the organisms were cooperating. Species ratios were more stable in the cooperative community, and community biomass had equal variance in the two contexts. Additionally, the number of mutations that have a substantial effect is lower when the species cooperate than when they are competing. In contrast, when mutations were added to the S. enterica network the system was more robust when the bacteria were competing. These results highlight the utility of connecting metabolic mechanisms and studies of ecological stability. Cooperation and conflict alter the connection between genetic changes and properties that emerge at higher levels of biological organization."
DANIEL SEGRE,"Twenty years of ""Lipid World"": a fertile partnership with David Deamer","""The Lipid World"" was published in 2001, stemming from a highly effective collaboration with David Deamer during a sabbatical year 20 years ago at the Weizmann Institute of Science in Israel. The present review paper highlights the benefits of this scientific interaction and assesses the impact of the lipid world paper on the present understanding of the possible roles of amphiphiles and their assemblies in the origin of life. The lipid world is defined as a putative stage in the progression towards life's origin, during which diverse amphiphiles or other spontaneously aggregating small molecules could have concurrently played multiple key roles, including compartment formation, the appearance of mutually catalytic networks, molecular information processing, and the rise of collective self-reproduction and compositional inheritance. This review brings back into a broader perspective some key points originally made in the lipid world paper, stressing the distinction between the widely accepted role of lipids in forming compartments and their expanded capacities as delineated above. In the light of recent advancements, we discussed the topical relevance of the lipid worldview as an alternative to broadly accepted scenarios, and the need for further experimental and computer-based validation of the feasibility and implications of the individual attributes of this point of view. Finally, we point to possible avenues for exploring transition paths from small molecule-based noncovalent structures to more complex biopolymer-containing proto-cellular systems."
DANIEL SEGRE,Emergent simplicity in microbial community assembly,"A major unresolved question in microbiome research is whether the complex taxonomic architectures observed in surveys of natural communities can be explained and predicted by fundamental, quantitative principles. Bridging theory and experiment is hampered by the multiplicity of ecological processes that simultaneously affect community assembly in natural ecosystems. We addressed this challenge by monitoring the assembly of hundreds of soil- and plant-derived microbiomes in well-controlled minimal synthetic media. Both the community-level function and the coarse-grained taxonomy of the resulting communities are highly predictable and governed by nutrient availability, despite substantial species variability. By generalizing classical ecological models to include widespread nonspecific cross-feeding, we show that these features are all emergent properties of the assembly of large microbial communities, explaining their ubiquity in natural microbiomes."
DANIEL SEGRE,Model-driven analysis of experimentally determined growth phenotypes for 465 yeast gene deletion mutants under 16 different conditions,"BACKGROUND: Understanding the response of complex biochemical networks to genetic perturbations and environmental variability is a fundamental challenge in biology. Integration of high-throughput experimental assays and genome-scale computational methods is likely to produce insight otherwise unreachable, but specific examples of such integration have only begun to be explored. RESULTS: In this study, we measured growth phenotypes of 465 Saccharomyces cerevisiae gene deletion mutants under 16 metabolically relevant conditions and integrated them with the corresponding flux balance model predictions. We first used discordance between experimental results and model predictions to guide a stage of experimental refinement, which resulted in a significant improvement in the quality of the experimental data. Next, we used discordance still present in the refined experimental data to assess the reliability of yeast metabolism models under different conditions. In addition to estimating predictive capacity based on growth phenotypes, we sought to explain these discordances by examining predicted flux distributions visualized through a new, freely available platform. This analysis led to insight into the glycerol utilization pathway and the potential effects of metabolic shortcuts on model results. Finally, we used model predictions and experimental data to discriminate between alternative raffinose catabolism routes. CONCLUSIONS: Our study demonstrates how a new level of integration between high throughput measurements and flux balance model predictions can improve understanding of both experimental and computational results. The added value of a joint analysis is a more reliable platform for specific testing of biological hypotheses, such as the catabolic routes of different carbon sources."
DANIEL SEGRE,Signatures of arithmetic simplicity in metabolic network architecture,"Metabolic networks perform some of the most fundamental functions in living cells, including energy transduction and building block biosynthesis. While these are the best characterized networks in living systems, understanding their evolutionary history and complex wiring constitutes one of the most fascinating open questions in biology, intimately related to the enigma of life's origin itself. Is the evolution of metabolism subject to general principles, beyond the unpredictable accumulation of multiple historical accidents? Here we search for such principles by applying to an artificial chemical universe some of the methodologies developed for the study of genome scale models of cellular metabolism. In particular, we use metabolic flux constraint-based models to exhaustively search for artificial chemistry pathways that can optimally perform an array of elementary metabolic functions. Despite the simplicity of the model employed, we find that the ensuing pathways display a surprisingly rich set of properties, including the existence of autocatalytic cycles and hierarchical modules, the appearance of universally preferable metabolites and reactions, and a logarithmic trend of pathway length as a function of input/output molecule size. Some of these properties can be derived analytically, borrowing methods previously used in cryptography. In addition, by mapping biochemical networks onto a simplified carbon atom reaction backbone, we find that properties similar to those predicted for the artificial chemistry hold also for real metabolic networks. These findings suggest that optimality principles and arithmetic simplicity might lie beneath some aspects of biochemical complexity."
DANIEL SEGRE,Model-Driven Analysis of Experimentally Determined Growth Phenotypes for 465 Yeast Gene Deletion Mutants Under 16 Different Conditions,"An iterative approach that integrates high-throughput measurements of yeast deletion mutants and flux balance model predictions improves understanding of both experimental and computational results. BACKGROUND. Understanding the response of complex biochemical networks to genetic perturbations and environmental variability is a fundamental challenge in biology. Integration of high-throughput experimental assays and genome-scale computational methods is likely to produce insight otherwise unreachable, but specific examples of such integration have only begun to be explored. RESULTS. In this study, we measured growth phenotypes of 465 Saccharomyces cerevisiae gene deletion mutants under 16 metabolically relevant conditions and integrated them with the corresponding flux balance model predictions. We first used discordance between experimental results and model predictions to guide a stage of experimental refinement, which resulted in a significant improvement in the quality of the experimental data. Next, we used discordance still present in the refined experimental data to assess the reliability of yeast metabolism models under different conditions. In addition to estimating predictive capacity based on growth phenotypes, we sought to explain these discordances by examining predicted flux distributions visualized through a new, freely available platform. This analysis led to insight into the glycerol utilization pathway and the potential effects of metabolic shortcuts on model results. Finally, we used model predictions and experimental data to discriminate between alternative raffinose catabolism routes. CONCLUSIONS. Our study demonstrates how a new level of integration between high throughput measurements and flux balance model predictions can improve understanding of both experimental and computational results. The added value of a joint analysis is a more reliable platform for specific testing of biological hypotheses, such as the catabolic routes of different carbon sources."
DANIEL SEGRE,The dynamics of hybrid metabolic-genetic oscillators,"The synthetic construction of intracellular circuits is frequently hindered by a poor knowledge of appropriate kinetics and precise rate parameters. Here, we use generalized modeling (GM) to study the dynamical behavior of topological models of a family of hybrid metabolic-genetic circuits known as ""metabolators."" Under mild assumptions on the kinetics, we use GM to analytically prove that all explicit kinetic models which are topologically analogous to one such circuit, the ""core metabolator,"" cannot undergo Hopf bifurcations. Then, we examine more detailed models of the metabolator. Inspired by the experimental observation of a Hopf bifurcation in a synthetically constructed circuit related to the core metabolator, we apply GM to identify the critical components of the synthetically constructed metabolator which must be reintroduced in order to recover the Hopf bifurcation. Next, we study the dynamics of a re-wired version of the core metabolator, dubbed the ""reverse"" metabolator, and show that it exhibits a substantially richer set of dynamical behaviors, including both local and global oscillations. Prompted by the observation of relaxation oscillations in the reverse metabolator, we study the role that a separation of genetic and metabolic time scales may play in its dynamics, and find that widely separated time scales promote stability in the circuit. Our results illustrate a generic pipeline for vetting the potential success of a circuit design, simply by studying the dynamics of the corresponding generalized model."
DANIEL SEGRE,Costless metabolic secretions as drivers of interspecies interactions in microbial ecosystems,"Metabolic exchange mediates interactions among microbes, helping explain diversity in microbial communities. As these interactions often involve a fitness cost, it is unclear how stable cooperation can emerge. Here we use genome-scale metabolic models to investigate whether the release of “costless” metabolites (i.e. those that cause no fitness cost to the producer), can be a prominent driver of intermicrobial interactions. By performing over 2 million pairwise growth simulations of 24 species in a combinatorial assortment of environments, we identify a large space of metabolites that can be secreted without cost, thus generating ample cross-feeding opportunities. In addition to providing an atlas of putative interactions, we show that anoxic conditions can promote mutualisms by providing more opportunities for exchange of costless metabolites, resulting in an overrepresentation of stable ecological network motifs. These results may help identify interaction patterns in natural communities and inform the design of synthetic microbial consortia."
DANIEL SEGRE,Genome-driven evolutionary game theory helps understand the rise of metabolic interdependencies in microbial communities,"Metabolite exchanges in microbial communities give rise to ecological interactions that govern ecosystem diversity and stability. It is unclear, however, how the rise of these interactions varies across metabolites and organisms. Here we address this question by integrating genome-scale models of metabolism with evolutionary game theory. Specifically, we use microbial fitness values estimated by metabolic models to infer evolutionarily stable interactions in multi-species microbial “games”. We first validate our approach using a well-characterized yeast cheater-cooperator system. We next perform over 80,000 in silico experiments to infer how metabolic interdependencies mediated by amino acid leakage in Escherichia coli vary across 189 amino acid pairs. While most pairs display shared patterns of inter-species interactions, multiple deviations are caused by pleiotropy and epistasis in metabolism. Furthermore, simulated invasion experiments reveal possible paths to obligate cross-feeding. Our study provides genomically driven insight into the rise of ecological interactions, with implications for microbiome research and synthetic ecology."
DANIEL SEGRE,"Upon accounting for the impact of isoenzyme loss, gene deletion costs anticorrelate with their evolutionary rates","System-level metabolic network models enable the computation of growth and metabolic phenotypes from an organism’s genome. In particular, flux balance approaches have been used to estimate the contribution of individual metabolic genes to organismal fitness, offering the opportunity to test whether such contributions carry information about the evolutionary pressure on the corresponding genes. Previous failure to identify the expected negative correlation between such computed gene-loss cost and sequence-derived evolutionary rates in Saccharomyces cerevisiae has been ascribed to a real biological gap between a gene’s fitness contribution to an organism “here and now” and the same gene’s historical importance as evidenced by its accumulated mutations over millions of years of evolution. Here we show that this negative correlation does exist, and can be exposed by revisiting a broadly employed assumption of flux balance models. In particular, we introduce a new metric that we call “function-loss cost”, which estimates the cost of a gene loss event as the total potential functional impairment caused by that loss. This new metric displays significant negative correlation with evolutionary rate, across several thousand minimal environments. We demonstrate that the improvement gained using function-loss cost over gene-loss cost is explained by replacing the base assumption that isoenzymes provide unlimited capacity for backup with the assumption that isoenzymes are completely non-redundant. We further show that this change of the assumption regarding isoenzymes increases the recall of epistatic interactions predicted by the flux balance model at the cost of a reduction in the precision of the predictions. In addition to suggesting that the gene-to-reaction mapping in genome-scale flux balance models should be used with caution, our analysis provides new evidence that evolutionary gene importance captures much more than strict essentiality."
DANIEL SEGRE,Mapping the landscape of metabolic goals of a cell,"Genome-scale flux balance models of metabolism provide testable predictions of all metabolic rates in an organism, by assuming that the cell is optimizing a metabolic goal known as the objective function. We introduce an efficient inverse flux balance analysis (invFBA) approach, based on linear programming duality, to characterize the space of possible objective functions compatible with measured fluxes. After testing our algorithm on simulated E. coli data and time-dependent S. oneidensis fluxes inferred from gene expression, we apply our inverse approach to flux measurements in long-term evolved E. coli strains, revealing objective functions that provide insight into metabolic adaptation trajectories."
DANIEL SEGRE,The quasi-steady state assumption in an enzymatically open system,"The quasi-steady state assumption (QSSA) forms the basis for rigorous mathematical justification of the Michaelis-Menten formalism commonly used in modeling a broad range of intracellular phenomena. A critical supposition of QSSA-based analyses is that the underlying biochemical reaction is enzymatically ""closed,"" so that free enzyme is neither added to nor removed from the reaction over the relevant time period. Yet there are multiple circumstances in living cells under which this assumption may not hold, e.g. during translation of genetic elements or metabolic regulatory events. Here we consider a modified version of the most basic enzyme-catalyzed reaction which incorporates enzyme input and removal. We extend the QSSA to this enzymatically ""open"" system, computing inner approximations to its dynamics, and we compare the behavior of the full open system, our approximations, and the closed system under broad range of kinetic parameters. We also derive conditions under which our new approximations are provably valid; numerical simulations demonstrate that our approximations remain quite accurate even when these conditions are not satisfied. Finally, we investigate the possibility of damped oscillatory behavior in the enzymatically open reaction."
DANIEL SEGRE,Remnants of an ancient metabolism without phosphate,"Phosphate is essential for all living systems, serving as a building block of genetic and metabolic machinery. However, it is unclear how phosphate could have assumed these central roles on primordial Earth, given its poor geochemical accessibility. We used systems biology approaches to explore the alternative hypothesis that a protometabolism could have emerged prior to the incorporation of phosphate. Surprisingly, we identified a cryptic phosphate-independent core metabolism producible from simple prebiotic compounds. This network is predicted to support the biosynthesis of a broad category of key biomolecules. Its enrichment for enzymes utilizing iron-sulfur clusters, and the fact that thermodynamic bottlenecks are more readily overcome by thioester rather than phosphate couplings, suggest that this network may constitute a ""metabolic fossil"" of an early phosphate-free nonenzymatic biochemistry. Our results corroborate and expand previous proposals that a putative thioester-based metabolism could have predated the incorporation of phosphate and an RNA-based genetic system. PAPERCLIP."
DANIEL SEGRE,Epistatic interaction maps relative to multiple metabolic phenotypes,"An epistatic interaction between two genes occurs when the phenotypic impact of one gene depends on another gene, often exposing a functional association between them. Due to experimental scalability and to evolutionary significance, abundant work has been focused on studying how epistasis affects cellular growth rate, most notably in yeast. However, epistasis likely influences many different phenotypes, affecting our capacity to understand cellular functions, biochemical networks adaptation, and genetic diseases. Despite its broad significance, the extent and nature of epistasis relative to different phenotypes remain fundamentally unexplored. Here we use genome-scale metabolic network modeling to investigate the extent and properties of epistatic interactions relative to multiple phenotypes. Specifically, using an experimentally refined stoichiometric model for Saccharomyces cerevisiae, we computed a three-dimensional matrix of epistatic interactions between any two enzyme gene deletions, with respect to all metabolic flux phenotypes. We found that the total number of epistatic interactions between enzymes increases rapidly as phenotypes are added, plateauing at approximately 80 phenotypes, to an overall connectivity that is roughly 8-fold larger than the one observed relative to growth alone. Looking at interactions across all phenotypes, we found that gene pairs interact incoherently relative to different phenotypes, i.e. antagonistically relative to some phenotypes and synergistically relative to others. Specific deletion-deletion-phenotype triplets can be explained metabolically, suggesting a highly informative role of multi-phenotype epistasis in mapping cellular functions. Finally, we found that genes involved in many interactions across multiple phenotypes are more highly expressed, evolve slower, and tend to be associated with diseases, indicating that the importance of genes is hidden in their total phenotypic impact. Our predictions indicate a pervasiveness of nonlinear effects in how genetic perturbations affect multiple metabolic phenotypes. The approaches and results reported could influence future efforts in understanding metabolic diseases and the role of biochemical regulation in the cell."
DANIEL SEGRE,BowSaw: inferring higher-order trait interactions associated with complex biological phenotypes,"Machine learning is helping the interpretation of biological complexity by enabling the inference and classification of cellular, organismal and ecological phenotypes based on large datasets, e.g. from genomic, transcriptomic and metagenomic analyses. A number of available algorithms can help search these datasets to uncover patterns associated with specific traits, including disease-related attributes. While, in many instances, treating an algorithm as a black box is sufficient, it is interesting to pursue an enhanced understanding of how system variables end up contributing to a specific output, as an avenue towards new mechanistic insight. Here we address this challenge through a suite of algorithms, named BowSaw, which takes advantage of the structure of a trained random forest algorithm to identify combinations of variables (“rules”) frequently used for classification. We first apply BowSaw to a simulated dataset, and show that the algorithm can accurately recover the sets of variables used to generate the phenotypes through complex Boolean rules, even under challenging noise levels. We next apply our method to data from the integrative Human Microbiome Project and find previously unreported high-order combinations of microbial taxa putatively associated with Crohn’s disease. By leveraging the structure of trees within a random forest, BowSaw provides a new way of using decision trees to generate testable biological hypotheses."
DANIEL SEGRE,Machine learning reveals missing edges and putative interaction mechanisms in microbial ecosystem networks,"Microbes affect each other’s growth in multiple, often elusive, ways. The ensuing interdependencies form complex networks, believed to reflect taxonomic composition as well as community-level functional properties and dynamics. The elucidation of these networks is often pursued by measuring pairwise interactions in coculture experiments. However, the combinatorial complexity precludes an exhaustive experimental analysis of pairwise interactions, even for moderately sized microbial communities. Here, we used a machine learning random forest approach to address this challenge. In particular, we show how partial knowledge of a microbial interaction network, combined with trait-level representations of individual microbial species, can provide accurate inference of missing edges in the network and putative mechanisms underlying the interactions. We applied our algorithm to three case studies: an experimentally mapped network of interactions between auxotrophic Escherichia coli strains, a community of soil microbes, and a large in silico network of metabolic interdependencies between 100 human gut-associated bacteria. For this last case, 5% of the network was sufficient to predict the remaining 95% with 80% accuracy, and the mechanistic hypotheses produced by the algorithm accurately reflected known metabolic exchanges. Our approach, broadly applicable to any microbial or other ecological network, may drive the discovery of new interactions and new molecular mechanisms, both for therapeutic interventions involving natural communities and for the rational design of synthetic consortia."
DANIEL SEGRE,Modern views of ancient metabolic networks,"Metabolism is a molecular, cellular, ecological and planetary phenomenon, whose fundamental principles are likely at the heart of what makes living matter different from inanimate one. Systems biology approaches developed for the quantitative analysis of metabolism at multiple scales can help understand metabolism's ancient history. In this review, we highlight work that uses network-level approaches to shed light on key innovations in ancient life, including the emergence of proto-metabolic networks, collective autocatalysis and bioenergetics coupling. Recent experiments and computational analyses have revealed new aspects of this ancient history, paving the way for the use of large datasets to further improve our understanding of life's principles and abiogenesis."
DANIEL SEGRE,Visualization of metabolic interaction networks in microbial communities using VisANT 5.0,"The complexity of metabolic networks in microbial communities poses an unresolved visualization and interpretation challenge. We address this challenge in the newly expanded version of a software tool for the analysis of biological networks, VisANT 5.0. We focus in particular on facilitating the visual exploration of metabolic interaction between microbes in a community, e.g. as predicted by COMETS (Computation of Microbial Ecosystems in Time and Space), a dynamic stoichiometric modeling framework. Using VisANT's unique metagraph implementation, we show how one can use VisANT 5.0 to explore different time-dependent ecosystem-level metabolic networks. In particular, we analyze the metabolic interaction network between two bacteria previously shown to display an obligate cross-feeding interdependency. In addition, we illustrate how a putative minimal gut microbiome community could be represented in our framework, making it possible to highlight interactions across multiple coexisting species. We envisage that the ""symbiotic layout"" of VisANT can be employed as a general tool for the analysis of metabolism in complex microbial communities as well as heterogeneous human tissues."
DANIEL SEGRE,Designing metabolic division of labor in microbial communities,"Microbes face a trade-off between being metabolically independent and relying on neighboring organisms for the supply of some essential metabolites. This balance of conflicting strategies affects microbial community structure and dynamics, with important implications for microbiome research and synthetic ecology. A “gedanken” (thought) experiment to investigate this trade-off would involve monitoring the rise of mutual dependence as the number of metabolic reactions allowed in an organism is increasingly constrained. The expectation is that below a certain number of reactions, no individual organism would be able to grow in isolation and cross-feeding partnerships and division of labor would emerge. We implemented this idealized experiment using in silico genome-scale models. In particular, we used mixed-integer linear programming to identify trade-off solutions in communities of Escherichia coli strains. The strategies that we found revealed a large space of opportunities in nuanced and nonintuitive metabolic division of labor, including, for example, splitting the tricarboxylic acid (TCA) cycle into two separate halves. The systematic computation of possible solutions in division of labor for 1-, 2-, and 3-strain consortia resulted in a rich and complex landscape. This landscape displayed a nonlinear boundary, indicating that the loss of an intracellular reaction was not necessarily compensated for by a single imported metabolite. Different regions in this landscape were associated with specific solutions and patterns of exchanged metabolites. Our approach also predicts the existence of regions in this landscape where independent bacteria are viable but are outcompeted by cross-feeding pairs, providing a possible incentive for the rise of division of labor."
DANIEL SEGRE,Metabolic proximity in the order of colonization of a microbial community,"Microbial biofilms are often composed of multiple bacterial species that accumulate by adhering to a surface and to each other. Biofilms can be resistant to antibiotics and physical stresses, posing unresolved challenges in the fight against infectious diseases. It has been suggested that early colonizers of certain biofilms could cause local environmental changes, favoring the aggregation of subsequent organisms. Here we ask whether the enzyme content of different microbes in a well-characterized dental biofilm can be used to predict their order of colonization. We define a metabolic distance between different species, based on the overlap in their enzyme content. We next use this metric to quantify the average metabolic distance between neighboring organisms in the biofilm. We find that this distance is significantly smaller than the one observed for a random choice of prokaryotes, probably reflecting the environmental constraints on metabolic function of the community. More surprisingly, this metabolic metric is able to discriminate between observed and randomized orders of colonization of the biofilm, with the observed orders displaying smaller metabolic distance than randomized ones. By complementing these results with the analysis of individual vs. joint metabolic networks, we find that the tendency towards minimal metabolic distance may be counter-balanced by a propensity to pair organisms with maximal joint potential for synergistic interactions. The trade-off between these two tendencies may create a “sweet spot” of optimal inter-organism distance, with possible broad implications for our understanding of microbial community organization."
DANIEL SEGRE,Flux imbalance analysis and the sensitivity of cellular growth to changes in metabolite pools,"Stoichiometric models of metabolism, such as flux balance analysis (FBA), are classically applied to predicting steady state rates - or fluxes - of metabolic reactions in genome-scale metabolic networks. Here we revisit the central assumption of FBA, i.e. that intracellular metabolites are at steady state, and show that deviations from flux balance (i.e. flux imbalances) are informative of some features of in vivo metabolite concentrations. Mathematically, the sensitivity of FBA to these flux imbalances is captured by a native feature of linear optimization, the dual problem, and its corresponding variables, known as shadow prices. First, using recently published data on chemostat growth of Saccharomyces cerevisae under different nutrient limitations, we show that shadow prices anticorrelate with experimentally measured degrees of growth limitation of intracellular metabolites. We next hypothesize that metabolites which are limiting for growth (and thus have very negative shadow price) cannot vary dramatically in an uncontrolled way, and must respond rapidly to perturbations. Using a collection of published datasets monitoring the time-dependent metabolomic response of Escherichia coli to carbon and nitrogen perturbations, we test this hypothesis and find that metabolites with negative shadow price indeed show lower temporal variation following a perturbation than metabolites with zero shadow price. Finally, we illustrate the broader applicability of flux imbalance analysis to other constraint-based methods. In particular, we explore the biological significance of shadow prices in a constraint-based method for integrating gene expression data with a stoichiometric model. In this case, shadow prices point to metabolites that should rise or drop in concentration in order to increase consistency between flux predictions and gene expression data. In general, these results suggest that the sensitivity of metabolic optima to violations of the steady state constraints carries biologically significant information on the processes that control intracellular metabolites in the cell."
DANIEL SEGRE,Emergent subpopulation behavior uncovered with a community dynamic metabolic model of Escherichia coli diauxic growth,"Microbes have adapted to greatly variable environments in order to survive both short-term perturbations and permanent changes. A classical and yet still actively studied example of adaptation to dynamic environments is the diauxic shift of Escherichia coli, in which cells grow on glucose until its exhaustion and then transition to using previously secreted acetate. Here we tested different hypotheses concerning the nature of this transition by using dynamic metabolic modeling. To reach this goal, we developed an open source modeling framework integrating dynamic models (ordinary differential equation systems) with structural models (metabolic networks) which can take into account the behavior of multiple subpopulations and smooth flux transitions between time points. We used this framework to model the diauxic shift, first with a single E. coli model whose metabolic state represents the overall population average and then with a community of two subpopulations, each growing exclusively on one carbon source (glucose or acetate). After introduction of an environment-dependent transition function that determined the balance between subpopulations, our model generated predictions that are in strong agreement with published data. Our results thus support recent experimental evidence that diauxie, rather than a coordinated metabolic shift, would be the emergent pattern of individual cells differentiating for optimal growth on different substrates. This work offers a new perspective on the use of dynamic metabolic modeling to investigate population heterogeneity dynamics. The proposed approach can easily be applied to other biological systems composed of metabolically distinct, interconverting subpopulations and could be extended to include single-cell-level stochasticity."
DANIEL SEGRE,Experiments and simulations on short chain fatty acid production in a colonic bacterial community,"Understanding how production of specific metabolites by gut microbes is modulated by interactions with surrounding species and by environmental nutrient availability is an important open challenge in microbiome research. As part of this endeavor, we explore interactions between F. prausnitzii, a major butyrate producer, and B. thetaiotaomicron, an acetate producer, under three different in vitro media conditions in monoculture and coculture. In silico Genome-scale dynamic flux balance analysis (dFBA) models of metabolism in the system using COMETS (Computation of Microbial Ecosystems in Time and Space) are also tested for explanatory, predictive and inferential power. Experimental findings indicate enhancement of butyrate production in coculture relative to F. prausnitzii monoculture but defy a simple model of monotonic increases in butyrate production as a function of acetate availability in the medium. Simulations recapitulate biomass production curves for monocultures and accurately predict the growth curve of coculture total biomass, using parameters learned from monocultures, suggesting that the model captures some aspects of how the two bacteria interact. However, a comparison of data and simulations for environmental acetate and butyrate changes suggest that the organisms adopt one of many possible metabolic strategies equivalent in terms of growth efficiency. Furthermore, the model seems not to capture subsequent shifts in metabolic activities observed experimentally under low-nutrient regimes. Some discrepancies can be explained by the multiplicity of possible fermentative states for F. prausnitzii. In general, these results provide valuable guidelines for design of future experiments aimed at better determining the mechanisms leading to enhanced butyrate in this ecosystem."
DANIEL SEGRE,The thermodynamic landscape of carbon redox biochemistry,"Redox biochemistry plays a key role in the transduction of chemical energy in all living systems. Observed redox reactions in metabolic networks represent only a minuscule fraction of the space of all possible redox reactions. Here we ask what distinguishes observed, natural redox biochemistry from the space of all possible redox reactions between natural and non-natural compounds. We generate the set of all possible biochemical redox reactions involving linear chain molecules with a fixed numbers of carbon atoms. Using cheminformatics and quantum chemistry tools we analyze the physicochemical and thermodynamic properties of natural and non-natural compounds and reactions. We find that among all compounds, aldose sugars are the ones with the highest possible number of connections (reductions and oxidations) to other molecules. Natural metabolites are significantly enriched in carboxylic acid functional groups and depleted in carbonyls, and have significantly higher solubilities than non-natural compounds. Upon constructing a thermodynamic landscape for the full set of reactions as a function of pH and of steady-state redox cofactor potential, we find that, over this whole range of conditions, natural metabolites have significantly lower energies than the non-natural compounds. For the set of 4-carbon compounds, we generate a Pourbaix phase diagram to determine which metabolites are local energetic minima in the landscape as a function of pH and redox potential. Our results suggest that, across a set of conditions, succinate and butyrate are local minima and would thus tend to accumulate at equilibrium. Our work suggests that metabolic compounds could have been selected for thermodynamic stability, and yields insight into thermodynamic and design principles governing nature’s metabolic redox reactions."
DANIEL SEGRE,Optimality of extracellular enzyme production and activity in dynamic flux balance modeling,"In microbial communities, many vital metabolic functions, including the degradation of cellulose, proteins and other complex macromolecules, are carried out by costly, extracellularly secreted enzymes. While significant effort has been dedicated to analyzing genome-scale metabolic networks for individual microbes and communities, little is known about the interplay between global allocation of metabolic resources in the cell and extracellular enzyme secretion and activity. Here we introduce a method for modeling the secretion and catalytic functions of extracellular enzymes using dynamic flux balance analysis. This new addition, implemented within COMETS (Computation Of Microbial Ecosystems in Time and Space), simulates the costly production and secretion of enzymes and their diffusion and activity throughout the environment, independent of the producing organism. After tuning our model based on data for a Saccharomyces cerevisiae strain engineered to produce exogenous cellulases, we explored the dynamics of the system at different cellulose concentrations and enzyme production rates. We found that there are distinct rates of constitutive enzyme secretion which maximize either growth rate or biomass yield. These optimal rates are strongly dependent on enzyme kinetic properties and environmental conditions, including the amount of cellulose substrate available. Our framework will facilitate the development of more realistic simulations of microbial community dynamics within environments rich in complex macromolecules, with applications in the study of soil and plant-associated ecosystems, and other natural and engineered microbiomes."
DANIEL SEGRE,Quantifying biosynthetic network robustness across the human oral microbiome,"Metabolic interactions, such as cross-feeding, play a prominent role in microbial communitystructure. For example, they may underlie the ubiquity of uncultivated microorganisms. We investigated this phenomenon in the human oral microbiome, by analyzing microbial metabolic networks derived from sequenced genomes. Specifically, we devised a probabilistic biosynthetic network robustness metric that describes the chance that an organism could produce a given metabolite, and used it to assemble a comprehensive atlas of biosynthetic capabilities for 88 metabolites across 456 human oral microbiome strains. A cluster of organisms characterized by reduced biosynthetic capabilities stood out within this atlas. This cluster included several uncultivated taxa and three recently co-cultured Saccharibacteria (TM7) phylum species. Comparison across strains also allowed us to systematically identify specific putative metabolic interdependences between organisms. Our method, which provides a new way of converting annotated genomes into metabolic predictions, is easily extendible to other microbial communities and metabolic products."
DANIEL SEGRE,COMBREX: A Project to Accelerate the Functional Annotation of Prokaryotic Genomes,COMBREX (http://combrex.bu.edu) is a project to increase the speed of the functional annotation of new bacterial and archaeal genomes. It consists of a database of functional predictions produced by computational biologists and a mechanism for experimental biochemists to bid for the validation of those predictions. Small grants are available to support successful bids.
DANIEL SEGRE,Environments that Induce Synthetic Microbial Ecosystems,"Interactions between microbial species are sometimes mediated by the exchange of small molecules, secreted by one species and metabolized by another. Both one-way (commensal) and two-way (mutualistic) interactions may contribute to complex networks of interdependencies. Understanding these interactions constitutes an open challenge in microbial ecology, with applications ranging from the human microbiome to environmental sustainability. In parallel to natural communities, it is possible to explore interactions in artificial microbial ecosystems, e.g. pairs of genetically engineered mutualistic strains. Here we computationally generate artificial microbial ecosystems without re-engineering the microbes themselves, but rather by predicting their growth on appropriately designed media. We use genome-scale stoichiometric models of metabolism to identify media that can sustain growth for a pair of species, but fail to do so for one or both individual species, thereby inducing putative symbiotic interactions. We first tested our approach on two previously studied mutualistic pairs, and on a pair of highly curated model organisms, showing that our algorithms successfully recapitulate known interactions, robustly predict new ones, and provide novel insight on exchanged molecules. We then applied our method to all possible pairs of seven microbial species, and found that it is always possible to identify putative media that induce commensalism or mutualism. Our analysis also suggests that symbiotic interactions may arise more readily through environmental fluctuations than genetic modifications. We envision that our approach will help generate microbe-microbe interaction maps useful for understanding microbial consortia dynamics and evolution, and for exploring the full potential of natural metabolic pathways for metabolic engineering applications. Author Summary Microbial metabolism affects biogeochemical cycles and human health. In most natural environments, multiple microbial species interact with each other, forming complex ecosystems whose properties are poorly understood. In an effort to understand inter-microbial interactions, and to explore new metabolic engineering avenues, researchers have started building artificial microbial ecosystems, e.g. pairs of genetically engineered strains that require each other for survival. Here we computationally explore the possibility of creating artificial microbial ecosystems without re-engineering the microbes themselves, but rather by manipulating the environment in which they grow. Specifically, using the framework of flux balance analysis, we predict environments in which either one or both microbes in a pair would not be able to grow without the other, inducing commensal (one-way) or mutualistic (two-way) interactions, respectively. Our algorithms can successfully recapitulate known inter-microbial interactions, and predict millions of new ones across any pair amongst different microbial species. Surprisingly, we find that it is always possible to identify conditions that induce mutualistic or commensal interactions between any two species. Hence, our method should help in mapping naturally occurring microbe-microbe interactions, and in engineering new ones through a novel, environment-driven branch of synthetic ecology."
DANIEL SEGRE,Signatures of Arithmetic Simplicity in Metabolic Network Architecture,"Metabolic networks perform some of the most fundamental functions in living cells, including energy transduction and building block biosynthesis. While these are the best characterized networks in living systems, understanding their evolutionary history and complex wiring constitutes one of the most fascinating open questions in biology, intimately related to the enigma of life's origin itself. Is the evolution of metabolism subject to general principles, beyond the unpredictable accumulation of multiple historical accidents? Here we search for such principles by applying to an artificial chemical universe some of the methodologies developed for the study of genome scale models of cellular metabolism. In particular, we use metabolic flux constraint-based models to exhaustively search for artificial chemistry pathways that can optimally perform an array of elementary metabolic functions. Despite the simplicity of the model employed, we find that the ensuing pathways display a surprisingly rich set of properties, including the existence of autocatalytic cycles and hierarchical modules, the appearance of universally preferable metabolites and reactions, and a logarithmic trend of pathway length as a function of input/output molecule size. Some of these properties can be derived analytically, borrowing methods previously used in cryptography. In addition, by mapping biochemical networks onto a simplified carbon atom reaction backbone, we find that properties similar to those predicted for the artificial chemistry hold also for real metabolic networks. These findings suggest that optimality principles and arithmetic simplicity might lie beneath some aspects of biochemical complexity. Author Summary Metabolism is the network of biochemical reactions that transforms available resources (""inputs"") into energy currency and building blocks (""outputs""). Different organisms have different assortments of metabolic pathways and input/output requirements, reflecting their adaptation to specific environments, and to specific strategies for reproduction and survival. Here we ask whether, beneath the intricate wiring of these networks, it is possible to discern signatures of optimal (i.e., shortest and maximally efficient) pathway architectures. A systematic search for such optimal pathways between all possible pairs of input and output molecules in real organic chemistry is computationally intractable. However, we can implement such a search in a simple artificial chemistry, which roughly resembles a single atom (e.g., carbon) version of real biochemistry. We find that optimal pathways in our idealized chemistry display a logarithmic dependence of pathway length on input/output molecule size. They also display recurring topologies, including autocatalytic cycles reminiscent of ancient and highly conserved cores of real biochemistry. Finally, across all optimal pathways, we identify universally important metabolites and reactions, as well as a characteristic distribution of reaction utilization. Similar features can be observed in real metabolic networks, suggesting that arithmetic simplicity may lie beneath some aspects of biochemical complexity."
DANIEL SEGRE,Microbial carbon use efficiency predicted from genome-scale metabolic models,"Respiration by soil bacteria and fungi is one of the largest fluxes of carbon (C) from the land surface. Although this flux is a direct product of microbial metabolism, controls over metabolism and their responses to global change are a major uncertainty in the global C cycle. Here, we explore an in silico approach to predict bacterial C-use efficiency (CUE) for over 200 species using genome-specific constraint-based metabolic modeling. We find that potential CUE averages 0.62 ± 0.17 with a range of 0.22 to 0.98 across taxa and phylogenetic structuring at the subphylum levels. Potential CUE is negatively correlated with genome size, while taxa with larger genomes are able to access a wider variety of C substrates. Incorporating the range of CUE values reported here into a next-generation model of soil biogeochemistry suggests that these differences in physiology across microbial taxa can feed back on soil-C cycling."
DANIEL SEGRE,Environmental boundary conditions for the origin of life converge to an organo-sulfur metabolism,"It has been suggested that a deep memory of early life is hidden in the architecture of metabolic networks, whose reactions could have been catalyzed by small molecules or minerals before genetically encoded enzymes. A major challenge in unravelling these early steps is assessing the plausibility of a connected, thermodynamically consistent proto-metabolism under different geochemical conditions, which are still surrounded by high uncertainty. Here we combine network-based algorithms with physico-chemical constraints on chemical reaction networks to systematically show how different combinations of parameters (temperature, pH, redox potential and availability of molecular precursors) could have affected the evolution of a proto-metabolism. Our analysis of possible trajectories indicates that a subset of boundary conditions converges to an organo-sulfur-based proto-metabolic network fuelled by a thioester- and redox-driven variant of the reductive tricarboxylic acid cycle that is capable of producing lipids and keto acids. Surprisingly, environmental sources of fixed nitrogen and low-potential electron donors are not necessary for the earliest phases of biochemical evolution. We use one of these networks to build a steady-state dynamical metabolic model of a protocell, and find that different combinations of carbon sources and electron donors can support the continuous production of a minimal ancient 'biomass' composed of putative early biopolymers and fatty acids."
DANIEL SEGRE,A multidimensional perspective on microbial interactions,"Beyond being simply positive or negative, beneficial or inhibitory, microbial interactions can involve a diverse set of mechanisms, dependencies and dynamical properties. These more nuanced features have been described in great detail for some specific types of interactions, (e.g. pairwise metabolic cross-feeding, quorum sensing or antibiotic killing), often with the use of quantitative measurements and insight derived from modeling. With a growing understanding of the composition and dynamics of complex microbial communities for human health and other applications, we face the challenge of integrating information about these different interactions into comprehensive quantitative frameworks. Here, we review the literature on a wide set of microbial interactions, and explore the potential value of a formal categorization based on multidimensional vectors of attributes. We propose that such an encoding can facilitate systematic, direct comparisons of interaction mechanisms and dependencies, and we discuss the relevance of an atlas of interactions for future modeling and rational design efforts."
DANIEL SEGRE,BowSaw: inferring higher-order trait interactions associated with complex biological phenotypes,"Machine learning is helping the interpretation of biological complexity by enabling the inference and classification of cellular, organismal and ecological phenotypes based on large datasets, e.g., from genomic, transcriptomic and metagenomic analyses. A number of available algorithms can help search these datasets to uncover patterns associated with specific traits, including disease-related attributes. While, in many instances, treating an algorithm as a black box is sufficient, it is interesting to pursue an enhanced understanding of how system variables end up contributing to a specific output, as an avenue toward new mechanistic insight. Here we address this challenge through a suite of algorithms, named BowSaw, which takes advantage of the structure of a trained random forest algorithm to identify combinations of variables (""rules"") frequently used for classification. We first apply BowSaw to a simulated dataset and show that the algorithm can accurately recover the sets of variables used to generate the phenotypes through complex Boolean rules, even under challenging noise levels. We next apply our method to data from the integrative Human Microbiome Project and find previously unreported high-order combinations of microbial taxa putatively associated with Crohn's disease. By leveraging the structure of trees within a random forest, BowSaw provides a new way of using decision trees to generate testable biological hypotheses."
DANIEL SEGRE,Metabolic network percolation quantifies biosynthetic capabilities across the human oral microbiome,"The biosynthetic capabilities of microbes underlie their growth and interactions, playing a prominent role in microbial community structure. For large, diverse microbial communities, prediction of these capabilities is limited by uncertainty about metabolic functions and environmental conditions. To address this challenge, we propose a probabilistic method, inspired by percolation theory, to computationally quantify how robustly a genome-derived metabolic network produces a given set of metabolites under an ensemble of variable environments. We used this method to compile an atlas of predicted biosynthetic capabilities for 97 metabolites across 456 human oral microbes. This atlas captures taxonomically-related trends in biomass composition, and makes it possible to estimate inter-microbial metabolic distances that correlate with microbial co-occurrences. We also found a distinct cluster of fastidious/uncultivated taxa, including several Saccharibacteria (TM7) species, characterized by their abundant metabolic deficiencies. By embracing uncertainty, our approach can be broadly applied to understanding metabolic interactions in complex microbial ecosystems."
DANIEL SEGRE,Construction and modeling of a coculture microplate for real-time measurement of microbial interactions,"The dynamic structures of microbial communities emerge from the complex network of interactions between their constituent microorganisms. Quantitative measurements of these interactions are important for understanding and engineering ecosystem structure. Here, we present the development and application of the BioMe plate, a redesigned microplate device in which pairs of wells are separated by porous membranes. BioMe facilitates the measurement of dynamic microbial interactions and integrates easily with standard laboratory equipment. We first applied BioMe to recapitulate recently characterized, natural symbiotic interactions between bacteria isolated from the Drosophila melanogaster gut microbiome. Specifically, the BioMe plate allowed us to observe the benefit provided by two Lactobacillus strains to an Acetobacter strain. We next explored the use of BioMe to gain quantitative insight into the engineered obligate syntrophic interaction between a pair of Escherichia coli amino acid auxotrophs. We integrated experimental observations with a mechanistic computational model to quantify key parameters associated with this syntrophic interaction, including metabolite secretion and diffusion rates. This model also allowed us to explain the slow growth observed for auxotrophs growing in adjacent wells by demonstrating that, under the relevant range of parameters, local exchange between auxotrophs is essential for efficient growth. The BioMe plate provides a scalable and flexible approach for the study of dynamic microbial interactions. IMPORTANCE Microbial communities participate in many essential processes from biogeochemical cycles to the maintenance of human health. The structure and functions of these communities are dynamic properties that depend on poorly understood interactions among different species. Unraveling these interactions is therefore a crucial step toward understanding natural microbiota and engineering artificial ones. Microbial interactions have been difficult to measure directly, largely due to limitations of existing methods to disentangle the contribution of different organisms in mixed cocultures. To overcome these limitations, we developed the BioMe plate, a custom microplate-based device that enables direct measurement of microbial interactions, by detecting the abundance of segregated populations of microbes that can exchange small molecules through a membrane. We demonstrated the possible application of the BioMe plate for studying both natural and artificial consortia. BioMe is a scalable and accessible platform that can be used to broadly characterize microbial interactions mediated by diffusible molecules."
DANIEL SEGRE,Growth instabilities shape morphology and genetic diversity of microbial colonies,"Cellular populations assume an incredible variety of shapes ranging from circular molds to irregular tumors. While we understand many of the mechanisms responsible for these spatial patterns, little is known about how the shape of a population influences its ecology and evolution. Here, we investigate this relationship in the context of microbial colonies grown on hard agar plates. This a well-studied system that exhibits a transition from smooth circular disks to more irregular and rugged shapes as either the nutrient concentration or cellular motility is decreased. Starting from a mechanistic model of colony growth, we identify two dimensionless quantities that determine how morphology and genetic diversity of the population depend on the model parameters. Our simulations further reveal that population dynamics cannot be accurately described by the commonly-used surface growth models. Instead, one has to explicitly account for the emergent growth instabilities and demographic fluctuations. Overall, our work links together environmental conditions, colony morphology, and evolution. This link is essential for a rational design of concrete, biophysical perturbations to steer evolution in the desired direction."
DANIEL SEGRE,COMBREX: a project to accelerate the functional annotation of prokaryotic genomes,COMBREX (http://combrex.bu.edu) is a project to increase the speed of the functional annotation of new bacterial and archaeal genomes. It consists of a database of functional predictions produced by computational biologists and a mechanism for experimental biochemists to bid for the validation of those predictions. Small grants are available to support successful bids.
DANIEL SEGRE,Genome-scale architecture of small molecule regulatory networks and the fundamental trade-off between regulation and enzymatic activity,"Metabolic flux is in part regulated by endogenous small molecules that modulate the catalytic activity of an enzyme, e.g., allosteric inhibition. In contrast to transcriptional regulation of enzymes, technical limitations have hindered the production of a genome-scale atlas of small molecule-enzyme regulatory interactions. Here, we develop a framework leveraging the vast, but fragmented, biochemical literature to reconstruct and analyze the small molecule regulatory network (SMRN) of the model organism Escherichia coli, including the primary metabolite regulators and enzyme targets. Using metabolic control analysis, we prove a fundamental trade-off between regulation and enzymatic activity, and we combine it with metabolomic measurements and the SMRN to make inferences on the sensitivity of enzymes to their regulators. Generalizing the analysis to other organisms, we identify highly conserved regulatory interactions across evolutionarily divergent species, further emphasizing a critical role for small molecule interactions in the maintenance of metabolic homeostasis."
TUHINA NEOGI,Association between radiographic features of knee osteoarthritis and pain: results from two cohort studies,"Objective To examine the relation of radiographic features of osteoarthritis to knee pain in people with knees discordant for knee pain in two cohorts. Design Within person, knee matched, case-control study. Setting and participants Participants in the Multicenter Osteoarthritis (MOST) and Framingham Osteoarthritis studies who had knee radiographs and assessments of knee pain. Main outcome measures Association of each pain measure (frequency, consistency, and severity) with radiographic osteoarthritis, as assessed by Kellgren and Lawrence grade (0-4) and osteophyte and joint space narrowing grades (0-3) among matched sets of two knees within individual participants whose knees were discordant for pain status. Results 696 people from MOST and 336 people from Framingham were included. Kellgren and Lawrence grades were strongly associated with frequent knee pain—for example, for Kellgren and Lawrence grade 4 v grade 0 the odds ratio for pain was 151 (95% confidence interval 43 to 526) in MOST and 73 (16 to 331) in Framingham (both P<0.001 for trend). Similar results were also seen for the relation of Kellgren and Lawrence scores to consistency and severity of knee pain. Joint space narrowing was more strongly associated with each pain measure than were osteophytes. Conclusions Using a method that minimises between person confounding, this study found that radiographic osteoarthritis and individual radiographic features of osteoarthritis were strongly associated with knee pain."
ALI GUERMAZI,Longitudinal assessment of cyst-like lesions of the knee and their relation to radiographic osteoarthritis and MRI-detected effusion and synovitis in patients with knee pain,"INTRODUCTION. The purpose of the present study was to determine the prevalence of cystic lesions and cyst-like bursitides in subjects with frequent knee pain and to assess their relation to radiographic osteoarthritis (OA) severity; to describe bilaterality and size fluctuation of the lesions over 6 months; and to assess relations between the prevalence of synovium-lined lesions communicating with the joint capsule and severity of magnetic resonance imaging (MRI)-detected effusion and synovitis. METHODS. One hundred and sixty-three subjects (total 319 knees) aged 35 to 65 with chronic, frequent knee pain were included. Imaging with 3 Tesla MRI was performed at baseline and 6-month follow-up with the same protocols as those used in the Osteoarthritis Initiative. Severity of radiographic OA was assessed using the Kellgren-Lawrence grade (0 to 4). Severity of effusion and synovitis was graded 0 to 3 based on the Whole Organ Magnetic Resonance Imaging Score system. The associations of cysts and cyst-like bursitides and severity of radiographic OA, MRI-detected effusion and synovitis were analyzed using logistic regression controlling for clustering by person. The Wilcoxon signed-rank test was used to determine whether there was a significant change in the size of lesions between baseline and follow-up. RESULTS. At least one lesion (any type) was present in 222 (70%) knees. The most prevalent lesions were popliteal cysts (40%, 128/319), followed by subgastrocnemius bursitis (15%, 49/319) and proximal tibiofibular joint cysts (8%, 26/319). Bilateral lesions were seen in 49% of the subjects. Only popliteal cysts and subgastrocnemius bursitis showed a significant change in size (P < 0.001). No trend was observed between prevalence of any of the cyst-like lesions analyzed and the increasing radiographic OA severity. Increasing prevalence of subgastrocnemius bursitis was associated with increasing severity of effusion (P = 0.0072) and synovitis (P = 0.0033). CONCLUSIONS. None of the cyst-like lesions analyzed seems to be a marker of radiographic OA severity in knees with chronic frequent pain. Subgastrocnemius bursitis may be used as a marker of effusion/synovitis severity. Bilateral cyst-like lesions are relatively commonly observed in people with chronic knee pain."
ALI GUERMAZI,Correspondence between bone mineral density and intervertebral disc degeneration across age and sex,"The distribution of bone tissue within the vertebra can modulate vertebral strength independently of average density and may change with age and disc degeneration. Our results show that the age-associated decrease in bone density is spatially non-uniform and associated with disc health, suggesting a mechanistic interplay between disc and vertebra. PURPOSE: While the decline of bone mineral density (BMD) in the aging spine is well established, the extent to which age influences BMD distribution within the vertebra is less clear. Measures of regional BMD (rBMD) may improve predictions of vertebral strength and suggest how vertebrae might adapt with intervertebral disc degeneration. Thus, we aimed to assess how rBMD values were associated with age, sex, and disc height loss (DHL). METHODS: We measured rBMD in the L3 vertebra of 377 participants from the Framingham Heart Study (41-83 years, 181 M/196 F). Integral (Int.BMD) and trabecular BMD (Tb.BMD) were measured from QCT images. rBMD ratios (anterior/posterior, superior/mid-transverse, inferior/mid-transverse, and central/outer) were calculated from the centrum. A radiologist assigned a DHL severity score to adjacent intervertebral discs (L2-L3 and L3-L4). RESULTS: Int.BMD and Tb.BMD were both associated with age, though the decrease across age was greater in women (Int.BMD, - 2.6 mg/cm3 per year; Tb.BMD, - 2.6 mg/cm3 per year) than men (Int.BMD, - 0.5 mg/cm3 per year; Tb.BMD, - 1.2 mg/cm3 per year). The central/outer (- 0.027/decade) and superior/mid-transverse (- 0.018/decade) rBMD ratios were negatively associated with age, with similar trends in men and women. Higher Int.BMD or Tb.BMD was associated with increased odds of DHL after adjusting for age and sex. Low central/outer ratio and high anterior/poster and superior/mid-transverse ratios were also associated with increased odds of DHL. CONCLUSIONS: Our results indicate that the distribution of bone within the L3 vertebra is different across age, but not between sexes, and is associated with disc degeneration."
ALI GUERMAZI,Biochemical Markers of Bone Turnover and Their Association with Bone Marrow Lesions,"INTRODUCTION. Our objective was to determine whether markers of bone resorption and formation could serve as markers for the presence of bone marrow lesions (BMLs). METHODS. We conducted an analysis of data from the Boston Osteoarthritis of the Knee Study (BOKS). Knee magnetic resonance images were scored for BMLs using a semiquantitative grading scheme. In addition, a subset of persons with BMLs underwent quantitative volume measurement of their BML, using a proprietary software method. Within the BOKS population, 80 people with BMLs and 80 without BMLs were selected for the purposes of this case-control study. Bone biomarkers assayed included type I collagen N-telopeptide (NTx) corrected for urinary creatinine, bone-specific alkaline phosphatase, and osteocalcin. The same methods were used and applied to a nested case-control sample from the Framingham study, in which BMD assessments allowed evaluation of this as a covariate. Logistic regression models were fit using BML as the outcome and biomarkers, age, sex, and body mass index as predictors. An receiver operating characteristic curve was generated for each model and the area under the curve assessed. RESULTS. A total of 151 subjects from BOKS with knee OA were assessed. The mean (standard deviation) age was 67 (9) years and 60% were male. Sixty-nine per cent had maximum BML score above 0, and 48% had maximum BML score above 1. The only model that reached statistical significance used maximum score of BML above 0 as the outcome. Ln-NTx (Ln is the natural log) exhibited a significant association with BMLs, with the odds of a BML being present increasing by 1.4-fold (95% confidence interval = 1.0-fold to 2.0-fold) per 1 standard deviation increase in the LnNTx, and with a small partial R2 of 3.05. We also evaluated 144 participants in the Framingham Osteoarthritis Study, whose mean age was 68 years and body mass index was 29 kg/m2, and of whom 40% were male. Of these participants 55% had a maximum BML score above 0. The relationship between NTx and maximum score of BML above 0 revealed a significant association, with an odds ratio fo 1.7 (95% confidence interval = 1.1 to 2.7) after adjusting for age, sex, and body mass index. CONCLUSIONS. Serum NTx was weakly associated with the presence of BMLs in both study samples. This relationship was not strong and we would not advocate the use of NTx as a marker of the presence of BMLs."
ALI GUERMAZI,Cartilage Markers and Their Association with Cartilage Loss on Magnetic Resonance Imaging in Knee Osteoarthritis: The Boston Osteoarthritis Knee Study,"We used data from a longitudinal observation study to determine whether markers of cartilage turnover could serve as predictors of cartilage loss on magnetic resonance imaging (MRI). We conducted a study of data from the Boston Osteoarthritis of the Knee Study (BOKS), a completed natural history study of knee osteoarthritis (OA). All subjects in the study met American College of Rheumatology criteria for knee OA. Baseline and follow-up knee magnetic resonance images were scored for cartilage loss by means of the WORMS (Whole Organ Magnetic Resonance Imaging Score) semiquantitative grading scheme. Within the BOKS population, 80 subjects who experienced cartilage loss and 80 subjects who did not were selected for the purposes of this nested case control study. We assessed the baseline levels of cartilage degradation and synthesis products by means of assays for type I and II cleavage by collagenases (Col2:3/4Cshort or C1,2C), type II cleavage only with Col2:3/4Clongmono (C2C), type II synthesis (C-propeptide), the C-telopeptide of type II (Col2CTx), aggrecan 846 epitope, and cartilage oligomeric matrix protein (COMP). We performed a logistic regression to examine the relation of levels of each biomarker to the risk of cartilage loss in any knee. All analyses were adjusted for gender, age, and body mass index (BMI); results stratified by gender gave similar results. One hundred thirty-seven patients with symptomatic knee OA were assessed. At baseline, the mean (standard deviation) age was 67 (9) years and 54% were male. Seventy-six percent of the subjects had radiographic tibiofemoral OA (Kellgren & Lawrence grade of greater than or equal to 2) and the remainder had patellofemoral OA. With the exception of COMP, none of the other biomarkers was a statistically significant predictor of cartilage loss. For a 1-unit increase in COMP, the odds of cartilage loss increased 6.09 times (95% confidence interval [CI] 1.34 to 27.67). After the analysis of COMP was adjusted for age, gender, and BMI, the risk for cartilage loss was 6.35 (95% CI 1.36 to 29.65). Among subjects with symptomatic knee OA, a single measurement of increased COMP predicted subsequent cartilage loss on MRI. The other biochemical markers of cartilage synthesis and degradation do not facilitate prediction of cartilage loss. With the exception of COMP, if changes in cartilage turnover in patients with symptomatic knee OA are associated with cartilage loss, they do not appear to affect systemic biomarker levels."
SEAN J ELLIOTT,Validation of 12 small Kepler transiting planets in the habitable zone,"We present an investigation of 12 candidate transiting planets from Kepler with orbital periods ranging from 34 to 207 days, selected from initial indications that they are small and potentially in the habitable zone (HZ) of their parent stars. Few of these objects are known. The expected Doppler signals are too small to confirm them by demonstrating that their masses are in the planetary regime. Here we verify their planetary nature by validating them statistically using the BLENDER technique, which simulates large numbers of false positives and compares the resulting light curves with the Kepler photometry. This analysis was supplemented with new follow-up observations (high-resolution optical and near-infrared spectroscopy, adaptive optics imaging, and speckle interferometry), as well as an analysis of the flux centroids. For 11 of them (KOI-0571.05, 1422.04, 1422.05, 2529.02, 3255.01, 3284.01, 4005.01, 4087.01, 4622.01, 4742.01, and 4745.01) we show that the likelihood they are true planets is far greater than that of a false positive, to a confidence level of 99.73% (3σ ) or higher. For KOI-4427.01 the confidence level is about 99.2% (2.6σ ). With our accurate characterization of the GKM host stars, the derived planetary radii range from 1.1 to 2.7R⊕. All 12 objects are confirmed to be in the HZ, and nine are small enough to be rocky. Excluding three of them that have been previously validated by others, our study doubles the number of known rocky planets in the HZ. KOI-3284.01 (Kepler-438b) and KOI-4742.01 (Kepler-442b) are the planets most similar to the Earth discovered to date when considering their size and incident flux jointly."
SEAN J ELLIOTT,The long-baseline neutrino experiment: exploring fundamental symmetries of the universe,"The preponderance of matter over antimatter in the early Universe, the dynamics of the supernova bursts that produced the heavy elements necessary for life and whether protons eventually decay --- these mysteries at the forefront of particle physics and astrophysics are key to understanding the early evolution of our Universe, its current state and its eventual fate. The Long-Baseline Neutrino Experiment (LBNE) represents an extensively developed plan for a world-class experiment dedicated to addressing these questions. LBNE is conceived around three central components: (1) a new, high-intensity neutrino source generated from a megawatt-class proton accelerator at Fermi National Accelerator Laboratory, (2) a near neutrino detector just downstream of the source, and (3) a massive liquid argon time-projection chamber deployed as a far detector deep underground at the Sanford Underground Research Facility. This facility, located at the site of the former Homestake Mine in Lead, South Dakota, is approximately 1,300 km from the neutrino source at Fermilab -- a distance (baseline) that delivers optimal sensitivity to neutrino charge-parity symmetry violation and mass ordering effects. This ambitious yet cost-effective design incorporates scalability and flexibility and can accommodate a variety of upgrades and contributions. With its exceptional combination of experimental configuration, technical capabilities, and potential for transformative discoveries, LBNE promises to be a vital facility for the field of particle physics worldwide, providing physicists from around the globe with opportunities to collaborate in a twenty to thirty year program of exciting science. In this document we provide a comprehensive overview of LBNE's scientific objectives, its place in the landscape of neutrino physics worldwide, the technologies it will incorporate and the capabilities it will possess."
CARYL RIVERS,"Bostonia: 2000-2001, no. 1-4",
CARYL RIVERS,"Bostonia: 1998-1999, no. 1, 3-4",
LYNN L MOORE,"Association of urinary phthalate metabolite concentrations with body mass index and waist circumference: a cross-sectional study of NHANES data, 1999–2002","BACKGROUND: Although diet and activity are key factors in the obesity epidemic, laboratory studies suggest that endocrine disrupting chemicals may also affect obesity. METHODS: We analyzed associations between six phthalate metabolites measured in urine and body mass index (BMI) and waist circumference (WC) in National Health and Nutrition Examination Survey (NHANES) participants aged 6–80. We included 4369 participants from NHANES 1999–2002, with data on mono-ethyl (MEP), mono-2-ethylhexyl (MEHP), mono-n-butyl (MBP), and mono-benzyl (MBzP) phthalate; 2286 also had data on mono-2-ethyl-5-hydroxyhexyl (MEHHP) and mono-2-ethyl-5-oxohexyl (MEOHP) phthalate (2001–2002). Using multiple regression, we computed mean BMI and WC within phthalate quartiles in eight age/gender specific models. RESULTS: The most consistent associations were in males aged 20–59; BMI and WC increased across quartiles of MBzP (adjusted mean BMI = 26.7, 27.2, 28.4, 29.0, p-trend = 0.0002), and positive associations were also found for MEOHP, MEHHP, MEP, and MBP. In females, BMI and WC increased with MEP quartile in adolescent girls (adjusted mean BMI = 22.9, 23.8, 24.1, 24.7, p-trend = 0.03), and a similar but less strong pattern was seen in 20–59 year olds. In contrast, MEHP was inversely related to BMI in adolescent girls (adjusted mean BMI = 25.4, 23.8, 23.4, 22.9, p-trend = 0.02) and females aged 20–59 (adjusted mean BMI = 29.9, 29.9, 27.9, 27.6, p-trend = 0.02). There were no important associations among children, but several inverse associations among 60–80 year olds. CONCLUSION: This exploratory, cross-sectional analysis revealed a number of interesting associations with different phthalate metabolites and obesity outcomes, including notable differences by gender and age subgroups. Effects of endocrine disruptors, such as phthalates, may depend upon endogenous hormone levels, which vary dramatically by age and gender. Individual phthalates also have different biologic and hormonal effects. Although our study has limitations, both of these factors could explain some of the variation in the observed associations. These preliminary data support the need for prospective studies in populations at risk for obesity."
LYNN L MOORE,A Cross-Sectional Study of Food Group Intake and C-Reactive Protein among Children,"BACKGROUND. C-reactive protein (CRP), a marker of sub-clinical inflammation, is a predictor of future cardiovascular diseases. Dietary habits affect serum CRP level however the relationship between consumption of individual food groups and CRP levels has not been established. METHODS. This study was designed to explore the relation between food intake and CRP levels in children using data from the cross-sectional 1999-2002 National Health and Nutrition Examination Surveys. CRP level was classified as low, average or high (>1.0, 1.0-3.0, and <3.0 mg/L, respectively). Adjusted mean daily intakes of dairy, grains, fruit, vegetables, and meat/other proteins in each CRP category were estimated using multivariate analysis of covariance modeling. The effect modification by age (5-11 years vs. 12-16 years), gender and race/ethnicity was explored. We examined whether total or central body fat (using BMI Z-scores and waist circumference) explained any of the observed associations. RESULTS. A total of 4,010 children and adolescents had complete information on diet, CRP and all covariates of interest and were included in the analyses. Individuals with high CRP levels had significantly lower intake of grains (p < 0.001) and vegetables (p = 0.0002). Selected individual food subgroups (e.g., fluid milk and ""citrus, melon and berry"" consumption) were more strongly associated with lower CRP than were their respective major food groups. Consumption of meat/other proteins did not influence CRP levels. The addition of body composition variables to the models attenuated the results for all food groups to varying degrees. CONCLUSION. Children and adolescents with higher CRP levels had significantly lower intakes of grains and vegetables. The associations between selected childhood dietary patterns and CRP levels seem largely mediated through effects on body composition."
LYNN L MOORE,Buildout and integration of an automated high-throughput CLIA laboratory for SARS-CoV-2 testing on a large urban campus,"In 2019, the first cases of SARS-CoV-2 were detected in Wuhan, China, and by early 2020 the first cases were identified in the United States. SARS-CoV-2 infections increased in the US causing many states to implement stay-at-home orders and additional safety precautions to mitigate potential outbreaks. As policies changed throughout the pandemic and restrictions lifted, there was an increase in demand for COVID-19 testing which was costly, difficult to obtain, or had long turn-around times. Some academic institutions, including Boston University (BU), created an on-campus COVID-19 screening protocol as part of a plan for the safe return of students, faculty, and staff to campus with the option for in-person classes. At BU, we put together an automated high-throughput clinical testing laboratory with the capacity to run 45,000 individual tests weekly by Fall of 2020, with a purpose-built clinical testing laboratory, a multiplexed reverse transcription PCR (RT-qPCR) test, robotic instrumentation, and trained staff. There were many challenges including supply chain issues for personal protective equipment and testing materials in addition to equipment that were in high demand. The BU Clinical Testing Laboratory (CTL) was operational at the start of Fall 2020 and performed over 1 million SARS-CoV-2 PCR tests during the 2020-2021 academic year."
LYNN L MOORE,Broadband multi-wavelength properties of M87 during the 2017 Event Horizon Telescope campaign,"In 2017, the Event Horizon Telescope (EHT) Collaboration succeeded in capturing the first direct image of the center of the M87 galaxy. The asymmetric ring morphology and size are consistent with theoretical expectations for a weakly accreting supermassive black hole of mass ∼6.5 × 109 M ⊙. The EHTC also partnered with several international facilities in space and on the ground, to arrange an extensive, quasi-simultaneous multi-wavelength campaign. This Letter presents the results and analysis of this campaign, as well as the multi-wavelength data as a legacy data repository. We captured M87 in a historically low state, and the core flux dominates over HST-1 at high energies, making it possible to combine core flux constraints with the more spatially precise very long baseline interferometry data. We present the most complete simultaneous multi-wavelength spectrum of the active nucleus to date, and discuss the complexity and caveats of combining data from different spatial scales into one broadband spectrum. We apply two heuristic, isotropic leptonic single-zone models to provide insight into the basic source properties, but conclude that a structured jet is necessary to explain M87’s spectrum. We can exclude that the simultaneous γ-ray emission is produced via inverse Compton emission in the same region producing the EHT mm-band emission, and further conclude that the γ-rays can only be produced in the inner jets (inward of HST-1) if there are strongly particle-dominated regions. Direct synchrotron emission from accelerated protons and secondaries cannot yet be excluded."
LYNN L MOORE,Long-term yogurt consumption and risk of incident hypertension in adults,
MICHAEL REYNOLDS,Underdiagnosis of mild cognitive impairment: A consequence of ignoring practice effects,"INTRODUCTION: Longitudinal testing is necessary to accurately measure cognitive change. However, repeated testing is susceptible to practice effects, which may obscure true cognitive decline and delay detection of mild cognitive impairment (MCI). METHODS: We retested 995 late-middle-aged men in a ∼6-year follow-up of the Vietnam Era Twin Study of Aging. In addition, 170 age-matched replacements were tested for the first time at study wave 2. Group differences were used to calculate practice effects after controlling for attrition effects. MCI diagnoses were generated from practice-adjusted scores. RESULTS: There were significant practice effects on most cognitive domains. Conversion to MCI doubled after correcting for practice effects, from 4.5% to 9%. Importantly, practice effects were present although there were declines in uncorrected scores. DISCUSSION: Accounting for practice effects is critical to early detection of MCI. Declines, when lower than expected, can still indicate practice effects. Replacement participants are needed for accurately assessing disease progression."
MICHAEL REYNOLDS,Mediators of the effect of childhood socioeconomic status on late midlife cognitive abilities: a four decade longitudinal study,"BACKGROUND AND OBJECTIVES: Childhood socioeconomic status (cSES) is found to predict later-life cognitive abilities, yet the mechanisms underlying these associations remain unclear. The objective of this longitudinal study was to examine the direct and indirect paths through which cSES influences late midlife cognitive outcomes. RESEARCH DESIGN AND METHODS: Participants were 1,009 male twins in the Vietnam Era Twin Study of Aging (VETSA). At mean ages 20 and 62, participants completed a standardized test for general cognitive ability (GCA). The age 62 cognitive assessment also included in-person tests of processing speed, episodic memory, abstract reasoning, working memory, verbal fluency, visual-spatial ability, and executive functions. At mean age 56, participants were interviewed regarding their own and their parents' education and occupation, and completed questionnaires about cognitive leisure activities and sociodemographic information. Multiple mediation analyses were conducted to examine the direct path effects and indirect path effects of cSES through age 20 GCA, adult SES, and cognitive leisure activities on seven cognitive outcomes at age 62, adjusting for age, ethnicity, and non-independence of observations. RESULTS: Total (direct plus indirect) effects were significant for all measures with the exception of executive functions. Men from lower cSES backgrounds had poorer cognitive functioning in late midlife. The direct effect of cSES was partially mediated for abstract reasoning, and was fully mediated for the remaining six cognitive outcomes. Total indirect effects accounted for at least half of the total effects in each model, with paths through age 20 GCA explaining most of the total indirect effects. DISCUSSION AND IMPLICATIONS: cSES predicted cognitive functioning in late middle age Using multiple mediation models, we show that lower cSES predicts poorer cognition in late midlife primarily through young adult cognitive ability and to a lesser extent through SES in adulthood and engagement in cognitively stimulating activities."
MICHAEL REYNOLDS,Genetic underpinnings of increased BMI and its association with late midlife cognitive abilities,"OBJECTIVES: First, we test for differences in various cognitive abilities across trajectories of body mass index (BMI) over the later life course. Second, we examine whether genetic risk factors for unhealthy BMIs-assessed via polygenic risk scores (PRS)-predict cognitive abilities in late-life. METHODS: The study used a longitudinal sample of Vietnam veteran males to explore the associations between BMI trajectories, measured across four time points, and later cognitive abilities. The sample of 977 individuals was drawn from the Vietnam Era Twin Study of Aging. Cognitive abilities evaluated included executive function, abstract reasoning, episodic memory, processing speed, verbal fluency, and visual spatial ability. Multilevel linear regression models were used to estimate the associations between BMI trajectories and cognitive abilities. Then, BMI PRS was added to the models to evaluate polygenic associations with cognitive abilities. RESULTS: There were no significant differences in cognitive ability between any of the BMI trajectory groups. There was a significant inverse relationship between BMI-PRS and several cognitive ability measures. DISCUSSION: While no associations emerged for BMI trajectories and cognitive abilities at the phenotypic levels, BMI PRS measures did correlate with key cognitive domains. Our results suggest possible polygenic linkages cutting across key components of the central and peripheral nervous system."
MICHAEL REYNOLDS,"First Sagittarius A* Event Horizon Telescope results. II. EHT and multiwavelength observations, data processing, and calibration","We present Event Horizon Telescope (EHT) 1.3 mm measurements of the radio source located at the position of the supermassive black hole Sagittarius A* (Sgr A*), collected during the 2017 April 5–11 campaign. The observations were carried out with eight facilities at six locations across the globe. Novel calibration methods are employed to account for Sgr A*'s flux variability. The majority of the 1.3 mm emission arises from horizon scales, where intrinsic structural source variability is detected on timescales of minutes to hours. The effects of interstellar scattering on the image and its variability are found to be subdominant to intrinsic source structure. The calibrated visibility amplitudes, particularly the locations of the visibility minima, are broadly consistent with a blurred ring with a diameter of ∼50 μas, as determined in later works in this series. Contemporaneous multiwavelength monitoring of Sgr A* was performed at 22, 43, and 86 GHz and at near-infrared and X-ray wavelengths. Several X-ray flares from Sgr A* are detected by Chandra, one at low significance jointly with Swift on 2017 April 7 and the other at higher significance jointly with NuSTAR on 2017 April 11. The brighter April 11 flare is not observed simultaneously by the EHT but is followed by a significant increase in millimeter flux variability immediately after the X-ray outburst, indicating a likely connection in the emission physics near the event horizon. We compare Sgr A*’s broadband flux during the EHT campaign to its historical spectral energy distribution and find that both the quiescent emission and flare emission are consistent with its long-term behavior."
MICHAEL REYNOLDS,"The therapeutic potential of exercise to improve mood, cognition, and sleep in Parkinson's disease","In addition to the classic motor symptoms, Parkinson's disease (PD) is associated with a variety of nonmotor symptoms that significantly reduce quality of life, even in the early stages of the disease. There is an urgent need to develop evidence‐based treatments for these symptoms, which include mood disturbances, cognitive dysfunction, and sleep disruption. We focus here on exercise interventions, which have been used to improve mood, cognition, and sleep in healthy older adults and clinical populations, but to date have primarily targeted motor symptoms in PD. We synthesize the existing literature on the benefits of aerobic exercise and strength training on mood, sleep, and cognition as demonstrated in healthy older adults and adults with PD, and suggest that these types of exercise offer a feasible and promising adjunct treatment for mood, cognition, and sleep difficulties in PD. Across stages of the disease, exercise interventions represent a treatment strategy with the unique ability to improve a range of nonmotor symptoms while also alleviating the classic motor symptoms of the disease. Future research in PD should include nonmotor outcomes in exercise trials with the goal of developing evidence‐based exercise interventions as a safe, broad‐spectrum treatment approach to improve mood, cognition, and sleep for individuals with PD."
MICHAEL REYNOLDS,First Sagittarius A* Event Horizon Telescope results. I. The shadow of the supermassive black hole in the center of the Milky Way,"We present the first Event Horizon Telescope (EHT) observations of Sagittarius A* (Sgr A*), the Galactic center source associated with a supermassive black hole. These observations were conducted in 2017 using a global interferometric array of eight telescopes operating at a wavelength of λ = 1.3 mm. The EHT data resolve a compact emission region with intrahour variability. A variety of imaging and modeling analyses all support an image that is dominated by a bright, thick ring with a diameter of 51.8 ± 2.3 μas (68% credible interval). The ring has modest azimuthal brightness asymmetry and a comparatively dim interior. Using a large suite of numerical simulations, we demonstrate that the EHT images of Sgr A* are consistent with the expected appearance of a Kerr black hole with mass ∼4 × 106 M ⊙, which is inferred to exist at this location based on previous infrared observations of individual stellar orbits, as well as maser proper-motion studies. Our model comparisons disfavor scenarios where the black hole is viewed at high inclination (i &gt; 50°), as well as nonspinning black holes and those with retrograde accretion disks. Our results provide direct evidence for the presence of a supermassive black hole at the center of the Milky Way, and for the first time we connect the predictions from dynamical measurements of stellar orbits on scales of 103–105 gravitational radii to event-horizon-scale images and variability. Furthermore, a comparison with the EHT results for the supermassive black hole M87* shows consistency with the predictions of general relativity spanning over three orders of magnitude in central mass."
MICHAEL REYNOLDS,Broadband multi-wavelength properties of M87 during the 2017 Event Horizon Telescope campaign,"In 2017, the Event Horizon Telescope (EHT) Collaboration succeeded in capturing the first direct image of the center of the M87 galaxy. The asymmetric ring morphology and size are consistent with theoretical expectations for a weakly accreting supermassive black hole of mass ∼6.5 × 109 M ⊙. The EHTC also partnered with several international facilities in space and on the ground, to arrange an extensive, quasi-simultaneous multi-wavelength campaign. This Letter presents the results and analysis of this campaign, as well as the multi-wavelength data as a legacy data repository. We captured M87 in a historically low state, and the core flux dominates over HST-1 at high energies, making it possible to combine core flux constraints with the more spatially precise very long baseline interferometry data. We present the most complete simultaneous multi-wavelength spectrum of the active nucleus to date, and discuss the complexity and caveats of combining data from different spatial scales into one broadband spectrum. We apply two heuristic, isotropic leptonic single-zone models to provide insight into the basic source properties, but conclude that a structured jet is necessary to explain M87’s spectrum. We can exclude that the simultaneous γ-ray emission is produced via inverse Compton emission in the same region producing the EHT mm-band emission, and further conclude that the γ-rays can only be produced in the inner jets (inward of HST-1) if there are strongly particle-dominated regions. Direct synchrotron emission from accelerated protons and secondaries cannot yet be excluded."
HEE-YOUNG PARK,Role of BMP-4 and Its Signaling Pathways in Cultured Human Melanocytes,"Bone Morphogenetic Protein (BMP-4) was shown to down-regulate melanogenesis, in part, by decreasing the level of tyrosinase [Yaar et al. (2006) JBC:281]. Results presented here show that BMP-4 down-regulated the protein levels of TRP-1, PKC-β, and MCI-R. When paired cultures of human melanocytes were treated with vehicle or BMP-4 (25ng/ml), MAPK/ERK were phosphorylated within one hour of BMP-4 treatment. Then the activated MAPK/ERK caused an acute phosphorylation of MITF, followed by proteosome-mediated degradation of MITF, the key transcription factor for melanogenic proteins [Wu et al. (2000) Gene & Development:14]. However, prolonged exposure of melanocytes to BMP-4 (up to 48 hours) caused a decrease in the level of MITF-M transcript. In addition, BMP-4 decreased the intracellular level of cAMP, the key regulator of MITF expression. These results demonstrate that BMP-4 activates MAPK/ERK signaling pathway to transiently activate MITF; however, chronic treatment of BMP-4 to melanocytes causes a down-regulation of the expression of MITF, possibly in a cAMP-dependent pathway."
IOANNIS PASCHALIDIS,Robust anomaly detection in dynamic networks,"We propose two robust methods for anomaly detection in dynamic networks in which the properties of normal traffic evolve dynamically. We formulate the robust anomaly detection problem as a binary composite hypothesis testing problem and propose two methods: a model-free and a model-based one, leveraging techniques from the theory of large deviations. Both methods require a family of Probability Laws (PLs) that represent normal properties of traffic. We devise a two-step procedure to estimate this family of PLs. We compare the performance of our robust methods and their vanilla counterparts, which assume that normal traffic is stationary, on a network with a diurnal normal pattern and a common anomaly related to data exfiltration. Simulation results show that our robust methods perform better than their vanilla counterparts in dynamic networks."
IOANNIS PASCHALIDIS,Predicting chronic disease hospitalizations from electronic health records: an interpretable classification approach,"Urban living in modern large cities has significant adverse effects on health, increasing the risk of several chronic diseases. We focus on the two leading clusters of chronic diseases, heart disease and diabetes, and develop data-driven methods to predict hospitalizations due to these conditions. We base these predictions on the patients' medical history, recent and more distant, as described in their Electronic Health Records (EHRs). We formulate the prediction problem as a binary classification problem and consider a variety of machine learning methods, including kernelized and sparse Support Vector Machines (SVMs), sparse logistic regression, and random forests. To strike a balance between accuracy and interpretability of the prediction, which is important in a medical setting, we propose two novel methods: K -LRT, a likelihood ratio test-based method, and a Joint Clustering and Classification (JCC) method which identifies hidden patient clusters and adapts classifiers to each cluster. We develop theoretical out-of-sample guarantees for the latter method. We validate our algorithms on large data sets from the Boston Medical Center, the largest safety-net hospital system in New England."
IOANNIS PASCHALIDIS,Data-driven estimation of origin-destination demand and user cost functions for the optimization of transportation networks,
IOANNIS PASCHALIDIS,Outlier detection using distributionally robust optimization under the Wasserstein metric,"We present a Distributionally Robust Optimization (DRO) approach to outlier detection in a linear regression setting, where the closeness of probability distributions is measured using the Wasserstein metric. Training samples contaminated with outliers skew the regression plane computed by least squares and thus impede outlier detection. Classical approaches, such as robust regression, remedy this problem by downweighting the contribution of atypical data points. In contrast, our Wasserstein DRO approach hedges against a family of distributions that are close to the empirical distribution. We show that the resulting formulation encompasses a class of models, which include the regularized Least Absolute Deviation (LAD) as a special case. We provide new insights into the regularization term and give guidance on the selection of the regularization coefficient from the standpoint of a confidence region. We establish two types of performance guarantees for the solution to our formulation under mild conditions. One is related to its out-of-sample behavior, and the other concerns the discrepancy between the estimated and true regression planes. Extensive numerical results demonstrate the superiority of our approach to both robust regression and the regularized LAD in terms of estimation accuracy and outlier detection rates."
IOANNIS PASCHALIDIS,Learning policies for Markov decision processes from data,"We consider the problem of learning a policy for a Markov decision process consistent with data captured on the state-actions pairs followed by the policy. We assume that the policy belongs to a class of parameterized policies which are defined using features associated with the state-action pairs. The features are known a priori, however, only an unknown subset of them could be relevant. The policy parameters that correspond to an observed target policy are recovered using `1-regularized logistic regression that best fits the observed state-action samples. We establish bounds on the difference between the average reward of the estimated and the original policy (regret) in terms of the generalization error and the ergodic coefficient of the underlying Markov chain. To that end, we combine sample complexity theory and sensitivity analysis of the stationary distribution of Markov chains. Our analysis suggests that to achieve regret within order O( √ ), it suffices to use training sample size on the order of Ω(logn · poly(1/ )), where n is the number of the features. We demonstrate the effectiveness of our method on a synthetic robot navigation example."
IOANNIS PASCHALIDIS,Protein Docking by the Underestimation of Free Energy Funnels in the Space of Encounter Complexes,"Similarly to protein folding, the association of two proteins is driven by a free energy funnel, determined by favorable interactions in some neighborhood of the native state. We describe a docking method based on stochastic global minimization of funnel-shaped energy functions in the space of rigid body motions (SE(3)) while accounting for flexibility of the interface side chains. The method, called semi-definite programming-based underestimation (SDU), employs a general quadratic function to underestimate a set of local energy minima and uses the resulting underestimator to bias further sampling. While SDU effectively minimizes functions with funnel-shaped basins, its application to docking in the rotational and translational space SE(3) is not straightforward due to the geometry of that space. We introduce a strategy that uses separate independent variables for side-chain optimization, center-to-center distance of the two proteins, and five angular descriptors of the relative orientations of the molecules. The removal of the center-to-center distance turns out to vastly improve the efficiency of the search, because the five-dimensional space now exhibits a well-behaved energy surface suitable for underestimation. This algorithm explores the free energy surface spanned by encounter complexes that correspond to local free energy minima and shows similarity to the model of macromolecular association that proceeds through a series of collisions. Results for standard protein docking benchmarks establish that in this space the free energy landscape is a funnel in a reasonably broad neighborhood of the native state and that the SDU strategy can generate docking predictions with less than 5 � ligand interface Ca root-mean-square deviation while achieving an approximately 20-fold efficiency gain compared to Monte Carlo methods."
IOANNIS PASCHALIDIS,Protein docking by the underestimation of free energy funnels in the space of encounter complexes,"Similarly to protein folding, the association of two proteins is driven by a free energy funnel, determined by favorable interactions in some neighborhood of the native state. We describe a docking method based on stochastic global minimization of funnel-shaped energy functions in the space of rigid body motions (SE(3)) while accounting for flexibility of the interface side chains. The method, called semi-definite programming-based underestimation (SDU), employs a general quadratic function to underestimate a set of local energy minima and uses the resulting underestimator to bias further sampling. While SDU effectively minimizes functions with funnel-shaped basins, its application to docking in the rotational and translational space SE(3) is not straightforward due to the geometry of that space. We introduce a strategy that uses separate independent variables for side-chain optimization, center-to-center distance of the two proteins, and five angular descriptors of the relative orientations of the molecules. The removal of the center-to-center distance turns out to vastly improve the efficiency of the search, because the five-dimensional space now exhibits a well-behaved energy surface suitable for underestimation. This algorithm explores the free energy surface spanned by encounter complexes that correspond to local free energy minima and shows similarity to the model of macromolecular association that proceeds through a series of collisions. Results for standard protein docking benchmarks establish that in this space the free energy landscape is a funnel in a reasonably broad neighborhood of the native state and that the SDU strategy can generate docking predictions with less than 5 A ligand interface C(alpha) root-mean-square deviation while achieving an approximately 20-fold efficiency gain compared to Monte Carlo methods."
IOANNIS PASCHALIDIS,Efficient maintenance and update of nonbonded lists in macromolecular simulations,"Molecular mechanics and dynamics simulations use distance based cutoff approximations for faster computation of pairwise van der Waals and electrostatic energy terms. These approximations traditionally use a precalculated and periodically updated list of interacting atom pairs, known as the “nonbonded neighborhood lists” or nblists, in order to reduce the overhead of finding atom pairs that are within distance cutoff. The size of nblists grows linearly with the number of atoms in the system and superlinearly with the distance cutoff, and as a result, they require significant amount of memory for large molecular systems. The high space usage leads to poor cache performance, which slows computation for large distance cutoffs. Also, the high cost of updates means that one cannot afford to keep the data structure always synchronized with the configuration of the molecules when efficiency is at stake. We propose a dynamic octree data structure for implicit maintenance of nblists using space linear in the number of atoms but independent of the distance cutoff. The list can be updated very efficiently as the coordinates of atoms change during the simulation. Unlike explicit nblists, a single octree works for all distance cutoffs. In addition, octree is a cache-friendly data structure, and hence, it is less prone to cache miss slowdowns on modern memory hierarchies than nblists. Octrees use almost 2 orders of magnitude less memory, which is crucial for simulation of large systems, and while they are comparable in performance to nblists when the distance cutoff is small, they outperform nblists for larger systems and large cutoffs. Our tests show that octree implementation is approximately 1.5 times faster in practical use case scenarios as compared to nblists."
IOANNIS PASCHALIDIS,Mapping the landscape of metabolic goals of a cell,"Genome-scale flux balance models of metabolism provide testable predictions of all metabolic rates in an organism, by assuming that the cell is optimizing a metabolic goal known as the objective function. We introduce an efficient inverse flux balance analysis (invFBA) approach, based on linear programming duality, to characterize the space of possible objective functions compatible with measured fluxes. After testing our algorithm on simulated E. coli data and time-dependent S. oneidensis fluxes inferred from gene expression, we apply our inverse approach to flux measurements in long-term evolved E. coli strains, revealing objective functions that provide insight into metabolic adaptation trajectories."
IOANNIS PASCHALIDIS,Congestion-aware routing and rebalancing of autonomous mobility-on-demand systems in mixed traffic,"This paper studies congestion-aware route-planning policies for Autonomous Mobility-on-Demand (AMoD) systems, whereby a fleet of autonomous vehicles provides on demand mobility under mixed traffic conditions. Specifically, we first devise a network flow model to optimize the AMoD routing and rebalancing strategies in a congestion-aware fashion by accounting for the endogenous impact of AMoD flows on travel time. Second, we capture reactive exogenous traffic consisting of private vehicles selfishly adapting to the AMoD flows in a user centric fashion by leveraging an iterative approach. Finally, we showcase the effectiveness of our framework with a case-study considering the transportation sub-network in New York City. Our results suggest that for high levels of demand, pure AMoD travel can be detrimental due to the additional traffic stemming from its rebalancing flows, whilst the combination of AMoD with walking or micro mobility options can significantly improve the overall system performance."
IOANNIS PASCHALIDIS,Predicting diabetes-related hospitalizations based on electronic health records,"OBJECTIVE: To derive a predictive model to identify patients likely to be hospitalized during the following year due to complications attributed to Type II diabetes. METHODS: A variety of supervised machine learning classification methods were tested and a new method that discovers hidden patient clusters in the positive class (hospitalized) was developed while, at the same time, sparse linear support vector machine classifiers were derived to separate positive samples from the negative ones (non-hospitalized). The convergence of the new method was established and theoretical guarantees were proved on how the classifiers it produces generalize to a test set not seen during training. RESULTS: The methods were tested on a large set of patients from the Boston Medical Center - the largest safety net hospital in New England. It is found that our new joint clustering/classification method achieves an accuracy of 89% (measured in terms of area under the ROC Curve) and yields informative clusters which can help interpret the classification results, thus increasing the trust of physicians to the algorithmic output and providing some guidance towards preventive measures. While it is possible to increase accuracy to 92% with other methods, this comes with increased computational cost and lack of interpretability. The analysis shows that even a modest probability of preventive actions being effective (more than 19%) suffices to generate significant hospital care savings. CONCLUSIONS: Predictive models are proposed that can help avert hospitalizations, improve health outcomes and drastically reduce hospital expenditures. The scope for savings is significant as it has been estimated that in the USA alone, about $5.8 billion are spent each year on diabetes-related hospitalizations that could be prevented."
IOANNIS PASCHALIDIS,Distributed MPC for coordinated energy efficiency utilization in microgrid systems,"To improve the renewable energy utilization of distributed microgrid systems, this paper presents an optimal distributed model predictive control strategy to coordinate energy management among microgrid systems. In particular, through information exchange among systems, each microgrid in the network, which includes renewable generation, storage systems, and some controllable loads, can maintain its own systemwide supply and demand balance. With our mechanism, the closed-loop stability of the distributed microgrid systems can be guaranteed. In addition, we provide evaluation criteria of renewable energy utilization to validate our proposed method. Simulations show that the supply demand balance in each microgrid is achieved while, at the same time, the system operation cost is reduced, which demonstrates the effectiveness and efficiency of our proposed policy."
IOANNIS PASCHALIDIS,The price of anarchy in transportation networks by estimating user cost functions from actual traffic data,"We have considered a large-scale road network in Eastern Massachusetts. Using real traffic data in the form of spatial average speeds and the flow capacity for each road segment of the network, we converted the speed data to flow data and estimated the origin-destination flow demand matrices for the network. Assuming that the observed traffic data correspond to user (Wardrop) equilibria for different times-of-the-day and days-of-the-week, we formulated appropriate inverse problems to recover the per-road cost (congestion) functions determining user route selection for each month and time-of-day period. In addition, we analyzed the sensitivity of the total user latency cost to important parameters such as road capacities and minimum travel times. Finally, we formulated a system-optimum problem in order to find socially optimal flows for the network. We investigated the network performance, in terms of the total latency, under a user-optimal policy versus a system-optimal policy. The ratio of these two quantities is defined as the Price of Anarchy (POA) and quantifies the efficiency loss of selfish actions compared to socially optimal ones. Our findings contribute to efforts for a smarter and more efficient city."
IOANNIS PASCHALIDIS,Context-aware destination and time-to-destination prediction using machine learning,
IOANNIS PASCHALIDIS,Personalized hypertension treatment recommendations by a data-driven model,"BACKGROUND: Hypertension is a prevalent cardiovascular disease with severe longer-term implications. Conventional management based on clinical guidelines does not facilitate personalized treatment that accounts for a richer set of patient characteristics. METHODS: Records from 1/1/2012 to 1/1/2020 at the Boston Medical Center were used, selecting patients with either a hypertension diagnosis or meeting diagnostic criteria (≥ 130 mmHg systolic or ≥ 90 mmHg diastolic, n = 42,752). Models were developed to recommend a class of antihypertensive medications for each patient based on their characteristics. Regression immunized against outliers was combined with a nearest neighbor approach to associate with each patient an affinity group of other patients. This group was then used to make predictions of future Systolic Blood Pressure (SBP) under each prescription type. For each patient, we leveraged these predictions to select the class of medication that minimized their future predicted SBP. RESULTS: The proposed model, built with a distributionally robust learning procedure, leads to a reduction of 14.28 mmHg in SBP, on average. This reduction is 70.30% larger than the reduction achieved by the standard-of-care and 7.08% better than the corresponding reduction achieved by the 2nd best model which uses ordinary least squares regression. All derived models outperform following the previous prescription or the current ground truth prescription in the record. We randomly sampled and manually reviewed 350 patient records; 87.71% of these model-generated prescription recommendations passed a sanity check by clinicians. CONCLUSION: Our data-driven approach for personalized hypertension treatment yielded significant improvement compared to the standard-of-care. The model implied potential benefits of computationally deprescribing and can support situations with clinical equipoise."
IOANNIS PASCHALIDIS,Informative predictors of pregnancy after first IVF cycle using eIVF practice highway electronic health records,"The aim of this study is to determine the most informative pre- and in-cycle variables for predicting success for a first autologous oocyte in-vitro fertilization (IVF) cycle. This is a retrospective study using 22,413 first autologous oocyte IVF cycles from 2001 to 2018. Models were developed to predict pregnancy following an IVF cycle with a fresh embryo transfer. The importance of each variable was determined by its coefficient in a logistic regression model and the prediction accuracy based on different variable sets was reported. The area under the receiver operating characteristic curve (AUC) on a validation patient cohort was the metric for prediction accuracy. Three factors were found to be of importance when predicting IVF success: age in three groups (38-40, 41-42, and above 42 years old), number of transferred embryos, and number of cryopreserved embryos. For predicting first-cycle IVF pregnancy using all available variables, the predictive model achieved an AUC of 68% + /- 0.01%. A parsimonious predictive model utilizing age (38-40, 41-42, and above 42 years old), number of transferred embryos, and number of cryopreserved embryos achieved an AUC of 65% + /- 0.01%. The proposed models accurately predict a single IVF cycle pregnancy outcome and identify important predictive variables associated with the outcome. These models are limited to predicting pregnancy immediately after the IVF cycle and not live birth. These models do not include indicators of multiple gestation and are not intended for clinical application."
IOANNIS PASCHALIDIS,Communication-efficient SGD: from local SGD to one-shot averaging,"We consider speeding up stochastic gradient descent (SGD) by parallelizing it across multiple workers. We assume the same data set is shared among N workers, who can take SGD steps and coordinate with a central server. While it is possible to obtain a linear reduction in the variance by averaging all the stochastic gradients at every step, this requires a lot of communication between the workers and the server, which can dramatically reduce the gains from parallelism. The Local SGD method, proposed and analyzed in the earlier literature, suggests machines should make many local steps between such communications. While the initial analysis of Local SGD showed it needs Ω (√T) communications for T local gradient steps in order for the error to scale proportionately to 1=(NT), this has been successively improved in a string of papers, with the state of the art requiring Ω(N ( poly(log T))) communications. In this paper, we suggest a Local SGD scheme that communicates less overall by communicating less frequently as the number of iterations grows. Our analysis shows that this can achieve an error that scales as 1=(NT) with a number of communications that is completely independent of T. In particular, we show that Ω (N) communications are sufficient. Empirical evidence suggests this bound is close to tight as we further show that √ N or N^3/4 communications fail to achieve linear speed-up in simulations. Moreover, we show that under mild assumptions, the main of which is twice differentiability on any neighborhood of the optimal solution, one-shot averaging which only uses a single round of communication can also achieve the optimal convergence rate asymptotically."
IOANNIS PASCHALIDIS,Generalized proximal policy optimization with sample reuse,"In real-world decision making tasks, it is critical for data-driven reinforcement learning methods to be both stable and sample efficient. On-policy methods typically generate reliable policy improvement throughout training, while off-policy methods make more efficient use of data through sample reuse. In this work, we combine the theoretically supported stability benefits of on-policy algorithms with the sample efficiency of off-policy algorithms. We develop policy improvement guarantees that are suitable for the off-policy setting, and connect these bounds to the clipping mechanism used in Proximal Policy Optimization. This motivates an off-policy version of the popular algorithm that we call Generalized Proximal Policy Optimization with Sample Reuse. We demonstrate both theoretically and empirically that our algorithm delivers improved performance by effectively balancing the competing goals of stability and sample efficiency."
IOANNIS PASCHALIDIS,The impact of payer status on hospital admissions: evidence from an academic medical center,"BACKGROUND: There are plenty of studies investigating the disparity of payer status in accessing to care. However, most studies are either disease-specific or cohort-specific. Quantifying the disparity from the level of facility through a large controlled study are rare. This study aims to examine how the payer status affects patient hospitalization from the perspective of a facility. METHODS: We extracted all patients with visiting record in a medical center between 5/1/2009-4/30/2014, and then linked the outpatient and inpatient records three year before target admission time to patients. We conduct a retrospective observational study using a conditional logistic regression methodology. To control the illness of patients with different diseases in training the model, we construct a three-dimension variable with data stratification technology. The model is validated on a dataset distinct from the one used for training. RESULTS: Patients covered by private insurance or uninsured are less likely to be hospitalized than patients insured by government. For uninsured patients, inequity in access to hospitalization is observed. The value of standardized coefficients indicates that government-sponsored insurance has the greatest impact on improving patients' hospitalization. CONCLUSION: Attention is needed on improving the access to care for uninsured patients. Also, basic preventive care services should be enhanced, especially for people insured by government. The findings can serve as a baseline from which to measure the anticipated effect of measures to reduce disparity of payer status in hospitalization."
IOANNIS PASCHALIDIS,Physiological and socioeconomic characteristics predict COVID-19 mortality and resource utilization in Brazil,"BACKGROUND: Given the severity and scope of the current COVID-19 pandemic, it is critical to determine predictive features of COVID-19 mortality and medical resource usage to effectively inform health, risk-based physical distancing, and work accommodation policies. Non-clinical sociodemographic features are important explanatory variables of COVID-19 outcomes, revealing existing disparities in large health care systems. METHODS AND FINDINGS: We use nation-wide multicenter data of COVID-19 patients in Brazil to predict mortality and ventilator usage. The dataset contains hospitalized patients who tested positive for COVID-19 and had either recovered or were deceased between March 1 and June 30, 2020. A total of 113,214 patients with 50,387 deceased, were included. Both interpretable (sparse versions of Logistic Regression and Support Vector Machines) and state-of-the-art non-interpretable (Gradient Boosted Decision Trees and Random Forest) classification methods are employed. Death from COVID-19 was strongly associated with demographics, socioeconomic factors, and comorbidities. Variables highly predictive of mortality included geographic location of the hospital (OR = 2.2 for Northeast region, OR = 2.1 for North region); renal (OR = 2.0) and liver (OR = 1.7) chronic disease; immunosuppression (OR = 1.7); obesity (OR = 1.7); neurological (OR = 1.6), cardiovascular (OR = 1.5), and hematologic (OR = 1.2) disease; diabetes (OR = 1.4); chronic pneumopathy (OR = 1.4); immunosuppression (OR = 1.3); respiratory symptoms, ranging from respiratory discomfort (OR = 1.4) and dyspnea (OR = 1.3) to oxygen saturation less than 95% (OR = 1.7); hospitalization in a public hospital (OR = 1.2); and self-reported patient illiteracy (OR = 1.1). Validation accuracies (AUC) for predicting mortality and ventilation need reach 79% and 70%, respectively, when using only pre-admission variables. Models that use post-admission disease progression information reach accuracies (AUC) of 86% and 87% for predicting mortality and ventilation use, respectively. CONCLUSIONS: The results highlight the predictive power of socioeconomic information in assessing COVID-19 mortality and medical resource allocation, and shed light on existing disparities in the Brazilian health care system during the COVID-19 pandemic."
IOANNIS PASCHALIDIS,Joint pricing and rebalancing of autonomous mobility-on-demand systems,"This paper studies optimal pricing and rebalancing policies for Autonomous Mobility-on-Demand (AMoD) systems. We adopt a macroscopic planning perspective to tackle a profit maximization problem while ensuring that the system is load-balanced. We describe the system using a dynamic fluid model to show the existence and stability of an equilibrium (i.e., load balance) through pricing policies. We then develop an optimization framework that allows us to find optimal policies in terms of both pricing and rebalancing. We first maximize profit by only using pricing policies, then incorporate rebalancing, and finally we consider whether the solution is found sequentially or jointly. We apply each approach to a data-driven case study using real taxi data from New York City. Depending on which benchmarking solution we use, the joint problem (i.e., pricing and rebalancing) increases profits by 7% to 40%."
IOANNIS PASCHALIDIS,Designing metabolic division of labor in microbial communities,"Microbes face a trade-off between being metabolically independent and relying on neighboring organisms for the supply of some essential metabolites. This balance of conflicting strategies affects microbial community structure and dynamics, with important implications for microbiome research and synthetic ecology. A “gedanken” (thought) experiment to investigate this trade-off would involve monitoring the rise of mutual dependence as the number of metabolic reactions allowed in an organism is increasingly constrained. The expectation is that below a certain number of reactions, no individual organism would be able to grow in isolation and cross-feeding partnerships and division of labor would emerge. We implemented this idealized experiment using in silico genome-scale models. In particular, we used mixed-integer linear programming to identify trade-off solutions in communities of Escherichia coli strains. The strategies that we found revealed a large space of opportunities in nuanced and nonintuitive metabolic division of labor, including, for example, splitting the tricarboxylic acid (TCA) cycle into two separate halves. The systematic computation of possible solutions in division of labor for 1-, 2-, and 3-strain consortia resulted in a rich and complex landscape. This landscape displayed a nonlinear boundary, indicating that the loss of an intracellular reaction was not necessarily compensated for by a single imported metabolite. Different regions in this landscape were associated with specific solutions and patterns of exchanged metabolites. Our approach also predicts the existence of regions in this landscape where independent bacteria are viable but are outcompeted by cross-feeding pairs, providing a possible incentive for the rise of division of labor."
IOANNIS PASCHALIDIS,Robust measurement-based buffer overflow probability estimators for QoS provisioning and traffic anomaly prediction applications,"Suitable estimators for a class of Large Deviation approximations of rare event probabilities based on sample realizations of random processes have been proposed in our earlier work. These estimators are expressed as non-linear multi-dimensional optimization problems of a special structure. In this paper, we develop an algorithm to solve these optimization problems very efficiently based on their characteristic structure. After discussing the nature of the objective function and constraint set and their peculiarities, we provide a formal proof that the developed algorithm is guaranteed to always converge. The existence of efficient and provably convergent algorithms for solving these problems is a prerequisite for using the proposed estimators in real time problems such as call admission control, adaptive modulation and coding with QoS constraints, and traffic anomaly detection in high data rate communication networks."
IOANNIS PASCHALIDIS,Protein docking refinement by convex underestimation in the low-dimensional subspace of encounter complexes,"We propose a novel stochastic global optimization algorithm with applications to the refinement stage of protein docking prediction methods. Our approach can process conformations sampled from multiple clusters, each roughly corresponding to a different binding energy funnel. These clusters are obtained using a density-based clustering method. In each cluster, we identify a smooth “permissive” subspace which avoids high-energy barriers and then underestimate the binding energy function using general convex polynomials in this subspace. We use the underestimator to bias sampling towards its global minimum. Sampling and subspace underestimation are repeated several times and the conformations sampled at the last iteration form a refined ensemble. We report computational results on a comprehensive benchmark of 224 protein complexes, establishing that our refined ensemble significantly improves the quality of the conformations of the original set given to the algorithm. We also devise a method to enhance the ensemble from which near-native models are selected."
IOANNIS PASCHALIDIS,Learning from past bids to participate strategically in day-ahead electricity markets,"We consider the process of bidding by electricity suppliers in a day-ahead market context, where each supplier bids a linear non-decreasing function of her generating capacity with the goal of maximizing her individual profit given other competing suppliers' bids. Based on the submitted bids, the market operator schedules suppliers to meet demand during each hour and determines hourly market clearing prices. Eventually, this game-theoretic process reaches a Nash equilibrium when no supplier is motivated to modify her bid. However, solving the individual profit maximization problem requires information of rivals' bids, which are typically not available. To address this issue, we develop an inverse optimization approach for estimating rivals' production cost functions given historical market clearing prices and production levels. We then use these functions to bid strategically and compute Nash equilibrium bids. We present numerical experiments illustrating our methodology, showing good agreement between bids based on the estimated production cost functions with the bids based on the true cost functions. We discuss an extension of our approach that takes into account network congestion resulting in location-dependent prices"
IOANNIS PASCHALIDIS,Data-Driven Estimation in Equilibrium Using Inverse Optimization,"Equilibrium modeling is common in a variety of fields such as game theory and transportation science. The inputs for these models, however, are often difficult to estimate, while their outputs, i.e., the equilibria they are meant to describe, are often directly observable. By combining ideas from inverse optimization with the theory of variational inequalities, we develop an efficient, data-driven technique for estimating the parameters of these models from observed equilibria. We use this technique to estimate the utility functions of players in a game from their observed actions and to estimate the congestion function on a road network from traffic count data. A distinguishing feature of our approach is that it supports both parametric and \emph{nonparametric} estimation by leveraging ideas from statistical learning (kernel methods and regularization operators). In computational experiments involving Nash and Wardrop equilibria in a nonparametric setting, we find that a) we effectively estimate the unknown demand or congestion function, respectively, and b) our proposed regularization technique substantially improves the out-of-sample performance of our estimators."
IOANNIS PASCHALIDIS,Botnet detection using social graph analysis,"We consider the problem of finding a control policy for a Markov Decision Process (MDP) to maximize the probability of reaching some states while avoiding some other states. This problem is motivated by applications in robotics, where such problems naturally arise when probabilistic models of robot motion are required to satisfy temporal logic task specifications. We transform this problem into a Stochastic Shortest Path (SSP) problem and develop a new approximate dynamic programming algorithm to solve it. This algorithm is of the actor-critic type and uses a least-square temporal difference learning method. It operates on sample paths of the system and optimizes the policy within a pre-specified class parameterized by a parsimonious set of parameters. We show its convergence to a policy corresponding to a stationary point in the parameters' space. Simulation results confirm the effectiveness of the proposed solution."
IOANNIS PASCHALIDIS,An Improved Composite Hypothesis Test for Markov Models with Applications in Network Anomaly Detection,"Recent work has proposed the use of a composite hypothesis Hoeffding test for statistical anomaly detection. Setting an appropriate threshold for the test given a desired false alarm probability involves approximating the false alarm probability. To that end, a large deviations asymptotic is typically used which, however, often results in an inaccurate setting of the threshold, especially for relatively small sample sizes. This, in turn, results in an anomaly detection test that does not control well for false alarms. In this paper, we develop a tighter approximation using the Central Limit Theorem (CLT) under Markovian assumptions. We apply our result to a network anomaly detection application and demonstrate its advantages over earlier work."
IOANNIS PASCHALIDIS,Automated detection of mild cognitive impairment and dementia from voice recordings: A natural language processing approach,"INTRODUCTION: Automated computational assessment of neuropsychological tests would enable widespread, cost-effective screening for dementia. METHODS: A novel natural language processing approach is developed and validated to identify different stages of dementia based on automated transcription of digital voice recordings of subjects' neuropsychological tests conducted by the Framingham Heart Study (n = 1084). Transcribed sentences from the test were encoded into quantitative data and several models were trained and tested using these data and the participants' demographic characteristics. RESULTS: Average area under the curve (AUC) on the held-out test data reached 92.6%, 88.0%, and 74.4% for differentiating Normal cognition from Dementia, Normal or Mild Cognitive Impairment (MCI) from Dementia, and Normal from MCI, respectively. DISCUSSION: The proposed approach offers a fully automated identification of MCI and dementia based on a recorded neuropsychological test, providing an opportunity to develop a remote screening tool that could be adapted easily to any language."
IOANNIS PASCHALIDIS,Protein Docking by the Underestimation of Free Energy Funnels in the Space of Encounter Complexes,"Similarly to protein folding, the association of two proteins is driven by a free energy funnel, determined by favorable interactions in some neighborhood of the native state. We describe a docking method based on stochastic global minimization of funnel-shaped energy functions in the space of rigid body motions (SE(3)) while accounting for flexibility of the interface side chains. The method, called semi-definite programming-based underestimation (SDU), employs a general quadratic function to underestimate a set of local energy minima and uses the resulting underestimator to bias further sampling. While SDU effectively minimizes functions with funnel-shaped basins, its application to docking in the rotational and translational space SE(3) is not straightforward due to the geometry of that space. We introduce a strategy that uses separate independent variables for side-chain optimization, center-to-center distance of the two proteins, and five angular descriptors of the relative orientations of the molecules. The removal of the center-to-center distance turns out to vastly improve the efficiency of the search, because the five-dimensional space now exhibits a well-behaved energy surface suitable for underestimation. This algorithm explores the free energy surface spanned by encounter complexes that correspond to local free energy minima and shows similarity to the model of macromolecular association that proceeds through a series of collisions. Results for standard protein docking benchmarks establish that in this space the free energy landscape is a funnel in a reasonably broad neighborhood of the native state and that the SDU strategy can generate docking predictions with less than 5 Å ligand interface Cα root-mean-square deviation while achieving an approximately 20-fold efficiency gain compared to Monte Carlo methods. Author SummaryProtein–protein interactions play a central role in various aspects of the structural and functional organization of the cell, and their elucidation is crucial for a better understanding of processes such as metabolic control, signal transduction, and gene regulation. Genomewide proteomics studies, primarily yeast two-hybrid assays, will provide an increasing list of interacting proteins, but only a small fraction of the potential complexes will be amenable to direct experimental analysis. Thus, it is important to develop computational docking methods that can elucidate the details of specific interactions at the atomic level. Protein–protein docking generally starts with a rigid body search that generates a large number of docked conformations with good shape, electrostatic, and chemical complementarity. The conformations are clustered to obtain a manageable number of models, but the current methods are unable to select the most likely structure among these models. Here we describe a refinement algorithm that, applied to the individual clusters, improves the quality of the models. The better models are suitable for higher-accuracy energy calculation, thereby increasing the chances that near-native structures can be identified, and thus the refinement increases the reliability of the entire docking algorithm."
IOANNIS PASCHALIDIS,An optimal transmission line switching and bus splitting heuristic incorporating AC and N-1 contingency constraints,"Optimal transmission line switching and/or bus splitting is shown to contribute in relieving congestion and reducing the operation cost by rerouting power flows throughout the network. Although bus splitting may be as powerful as line switching in congestion mitigation and is typically considered a smaller disturbance compared with line switching, it has received less attention in the literature in part due to the more complicated node-breaker modeling requirement. In this paper, an optimal transmission line switching and bus splitting heuristic is presented to minimize the operation cost while respecting AC and N-1 contingency constraints. We present a two-level solution method where switching decisions are made in the upper level problem formulated as a mixed integer second order cone programming master problem, while the resulting network topology is checked against AC and N-1 contingency constraints in lower level subproblems. Line switching and bus splitting are modeled as switching actions assuming double-bus double-breaker substation arrangements where all elements at a substation, including generators, loads, lines and shunt elements, are given switches to connect to either of the busbars if the respective substation is split. We also introduce additional constraints to model a breaker-and-a-half substation scheme. Furthermore, a pre-screening step is presented to limit the search space of the problem, thus accelerating the solution process. We demonstrate the application of the proposed method on IEEE standard test systems."
IOANNIS PASCHALIDIS,Distributionally robust multiclass classification and applications in deep image classifiers,
IOANNIS PASCHALIDIS,Encounter complexes and dimensionality reduction in protein-protein association,"An outstanding challenge has been to understand the mechanism whereby proteins associate. We report here the results of exhaustively sampling the conformational space in protein–protein association using a physics-based energy function. The agreement between experimental intermolecular paramagnetic relaxation enhancement (PRE) data and the PRE profiles calculated from the docked structures shows that the method captures both specific and non-specific encounter complexes. To explore the energy landscape in the vicinity of the native structure, the nonlinear manifold describing the relative orientation of two solid bodies is projected onto a Euclidean space in which the shape of low energy regions is studied by principal component analysis. Results show that the energy surface is canyon-like, with a smooth funnel within a two dimensional subspace capturing over 75% of the total motion. Thus, proteins tend to associate along preferred pathways, similar to sliding of a protein along DNA in the process of protein-DNA recognition."
IOANNIS PASCHALIDIS,Temporal Logic Motion Control using Actor-Critic Methods,"In this paper, we consider the problem of deploying a robot from a specification given as a temporal logic statement about some properties satisfied by the regions of a large, partitioned environment. We assume that the robot has noisy sensors and actuators and model its motion through the regions of the environment as a Markov Decision Process (MDP). The robot control problem becomes finding the control policy maximizing the probability of satisfying the temporal logic task on the MDP. For a large environment, obtaining transition probabilities for each state-action pair, as well as solving the necessary optimization problem for the optimal policy are usually not computationally feasible. To address these issues, we propose an approximate dynamic programming framework based on a least-square temporal difference learning method of the actor-critic type. This framework operates on sample paths of the robot and optimizes a randomized control policy with respect to a small set of parameters. The transition probabilities are obtained only when needed. Hardware-in-the-loop simulations confirm that convergence of the parameters translates to an approximately optimal policy."
IOANNIS PASCHALIDIS,Least squares temporal difference actor-critic methods with applications to robot motion control,"We consider the problem of finding a control policy for a Markov Decision Process (MDP) to maximize the probability of reaching some states while avoiding some other states. This problem is motivated by applications in robotics, where such problems naturally arise when probabilistic models of robot motion are required to satisfy temporal logic task specifications. We transform this problem into a Stochastic Shortest Path (SSP) problem and develop a new approximate dynamic programming algorithm to solve it. This algorithm is of the actor-critic type and uses a least-square temporal difference learning method. It operates on sample paths of the system and optimizes the policy within a pre-specified class parameterized by a parsimonious set of parameters. We show its convergence to a policy corresponding to a stationary point in the parameters' space. Simulation results confirm the effectiveness of the proposed solution."
IOANNIS PASCHALIDIS,Data center and load aggregator coordination towards electricity demand response,
IOANNIS PASCHALIDIS,Predicting polycystic ovary syndrome with machine learning algorithms from electronic health records,"INTRODUCTION: Predictive models have been used to aid early diagnosis of PCOS, though existing models are based on small sample sizes and limited to fertility clinic populations. We built a predictive model using machine learning algorithms based on an outpatient population at risk for PCOS to predict risk and facilitate earlier diagnosis, particularly among those who meet diagnostic criteria but have not received a diagnosis. METHODS: This is a retrospective cohort study from a SafetyNet hospital's electronic health records (EHR) from 2003-2016. The study population included 30,601 women aged 18-45 years without concurrent endocrinopathy who had any visit to Boston Medical Center for primary care, obstetrics and gynecology, endocrinology, family medicine, or general internal medicine. Four prediction outcomes were assessed for PCOS. The first outcome was PCOS ICD-9 diagnosis with additional model outcomes of algorithm-defined PCOS. The latter was based on Rotterdam criteria and merging laboratory values, radiographic imaging, and ICD data from the EHR to define irregular menstruation, hyperandrogenism, and polycystic ovarian morphology on ultrasound. RESULTS: We developed predictive models using four machine learning methods: logistic regression, supported vector machine, gradient boosted trees, and random forests. Hormone values (follicle-stimulating hormone, luteinizing hormone, estradiol, and sex hormone binding globulin) were combined to create a multilayer perceptron score using a neural network classifier. Prediction of PCOS prior to clinical diagnosis in an out-of-sample test set of patients achieved an average AUC of 85%, 81%, 80%, and 82%, respectively in Models I, II, III and IV. Significant positive predictors of PCOS diagnosis across models included hormone levels and obesity; negative predictors included gravidity and positive bHCG. CONCLUSION: Machine learning algorithms were used to predict PCOS based on a large at-risk population. This approach may guide early detection of PCOS within EHR-interfaced populations to facilitate counseling and interventions that may reduce long-term health consequences. Our model illustrates the potential benefits of an artificial intelligence-enabled provider assistance tool that can be integrated into the EHR to reduce delays in diagnosis. However, model validation in other hospital-based populations is necessary."
IOANNIS PASCHALIDIS,Network anomaly detection: a survey and comparative analysis of stochastic and deterministic methods,"We present five methods to the problem of network anomaly detection. These methods cover most of the common techniques in the anomaly detection field, including Statistical Hypothesis Tests (SHT), Support Vector Machines (SVM) and clustering analysis. We evaluate all methods in a simulated network that consists of nominal data, three flow-level anomalies and one packet-level attack. Through analyzing the results, we point out the advantages and disadvantages of each method and conclude that combining the results of the individual methods can yield improved anomaly detection results."
IOANNIS PASCHALIDIS,On the performance of temporal difference learning with neural networks,
IOANNIS PASCHALIDIS,Distributionally robust multiclass classification and applications in deep image classifiers,
IOANNIS PASCHALIDIS,A new paradigm for pandemic preparedness,"PURPOSE OF REVIEW: Preparing for pandemics requires a degree of interdisciplinary work that is challenging under the current paradigm. This review summarizes the challenges faced by the field of pandemic science and proposes how to address them. RECENT FINDINGS: The structure of current siloed systems of research organizations hinders effective interdisciplinary pandemic research. Moreover, effective pandemic preparedness requires stakeholders in public policy and health to interact and integrate new findings rapidly, relying on a robust, responsive, and productive research domain. Neither of these requirements are well supported under the current system. SUMMARY: We propose a new paradigm for pandemic preparedness wherein interdisciplinary research and close collaboration with public policy and health practitioners can improve our ability to prevent, detect, and treat pandemics through tighter integration among domains, rapid and accurate integration, and translation of science to public policy, outreach and education, and improved venues and incentives for sustainable and robust interdisciplinary work."
IOANNIS PASCHALIDIS,Large language models in neurology research and future practice,"Recent advancements in generative artificial intelligence, particularly using large language models (LLMs), are gaining increased public attention. We provide a perspective on the potential of LLMs to analyze enormous amounts of data from medical records and gain insights on specific topics in neurology. In addition, we explore use cases for LLMs, such as early diagnosis, supporting patient and caregivers, and acting as an assistant for clinicians. We point to the potential ethical and technical challenges raised by LLMs, such as concerns about privacy and data security, potential biases in the data for model training, and the need for careful validation of results. Researchers must consider these challenges and take steps to address them to ensure that their work is conducted in a safe and responsible manner. Despite these challenges, LLMs offer promising opportunities for improving care and treatment of various neurologic disorders."
IOANNIS PASCHALIDIS,Improved prediction of MHC-peptide binding using protein language models,"Major histocompatibility complex Class I (MHC-I) molecules bind to peptides derived from intracellular antigens and present them on the surface of cells, allowing the immune system (T cells) to detect them. Elucidating the process of this presentation is essential for regulation and potential manipulation of the cellular immune system. Predicting whether a given peptide binds to an MHC molecule is an important step in the above process and has motivated the introduction of many computational approaches to address this problem. NetMHCPan, a pan-specific model for predicting binding of peptides to any MHC molecule, is one of the most widely used methods which focuses on solving this binary classification problem using shallow neural networks. The recent successful results of Deep Learning (DL) methods, especially Natural Language Processing (NLP-based) pretrained models in various applications, including protein structure determination, motivated us to explore their use in this problem. Specifically, we consider the application of deep learning models pretrained on large datasets of protein sequences to predict MHC Class I-peptide binding. Using the standard performance metrics in this area, and the same training and test sets, we show that our models outperform NetMHCpan4.1, currently considered as the-state-of-the-art."
IOANNIS PASCHALIDIS,Distributionally robust learning-to-rank under the Wasserstein metric,
IOANNIS PASCHALIDIS,A Stackelberg game approach to control the overall load consumption of a residential neighborhood,
MATTHEW CARTMILL,A sort of revolution: Systematics and physical anthropology in the 20th century,
MATTHEW CARTMILL,"Acceptance speech, 2019 Darwin Award",
MATTHEW CARTMILL,The gaits of marsupials and the evolution of diagonal-sequence walking in primates,"OBJECTIVES Documenting the variety of quadrupedal walking gaits in a variety of marsupials (arboreal vs. terrestrial, with and without grasping hind feet), to aid in developing and refining a general theory of gait evolution in primates. MATERIALS AND METHODS Video records of koalas, ringtail possums, tree kangaroos, sugar gliders, squirrel gliders, wombats, numbats, quolls, a thylacine, and an opossum walking on a variety of substrates were made and analyzed to derive duty factors and diagonalities for symmetrical walking gaits. The resulting distributions of data points were compared with published data and theories. RESULTS Terrestrial marsupials' gaits overwhelmingly plot slightly below the theoretical “horse line” (Cartmill et al., Zoological Journal of the Linnean Society. 2002;136:401–420) typical of terrestrial mammals; arboreal marsupials' gaits overwhelmingly plot more decisively above it. Both distributions are roughly parallel to the horse line, but arboreal animals exhibit increased diagonality, so that their higher‐speed walking gaits overlap with those of typical primates on the Hildebrand diagram of diagonality against duty factor. CONCLUSIONS Quadrupeds avoid gaits lying exactly on the (theoretically optimum) horse line, to avoid fore/hind limb interference (“forging”). This can be accomplished by either a slight reduction in diagonality (“downshifting”) or a more decisive increase (“upshifting”). Tree‐dwellers adopt the second option to eliminate unilateral bipods of support from the gait cycle. The upshifted horse line represents an early phase in the evolution of primate‐like diagonal‐sequence gaits."
MATTHEW CARTMILL,"Come back, Lucretius",
MATTHEW CARTMILL,"Those who can, teach",
MATTHEW CARTMILL,Do beetles have experiences? How can we tell?,
MATTHEW CARTMILL,Clavicle length and shoulder breadth in hominoid evolution,"For a given body mass, hominoids have longer clavicles than typical monkeys, reflecting the laterad reorientation of the hominoid glenoid. Relative length of the clavicle varies among hominoids, with orangutans having longer clavicles than expected for body mass and gorillas and chimpanzees having shorter clavicles than expected. Modern humans conform to the general hominoid distribution, but Neandertals have longer clavicles than expected for their size. [TRUNCATED]"
MATTHEW CARTMILL,A re-evaluation of fossil hominin obstetric constraints,
MATTHEW CARTMILL,"What, if anything, is Australopithecus afarensis?",
MATTHEW CARTMILL,Birth canal shape and fetal rotation in Australopithecus and Neandertals,
MATTHEW CARTMILL,Convergent? Minds? Some questions about mental evolution,"In investigating convergent minds, we need to be sure that the things we are looking at are both minds and convergent. In determining whether a shared character state represents a convergence between two organisms, we must know the wider distribution and primitive state of that character so that we can map that character and its state transitions onto a phylogenetic tree. When we do this, some apparently primitive shared traits may prove to represent convergent losses of cognitive capacities. To avoid having to talk about the minds of plants and paramecia, we need to go beyond assessments of behaviourally defined cognition to ask questions about mind in the primary sense of the word, defined by the presence of mental events and consciousness. These phenomena depend upon the possession of brains of adequate size and centralized ontogeny and organization. They are probably limited to vertebrates. Recent discoveries suggest that consciousness is adaptively valuable as a late error-detection mechanism in the initiation of action, and point to experimental techniques for assessing its presence or absence in non-human mammals."
WILLIAM J LEHMAN,Inhibition of the translation initiation factor eIF4A enhances tumor cell radiosensitivity,"A fundamental component of cellular radioresponse is the translational control of gene expression. Because a critical regulator of translational control is the eukaryotic translation initiation factor 4F (eIF4F) cap binding complex, we investigated whether eIF4A, the RNA helicase component of eIF4F, can serve as a target for radiosensitization. Knockdown of eIF4A using siRNA reduced translational efficiency, as determined from polysome profiles, and enhanced tumor cell radiosensitivity as determined by clonogenic survival. The increased radiosensitivity was accompanied by a delayed dispersion of radiation-induced γH2AX foci, suggestive of an inhibition of DNA double-strand break repair. Studies were then extended to (-)-SDS-1-021, a pharmacologic inhibitor of eIF4A. Treatment of cells with the rocaglate (-)-SDS-1-021 resulted in a decrease in translational efficiency as well as protein synthesis. (-)-SDS-1-021 treatment also enhanced the radiosensitivity of tumor cell lines. This (-)-SDS-1-021-induced radiosensitization was accompanied by a delay in radiation-induced γH2AX foci dispersal, consistent with a causative role for the inhibition of double-strand break repair. In contrast, although (-)-SDS-1-021 inhibited translation and protein synthesis in a normal fibroblast cell line, it had no effect on radiosensitivity of normal cells. Subcutaneous xenografts were then used to evaluate the in vivo response to (-)-SDS-1-021 and radiation. Treatment of mice bearing subcutaneous xenografts with (-)-SDS-1-021 decreased tumor translational efficiency as determined by polysome profiles. Although (-)-SDS-1-021 treatment alone had no effect on tumor growth, it significantly enhanced the radiation-induced growth delay. These results suggest that eIF4A is a tumor-selective target for radiosensitization."
WESLEY J WILDMAN,A Neuropsychological Semiotic Model of Religious Experiences,
WESLEY J WILDMAN,Conciousness Expanded,"Many kinds of human states of consciousness have been distinguished, including colourful or anomalous experiences that are felt to have spiritual significance by most people who have them. The neurosciences have isolated brain-state correlates for some of these colourful states of consciousness, thereby strengthening the hypothesis that these experiences are mediated by the brain. This result both challenges metaphysically dualist accounts of human nature and suggests that any adequate causal explanation of colourful experiences would have to make detailed reference to the evolutionary and genetic conditions that give rise to brains capable of such conscious phenomena. This paper quickly surveys types of conscious states and neurological interpretations of them. In order to deal with the question of the significance of such experiences, the paper then attempts to identify evolutionary and genetic constraints on proposals for causal explanations of such experiences. The conclusion is that a properly sensitive evolutionary account of human consciousness supports a rebuttal of the argument that the cognitive content of colourful experiences is pure delusion, but that this evolutionary account also heavily constrains what might be inferred theologically from such experiences. They are not necessarily delusory, therefore, but they are often highly misleading. Their significance must be construed consistently with this conclusion."
WESLEY J WILDMAN,2000 Editors,
WESLEY J WILDMAN,But consciousness isn't everything.,"Comments on an article entitled `No Good News for DATA,' by Norman Lillegard in the Spring 1994 issue of `Cross Currents' magazine. Lillegard's stand that the android Commander Data in `Star Trek' fails to satisfy the biblical conception of persons; Lillegard's presentation of models that espouse functionalist theories of mind; Views of the essence of the human person."
WESLEY J WILDMAN,The Significance of the Evolution of Religious Belief and Behavior for Religious Studies and Theology,
WESLEY J WILDMAN,"A Theological Challenge: Coordinating Biological, Social, and Religious Visions of Humanity","This paper attempts two tasks. First, it sketches how the natural sciences (including especially the biological sciences), the social sciences, and the scientific study of religion can be understood to furnish complementary, consonant perspectives on human beings and human groups. This suggests that it is possible to speak of a modern secular interpretation of humanity (MSIH) to which these perspectives contribute (though not without tensions). MSIH is not a comprehensive interpretation of human beings, if only because it adopts a posture of neutrality with regard to the reality of religious objects and the truth of theological claims about them. MSIH is certainly an impressively forceful interpretation, however, and it needs to be reckoned with by any perspective on human life that seeks to insert its truth claims into the arena of public debate. Second, the paper considers two challenges that MSIH poses to specifically theological interpretations of human beings. On the one hand, in spite of its posture of religious neutrality, MSIH is a key element in a class of wider, seemingly antireligious interpretations of humanity, including especially projectionist and illusionist critiques of religion. It is consonance with MSIH that makes these critiques such formidable competitors for traditional theological interpretations of human beings. On the other hand, and taking the religiously neutral posture of MSIH at face value, theological accounts of humanity that seek to coordinate the insights of MSIH with positive religious visions of human life must find ways to overcome or manage such dissonance as arises. The goal of synthesis is defended as important, and strategies for managing these challenges, especially in light of the pluralism of extant philosophical and theological interpretations of human beings, are advocated."
WESLEY J WILDMAN,"Behind, Between, and Beyond Anthropomorphic Models","The plurality of models of ultimate reality is a central problem for religious philosophy. This essay sketches what is involved in mounting comparative inquiries across the plurality of models. In order to illustrate what advance would look like in such a comparative inquiry, an argument is presented to show that highly anthropomorphic models of ultimate reality are inferior to a number of competitors. This paper was delivered as a keynote address during the APA Pacific 2007 Mini-Conference on Models of God."
WESLEY J WILDMAN,Comparing Religious Ideas: There’s Method in the Mob’s Madness,
WESLEY J WILDMAN,"The Divine Action Project, 1988–2003","This article explores the state of the art in theories of special divine action by means of a study of the Divine Action Project (DAP) co-sponsored by the Vatican Observatory and the Center for Theology and the Natural Sciences in Berkeley. The basic aim is to introduce the DAP and to summarize its results, especially as these were compiled in the final “capstone” meeting of the DAP, and drawing on the published output of the project where possible. The subsidiary aim is to evaluate criticisms of theories of special divine action developed within the DAP."
WESLEY J WILDMAN,Comparative natural theology,
MARK A FRIEDL,A high spatial resolution land surface phenology dataset for AmeriFlux and NEON sites,"Vegetation phenology is a key control on water, energy, and carbon fluxes in terrestrial ecosystems. Because vegetation canopies are heterogeneous, spatially explicit information related to seasonality in vegetation activity provides valuable information for studies that use eddy covariance measurements to study ecosystem function and land-atmosphere interactions. Here we present a land surface phenology (LSP) dataset derived at 3 m spatial resolution from PlanetScope imagery across a range of plant functional types and climates in North America. The dataset provides spatially explicit information related to the timing of phenophase changes such as the start, peak, and end of vegetation activity, along with vegetation index metrics and associated quality assurance flags for the growing seasons of 2017-2021 for 10 × 10 km windows centred over 104 eddy covariance towers at AmeriFlux and National Ecological Observatory Network (NEON) sites. These LSP data can be used to analyse processes controlling the seasonality of ecosystem-scale carbon, water, and energy fluxes, to evaluate predictions from land surface models, and to assess satellite-based LSP products."
MARK A FRIEDL,The role of land cover change in Arctic-Boreal greening and browning trends,"Many studies have used time series of satellite-derived vegetation indices to identify so-called greening and browning trends across the northern high-latitudes and to suggest that the productivity of Arctic-Boreal ecosystems is changing in response to climate forcing at local and continental scales. However, disturbances that alter land cover are prevalent in Arctic-Boreal ecosystems, and changes in Arctic-Boreal land cover, which complicate interpretation of trends in vegetation indices, have mostly been ignored in previous studies. Here we use a new land cover change dataset derived from Landsat imagery to explore the extent to which land cover and land cover change influence trends in the normalized difference vegetation index (NDVI) over a large (3.76 M km2) area of NASA's Arctic Boreal Vulnerability Experiment, which spans much of northwestern Canada and Alaska. Between 1984 and 2012, 21.2% of the study domain experienced land cover change and 42.7% had significant NDVI trends. Land cover change occurred in 27.6% of locations with significant NDVI trends during this period and resulted in greening and browning rates 48%–128% higher than in areas of stable land cover. While the majority of land cover change areas experienced significant NDVI trends, more than half of areas with stable land cover did not. Further, the extent and magnitude of browning and greening trends varied substantially as a function of land cover class and land cover change type. Forest disturbance from fire and timber harvest drove over one third of statistically significant NDVI trends and created complex mosaics of recent forest loss (as browning) and post-disturbance recovery (as greening) at both landscape and continental scale. Our results demonstrate the importance of land cover changes in highly disturbed high-latitude ecosystems for interpreting trends of NDVI and productivity across multiple spatial scales."
MARK A FRIEDL,Sensitivity of global pasturelands to climate variation,"Pasturelands are globally extensive, sensitive to climate, and support livestock production systems that provide an essential source of food in many parts of the world. In this paper, we integrate information from remote sensing, global climate, and land use databases to improve understanding of the resilience and resistance of this ecologically vulnerable and societally-critical land use. To characterize the effect of climate on pastureland productivity, we analyze the relationship between satellite-derived vegetation index and gridded precipitation datasets at 1 to 6-month time lags. To account for the effects of different production systems, we stratify our analysis by agroecological zone and by rangeland-based versus mixed crop-livestock system. Results show that 14.5% of global pasturelands experienced statistically significant greening or browning trends over the 15-year study period, with the majority of these locations showing greening. In arid ecosystems, precipitation and lagged vegetation index anomalies explain up to 69% of variation in vegetation productivity in both crop-livestock and rangeland-based production systems. Livestock production systems in Australia are least resistant to contemporaneous and short-term precipitation anomalies, while arid livestock production systems in Latin America are least resilient to short-term vegetation greenness anomalies. More generally, large swaths of semi-arid global pasturelands show substantial sensitivity to variation in precipitation, and hence, are vulnerable to climate change. Because many arid regions of the world are projected to experience decreased total precipitation and increased precipitation variability in the coming decades, improved understanding regarding the sensitivity of pasturelands to the joint effects of climate change and production system is required to support sustainable land management in global pasturelands."
MARK A FRIEDL,Using time series of MODIS land surface phenology to model temperature and photoperiod controls on spring greenup in North American deciduous forests,"The timing of leaf emergence in temperate and boreal forests is changing, which has profound implications for a wide array of ecosystem processes and services. Spring phenology models, which have been widely used to predict the timing of leaf emergence, generally assume that a combination of photoperiod and thermal forcing control when leaves emerge. However, the exact nature and magnitude of how photoperiod and temperature individually and jointly control leaf emergence is the subject of ongoing debate. Here we use a continuous development model in combination with time series of land surface phenology measurements from MODIS to quantify the relative importance of photoperiod and thermal forcing in controlling the timing of canopy greenup in eastern temperate and boreal forests of North America. The model accurately predicts biogeographic and interannual variation in the timing of greenup across the study region (median RMSE = 4.6 days, median bias = 0.30 days). Results reveal strong biogeographic variation in the period prior to greenup when temperature and photoperiod influence greenup that covaries with the importance of photoperiod versus thermal controls. Photoperiod control on leaf emergence is dominant in warmer climates, but exerts only modest influence on the timing of leaf emergence in colder climates. Results from models estimated using ground-based observations of cloned lilac are consistent with those from remote sensing, which supports the realism of remote sensing-based models. Overall, results from this study suggest that apparent changes in the sensitivity of trees to temperature are modest and reflect a trade-off between decreased sensitivity to temperature and increased photoperiod control, and identify a transition in the relative importance of temperature versus photoperiod near the 10 °C isotherm in mean annual temperature. This suggests that the timing of leaf emergence will continue to move earlier as the climate warms, and that the magnitude of change will be more pronounced in colder regions with mean annual temperatures below 10 °C."
MARK A FRIEDL,Widespread changes in 21st century vegetation cover in the Southern Cone of South America,"South America has been an epicenter of land cover and land use change (LCLUC) for over five decades due to rapid agricultural expansion along forest frontiers, the establishment of plantations in savannas, and desertification in drylands. Most attention has focused on LCLUC in tropical forests, and so information regarding the magnitude, geography, and rate of LCLUC across the Southern Cone region (SCR) of South America is incomplete. To address this, we used Landsat to map changes in the fractional cover of bare ground, woody cover, and herbaceous vegetation at annual time steps from 1999 to 2019 over the SCR. Using field observations and Landsat imagery, we created a spectral library representative of these three cover types. We trained a machine learning model to map annual fractional cover at 30-meter spatial resolution, and used a Bayesian change point algorithm to characterize spatial and temporal trajectories of LCLUC. Our results reveal substantial changes in land cover composition over the SCR between 1999 and 2019, totaling 389,973.2 km2, equivalent to 11.6% of the study domain. Herbaceous cover in Paraguay increased by 51%, mostly because of deforestation for cattle ranching in the Dry Chaco and commodity crop agriculture in the Atlantic Forest. Uruguay showed a 62% increase in woody cover arising from the emergence and growth of pine and eucalyptus plantations. Argentina, the largest and most heterogeneous of the three countries, experienced a 38% increase in bare ground in the Patagonian Steppe due to climate and anthropogenic drivers, including reduced precipitation. Quantification of these abrupt and gradual LCLUC processes can be used to improve models of the carbon budget in the SCR and to measure carbon exchange in arid and semi-arid ecoregions, as they are increasingly understood to be important drivers of the interannual variability of the global carbon cycle."
MARK A FRIEDL,Need and vision for global medium-resolution Landsat and Sentinel-2 data products,
DAVID J SALANT,Noninvasive Assessment of Antenatal Hydronephrosis in Mice Reveals a Critical Role for Robo2 in Maintaining Anti-Reflux Mechanism,"Antenatal hydronephrosis and vesicoureteral reflux (VUR) are common renal tract birth defects. We recently showed that disruption of the Robo2 gene is associated with VUR in humans and antenatal hydronephrosis in knockout mice. However, the natural history, causal relationship and developmental origins of these clinical conditions remain largely unclear. Although the hydronephrosis phenotype in Robo2 knockout mice has been attributed to the coexistence of ureteral reflux and obstruction in the same mice, this hypothesis has not been tested experimentally. Here we used noninvasive high- resolution micro-ultrasonography and pathological analysis to follow the progression of antenatal hydronephrosis in individual Robo2-deficient mice from embryo to adulthood. We found that hydronephrosis progressed continuously after birth with no spontaneous resolution. With the use of a microbubble ultrasound contrast agent and ultrasound-guided percutaneous aspiration, we demonstrated that antenatal hydronephrosis in Robo2-deficient mice is caused by high-grade VUR resulting from a dilated and incompetent ureterovesical junction rather than ureteral obstruction. We further documented Robo2 expression around the developing ureterovesical junction and identified early dilatation of ureteral orifice structures as a potential fetal origin of antenatal hydronephrosis and VUR. Our results thus demonstrate that Robo2 is crucial for the formation of a normal ureteral orifice and for the maintenance of an effective anti-reflux mechanism. This study also establishes a reproducible genetic mouse model of progressive antenatal hydronephrosis and primary high- grade VUR."
DAVID J SALANT,Inhibitory Effects of Robo2 on Nephrin: A Crosstalk between Positive and Negative Signals Regulating Podocyte Structure,"Robo2 is the cell surface receptor for the repulsive guidance cue Slit and is involved in axon guidance and neuronal migration. Nephrin is a podocyte slit- diaphragm protein that functions in the kidney glomerular filtration barrier. Here, we report that Robo2 is expressed at the basal surface of mouse podocytes and colocalizes with nephrin. Biochemical studies indicate that Robo2 forms a complex with nephrin in the kidney through adaptor protein Nck. In contrast to the role of nephrin that promotes actin polymerization, Slit2-Robo2 signaling inhibits nephrin-induced actin polymerization. In addition, the amount of F-actin associated with nephrin is increased in Robo2 knockout mice that develop an altered podocyte foot process structure. Genetic interaction study further reveals that loss of Robo2 alleviates the abnormal podocyte structural pheno- type in nephrin null mice. These results suggest that Robo2 signaling acts as a negative regulator on neph- rin to influence podocyte foot process architecture."
DAVID J SALANT,"Towards minimally-invasive, quantitative assessment of chronic kidney disease using optical spectroscopy","The universal pathologic features implicated in the progression of chronic kidney disease (CKD) are interstitial fibrosis and tubular atrophy (IFTA). Current methods of estimating IFTA are slow, labor-intensive and fraught with variability and sampling error, and are not quantitative. As such, there is pressing clinical need for a less-invasive and faster method that can quantitatively assess the degree of IFTA. We propose a minimally-invasive optical method to assess the macro-architecture of kidney tissue, as an objective, quantitative assessment of IFTA, as an indicator of the degree of kidney disease. The method of elastic-scattering spectroscopy (ESS) measures backscattered light over the spectral range 320-900 nm and is highly sensitive to micromorphological changes in tissues. Using two discrete mouse models of CKD, we observed spectral trends of increased scattering intensity in the near-UV to short-visible region (350-450 nm), relative to longer wavelengths, for fibrotic kidneys compared to normal kidney, with a quasi-linear correlation between the ESS changes and the histopathology-determined degree of IFTA. These results suggest the potential of ESS as an objective, quantitative and faster assessment of IFTA for the management of CKD patients and in the allocation of organs for kidney transplantation."
GHEORGHE DOROS,"Caregiving, Metabolic Syndrome Indicators, and 1-year Decline in Walking Speed: Results of Caregiver-SOF","BACKGROUND Chronic stress may lead to health decline through metabolic syndrome. Thus, persons in stressful caregiving situations who also have more indicators of metabolic syndrome may experience more decline than other caregivers or noncaregivers. METHODS The sample included 921 women (338 caregivers and 583 noncaregivers) from the Caregiver-Study of Osteoporotic Fractures study. Participants had home-based baseline and 1-year follow-up interviews between 1999 and 2003. At baseline, caregivers were categorized as long term (³4 years) versus short term (<4 years), and caring for someone with Alzheimer's disease/dementia or not. A metabolic risk composite score was the sum of four indicators: body mass index ³30, and diagnosis or using medications for hypertension, diabetes, or high cholesterol. Walking speed (m/second) was measured at both interviews. RESULTS Walking speed declined for the total sample (adjusted mean = −0.005 m/second, ±0.16) over an average of 1.04 years (±0.16). Overall, caregiving was not associated with decline. Increasing metabolic risk score was associated with greater decline for the total sample and long-term and dementia caregivers, but not other caregivers or noncaregivers. Metabolic risk score modified the adjusted associations between years of caregiving and dementia caregiving with walking speed decline (p values for interaction terms were 0.039 and 0.057, respectively). The biggest declines were in long-term caregivers and dementia caregivers who also had 3–4 metabolic indicators (−0.10 m/second and −0.155 m/second, respectively). CONCLUSIONS Walking speed declined the most among older women who had both stressful caregiving situations and more metabolic syndrome indicators, suggesting these caregiver subgroups may have increased risk of health decline."
M SELIM UNLU,Surface chemistry and morphology in single particle optical imaging,"Biological nanoparticles such as viruses and exosomes are important biomarkers for a range of medical conditions, from infectious diseases to cancer. Biological sensors that detect whole viruses and exosomes with high specificity, yet without additional labeling, are promising because they reduce the complexity of sample preparation and may improve measurement quality by retaining information about nanoscale physical structure of the bio-nanoparticle (BNP). Towards this end, a variety of BNP biosensor technologies have been developed, several of which are capable of enumerating the precise number of detected viruses or exosomes and analyzing physical properties of each individual particle. Optical imaging techniques are promising candidates among broad range of label-free nanoparticle detectors. These imaging BNP sensors detect the binding of single nanoparticles on a flat surface functionalized with a specific capture molecule or an array of multiplexed capture probes. The functionalization step confers all molecular specificity for the sensor’s target but can introduce an unforeseen problem; a rough and inhomogeneous surface coating can be a source of noise, as these sensors detect small local changes in optical refractive index. In this paper, we review several optical technologies for label-free BNP detectors with a focus on imaging systems. We compare the surface-imaging methods including dark-field, surface plasmon resonance imaging and interference reflectance imaging. We discuss the importance of ensuring consistently uniform and smooth surface coatings of capture molecules for these types of biosensors and finally summarize several methods that have been developed towards addressing this challenge."
M SELIM UNLU,Active C4 electrodes for local field potential recording applications,"Extracellular neural recording, with multi-electrode arrays (MEAs), is a powerful method used to study neural function at the network level. However, in a high density array, it can be costly and time consuming to integrate the active circuit with the expensive electrodes. In this paper, we present a 4 mm × 4 mm neural recording integrated circuit (IC) chip, utilizing IBM C4 bumps as recording electrodes, which enable a seamless active chip and electrode integration. The IC chip was designed and fabricated in a 0.13 μm BiCMOS process for both in vitro and in vivo applications. It has an input-referred noise of 4.6 μV rms for the bandwidth of 10 Hz to 10 kHz and a power dissipation of 11.25 mW at 2.5 V, or 43.9 μW per input channel. This prototype is scalable for implementing larger number and higher density electrode arrays. To validate the functionality of the chip, electrical testing results and acute in vivo recordings from a rat barrel cortex are presented."
M SELIM UNLU,Tunable resonant Raman scattering from singly resonant single wall carbon nanotubes,"We perform tunable resonant Raman scattering on 17 semiconducting and seven metallic singly resonant single wall carbon nanotubes. The measured scattering cross section as a function of laser energy provides information about a tube's electronic structure, the lifetime of intermediate states involved in the scattering process, and also the energies of zone center optical phonons. Recording the scattered Raman signal as a function of tube location in the microscope focal plane allows us to construct two-dimensional spatial maps of singly resonant tubes. We also describe a spectral nano-scale artifact, which we have termed as the ""nano-slit effect."""
M SELIM UNLU,Phase-sensitive detection of dipole radiation in a fiber-based high numerical aperture optical system,"We theoretically study the problem of detecting dipole radiation in a fiber-based confocal microscope of high numerical aperture. By using a single-mode fiber, in contrast to a hard-stop pinhole aperture, the detector becomes sensitive to the phase of the field amplitude. We find that the maximum in collection efficiency of the dipole radiation does not coincide with the optimum resolution for the light-gathering instrument. The derived expressions are important for analyzing fiber-based confocal microscope performance in fluorescence and spectroscopic studies of single molecules and/or quantum dots."
M SELIM UNLU,Exciton-mediated one-phonon resonant Raman scattering from one-dimensional systems,We use the Kramers-Heisenberg approach to derive a general expression for the resonant Raman scattering cross section from a one-dimensional (1D) system explicitly accounting for excitonic effects. The result should prove useful for analyzing the Raman resonance excitation profile line shapes for a variety of 1D systems including carbon nanotubes and semiconductor quantum wires. We apply this formalism to a simple 1D model system to illustrate the similarities and differences between the free electron and correlated electron-hole theories.
M SELIM UNLU,"Screening of excitons in single, suspended carbon nanotubes",Resonant Raman spectroscopy of single carbon nanotubes suspended across trenches displays red-shifts of up to 30 meV of the electronic transition energies as a function of the surrounding dielectric environment. We develop a simple scaling relationship between the exciton binding energy and the external dielectric function and thus quantify the effect of screening. Our results imply that the underlying particle interaction energies change by hundreds of meV.
M SELIM UNLU,Bond-selective full-field optical coherence tomography,
M SELIM UNLU,A spatially uniform illumination source for widefield multi-spectral optical microscopy,
M SELIM UNLU,Bond-selective interferometric scattering microscopy,"Interferometric scattering microscopy has been a very promising technology for highly sensitive label-free imaging of a broad spectrum of biological nanoparticles from proteins to viruses in a high-throughput manner. Although it can reveal the specimen's size and shape information, the chemical composition is inaccessible in interferometric measurements. Infrared spectroscopic imaging provides chemical specificity based on inherent chemical bond vibrations of specimens but lacks the ability to image and resolve individual nanoparticles due to long infrared wavelengths. Here, we describe a bond-selective interferometric scattering microscope where the mid-infrared induced photothermal signal is detected by a visible beam in a wide-field common-path interferometry configuration. A thin film layered substrate is utilized to reduce the reflected light and provide a reference field for the interferometric detection of the weakly scattered field. A pulsed mid-IR laser is employed to modulate the interferometric signal. Subsequent demodulation via a virtual lock-in camera offers simultaneous chemical information about tens of micro- or nano-particles. The chemical contrast arises from a minute change in the particle's scattered field in consequence of the vibrational absorption at the target molecule. We characterize the system with sub-wavelength polymer beads and highlight biological applications by chemically imaging several microorganisms including Staphylococcus aureus, Escherichia coli, and Candida albicans. A theoretical framework is established to extend bond-selective interferometric scattering microscopy to a broad range of biological micro- and nano-particles."
M SELIM UNLU,Intrinsic optical transition energies in carbon nanotubes,Intrinsic optical transition energies for isolated and individual single wall carbon nanotubes grown over trenches are measured using tunable resonant Raman scattering. Previously measured E22_S optical transitions from nanotubes in surfactants are blue shifted 70-90 meV with respect to our measurements of nanotubes in air. This large shift in the exciton energy is attributed to a larger change of the exciton binding energy than the band-gap renormalization as the surrounding dielectric constant increases.
M SELIM UNLU,The role of surface chemistry in the efficacy of protein and DNA microarrays for label-free detection: an overview,"The importance of microarrays in diagnostics and medicine has drastically increased in the last few years. Nevertheless, the efficiency of a microarray-based assay intrinsically depends on the density and functionality of the biorecognition elements immobilized onto each sensor spot. Recently, researchers have put effort into developing new functionalization strategies and technologies which provide efficient immobilization and stability of any sort of molecule. Here, we present an overview of the most widely used methods of surface functionalization of microarray substrates, as well as the most recent advances in the field, and compare their performance in terms of optimal immobilization of the bioreceptor molecules. We focus on label-free microarrays and, in particular, we aim to describe the impact of surface chemistry on two types of microarray-based sensors: microarrays for single particle imaging and for label-free measurements of binding kinetics. Both protein and DNA microarrays are taken into consideration, and the effect of different polymeric coatings on the molecules' functionalities is critically analyzed."
M SELIM UNLU,A high-throughput single-particle imaging platform for antibody characterization and a novel competition assay for therapeutic antibodies,"Monoclonal antibodies (mAbs) play an important role in diagnostics and therapy of infectious diseases. Here we utilize a single-particle interferometric reflectance imaging sensor (SP-IRIS) for screening 30 mAbs against Ebola, Sudan, and Lassa viruses (EBOV, SUDV, and LASV) to find out the ideal capture antibodies for whole virus detection using recombinant vesicular stomatitis virus (rVSV) models expressing surface glycoproteins (GPs) of EBOV, SUDV, and LASV. We also make use of the binding properties on SP-IRIS to develop a model for mapping the antibody epitopes on the GP structure. mAbs that bind to mucin-like domain or glycan cap of the EBOV surface GP show the highest signal on SP-IRIS, followed by mAbs that target the GP1-GP2 interface at the base domain. These antibodies were shown to be highly efficacious against EBOV infection in non-human primates in previous studies. For LASV detection, 8.9F antibody showed the best performance on SP-IRIS. This antibody binds to a unique region on the surface GP compared to other 15 mAbs tested. In addition, we demonstrate a novel antibody competition assay using SP-IRIS and rVSV-EBOV models to reveal the competition between mAbs in three successful therapeutic mAb cocktails against EBOV infection. We provide an explanation as to why ZMapp cocktail has higher efficacy compared to the other two cocktails by showing that three mAbs in this cocktail (13C6, 2G4, 4G7) do not compete with each other for binding to EBOV GP. In fact, the binding of 13C6 enhances the binding of 2G4 and 4G7 antibodies. Our results establish SP-IRIS as a versatile tool that can provide high-throughput screening of mAbs, multiplexed and sensitive detection of viruses, and evaluation of therapeutic antibody cocktails."
M SELIM UNLU,Highly multiplexed label-free imaging sensor for accurate quantification of small-molecule binding kinetics,"Investigating the binding interaction of small molecules to large ligands is a compelling task for the field of drug development, as well as agro-biotechnology, since a common trait of drugs and toxins is often a low molecular weight (MW). Here, we improve the limit of detection of the Interferometric Reflectance Imaging Sensor (IRIS), a label-free, highly multiplexed biosensor, to perform small-molecule screening. In this work, characterization of small molecules binding to immobilized probes in a microarray format is demonstrated, with a limit of detection of 1 pg/mm2 in mass density. First, as a proof of concept to show the impact of spatial and temporal averaging on the system noise, detection of biotin (MW = 244.3 Da) binding to a streptavidin-functionalized chip is performed and the parameters are tuned to achieve maximum signal-to-noise ratio (SNR ≈ 34). The optimized system is then applied to the screening of a 20-multiplexed antibody chip against fumonisin B1 (MW = 721.8 Da), a mycotoxin found in cereal grains. The simultaneously recorded binding curves yield an SNR ≈ 8. Five out of twenty antibodies are also screened against the toxin in a lateral flow assay, obtaining consistent results. With the demonstrated noise characteristics, further sensitivity improvements are expected with the advancement of camera sensor technology."
M SELIM UNLU,Digital microarrays: single-molecule readout with interferometric detection of plasmonic nanorod labels,"DNA and protein microarrays are a high-throughput technology that allow the simultaneous quantification of tens of thousands of different biomolecular species. The mediocre sensitivity and limited dynamic range of traditional fluorescence microarrays compared to other detection techniques have been the technology’s Achilles’ heel and prevented their adoption for many biomedical and clinical diagnostic applications. Previous work to enhance the sensitivity of microarray readout to the single-molecule (“digital”) regime have either required signal amplifying chemistry or sacrificed throughput, nixing the platform’s primary advantages. Here, we report the development of a digital microarray which extends both the sensitivity and dynamic range of microarrays by about 3 orders of magnitude. This technique uses functionalized gold nanorods as single-molecule labels and an interferometric scanner which can rapidly enumerate individual nanorods by imaging them with a 10× objective lens. This approach does not require any chemical signal enhancement such as silver deposition and scans arrays with a throughput similar to commercial fluorescence scanners. By combining single-nanoparticle enumeration and ensemble measurements of spots when the particles are very dense, this system achieves a dynamic range of about 6 orders of magnitude directly from a single scan. As a proof-of-concept digital protein microarray assay, we demonstrated detection of hepatitis B virus surface antigen in buffer with a limit of detection of 3.2 pg/mL. More broadly, the technique’s simplicity and high-throughput nature make digital microarrays a flexible platform technology with a wide range of potential applications in biomedical research and clinical diagnostics."
M SELIM UNLU,Quantitative interferometric reflectance imaging for the detection and measurement of biological nanoparticles,"The sensitive detection and quantitative measurement of biological nanoparticles such as viruses or exosomes is of growing importance in biology and medicine since these structures are implicated in many biological processes and diseases. Interferometric reflectance imaging is a label-free optical biosensing method which can directly detect individual biological nanoparticles when they are immobilized onto a protein microarray. Previous efforts to infer bio-nanoparticle size and shape have relied on empirical calibration using a ‘ruler’ of particle samples of known size, which was inconsistent and qualitative. Here, we present a mechanistic physical explanation and experimental approach by which interferometric reflectance imaging may be used to not only detect but also quantitatively measure bio-nanoparticle size and shape. We introduce a comprehensive optical model that can quantitatively simulate the scattering of arbitrarily-shaped nanoparticles such as rod-shaped or filamentous virions. Finally, we optimize the optical design for the detection and quantitative measurement of small and low-index bio-nanoparticles immersed in water."
M SELIM UNLU,Pupil function engineering for enhanced nanoparticle visibility in wide-field interferometric microscopy,"Wide-field interferometric microscopy techniques have demonstrated their utility in sensing minute changes in the optical path length as well as visualization of sub-diffraction-limited nanoparticles. In this work, we demonstrate enhanced signal levels for nanoparticle detection by pupil function engineering in wide-field common-path interferometric microscopy. We quantify the improvements in nanoparticle signal achieved by novel optical filtering schemes, benchmark them against theory, and provide physical explanations for the signal enhancements. Our refined common-path interferometric microscopy technique provides an overall ten-fold enhancement in the visibility of low-index, non-resonant polystyrene nanospheres (𝑟∼25  nm), resulting in nearly 8% signal-to-background ratio. Our method can be a highly sensitive, low-cost, label-free, high-throughput platform for accurate detection and characterization of weakly scattering low-index nanoparticles with sizes ranging from several hundred down to a few tens of nanometers, covering nearly the entire size spectrum of biological particles."
M SELIM UNLU,Simultaneous evaluation of multiple microarray surface chemistries through real-time interferometric imaging.,"Surface chemistry is a crucial aspect for microarray modality biosensor development. The immobilization capability of the functionalized surface is indeed a limiting factor for the final yield of the binding reaction. In this work, we were able to simultaneously compare the functionality of protein ligands that were locally immobilized on different polymers, while on the same solid support, therefore demonstrating a new way of multiplexing. Our goal was to investigate, in a single experiment, both the immobilization efficiency of a group of reactive polymers and the resulting affinity of the tethered molecules. This idea was demonstrated by spotting many reactive polymers on a Si/SiO2 chip and depositing the molecular probes on the spots immediately after. As a proof of concept, we focused on which polymers would better immobilize a model protein (α-Lactalbumin) and a peptide (LAC-1). We successfully showed that this protocol is applicable to proteins and peptides with a good efficiency. By means of real-time binding measurements performed with the interferometric reflectance imaging sensor (IRIS), local functionalization proved to be comparable to the classical flat coating solution. The final outcome highlights the multiplexing power of this method: first, it allows to characterize dozens of polymers at once. Secondly, it removes the limitation, related to coated surfaces, that only molecules with the same functional groups can be tethered to the same solid support. By applying this protocol, many types of molecules can be studied simultaneously and immobilization for each probe can be individually optimized."
M SELIM UNLU,"Developments in transduction, connectivity and AI/Machine learning for Point-of-Care Testing","We review some emerging trends in transduction, connectivity and data analytics for Point-of-Care Testing (POCT) of infectious and non-communicable diseases. The patient need for POCT is described along with developments in portable diagnostics, specifically in respect of Lab-on-chip and microfluidic systems. We describe some novel electrochemical and photonic systems and the use of mobile phones in terms of hardware components and device connectivity for POCT. Developments in data analytics that are applicable for POCT are described with an overview of data structures and recent AI/Machine learning trends. The most important methodologies of machine learning, including deep learning methods, are summarised. The potential value of trends within POCT systems for clinical diagnostics within Lower Middle Income Countries (LMICs) and the Least Developed Countries (LDCs) are highlighted."
M SELIM UNLU,"High-throughput, high-resolution interferometric light microscopy of biological nanoparticles","Label-free, visible light microscopy is an indispensable tool for studying biological nanoparticles (BNPs). However, conventional imaging techniques have two major challenges: (i) weak contrast due to low-refractive-index difference with the surrounding medium and exceptionally small size and (ii) limited spatial resolution. Advances in interferometric microscopy have overcome the weak contrast limitation and enabled direct detection of BNPs, yet lateral resolution remains as a challenge in studying BNP morphology. Here, we introduce a wide-field interferometric microscopy technique augmented by computational imaging to demonstrate a 2-fold lateral resolution improvement over a large field-of-view (>100 × 100 μm2), enabling simultaneous imaging of more than 104 BNPs at a resolution of ∼150 nm without any labels or sample preparation. We present a rigorous vectorial-optics-based forward model establishing the relationship between the intensity images captured under partially coherent asymmetric illumination and the complex permittivity distribution of nanoparticles. We demonstrate high-throughput morphological visualization of a diverse population of Ebola virus-like particles and a structurally distinct Ebola vaccine candidate. Our approach offers a low-cost and robust label-free imaging platform for high-throughput and high-resolution characterization of a broad size range of BNPs."
M SELIM UNLU,Scaling of exciton binding energy with external dielectric function in carbon nanotubes,"We develop a scaling relationship between the exciton binding energy and the external dielectric function in carbon nanotubes. We show that the electron–electron and electron–hole interaction energies are strongly affected by screening yet largely counteract each other, resulting in much smaller changes in the optical transition energy. The model indicates that the relevant particle interaction energies are reduced by as much as 50% upon screening by water and that the unscreened electron–electron interaction energy is larger than the unscreened electron–hole interaction energy, in agreement with explanations of the “ratio problem.” We apply the model to measurements of the changes in the optical transition energies in single, suspended carbon nanotubes as the external dielectric environment is altered."
M SELIM UNLU,Rapid mapping of digital integrated circuit logic gates via multi-spectral backside imaging,"Modern semiconductor integrated circuits are increasingly fabricated at untrusted third party foundries. There now exist myriad security threats of malicious tampering at the hardware level and hence a clear and pressing need for new tools that enable rapid, robust and low-cost validation of circuit layouts. Optical backside imaging offers an attractive platform, but its limited resolution and throughput cannot cope with the nanoscale sizes of modern circuitry and the need to image over a large area. We propose and demonstrate a multi-spectral imaging approach to overcome these obstacles by identifying key circuit elements on the basis of their spectral response. This obviates the need to directly image the nanoscale components that define them, thereby relaxing resolution and spatial sampling requirements by 1 and 2 - 4 orders of magnitude respectively. Our results directly address critical security needs in the integrated circuit supply chain and highlight the potential of spectroscopic techniques to address fundamental resolution obstacles caused by the need to image ever shrinking feature sizes in semiconductor integrated circuits."
M SELIM UNLU,Enhanced light microscopy visualization of virus particles from Zika virus to filamentous ebolaviruses,"Light microscopy is a powerful tool in the detection and analysis of parasites, fungi, and prokaryotes, but has been challenging to use for the detection of individual virus particles. Unlabeled virus particles are too small to be visualized using standard visible light microscopy. Characterization of virus particles is typically performed using higher resolution approaches such as electron microscopy or atomic force microscopy. These approaches require purification of virions away from their normal millieu, requiring significant levels of expertise, and can only enumerate small numbers of particles per field of view. Here, we utilize a visible light imaging approach called Single Particle Interferometric Reflectance Imaging Sensor (SP-IRIS) that allows automated counting and sizing of thousands of individual virions. Virions are captured directly from complex solutions onto a silicon chip and then detected using a reflectance interference imaging modality. We show that the use of different imaging wavelengths allows the visualization of a multitude of virus particles. Using Violet/UV illumination, the SP-IRIS technique is able to detect individual flavivirus particles (~40 nm), while green light illumination is capable of identifying and discriminating between vesicular stomatitis virus and vaccinia virus (~360 nm). Strikingly, the technology allows the clear identification of filamentous infectious ebolavirus particles and virus-like particles. The ability to differentiate and quantify unlabeled virus particles extends the usefulness of traditional light microscopy and can be embodied in a straightforward benchtop approach allowing widespread applications ranging from rapid detection in biological fluids to analysis of virus-like particles for vaccine development and production."
M SELIM UNLU,Digital detection of exosomes by interferometric imaging,"Exosomes, which are membranous nanovesicles, are actively released by cells and have been attributed to roles in cell-cell communication, cancer metastasis, and early disease diagnostics. The small size (30–100 nm) along with low refractive index contrast of exosomes makes direct characterization and phenotypical classification very difficult. In this work we present a method based on Single Particle Interferometric Reflectance Imaging Sensor (SP-IRIS) that allows multiplexed phenotyping and digital counting of various populations of individual exosomes (>50 nm) captured on a microarray-based solid phase chip. We demonstrate these characterization concepts using purified exosomes from a HEK 293 cell culture. As a demonstration of clinical utility, we characterize exosomes directly from human cerebrospinal fluid (hCSF). Our interferometric imaging method could capture, from a very small hCSF volume (20 uL), nanoparticles that have a size compatible with exosomes, using antibodies directed against tetraspanins. With this unprecedented capability, we foresee revolutionary implications in the clinical field with improvements in diagnosis and stratification of patients affected by different disorders."
M SELIM UNLU,Beating the reaction limits of biosensor sensitivity with dynamic tracking of single binding events,"The clinical need for ultrasensitive molecular analysis has motivated the development of several endpoint-assay technologies capable of single-molecule readout. These endpoint assays are now primarily limited by the affinity and specificity of the molecular-recognition agents for the analyte of interest. In contrast, a kinetic assay with single-molecule readout could distinguish between low-abundance, high-affinity (specific analyte) and high-abundance, low-affinity (nonspecific background) binding by measuring the duration of individual binding events at equilibrium. Here, we describe such a kinetic assay, in which individual binding events are detected and monitored during sample incubation. This method uses plasmonic gold nanorods and interferometric reflectance imaging to detect thousands of individual binding events across a multiplex solid-phase sensor with a large area approaching that of leading bead-based endpoint-assay technologies. A dynamic tracking procedure is used to measure the duration of each event. From this, the total rates of binding and debinding as well as the distribution of binding-event durations are determined. We observe a limit of detection of 19 fM for a proof-of-concept synthetic DNA analyte in a 12-plex assay format."
M SELIM UNLU,"Digital detection of biomarkers for low-cost, high-sensitivity diagnostics","We have demonstrated Interferometric Reflectance Imaging Sensor (IRIS) with the ability to detect single nanoscale particles. By extending single-particle IRIS to in-liquid dynamic imaging, we demonstrated real-time digital detection of individual viral pathogens as well as single molecules labeled with Au nanoparticles. With this technique we demonstrate real-time simultaneous detection of multiple targets in a single sample, as well as quantitative dynamic detection of individual biomolecular interactions for reaction kinetics measurements. This approach promises to simplify and reduce the cost of rapid diagnostics."
M SELIM UNLU,Bulk-effect-free method for binding kinetic measurements enabling small-molecule affinity characterization,"Optical technologies for label-free detection are an attractive solution for monitoring molecular binding kinetics; however, these techniques measure the changes in the refractive index, making it difficult to distinguish surface binding from a change in the refractive index of the analyte solution in the proximity of the sensor surface. The solution refractive index changes, due to solvents, temperature changes, or pH variations, can create an unwanted background signal known as the bulk effect. Technologies such as biolayer interferometry and surface plasmon resonance offer no bulk-effect compensation, or they alternatively offer a reference channel to correct in postprocessing. Here, we present a virtually bulk-effect-free method, without a reference channel or any computational correction, for measuring kinetic binding using the interferometric reflectance imaging sensor (IRIS), an optical label-free biomolecular interaction analysis tool. Dynamic spectral illumination engineering, through tailored LED contributions, is combined with the IRIS technology to minimize the bulk effect, with the potential to enable kinetic measurements of a broader range of analytes. We demonstrate that the deviation in the reflectivity signal is reduced to ∼8 × 10-6 for a solution change from phosphate-buffered saline (PBS) (n = 1.335) to 1% dimethyl sulfoxide (DMSO) in PBS (n = 1.336). As a proof of concept, we applied the method to a biotin-streptavidin interaction, where biotin (MW = 244.3 Da) was dissolved at a final concentration of 1 μM in a 1% solution of DMSO in PBS and flowed over immobilized streptavidin. Clear binding results were obtained without a reference channel or any computational correction."
M SELIM UNLU,Computational nanosensing from defocus in single particle interferometric reflectance microscopy,"Single particle interferometric reflectance (SPIR) microscopy has been studied as a powerful imaging platform for label-free and highly sensitive biological nanoparticle detection and characterization. SPIR's interferometric nature yields a unique 3D defocus intensity profile of the nanoparticles over a large field of view. Here, we utilize this defocus information to recover high signal-to-noise ratio nanoparticle images with a computationally and memory efficient reconstruction framework. Our direct inversion approach recovers this image from a 3D defocus intensity stack using the vectorial-optics-based forward model developed for sub-diffraction-limited dielectric nanoparticles captured on a layered substrate. We demonstrate proof-of-concept experiments on silica beads with a 50 nm nominal diameter."
M SELIM UNLU,Background-suppressed high-throughput mid-infrared photothermal microscopy via pupil engineering,
M SELIM UNLU,"Multiplexed, high-sensitivity measurements of antibody affinity using interferometric reflectance imaging sensor","Anthrax lethal factor (LF) is one of the enzymatic components of the anthrax toxin responsible for the pathogenic responses of the anthrax disease. The ability to screen multiplexed ligands against LF and subsequently estimate the effective kinetic rates (kon and koff) and complementary binding behavior provides critical information useful in diagnostic and therapeutic development for anthrax. Tools such as biolayer interferometry (BLI) and surface plasmon resonance imaging (SPRi) have been developed for this purpose; however, these tools suffer from limitations such as signal jumps when the solution in the chamber is switched or low sensitivity. Here, we present multiplexed antibody affinity measurements obtained by the interferometric reflectance imaging sensor (IRIS), a highly sensitive, label-free optical biosensor, whose stability, simplicity, and imaging modality overcomes many of the limitations of other multiplexed methods. We compare the multiplexed binding results obtained with the IRIS system using two ligands targeting the anthrax lethal factor (LF) against previously published results obtained with more traditional surface plasmon resonance (SPR), which showed consistent results, as well as kinetic information previously unattainable with SPR. Additional exemplary data demonstrating multiplexed binding and the corresponding complementary binding to sequentially injected ligands provides an additional layer of information immediately useful to the researcher."
M SELIM UNLU,Nanoparticle classification in wide-field interferometric microscopy by supervised learning from model,"Interference-enhanced wide-field nanoparticle imaging is a highly sensitive technique that has found numerous applications in labeled and label-free subdiffraction-limited pathogen detection. It also provides unique opportunities for nanoparticle classification upon detection. More specifically, the nanoparticle defocus images result in a particle-specific response that can be of great utility for nanoparticle classification, particularly based on type and size. In this work, we combine a model-based supervised learning algorithm with a wide-field common-path interferometric microscopy method to achieve accurate nanoparticle classification. We verify our classification schemes experimentally by blindly detecting gold and polystyrene nanospheres, and then classifying them in terms of type and size."
M SELIM UNLU,Robust visualization and discrimination of nanoparticles by interferometric imaging,"Single-molecule and single-nanoparticle biosensors are a growing frontier in diagnostics. Digital biosensors are those which enumerate all specifically immobilized biomolecules or biological nanoparticles, and thereby achieve limits of detection usually beyond the reach of ensemble measurements. Here we review modern optical techniques for single nanoparticle detection and describe the single-particle interferometric reflectance imaging sensor (SP-IRIS). We present challenges associated with reliably detecting faint nanoparticles with SP-IRIS, and describe image acquisition processes and software modifications to address them. Specifically, we describe a image acquisition processing method for the discrimination and accurate counting of nanoparticles that greatly reduces both the number of false positives and false negatives. These engineering improvements are critical steps in the translation of SP-IRIS towards applications in medical diagnostics."
M SELIM UNLU,The effects of three-dimensional ligand immobilization on kinetic measurements in biosensors,"The field of biosensing is in constant evolution, propelled by the need for sensitive, reliable platforms that provide consistent results, especially in the drug development industry, where small molecule characterization is of uttermost relevance. Kinetic characterization of small biochemicals is particularly challenging, and has required sensor developers to find solutions to compensate for the lack of sensitivity of their instruments. In this regard, surface chemistry plays a crucial role. The ligands need to be efficiently immobilized on the sensor surface, and probe distribution, maintenance of their native structure and efficient diffusion of the analyte to the surface need to be optimized. In order to enhance the signal generated by low molecular weight targets, surface plasmon resonance sensors utilize a high density of probes on the surface by employing a thick dextran matrix, resulting in a three-dimensional, multilayer distribution of molecules. Despite increasing the binding signal, this method can generate artifacts, due to the diffusion dependence of surface binding, affecting the accuracy of measured affinity constants. On the other hand, when working with planar surface chemistries, an incredibly high sensitivity is required for low molecular weight analytes, and furthermore the standard method for immobilizing single layers of molecules based on self-assembled monolayers (SAM) of epoxysilane has been demonstrated to promote protein denaturation, thus being far from ideal. Here, we will give a concise overview of the impact of tridimensional immobilization of ligands on label-free biosensors, mostly focusing on the effect of diffusion on binding affinity constants measurements. We will comment on how multilayering of probes is certainly useful in terms of increasing the sensitivity of the sensor, but can cause steric hindrance, mass transport and other diffusion effects. On the other hand, probe monolayers on epoxysilane chemistries do not undergo diffusion effect but rather other artifacts can occur due to probe distortion. Finally, a combination of tridimensional polymeric chemistry and probe monolayer is presented and reviewed, showing advantages and disadvantages over the other two approaches."
M SELIM UNLU,Strong extinction of a far-field laser beam by a single quantum dot,"Through the utilization of index-matched GaAs immersion lens techniques, we demonstrate a record extinction (12%) of a far-field focused laser beam by a single InAs/GaAs quantum dot. This contrast level enables us to report for the first time resonant laser transmission spectroscopy on a single InAs/GaAs quantum dot without the need for phase-sensitive lock-in detection."
M SELIM UNLU,"Highly-sensitive, label-free detection of microorganisms and viruses via interferometric reflectance imaging sensor","Pathogenic microorganisms and viruses can easily transfer from one host to another and cause disease in humans. The determination of these pathogens in a time- and cost-effective way is an extreme challenge for researchers. Rapid and label-free detection of pathogenic microorganisms and viruses is critical in ensuring rapid and appropriate treatment. Sensor technologies have shown considerable advancements in viral diagnostics, demonstrating their great potential for being fast and sensitive detection platforms. In this review, we present a summary of the use of an interferometric reflectance imaging sensor (IRIS) for the detection of microorganisms. We highlight low magnification modality of IRIS as an ensemble biomolecular mass measurement technique and high magnification modality for the digital detection of individual nanoparticles and viruses. We discuss the two different modalities of IRIS and their applications in the sensitive detection of microorganisms and viruses."
M SELIM UNLU,Interferometric Reflectance Imaging Sensor (IRIS)-a platform technology for multiplexed diagnostics and digital detection,"Over the last decade, the growing need in disease diagnostics has stimulated rapid development of new technologies with unprecedented capabilities. Recent emerging infectious diseases and epidemics have revealed the shortcomings of existing diagnostics tools, and the necessity for further improvements. Optical biosensors can lay the foundations for future generation diagnostics by providing means to detect biomarkers in a highly sensitive, specific, quantitative and multiplexed fashion. Here, we review an optical sensing technology, Interferometric Reflectance Imaging Sensor (IRIS), and the relevant features of this multifunctional platform for quantitative, label-free and dynamic detection. We discuss two distinct modalities for IRIS: (i) low-magnification (ensemble biomolecular mass measurements) and (ii) high-magnification (digital detection of individual nanoparticles) along with their applications, including label-free detection of multiplexed protein chips, measurement of single nucleotide polymorphism, quantification of transcription factor DNA binding, and high sensitivity digital sensing and characterization of nanoparticles and viruses."
M SELIM UNLU,Highly sensitive and label-free digital detection of whole cell E. coli with interferometric reflectance imaging,"Bacterial infectious diseases are a major threat to human health. Timely and sensitive pathogenic bacteria detection is crucial in identifying the bacterial contaminations and preventing the spread of infectious diseases. Due to limitations of conventional bacteria detection techniques there have been concerted research efforts towards development of new biosensors. Biosensors offering label free, whole bacteria detection are highly desirable over those relying on label based or pathogenic molecular components detection. The major advantage is eliminating the additional time and cost required for labeling or extracting the desired bacterial components. Here, we demonstrate rapid, sensitive and label free E. coli detection utilizing interferometric reflectance imaging enhancement allowing for visualizing individual pathogens captured on the surface. Enabled by our ability to count individual bacteria on a large sensor surface, we demonstrate a limit of detection of 2.2 CFU/ml from a buffer solution with no sample preparation. To the best of our knowledge, this high level of sensitivity for whole E. coli detection is unprecedented in label free biosensing. The specificity of our biosensor is validated by comparing the response to target bacteria E. coli and non target bacteria S. aureus, K. pneumonia and P. aeruginosa. The biosensor performance in tap water also proves that its detection capability is unaffected by the sample complexity. Furthermore, our sensor platform provides high optical magnification imaging and thus validation of recorded detection events as the target bacteria based on morphological characterization. Therefore, our sensitive and label free detection method offers new perspectives for direct bacterial detection in real matrices and clinical samples."
M SELIM UNLU,Attomolar sensitivity microRNA detection using real-time digital microarrays.,"MicroRNAs (miRNAs) are a family of noncoding, functional RNAs. With recent developments in molecular biology, miRNA detection has attracted significant interest, as hundreds of miRNAs and their expression levels have shown to be linked to various diseases such as infections, cardiovascular disorders and cancers. A powerful and high throughput tool for nucleic acid detection is the DNA microarray technology. However, conventional methods do not meet the demands in sensitivity and specificity, presenting significant challenges for the adaptation of miRNA detection for diagnostic applications. In this study, we developed a highly sensitive and multiplexed digital microarray using plasmonic gold nanorods as labels. For proof of concept studies, we conducted experiments with two miRNAs, miRNA-451a (miR-451) and miRNA-223-3p (miR-223). We demonstrated improvements in sensitivity in comparison to traditional end-point assays that employ capture on solid phase support, by implementing real-time tracking of the target molecules on the sensor surface. Particle tracking overcomes the sensitivity limitations for detection of low-abundance biomarkers in the presence of low-affinity but high-abundance background molecules, where endpoint assays fall short. The absolute lowest measured concentration was 100 aM. The measured detection limit being well above the blank samples, we performed theoretical calculations for an extrapolated limit of detection (LOD). The dynamic tracking improved the extrapolated LODs from femtomolar range to [Formula: see text] 10 attomolar (less than 1300 copies in 0.2 ml of sample) for both miRNAs and the total incubation time was decreased from 5 h to 35 min."
M SELIM UNLU,High-resolution Imaging of nanoparticles in wide-field interferometric scattering microscopy,"Single particle interferometric scattering microscopy has demonstrated great capability in label-free imaging of sub-wavelength dielectric nanoparticles (r<25 nm); however, it suffers from diffraction-limited resolution. Here, we demonstrate ~2-fold improvement in lateral resolution upon asymmetric illumination."
M SELIM UNLU,Interferometric detection and enumeration of viral particles using Si-based microfluidics,"Single-particle interferometric reflectance imaging sensor enables optical visualization and characterization of individual nanoparticles without any labels. Using this technique, we have shown end-point and real-time detection of viral particles using laminate-based active and passive cartridge configurations. Here, we present a new concept for low-cost microfluidic integration of the sensor chips into compact cartridges through utilization of readily available silicon fabrication technologies. This new cartridge configuration will allow simultaneous detection of individual virus binding events on a 9-spot microarray, and provide the needed simplicity and robustness for routine real-time operation for discrete detection of viral particles in a multiplex format."
M SELIM UNLU,Optical determination of electron-phonon coupling in carbon nanotubes,We report on an optical method to directly measure electron-phonon coupling in carbon nanotubes by correlating the first and second harmonic of the resonant Raman excitation profile. The method is applicable to 1D and 0D systems and is not limited to materials that exhibit photoluminescence. Experimental results for electron-phonon coupling with the radial breathing mode in 5 different nanotubes show coupling strengths from 3–11 meV. The results are in good agreement with the chirality and diameter dependence of the e -ph coupling calculated by Goupalov et al.
M SELIM UNLU,Multiplexed affinity measurements of extracellular vesicles binding kinetics,"Extracellular vesicles (EVs) have attracted significant attention as impactful diagnostic biomarkers, since their properties are closely related to specific clinical conditions. However, designing experiments that involve EVs phenotyping is usually highly challenging and time-consuming, due to laborious optimization steps that require very long or even overnight incubation durations. In this work, we demonstrate label-free, real-time detection, and phenotyping of extracellular vesicles binding to a multiplexed surface. With the ability for label-free kinetic binding measurements using the Interferometric Reflectance Imaging Sensor (IRIS) in a microfluidic chamber, we successfully optimize the capture reaction by tuning various assay conditions (incubation time, flow conditions, surface probe density, and specificity). A single (less than 1 h) experiment allows for characterization of binding affinities of the EVs to multiplexed probes. We demonstrate kinetic characterization of 18 different probe conditions, namely three different antibodies, each spotted at six different concentrations, simultaneously. The affinity characterization is then analyzed through a model that considers the complexity of multivalent binding of large structures to a carpet of probes and therefore introduces a combination of fast and slow association and dissociation parameters. Additionally, our results confirm higher affinity of EVs to aCD81 with respect to aCD9 and aCD63. Single-vesicle imaging measurements corroborate our findings, as well as confirming the EVs nature of the captured particles through fluorescence staining of the EVs membrane and cargo."
M SELIM UNLU,A digital microarray using interferometric detection of plasmonic nanorod labels,"DNA and protein microarrays are a high-throughput technology that allow the simultaneous quantification of tens of thousands of different biomolecular species. The mediocre sensitivity and dynamic range of traditional fluorescence microarrays compared to other techniques have been the technology's Achilles' Heel, and prevented their adoption for many biomedical and clinical diagnostic applications. Previous work to enhance the sensitivity of microarray readout to the single-molecule ('digital') regime have either required signal amplifying chemistry or sacrificed throughput, nixing the platform's primary advantages. Here, we report the development of a digital microarray which extends both the sensitivity and dynamic range of microarrays by about three orders of magnitude. This technique uses functionalized gold nanorods as single-molecule labels and an interferometric scanner which can rapidly enumerate individual nanorods by imaging them with a 10x objective lens. This approach does not require any chemical enhancement such as silver deposition, and scans arrays with a throughput similar to commercial fluorescence devices. By combining single-nanoparticle enumeration and ensemble measurements of spots when the particles are very dense, this system achieves a dynamic range of about one million directly from a single scan."
M SELIM UNLU,Polarization enhanced interferometric imaging,
M SELIM UNLU,Solid-phase optical sensing techniques for sensitive virus detection,"Viral infections can pose a major threat to public health by causing serious illness, leading to pandemics, and burdening healthcare systems. The global spread of such infections causes disruptions to every aspect of life including business, education, and social life. Fast and accurate diagnosis of viral infections has significant implications for saving lives, preventing the spread of the diseases, and minimizing social and economic damages. Polymerase chain reaction (PCR)-based techniques are commonly used to detect viruses in the clinic. However, PCR has several drawbacks, as highlighted during the recent COVID-19 pandemic, such as long processing times and the requirement for sophisticated laboratory instruments. Therefore, there is an urgent need for fast and accurate techniques for virus detection. For this purpose, a variety of biosensor systems are being developed to provide rapid, sensitive, and high-throughput viral diagnostic platforms, enabling quick diagnosis and efficient control of the virus's spread. Optical devices, in particular, are of great interest due to their advantages such as high sensitivity and direct readout. The current review discusses solid-phase optical sensing techniques for virus detection, including fluorescence-based sensors, surface plasmon resonance (SPR), surface-enhanced Raman scattering (SERS), optical resonators, and interferometry-based platforms. Then, we focus on an interferometric biosensor developed by our group, the single-particle interferometric reflectance imaging sensor (SP-IRIS), which has the capability to visualize single nanoparticles, to demonstrate its application for digital virus detection."
M SELIM UNLU,Single virus fingerprinting by widefield interferometric defocus-enhanced mid-infrared photothermal microscopy,"Clinical identification and fundamental study of viruses rely on the detection of viral proteins or viral nucleic acids. Yet, amplification-based and antigen-based methods are not able to provide precise compositional information of individual virions due to small particle size and low-abundance chemical contents (e.g., ~ 5000 proteins in a vesicular stomatitis virus). Here, we report a widefield interferometric defocus-enhanced mid-infrared photothermal (WIDE-MIP) microscope for high-throughput fingerprinting of single viruses. With the identification of feature absorption peaks, WIDE-MIP reveals the contents of viral proteins and nucleic acids in single DNA vaccinia viruses and RNA vesicular stomatitis viruses. Different nucleic acid signatures of thymine and uracil residue vibrations are obtained to differentiate DNA and RNA viruses. WIDE-MIP imaging further reveals an enriched β sheet components in DNA varicella-zoster virus proteins. Together, these advances open a new avenue for compositional analysis of viral vectors and elucidating protein function in an assembled virion."
MARTIN CHARNS,"Improving Quality of Care through Routine, Successful Implementation of Evidence-Based Practice at the Bedside: An Organizational Case Study Protocol Using the Pettigrew and Whipp Model of Strategic Change","BACKGROUND. Evidence-based practice (EBP) is an expected approach to improving the quality of patient care and service delivery in health care systems internationally that is yet to be realized. Given the current evidence-practice gap, numerous authors describe barriers to achieving EBP. One recurrently identified barrier is the setting or context of practice, which is likewise cited as a potential part of the solution to the gap. The purpose of this study is to identify key contextual elements and related strategic processes in organizations that find and use evidence at multiple levels, in an ongoing, integrated fashion, in contrast to those that do not. METHODS. The core theoretical framework for this multi-method explanatory case study is Pettigrew and Whipp's Content, Context, and Process model of strategic change. This framework focuses data collection on three entities: the Why of strategic change, the What of strategic change, and the How of strategic change, in this case related to implementation and normalization of EBP. The data collection plan, designed to capture relevant organizational context and related outcomes, focuses on eight interrelated factors said to characterize a receptive context. Selective, purposive sampling will provide contrasting results between two cases (departments of nursing) and three embedded units in each. Data collection methods will include quantitative tools (e.g., regarding culture) and qualitative approaches including focus groups, interviews, and documents review (e.g., regarding integration and ""success"") relevant to the EBP initiative. DISCUSSION. This study should provide information regarding contextual elements and related strategic processes key to successful implementation and sustainability of EBP, specifically in terms of a pervasive pattern in an acute care hospital-based health care setting. Additionally, this study will identify key contextual elements that differentiate successful implementation and sustainability of EBP efforts, both within varying levels of a hospital-based clinical setting and across similar hospital settings interested in EBP."
MARTIN CHARNS,Institutionalizing Evidence-Based Practice: An Organizational Case Study Using a Model of Strategic Change,"BACKGROUND. There is a general expectation within healthcare that organizations should use evidence-based practice (EBP) as an approach to improving the quality of care. However, challenges exist regarding how to make EBP a reality, particularly at an organizational level and as a routine, sustained aspect of professional practice. METHODS. A mixed method explanatory case study was conducted to study context; i.e., in terms of the presence or absence of multiple, inter-related contextual elements and associated strategic approaches required for integrated, routine use of EBP ('institutionalization'). The Pettigrew et al. Content, Context, and Process model was used as the theoretical framework. Two sites in the US were purposively sampled to provide contrasting cases: i.e., a 'role model' site, widely recognized as demonstrating capacity to successfully implement and sustain EBP to a greater degree than others; and a 'beginner' site, self-perceived as early in the journey towards institutionalization. RESULTS. The two sites were clearly different in terms of their organizational context, level of EBP activity, and degree of institutionalization. For example, the role model site had a pervasive, integrated presence of EBP versus a sporadic, isolated presence in the beginner site. Within the inner context of the role model site, there was also a combination of the Pettigrew and colleagues' receptive elements that, together, appeared to enhance its ability to effectively implement EBP-related change at multiple levels. In contrast, the beginner site, which had been involved for a few years in EBP-related efforts, had primarily non-receptive conditions in several contextual elements and a fairly low overall level of EBP receptivity. The beginner site thus appeared, at the time of data collection, to lack an integrated context to either support or facilitate the institutionalization of EBP. CONCLUSION. Our findings provide evidence of some of the key contextual elements that may require attention if institutionalization of EBP is to be realized. They also suggest the need for an integrated set of receptive contextual elements to achieve EBP institutionalization; and they further support the importance of specific interactions among these elements, including ways in which leadership affects other contextual elements positively or negatively."
CLAIRE WOLFTEICH,Focus: Summer 2015,
CLAIRE WOLFTEICH,"Review of practical sacramental theology: at the intersection of liturgy and ethics, by Bruce T. Morrill",
SOUMENDRA N BASU,Co-infiltration of nickel and mixed conducting Gd0.1Ce0.9O2−δ and La0.6Sr0.3Ni0.15Cr0.85O3−δ phases in Ni-YSZ anodes for improved stability and performance,
SOUMENDRA N BASU,Effect of anodic current density on the spreading of infiltrated nickel nanoparticles in nickel-yttria stabilized zirconia cermet anodes,
SOUMENDRA N BASU,"Chromium poisoning effects on performance of (La, Sr) MnO3-based cathode in anode-supported solid oxide fuel cells","Chromium (Cr) vapor species from chromia-forming alloy interconnects are known to cause cathode performance degradation in solid oxide fuel cells (SOFCs). To understand the impact of Cr-poisoning on cathode performance, it is important to determine its effects on different cathode polarization losses. In this study, anode-supported SOFCs, with a (La,Sr)MnO3 (LSM) + yttria-stabilized zirconia (YSZ) cathode active layer and a LSM cathode current collector layer were fabricated. At 800°C, cells were electrochemically tested in direct contact with Crofer22H meshes, under different cathode atmospheres (dry air or humidified air) and current conditions (open-circuit or galvanostatic). Significant performance degradation was observed when cell was tested under galvanostatic condition (0.5 A/cm2), which was not the case under open-circuit condition. Humidity was found to accelerate the performance degradation. By curve-fitting the experimentally measured current-voltage traces to a polarization model, the effects of Cr-poisoning on different cathodic polarization losses were estimated. It is found that, under normal operating conditions, increase of activation polarization dominates the cathode performance degradation. Microstructures of the cathodes were characterized and Cr-containing deposits were identified. Higher concentrations of Cr-containing deposits were found at the cathode/electrolyte interface and the amounts directly correlated with the cell performance degradations."
SOUMENDRA N BASU,Quantifying percolated triple phase boundary density and its effects on anodic polarization in Ni-infiltrated Ni/YSZ SOFC snodes,"Increasing the density of percolated triple phase boundaries (TPBs) by infiltrating nanoscale electrocatalysts can improve the performance of solid oxide fuel cell (SOFC) anodes. However, the complex microstructure of these infiltrated nanocatalysts creates challenges in quantifying their role in anode performance improvements. In this research, scanning electron microscopy of fractured cross-sections of a Ni-nanocatalyst infiltrated anodic symmetric cell along with three-dimensional (3-D) reconstruction of the same anode have been used to quantify the changes in percolated TPB densities due to infiltration. This change in percolated TPB density has been compared to the improvement in anode activation polarization resistance measured by electrochemical impedance spectroscopy (EIS). It was found that increased TPB densities only partially accounted for the measured performance improvement. Distribution of relaxation times (DRT) analyses showed that a reduction in the time constants of the catalytic processes in the anode also play a role, suggesting that the added nanoscale percolated TPB boundaries are more electrochemically active as compared to the cermet TPB boundaries."
MICHAEL D KLEIN,"D-cycloserine augmentation of exposure-based cognitive behavior therapy for anxiety, obsessive-compulsive, and posttraumatic stress disorders: a systematic review and meta-analysis of individual participant data","Importance: Whether and under which conditions D-cycloserine (DCS) augments the effects of exposure-based cognitive behavior therapy for anxiety, obsessive-compulsive, and posttraumatic stress disorders is unclear. Objective: To clarify whether DCS is superior to placebo in augmenting the effects of cognitive behavior therapy for anxiety, obsessive-compulsive, and posttraumatic stress disorders and to evaluate whether antidepressants interact with DCS and the effect of potential moderating variables. Data Sources: PubMed, EMBASE, and PsycINFO were searched from inception to February 10, 2016. Reference lists of previous reviews and meta-analyses and reports of randomized clinical trials were also checked. Study Selection: Studies were eligible for inclusion if they were (1) double-blind randomized clinical trials of DCS as an augmentation strategy for exposure-based cognitive behavior therapy and (2) conducted in humans diagnosed as having specific phobia, social anxiety disorder, panic disorder with or without agoraphobia, obsessive-compulsive disorder, or posttraumatic stress disorder. Data Extraction and Synthesis: Raw data were obtained from the authors and quality controlled. Data were ranked to ensure a consistent metric across studies (score range, 0-100). We used a 3-level multilevel model nesting repeated measures of outcomes within participants, who were nested within studies. Results: Individual participant data were obtained for 21 of 22 eligible trials, representing 1047 of 1073 eligible participants. When controlling for antidepressant use, participants receiving DCS showed greater improvement from pretreatment to posttreatment (mean difference, -3.62; 95% CI, -0.81 to -6.43; P = .01; d = -0.25) but not from pretreatment to midtreatment (mean difference, -1.66; 95% CI, -4.92 to 1.60; P = .32; d = -0.14) or from pretreatment to follow-up (mean difference, -2.98, 95% CI, -5.99 to 0.03; P = .05; d = -0.19). Additional analyses showed that participants assigned to DCS were associated with lower symptom severity than those assigned to placebo at posttreatment and at follow-up. Antidepressants did not moderate the effects of DCS. None of the prespecified patient-level or study-level moderators was associated with outcomes. Conclusions and Relevance: D-cycloserine is associated with a small augmentation effect on exposure-based therapy. This effect is not moderated by the concurrent use of antidepressants. Further research is needed to identify patient and/or therapy characteristics associated with DCS response."
MICHAEL D KLEIN,"First Sagittarius A* Event Horizon Telescope results. II. EHT and multiwavelength observations, data processing, and calibration","We present Event Horizon Telescope (EHT) 1.3 mm measurements of the radio source located at the position of the supermassive black hole Sagittarius A* (Sgr A*), collected during the 2017 April 5–11 campaign. The observations were carried out with eight facilities at six locations across the globe. Novel calibration methods are employed to account for Sgr A*'s flux variability. The majority of the 1.3 mm emission arises from horizon scales, where intrinsic structural source variability is detected on timescales of minutes to hours. The effects of interstellar scattering on the image and its variability are found to be subdominant to intrinsic source structure. The calibrated visibility amplitudes, particularly the locations of the visibility minima, are broadly consistent with a blurred ring with a diameter of ∼50 μas, as determined in later works in this series. Contemporaneous multiwavelength monitoring of Sgr A* was performed at 22, 43, and 86 GHz and at near-infrared and X-ray wavelengths. Several X-ray flares from Sgr A* are detected by Chandra, one at low significance jointly with Swift on 2017 April 7 and the other at higher significance jointly with NuSTAR on 2017 April 11. The brighter April 11 flare is not observed simultaneously by the EHT but is followed by a significant increase in millimeter flux variability immediately after the X-ray outburst, indicating a likely connection in the emission physics near the event horizon. We compare Sgr A*’s broadband flux during the EHT campaign to its historical spectral energy distribution and find that both the quiescent emission and flare emission are consistent with its long-term behavior."
MICHAEL D KLEIN,First Sagittarius A* Event Horizon Telescope results. I. The shadow of the supermassive black hole in the center of the Milky Way,"We present the first Event Horizon Telescope (EHT) observations of Sagittarius A* (Sgr A*), the Galactic center source associated with a supermassive black hole. These observations were conducted in 2017 using a global interferometric array of eight telescopes operating at a wavelength of λ = 1.3 mm. The EHT data resolve a compact emission region with intrahour variability. A variety of imaging and modeling analyses all support an image that is dominated by a bright, thick ring with a diameter of 51.8 ± 2.3 μas (68% credible interval). The ring has modest azimuthal brightness asymmetry and a comparatively dim interior. Using a large suite of numerical simulations, we demonstrate that the EHT images of Sgr A* are consistent with the expected appearance of a Kerr black hole with mass ∼4 × 106 M ⊙, which is inferred to exist at this location based on previous infrared observations of individual stellar orbits, as well as maser proper-motion studies. Our model comparisons disfavor scenarios where the black hole is viewed at high inclination (i &gt; 50°), as well as nonspinning black holes and those with retrograde accretion disks. Our results provide direct evidence for the presence of a supermassive black hole at the center of the Milky Way, and for the first time we connect the predictions from dynamical measurements of stellar orbits on scales of 103–105 gravitational radii to event-horizon-scale images and variability. Furthermore, a comparison with the EHT results for the supermassive black hole M87* shows consistency with the predictions of general relativity spanning over three orders of magnitude in central mass."
MICHAEL D KLEIN,The long-baseline neutrino experiment: exploring fundamental symmetries of the universe,"The preponderance of matter over antimatter in the early Universe, the dynamics of the supernova bursts that produced the heavy elements necessary for life and whether protons eventually decay --- these mysteries at the forefront of particle physics and astrophysics are key to understanding the early evolution of our Universe, its current state and its eventual fate. The Long-Baseline Neutrino Experiment (LBNE) represents an extensively developed plan for a world-class experiment dedicated to addressing these questions. LBNE is conceived around three central components: (1) a new, high-intensity neutrino source generated from a megawatt-class proton accelerator at Fermi National Accelerator Laboratory, (2) a near neutrino detector just downstream of the source, and (3) a massive liquid argon time-projection chamber deployed as a far detector deep underground at the Sanford Underground Research Facility. This facility, located at the site of the former Homestake Mine in Lead, South Dakota, is approximately 1,300 km from the neutrino source at Fermilab -- a distance (baseline) that delivers optimal sensitivity to neutrino charge-parity symmetry violation and mass ordering effects. This ambitious yet cost-effective design incorporates scalability and flexibility and can accommodate a variety of upgrades and contributions. With its exceptional combination of experimental configuration, technical capabilities, and potential for transformative discoveries, LBNE promises to be a vital facility for the field of particle physics worldwide, providing physicists from around the globe with opportunities to collaborate in a twenty to thirty year program of exciting science. In this document we provide a comprehensive overview of LBNE's scientific objectives, its place in the landscape of neutrino physics worldwide, the technologies it will incorporate and the capabilities it will possess."
MICHAEL D KLEIN,Event Horizon Telescope imaging of the archetypal blazar 3C 279 at an extreme 20 microarcsecond resolution,"3C 279 is an archetypal blazar with a prominent radio jet that show broadband flux density variability across the entire electromagnetic spectrum. We use an ultra-high angular resolution technique – global Very Long Baseline Interferometry (VLBI) at 1.3 mm (230 GHz) – to resolve the innermost jet of 3C 279 in order to study its fine-scale morphology close to the jet base where highly variable γ-ray emission is thought to originate, according to various models. The source was observed during four days in April 2017 with the Event Horizon Telescope at 230 GHz, including the phased Atacama Large Millimeter/submillimeter Array (ALMA), at an angular resolution of ∼20 μas (at a redshift of z = 0.536 this corresponds to ∼0.13 pc  ∼ 1700 Schwarzschild radii with a black hole mass M<jats:sub>BH</jats:sub> = 8 × 10^8 M_⊙). Imaging and model-fitting techniques were applied to the data to parameterize the fine-scale source structure and its variation. We find a multicomponent inner jet morphology with the northernmost component elongated perpendicular to the direction of the jet, as imaged at longer wavelengths. The elongated nuclear structure is consistent on all four observing days and across different imaging methods and model-fitting techniques, and therefore appears robust. Owing to its compactness and brightness, we associate the northern nuclear structure as the VLBI “core”. This morphology can be interpreted as either a broad resolved jet base or a spatially bent jet. We also find significant day-to-day variations in the closure phases, which appear most pronounced on the triangles with the longest baselines. Our analysis shows that this variation is related to a systematic change of the source structure. Two inner jet components move non-radially at apparent speeds of ∼15 c and ∼20 c (∼1.3 and ∼1.7 μas day^−1, respectively), which more strongly supports the scenario of traveling shocks or instabilities in a bent, possibly rotating jet. The observed apparent speeds are also coincident with the 3C 279 large-scale jet kinematics observed at longer (cm) wavelengths, suggesting no significant jet acceleration between the 1.3 mm core and the outer jet. The intrinsic brightness temperature of the jet components are ≲10^10 K, a magnitude or more lower than typical values seen at ≥7 mm wavelengths. The low brightness temperature and morphological complexity suggest that the core region of 3C 279 becomes optically thin at short (mm) wavelengths."
MICHAEL D KLEIN,First M87 Event Horizon Telescope results. II. Array and instrumentation,"The Event Horizon Telescope (EHT) is a very long baseline interferometry (VLBI) array that comprises millimeter- and submillimeter-wavelength telescopes separated by distances comparable to the diameter of the Earth. At a nominal operating wavelength of ~1.3 mm, EHT angular resolution (λ/D) is ~25 μas, which is sufficient to resolve nearby supermassive black hole candidates on spatial and temporal scales that correspond to their event horizons. With this capability, the EHT scientific goals are to probe general relativistic effects in the strong-field regime and to study accretion and relativistic jet formation near the black hole boundary. In this Letter we describe the system design of the EHT, detail the technology and instrumentation that enable observations, and provide measures of its performance. Meeting the EHT science objectives has required several key developments that have facilitated the robust extension of the VLBI technique to EHT observing wavelengths and the production of instrumentation that can be deployed on a heterogeneous array of existing telescopes and facilities. To meet sensitivity requirements, high-bandwidth digital systems were developed that process data at rates of 64 gigabit s−1, exceeding those of currently operating cm-wavelength VLBI arrays by more than an order of magnitude. Associated improvements include the development of phasing systems at array facilities, new receiver installation at several sites, and the deployment of hydrogen maser frequency standards to ensure coherent data capture across the array. These efforts led to the coordination and execution of the first Global EHT observations in 2017 April, and to event-horizon-scale imaging of the supermassive black hole candidate in M87."
MICHAEL D KLEIN,First M87 Event Horizon Telescope results. I. The shadow of the supermassive black hole,"When surrounded by a transparent emission region, black holes are expected to reveal a dark shadow caused by gravitational light bending and photon capture at the event horizon. To image and study this phenomenon, we have assembled the Event Horizon Telescope, a global very long baseline interferometry array observing at a wavelength of 1.3 mm. This allows us to reconstruct event-horizon-scale images of the supermassive black hole candidate in the center of the giant elliptical galaxy M87. We have resolved the central compact radio source as an asymmetric bright emission ring with a diameter of 42 ± 3 μas, which is circular and encompasses a central depression in brightness with a flux ratio gsim10:1. The emission ring is recovered using different calibration and imaging schemes, with its diameter and width remaining stable over four different observations carried out in different days. Overall, the observed image is consistent with expectations for the shadow of a Kerr black hole as predicted by general relativity. The asymmetry in brightness in the ring can be explained in terms of relativistic beaming of the emission from a plasma rotating close to the speed of light around a black hole. We compare our images to an extensive library of ray-traced general-relativistic magnetohydrodynamic simulations of black holes and derive a central mass of M = (6.5 ± 0.7) × 10^9 M ⊙. Our radio-wave observations thus provide powerful evidence for the presence of supermassive black holes in centers of galaxies and as the central engines of active galactic nuclei. They also present a new tool to explore gravity in its most extreme limit and on a mass scale that was so far not accessible."
ALEXANDER WALLEY,Implications of Cannabis Use and Heavy Alcohol Use on HIV Drug Risk Behaviors in Russian Heroin Users,"Cannabis and heavy alcohol use potentially increase HIV transmission by increasing risky drug behaviors. We studied 404 subjects entering treatment for heroin dependence, in St. Petersburg, Russia. We used the HIV Risk Assessment Battery (RAB) drug subscale to measure risky drug behavior. Although all heavy alcohol users had risky drug behaviors, their drug RAB scores did not differ from non-heavy alcohol users in unadjusted or adjusted analyses. Cannabis use was significantly associated with drug RAB scores in unadjusted analyses (mean difference 1.7 points) and analyses adjusted for age, sex, and employment (mean difference 1.3 points). When also adjusting for stimulant use, the impact of cannabis use was attenuated and no longer statistically significant (mean difference 1.1 points). Because of the central role of risky drug behaviors in the Russian HIV epidemic, it is important to understand how the use of multiple substances, including cannabis and alcohol, impacts risky drug behaviors."
JOSHUA FINEBERG,Autant de libertés que l'esprit prend avec la nature,Original composition for 6 instruments commissioned by Radio France and the Ensemble InterContemporain.
KATHRYN LUNETTA,The Framingham Heart Study 100K SNP Genome-Wide Association Study Resource: Overview of 17 Phenotype Working Group Reports,"BACKGROUND: The Framingham Heart Study (FHS), founded in 1948 to examine the epidemiology of cardiovascular disease, is among the most comprehensively characterized multi-generational studies in the world. Many collected phenotypes have substantial genetic contributors; yet most genetic determinants remain to be identified. Using single nucleotide polymorphisms (SNPs) from a 100K genome-wide scan, we examine the associations of common polymorphisms with phenotypic variation in this community-based cohort and provide a full-disclosure, web-based resource of results for future replication studies. METHODS: Adult participants (n = 1345) of the largest 310 pedigrees in the FHS, many biologically related, were genotyped with the 100K Affymetrix GeneChip. These genotypes were used to assess their contribution to 987 phenotypes collected in FHS over 56 years of follow up, including: cardiovascular risk factors and biomarkers; subclinical and clinical cardiovascular disease; cancer and longevity traits; and traits in pulmonary, sleep, neurology, renal, and bone domains. We conducted genome-wide variance components linkage and population-based and family-based association tests. RESULTS: The participants were white of European descent and from the FHS Original and Offspring Cohorts (examination 1 Offspring mean age 32 ± 9 years, 54% women). This overview summarizes the methods, selected findings and limitations of the results presented in the accompanying series of 17 manuscripts. The presented association results are based on 70,897 autosomal SNPs meeting the following criteria: minor allele frequency ≥ 10%, genotype call rate ≥ 80%, Hardy-Weinberg equilibrium p-value ≥ 0.001, and satisfying Mendelian consistency. Linkage analyses are based on 11,200 SNPs and short-tandem repeats. Results of phenotype-genotype linkages and associations for all autosomal SNPs are posted on the NCBI dbGaP website at. CONCLUSION: We have created a full-disclosure resource of results, posted on the dbGaP website, from a genome-wide association study in the FHS. Because we used three analytical approaches to examine the association and linkage of 987 phenotypes with thousands of SNPs, our results must be considered hypothesis-generating and need to be replicated. Results from the FHS 100K project with NCBI web posting provides a resource for investigators to identify high priority findings for replication."
KATHRYN LUNETTA,Genome-Wide Association with Select Biomarker Traits in the Framingham Heart Study,"BACKGROUND: Systemic biomarkers provide insights into disease pathogenesis, diagnosis, and risk stratification. Many systemic biomarker concentrations are heritable phenotypes. Genome-wide association studies (GWAS) provide mechanisms to investigate the genetic contributions to biomarker variability unconstrained by current knowledge of physiological relations. METHODS: We examined the association of Affymetrix 100K GeneChip single nucleotide polymorphisms (SNPs) to 22 systemic biomarker concentrations in 4 biological domains: inflammation/oxidative stress; natriuretic peptides; liver function; and vitamins. Related members of the Framingham Offspring cohort (n = 1012; mean age 59 ± 10 years, 51% women) had both phenotype and genotype data (minimum-maximum per phenotype n = 507–1008). We used Generalized Estimating Equations (GEE), Family Based Association Tests (FBAT) and variance components linkage to relate SNPs to multivariable-adjusted biomarker residuals. Autosomal SNPs (n = 70,987) meeting the following criteria were studied: minor allele frequency ≥ 10%, call rate ≥ 80% and Hardy-Weinberg equilibrium p ≥ 0.001. RESULTS: With GEE, 58 SNPs had p < 10-6: the top SNPs were rs2494250 (p = 1.00*10-14) and rs4128725 (p = 3.68*10-12) for monocyte chemoattractant protein-1 (MCP1), and rs2794520 (p = 2.83*10-8) and rs2808629 (p = 3.19*10-8) for C-reactive protein (CRP) averaged from 3 examinations (over about 20 years). With FBAT, 11 SNPs had p < 10-6: the top SNPs were the same for MCP1 (rs4128725, p = 3.28*10-8, and rs2494250, p = 3.55*10-8), and also included B-type natriuretic peptide (rs437021, p = 1.01*10-6) and Vitamin K percent undercarboxylated osteocalcin (rs2052028, p = 1.07*10-6). The peak LOD (logarithm of the odds) scores were for MCP1 (4.38, chromosome 1) and CRP (3.28, chromosome 1; previously described) concentrations; of note the 1.5 support interval included the MCP1 and CRP SNPs reported above (GEE model). Previous candidate SNP associations with circulating CRP concentrations were replicated at p < 0.05; the SNPs rs2794520 and rs2808629 are in linkage disequilibrium with previously reported SNPs. GEE, FBAT and linkage results are posted at . CONCLUSION: The Framingham GWAS represents a resource to describe potentially novel genetic influences on systemic biomarker variability. The newly described associations will need to be replicated in other studies."
KATHRYN LUNETTA,Genetic Correlates of Longevity and Selected Age-Related Phenotypes: A Genome-Wide Association Study in the Framingham Study,"BACKGROUND: Family studies and heritability estimates provide evidence for a genetic contribution to variation in the human life span. METHODS: We conducted a genome wide association study (Affymetrix 100K SNP GeneChip) for longevity-related traits in a community-based sample. We report on 5 longevity and aging traits in up to 1345 Framingham Study participants from 330 families. Multivariable-adjusted residuals were computed using appropriate models (Cox proportional hazards, logistic, or linear regression) and the residuals from these models were used to test for association with qualifying SNPs (70, 987 autosomal SNPs with genotypic call rate ≥80%, minor allele frequency ≥10%, Hardy-Weinberg test p ≥ 0.001). RESULTS: In family-based association test (FBAT) models, 8 SNPs in two regions approximately 500 kb apart on chromosome 1 (physical positions 73,091,610 and 73, 527,652) were associated with age at death (p-value < 10-5). The two sets of SNPs were in high linkage disequilibrium (minimum r2 = 0.58). The top 30 SNPs for generalized estimating equation (GEE) tests of association with age at death included rs10507486 (p = 0.0001) and rs4943794 (p = 0.0002), SNPs intronic to FOXO1A, a gene implicated in lifespan extension in animal models. FBAT models identified 7 SNPs and GEE models identified 9 SNPs associated with both age at death and morbidity-free survival at age 65 including rs2374983 near PON1. In the analysis of selected candidate genes, SNP associations (FBAT or GEE p-value < 0.01) were identified for age at death in or near the following genes: FOXO1A, GAPDH, KL, LEPR, PON1, PSEN1, SOD2, and WRN. Top ranked SNP associations in the GEE model for age at natural menopause included rs6910534 (p = 0.00003) near FOXO3a and rs3751591 (p = 0.00006) in CYP19A1. Results of all longevity phenotype-genotype associations for all autosomal SNPs are web posted at . CONCLUSION. Longevity and aging traits are associated with SNPs on the Affymetrix 100K GeneChip. None of the associations achieved genome-wide significance. These data generate hypotheses and serve as a resource for replication as more genes and biologic pathways are proposed as contributing to longevity and healthy aging."
KATHRYN LUNETTA,"Concordance of Metabolic Enzyme Genotypes Assayed from Paraffin-Embedded, Formalin-Fixed Breast Tumors and Normal Lymphatic Tissue","OBJECTIVES: Translational epidemiology studies often use archived tumor specimens to evaluate genetic hypotheses involving cancer outcomes. When the exposure of interest is a germline polymorphism, a key concern is whether the genotype assayed from tumor-derived DNA is representative of the germline. We evaluated the concordance between breast tumor-derived and normal lymph node-derived genotypes for three polymorphic tamoxifen-metabolizing enzymes. METHODS. We assayed paired DNA samples extracted from archived tumor and normal lymph node tissues from 106 breast cancer patients. We used TaqMan assays to determine the genotypes of three enzyme variants hypothesized to modify tamoxifen effectiveness, ie, CYP2D6*4, UGT2B15*2, and UGT1A8*2. We assessed genotype agreement between the two DNA sources by calculating the percent agreement and the weighted kappa statistic. RESULTS: We successfully obtained genotypes for CYP2D6*4, UGT2B15*2, and UGT1A8*2 in 99%, 100%, and 84% of the paired samples, respectively. Genotype concordance was perfect for the CYP2D6*4 and UGT1A8*2 variants (weighted kappa for both = 1.00; 95% confidence interval [CI] 1.00, 1.00). For UGT2B15*2, one pair out of 106 gave a discordant result that persisted over several assay repeats. CONCLUSIONS: We observed strong agreement between DNA from breast tumors and normal lymphatic tissue in the genotyping of polymorphisms in three tamoxifen-metabolizing enzymes. Genotyping DNA extracted from tumor tissue avoids the time-consuming practice of microdissecting adjacent normal tissue when other normal tissue sources are not available. Therefore, the demonstrated reliability of tumor-derived DNA allows resources to be spent instead on increasing sample size or the number of polymorphisms examined."
KATHRYN LUNETTA,Genome-wide association with bone mass and geometry in the Framingham Heart Study,"BACKGROUND:Osteoporosis is characterized by low bone mass and compromised bone structure, heritable traits that contribute to fracture risk. There have been no genome-wide association and linkage studies for these traits using high-density genotyping platforms.METHODS:We used the Affymetrix 100K SNP GeneChip marker set in the Framingham Heart Study (FHS) to examine genetic associations with ten primary quantitative traits: bone mineral density (BMD), calcaneal ultrasound, and geometric indices of the hip. To test associations with multivariable-adjusted residual trait values, we used additive generalized estimating equation (GEE) and family-based association tests (FBAT) models within each sex as well as sexes combined. We evaluated 70,987 autosomal SNPs with genotypic call rates [greater than or equal to]80%, HWE p [greater than or equal to] 0.001, and MAF [greater than or equal to]10% in up to 1141 phenotyped individuals (495 men and 646 women, mean age 62.5 yrs). Variance component linkage analysis was performed using 11,200 markers.RESULTS:Heritability estimates for all bone phenotypes were 30-66%. LOD scores [greater than or equal to]3.0 were found on chromosomes 15 (1.5 LOD confidence interval: 51,336,679-58,934,236 bp) and 22 (35,890,398-48,603,847 bp) for femoral shaft section modulus. The ten primary phenotypes had 12 associations with 100K SNPs in GEE models at p < 0.000001 and 2 associations in FBAT models at p < 0.000001. The 25 most significant p-values for GEE and FBAT were all less than 3.5 x 10-6 and 2.5 x 10-5, respectively. Of the 40 top SNPs with the greatest numbers of significantly associated BMD traits (including femoral neck, trochanter, and lumbar spine), one half to two-thirds were in or near genes that have not previously been studied for osteoporosis. Notably, pleiotropic associations between BMD and bone geometric traits were uncommon. Evidence for association (FBAT or GEE p < 0.05) was observed for several SNPs in candidate genes for osteoporosis, such as rs1801133 in MTHFR; rs1884052 and rs3778099 in ESR1; rs4988300 in LRP5; rs2189480 in VDR; rs2075555 in COLIA1; rs10519297 and rs2008691 in CYP19, as well as SNPs in PPARG (rs10510418 and rs2938392) and ANKH (rs2454873 and rs379016). All GEE, FBAT and linkage results are provided as an open-access results resource at http://www.ncbi.nlm.nih.gov/projects/gap/cgi-bin/study.cgi?id=phs000007.CONCLUSION:The FHS 100K SNP project offers an unbiased genome-wide strategy to identify new candidate loci and to replicate previously suggested candidate genes for osteoporosis."
KATHRYN LUNETTA,The Framingham Heart Study 100K SNP Genome-Wide Association Study Resource: Overview of 17 Phenotype Working Group Reports,"BACKGROUND: The Framingham Heart Study (FHS), founded in 1948 to examine the epidemiology of cardiovascular disease, is among the most comprehensively characterized multi-generational studies in the world. Many collected phenotypes have substantial genetic contributors; yet most genetic determinants remain to be identified. Using single nucleotide polymorphisms (SNPs) from a 100K genome-wide scan, we examine the associations of common polymorphisms with phenotypic variation in this community-based cohort and provide a full-disclosure, web-based resource of results for future replication studies. METHODS: Adult participants (n = 1345) of the largest 310 pedigrees in the FHS, many biologically related, were genotyped with the 100K Affymetrix GeneChip. These genotypes were used to assess their contribution to 987 phenotypes collected in FHS over 56 years of follow up, including: cardiovascular risk factors and biomarkers; subclinical and clinical cardiovascular disease; cancer and longevity traits; and traits in pulmonary, sleep, neurology, renal, and bone domains. We conducted genome-wide variance components linkage and population-based and family-based association tests. RESULTS: The participants were white of European descent and from the FHS Original and Offspring Cohorts (examination 1 Offspring mean age 32 ± 9 years, 54% women). This overview summarizes the methods, selected findings and limitations of the results presented in the accompanying series of 17 manuscripts. The presented association results are based on 70,897 autosomal SNPs meeting the following criteria: minor allele frequency ≥ 10%, genotype call rate ≥ 80%, Hardy-Weinberg equilibrium p-value ≥ 0.001, and satisfying Mendelian consistency. Linkage analyses are based on 11,200 SNPs and short-tandem repeats. Results of phenotype-genotype linkages and associations for all autosomal SNPs are posted on the NCBI dbGaP website at. CONCLUSION: We have created a full-disclosure resource of results, posted on the dbGaP website, from a genome-wide association study in the FHS. Because we used three analytical approaches to examine the association and linkage of 987 phenotypes with thousands of SNPs, our results must be considered hypothesis-generating and need to be replicated. Results from the FHS 100K project with NCBI web posting provides a resource for investigators to identify high priority findings for replication."
KATHRYN LUNETTA,Two-Stage Approach for Identifying Single-Nucleotide Polymorphisms Associated with Rheumatoid Arthritis Using Random Forests and Bayesian Networks,"We used the simulated data set from Genetic Analysis Workshop 15 Problem 3 to assess a two-stage approach for identifying single-nucleotide polymorphisms (SNPs) associated with rheumatoid arthritis (RA). In the first stage, we used random forests (RF) to screen large amounts of genetic data using the variable importance measure, which takes into account SNP interaction effects as well as main effects without requiring model specification. We used the simulated 9187 SNPs mimicking a 10 K SNP chip, along with covariates DR (the simulated DRB1 gentoype), smoking, and sex as input to the RF analyses with a training set consisting of 750 unrelated RA cases and 750 controls. We used an iterative RF screening procedure to identify a smaller set of variables for further analysis. In the second stage, we used the software program CaMML for producing Bayesian networks, and developed complex etiologic models for RA risk using the variables identified by our RF screening procedure. We evaluated the performance of this method using independent test data sets for up to 100 replicates."
KATHRYN LUNETTA,Principal-Component-Based Population Structure Adjustment in the North American Rheumatoid Arthritis Consortium Data: Impact of Single-Nucleotide Polymorphism Set and Analysis Method,"Population structure occurs when a sample is composed of individuals with different ancestries and can result in excess type I error in genome-wide association studies. Genome-wide principal-component analysis (PCA) has become a popular method for identifying and adjusting for subtle population structure in association studies. Using the Genetic Analysis Workshop 16 (GAW16) NARAC data, we explore two unresolved issues concerning the use of genome-wide PCA to account for population structure in genetic associations studies: the choice of single-nucleotide polymorphism (SNP) subset and the choice of adjustment model. We computed PCs for subsets of genome-wide SNPs with varying levels of LD. The first two PCs were similar for all subsets and the first three PCs were associated with case status for all subsets. When the PCs associated with case status were included as covariates in an association model, the reduction in genomic inflation factor was similar for all SNP sets. Several models have been proposed to account for structure using PCs, but it is not yet clear whether the different methods will result in substantively different results for association studies with individuals of European descent. We compared genome-wide association p-values and results for two positive-control SNPs previously associated with rheumatoid arthritis using four PC adjustment methods as well as no adjustment and genomic control. We found that in this sample, adjusting for the continuous PCs or adjusting for discrete clusters identified using the PCs adequately accounts for the case-control population structure, but that a recently proposed randomization test performs poorly."
KATHRYN LUNETTA,Performance of Random Forest When SNPs Are in Linkage Disequilibrium,"BACKGROUND. Single nucleotide polymorphisms (SNPs) may be correlated due to linkage disequilibrium (LD). Association studies look for both direct and indirect associations with disease loci. In a Random Forest (RF) analysis, correlation between a true risk SNP and SNPs in LD may lead to diminished variable importance for the true risk SNP. One approach to address this problem is to select SNPs in linkage equilibrium (LE) for analysis. Here, we explore alternative methods for dealing with SNPs in LD: change the tree-building algorithm by building each tree in an RF only with SNPs in LE, modify the importance measure (IM), and use haplotypes instead of SNPs to build a RF. RESULTS. We evaluated the performance of our alternative methods by simulation of a spectrum of complex genetics models. When a haplotype rather than an individual SNP is the risk factor, we find that the original Random Forest method performed on SNPs provides good performance. When individual, genotyped SNPs are the risk factors, we find that the stronger the genetic effect, the stronger the effect LD has on the performance of the original RF. A revised importance measure used with the original RF is relatively robust to LD among SNPs; this revised importance measure used with the revised RF is sometimes inflated. Overall, we find that the revised importance measure used with the original RF is the best choice when the genetic model and the number of SNPs in LD with risk SNPs are unknown. For the haplotype-based method, under a multiplicative heterogeneity model, we observed a decrease in the performance of RF with increasing LD among the SNPs in the haplotype. CONCLUSION. Our results suggest that by strategically revising the Random Forest method tree-building or importance measure calculation, power can increase when LD exists between SNPs. We conclude that the revised Random Forest method performed on SNPs offers an advantage of not requiring genotype phase, making it a viable tool for use in the context of thousands of SNPs, such as candidate gene studies and follow-up of top candidates from genome wide association studies."
KATHRYN LUNETTA,Genetic correlates of longevity and selected age-related phenotypes: a genome-wide association study in the Framingham Study,"BACKGROUND: Family studies and heritability estimates provide evidence for a genetic contribution to variation in the human life span. METHODS:We conducted a genome wide association study (Affymetrix 100K SNP GeneChip) for longevity-related traits in a community-based sample. We report on 5 longevity and aging traits in up to 1345 Framingham Study participants from 330 families. Multivariable-adjusted residuals were computed using appropriate models (Cox proportional hazards, logistic, or linear regression) and the residuals from these models were used to test for association with qualifying SNPs (70, 987 autosomal SNPs with genotypic call rate [greater than or equal to]80%, minor allele frequency [greater than or equal to]10%, Hardy-Weinberg test p [greater than or equal to] 0.001).RESULTS:In family-based association test (FBAT) models, 8 SNPs in two regions approximately 500 kb apart on chromosome 1 (physical positions 73,091,610 and 73, 527,652) were associated with age at death (p-value < 10-5). The two sets of SNPs were in high linkage disequilibrium (minimum r2 = 0.58). The top 30 SNPs for generalized estimating equation (GEE) tests of association with age at death included rs10507486 (p = 0.0001) and rs4943794 (p = 0.0002), SNPs intronic to FOXO1A, a gene implicated in lifespan extension in animal models. FBAT models identified 7 SNPs and GEE models identified 9 SNPs associated with both age at death and morbidity-free survival at age 65 including rs2374983 near PON1. In the analysis of selected candidate genes, SNP associations (FBAT or GEE p-value < 0.01) were identified for age at death in or near the following genes: FOXO1A, GAPDH, KL, LEPR, PON1, PSEN1, SOD2, and WRN. Top ranked SNP associations in the GEE model for age at natural menopause included rs6910534 (p = 0.00003) near FOXO3a and rs3751591 (p = 0.00006) in CYP19A1. Results of all longevity phenotype-genotype associations for all autosomal SNPs are web posted at http://www.ncbi.nlm.nih.gov/projects/gap/cgi-bin/study.cgi?id=phs000007. CONCLUSION: Longevity and aging traits are associated with SNPs on the Affymetrix 100K GeneChip. None of the associations achieved genome-wide significance. These data generate hypotheses and serve as a resource for replication as more genes and biologic pathways are proposed as contributing to longevity and healthy aging."
KATHRYN LUNETTA,Screening large-scale association study data: exploiting interactions using random forests,"BACKGROUND. Genome-wide association studies for complex diseases will produce genotypes on hundreds of thousands of single nucleotide polymorphisms (SNPs). A logical first approach to dealing with massive numbers of SNPs is to use some test to screen the SNPs, retaining only those that meet some criterion for futher study. For example, SNPs can be ranked by p-value, and those with the lowest p-values retained. When SNPs have large interaction effects but small marginal effects in a population, they are unlikely to be retained when univariate tests are used for screening. However, model-based screens that pre-specify interactions are impractical for data sets with thousands of SNPs. Random forest analysis is an alternative method that produces a single measure of importance for each predictor variable that takes into account interactions among variables without requiring model specification. Interactions increase the importance for the individual interacting variables, making them more likely to be given high importance relative to other variables. We test the performance of random forests as a screening procedure to identify small numbers of risk-associated SNPs from among large numbers of unassociated SNPs using complex disease models with up to 32 loci, incorporating both genetic heterogeneity and multi-locus interaction. RESULTS. Keeping other factors constant, if risk SNPs interact, the random forest importance measure significantly outperforms the Fisher Exact test as a screening tool. As the number of interacting SNPs increases, the improvement in performance of random forest analysis relative to Fisher Exact test for screening also increases. Random forests perform similarly to the univariate Fisher Exact test as a screening tool when SNPs in the analysis do not interact. CONCLUSIONS. In the context of large-scale genetic association studies where unknown interactions exist among true risk-associated SNPs or SNPs and environmental covariates, screening SNPs using random forest analyses can significantly reduce the number of SNPs that need to be retained for further study compared to standard univariate screening methods."
KATHRYN LUNETTA,Genome-wide association with bone mass and geometry in the Framingham Heart Study,"BACKGROUND: Osteoporosis is characterized by low bone mass and compromised bone structure, heritable traits that contribute to fracture risk. There have been no genome-wide association and linkage studies for these traits using high-density genotyping platforms. METHODS: We used the Affymetrix 100K SNP GeneChip marker set in the Framingham Heart Study (FHS) to examine genetic associations with ten primary quantitative traits: bone mineral density (BMD), calcaneal ultrasound, and geometric indices of the hip. To test associations with multivariable-adjusted residual trait values, we used additive generalized estimating equation (GEE) and family-based association tests (FBAT) models within each sex as well as sexes combined. We evaluated 70,987 autosomal SNPs with genotypic call rates ≥80%, HWE p ≥ 0.001, and MAF ≥10% in up to 1141 phenotyped individuals (495 men and 646 women, mean age 62.5 yrs). Variance component linkage analysis was performed using 11,200 markers. RESULTS: Heritability estimates for all bone phenotypes were 30–66%. LOD scores ≥3.0 were found on chromosomes 15 (1.5 LOD confidence interval: 51,336,679–58,934,236 bp) and 22 (35,890,398–48,603,847 bp) for femoral shaft section modulus. The ten primary phenotypes had 12 associations with 100K SNPs in GEE models at p < 0.000001 and 2 associations in FBAT models at p < 0.000001. The 25 most significant p-values for GEE and FBAT were all less than 3.5 × 10-6 and 2.5 × 10-5, respectively. Of the 40 top SNPs with the greatest numbers of significantly associated BMD traits (including femoral neck, trochanter, and lumbar spine), one half to two-thirds were in or near genes that have not previously been studied for osteoporosis. Notably, pleiotropic associations between BMD and bone geometric traits were uncommon. Evidence for association (FBAT or GEE p < 0.05) was observed for several SNPs in candidate genes for osteoporosis, such as rs1801133 in MTHFR; rs1884052 and rs3778099 in ESR1; rs4988300 in LRP5; rs2189480 in VDR; rs2075555 in COLIA1; rs10519297 and rs2008691 in CYP19, as well as SNPs in PPARG (rs10510418 and rs2938392) and ANKH (rs2454873 and rs379016). All GEE, FBAT and linkage results are provided as an open-access results resource at. CONCLUSION: The FHS 100K SNP project offers an unbiased genome-wide strategy to identify new candidate loci and to replicate previously suggested candidate genes for osteoporosis."
MARC W HOWARD,DeepSITH: efficient learning via decomposition of what and when across time scales,"Extracting temporal relationships over a range of scales is a hallmark of human perception and cognition -- and thus it is a critical feature of machine learning applied to real-world problems. Neural networks are either plagued by the exploding/vanishing gradient problem in recurrent neural networks (RNNs) or must adjust their parameters to learn the relevant time scales (e.g., in LSTMs). This paper introduces DeepSITH, a network comprising biologically-inspired Scale-Invariant Temporal History (SITH) modules in series with dense connections between layers. SITH modules respond to their inputs with a geometrically-spaced set of time constants, enabling the DeepSITH network to learn problems along a continuum of time-scales. We compare DeepSITH to LSTMs and other recent RNNs on several time series prediction and decoding tasks. DeepSITH achieves state-of-the-art performance on these problems."
MARC W HOWARD,Neural scaling laws for an uncertain world,"Autonomous neural systems must efficiently process information in a wide range of novel environments, which may have very different statistical properties. We consider the problem of how to optimally distribute receptors along a one-dimensional continuum consistent with the following design principles. First, neural representations of the world should obey a neural uncertainty principle—making as few assumptions as possible about the statistical structure of the world. Second, neural representations should convey, as much as possible, equivalent information about environments with different statistics. The results of these arguments resemble the structure of the visual system and provide a natural explanation of the behavioral WeberFechner law, a foundational result in psychology. Because the derivation is extremely general, this suggests that similar scaling relationships should be observed not only in sensory continua, but also in neural representations of “cognitive’ one-dimensional quantities such as time or numerosity."
MARC W HOWARD,"Is working memory stored along a logarithmic timeline? Converging evidence from neuroscience, behavior and models","A growing body of evidence suggests that short-term memory does not only store the identity of recently experienced stimuli, but also information about when they were presented. This representation of 'what' happened 'when' constitutes a neural timeline of recent past. Behavioral results suggest that people can sequentially access memories for the recent past, as if they were stored along a timeline to which attention is sequentially directed. In the short-term judgment of recency (JOR) task, the time to choose between two probe items depends on the recency of the more recent probe but not on the recency of the more remote probe. This pattern of results suggests a backward self-terminating search model. We review recent neural evidence from the macaque lateral prefrontal cortex (lPFC) (Tiganj, Cromer, Roy, Miller, & Howard, in press) and behavioral evidence from human JOR task (Singh & Howard, 2017) bearing on this question. Notably, both lines of evidence suggest that the timeline is logarithmically compressed as predicted by Weber-Fechner scaling. Taken together, these findings provide an integrative perspective on temporal organization and neural underpinnings of short-term memory."
MARC W HOWARD,Formal models of memory based on temporally-varying representations,"The idea that memory behavior relies on a gradually-changing internal state has a long history in mathematical psychology. This chapter traces this line of thought from statistical learning theory in the 1950s, through distributed memory models in the latter part of the 20th century and early part of the 21st century through to modern models based on a scale-invariant temporal history. We discuss the neural phenomena consistent with this form of representation and sketch the kinds of cognitive models that can be constructed using it and connections with formal models of various memory tasks."
MARC W HOWARD,Memory for time,"The brain maintains a record of recent events including information about the time at which events were experienced. We review behavioral and neuro-physiological evidence as well as computational models to better understand memory for time. Neurophysiologically, populations of neurons that record the time of recent events have been observed in many brain regions. Time cells fire in long sequences after a triggering event demonstrating memory for the past. Populations of exponentially-decaying neurons record past events at many delays by decaying at different rates. Both kinds of representations record distant times with less temporal resolution. The work reviewed here converges on the idea that the brain maintains a representation of past events along a scale-invariant compressed timeline."
MARC W HOWARD,Compressed timeline of recent experience in monkey lateral prefrontal cortex,"Cognitive theories suggest that working memory maintains not only the identity of recently presented stimuli but also a sense of the elapsed time since the stimuli were presented. Previous studies of the neural underpinnings of working memory have focused on sustained firing, which can account for maintenance of the stimulus identity, but not for representation of the elapsed time. We analyzed single-unit recordings from the lateral prefrontal cortex of macaque monkeys during performance of a delayed match-to-category task. Each sample stimulus triggered a consistent sequence of neurons, with each neuron in the sequence firing during a circumscribed period. These sequences of neurons encoded both stimulus identity and elapsed time. The encoding of elapsed time became less precise as the sample stimulus receded into the past. These findings suggest that working memory includes a compressed timeline of what happened when, consistent with long-standing cognitive theories of human memory."
MARC W HOWARD,Evidence accumulation in a Laplace domain decision space,"Evidence accumulation models of simple decision-making have long assumed that the brain estimates a scalar decision variable corresponding to the log likelihood ratio of the two alternatives. Typical neural implementations of this algorithmic cognitive model assume that large numbers of neurons are each noisy exemplars of the scalar decision variable. Here, we propose a neural implementation of the diffusion model in which many neurons construct and maintain the Laplace transform of the distance to each of the decision bounds. As in classic findings from brain regions including LIP, the firing rate of neurons coding for the Laplace transform of net accumulated evidence grows to a bound during random dot motion tasks. However, rather than noisy exemplars of a single mean value, this approach makes the novel prediction that firing rates grow to the bound exponentially; across neurons, there should be a distribution of different rates. A second set of neurons records an approximate inversion of the Laplace transform; these neurons directly estimate net accumulated evidence. In analogy to time cells and place cells observed in the hippocampus and other brain regions, the neurons in this second set have receptive fields along a “decision axis.” This finding is consistent with recent findings from rodent recordings. This theoretical approach places simple evidence accumulation models in the same mathematical language as recent proposals for representing time and space in cognitive models for memory."
MARC W HOWARD,Human episodic memory retrieval is accompanied by a neural contiguity effect,"Cognitive psychologists have long hypothesized that experiences are encoded in a temporal context that changes gradually over time. When an episodic memory is retrieved, the state of context is recovered—a jump back in time. We recorded from single units in the medial temporal lobe of epilepsy patients performing an item recognition task. The population vector changed gradually over minutes during presentation of the list. When a probe from the list was remembered with high confidence, the population vector reinstated the temporal context of the original presentation of that probe during study, a neural contiguity effect that provides a possible mechanism for behavioral contiguity effects. This pattern was only observed for well remembered probes; old probes that were not well remembered showed an anti-contiguity effect. These results constitute the first direct evidence that recovery of an episodic memory in humans is associated with retrieval of a gradually changing state of temporal context, a neural “jump back in time” that parallels the act of remembering."
MARC W HOWARD,Temporal and spatial context in the mind and brain,Theories of episodic memory have long hypothesized that recollection of a specific instance from one's life is mediated by recovery of a neural state of spatiotemporal context. This paper reviews recent theoretical advances in formal models of spatiotemporal context and a growing body of neurophysiological evidence from human imaging studies and animal work that neural populations in the hippocampus and other brain regions support a representation of spatiotemporal context.
MARC W HOWARD,Towards a neural-level cognitive architecture: modeling behavior in working memory tasks with neurons,"Constrained by results from classic behavioral experiments we provide a neural-level cognitive architecture for modeling behavior in working memory tasks. We propose a canonical microcircuit that can be used as a building block for working memory, decision making and cognitive control. The controller controls gates to route the flow of information between the working memory and the evidence accumulator and sets parameters of the circuits. We show that this type of cognitive architecture can account for results in behavioral experiments such as judgment of recency, probe recognition and delayedmatch- to-sample. In addition, the neural dynamics generated by the cognitive architecture provides a good match with neurophysiological data from rodents and monkeys. For instance, it generates cells tuned to a particular amount of elapsed time (time cells), to a particular position in space (place cells) and to a particular amount of accumulated evidence."
MARC W HOWARD,Scale-invariant temporal history (SITH): optimal slicing of the past in an uncertain world,"In both the human brain and any general artificial intelligence (AI), a representation of the past is necessary to predict the future. However, perfect storage of all experiences is not possible. One possibility, utilized in many applications, is to retain information about the past in a buffer. A limitation of this approach is that although events in the buffer are represented with perfect accuracy, the resources necessary to represent information at a particular time scale go up rapidly. Here we present a neurally-plausible, compressed, scale-free memory representation we call Scale-Invariant Temporal History (SITH). This representation covers an exponentially large period of time in the past at the cost of sacrificing temporal accuracy for events further in the past. The form of this decay is scale-invariant and can be shown to be optimal in that it is able to respond to worlds with a wide range of time scales. We demonstrate the utility of this representation in learning to play a simple video game. In this environment, SITH exhibits better learning performance than a fixed-size buffer history representation. Whereas the buffer performs well as long as the temporal dependencies can be represented within the buffer, SITH performs well over a much larger range of time scales for the same amount of resources. Finally, we discuss how the application of SITH, along with other human-inspired models of cognition, could improve reinforcement and machine learning algorithms in general."
MARC W HOWARD,Cognitive computation using neural representations of time and space in the Laplace domain,"Memory for the past makes use of a record of what happened when---a function over past time. Time cells in the hippocampus and temporal context cells in the entorhinal cortex both code for events as a function of past time, but with very different receptive fields. Time cells in the hippocampus can be understood as a compressed estimate of events as a function of the past. Temporal context cells in the entorhinal cortex can be understood as the Laplace transform of that function, respectively. Other functional cell types in the hippocampus and related regions, including border cells, place cells, trajectory coding, splitter cells, can be understood as coding for functions over space or past movements or their Laplace transforms. More abstract quantities, like distance in an abstract conceptual space or numerosity could also be mapped onto populations of neurons coding for the Laplace transform of functions over those variables. Quantitative cognitive models of memory and evidence accumulation can also be specified in this framework allowing constraints from both behavior and neurophysiology. More generally, the computational power of the Laplace domain could be important for efficiently implementing data-independent operators, which could serve as a basis for neural models of a very broad range of cognitive computations."
MARC W HOWARD,Internally generated time in the rodent hippocampus is logarithmically compressed,"The Weber-Fechner law proposes that our perceived sensory input increases with physical input on a logarithmic scale. Hippocampal 'time cells' carry a record of recent experience by firing sequentially during a circumscribed period of time after a triggering stimulus. Different cells have 'time fields' at different delays up to at least tens of seconds. Past studies suggest that time cells represent a compressed timeline by demonstrating that fewer time cells fire late in the delay and their time fields are wider. This paper asks whether the compression of time cells obeys the Weber-Fechner Law. Time cells were studied with a hierarchical Bayesian model that simultaneously accounts for the firing pattern at the trial level, cell level, and population level. This procedure allows separate estimates of the within-trial receptive field width and the across-trial variability. After isolating across-trial variability, time field width increased linearly with delay. Further, the time cell population was distributed evenly along a logarithmic time axis. These findings provide strong quantitative evidence that the neural temporal representation in rodent hippocampus is logarithmically compressed and obeys a neural Weber-Fechner Law."
MARC W HOWARD,A temporal record of the past with a spectrum of time constants in the monkey entorhinal cortex,"Episodic memory is believed to be intimately related to our experience of the passage of time. Indeed, neurons in the hippocampus and other brain regions critical to episodic memory code for the passage of time at a range of timescales. The origin of this temporal signal, however, remains unclear. Here, we examined temporal responses in the entorhinal cortex of macaque monkeys as they viewed complex images. Many neurons in the entorhinal cortex were responsive to image onset, showing large deviations from baseline firing shortly after image onset but relaxing back to baseline at different rates. This range of relaxation rates allowed for the time since image onset to be decoded on the scale of seconds. Further, these neurons carried information about image content, suggesting that neurons in the entorhinal cortex carry information about not only when an event took place but also, the identity of that event. Taken together, these findings suggest that the primate entorhinal cortex uses a spectrum of time constants to construct a temporal record of the past in support of episodic memory."
SUCHARITA GOPAL,Artificial neural networks in geospatial analysis,"Artificial neural networks are computational models widely used in geospatial analysis for data classification, change detection, clustering, function approximation, and forecasting or prediction. There are many types of neural networks based on learning paradigm and network architectures. Their use is expected to grow with increasing availability of massive data from remote sensing and mobile platforms."
SUCHARITA GOPAL,Seeing the invisible: from imagined to virtual urban landscapes,"Urban ecosystems consist of infrastructure features working together to provide services for inhabitants. Infrastructure functions akin to an ecosystem, having dynamic relationships and interdependencies. However, with age, urban infrastructure can deteriorate and stop functioning. Additional pressures on infrastructure include urbanizing populations and a changing climate that exposes vulnerabilities. To manage the urban infrastructure ecosystem in a modernizing world, urban planners need to integrate a coordinated management plan for these co-located and dependent infrastructure features. To implement such a management practice, an improved method for communicating how these infrastructure features interact is needed. This study aims to define urban infrastructure as a system, identify the systematic barriers preventing implementation of a more coordinated management model, and develop a virtual reality tool to provide visualization of the spatial system dynamics of urban infrastructure. Data was collected from a stakeholder workshop that highlighted a lack of appreciation for the system dynamics of urban infrastructure. An urban ecology VR model was created to highlight the interconnectedness of infrastructure features. VR proved to be useful for communicating spatial information to urban stakeholders about the complexities of infrastructure ecology and the interactions between infrastructure features."
SUCHARITA GOPAL,"ART and ARTMAP Neural Networks for Applications: Self-Organizing Learning, Recognition, and Prediction","ART and ARTMAP neural networks for adaptive recognition and prediction have been applied to a variety of problems. Applications include parts design retrieval at the Boeing Company, automatic mapping from remote sensing satellite measurements, medical database prediction, and robot vision. This chapter features a self-contained introduction to ART and ARTMAP dynamics and a complete algorithm for applications. Computational properties of these networks are illustrated by means of remote sensing and medical database examples. The basic ART and ARTMAP networks feature winner-take-all (WTA) competitive coding, which groups inputs into discrete recognition categories. WTA coding in these networks enables fast learning, that allows the network to encode important rare cases but that may lead to inefficient category proliferation with noisy training inputs. This problem is partially solved by ART-EMAP, which use WTA coding for learning but distributed category representations for test-set prediction. In medical database prediction problems, which often feature inconsistent training input predictions, the ARTMAP-IC network further improves ARTMAP performance with distributed prediction, category instance counting, and a new search algorithm. A recently developed family of ART models (dART and dARTMAP) retains stable coding, recognition, and prediction, but allows arbitrarily distributed category representation during learning as well as performance."
SUCHARITA GOPAL,ART Neural Networks for Remote Sensing: Vegetation Classification from Landsat TM and Terrain Data,"A new methodology for automatic mapping from Landsat Thematic Mapper (TM) and terrain data, based on the fuzzy ARTMAP neural network, is developed. System capabilities are tested on a challenging remote sensing classification problem, using spectral and terrain features for vegetation classification in the Cleveland National Forest. After training at the pixel level, system capabilities arc tested at the stand level, using sites not seen during training. Results are compared to those of maximum likelihood classifiers, as well as back propagation neural networks and K Nearest Neighbor algorithms. ARTMAP dynamics arc fast, stable, and scalable, overcoming common limitations of back propagation, which did not give satisfactory performance. Best results arc obtained using a hybrid system based on a convex combination of fuzzy ARTMAP and maximum likelihood predictions. Fuzzy ARTMAP automatically constructs a minimal number of recognition categories to meet accuracy criteria. A voting strategy improves prediction by training the system several times. on different orderings of an Input set. Voting assigns confidence estimates to competing predictions."
SUCHARITA GOPAL,Evaluation of neural network pattern classifiers for a remote sensing application,"This paper evaluates the classification accuracy of three neural network classifiers on a satellite image-based pattern classification problem. The neural network classifiers used include two types of the Multi-Layer-Perceptron (MLP) and the Radial Basis Function Network. A normal (conventional) classifier is used as a benchmark to evaluate the performance of neural network classifiers. The satellite image consists of 2,460 pixels selected from a section (270 x 360) of a Landsat-5 TM scene from the city of Vienna and its northern surroundings. In addition to evaluation of classification accuracy, the neural classifiers are analysed for generalization capability and stability of results. Best overall results (in terms of accuracy and convergence time) are provided by the MLP-1 classifier with weight elimination. It has a small number of parameters and requires no problem-specific system of initial weight values. Its in-sample classification error is 7.87% and its out-of-sample classification error is 10.24% for the problem at hand. Four classes of simulations serve to illustrate the properties of the classifier in general and the stability of the result with respect to control parameters, and on the training time, the gradient descent control term, initial parameter conditions, and different training and testing sets"
SUCHARITA GOPAL,The evolving landscape of big data analytics and ESG materiality mapping,"Raging hurricanes, devastating floods, sea-level rise, heatwaves, and other extreme weather conditions are now attributed to climate change. The authors propose that climate change poses a significant investment risk in terms of economic losses and societal disruptions such as migration, infectious diseases, and increasing vulnerability of exposure to more frequently recurring weather events. They discuss optimum utilization of big data and data analytics along with artificial intelligence to assess the materiality of these potential risks in portfolios. Further, they highlight emerging and established approaches through two case studies to highlight how the overall investment management community can benchmark its exposure, risk, and vulnerabilities, coupled with future impacts and building resiliency, across portfolio management and investments."
SUCHARITA GOPAL,A Neural Network Method for Efficient Vegetation Mapping,"This paper describes the application of a neural network method designed to improve the efficiency of map production from remote sensing data. Specifically, the ARTMAP neural network produces vegetation maps of the Sierra National Forest, in Northern California, using Landsat Thematic Mapper (TM) data. In addition to spectral values, the data set includes terrain and location information for each pixel. The maps produced by ARTMAP are of comparable accuracy to maps produced by a currently used method, which requires expert knowledge of the area as well as extensive manual editing. In fact, once field observations of vegetation classes had been collected for selected sites, ARTMAP took only a few hours to accomplish a mapping task that had previously taken many months. The ARTMAP network features fast on-line learning, so the system can be updated incrementally when new field observations arrive, without the need for retraining on the entire data set. In addition to maps that identify lifeform and Calveg species, ARTMAP produces confidence maps, which indicate where errors are most likely to occur and which can, therefore, be used to guide map editing."
SUCHARITA GOPAL,"A Neural Network Method for Land Use Change Classification, with Application to the Nile River Delta","Detecting and monitoring changes in conditions at the Earth's surface are essential for understanding human impact on the environment and for assessing the sustainability of development. In the next decade, NASA will gather high-resolution multi-spectral and multi-temporal data, which could be used for analyzing long-term changes, provided that available methods can keep pace with the accelerating flow of information. This paper introduces an automated technique for change identification, based on the ARTMAP neural network. This system overcomes some of the limitations of traditional change detection methods, and also produces a measure of confidence in classification accuracy. Landsat thematic mapper (TM) imagery of the Nile River delta provides a testbed for land use change classification methods. This dataset consists of a sequence of ten images acquired between 1984 and 1993 at various times of year. Field observations and photo interpretations have identified 358 sites as belonging to eight classes, three of which represent changes in land use over the ten-year period. Aparticular challenge posed by this database is the unequal representation of various land use categories: three classes, urban, agriculture in delta, and other, comprise 95% of pixels in labeled sites. A two-step sampling method enables unbiased training of the neural network system across sites."
SUCHARITA GOPAL,ARTMAP Neural Network Classification of Land Use Change,"The ability to detect and monitor changes in land use is essential for assessment of the sustainability of development. In the next decade, NASA will gather high-resolution multi-spectral and multi-temporal data, which could be used for detecting and monitoring long-term changes. Existing methods are insufficient for detecting subtle long-term changes from high-dimensional data. This project employs neural network architectures as alternatives to conventional systems for classifying changes in the status of agricultural lands from a sequence of satellite images. Landsat TM imagery of the Nile River delta provides a testbed for these land use change classification methods. A sequence often images was taken, at various times of year, from 1984 to 1993. Field data were collected during the summer of 1993 at88 sites in the Nile Delta and surrounding desert areas. Ground truth data for 231 additional sites were determined by expert site assessment at the Boston University Center for Remote Sensing. The field observations are grouped into classes including urban, reduced productivity agriculture, agriculture in delta, desert/coast reclamation, wetland reclamation, and agriculture in desert/coast. Reclamation classes represent land use changes. A particular challenge posed by this database is the unequal representation of various land use categories: urban and agriculture in delta pixels comprise the vast majority of the ground truth data available in the database. A new, two-step training data selection method was introduced to enable unbiased training of neural network systems on sites with unequal numbers of pixels. Data were successfully classified by using multi-date feature vectors containing data from all of the available satellite images as inputs to the neural network system."
SUCHARITA GOPAL,Mapping China’s foreign direct investment in the global energy sector,"As China’s current account surplus has grown, and the nation has liberalized its capital account, Chinese overseas foreign direct investment has increased significantly. In 2016, Chinese overseas foreign investment flows were more than $1.3 trillion, and outward investment from Hong Kong was $1.5 trillion, combining to $2.8 trillion in total—up a factor of 10 since 2005 and second only second to the United States in total outbound foreign direct investment. This paper examines the extent to which Chinese overseas foreign investment is significantly different from the United States and other OECD foreign investors in general, and with a specific focus on the electricity energy sector. After creating a unique spatial database that allows us to analyze both greenfield and merger & acquisition flows into the electricity generation sector, we examine the extent to which Chinese foreign investment differs from the other major players in electricity by energy source, level of technology, and level of pollution. We examine the geographical concentration of China’s FDI investment in Asia and Europe. We present initial results of our origin and destination analysis. China’s FDI trends have tremendous policy implications, both in geopolitical terms and for international trade and investment promotion policies."
SUCHARITA GOPAL,Characterizing urban landscapes using fuzzy sets,"Characterizing urban landscapes is important given the present and future projections of global population that favor urban growth. The definition of “urban” on a thematic map has proven to be problematic since urban areas are heterogeneous in terms of land use and land cover. Further, certain urban classes are inherently imprecise due to the difficulty in integrating various social and environmental inputs into a precise definition. Social components often include demographic patterns, transportation, building type and density while ecological components include soils, elevation, hydrology, climate, vegetation and tree cover. In this paper, we adopt a coupled human and natural system (CHANS) integrated scientific framework for characterizing urban landscapes. We implement the framework by adopting a fuzzy sets concept of “urban characterization” since fuzzy sets relate to classes of object with imprecise boundaries in which membership is a matter of degree. For dynamic mapping applications, user-defined classification schemes involving rules combining different social and ecological inputs can lead to a degree of quantification in class labeling varying from “highly urban” to “least urban”. A socio-economic perspective of urban may include threshold values for population and road network density while a more ecological perspective of urban may utilize the ratio of natural versus built area and percent forest cover. Threshold values are defined to derive the fuzzy rules of membership, in each case, and various combinations of rules offer a greater flexibility to characterize the many facets of the urban landscape. We illustrate the flexibility and utility of this fuzzy inference approach called the Fuzzy Urban Index for the Boston Metro region with five inputs and eighteen rules. The resulting classification map shows levels of fuzzy membership ranging from highly urban to least urban or rural in the Boston study region. We validate our approach using two experts assessing accuracy of the resulting fuzzy urban map. We discuss how our approach can be applied in other urban contexts with newly emerging descriptors of urban sustainability, urban ecology and urban metabolism."
SUCHARITA GOPAL,Characterizing the spatial determinants and prevention of malaria in Kenya,"The United Nations' Sustainable Development Goal 3 is to ensure health and well-being for all at all ages with a specific target to end malaria by 2030. Aligned with this goal, the primary objective of this study is to determine the effectiveness of utilizing local spatial variations to uncover the statistical relationships between malaria incidence rate and environmental and behavioral factors across the counties of Kenya. Two data sources are used-Kenya Demographic and Health Surveys of 2000, 2005, 2010, and 2015, and the national Malaria Indicator Survey of 2015. The spatial analysis shows clustering of counties with high malaria incidence rate, or hot spots, in the Lake Victoria region and the east coastal area around Mombasa; there are significant clusters of counties with low incidence rate, or cold spot areas in Nairobi. We apply an analysis technique, geographically weighted regression, that helps to better model how environmental and social determinants are related to malaria incidence rate while accounting for the confounding effects of spatial non-stationarity. Some general patterns persist over the four years of observation. We establish that variables including rainfall, proximity to water, vegetation, and population density, show differential impacts on the incidence of malaria in Kenya. The El-Nino-southern oscillation (ENSO) event in 2015 was significant in driving up malaria in the southern region of Lake Victoria compared with prior time-periods. The applied spatial multivariate clustering analysis indicates the significance of social and behavioral survey responses. This study can help build a better spatially explicit predictive model for malaria in Kenya capturing the role and spatial distribution of environmental, social, behavioral, and other characteristics of the households."
SUCHARITA GOPAL,Fueling global energy finance: the emergence of China in global energy investment,"Global financial investments in energy production and consumption are significant since all aspects of a country’s economic activity and development require energy resources. In this paper, we assess the investment trends in the global energy sector during, before, and after the financial crisis of 2008 using two data sources: (1) The Dealogic database providing cross-border mergers and acquisitions (M&As); and (2) The “fDi Intelligence fDi Markets” database providing Greenfield (GF) foreign direct investments (FDIs). We highlight the changing role of China and compare its M&A and GF FDI activities to those of the United States, Germany, UK, Japan, and others during this period. We analyze the investments along each segment of the energy supply chain of these countries to highlight the geographical origin and destination, sectoral distribution, and cross-border M&As and GF FDI activities. Our paper shows that while energy accounts for nearly 25% of all GF FDI, it only accounts for 4.82% of total M&A FDI activity in the period 1996–2016. China’s outbound FDI in the energy sector started its ascent around the time of the global recession and accelerated in the post-recession phase. In the energy sector, China’s outbound cross-border M&As are similar to the USA or UK, located mostly in the developed countries of the West, while their outbound GF investments are spread across many countries around the world. Also, China’s outbound energy M&As are concentrated in certain segments of the energy supply chain (extraction, and electricity generation) while their GF FDI covers other segments (electricity generation and power/pipeline transmission) of the energy supply chain."
SUCHARITA GOPAL,ART Neural Networks for Remote Sensing Image Analysis,"ART and ARTMAP neural networks for adaptive recognition and prediction have been applied to a variety of problems, including automatic mapping from remote sensing satellite measurements, parts design retrieval at the Boeing Company, medical database prediction, and robot vision. This paper features a self-contained introduction to ART and ARTMAP dynamics. An application of these networks to image processing is illustrated by means of a remote sensing example. The basic ART and ARTMAP networks feature winner-take-all (WTA) competitive coding, which groups inputs into discrete recognition categories. WTA coding in these networks enables fast learning, which allows the network to encode important rare cases but which may lead to inefficient category proliferation with noisy training inputs. This problem is partially solved by ART-EMAP, which use WTA coding for learning but distributed category representations for test-set prediction. Recently developed ART models (dART and dARTMAP) retain stable coding, recognition, and prediction, but allow arbitrarily distributed category representation during learning as well as performance."
SUCHARITA GOPAL,Art Neural Networks for Remote Sensing: Vegetation Classification from Landsat TM and Terrain Data,"A new methodology for automatic mapping from Landsat Thematic Mapper (TM) and terrain data, based on the fuzzy ARTMAP neural network, is developed. System capabilities are tested on a challenging remote sensing classification problem, using spectral and terrain features for vegetation classification in the Cleveland National Forest. After training at the pixel level, system performance is tested at the stand level, using sites not seen during training. Results are compared to those of maximum likelihood classifiers, as well as back propagation neural networks and K Nearest Neighbor algorithms. ARTMAP dynamics are fast, stable, and scalable, overcoming common limitations of back propagation, which did not give satisfactory performance. Best results are obtained using a hybrid system based on a convex combination of fuzzy ARTMAP and maximum likelihood predictions. A prototype remote sensing example introduces each aspect of data processing and fuzzy ARTMAP classification. The example shows how the network automatically constructs a minimal number of recognition categories to meet accuracy criteria. A voting strategy improves prediction and assigns confidence estimates by training the system several times on different orderings of an input set."
SUCHARITA GOPAL,A Neural Network Method for Mixture Estimation for Vegetation Mapping,"While most forest maps identify only the dominant vegetation class in delineated stands, individual stands are often better characterized by a mix of vegetation types. Many land management applications, including wildlife habitat studies, can benefit from knowledge of mixes. This paper examines various algorithms that use data from the Landsat Thematic Mapper (TM) satellite to estimate mixtures of vegetation types within forest stands. Included in the study are maximum likelihood classification and linear mixture models as well as a new methodology based on the ARTMAP neural network. Two paradigms are considered: classification methods, which describe stand-level vegetation mixtures as mosaics of pixels, each identified with its primary vegetation class; and mixture methods, which treat samples as blends of vegetation, even at the pixel level. Comparative analysis of these mixture estimation methods, tested on data from the Plumas National Forest, yields the following conclusions: (1) accurate estimates of proportions of hardwood and conifer cover within stands can be obtained, particularly when brush is not present in the understory; (2) ARTMAP outperforms statistical methods and linear mixture models in both the classification and the mixture paradigms; (3) topographic correction fails to improve mapping accuracy; and (4) the new ARTMAP mixture system produces the most accurate overall results. The Plumas data set has been made available to other researchers for further development of new mapping methods and comparison with the quantitative studies presented here, which establish initial benchmark standards."
SUCHARITA GOPAL,Connecting the dots: information visualization and text analysis of the Searchlight Project newsletters,"This report is the product of the Pardee Center’s work on the Searchlight:Visualization and Analysis of Trend Data project sponsored by the Rockefeller Foundation. Part of a larger effort to analyze and disseminate on-the-ground information about important societal trends as reported in a large number of regional newsletters developed in Asia, Africa and the Americas specifically for the Foundation, the Pardee Center developed sophisticated methods to systematically review, categorize, analyze, visualize, and draw conclusions from the information in the newsletters."
SUCHARITA GOPAL,Sub-Saharan Africa at a crossroads: a quantitative analysis of regional development,"Sub-Saharan Africa is at a crossroads of development. Despite a quarter of a century of economic reforms propagated by national policies and international financial agencies and institutions, sub-Saharan Africa is still lagging in development. In this paper, the authors adopt two techniques using both qualitative (e.g. governance) and quantitative factors (e.g., GDP) to examine regional patterns of development in sub-Saharan Africa. More specifically, they examine and analyze similarities and differences among the countries in this region using a multivariate statistical technique, Principal Component Analysis (PCA), and a unsupervised neural network called Kohonen’s Self-Organizing Map (SOM) to cluster levels of development. PCA serves as a tool for determining regional patterns while SOM is more useful for determining continental patterns in development. Both PCA and SOM results show a “developed” cluster in Southern Africa (South Africa, Namibia, Botswana, and Gabon). SOM exhibits a cluster of least developed countries in southern Western Africa and western Central Africa. The results demonstrate that the applied techniques are highly effective to compress multidimensional qualitative and quantitative data sets to extract relevant information about development from a policy perspective. Our analysis indicates the significance of governance variables in some clusters while a combination of variables explains other regional clusters. Zachary Tyler works for a consulting firm in Massachusetts that conducts program evaluations for energy efficiency programs, and he continues to work on statistical and geospatial analyses of human development issues. In 2010, he will receive a master’s degree in energy and environmental analysis from Boston University. Sucharita Gopal is Professor and Director of Graduate Studies in the Department of Geography and Environment and a member of the Cognitive & Neural Systems (CNS) Technology Lab at Boston University. She teaches and conducts research in geographical information systems (GIS), spatial analysis and modeling, and remote sensing for environmental and public health applications. Her recent research includes the development of a marin integrated decision analysis system (MIDAS) for Belize, Panama, and Massachusetts, and a post-disaster geospatial risk model for Haiti. This paper is part of the Africa 2060 Project, a Pardee Center program of research, publications, and symposia exploring African futures in various aspects related to development on continental and regional scales. For more information, visit www-staging.bu.edu/pardee/research/."
SUCHARITA GOPAL,"Analysis of land use and land cover changes through the lens of SDGs in Semarang, Indonesia","Land Use and Land Cover Changes (LULCC) are occurring rapidly around the globe, particularly in developing island nations. We use the lens of the United Nations’ Sustainable Development Goals (SDG) to determine potential policies to address LULCC due to increasing population, suburbia, and rubber plantations in Semarang, Indonesia between 2006 and 2015. Using remote sensing, overlay analysis, optimized hot spot analysis, expert validation, and Continuous Change Detection and Classification, we found that there was a spread of urban landscapes towards the southern and western portions of Semarang that had previously been occupied by forests, plantations, agriculture, and aquaculture. We also witnessed a transition in farming from agriculture to rubber plantations, a cash crop. The implications of this study show that these geospatial analyses and big data can be used to characterize the SDGs, the complex interplay of these goals, and potentially alleviate some of the conflicts between disparate SDGs. We recommend certain policies that can assist in preserving the terrestrial ecosystem of Semarang (SDG 15) while creating a sustainable city (SDG 11, SDG 9) and providing sufficient work for individuals (SDG 1) in a growing economy (SDG 8) while simultaneously maintaining a sufficient food supply (SDG 2)."
SUCHARITA GOPAL,The BosWash infrastructure biome and energy system succession,"The BosWash corridor is a megalopolis, or large urbanized region composed of interconnected transportation, infrastructure, physiography, and sociopolitical systems. Previous work has not considered the BosWash corridor as an integrated, holistic ecosystem. Building on the emerging field of infrastructure ecology, the region is conceptualized here as an infrastructure biome, and this concept is applied to the region’s energy transition to a post-fossil fueled heating sector, in analogy to ecosystem succession. In this conception, infrastructure systems are analogous to focal species. A case study for an energy succession from an aging natural gas infrastructure to a carbon-free heating sector is presented, in order to demonstrate the utility of the infrastructure biome framework to address climate and energy challenges facing BosWash communities. Natural gas is a dominant energy source that emits carbon dioxide when burned and methane when leaked along the process chain; therefore, a transition to electricity is widely seen as necessary toward reducing greenhouse gas emissions. Utilizing an infrastructure biome framework for energy policy, a regional gas transition plan akin to the Regional Greenhouse Gas Initiative is generated to harmonize natural gas transition within the BosWash infrastructure biome and resolve conflict arising from a siloed approach to infrastructure management at individual city and state levels. This work generates and utilizes the novel infrastructure biome concept to prescribe a regional energy policy for an element of infrastructure that has not previously been explored at the regional scale—natural gas."
SUCHARITA GOPAL,Spatial analysis of malaria incidence at the village level in areas with unstable transmission in Ethiopia,"BACKGROUND: Malaria is the leading cause of morbidity and mortality in Ethiopia, accounting for over five million cases and thousands of deaths annually. The risks of morbidity and mortality associated with malaria are characterized by spatial and temporal variation across the country. This study examines the spatial and temporal patterns of malaria transmission at the local level and implements a risk mapping tool to aid in monitoring and disease control activities. METHODS: In this study, we examine the global and local patterns of malaria distribution in 543 villages in East Shoa, central Ethiopia using individual-level morbidity data collected from six laboratory and treatment centers between September 2002 and August 2006. RESULTS: Statistical analysis of malaria incidence by sex, age, and village through time reveal the presence of significant spatio-temporal variations. Poisson regression analysis shows a decrease in malaria incidence with increasing age. A significant difference in the malaria incidence density ratio (IDRs) is detected in males but not in females. A significant decrease in the malaria IDRs with increasing age is captured by a quadratic model. Local spatial statistics reveals clustering or hot spots within a 5 and 10 km distance of most villages in the study area. In addition, there are temporal variations in malaria incidence. CONCLUSION: Malaria incidence varies according to gender and age, with males age 5 and above showing a statistically higher incidence. Significant local clustering of malaria incidence occurs between pairs of villages within 1–10 km distance lags. Malaria incidence was higher in 2002–2003 than in other periods of observation. Malaria hot spots are displayed as risk maps that are useful for monitoring and spatial targeting of prevention and control measures against the disease."
SUCHARITA GOPAL,Spatial Analysis of Malaria Incidence at the Village Level in Areas with Unstable Transmission in Ethiopia,"BACKGROUND. Malaria is the leading cause of morbidity and mortality in Ethiopia, accounting for over five million cases and thousands of deaths annually. The risks of morbidity and mortality associated with malaria are characterized by spatial and temporal variation across the country. This study examines the spatial and temporal patterns of malaria transmission at the local level and implements a risk mapping tool to aid in monitoring and disease control activities. METHODS. In this study, we examine the global and local patterns of malaria distribution in 543 villages in East Shoa, central Ethiopia using individual-level morbidity data collected from six laboratory and treatment centers between September 2002 and August 2006. RESULTS. Statistical analysis of malaria incidence by sex, age, and village through time reveal the presence of significant spatio-temporal variations. Poisson regression analysis shows a decrease in malaria incidence with increasing age. A significant difference in the malaria incidence density ratio (IDRs) is detected in males but not in females. A significant decrease in the malaria IDRs with increasing age is captured by a quadratic model. Local spatial statistics reveals clustering or hot spots within a 5 and 10 km distance of most villages in the study area. In addition, there are temporal variations in malaria incidence. CONCLUSION. Malaria incidence varies according to gender and age, with males age 5 and above showing a statistically higher incidence. Significant local clustering of malaria incidence occurs between pairs of villages within 1–10 km distance lags. Malaria incidence was higher in 2002–2003 than in other periods of observation. Malaria hot spots are displayed as risk maps that are useful for monitoring and spatial targeting of prevention and control measures against the disease."
SUCHARITA GOPAL,"Data and code for: Feedbacks among electric vehicle adoption, charging, and the cost and installation of rooftop solar","Identifying feedback loops in consumer behaviors is important to develop policies to accentuate desired behavior. Here, we use Granger causality to provide empirical evidence for feedback loops among four important components of a low-carbon economy. One loop includes the cost of installing rooftop solar (Cost) and the installation of rooftop solar (PV); this loop is likely generated by learning by doing and reductions in the levelized cost of electricity. The second includes the purchase of electric vehicles (EV) and the installation of rooftop solar that is likely created by environmental complementarity. Finally, we address whether installing charging stations enhances the purchase of electric vehicles and vice versa; surprisingly, there is no evidence for a causal relation in either direction. Together, these results suggest ways to modify existing policy in ways that could trigger the Cost ↔PV ↔EV feedback loops and accelerate the transition to carbon free technologies."
SUCHARITA GOPAL,Geographically weighted regression models in estimating median home prices in towns of Massachusetts based on an urban sustainability framework,"Housing is a key component of urban sustainability. The objective of this study was to assess the significance of key spatial determinants of median home price in towns in Massachusetts that impact sustainable growth. Our analysis investigates the presence or absence of spatial non-stationarity in the relationship between sustainable growth, measured in terms of the relationship between home values and various parameters including the amount of unprotected forest land, residential land, unemployment, education, vehicle ownership, accessibility to commuter rail stations, school district performance, and senior population. We use the standard geographically weighted regression (GWR) and Mixed GWR models to analyze the effects of spatial non-stationarity. Mixed GWR performed better than GWR in terms of Akaike Information Criterion (AIC) values. Our findings highlight the nature and spatial extent of the non-stationary vs. stationary qualities of key environmental and social determinants of median home price. Understanding the key determinants of housing values, such as valuation of green spaces, public school performance metrics, and proximity to public transport, enable towns to use different strategies of sustainable urban planning, while understanding urban housing determinants—such as unemployment and senior population—can help modify urban sustainable housing policies."
SUCHARITA GOPAL,"Leveraging big data and analytics to improve food, energy, and water system sustainability",
SUCHARITA GOPAL,Opioid mortality in the US: quantifying the direct and indirect impact of sociodemographic and socioeconomic factors,"This paper employs a spatial Durbin panel data model, an extension of the cross-sectional spatial Durbin model to a panel data framework, to quantify the impact of a set of sociodemographic and socioeconomic factors that influence opioid-related mortality in the US. The empirical model uses a pool of 49 US states over six years from 2014 to 2019, and a nearest-neighbor matrix that represents the topological structure between the states. Calculation of direct (own-state) and indirect (cross-state spillover) effects estimates is based on Bayesian estimation and inference reflecting a proper interpretation of the marginal effects for the model that involves spatial lags of the dependent and independent variables. The study provides evidence that opioid mortality depends not only on the characteristics of the state itself (direct effects), but also on those of nearby states (indirect effects). Direct effects are important, but externalities (spatial spillovers) are more important. The sociodemographic structure (age and race) of a state is important whereas economic distress of a state is less so, as indicated by the total impact estimates. The methodology and the research findings provide a useful template for future empirical work using other geographic locations or shifting interest to other epidemics."
SUCHARITA GOPAL,The deforestation and biodiversity risks of power plant projects in Southeast Asia: a big data spatial analytical framework,"Ecosystem destruction and biodiversity loss are now widespread, extremely rapid, and among the top global anthropogenic risks both in terms of likelihood and overall impact. Thorough impact evaluation of these environmental abuses—essential for conservation and future project planning—requires good analysis of local ecological and environmental data in addition to social and economic impacts. We characterized the deforestation and biodiversity impacts of energy investments in Southeast Asia using multiple geospatial data sources related to forest cover and loss data from 2000 to 2018, other landcover data, and the location, type, and characteristics of energy investments. This study paid particular attention to different types of power plants and financing sources. We identified critical buffer zones and forest structures impacted by these projects in accordance with IUCN criteria and spatial ecology. The paper introduces a novel, replicable analytical framework that goes beyond earlier studies in which all forests are treated as equivalent. It characterizes forests based on spatial morphological structures such as core forest, edges, islands, and bridges, allowing for a more nuanced understanding of deforestation and its impacts on biodiversity. Preliminary findings suggest that projects financed by Chinese development banks pose different risks compared to non-Chinese financing. The study also reveals significant differences in biodiversity impacts based on the type of energy source, be it coal or hydro. The study offers critical insights into the trade-offs between energy development and biodiversity conservation. It provides actionable metrics and strategies for policymakers, conservationists, and development banks to prioritize forest and habitat preservation in Southeast Asia and globally."
MARIA TROJANOWSKA,Autocrine Transforming Growth Factor β Signaling Regulates Extracellular Signal-regulated Kinase 1/2 Phosphorylation via Modulation of Protein Phosphatase 2A Expression in Scleroderma Fibroblasts,"BACKGROUND. During scleroderma (SSc) pathogenesis, fibroblasts acquire an activated phenotype characterized by enhanced production of extracellular matrix (ECM) and constitutive activation of several major signaling pathways including extracellular signal-related kinase (ERK1/2). Several studies have addressed the role of ERK1/2 in SSc fibrosis however the mechanism of its prolonged activation in SSc fibroblasts is still unknown. Protein phosphatase 2A (PP2A) is a key serine threonine phosphatase responsible for dephosphorylation of a wide array of signaling molecules. Recently published microarray data from cultured SSc fibroblasts suggests that the catalytic subunit (C-subunit) of PP2A is downregulated in SSc. In this study we examined the role and regulation of PP2A in SSc fibroblasts in the context of ERK1/2 phosphorylation and matrix production. RESULTS. We show for the first time that PP2A mRNA and protein expression are significantly reduced in SSc fibroblasts and correlate with an increase in ERK1/2 phosphorylation and collagen expression. Furthermore, transforming growth factor β (TGFβ), a major profibrotic cytokine implicated in SSc fibrosis, downregulates PP2A expression in healthy fibroblasts. PP2A-specific small interfering RNA (siRNA) was utilized to confirm the role of PP2A in ERK1/2 dephosphorylation in dermal fibroblasts. Accordingly, blockade of autocrine TGFβ signaling in SSc fibroblasts using soluble recombinant TGFβ receptor II (SRII) restored PP2A levels and decreased ERK1/2 phosphorylation and collagen expression. In addition, we observed that inhibition of ERK1/2 in SSc fibroblasts increased PP2A expression suggesting that ERK1/2 phosphorylation also contributes to maintaining low levels of PP2A, leading to an even further amplification of ERK1/2 phosphorylation. CONCLUSIONS. Taken together, these studies suggest that decreased PP2A levels in SSc is a result of constitutively activated autocrine TGFβ signaling and could contribute to enhanced phosphorylation of ERK1/2 and matrix production in SSc fibroblasts."
MICHAEL STEIN,Clinicopathological evaluation of chronic traumatic encephalopathy in players of American football,"IMPORTANCE: Players of American football may be at increased risk of long-term neurological conditions, particularly chronic traumatic encephalopathy (CTE). OBJECTIVE: To determine the neuropathological and clinical features of deceased football players with CTE. DESIGN, SETTING, AND PARTICIPANTS: Case series of 202 football players whose brains were donated for research. Neuropathological evaluations and retrospective telephone clinical assessments (including head trauma history) with informants were performed blinded. Online questionnaires ascertained athletic and military history. EXPOSURES: Participation in American football at any level of play. MAIN OUTCOMES AND MEASURES: Neuropathological diagnoses of neurodegenerative diseases, including CTE, based on defined diagnostic criteria; CTE neuropathological severity (stages I to IV or dichotomized into mild [stages I and II] and severe [stages III and IV]); informant-reported athletic history and, for players who died in 2014 or later, clinical presentation, including behavior, mood, and cognitive symptoms and dementia. RESULTS: Among 202 deceased former football players (median age at death, 66 years [interquartile range, 47-76 years]), CTE was neuropathologically diagnosed in 177 players (87%; median age at death, 67 years [interquartile range, 52-77 years]; mean years of football participation, 15.1 [SD, 5.2]), including 0 of 2 pre–high school, 3 of 14 high school (21%), 48 of 53 college (91%), 9 of 14 semiprofessional (64%), 7 of 8 Canadian Football League (88%), and 110 of 111 National Football League (99%) players. Neuropathological severity of CTE was distributed across the highest level of play, with all 3 former high school players having mild pathology and the majority of former college (27 [56%]), semiprofessional (5 [56%]), and professional (101 [86%]) players having severe pathology. Among 27 participants with mild CTE pathology, 26 (96%) had behavioral or mood symptoms or both, 23 (85%) had cognitive symptoms, and 9 (33%) had signs of dementia. Among 84 participants with severe CTE pathology, 75 (89%) had behavioral or mood symptoms or both, 80 (95%) had cognitive symptoms, and 71 (85%) had signs of dementia. CONCLUSIONS AND RELEVANCE: In a convenience sample of deceased football players who donated their brains for research, a high proportion had neuropathological evidence of CTE, suggesting that CTE may be related to prior participation in football."
MICHAEL STEIN,"The ISCIP Analyst, Volume VII, Issue 11",
MICHAEL STEIN,"The ISCIP Analyst, Volume VIII, Issue 3",
MICHAEL STEIN,"The ISCIP Analyst, Volume VIII, Issue 8",
MICHAEL STEIN,"The ISCIP Analyst, Volume VIII, Issue 2",
MICHAEL STEIN,"The ISCIP Analyst, Volume VIII, Issue 4",
MICHAEL STEIN,"The ISCIP Analyst, Volume VIII, Issue 7",
MICHAEL STEIN,"The ISCIP Analyst, Volume VII, Issue 14",
MICHAEL STEIN,"The ISCIP Analyst, Volume VIII, Issue 6",
MICHAEL STEIN,"The ISCIP Analyst, Volume VII, Issue 10",
MICHAEL STEIN,"The ISCIP Analyst, Volume VIII, Issue 5",
MICHAEL STEIN,"The ISCIP Analyst, Volume VII, Issue 18",
MICHAEL STEIN,"The ISCIP Analyst, Volume VII, Issue 15",
MICHAEL STEIN,"The ISCIP Analyst, Volume VII, Issue 20",
MICHAEL STEIN,"The ISCIP Analyst, Volume VII, Issue 19",
MICHAEL STEIN,"The ISCIP Analyst, Volume VII, Issue 17",
MICHAEL STEIN,"The ISCIP Analyst, Volume VII, Issue 16",
MICHAEL STEIN,"The ISCIP Analyst, Volume VII, Issue 12",
LINDA H. DOERRER,Imposing high-symmetry and tuneable geometry on lanthanide centres with chelating Pt and Pd metalloligands,"Exploitation of HSAB preferences allows for high-yield, one-pot syntheses of lanthanide complexes chelated by two Pd or Pt metalloligands, [MII(SAc)4]2− (SAc− = thioacetate, M = Pd, Pt). The resulting complexes with 8 oxygen donors surrounding the lanthanides can be isolated in crystallographically tetragonal environments as either [NEt4]+ (space group: P4/mcc) or [PPh4]+ (space group: P4/n) salts. In the case of M = Pt, the complete series of lanthanide complexes has been structurally characterized as the [NEt4]+ salts (except for Ln = Pm), while the [PPh4]+ salts have been structurally characterized for Ln = Gd–Er, Y. For M = Pd, selected lanthanide complexes have been structurally characterized as both salts. The only significant structural difference between salts of the two counter ions is the resulting twist angle connecting tetragonal prismatic and tetragonal anti-prismatic configurations, with the [PPh4]+ salts approaching ideal D4d symmetry very closely (φ = 44.52–44.61°) while the [NEt4]+ salts exhibit intermediate twist angles in the interval φ = 17.28–27.41°, the twist increasing as the complete 4f series is traversed. Static magnetic properties for the latter half of the lanthanide series are found to agree well in the high temperature limit with the expected Curie behavior. Perpendicular and parallel mode EPR spectroscopy on randomly oriented powder samples and single crystals of the Gd complexes with respectively Pd- and Pt-based metalloligands demonstrate the nature of the platinum metal to strongly affect the spectra. Consistent parametrization of all of the EPR spectra reveals the main difference to stem from a large difference in the magnitude of the leading axial term, B02, this being almost four times larger for the Pt-based complexes as compared to the Pd analogues, indicating a direct Pt(5dz2)–Ln interaction and an arguable coordination number of 10 rather than 8. The parametrization of the EPR spectra also confirms that off-diagonal operators are associated with non-zero parameters for the [NEt4]+ salts, while only contributing minimally for the [PPh4]+ salts in which lanthanide coordination approximates D4d point group symmetry closely."
LINDA H. DOERRER,Effect of lattice mismatch on film morphology of the quasi-one dimensional conductor K0.3MoO3,"High quality epitaxial thin films of the quasi-one dimensional conductor K0.3MoO3 have been successfully grown on SrTiO3(100), SrTiO3(110), and SrTiO3(510) substrates via pulsed laser deposition. Scanning electron microscopy revealed quasi-one dimensional rod-shaped structures parallel to the substrate surface, and the crystal structure was verified by using X-ray diffraction. The temperature dependence of the resistivity for the K0.3MoO3 thin films demonstrates a metal-to-semiconductor transition at about 180 K. Highly anisotropic resistivity was also observed for films grown on SrTiO3(510)."
LINDA H. DOERRER,Chemical tunnel-splitting-engineering in a dysprosium-based molecular nanomagnet,"Total control over the electronic spin relaxation in molecular nanomagnets is the ultimate goal in the design of new molecules with evermore realizable applications in spin-based devices. For single-ion lanthanide systems, with strong spin–orbit coupling, the potential applications are linked to the energetic structure of the crystal field levels and quantum tunneling within the ground state. Structural engineering of the timescale of these tunneling events via appropriate design of crystal fields represents a fundamental challenge for the synthetic chemist, since tunnel splittings are expected to be suppressed by crystal field environments with sufficiently high-order symmetry. Here, we report the long missing study of the effect of a non-linear (C₄) to pseudo-linear (D₄𝒅) change in crystal field symmetry in an otherwise chemically unaltered dysprosium complex. From a purely experimental study of crystal field levels and electronic spin dynamics at milliKelvin temperatures, we demonstrate the ensuing threefold reduction of the tunnel splitting."
LINDA H. DOERRER,"Bromido(2,2':6',2''-terpyridine)platinum(II) dibromidoaurate(I) dimethyl sulfoxide solvate.","The crystal structure of the title compound, [PtBr(C(15)H(11)N(3))][AuBr(2)]·(CH(3))(2)SO, exhibits infinite chains of {PtAuPt}(∞) metallophilic inter-actions along the b axis. Two cations and one anion stack in a trimer with a unique Pt⋯Au distance of 3.3361 (5) Å and Pt⋯Pt contacts of 3.4335 (6) Å. The remaining [AuBr(2)](-) anion forms no close contacts."
LINDA H. DOERRER,"Bromido(2,2′:6′,2′′-terpyridine)platinum(II) dibromidoaurate(I) Dimethyl Sulfoxide Solvate","The crystal structure of the title compound, [PtBr(C15H11N3)][AuBr2]·(CH3)2SO, exhibits infinite chains of {PtAuPt}∞ metallophilic interactions along the b axis. Two cations and one anion stack in a trimer with a unique Pt·Au distance of 3.3361 (5) Å and Pt·Pt contacts of 3.4335 (6) Å. The remaining [AuBr2]− anion forms no close contacts."
ANNA DEVOR,Phasor analysis of NADH FLIM identifies pharmacological disruptions to mitochondrial metabolic processes in the rodent cerebral cortex,"Investigating cerebral metabolism in vivo at a microscopic level is essential for understanding brain function and its pathological alterations. The intricate signaling and metabolic dynamics between neurons, glia, and microvasculature requires much more detailed understanding to better comprehend the mechanisms governing brain function and its disease-related changes. We recently demonstrated that pharmacologically-induced alterations to different steps of cerebral metabolism can be distinguished utilizing 2-photon fluorescence lifetime imaging of endogenous reduced nicotinamide adenine dinucleotide (NADH) fluorescence in vivo. Here, we evaluate the ability of the phasor analysis method to identify these pharmacological metabolic alterations and compare the method's performance with more conventional nonlinear curve-fitting analysis. Visualization of phasor data, both at the fundamental laser repetition frequency and its second harmonic, enables resolution of pharmacologically-induced alterations to mitochondrial metabolic processes from baseline cerebral metabolism. Compared to our previous classification models based on nonlinear curve-fitting, phasor-based models required fewer parameters and yielded comparable or improved classification accuracy. Fluorescence lifetime imaging of NADH and phasor analysis shows utility for detecting metabolic alterations and will lead to a deeper understanding of cerebral energetics and its pathological changes."
ANNA DEVOR,Two-photon high-resolution measurement of partial pressure of oxygen in cerebral vasculature and tissue,"Measurements of oxygen partial pressure (pO(2)) with high temporal and spatial resolution in three dimensions is crucial for understanding oxygen delivery and consumption in normal and diseased brain. Among existing pO(2) measurement methods, phosphorescence quenching is optimally suited for the task. However, previous attempts to couple phosphorescence with two-photon laser scanning microscopy have faced substantial difficulties because of extremely low two-photon absorption cross-sections of conventional phosphorescent probes. Here we report to our knowledge the first practical in vivo two-photon high-resolution pO(2) measurements in small rodents' cortical microvasculature and tissue, made possible by combining an optimized imaging system with a two-photon-enhanced phosphorescent nanoprobe. The method features a measurement depth of up to 250 microm, sub-second temporal resolution and requires low probe concentration. The properties of the probe allowed for direct high-resolution measurement of cortical extravascular (tissue) pO(2), opening many possibilities for functional metabolic brain studies."
ANNA DEVOR,"Modeling of cerebral oxygen transport based on in vivo microscopic imaging of microvascular network structure, blood flow, and oxygenation","Oxygen is delivered to brain tissue by a dense network of microvessels, which actively control cerebral blood flow (CBF) through vasodilation and contraction in response to changing levels of neural activity. Understanding these network-level processes is immediately relevant for (1) interpretation of functional Magnetic Resonance Imaging (fMRI) signals, and (2) investigation of neurological diseases in which a deterioration of neurovascular and neuro-metabolic physiology contributes to motor and cognitive decline. Experimental data on the structure, flow and oxygen levels of microvascular networks are needed, together with theoretical methods to integrate this information and predict physiologically relevant properties that are not directly measurable. Recent progress in optical imaging technologies for high-resolution in vivo measurement of the cerebral microvascular architecture, blood flow, and oxygenation enables construction of detailed computational models of cerebral hemodynamics and oxygen transport based on realistic three-dimensional microvascular networks. In this article, we review state-of-the-art optical microscopy technologies for quantitative in vivo imaging of cerebral microvascular structure, blood flow and oxygenation, and theoretical methods that utilize such data to generate spatially resolved models for blood flow and oxygen transport. These ""bottom-up"" models are essential for the understanding of the processes governing brain oxygenation in normal and disease states and for eventual translation of the lessons learned from animal studies to humans."
ANNA DEVOR,Awake mouse imaging: from two-photon microscopy to blood oxygen level-dependent functional magnetic resonance imaging,"BACKGROUND: Functional magnetic resonance imaging (fMRI) in awake behaving mice is well positioned to bridge the detailed cellular-level view of brain activity, which has become available owing to recent advances in microscopic optical imaging and genetics, to the macroscopic scale of human noninvasive observables. However, though microscopic (e.g., two-photon imaging) studies in behaving mice have become a reality in many laboratories, awake mouse fMRI remains a challenge. Owing to variability in behavior among animals, performing all types of measurements within the same subject is highly desirable and can lead to higher scientific rigor. METHODS: We demonstrated blood oxygenation level-dependent fMRI in awake mice implanted with long-term cranial windows that allowed optical access for microscopic imaging modalities and optogenetic stimulation. We started with two-photon imaging of single-vessel diameter changes (n = 1). Next, we implemented intrinsic optical imaging of blood oxygenation and flow combined with laser speckle imaging of blood flow obtaining a mesoscopic picture of the hemodynamic response (n = 16). Then we obtained corresponding blood oxygenation level-dependent fMRI data (n = 5). All measurements could be performed in the same mice in response to identical sensory and optogenetic stimuli. RESULTS: The cranial window did not deteriorate the quality of fMRI and allowed alternation between imaging modalities in each subject. CONCLUSIONS: This report provides a proof of feasibility for multiscale imaging approaches in awake mice. In the future, this protocol could be extended to include complex cognitive behaviors translatable to humans, such as sensory discrimination or attention."
ANNA DEVOR,Dependence of the MR signal on the magnetic susceptibility of blood studied with models based on real microvascular networks,"PURPOSE: The primary goal of this study was to estimate the value of beta , the exponent in the power law relating changes of the transverse relaxation rate and intra-extravascular local magnetic susceptibility differences as Delta R 2 * proportional, variant ( Delta chi ) beta. The secondary objective was to evaluate any differences that might exist in the value of beta obtained using a deoxyhemoglobin-weighted Delta chi distribution versus a constant Delta chi distribution assumed in earlier computations. The third objective was to estimate the value of beta that is relevant for methods based on susceptibility contrast agents with a concentration of Delta chi higher than that used for BOLD fMRI calculations. METHODS: Our recently developed model of real microvascular anatomical networks is used to extend the original simplified Monte-Carlo simulations to compute beta from the first principles. RESULTS: Our results show that beta = 1 for most BOLD fMRI measurements of real vascular networks, as opposed to earlier predictions of beta = 1 .5 using uniform Delta chi distributions. For perfusion or fMRI methods based on contrast agents, which generate larger values for Delta chi , beta = 1 for B 0 </= 9.4 T, whereas at 14 T beta can drop below 1 and the variation across subjects is large, indicating that a lower concentration of contrast agent with a lower value of Delta chi is desired for experiments at high B0. CONCLUSION: These results improve our understanding of the relationship between R2 (*) and the underlying microvascular properties. The findings will help to infer the cerebral metabolic rate of oxygen and cerebral blood volume from BOLD and perfusion MRI, respectively."
ANNA DEVOR,Development of a beam propagation method to simulate the point spread function degradation in scattering media,"Scattering is one of the main issues that limit the imaging depth in deep tissue optical imaging. To characterize the role of scattering, we have developed a forward model based on the beam propagation method and established the link between the macroscopic optical properties of the media and the statistical parameters of the phase masks applied to the wavefront. Using this model, we have analyzed the degradation of the point-spread function of the illumination beam in the transition regime from ballistic to diffusive light transport. Our method provides a wave-optic simulation toolkit to analyze the effects of scattering on image quality degradation in scanning microscopy. Our open-source implementation is available at https://github.com/BUNPC/Beam-Propagation-Method."
ANNA DEVOR,Correlation structure in micro-ECoG recordings is described by spatially coherent components,"Electrocorticography (ECoG) is becoming more prevalent due to improvements in fabrication and recording technology as well as its ease of implantation compared to intracortical electrophysiology, larger cortical coverage, and potential advantages for use in long term chronic implantation. Given the flexibility in the design of ECoG grids, which is only increasing, it remains an open question what geometry of the electrodes is optimal for an application. Conductive polymer, PEDOT:PSS, coated microelectrodes have an advantage that they can be made very small without losing low impedance. This makes them suitable for evaluating the required granularity of ECoG recording in humans and experimental animals. We used two-dimensional (2D) micro-ECoG grids to record intra-operatively in humans and during acute implantations in mouse with separation distance between neighboring electrodes (i.e., pitch) of 0.4 mm and 0.2/0.25 mm respectively. To assess the spatial properties of the signals, we used the average correlation between electrodes as a function of the pitch. In agreement with prior studies, we find a strong frequency dependence in the spatial scale of correlation. By applying independent component analysis (ICA), we find that the spatial pattern of correlation is largely due to contributions from multiple spatially extended, time-locked sources present at any given time. Our analysis indicates the presence of spatially structured activity down to the sub-millimeter spatial scale in ECoG despite the effects of volume conduction, justifying the use of dense micro-ECoG grids."
ANNA DEVOR,Cell type specificity of neurovascular coupling in cerebral cortex,"Identification of the cellular players and molecular messengers that communicate neuronal activity to the vasculature driving cerebral hemodynamics is important for (1) the basic understanding of cerebrovascular regulation and (2) interpretation of functional Magnetic Resonance Imaging (fMRI) signals. Using a combination of optogenetic stimulation and 2-photon imaging in mice, we demonstrate that selective activation of cortical excitation and inhibition elicits distinct vascular responses and identify the vasoconstrictive mechanism as Neuropeptide Y (NPY) acting on Y1 receptors. The latter implies that task-related negative Blood Oxygenation Level Dependent (BOLD) fMRI signals in the cerebral cortex under normal physiological conditions may be mainly driven by the NPY-positive inhibitory neurons. Further, the NPY-Y1 pathway may offer a potential therapeutic target in cerebrovascular disease."
ANNA DEVOR,More homogeneous capillary flow and oxygenation in deeper cortical layers correlate with increased oxygen extraction,"Our understanding of how capillary blood flow and oxygen distribute across cortical layers to meet the local metabolic demand is incomplete. We addressed this question by using two-photon imaging of resting-state microvascular oxygen partial pressure (PO2) and flow in the whisker barrel cortex in awake mice. Our measurements in layers I-V show that the capillary red-blood-cell flux and oxygenation heterogeneity, and the intracapillary resistance to oxygen delivery, all decrease with depth, reaching a minimum around layer IV, while the depth-dependent oxygen extraction fraction is increased in layer IV, where oxygen demand is presumably the highest. Our findings suggest that more homogeneous distribution of the physiological observables relevant to oxygen transport to tissue is an important part of the microvascular network adaptation to local brain metabolism. These results will inform the biophysical models of layer-specific cerebral oxygen delivery and consumption and improve our understanding of the diseases that affect cerebral microcirculation."
ANNA DEVOR,Extracting individual neural activity recorded through splayed optical microfibers,"Previously introduced bundles of hundreds or thousands of microfibers have the potential to extend optical access to deep brain regions, sampling fluorescence activity throughout a three-dimensional volume. Each fiber has a small diameter (8  μm) and follows a path of least resistance, splaying during insertion. By superimposing the fiber sensitivity profile for each fiber, we model the interface properties for a simulated neural population. Our modeling results suggest that for small (&lt;200) bundles of fibers, each fiber will collect fluorescence from a small number of nonoverlapping neurons near the fiber apertures. As the number of fibers increases, the bundle delivers more uniform excitation power to the region, moving to a regime where fibers collect fluorescence from more neurons and there is greater overlap between neighboring fibers. Under these conditions, it becomes feasible to apply source separation to extract individual neural contributions. In addition, we demonstrate a source separation technique particularly suited to the interface. Our modeling helps establish performance expectations for this interface and provides a framework for estimating neural contributions under a range of conditions."
ANNA DEVOR,Spatially resolved estimation of metabolic oxygen consumption from optical measurements in cortex.,"Significance: The cerebral metabolic rate of oxygen ( CMRO 2 ) is an important indicator of brain function and pathology. Knowledge about its magnitude is also required for proper interpretation of the blood oxygenation level-dependent (BOLD) signal measured with functional MRI. Despite the need for estimating CMRO 2 , no gold standard exists. Traditionally, the estimation of CMRO 2 has been pursued with somewhat indirect approaches combining several different types of measurements with mathematical modeling of the underlying physiological processes. The recent ability to measure the level of oxygen ( pO 2 ) in cortex with two-photon resolution in in vivo conditions has provided a more direct way for estimating CMRO 2 , but has so far only been used to estimate the average CMRO 2 close to cortical penetrating arterioles in rats. Aim: The aim of this study was to propose a method to provide spatial maps of CMRO 2 based on two-photon pO 2 measurements. Approach: The method has two key steps. First, the pO 2 maps are spatially smoothed to reduce the effects of noise in the measurements. Next, the Laplace operator (a double spatial derivative) in two spatial dimensions is applied on the smoothed pO 2 maps to obtain spatially resolved CMRO 2 estimates. Result: The smoothing introduces a bias, and a balance must be found where the effects of the noise are sufficiently reduced without introducing too much bias. In this model-based study, we explored this balance using synthetic model-based data, that is, data where the spatial maps of CMRO 2 were preset and thus known. The corresponding pO 2 maps were found by solving the Poisson equation, which relates CMRO 2 and pO 2 . MATLAB code for using the method is provided. Conclusion: Through this model-based study, we propose a method for estimating CMRO 2 with high spatial resolution based on measurements of pO 2 in cerebral cortex."
ANNA DEVOR,Understanding the genetic determinants of the brain with MOSTest,"Regional brain morphology has a complex genetic architecture, consisting of many common polymorphisms with small individual effects. This has proven challenging for genome-wide association studies (GWAS). Due to the distributed nature of genetic signal across brain regions, multivariate analysis of regional measures may enhance discovery of genetic variants. Current multivariate approaches to GWAS are ill-suited for complex, large-scale data of this kind. Here, we introduce the Multivariate Omnibus Statistical Test (MOSTest), with an efficient computational design enabling rapid and reliable inference, and apply it to 171 regional brain morphology measures from 26,502 UK Biobank participants. At the conventional genome-wide significance threshold of α = 5 × 10-8, MOSTest identifies 347 genomic loci associated with regional brain morphology, more than any previous study, improving upon the discovery of established GWAS approaches more than threefold. Our findings implicate more than 5% of all protein-coding genes and provide evidence for gene sets involved in neuron development and differentiation."
ANNA DEVOR,Chronic cranial windows for long term multimodal neurovascular imaging in mice,"Chronic cranial windows allow for longitudinal brain imaging experiments in awake, behaving mice. Different imaging technologies have their unique advantages and combining multiple imaging modalities offers measurements of a wide spectrum of neuronal, glial, vascular, and metabolic parameters needed for comprehensive investigation of physiological and pathophysiological mechanisms. Here, we detail a suite of surgical techniques for installation of different cranial windows targeted for specific imaging technologies and their combination. Following these techniques and practices will yield higher experimental success and reproducibility of results."
ANNA DEVOR,Author Correction: Understanding the genetic determinants of the brain with MOSTest,An amendment to this paper has been published and can be accessed via a link at the top of the paper.
ANNA DEVOR,Large arteriolar component of oxygen delivery implies a safe margin of oxygen supply to cerebral tissue,"What is the organization of cerebral microvascular oxygenation and morphology that allows adequate tissue oxygenation at different activity levels? We address this question in the mouse cerebral cortex using microscopic imaging of intravascular O2 partial pressure and blood flow combined with numerical modelling. Here we show that parenchymal arterioles are responsible for 50% of the extracted O2 at baseline activity, and the majority of the remaining O2 exchange takes place within the first few capillary branches. Most capillaries release little O2 at baseline acting as an O2 reserve that is recruited during increased neuronal activity or decreased blood flow. Our results challenge the common perception that capillaries are the major site of O2 delivery to cerebral tissue. The understanding of oxygenation distribution along arterio-capillary paths may have profound implications for the interpretation of blood-oxygen-level dependent (BOLD) contrast in functional magnetic resonance imaging and for evaluating microvascular O2 delivery capacity to support cerebral tissue in disease."
ANNA DEVOR,Efficient non-degenerate two-photon excitation for fluorescence microscopy,"Non-degenerate two-photon excitation (ND-TPE) has been explored in two-photon excitation microscopy. However, a systematic study of the efficiency of ND-TPE to guide the selection of fluorophore excitation wavelengths is missing. We measured the relative non-degenerate two-photon absorption cross-section (ND-TPACS) of several commonly used fluorophores (two fluorescent proteins and three small-molecule dyes) and generated 2-dimensional ND-TPACS spectra. We observed that the shape of a ND-TPACS spectrum follows that of the corresponding degenerate two-photon absorption cross-section (D-TPACS) spectrum, but is higher in magnitude. We found that the observed enhancements are higher than theoretical predictions."
ANNA DEVOR,Deep 2-photon imaging and artifact-free optogenetics through transparent graphene microelectrode arrays,"Recent advances in optical technologies such as multi-photon microscopy and optogenetics have revolutionized our ability to record and manipulate neuronal activity. Combining optical techniques with electrical recordings is of critical importance to connect the large body of neuroscience knowledge obtained from animal models to human studies mainly relying on electrophysiological recordings of brain-scale activity. However, integration of optical modalities with electrical recordings is challenging due to generation of light-induced artifacts. Here we report a transparent graphene microelectrode technology that eliminates light-induced artifacts to enable crosstalk-free integration of 2-photon microscopy, optogenetic stimulation, and cortical recordings in the same in vivo experiment. We achieve fabrication of crack- and residue-free graphene electrode surfaces yielding high optical transmittance for 2-photon imaging down to ~ 1 mm below the cortical surface. Transparent graphene microelectrode technology offers a practical pathway to investigate neuronal activity over multiple spatial scales extending from single neurons to large neuronal populations."
ANNA DEVOR,Neurovascular Network Explorer 2.0: a database of 2-photon single-vessel diameter measurements from mouse SI cortex in response to optogenetic stimulation,
DAVID A MAYERS,BRCA mutational status shapes the stromal microenvironment of pancreatic cancer linking clusterin expression in cancer associated fibroblasts with HSF1 signaling,"Tumors initiate by mutations in cancer cells, and progress through interactions of the cancer cells with non-malignant cells of the tumor microenvironment. Major players in the tumor microenvironment are cancer-associated fibroblasts (CAFs), which support tumor malignancy, and comprise up to 90% of the tumor mass in pancreatic cancer. CAFs are transcriptionally rewired by cancer cells. Whether this rewiring is differentially affected by different mutations in cancer cells is largely unknown. Here we address this question by dissecting the stromal landscape of BRCA-mutated and BRCA Wild-type pancreatic ductal adenocarcinoma. We comprehensively analyze pancreatic cancer samples from 42 patients, revealing different CAF subtype compositions in germline BRCA-mutated vs. BRCA Wild-type tumors. In particular, we detect an increase in a subset of immune-regulatory clusterin-positive CAFs in BRCA-mutated tumors. Using cancer organoids and mouse models we show that this process is mediated through activation of heat-shock factor 1, the transcriptional regulator of clusterin. Our findings unravel a dimension of stromal heterogeneity influenced by germline mutations in cancer cells, with direct implications for clinical research."
DAVID A MAYERS,Heat Shock Factor 1-dependent extracellular matrix remodeling mediates the transition from chronic intestinal inflammation to colon cancer,"In the colon, long-term exposure to chronic inflammation drives colitis-associated colon cancer (CAC) in patients with inflammatory bowel disease. While the causal and clinical links are well established, molecular understanding of how chronic inflammation leads to the development of colon cancer is lacking. Here we deconstruct the evolving microenvironment of CAC by measuring proteomic changes and extracellular matrix (ECM) organization over time in a mouse model of CAC. We detect early changes in ECM structure and composition, and report a crucial role for the transcriptional regulator heat shock factor 1 (HSF1) in orchestrating these events. Loss of HSF1 abrogates ECM assembly by colon fibroblasts in cell-culture, prevents inflammation-induced ECM remodeling in mice and inhibits progression to CAC. Establishing relevance to human disease, we find high activation of stromal HSF1 in CAC patients, and detect the HSF1-dependent proteomic ECM signature in human colorectal cancer. Thus, HSF1-dependent ECM remodeling plays a crucial role in mediating inflammation-driven colon cancer."
DAVID A MAYERS,The long-baseline neutrino experiment: exploring fundamental symmetries of the universe,"The preponderance of matter over antimatter in the early Universe, the dynamics of the supernova bursts that produced the heavy elements necessary for life and whether protons eventually decay --- these mysteries at the forefront of particle physics and astrophysics are key to understanding the early evolution of our Universe, its current state and its eventual fate. The Long-Baseline Neutrino Experiment (LBNE) represents an extensively developed plan for a world-class experiment dedicated to addressing these questions. LBNE is conceived around three central components: (1) a new, high-intensity neutrino source generated from a megawatt-class proton accelerator at Fermi National Accelerator Laboratory, (2) a near neutrino detector just downstream of the source, and (3) a massive liquid argon time-projection chamber deployed as a far detector deep underground at the Sanford Underground Research Facility. This facility, located at the site of the former Homestake Mine in Lead, South Dakota, is approximately 1,300 km from the neutrino source at Fermilab -- a distance (baseline) that delivers optimal sensitivity to neutrino charge-parity symmetry violation and mass ordering effects. This ambitious yet cost-effective design incorporates scalability and flexibility and can accommodate a variety of upgrades and contributions. With its exceptional combination of experimental configuration, technical capabilities, and potential for transformative discoveries, LBNE promises to be a vital facility for the field of particle physics worldwide, providing physicists from around the globe with opportunities to collaborate in a twenty to thirty year program of exciting science. In this document we provide a comprehensive overview of LBNE's scientific objectives, its place in the landscape of neutrino physics worldwide, the technologies it will incorporate and the capabilities it will possess."
JONATHAN I LEVY,Evaluation of the Public Health Impacts of Traffic Congestion: A Health Risk Assessment,"BACKGROUND: Traffic congestion is a significant issue in urban areas in the United States and around the world. Previous analyses have estimated the economic costs of congestion, related to fuel and time wasted, but few have quantified the public health impacts or determined how these impacts compare in magnitude to the economic costs. Moreover, the relative magnitudes of economic and public health impacts of congestion would be expected to vary significantly across urban areas, as a function of road infrastructure, population density, and atmospheric conditions influencing pollutant formation, but this variability has not been explored. METHODS: In this study, we evaluate the public health impacts of ambient exposures to fine particulate matter (PM2.5) concentrations associated with a business-as-usual scenario of predicted traffic congestion. We evaluate 83 individual urban areas using traffic demand models to estimate the degree of congestion in each area from 2000 to 2030. We link traffic volume and speed data with the MOBILE6 model to characterize emissions of PM2.5 and particle precursors attributable to congestion, and we use a source-receptor matrix to evaluate the impact of these emissions on ambient PM2.5 concentrations. Marginal concentration changes are related to a concentration-response function for mortality, with a value of statistical life approach used to monetize the impacts. RESULTS: We estimate that the monetized value of PM2.5-related mortality attributable to congestion in these 83 cities in 2000 was approximately $31 billion (2007 dollars), as compared with a value of time and fuel wasted of $60 billion. In future years, the economic impacts grow (to over $100 billion in 2030) while the public health impacts decrease to $13 billion in 2020 before increasing to $17 billion in 2030, given increasing population and congestion but lower emissions per vehicle. Across cities and years, the public health impacts range from more than an order of magnitude less to in excess of the economic impacts. CONCLUSIONS: Our analyses indicate that the public health impacts of congestion may be significant enough in magnitude, at least in some urban areas, to be considered in future evaluations of the benefits of policies to mitigate congestion."
JONATHAN I LEVY,First Sagittarius A* Event Horizon Telescope results. V. Testing astrophysical models of the galactic center black hole,"In this paper we provide a first physical interpretation for the Event Horizon Telescope's (EHT) 2017 observations of Sgr A*. Our main approach is to compare resolved EHT data at 230 GHz and unresolved non-EHT observations from radio to X-ray wavelengths to predictions from a library of models based on time-dependent general relativistic magnetohydrodynamics simulations, including aligned, tilted, and stellar-wind-fed simulations; radiative transfer is performed assuming both thermal and nonthermal electron distribution functions. We test the models against 11 constraints drawn from EHT 230 GHz data and observations at 86 GHz, 2.2 μm, and in the X-ray. All models fail at least one constraint. Light-curve variability provides a particularly severe constraint, failing nearly all strongly magnetized (magnetically arrested disk (MAD)) models and a large fraction of weakly magnetized models. A number of models fail only the variability constraints. We identify a promising cluster of these models, which are MAD and have inclination i ≤ 30°. They have accretion rate (5.2–9.5) × 10−9 M ⊙ yr−1, bolometric luminosity (6.8–9.2) × 1035 erg s−1, and outflow power (1.3–4.8) × 1038 erg s−1. We also find that all models with i ≥ 70° fail at least two constraints, as do all models with equal ion and electron temperature; exploratory, nonthermal model sets tend to have higher 2.2 μm flux density; and the population of cold electrons is limited by X-ray constraints due to the risk of bremsstrahlung overproduction. Finally, we discuss physical and numerical limitations of the models, highlighting the possible importance of kinetic effects and duration of the simulations."
JONATHAN I LEVY,First M87 Event Horizon Telescope results. VII. Polarization of the ring,"In 2017 April, the Event Horizon Telescope (EHT) observed the near-horizon region around the supermassive black hole at the core of the M87 galaxy. These 1.3 mm wavelength observations revealed a compact asymmetric ring-like source morphology. This structure originates from synchrotron emission produced by relativistic plasma located in the immediate vicinity of the black hole. Here we present the corresponding linear-polarimetric EHT images of the center of M87. We find that only a part of the ring is significantly polarized. The resolved fractional linear polarization has a maximum located in the southwest part of the ring, where it rises to the level of ∼15%. The polarization position angles are arranged in a nearly azimuthal pattern. We perform quantitative measurements of relevant polarimetric properties of the compact emission and find evidence for the temporal evolution of the polarized source structure over one week of EHT observations. The details of the polarimetric data reduction and calibration methodology are provided. We carry out the data analysis using multiple independent imaging and modeling techniques, each of which is validated against a suite of synthetic data sets. The gross polarimetric structure and its apparent evolution with time are insensitive to the method used to reconstruct the image. These polarimetric images carry information about the structure of the magnetic fields responsible for the synchrotron emission. Their physical interpretation is discussed in an accompanying publication."
JONATHAN I LEVY,First M87 Event Horizon Telescope results. VIII. Magnetic field structure near The Event Horizon,"Event Horizon Telescope (EHT) observations at 230 GHz have now imaged polarized emission around the supermassive black hole in M87 on event-horizon scales. This polarized synchrotron radiation probes the structure of magnetic fields and the plasma properties near the black hole. Here we compare the resolved polarization structure observed by the EHT, along with simultaneous unresolved observations with the Atacama Large Millimeter/submillimeter Array, to expectations from theoretical models. The low fractional linear polarization in the resolved image suggests that the polarization is scrambled on scales smaller than the EHT beam, which we attribute to Faraday rotation internal to the emission region. We estimate the average density n_e ∼ 10^4–7 cm^−3, magnetic field strength B ∼ 1–30 G, and electron temperature T_e ∼ (1–12) × 10^10 K of the radiating plasma in a simple one-zone emission model. We show that the net azimuthal linear polarization pattern may result from organized, poloidal magnetic fields in the emission region. In a quantitative comparison with a large library of simulated polarimetric images from general relativistic magnetohydrodynamic (GRMHD) simulations, we identify a subset of physical models that can explain critical features of the polarimetric EHT observations while producing a relativistic jet of sufficient power. The consistent GRMHD models are all of magnetically arrested accretion disks, where near-horizon magnetic fields are dynamically important. We use the models to infer a mass accretion rate onto the black hole in M87 of (3–20) × 10^−4 M ⊙ yr^−1."
JONATHAN I LEVY,Resolving the inner parsec of the blazar J1924–2914 with the event horizon telescope,"The blazar J1924–2914 is a primary Event Horizon Telescope (EHT) calibrator for the Galactic center’s black hole Sagittarius A*. Here we present the first total and linearly polarized intensity images of this source obtained with the unprecedented 20 μas resolution of the EHT. J1924–2914 is a very compact flat-spectrum radio source with strong optical variability and polarization. In April 2017 the source was observed quasi-simultaneously with the EHT (April 5–11), the Global Millimeter VLBI Array (April 3), and the Very Long Baseline Array (April 28), giving a novel view of the source at four observing frequencies, 230, 86, 8.7, and 2.3 GHz. These observations probe jet properties from the subparsec to 100 pc scales. We combine the multifrequency images of J1924–2914 to study the source morphology. We find that the jet exhibits a characteristic bending, with a gradual clockwise rotation of the jet projected position angle of about 90° between 2.3 and 230 GHz. Linearly polarized intensity images of J1924–2914 with the extremely fine resolution of the EHT provide evidence for ordered toroidal magnetic fields in the blazar compact core."
JONATHAN I LEVY,"A cost-benefit analysis of a pellet boiler with electrostatic precipitator versus conventional biomass technology: A case study of an institutional boiler in Syracuse, New York","BACKGROUND: Biomass facilities have received increasing attention as a strategy to increase the use of renewable fuels and decrease greenhouse gas emissions from the electric generation and heating sectors, but these facilities can potentially increase local air pollution and associated health effects. Comparing the economic costs and public health benefits of alternative biomass fuel, heating technology, and pollution control technology options provides decision-makers with the necessary information to make optimal choices in a given location. METHODS: For a case study of a combined heat and power biomass facility in Syracuse, New York, we used stack testing to estimate emissions of fine particulate matter (PM2.5) for both the deployed technology (staged combustion pellet boiler with an electrostatic precipitator) and a conventional alternative (wood chip stoker boiler with a multicyclone). We used the atmospheric dispersion model AERMOD to calculate the contribution of either fuel-technology configuration to ambient primary PM2.5 in a 10 km x 10 km region surrounding the facility, and we quantified the incremental contribution to population mortality and morbidity. We assigned economic values to health outcomes and compared the health benefits of the lower-emitting technology with the incremental costs. RESULTS: In total, the incremental annualized cost of the lower-emitting pellet boiler was $190,000 greater, driven by a greater cost of the pellet fuel and pollution control technology, offset in part by reduced fuel storage costs. PM2.5 emissions were a factor of 23 lower with the pellet boiler with electrostatic precipitator, with corresponding differences in contributions to ambient primary PM2.5 concentrations. The monetary value of the public health benefits of selecting the pellet-fired boiler technology with electrostatic precipitator was $1.7 million annually, greatly exceeding the differential costs even when accounting for uncertainties. Our analyses also showed complex spatial patterns of health benefits given non-uniform age distributions and air pollution levels. CONCLUSIONS: The incremental investment in a lower-emitting staged combustion pellet boiler with an electrostatic precipitator was well justified by the population health improvements over the conventional wood chip technology with a multicyclone, even given the focus on only primary PM2.5 within a small spatial domain. Our analytical framework could be generalized to other settings to inform optimal strategies for proposed new facilities or populations."
JONATHAN I LEVY,Carbon Free Boston: Technical Summary,"OVERVIEW: This technical summary is intended to argument the rest of the Carbon Free Boston technical reports that seek to achieve this goal of deep mitigation. This document provides below: a rationale for carbon neutrality, a high level description of Carbon Free Boston’s analytical approach; a summary of crosssector strategies; a high level analysis of air quality impacts; and, a brief analysis of off-road and street light emissions."
JONATHAN I LEVY,A universal power-law prescription for variability from synthetic images of black hole accretion flows,"We present a framework for characterizing the spatiotemporal power spectrum of the variability expected from the horizon-scale emission structure around supermassive black holes, and we apply this framework to a library of general relativistic magnetohydrodynamic (GRMHD) simulations and associated general relativistic ray-traced images relevant for Event Horizon Telescope (EHT) observations of Sgr A*. We find that the variability power spectrum is generically a red-noise process in both the temporal and spatial dimensions, with the peak in power occurring on the longest timescales and largest spatial scales. When both the time-averaged source structure and the spatially integrated light-curve variability are removed, the residual power spectrum exhibits a universal broken power-law behavior. On small spatial frequencies, the residual power spectrum rises as the square of the spatial frequency and is proportional to the variance in the centroid of emission. Beyond some peak in variability power, the residual power spectrum falls as that of the time-averaged source structure, which is similar across simulations; this behavior can be naturally explained if the variability arises from a multiplicative random field that has a steeper high-frequency power-law index than that of the time-averaged source structure. We briefly explore the ability of power spectral variability studies to constrain physical parameters relevant for the GRMHD simulations, which can be scaled to provide predictions for black holes in a range of systems in the optically thin regime. We present specific expectations for the behavior of the M87* and Sgr A* accretion flows as observed by the EHT."
JONATHAN I LEVY,Millimeter light curves of Sagittarius A* observed during the 2017 Event Horizon Telescope campaign,"The Event Horizon Telescope (EHT) observed the compact radio source, Sagittarius A* (Sgr A*), in the Galactic Center on 2017 April 5–11 in the 1.3 mm wavelength band. At the same time, interferometric array data from the Atacama Large Millimeter/submillimeter Array and the Submillimeter Array were collected, providing Sgr A* light curves simultaneous with the EHT observations. These data sets, complementing the EHT very long baseline interferometry, are characterized by a cadence and signal-to-noise ratio previously unattainable for Sgr A* at millimeter wavelengths, and they allow for the investigation of source variability on timescales as short as a minute. While most of the light curves correspond to a low variability state of Sgr A*, the April 11 observations follow an X-ray flare and exhibit strongly enhanced variability. All of the light curves are consistent with a red-noise process, with a power spectral density (PSD) slope measured to be between −2 and −3 on timescales between 1 minute and several hours. Our results indicate a steepening of the PSD slope for timescales shorter than 0.3 hr. The spectral energy distribution is flat at 220 GHz, and there are no time lags between the 213 and 229 GHz frequency bands, suggesting low optical depth for the event horizon scale source. We characterize Sgr A*’s variability, highlighting the different behavior observed just after the X-ray flare, and use Gaussian process modeling to extract a decorrelation timescale and a PSD slope. We also investigate the systematic calibration uncertainties by analyzing data from independent data reduction pipelines."
JONATHAN I LEVY,Selective dynamical imaging of interferometric data,"Recent developments in very long baseline interferometry (VLBI) have made it possible for the Event Horizon Telescope (EHT) to resolve the innermost accretion flows of the largest supermassive black holes on the sky. The sparse nature of the EHT’s (u, v)-coverage presents a challenge when attempting to resolve highly time-variable sources. We demonstrate that the changing (u, v)-coverage of the EHT can contain regions of time over the course of a single observation that facilitate dynamical imaging. These optimal time regions typically have projected baseline distributions that are approximately angularly isotropic and radially homogeneous. We derive a metric of coverage quality based on baseline isotropy and density that is capable of ranking array configurations by their ability to produce accurate dynamical reconstructions. We compare this metric to existing metrics in the literature and investigate their utility by performing dynamical reconstructions on synthetic data from simulated EHT observations of sources with simple orbital variability. We then use these results to make recommendations for imaging the 2017 EHT Sgr A* data set."
JONATHAN I LEVY,First Sagittarius A* Event Horizon Telescope results. VI. Testing the black hole metric,"Astrophysical black holes are expected to be described by the Kerr metric. This is the only stationary, vacuum, axisymmetric metric, without electromagnetic charge, that satisfies Einstein’s equations and does not have pathologies outside of the event horizon. We present new constraints on potential deviations from the Kerr prediction based on 2017 EHT observations of Sagittarius A* (Sgr A*). We calibrate the relationship between the geometrically defined black hole shadow and the observed size of the ring-like images using a library that includes both Kerr and non-Kerr simulations. We use the exquisite prior constraints on the mass-to-distance ratio for Sgr A* to show that the observed image size is within ∼10% of the Kerr predictions. We use these bounds to constrain metrics that are parametrically different from Kerr, as well as the charges of several known spacetimes. To consider alternatives to the presence of an event horizon, we explore the possibility that Sgr A* is a compact object with a surface that either absorbs and thermally reemits incident radiation or partially reflects it. Using the observed image size and the broadband spectrum of Sgr A*, we conclude that a thermal surface can be ruled out and a fully reflective one is unlikely. We compare our results to the broader landscape of gravitational tests. Together with the bounds found for stellar-mass black holes and the M87 black hole, our observations provide further support that the external spacetimes of all black holes are described by the Kerr metric, independent of their mass."
JONATHAN I LEVY,Polarimetric properties of Event Horizon Telescope targets from ALMA,"We present the results from a full polarization study carried out with the Atacama Large Millimeter/submillimeter Array (ALMA) during the first Very Long Baseline Interferometry (VLBI) campaign, which was conducted in 2017 April in the λ3 mm and λ1.3 mm bands, in concert with the Global mm-VLBI Array (GMVA) and the Event Horizon Telescope (EHT), respectively. We determine the polarization and Faraday properties of all VLBI targets, including Sgr A*, M87, and a dozen radio-loud active galactic nuclei (AGNs), in the two bands at several epochs in a time window of 10 days. We detect high linear polarization fractions (2%–15%) and large rotation measures (RM &gt; 103.3–105.5 rad m−2), confirming the trends of previous AGN studies at millimeter wavelengths. We find that blazars are more strongly polarized than other AGNs in the sample, while exhibiting (on average) order-of-magnitude lower RM values, consistent with the AGN viewing angle unification scheme. For Sgr A* we report a mean RM of (−4.2 ± 0.3) × 105 rad m−2 at 1.3 mm, consistent with measurements over the past decade and, for the first time, an RM of (–2.1 ± 0.1) × 105 rad m−2 at 3 mm, suggesting that about half of the Faraday rotation at 1.3 mm may occur between the 3 mm photosphere and the 1.3 mm source. We also report the first unambiguous measurement of RM toward the M87 nucleus at millimeter wavelengths, which undergoes significant changes in magnitude and sign reversals on a one year timescale, spanning the range from −1.2 to 0.3 × 105 rad m−2 at 3 mm and −4.1 to 1.5 × 105 rad m−2 at 1.3 mm. Given this time variability, we argue that, unlike the case of Sgr A*, the RM in M87 does not provide an accurate estimate of the mass accretion rate onto the black hole. We put forward a two-component model, comprised of a variable compact region and a static extended region, that can simultaneously explain the polarimetric properties observed by both the EHT (on horizon scales) and ALMA (which observes the combined emission from both components). These measurements provide critical constraints for the calibration, analysis, and interpretation of simultaneously obtained VLBI data with the EHT and GMVA."
JONATHAN I LEVY,"First Sagittarius A* Event Horizon Telescope results. IV. Variability, morphology, and black hole mass","In this paper we quantify the temporal variability and image morphology of the horizon-scale emission from Sgr A*, as observed by the EHT in 2017 April at a wavelength of 1.3 mm. We find that the Sgr A* data exhibit variability that exceeds what can be explained by the uncertainties in the data or by the effects of interstellar scattering. The magnitude of this variability can be a substantial fraction of the correlated flux density, reaching ∼100% on some baselines. Through an exploration of simple geometric source models, we demonstrate that ring-like morphologies provide better fits to the Sgr A* data than do other morphologies with comparable complexity. We develop two strategies for fitting static geometric ring models to the time-variable Sgr A* data; one strategy fits models to short segments of data over which the source is static and averages these independent fits, while the other fits models to the full data set using a parametric model for the structural variability power spectrum around the average source structure. Both geometric modeling and image-domain feature extraction techniques determine the ring diameter to be 51.8 ± 2.3 μas (68% credible intervals), with the ring thickness constrained to have an FWHM between ∼30% and 50% of the ring diameter. To bring the diameter measurements to a common physical scale, we calibrate them using synthetic data generated from GRMHD simulations. This calibration constrains the angular size of the gravitational radius to be 4.8_-0.7^+1.4 μas, which we combine with an independent distance measurement from maser parallaxes to determine the mass of Sgr A* to be 4.0_-0.6^+10^6 M⊙."
JONATHAN I LEVY,"First Sagittarius A* Event Horizon Telescope results. II. EHT and multiwavelength observations, data processing, and calibration","We present Event Horizon Telescope (EHT) 1.3 mm measurements of the radio source located at the position of the supermassive black hole Sagittarius A* (Sgr A*), collected during the 2017 April 5–11 campaign. The observations were carried out with eight facilities at six locations across the globe. Novel calibration methods are employed to account for Sgr A*'s flux variability. The majority of the 1.3 mm emission arises from horizon scales, where intrinsic structural source variability is detected on timescales of minutes to hours. The effects of interstellar scattering on the image and its variability are found to be subdominant to intrinsic source structure. The calibrated visibility amplitudes, particularly the locations of the visibility minima, are broadly consistent with a blurred ring with a diameter of ∼50 μas, as determined in later works in this series. Contemporaneous multiwavelength monitoring of Sgr A* was performed at 22, 43, and 86 GHz and at near-infrared and X-ray wavelengths. Several X-ray flares from Sgr A* are detected by Chandra, one at low significance jointly with Swift on 2017 April 7 and the other at higher significance jointly with NuSTAR on 2017 April 11. The brighter April 11 flare is not observed simultaneously by the EHT but is followed by a significant increase in millimeter flux variability immediately after the X-ray outburst, indicating a likely connection in the emission physics near the event horizon. We compare Sgr A*’s broadband flux during the EHT campaign to its historical spectral energy distribution and find that both the quiescent emission and flare emission are consistent with its long-term behavior."
JONATHAN I LEVY,"Carbon, indoor air, energy and financial benefits of coupled ventilation upgrade and enhanced rooftop garden installation: an interdisciplinary climate mitigation approach",
JONATHAN I LEVY,The polarized image of a synchrotron-emitting ring of gas orbiting a black hole,"Synchrotron radiation from hot gas near a black hole results in a polarized image. The image polarization is determined by effects including the orientation of the magnetic field in the emitting region, relativistic motion of the gas, strong gravitational lensing by the black hole, and parallel transport in the curved spacetime. We explore these effects using a simple model of an axisymmetric, equatorial accretion disk around a Schwarzschild black hole. By using an approximate expression for the null geodesics derived by Beloborodov and conservation of the Walker–Penrose constant, we provide analytic estimates for the image polarization. We test this model using currently favored general relativistic magnetohydrodynamic simulations of M87*, using ring parameters given by the simulations. For a subset of these with modest Faraday effects, we show that the ring model broadly reproduces the polarimetric image morphology. Our model also predicts the polarization evolution for compact flaring regions, such as those observed from Sgr A* with GRAVITY. With suitably chosen parameters, our simple model can reproduce the EVPA pattern and relative polarized intensity in Event Horizon Telescope images of M87*. Under the physically motivated assumption that the magnetic field trails the fluid velocity, this comparison is consistent with the clockwise rotation inferred from total intensity images."
JONATHAN I LEVY,First Sagittarius A* Event Horizon Telescope results. I. The shadow of the supermassive black hole in the center of the Milky Way,"We present the first Event Horizon Telescope (EHT) observations of Sagittarius A* (Sgr A*), the Galactic center source associated with a supermassive black hole. These observations were conducted in 2017 using a global interferometric array of eight telescopes operating at a wavelength of λ = 1.3 mm. The EHT data resolve a compact emission region with intrahour variability. A variety of imaging and modeling analyses all support an image that is dominated by a bright, thick ring with a diameter of 51.8 ± 2.3 μas (68% credible interval). The ring has modest azimuthal brightness asymmetry and a comparatively dim interior. Using a large suite of numerical simulations, we demonstrate that the EHT images of Sgr A* are consistent with the expected appearance of a Kerr black hole with mass ∼4 × 106 M ⊙, which is inferred to exist at this location based on previous infrared observations of individual stellar orbits, as well as maser proper-motion studies. Our model comparisons disfavor scenarios where the black hole is viewed at high inclination (i &gt; 50°), as well as nonspinning black holes and those with retrograde accretion disks. Our results provide direct evidence for the presence of a supermassive black hole at the center of the Milky Way, and for the first time we connect the predictions from dynamical measurements of stellar orbits on scales of 103–105 gravitational radii to event-horizon-scale images and variability. Furthermore, a comparison with the EHT results for the supermassive black hole M87* shows consistency with the predictions of general relativity spanning over three orders of magnitude in central mass."
JONATHAN I LEVY,Nitrogen dioxide concentrations in neighborhoods adjacent to a commercial airport: a land use regression modeling study,"BACKGROUND: There is growing concern in communities surrounding airports regarding the contribution of various emission sources (such as aircraft and ground support equipment) to nearby ambient concentrations. We used extensive monitoring of nitrogen dioxide (NO2) in neighborhoods surrounding T.F. Green Airport in Warwick, RI, and land-use regression (LUR) modeling techniques to determine the impact of proximity to the airport and local traffic on these concentrations. METHODS: Palmes diffusion tube samplers were deployed along the airport's fence line and within surrounding neighborhoods for one to two weeks. In total, 644 measurements were collected over three sampling campaigns (October 2007, March 2008 and June 2008) and each sampling location was geocoded. GIS-based variables were created as proxies for local traffic and airport activity. A forward stepwise regression methodology was employed to create general linear models (GLMs) of NO2 variability near the airport. The effect of local meteorology on associations with GIS-based variables was also explored. RESULTS: Higher concentrations of NO2 were seen near the airport terminal, entrance roads to the terminal, and near major roads, with qualitatively consistent spatial patterns between seasons. In our final multivariate model (R2 = 0.32), the local influences of highways and arterial/collector roads were statistically significant, as were local traffic density and distance to the airport terminal (all p < 0.001). Local meteorology did not significantly affect associations with principal GIS variables, and the regression model structure was robust to various model-building approaches. CONCLUSION: Our study has shown that there are clear local variations in NO2 in the neighborhoods that surround an urban airport, which are spatially consistent across seasons. LUR modeling demonstrated a strong influence of local traffic, except the smallest roads that predominate in residential areas, as well as proximity to the airport terminal."
JONATHAN I LEVY,Broadband multi-wavelength properties of M87 during the 2017 Event Horizon Telescope campaign,"In 2017, the Event Horizon Telescope (EHT) Collaboration succeeded in capturing the first direct image of the center of the M87 galaxy. The asymmetric ring morphology and size are consistent with theoretical expectations for a weakly accreting supermassive black hole of mass ∼6.5 × 109 M ⊙. The EHTC also partnered with several international facilities in space and on the ground, to arrange an extensive, quasi-simultaneous multi-wavelength campaign. This Letter presents the results and analysis of this campaign, as well as the multi-wavelength data as a legacy data repository. We captured M87 in a historically low state, and the core flux dominates over HST-1 at high energies, making it possible to combine core flux constraints with the more spatially precise very long baseline interferometry data. We present the most complete simultaneous multi-wavelength spectrum of the active nucleus to date, and discuss the complexity and caveats of combining data from different spatial scales into one broadband spectrum. We apply two heuristic, isotropic leptonic single-zone models to provide insight into the basic source properties, but conclude that a structured jet is necessary to explain M87’s spectrum. We can exclude that the simultaneous γ-ray emission is produced via inverse Compton emission in the same region producing the EHT mm-band emission, and further conclude that the γ-rays can only be produced in the inner jets (inward of HST-1) if there are strongly particle-dominated regions. Direct synchrotron emission from accelerated protons and secondaries cannot yet be excluded."
JONATHAN I LEVY,First Sagittarius A* Event Horizon Telescope results. III. Imaging of the Galactic center supermassive black hole,"We present the first event-horizon-scale images and spatiotemporal analysis of Sgr A* taken with the Event Horizon Telescope in 2017 April at a wavelength of 1.3 mm. Imaging of Sgr A* has been conducted through surveys over a wide range of imaging assumptions using the classical CLEAN algorithm, regularized maximum likelihood methods, and a Bayesian posterior sampling method. Different prescriptions have been used to account for scattering effects by the interstellar medium toward the Galactic center. Mitigation of the rapid intraday variability that characterizes Sgr A* has been carried out through the addition of a “variability noise budget” in the observed visibilities, facilitating the reconstruction of static full-track images. Our static reconstructions of Sgr A* can be clustered into four representative morphologies that correspond to ring images with three different azimuthal brightness distributions and a small cluster that contains diverse nonring morphologies. Based on our extensive analysis of the effects of sparse (u, v)-coverage, source variability, and interstellar scattering, as well as studies of simulated visibility data, we conclude that the Event Horizon Telescope Sgr A* data show compelling evidence for an image that is dominated by a bright ring of emission with a ring diameter of ∼50 μas, consistent with the expected “shadow” of a 4 × 106 M⊙ black hole in the Galactic center located at a distance of 8 kpc."
JONATHAN I LEVY,"Lung function, asthma symptoms, and quality of life for children in public housing in Boston: a case-series analysis","BACKGROUND: Children in urban public housing are at high risk for asthma, given elevated environmental and social exposures and suboptimal medical care. For a multifactorial disease like asthma, design of intervention studies can be influenced by the relative prevalence of key risk factors. To better understand risk factors for asthma morbidity in the context of an environmental intervention study, we conducted a detailed baseline evaluation of 78 children (aged 4–17 years) from three public housing developments in Boston. METHODS: Asthmatic children and their caregivers were recruited between April 2002 and January 2003. We conducted intake interviews that captured a detailed family and medical history, including questions regarding asthma symptom severity, access to health care, medication usage, and psychological stress. Quality of life was evaluated for both the child and caregiver with an asthma-specific scale. Pulmonary function was measured with a portable spirometer, and allergy testing for common indoor and outdoor allergens was conducted with skin testing using the prick puncture method. Exploratory linear and logistic regression models evaluating predictors of respiratory symptoms, quality of life, and pulmonary function were conducted using SAS. RESULTS: We found high rates of obesity (56%) and allergies to indoor contaminants such as cockroaches (59%) and dust mites (59%). Only 36% of children with persistent asthma reported being prescribed any daily controller medication, and most did not have an asthma action plan or a peak flow meter. One-time lung function measures were poorly correlated with respiratory symptoms or quality of life, which were significantly correlated with each other. In multivariate regression models, household size, body mass index, and environmental tobacco smoke exposure were positively associated with respiratory symptom severity (p < 0.10). Symptom severity was negatively associated with asthma-related quality of life for the child and the caregiver, with caregiver (but not child) quality of life significantly influenced by caregiver stress and whether the child was in the intensive care unit at birth. CONCLUSION: Given the elevated prevalence of multiple risk factors, coordinated improvements in the social environment, the built environment, and in medical management would likely yield the greatest health benefits in this high-risk population."
JONATHAN I LEVY,Characterizing and mitigating intraday variability: reconstructing source structure in accreting black holes with mm-VLBI,"The extraordinary physical resolution afforded by the Event Horizon Telescope has opened a window onto the astrophysical phenomena unfolding on horizon scales in two known black holes, M87* and Sgr A*. However, with this leap in resolution has come a new set of practical complications. Sgr A* exhibits intraday variability that violates the assumptions underlying Earth aperture synthesis, limiting traditional image reconstruction methods to short timescales and data sets with very sparse (u, v) coverage. We present a new set of tools to detect and mitigate this variability. We develop a data-driven, model-agnostic procedure to detect and characterize the spatial structure of intraday variability. This method is calibrated against a large set of mock data sets, producing an empirical estimator of the spatial power spectrum of the brightness fluctuations. We present a novel Bayesian noise modeling algorithm that simultaneously reconstructs an average image and statistical measure of the fluctuations about it using a parameterized form for the excess variance in the complex visibilities not otherwise explained by the statistical errors. These methods are validated using a variety of simulated data, including general relativistic magnetohydrodynamic simulations appropriate for Sgr A* and M87*. We find that the reconstructed source structure and variability are robust to changes in the underlying image model. We apply these methods to the 2017 EHT observations of M87*, finding evidence for variability across the EHT observing campaign. The variability mitigation strategies presented are widely applicable to very long baseline interferometry observations of variable sources generally, for which they provide a data-informed averaging procedure and natural characterization of inter-epoch image consistency."
NINA MAZAR,Toward a taxonomy and review of honesty interventions,"What types of honesty interventions have been tested and to what extent? We conducted a systematic literature review of single-element intervention studies designed to curtail individual-level dishonesty and classified the obtained interventions in a taxonomy that encompasses three frameworks: nudging, economic, and internal-reward. We find moral reminders that we classify as educative nudges as well as external commitments (pledges, oaths, honor codes) and priming that we classify under the internal-reward framework to be the most frequently studied interventions, whereas architectural nudges (defaults, sludge) have hardly been developed. Most importantly, we identify two areas for improvement essential for our collective ability to successfully translate and scale honesty interventions: a more thorough examination of the interventions’ underlying psychological processes and precise description of the experimental design."
NINA MAZAR,Effectiveness of planning prompts on organizations’ likelihood to file their overdue taxes: a multi-wave field experiment,"This paper investigates the effectiveness of planning prompts on organizations’ tax compliance behavior. We conducted a large-scale, multi-wave field experiment examining the tax-paying behavior of all organizations that failed to file timely annual returns for a payroll tax in the province of Ontario. Organizations were randomly assigned to receive one of two letters: Ontario’s standard late notice (control) and a revised experimental late notice, which included step-by-step instructions of when, where and how to file a return. Our data indicate that planning prompts are effective at increasing organizations’ timely tax payment. In addition to replicating these findings across two waves, we demonstrate that while our intervention did not appear to have effects that persisted across tax years, organizations also did not habituate to our manipulation and its effects were consistent across repeated exposures. Our study is among the first to demonstrate that a simple behavioral intervention that has typically been applied to individuals to help them to act upon their existing motivations can be effective in the realm of tax compliance and organizational behavior."
NINA MAZAR,Applying behavioral insights to tax compliance: experimental evidence from Latvia,"In recent years, tax authorities around the world have started using behavioral insights to encourage taxpayers to fulfill their obligations. We review and discuss some of the recent empirical literature on tax compliance. In line with recent trends, we report on a field experiment in collaboration with the tax authority of Latvia (SRS) to encourage previously non-compliant individuals, who also have own business income, to submit their tax declarations on time in 2017. These individuals were pre-emptively sent emails with behaviorally informed messages, in order to reach and influence an important target population, at a salient moment. Our results indicate that all the behaviorally informed messages increased submission by the submission deadline, compared to a control group. The best performer was a message that specifically framed non-compliant behavior as a deliberate choice, which increased timely submission by 9.4% (4.1 percentage points; p=0.05)."
NINA MAZAR,Signing at the beginning versus at the end does not decrease dishonesty,"Honest reporting is essential for society to function well. However, people frequently lie when asked to provide information, such as misrepresenting their income to save money on taxes. A landmark finding published in PNAS [L. L. Shu, N. Mazar, F. Gino, D. Ariely,M. H. Bazerman,Proc. Natl. Acad. Sci. U.S.A.109, 15197–15200 (2012)] provided evidence for a simple way of encouraging honest reporting: asking people to sign a veracity statement at the beginning instead of at the end of a self-report form. Since this finding was published, various government agencies have adopted this practice. However, in this project, we failed to replicate this result. Across five conceptual replications (n=4,559) and one highly powered, preregistered, direct replication (n=1,235) conducted with the authors of the original paper, we observed no effect of signing first on honest reporting. Given the policy applications of this result, it is important to update the scientific record regarding the veracity of these results."
NINA MAZAR,"A 680,000-person megastudy of nudges to encourage vaccination in pharmacies","Encouraging vaccination is a pressing policy problem. To assess whether text-based reminders can encourage pharmacy vaccination and what kinds of messages work best, we conducted a megastudy. We randomly assigned 689,693 Walmart pharmacy patients to receive one of 22 different text reminders using a variety of different behavioral science principles to nudge flu vaccination or to a business-as-usual control condition that received no messages. We found that the reminder texts that we tested increased pharmacy vaccination rates by an average of 2.0 percentage points, or 6.8%, over a 3-mo follow-up period. The most-effective messages reminded patients that a flu shot was waiting for them and delivered reminders on multiple days. The top-performing intervention included two texts delivered 3 d apart and communicated to patients that a vaccine was “waiting for you.” Neither experts nor lay people anticipated that this would be the best-performing treatment, underscoring the value of simultaneously testing many different nudges in a highly powered megastudy."
NINA MAZAR,Nudging to increase income tax payments in Poland (field experiment),
NINA MAZAR,Behavioral Economics: Ethics and Integrative Thinking,
NINA MAZAR,Motivating bureaucrats through social recognition: external validity—a tale of two states,"Bureaucratic performance is a crucial determinant of economic growth, but little real-world evidence exists on how to improve it, especially in resource-constrained settings. We conducted a field experiment of a social recognition intervention to improve record keeping in health facilities in two Nigerian states, replicating the intervention – implemented by a single organization – on bureaucrats performing identical tasks. Social recognition improved performance in one state but had no effect in the other, highlighting both the potential benefits and also the sometimes-limited generalizability of behavioral interventions. Furthermore, differences in facility-level observables did not explain cross-state differences in impacts, suggesting that it may often be difficult to predict external validity."
NINA MAZAR,"Twenty questions about design behavior for sustainability, report of the International Expert Panel on behavioral science for design","How behavioral scientists, engineers, and architects can work together to advance how we all understand and practice design—in order to enhance sustainability in the built environment, and beyond."
NINA MAZAR,Providing health checks as incentives to retain blood donors — evidence from two field experiments,"The collection of blood given by donors has proven to be a substantial societal and a managerial challenge. Consequently, blood donation services seek for incentive mechanisms to retain donors. However, economic or material rewards might entail negative side effects such as motivational crowding out or even attracting “bad blood”. In an effort to increase the retention of established blood donors, we conducted two randomized field trials (N1 = 53,257, N2 = 31,522) in cooperation with the German Red Cross Blood Donation Service and tested the effectiveness of an incentive strategy that is directly related to the blood donation itself: offering a comprehensive blood health check. Contrary to previous related research, we found substantial positive effects of a comprehensive blood health check incentive on donation behavior. In addition, unlike previous studies, we examine effects of repeated exposure to this incentive and do not find any wearout effects. Considering the positive effect of this incentive on donor retention and the relative low cost for providing this service to donors, our findings suggest that offering comprehensive blood health check incentives is a viable and cost-efficient marketing strategy to increase the retention among previous donors even if offered over the longer run."
NINA MAZAR,"If you are going to pay within the next 24 hours, press 1: automatic planning prompt reduces credit card delinquency","People often form intentions but fail to follow through on them. Mounting evidence suggests that such intention-action gaps can be narrowed with prompts to make concrete plans about when, where and how to act to achieve the intention. In this paper we pushed the notion of plan-concreteness to test the efficacy of a prompt under a minimalist automated calling setting, where respondents were only prompted to indicate a narrower duration within which they intent to act. In a field experiment this planning prompt significantly helped people to pay their past dues and get out of debt delinquency. These results suggest that minimalist automatic planning prompts are a scalable, cost-effective intervention."
NINA MAZAR,Overcoming behavioral obstacles to escaping poverty,
NINA MAZAR,Replicating the effect of the accessibility of moral standards on dishonesty: authors’ response to the replication attempt,
NINA MAZAR,A practitioner's guide to nudging,
NINA MAZAR,When retailing and Las Vegas meet: probabilistic free price promotions,"A number of retailers offer gambling- or lottery-type price promotions with a chance to receive one’s entire purchase for free. Although these retailers seem to share the intuition that probabilistic free price promotions are attractive to consumers, it is unclear how they compare to traditional sure price promotions of equal expected monetary value. We compared these two risky and sure price promotions for planned purchases across six experiments in the field and in the laboratory. Together, we found that consumers are not only more likely to purchase a product promoted with a probabilistic free discount over the same product promoted with a sure discount but that they are also likely to purchase more of it. This preference seems to be primarily due to a diminishing sensitivity to the prices. In addition, we find that the zero price effect, transaction cost, and novelty considerations are likely not implicated."
NINA MAZAR,Experiment aversion does not appear to generalize,"Over the past decade, governments and organizations around the world have established behavioral insights teams advocating for randomized experiments. However, recent findings by M. N. Meyer et al., Proc. Natl. Acad. Sci. U.S.A. 116, 10723-10728 (2019) and P. R. Heck, C. F. Chabris, D. J. Watts, M. N. Meyer, Proc. Natl. Acad. Sci. U.S.A. 117, 18948-18950 (2020) suggest that people often rate randomized experiments as less appropriate than the policies they contain even when approving the implementation of either policy untested and when none of the individual policies is clearly superior. The authors warn that this could cause policymakers to avoid running large-scale field experiments or being transparent about running them and might contribute to an adverse heterogeneity bias in terms of who is participating in experiments. In one direct and six conceptual preregistered replications (total N = 5,200) of the previously published larger-effect studies, using the same main dependent variable but with variations in scenario wordings, recruitment platforms, and countries, and the addition of further measures to assess people's views, we test the generalizability and robustness of these findings. Together, we find that the original results do not appear to generalize. That is, our triangulation reveals insufficient evidence to conclude that people exhibit a common pattern of behavior that would be consistent with relative experiment aversion, thereby supporting recent findings by R. Mislavsky, B. Dietvorst, U. Simonsohn, Mark. Sci. 39, 1092-1104 (2020). Thus, policymakers may not need to be concerned about employing evidence-based practices more so than about universally implementing policies."
NINA MAZAR,Reply to Bas et al.: the difference between a genuine tendency and a context-specific response,
JANUSZ KONRAD,VGAN-based image representation learning for privacy-preserving facial expression recognition,"Reliable facial expression recognition plays a critical role in human-machine interactions. However, most of the facial expression analysis methodologies proposed to date pay little or no attention to the protection of a user's privacy. In this paper, we propose a Privacy-Preserving Representation-Learning Variational Generative Adversarial Network (PPRL-VGAN) to learn an image representation that is explicitly disentangled from the identity information. At the same time, this representation is discriminative from the standpoint of facial expression recognition and generative as it allows expression-equivalent face image synthesis. We evaluate the proposed model on two public datasets under various threat scenarios. Quantitative and qualitative results demonstrate that our approach strikes a balance between the preservation of privacy and data utility. We further demonstrate that our model can be effectively applied to other tasks such as expression morphing and image completion."
JANUSZ KONRAD,A rehabilitation tool for functional balance using altered gravity and virtual reality,"BACKGROUND There is a need for effective and early functional rehabilitation of patients with gait and balance problems including those with spinal cord injury, neurological diseases and recovering from hip fractures, a common consequence of falls especially in the elderly population. Gait training in these patients using partial body weight support (BWS) on a treadmill, a technique that involves unloading the subject through a harness, improves walking better than training with full weight bearing. One problem with this technique not commonly acknowledged is that the harness provides external support that essentially eliminates associated postural adjustments (APAs) required for independent gait. We have developed a device to address this issue and conducted a training study for proof of concept of efficacy. METHODS We present a tool that can enhance the concept of BWS training by allowing natural APAs to occur mediolaterally. While in a supine position in a 90 deg tilted environment built around a modified hospital bed, subjects wear a backpack frame that is freely moving on air-bearings (cf. puck on an air hockey table) and attached through a cable to a pneumatic cylinder that provides a load that can be set to emulate various G-like loads. Veridical visual input is provided through two 3-D automultiscopic displays that allow glasses free 3-D vision representing a virtual surrounding environment that may be acquired from sites chosen by the patient. Two groups of 12 healthy subjects were exposed to either strength training alone or a combination of strength and balance training in such a tilted environment over a period of four weeks. RESULTS Isokinetic strength measured during upright squat extension improved similarly in both groups. Measures of balance assessed in upright showed statistically significant improvements only when balance was part of the training in the tilted environment. Postural measures indicated less reliance on visual and/or increased use of somatosensory cues after training. CONCLUSION Upright balance function can be improved following balance specific training performed in a supine position in an environment providing the perception of an upright position with respect to gravity. Future studies will implement this concept in patients."
JANUSZ KONRAD,A fully-convolutional neural network for background subtraction of unseen videos,"Background subtraction is a basic task in computer vision and video processing often applied as a pre-processing step for object tracking, people recognition, etc. Recently, a number of successful background-subtraction algorithms have been proposed, however nearly all of the top-performing ones are supervised. Crucially, their success relies upon the availability of some annotated frames of the test video during training. Consequently, their performance on completely “unseen” videos is undocumented in the literature. In this work, we propose a new, supervised, backgroundsubtraction algorithm for unseen videos (BSUV-Net) based on a fully-convolutional neural network. The input to our network consists of the current frame and two background frames captured at different time scales along with their semantic segmentation maps. In order to reduce the chance of overfitting, we also introduce a new data-augmentation technique which mitigates the impact of illumination difference between the background frames and the current frame. On the CDNet-2014 dataset, BSUV-Net outperforms stateof-the-art algorithms evaluated on unseen videos in terms of several metrics including F-measure, recall and precision."
JANUSZ KONRAD,A rehabilitation tool for functional balance using altered gravity and virtual reality,"BACKGROUND There is a need for effective and early functional rehabilitation of patients with gait and balance problems including those with spinal cord injury, neurological diseases and recovering from hip fractures, a common consequence of falls especially in the elderly population. Gait training in these patients using partial body weight support (BWS) on a treadmill, a technique that involves unloading the subject through a harness, improves walking better than training with full weight bearing. One problem with this technique not commonly acknowledged is that the harness provides external support that essentially eliminates associated postural adjustments (APAs) required for independent gait. We have developed a device to address this issue and conducted a training study for proof of concept of efficacy. METHODS We present a tool that can enhance the concept of BWS training by allowing natural APAs to occur mediolaterally. While in a supine position in a 90 deg tilted environment built around a modified hospital bed, subjects wear a backpack frame that is freely moving on air-bearings (cf. puck on an air hockey table) and attached through a cable to a pneumatic cylinder that provides a load that can be set to emulate various G-like loads. Veridical visual input is provided through two 3-D automultiscopic displays that allow glasses free 3-D vision representing a virtual surrounding environment that may be acquired from sites chosen by the patient. Two groups of 12 healthy subjects were exposed to either strength training alone or a combination of strength and balance training in such a tilted environment over a period of four weeks. RESULTS Isokinetic strength measured during upright squat extension improved similarly in both groups. Measures of balance assessed in upright showed statistically significant improvements only when balance was part of the training in the tilted environment. Postural measures indicated less reliance on visual and/or increased use of somatosensory cues after training. CONCLUSION Upright balance function can be improved following balance specific training performed in a supine position in an environment providing the perception of an upright position with respect to gravity. Future studies will implement this concept in patients."
JANUSZ KONRAD,Convolutional neural network denoising of focused ion beam micrographs,"Most research on deep learning algorithms for image denoising has focused on signal-independent additive noise. Focused ion beam (FIB) microscopy with direct secondary electron detection has an unusual Neyman Type A (compound Poisson) measurement model, and sample damage poses fundamental challenges in obtaining training data. Model-based estimation is difficult and ineffective because of the nonconvexity of the negative log likelihood. In this paper, we develop deep learning-based denoising methods for FIB micrographs using synthetic training data generated from natural images. To the best of our knowledge, this is the first attempt in the literature to solve this problem with deep learning. Our results show that the proposed methods slightly outperform a total variation-regularized model-based method that requires time-resolved measurements that are not conventionally available. Improvements over methods using conventional measurements and less accurate noise modeling are dramatic - around 10 dB in peak signal-to-noise ratio."
JANUSZ KONRAD,BSUV-Net: a fully-convolutional neural network for background subtraction of unseen videos,"Background subtraction is a basic task in computer vision and video processing often applied as a pre-processing step for object tracking, people recognition, etc. Recently, a number of successful background-subtraction algorithms have been proposed, however nearly all of the top-performing ones are supervised. Crucially, their success relies upon the availability of some annotated frames of the test video during training. Consequently, their performance on completely “unseen” videos is undocumented in the literature. In this work, we propose a new, supervised, background subtraction algorithm for unseen videos (BSUV-Net) based on a fully-convolutional neural network. The input to our network consists of the current frame and two background frames captured at different time scales along with their semantic segmentation maps. In order to reduce the chance of overfitting, we also introduce a new data-augmentation technique which mitigates the impact of illumination difference between the background frames and the current frame. On the CDNet-2014 dataset, BSUV-Net outperforms stateof-the-art algorithms evaluated on unseen videos in terms of several metrics including F-measure, recall and precision."
JANUSZ KONRAD,Semi-coupled two-stream fusion ConvNets for action recognition at extremely low resolutions,"Deep convolutional neural networks (ConvNets) have been recently shown to attain state-of-the-art performance for action recognition on standard-resolution videos. However, less attention has been paid to recognition performance at extremely low resolutions (eLR) (e.g., 16 12 pixels). Reliable action recognition using eLR cameras would address privacy concerns in various application environments such as private homes, hospitals, nursing/rehabilitation facilities, etc. In this paper, we propose a semi-coupled, filter-sharing network that leverages high-resolution (HR) videos during training in order to assist an eLR ConvNet. We also study methods for fusing spatial and temporal ConvNets customized for eLR videos in order to take advantage of appearance and motion information. Our method outperforms state-of-the-art methods at extremely low resolutions on IXMAS (93:7%) and HMDB (29:2%) datasets."
JANUSZ KONRAD,Behavior subtraction,"Background subtraction has been a driving engine for many computer vision and video analytics tasks. Although its many variants exist, they all share the underlying assumption that photometric scene properties are either static or exhibit temporal stationarity. While this works in many applications, the model fails when one is interested in discovering changes in scene dynamics instead of changes in scene's photometric properties; the detection of unusual pedestrian or motor traffic patterns are but two examples. We propose a new model and computational framework that assume the dynamics of a scene, not its photometry, to be stationary, i.e., a dynamic background serves as the reference for the dynamics of an observed scene. Central to our approach is the concept of an event, which we define as short-term scene dynamics captured over a time window at a specific spatial location in the camera field of view. Unlike in our earlier work, we compute events by time-aggregating vector object descriptors that can combine multiple features, such as object size, direction of movement, speed, etc. We characterize events probabilistically, but use low-memory, low-complexity surrogates in a practical implementation. Using these surrogates amounts to behavior subtraction, a new algorithm for effective and efficient temporal anomaly detection and localization. Behavior subtraction is resilient to spurious background motion, such as due to camera jitter, and is content-blind, i.e., it works equally well on humans, cars, animals, and other objects in both uncluttered and highly cluttered scenes. Clearly, treating video as a collection of events rather than colored pixels opens new possibilities for video analytics."
JANUSZ KONRAD,"Concussion, microvascular injury, and early tauopathy in young athletes after impact head injury and an impact concussion mouse model","The mechanisms underpinning concussion, traumatic brain injury, and chronic traumatic encephalopathy, and the relationships between these disorders, are poorly understood. We examined post-mortem brains from teenage athletes in the acute-subacute period after mild closed-head impact injury and found astrocytosis, myelinated axonopathy, microvascular injury, perivascular neuroinflammation, and phosphorylated tau protein pathology. To investigate causal mechanisms, we developed a mouse model of lateral closed-head impact injury that uses momentum transfer to induce traumatic head acceleration. Unanaesthetized mice subjected to unilateral impact exhibited abrupt onset, transient course, and rapid resolution of a concussion-like syndrome characterized by altered arousal, contralateral hemiparesis, truncal ataxia, locomotor and balance impairments, and neurobehavioural deficits. Experimental impact injury was associated with axonopathy, blood–brain barrier disruption, astrocytosis, microgliosis (with activation of triggering receptor expressed on myeloid cells, TREM2), monocyte infiltration, and phosphorylated tauopathy in cerebral cortex ipsilateral and subjacent to impact. Phosphorylated tauopathy was detected in ipsilateral axons by 24 h, bilateral axons and soma by 2 weeks, and distant cortex bilaterally at 5.5 months post-injury. Impact pathologies co-localized with serum albumin extravasation in the brain that was diagnostically detectable in living mice by dynamic contrast-enhanced MRI. These pathologies were also accompanied by early, persistent, and bilateral impairment in axonal conduction velocity in the hippocampus and defective long-term potentiation of synaptic neurotransmission in the medial prefrontal cortex, brain regions distant from acute brain injury. Surprisingly, acute neurobehavioural deficits at the time of injury did not correlate with blood–brain barrier disruption, microgliosis, neuroinflammation, phosphorylated tauopathy, or electrophysiological dysfunction. Furthermore, concussion-like deficits were observed after impact injury, but not after blast exposure under experimental conditions matched for head kinematics. Computational modelling showed that impact injury generated focal point loading on the head and seven-fold greater peak shear stress in the brain compared to blast exposure. Moreover, intracerebral shear stress peaked before onset of gross head motion. By comparison, blast induced distributed force loading on the head and diffuse, lower magnitude shear stress in the brain. We conclude that force loading mechanics at the time of injury shape acute neurobehavioural responses, structural brain damage, and neuropathological sequelae triggered by neurotrauma. These results indicate that closed-head impact injuries, independent of concussive signs, can induce traumatic brain injury as well as early pathologies and functional sequelae associated with chronic traumatic encephalopathy. These results also shed light on the origins of concussion and relationship to traumatic brain injury and its aftermath."
JAMES E FLEMING,Prospects for beyond the standard model physics searches at the deep underground neutrino experiment: DUNE collaboration,"The Deep Underground Neutrino Experiment (DUNE) will be a powerful tool for a variety of physics topics. The high-intensity proton beams provide a large neutrino flux, sampled by a near detector system consisting of a combination of capable precision detectors, and by the massive far detector system located deep underground. This configuration sets up DUNE as a machine for discovery, as it enables opportunities not only to perform precision neutrino measurements that may uncover deviations from the present three-flavor mixing paradigm, but also to discover new particles and unveil new interactions and symmetries beyond those predicted in the Standard Model (SM). Of the many potential beyond the Standard Model (BSM) topics DUNE will probe, this paper presents a selection of studies quantifying DUNE's sensitivities to sterile neutrino mixing, heavy neutral leptons, non-standard interactions, CPT symmetry violation, Lorentz invariance violation, neutrino trident production, dark matter from both beam induced and cosmogenic sources, baryon number violation, and other new physics topics that complement those at high-energy colliders and significantly extend the present reach."
JAMES E FLEMING,Identification and reconstruction of low-energy electrons in the ProtoDUNE-SP detector,
JAMES E FLEMING,The long-baseline neutrino experiment: exploring fundamental symmetries of the universe,"The preponderance of matter over antimatter in the early Universe, the dynamics of the supernova bursts that produced the heavy elements necessary for life and whether protons eventually decay --- these mysteries at the forefront of particle physics and astrophysics are key to understanding the early evolution of our Universe, its current state and its eventual fate. The Long-Baseline Neutrino Experiment (LBNE) represents an extensively developed plan for a world-class experiment dedicated to addressing these questions. LBNE is conceived around three central components: (1) a new, high-intensity neutrino source generated from a megawatt-class proton accelerator at Fermi National Accelerator Laboratory, (2) a near neutrino detector just downstream of the source, and (3) a massive liquid argon time-projection chamber deployed as a far detector deep underground at the Sanford Underground Research Facility. This facility, located at the site of the former Homestake Mine in Lead, South Dakota, is approximately 1,300 km from the neutrino source at Fermilab -- a distance (baseline) that delivers optimal sensitivity to neutrino charge-parity symmetry violation and mass ordering effects. This ambitious yet cost-effective design incorporates scalability and flexibility and can accommodate a variety of upgrades and contributions. With its exceptional combination of experimental configuration, technical capabilities, and potential for transformative discoveries, LBNE promises to be a vital facility for the field of particle physics worldwide, providing physicists from around the globe with opportunities to collaborate in a twenty to thirty year program of exciting science. In this document we provide a comprehensive overview of LBNE's scientific objectives, its place in the landscape of neutrino physics worldwide, the technologies it will incorporate and the capabilities it will possess."
JAMES E FLEMING,Scintillation light detection in the 6-m drift-length ProtoDUNE Dual Phase liquid argon TPC,"DUNE is a dual-site experiment for long-baseline neutrino oscillation studies, neutrino astrophysics and nucleon decay searches. ProtoDUNE Dual Phase (DP) is a 6  ×  6  ×  6 m 3 liquid argon time-projection-chamber (LArTPC) that recorded cosmic-muon data at the CERN Neutrino Platform in 2019-2020 as a prototype of the DUNE Far Detector. Charged particles propagating through the LArTPC produce ionization and scintillation light. The scintillation light signal in these detectors can provide the trigger for non-beam events. In addition, it adds precise timing capabilities and improves the calorimetry measurements. In ProtoDUNE-DP, scintillation and electroluminescence light produced by cosmic muons in the LArTPC is collected by photomultiplier tubes placed up to 7 m away from the ionizing track. In this paper, the ProtoDUNE-DP photon detection system performance is evaluated with a particular focus on the different wavelength shifters, such as PEN and TPB, and the use of Xe-doped LAr, considering its future use in giant LArTPCs. The scintillation light production and propagation processes are analyzed and a comparison of simulation to data is performed, improving understanding of the liquid argon properties."
THANH N NGUYEN,Cerebral Collateral Circulation in Carotid Artery Disease,"Carotid artery disease is common and increases the risk of stroke. However, there is wide variability on the severity of clinical manifestations of carotid disease, ranging from asymptomatic to fatal stroke. The collateral circulation has been recognized as an important aspect of cerebral circulation affecting the risk of stroke as well as other features of stroke presentation, such as stroke patterns in patients with carotid artery disease. The cerebral circulation attempts to maintain constant cerebral perfusion despite changes in systemic conditions, due to its ability to autoregulate blood flow. In case that one of the major cerebral arteries is compromised by occlusive disease, the cerebral collateral circulation plays an important role in preserving cerebral perfusion through enhanced recruitment of blood flow. With the advent of techniques that allow rapid evaluation of cerebral perfusion, the collateral circulation of the brain and its effectiveness may also be evaluated, allowing for prompt assessment of patients with acute stroke due to involvement of the carotid artery, and risk stratification of patients with carotid stenosis in chronic stages. Understanding the cerebral collateral circulation provides a basis for the future development of new diagnostic tools, risk stratification, predictive models and new therapeutic modalities. In the present review we discuss basic aspects of the cerebral collateral circulation, diagnostic methods to assess collateral circulation, and implications in occlusive carotid artery disease."
THOMAS J BARFIELD,Bringing multilateralism back in: ending the war in Afghanistan is not a one-nation job,"The United States’ unilateral deal with the Taliban in February 2020 needs to be expanded if it is to achieve success. Because the war in Afghanistan was never purely a domestic one, only a multilateral international agreement can end it and simultaneously empower Afghan stakeholders to determine their country’s future governance. A dual-track United Nations-led mediation platform, bolstered by a collaboration between Washington and Brussels, offers the best means to achieve this end. At the international and regional level, its goal would be conflict management: to end outside support for any faction unwilling to take part in the domestic peace process and to pledge support for any final negotiated peace agreement acceptable to a majority of the Afghan people. Since neither the Afghan government nor the Taliban can win a war or dictate the structure of a future constitutional order without such outside support, this would lay the groundwork for lasting conflict resolution within Afghanistan itself."
THOMAS J BARFIELD,The Islamic State as an empire of nostalgia,"Primary empires were the product of internal development and self-sustaining through the exploitation of their own resources, but there were also historically a large number of “shadow empires.” These were imperial polities that were the products of secondary empire formation, which came into existence as a response to the formation of primary empires elsewhere and could not exist except in interaction with them. One unusual subset of these were “empires of nostalgia” that claimed an imperial tradition and the outward trappings of an extinct empire, but did not themselves meet the basic requirements of an imperial state such as direct control of territory, true centralized rule, or significant urban centers. The most famous European example was the Carolingian Empire established by Charlemagne and its long lived successor, the Holy Roman Empire, which survived as an institution for a thousand years. The Islamic State’s proclamation of itself as a reborn caliphate is now a contemporary example built on nostalgia in the Islamic world for a long-dead empire that still exerts a strong cultural attraction upon many Muslims. The Islamic State justifies its actions and ideologies by attempting to ground them in a lost golden age that they propose to restore."
THOMAS J BARFIELD,Introduction: The American Institute of Afghanistan Studies,
DEAN TOLAN,"Structure of a rabbit muscle fructose-1,6-bisphosphate aldolase A dimer variant","Fructose-1,6-bisphosphate aldolase (aldolase) is an essential enzyme in glycolysis and gluconeogenesis. In addition to this primary function, aldolase is also known to bind to a variety of other proteins, a property that may allow it to perform 'moonlighting' roles in the cell. Although monomeric and dimeric aldolases possess full catalytic activity, the enzyme occurs as an unusually stable tetramer, suggesting a possible link between the oligomeric state and these noncatalytic cellular roles. Here, the first high-resolution X-ray crystal structure of rabbit muscle D128V aldolase, a dimeric form of aldolase mimicking the clinically important D128G mutation in humans associated with hemolytic anemia, is presented. The structure of the dimer was determined to 1.7 angstroms resolution with the product DHAP bound in the active site. The turnover of substrate to produce the product ligand demonstrates the retention of catalytic activity by the dimeric aldolase. The D128V mutation causes aldolase to lose intermolecular contacts with the neighboring subunit at one of the two interfaces of the tetramer. The tertiary structure of the dimer does not significantly differ from the structure of half of the tetramer. Analytical ultracentrifugation confirms the occurrence of the enzyme as a dimer in solution. The highly stable structure of aldolase with an independent active site is consistent with a model in which aldolase has evolved as a multimeric scaffold to perform other noncatalytic functions."
DEAN TOLAN,Climate change and the kidney,"The worldwide increase in temperature has resulted in a marked increase in heat waves (heat extremes) that carries a markedly increased risk for morbidity and mortality. The kidney has a unique role not only in protecting the host from heat and dehydration but also is an important site of heat-associated disease. Here we review the potential impact of global warming and heat extremes on kidney diseases. High temperatures can result in increased core temperatures, dehydration, and blood hyperosmolality. Heatstroke (both clinical and subclinical whole-body hyperthermia) may have a major role in causing both acute kidney disease, leading to increased risk of acute kidney injury from rhabdomyolysis, or heat-induced inflammatory injury to the kidney. Recurrent heat and dehydration can result in chronic kidney disease (CKD) in animals and theoretically plays a role in epidemics of CKD developing in hot regions of the world where workers are exposed to extreme heat. Heat stress and dehydration also has a role in kidney stone formation, and poor hydration habits may increase the risk for recurrent urinary tract infections. The resultant social and economic consequences include disability and loss of productivity and employment. Given the rise in world temperatures, there is a major need to better understand how heat stress can induce kidney disease, how best to provide adequate hydration, and ways to reduce the negative effects of chronic heat exposure."
DEAN TOLAN,"Structure of a Rabbit Muscle Fructose-1,6-Bisphosphate Aldolase A Dimer Variant","The X-ray crystallographic structure of a dimer variant of fructose-1,6-bisphosphate aldolase demonstrates a stable oligomer that mirrors half of the native tetramer. The presence of product demonstrates that this is an active form. Fructose-1,6-bisphosphate aldolase (aldolase) is an essential enzyme in glycolysis and gluconeogenesis. In addition to this primary function, aldolase is also known to bind to a variety of other proteins, a property that may allow it to perform 'moonlighting' roles in the cell. Although monomeric and dimeric aldolases possess full catalytic activity, the enzyme occurs as an unusually stable tetramer, suggesting a possible link between the oligomeric state and these noncatalytic cellular roles. Here, the first high-resolution X-ray crystal structure of rabbit muscle D128V aldolase, a dimeric form of aldolase mimicking the clinically important D128G mutation in humans associated with hemolytic anemia, is presented. The structure of the dimer was determined to 1.7 Å resolution with the product DHAP bound in the active site. The turnover of substrate to produce the product ligand demonstrates the retention of catalytic activity by the dimeric aldolase. The D128V mutation causes aldolase to lose intermolecular contacts with the neighboring subunit at one of the two interfaces of the tetramer. The tertiary structure of the dimer does not significantly differ from the structure of half of the tetramer. Analytical ultracentrifugation confirms the occurrence of the enzyme as a dimer in solution. The highly stable structure of aldolase with an independent active site is consistent with a model in which aldolase has evolved as a multimeric scaffold to perform other noncatalytic functions."
DEAN TOLAN,Buildout and integration of an automated high-throughput CLIA laboratory for SARS-CoV-2 testing on a large urban campus,"In 2019, the first cases of SARS-CoV-2 were detected in Wuhan, China, and by early 2020 the first cases were identified in the United States. SARS-CoV-2 infections increased in the US causing many states to implement stay-at-home orders and additional safety precautions to mitigate potential outbreaks. As policies changed throughout the pandemic and restrictions lifted, there was an increase in demand for COVID-19 testing which was costly, difficult to obtain, or had long turn-around times. Some academic institutions, including Boston University (BU), created an on-campus COVID-19 screening protocol as part of a plan for the safe return of students, faculty, and staff to campus with the option for in-person classes. At BU, we put together an automated high-throughput clinical testing laboratory with the capacity to run 45,000 individual tests weekly by Fall of 2020, with a purpose-built clinical testing laboratory, a multiplexed reverse transcription PCR (RT-qPCR) test, robotic instrumentation, and trained staff. There were many challenges including supply chain issues for personal protective equipment and testing materials in addition to equipment that were in high demand. The BU Clinical Testing Laboratory (CTL) was operational at the start of Fall 2020 and performed over 1 million SARS-CoV-2 PCR tests during the 2020-2021 academic year."
RICHARD WEST,Tuned pipes: end-to-end throughput and delay guarantees for USB devices,"A fundamental problem in real-time computing is handling device input and output in a timely manner. For example, a control system might require input data from a sensor to be sampled and processed at a regular rate so that output signals to actuators occur within specific delay bounds. Input/output (I/O) devices connect to the host computer using different types of bus interfaces. One of the most popular interfaces in use today is the universal serial bus (USB). USB is now ubiquitous, in part due to its support for many classes of devices with simplified hardware needed to connect to the host. However, typical USB host controller drivers suffer from potential timing delays that affect the delivery of data between tasks and devices. Consequently, this paper introduces tuned pipes, a host controller driver and system framework that guarantees end-to-end latency and throughput requirements for I/O transfers. We expand on our earlier work involving USB 2.0 to support higher bandwidth USB 3.x communication. As a case study, we show how a USB-Controller Area Network (CAN) guarantees temporal isolation and end-to-end guarantees on communication between a set of peripheral devices and host tasks. A comparable USB-CAN bus setup using Linux is not able to achieve the same level of temporal guarantees, even when using SCHED_DEADLINE."
RICHARD WEST,Partitioned real-time NAND flash storage,"This paper addresses the problem of guaranteeing performance and predictability of NAND flash memory in a real-time storage system. Our approach implements a new flash translation layer scheme that exploits internal parallelism within solid state storage devices. We describe the Partitioned Real-Time Flash Translation Layer (PaRT-FTL), which splits a set of flash chips into separate read and write sets. This ensures reads and writes to separate chips proceed in parallel. However, PaRT-FTL is also able to rebuild the data for a read request from a flash chip that is busy servicing a write request or performing garbage collection. Consequently, reads are never blocked by writes or storage space reclamation. PaRT-FTL is compared to previous real-time approaches including scheduling, over-provisioning and partial garbage collection. We show that by isolating read and write requests using encoding techniques, PaRT-FTL provides better latency guarantees for real-time applications."
RICHARD WEST,Building real-time embedded applications on QduinoMC: a web-connected 3D printer case study,"Single Board Computers (SBCs) are now emerging with multiple cores, ADCs, GPIOs, PWM channels, integrated graphics, and several serial bus interfaces. The low power consumption, small form factor and I/O interface capabilities of SBCs with sensors and actuators makes them ideal in embedded and real-time applications. However, most SBCs run non-realtime operating systems based on Linux and Windows, and do not provide a user-friendly API for application development. This paper presents QduinoMC, a multicore extension to the popular Arduino programming environment, which runs on the Quest real-time operating system. QduinoMC is an extension of our earlier single-core, real-time, multithreaded Qduino API. We show the utility of QduinoMC by applying it to a specific application: a web-connected 3D printer. This differs from existing 3D printers, which run relatively simple firmware and lack operating system support to spool multiple jobs, or interoperate with other devices (e.g., in a print farm). We show how QduinoMC empowers devices with the capabilities to run new services without impacting their timing guarantees. While it is possible to modify existing operating systems to provide suitable timing guarantees, the effort to do so is cumbersome and does not provide the ease of programming afforded by QduinoMC."
RICHARD WEST,Dynamic Window-Constrained Scheduling for Real-Time Media Streaming,"This paper describes an algorithm for scheduling packets in real-time multimedia data streams. Common to these classes of data streams are service constraints in terms of bandwidth and delay. However, it is typical for real-time multimedia streams to tolerate bounded delay variations and, in some cases, finite losses of packets. We have therefore developed a scheduling algorithm that assumes streams have window-constraints on groups of consecutive packet deadlines. A window-constraint defines the number of packet deadlines that can be missed in a window of deadlines for consecutive packets in a stream. Our algorithm, called Dynamic Window-Constrained Scheduling (DWCS), attempts to guarantee no more than x out of a window of y deadlines are missed for consecutive packets in real-time and multimedia streams. Using DWCS, the delay of service to real-time streams is bounded even when the scheduler is overloaded. Moreover, DWCS is capable of ensuring independent delay bounds on streams, while at the same time guaranteeing minimum bandwidth utilizations over tunable and finite windows of time. We show the conditions under which the total demand for link bandwidth by a set of real-time (i.e., window-constrained) streams can exceed 100% and still ensure all window-constraints are met. In fact, we show how it is possible to guarantee worst-case per-stream bandwidth and delay constraints while utilizing all available link capacity. Finally, we show how best-effort packets can be serviced with fast response time, in the presence of window-constrained traffic."
RICHARD WEST,User-Level Sandboxing: a Safe and Efficient Mechanism for Extensibility,"Extensible systems allow services to be configured and deployed for the specific needs of individual applications. This paper describes a safe and efficient method for user-level extensibility that requires only minimal changes to the kernel. A sandboxing technique is described that supports multiple logical protection domains within the same address space at user-level. This approach allows applications to register sandboxed code with the system, that may be executed in the context of any process. Our approach differs from other implementations that require special hardware support, such as segmentation or tagged translation look-aside buffers (TLBs), to either implement multiple protection domains in a single address space, or to support fast switching between address spaces. Likewise, we do not require the entire system to be written in a type-safe language, to provide fine-grained protection domains. Instead, our user-level sandboxing technique requires only paged-based virtual memory support, and the requirement that extension code is written either in a type-safe language, or by a trusted source. Using a fast method of upcalls, we show how our sandboxing technique for implementing logical protection domains provides significant performance improvements over traditional methods of invoking user-level services. Experimental results show our approach to be an efficient method for extensibility, with inter-protection domain communication costs close to those of hardware-based solutions leveraging segmentation."
RICHARD WEST,Adaptive Routing of QoS-constrained Media Streams over Scalable Overlay Topologies,"Current research on Internet-based distributed systems emphasizes the scalability of overlay topologies for efficient search and retrieval of data items, as well as routing amongst peers. However, most existing approaches fail to address the transport of data across these logical networks in accordance with quality of service (QoS) constraints. Consequently, this paper investigates the use of scalable overlay topologies for routing real-time media streams between publishers and potentially many thousands of subscribers. Specifically, we analyze the costs of using k-ary n-cubes for QoS-constrained routing. Given a number of nodes in a distributed system, we calculate the optimal k-ary n-cube structure for minimizing the average distance between any pair of nodes. Using this structure, we describe a greedy algorithm that selects paths between nodes in accordance with the real-time delays along physical links. We show this method improves the routing latencies by as much as 67%, compared to approaches that do not consider physical link costs. We are in the process of developing a method for adaptive node placement in the overlay topology, based upon the locations of publishers, subscribers, physical link costs and per-subscriber QoS constraints. One such method for repositioning nodes in logical space is discussed, to improve the likelihood of meeting service requirements on data routed between publishers and subscribers. Future work will evaluate the benefits of such techniques more thoroughly."
RICHARD WEST,Comparison of K-ary N-cube and de Bruijn Overlays in QoS-Constrained Multicast Applications,"Research on the construction of logical overlay networks has gained significance in recent times. This is partly due to work on peer-to-peer (P2P) systems for locating and retrieving distributed data objects, and also scalable content distribution using end-system multicast techniques. However, there are emerging applications that require the real-time transport of data from various sources to potentially many thousands of subscribers, each having their own quality-of-service (QoS) constraints. This paper primarily focuses on the properties of two popular topologies found in interconnection networks, namely k-ary n-cubes and de Bruijn graphs. The regular structure of these graph topologies makes them easier to analyze and determine possible routes for real-time data than complete or irregular graphs. We show how these overlay topologies compare in their ability to deliver data according to the QoS constraints of many subscribers, each receiving data from specific publishing hosts. Comparisons are drawn on the ability of each topology to route data in the presence of dynamic system effects, due to end-hosts joining and departing the system. Finally, experimental results show the service guarantees and physical link stress resulting from efficient multicast trees constructed over both kinds of overlay networks."
RICHARD WEST,Scalable Overlay Multicast Tree Construction for QoS-Constrained Media Streaming,"Overlay networks have become popular in recent times for content distribution and end-system multicasting of media streams. In the latter case, the motivation is based on the lack of widespread deployment of IP multicast and the ability to perform end-host processing. However, constructing routes between various end-hosts, so that data can be streamed from content publishers to many thousands of subscribers, each having their own QoS constraints, is still a challenging problem. First, any routes between end-hosts using trees built on top of overlay networks can increase stress on the underlying physical network, due to multiple instances of the same data traversing a given physical link. Second, because overlay routes between end-hosts may traverse physical network links more than once, they increase the end-to-end latency compared to IP-level routing. Third, algorithms for constructing efficient, large-scale trees that reduce link stress and latency are typically more complex. This paper therefore compares various methods to construct multicast trees between end-systems, that vary in terms of implementation costs and their ability to support per-subscriber QoS constraints. We describe several algorithms that make trade-offs between algorithmic complexity, physical link stress and latency. While no algorithm is best in all three cases we show how it is possible to efficiently build trees for several thousand subscribers with latencies within a factor of two of the optimal, and link stresses comparable to, or better than, existing technologies."
RICHARD WEST,An Efficient User-Level Shared Memory Mechanism for Application-Specific Extensions,"This paper focuses on an efficient user-level method for the deployment of application-specific extensions, using commodity operating systems and hardware. A sandboxing technique is described that supports multiple extensions within a shared virtual address space. Applications can register sandboxed code with the system, so that it may be executed in the context of any process. Such code may be used to implement generic routines and handlers for a class of applications, or system service extensions that complement the functionality of the core kernel. Using our approach, application-specific extensions can be written like conventional user-level code, utilizing libraries and system calls, with the advantage that they may be executed without the traditional costs of scheduling and context-switching between process-level protection domains. No special hardware support such as segmentation or tagged translation look-aside buffers (TLBs) is required. Instead, our ``user-level sandboxing'' mechanism requires only paged-based virtual memory support, given that sandboxed extensions are either written by a trusted source or are guaranteed to be memory-safe (e.g., using type-safe languages). Using a fast method of upcalls, we show how our mechanism provides significant performance improvements over traditional methods of invoking user-level services. As an application of our approach, we have implemented a user-level network subsystem that avoids data copying via the kernel and, in many cases, yields far greater network throughput than kernel-level approaches."
RICHARD WEST,Cuckoo: a Language for Implementing Memory- and Thread-safe System Services,"This paper is centered around the design of a thread- and memory-safe language, primarily for the compilation of application-specific services for extensible operating systems. We describe various issues that have influenced the design of our language, called Cuckoo, that guarantees safety of programs with potentially asynchronous flows of control. Comparisons are drawn between Cuckoo and related software safety techniques, including Cyclone and software-based fault isolation (SFI), and performance results suggest our prototype compiler is capable of generating safe code that executes with low runtime overheads, even without potential code optimizations. Compared to Cyclone, Cuckoo is able to safely guard accesses to memory when programs are multithreaded. Similarly, Cuckoo is capable of enforcing memory safety in situations that are potentially troublesome for techniques such as SFI."
RICHARD WEST,A Virtual Deadline Scheduler for Window-Constrained Service Guarantees,"This paper presents a new approach to window-constrained scheduling, suitable for multimedia and weakly-hard real-time systems. We originally developed an algorithm, called Dynamic Window-Constrained Scheduling (DWCS), that attempts to guarantee no more than x out of y deadlines are missed for real-time jobs such as periodic CPU tasks, or delay-constrained packet streams. While DWCS is capable of generating a feasible window-constrained schedule that utilizes 100% of resources, it requires all jobs to have the same request periods (or intervals between successive service requests). We describe a new algorithm called Virtual Deadline Scheduling (VDS), that provides window-constrained service guarantees to jobs with potentially different request periods, while still maximizing resource utilization. VDS attempts to service m out of k job instances by their virtual deadlines, that may be some finite time after the corresponding real-time deadlines. Notwithstanding, VDS is capable of outperforming DWCS and similar algorithms, when servicing jobs with potentially different request periods. Additionally, VDS is able to limit the extent to which a fraction of all job instances are serviced late. Results from simulations show that VDS can provide better window-constrained service guarantees than other related algorithms, while still having as good or better delay bounds for all scheduled jobs. Finally, an implementation of VDS in the Linux kernel compares favorably against DWCS for a range of scheduling loads."
RICHARD WEST,Friendly Virtual Machine: Leveraging a Feedback-Control Model for Application Adaptation,"With the increased use of ""Virtual Machines"" (VMs) as vehicles that isolate applications running on the same host, it is necessary to devise techniques that enable multiple VMs to share underlying resources both fairly and efficiently. To that end, one common approach is to deploy complex resource management techniques in the hosting infrastructure. Alternately, in this paper, we advocate the use of self-adaptation in the VMs themselves based on feedback about resource usage and availability. Consequently, we define a ""Friendly"" VM (FVM) to be a virtual machine that adjusts its demand for system resources, so that they are both efficiently and fairly allocated to competing FVMs. Such properties are ensured using one of many provably convergent control rules, such as AIMD. By adopting this distributed application-based approach to resource management, it is not necessary to make assumptions about the underlying resources nor about the requirements of FVMs competing for these resources. To demonstrate the elegance and simplicity of our approach, we present a prototype implementation of our FVM framework in User-Mode Linux (UML)-an implementation that consists of less than 500 lines of code changes to UML. We present an analytic, control-theoretic model of FVM adaptation, which establishes convergence and fairness properties. These properties are also backed up with experimental results using our prototype FVM implementation."
RICHARD WEST,Mixed-criticality scheduling with I/O,"This paper addresses the problem of scheduling tasks with different criticality levels in the presence of I/O requests. In mixed-criticality scheduling, higher criticality tasks are given precedence over those of lower criticality when it is impossible to guarantee the schedulability of all tasks. While mixed-criticality scheduling has gained attention in recent years, most approaches typically assume a periodic task model. This assumption does not always hold in practice, especially for real-time and embedded systems that perform I/O. In prior work, we developed a scheduling technique in the Quest real-time operating system, which integrates the time-budgeted management of I/O operations with Sporadic Server scheduling of tasks. This paper extends our previous scheduling approach with support for mixed-criticality tasks and I/O requests on the same processing core. Results show that in a real implementation the mixed-criticality scheduling method introduced in this paper outperforms a scheduling approach consisting of only Sporadic Servers."
RICHARD WEST,Efficient End-Host Architecture for High Performance Communication Using User-level Sandboxing,"Current low-level networking abstractions on modern operating systems are commonly implemented in the kernel to provide sufficient performance for general purpose applications. However, it is desirable for high performance applications to have more control over the networking subsystem to support optimizations for their specific needs. One approach is to allow networking services to be implemented at user-level. Unfortunately, this typically incurs costs due to scheduling overheads and unnecessary data copying via the kernel. In this paper, we describe a method to implement efficient application-specific network service extensions at user-level, that removes the cost of scheduling and provides protected access to lower-level system abstractions. We present a networking implementation that, with minor modifications to the Linux kernel, passes data between ""sandboxed"" extensions and the Ethernet device without copying or processing in the kernel. Using this mechanism, we put a customizable networking stack into a user-level sandbox and show how it can be used to efficiently process and forward data via proxies, or intermediate hosts, in the communication path of high performance data streams. Unlike other user-level networking implementations, our method makes no special hardware requirements to avoid unnecessary data copies. Results show that we achieve a substantial increase in throughput over comparable user-space methods using our networking stack implementation."
RICHARD WEST,Boomerang: real-time I/O meets legacy systems,"This paper presents Boomerang, an I/O system that integrates a legacy non-real-time OS with one that is customized for timing-sensitive tasks. A relatively small RTOS benefits from the pre-existing libraries, drivers and services of the legacy system. Additionally, timing-critical tasks are isolated from less critical tasks by securely partitioning machine resources among the separate OSes. Boomerang guarantees end-to-end processing delays on input data that requires outputs to be generated within specific time bounds.We show how to construct composable task pipelines in Boomerang that combine functionality spanning a custom RTOS and a legacy Linux system. By dedicating time-critical I/O to the RTOS, we ensure that complementary services provided by Linux are sufficiently predictable to meet end-to-end service guarantees. While Boomerang benefits from spatial isolation, it also outperforms a standalone Linux system using deadline-based CPU reservations for pipeline tasks. We also show how Boomerang outperforms a virtualized system called ACRN, designed for automotive systems."
RICHARD WEST,smARTflight: an environmentally-aware adaptive real-time flight management system (best paper award),"Multi-rotor drones require real-time sensor data processing and control to maintain flight stability, which is made more challenging by external disturbances such as wind. In this paper we introduce smARTflight: an environmentally-aware adaptive real-time flight management system. smARTflight adapts the execution frequencies of flight control tasks according to timing and safety-critical constraints, in response to transient fluctuations of a drone’s attitude. In contrast to current state-of-the-art methods, smARTflight’s criticality-aware scheduler reduces the latency to return to a steady-state target attitude. The system also improves the overall control accuracy and lowers the frequency of adjustments to motor speeds to conserve power. A comparative case-study with a well-known autopilot shows that smARTflight reduces unnecessary control loop executions under stable conditions, while reducing response time latency by as much as 60% in a given axis of rotation when subjected to a 15° step attitude disturbance."
RICHARD WEST,PAStime: progress-aware scheduling for time-critical computing,"Over-estimation of worst-case execution times (WCETs) of real-time tasks leads to poor resource utilization. In a mixed-criticality system (MCS), the over-provisioning of CPU time to accommodate the WCETs of highly critical tasks may lead to degraded service for less critical tasks. In this paper we present PAStime, a novel approach to monitor and adapt the runtime progress of highly time-critical applications, to allow for improved service to lower criticality tasks. In PAStime, CPU time is allocated to time-critical tasks according to the delays they experience as they progress through their control flow graphs. This ensures that as much time as possible is made available to improve the Quality-of-Service of less critical tasks, while high-criticality tasks are compensated after their delays. This paper describes the integration of PAStime with Adaptive Mixed-criticality (AMC) scheduling. The LO-mode budget of a high-criticality task is adjusted according to the delay observed at execution checkpoints. This is the first implementation of AMC in the scheduling framework of LITMUS^RT, which is extended with our PAStime runtime policy and tested with real-time Linux applications such as object classification and detection. We observe in our experimental evaluation that AMC-PAStime significantly improves the utilization of the low-criticality tasks while guaranteeing service to high-criticality tasks."
RICHARD WEST,The Quest-V separation kernel for mixed criticality systems,"Multi- and many-core processors are becoming increasingly popular in embedded systems. Many of these processors now feature hardware virtualization capabilities, such as the ARM Cortex A15, and x86 processors with Intel VT-x or AMD-V support. Hardware virtualization offers opportunities to partition physical resources, including processor cores, memory and I/O devices amongst guest virtual machines. Mixed criticality systems and services can then co-exist on the same platform in separate virtual machines. However, traditional virtual machine systems are too expensive because of the costs of trapping into hypervisors to multiplex and manage machine physical resources on behalf of separate guests. For example, hypervisors are needed to schedule separate VMs on physical processor cores. In this paper, we discuss the design of the Quest-V separation kernel, that partitions services of different criticalities in separate virtual machines, or {\em sandboxes}. Each sandbox encapsulates a subset of machine physical resources that it manages without requiring intervention of a hypervisor. Moreover, a hypervisor is not needed for normal operation, except to bootstrap the system and establish communication channels between sandboxes."
RICHARD WEST,Predictable migration and communication in the Quest-V multikernal,"Quest-V is a system we have been developing from the ground up, with objectives focusing on safety, predictability and efficiency. It is designed to work on emerging multicore processors with hardware virtualization support. Quest-V is implemented as a ``distributed system on a chip'' and comprises multiple sandbox kernels. Sandbox kernels are isolated from one another in separate regions of physical memory, having access to a subset of processing cores and I/O devices. This partitioning prevents system failures in one sandbox affecting the operation of other sandboxes. Shared memory channels managed by system monitors enable inter-sandbox communication. The distributed nature of Quest-V means each sandbox has a separate physical clock, with all event timings being managed by per-core local timers. Each sandbox is responsible for its own scheduling and I/O management, without requiring intervention of a hypervisor. In this paper, we formulate bounds on inter-sandbox communication in the absence of a global scheduler or global system clock. We also describe how address space migration between sandboxes can be guaranteed without violating service constraints. Experimental results on a working system show the conditions under which Quest-V performs real-time communication and migration."
RICHARD WEST,Quest-V: a virtualized multikernel for safety-critical real-time systems,"Modern processors are increasingly featuring multiple cores, as well as support for hardware virtualization. While these processors are common in desktop and server-class computing, they are less prevalent in embedded and real-time systems. However, smartphones and tablet PCs are starting to feature multicore processors with hardware virtualization. If the trend continues, it is possible that future real-time systems will feature more sophisticated processor architectures. Future automotive or avionics systems, for example, could replace complex networks of uniprocessors with consolidated services on a smaller number of multicore processors. Likewise, virtualization could be used to isolate services and increase the availability of a system even when failures occur. This paper investigates whether advances in modern processor technologies offer new opportunities to rethink the design of real-time operating systems. We describe some of the design principles behind Quest-V, which is being used as an exploratory vehicle for real-time system design on multicore processors with hardware virtualization capabilities. While not all embedded systems should assume such features, a case can be made that more robust, safety-critical systems can be built to use hardware virtualization without incurring significant overheads."
RICHARD WEST,MARACAS: a real-time multicore VCPU scheduling framework,"This paper describes a multicore scheduling and load-balancing framework called MARACAS, to address shared cache and memory bus contention. It builds upon prior work centered around the concept of virtual CPU (VCPU) scheduling. Threads are associated with VCPUs that have periodically replenished time budgets. VCPUs are guaranteed to receive their periodic budgets even if they are migrated between cores. A load balancing algorithm ensures VCPUs are mapped to cores to fairly distribute surplus CPU cycles, after ensuring VCPU timing guarantees. MARACAS uses surplus cycles to throttle the execution of threads running on specific cores when memory contention exceeds a certain threshold. This enables threads on other cores to make better progress without interference from co-runners. Our scheduling framework features a novel memory-aware scheduling approach that uses performance counters to derive an average memory request latency. We show that latency-based memory throttling is more effective than rate-based memory access control in reducing bus contention. MARACAS also supports cache-aware scheduling and migration using page recoloring to improve performance isolation amongst VCPUs. Experiments show how MARACAS reduces multicore resource contention, leading to improved task progress."
RICHARD WEST,Reinforcement learning for UAV attitude control,"Autopilot systems are typically composed of an “inner loop” providing stability and control, whereas an “outer loop” is responsible for mission-level objectives, such as way-point navigation. Autopilot systems for unmanned aerial vehicles are predominately implemented using Proportional-Integral-Derivative (PID) control systems, which have demonstrated exceptional performance in stable environments. However, more sophisticated control is required to operate in unpredictable and harsh environments. Intelligent flight control systems is an active area of research addressing limitations of PID control most recently through the use of reinforcement learning (RL), which has had success in other applications, such as robotics. Yet previous work has focused primarily on using RL at the mission-level controller. In this work, we investigate the performance and accuracy of the inner control loop providing attitude control when using intelligent flight control systems trained with state-of-the-art RL algorithms—Deep Deterministic Policy Gradient, Trust Region Policy Optimization, and Proximal Policy Optimization. To investigate these unknowns, we first developed an open source high-fidelity simulation environment to train a flight controller attitude control of a quadrotor through RL. We then used our environment to compare their performance to that of a PID controller to identify if using RL is appropriate in high-precision, time-critical flight control."
MICHEL ANTEBY,The darker side of strong organizational cultures: looking forward by looking back,"Organizational cultures encompass the norms, values, and beliefs that guide the thinking and actions of organizational members. In this chapter, we highlight the moral power and ambiguity of such cultures. We review early research on organizational culture, and showcase its historical roots in moral questions around ideological control. We then trace how an emphasis on strong culture and firm performance slowly eroded these moral underpinnings. We also highlight specific studies that have surfaced the oft-forgotten moral consequences of these strong cultures. Next, we illustrate our argument with two research streams (i.e., research on person-organization “fit” and research on the culture of business schools) that reveal a darker, more insidious side, of strong organizational culture. The darker moral side occurs when the moral repercussions of organizational culture are masked by good intentions from management, internalized by employees as beneficial, and lead to harmful consequences for workers, firms, and/or society. Finally, we discuss how increased public awareness of the moral dimensions of work necessitate a deeper understanding of the moral implications of organizational culture."
MICHEL ANTEBY,Individuals' decision to co-donate or donate alone: an archival study of married whole body donors in Hawaii,"BACKGROUND: Human cadavers are crucial to numerous aspects of health care, including initial and continuing training of medical doctors and advancement of medical research. Concerns have periodically been raised about the limited number of whole body donations. Little is known, however, about a unique form of donation, namely co-donations or instances when married individuals decide to register at the same time as their spouse as whole body donors. Our study aims to determine the extent of whole body co-donation and individual factors that might influence co-donation. METHODS AND FINDINGS: We reviewed all records of registrants to the University of Hawaii Medical School’s whole body donation program from 1967 through 2006 to identify married registrants. We then examined the 806 married individuals’ characteristics to understand their decision to register alone or with their spouse. We found that married individuals who registered at the same time as their spouse accounted for 38.2 percent of married registrants. Sex differences provided an initial lens to understand co-donation. Wives were more likely to co-donate than to register alone (p = 0.002). Moreover, registrants’ main occupational background had a significant effect on co-donations (p = 0.001). Married registrants (regardless of sex) in female-gendered occupations were more likely to co-donate than to donate alone (p = 0.014). Femalegendered occupations were defined as ones in which women represented more than 55 percent of the workforce (e.g., preschool teachers). Thus, variations in donors’ occupational backgrounds explained co-donation above and beyond sex differences. CONCLUSIONS: Efforts to secure whole body donations have historically focused on individual donations regardless of donors’ marital status. More attention needs to be paid, however, to co-donations since they represent a non-trivial number of total donations. Also, targeted outreach efforts to male and female members of female-gendered occupations might prove a successful way to increase donations through co-donations."
STEPHEN I PELTON,Sialic acid mediated transcriptional modulation of a highly conserved sialometabolism gene cluster in Haemophilus influenzae and its effect on virulence,"BACKGROUND: Sialic acid has been shown to be a major virulence determinant in the pathogenesis of otitis media caused by the bacterium Haemophilus influenzae. This study aimed to characterise the expression of genes required for the metabolism of sialic acid and to investigate the role of these genes in virulence. RESULTS: Using qRT-PCR, we observed decreased transcriptional activity of genes within a cluster that are required for uptake and catabolism of 5-acetyl neuraminic acid (Neu5Ac), when bacteria were cultured in the presence of the sugar. We show that these uptake and catabolic genes, including a sialic acid regulatory gene (siaR), are highly conserved in the H. influenzae natural population. Mutant strains were constructed for seven of the nine genes and their influence upon LPS sialylation and resistance of the bacteria to the killing effect of normal human serum were assessed. Mutations in the Neu5Ac uptake (TRAP transporter) genes decreased virulence in the chinchilla model of otitis media, but the attenuation was strain dependent. In contrast, mutations in catabolism genes and genes regulating sialic acid metabolism (siaR and crp) did not attenuate virulence. CONCLUSION: The commensal and pathogenic behaviour of H. influenzae involves LPS sialylation that can be influenced by a complex regulatory interplay of sialometabolism genes."
KITT SHAFFER,Whorl: 2024,
KITT SHAFFER,Medical imaging,
TRACY BATTAGLIA,Boston University School of Medicine Class of 1996,
TRACY BATTAGLIA,"Chronic Disease and Its Risk Factors Among Refugees and Asylees in Massachusetts, 2001-2005","INTRODUCTION. Better understanding of the health problems of refugees and people who are granted political asylum (asylees) in the United States may facilitate successful resettlement. We examined the prevalence of risk factors for and diagnoses of chronic disease among these groups in Massachusetts. METHODS. We retrospectively analyzed health screening data from 4,239 adult refugees and asylees who arrived in Massachusetts from January 1, 2001, through December 31, 2005. We determined prevalence of obesity/overweight, hypertension, coronary artery disease (CAD), diabetes, and anemia. Analyses included multivariate logistic regression to determine associations between CAD and diabetes with region of origin. RESULTS. Almost half of our sample (46.8%) was obese/overweight, and 22.6% had hypertension. CAD, diabetes, and anemia were documented in 3.7%, 3.1%, and 12.8%, respectively. People from the Europe and Central Asia region were more likely than those from other regions to have CAD (odds ratio, 5.55; 95% confidence interval, 2.95-10.47). CONCLUSIONS. The prevalence of obesity/overweight and hypertension was high among refugees and asylees, but the prevalence of documented CAD and diabetes was low. We noted significant regional variations in prevalence of risk factors and chronic diseases. Future populations resettling in the United States should be linked to more resources to address their long-term health care needs and to receive culturally appropriate counseling on risk reduction."
TRACY BATTAGLIA,Disclosing Intimate Partner Violence to Health Care Clinicians - What a Difference the Setting Makes: A Qualitative Study,"BACKGROUND. Despite endorsement by national organizations, the impact of screening for intimate partner violence (IPV) is understudied, particularly as it occurs in different clinical settings. We analyzed interviews of IPV survivors to understand the risks and benefits of disclosing IPV to clinicians across specialties. METHODS. Participants were English-speaking female IPV survivors recruited through IPV programs in Massachusetts. In-depth interviews describing medical encounters related to abuse were analyzed for common themes using Grounded Theory qualitative research methods. Encounters with health care clinicians were categorized by outcome (IPV disclosure by patient, discovery evidenced by discussion of IPV by clinician without patient disclosure, or non-disclosure), attribute (beneficial, unhelpful, harmful), and specialty (emergency department (ED), primary care (PC), obstetrics/gynecology (OB/GYN)). RESULTS. Of 27 participants aged 18–56, 5 were white, 10 Latina, and 12 black. Of 59 relevant health care encounters, 23 were in ED, 17 in OB/GYN, and 19 in PC. Seven of 9 ED disclosures were characterized as unhelpful; the majority of disclosures in PC and OB/GYN were characterized as beneficial. There were no harmful disclosures in any setting. Unhelpful disclosures resulted in emotional distress and alienation from health care. Regardless of whether disclosure occurred, beneficial encounters were characterized by familiarity with the clinician, acknowledgement of the abuse, respect and relevant referrals. CONCLUSION. While no harms resulted from IPV disclosure, survivor satisfaction with disclosure is shaped by the setting of the encounter. Clinicians should aim to build a therapeutic relationship with IPV survivors that empowers and educates patients and does not demand disclosure."
TRACY BATTAGLIA,Strategies to promote language inclusion at 17 CTSA hubs,"The prioritization of English language in clinical research is a barrier to translational science. We explored promising practices to advance the inclusion of people who speak languages other than English in research conducted within and supported by NIH Clinical Translational Science Award (CTSA) hubs. Key informant interviews were conducted with representatives (n = 24) from CTSA hubs (n = 17). Purposive sampling was used to identify CTSA hubs focused on language inclusion. Hubs electing to participate were interviewed via Zoom. Thematic analysis was performed to analyze interview transcripts. We report on strategies employed by hubs to advance linguistic inclusion and influence institutional change that were identified. Strategies ranged from translations, development of culturally relevant materials and consultations to policies and procedural changes and workforce initiatives. An existing framework was adapted to conceptualize hub strategies. Language justice is paramount to bringing more effective treatments to all people more quickly. Inclusion will require institutional transformation and CTSA hubs are well positioned to catalyze change."
TRACY BATTAGLIA,Community advisory board members’ perspectives on their contributions to a large multistate cluster RCT: a mixed methods study,"BACKGROUND: Community advisory boards (CABs) are an established approach to ensuring research reflects community priorities. This paper examines two CABs that are part of the HEALing Communities Study which aims to reduce overdose mortality. This analysis aimed to understand CAB members’ expectations, experiences, and perspectives on CAB structure, communication, facilitation, and effectiveness during the first year of an almost fully remote CAB implementation. Current literature exploring these perspectives is limited. METHODS: We collected qualitative and survey data simultaneously from members (n = 53) of two sites’ CABs in the first 9 months of CAB development. The survey assessed trust, communication, and relations; we also conducted 32 semi-structured interviews. We analyzed the survey results descriptively. The qualitative data were analyzed using a deductive codebook based on the RE-AIM PRISM framework. Themes were drawn from the combined qualitative data and triangulated with survey results to further enrich the findings. RESULTS: CAB members expressed strong commitment to overall study goals and valued the representation of occupational sectors. The qualitative data described a dissonance between CAB members’ commitment to the mission and unmet expectations for influencing the study within an advisory role. Survey results indicated lower satisfaction with the research teams’ ability to create a mutually beneficial process, clear communication, and sharing of power. CONCLUSION: Building a CAB on a remote platform, within a study utilizing a community engagement strategy, still presents challenges to fully realizing the potential of a CAB. These findings can inform more effective operationalizing of community-engaged research through enhanced CAB engagement."
TRACY BATTAGLIA,Community engagement and financial arrangements: navigating institutional change,"Despite their documented benefits, the widespread adoption of community-engaged and participatory approaches among health researchers remains limited. Institutional practices and policies influence the uptake of community engagement and participatory approaches. We examine the role of financial arrangements between university researchers and community partners, by exploring efforts to bridge the gap between research administration and researchers at two research-intensive institutions. The type of financial arrangement a researcher has with a community partner plays an important role in setting the stage for the structure of the partnership as it relates to shared decision-making and ownership of the research. Continued efforts to clarify and streamline subcontracting processes are needed as is infrastructure to support community partners and researchers as they navigate financial arrangements if progress is to be made."
ROBERT WEXELBLATT,On hypocrisy,An informal essay on the topic of hypocrisy
ROBERT WEXELBLATT,"Bostonia: v. 60, no. 1-3",
IRENA VODENSKA,Community analysis of global financial markets,"We analyze the daily returns of stock market indices and currencies of 56 countries over the period of 2002–2012. We build a network model consisting of two layers, one being the stock market indices and the other the foreign exchange markets. Synchronous and lagged correlations are used as measures of connectivity and causality among different parts of the global economic system for two different time intervals: non-crisis (2002–2006) and crisis (2007–2012) periods. We study community formations within the network to understand the influences and vulnerabilities of specific countries or groups of countries. We observe different behavior of the cross correlations and communities for crisis vs. non-crisis periods. For example, the overall correlation of stock markets increases during crisis while the overall correlation in the foreign exchange market and the correlation between stock and foreign exchange markets decrease, which leads to different community structures. We observe that the euro, while being central during the relatively calm period, loses its dominant role during crisis. Furthermore we discover that the troubled Eurozone countries, Portugal, Italy, Greece and Spain, form their own cluster during the crisis period."
IRENA VODENSKA,Interdependencies and causalities in coupled financial networks,"We explore the foreign exchange and stock market networks for 48 countries from 1999 to 2012 and propose a model, based on complex Hilbert principal component analysis, for extracting significant lead-lag relationships between these markets. The global set of countries, including large and small countries in Europe, the Americas, Asia, and the Middle East, is contrasted with the limited scopes of targets, e.g., G5, G7 or the emerging Asian countries, adopted by previous works. We construct a coupled synchronization network, perform community analysis, and identify formation of four distinct network communities that are relatively stable over time. In addition to investigating the entire period, we divide the time period into into “mild crisis,” (1999–2002), “calm,” (2003–2006) and “severe crisis” (2007–2012) sub-periods and find that the severe crisis period behavior dominates the dynamics in the foreign exchange-equity synchronization network. We observe that in general the foreign exchange market has predictive power for the global stock market performances. In addition, the United States, German and Mexican markets have forecasting power for the performances of other global equity markets."
IRENA VODENSKA,Analysis of cryptocurrency dependencies,"In a world where no country, market, or economy is an island, interconnectivity is becoming a fundamental feature of almost all social and economic systems. In the case of digital assets like cryptocurrencies, the impact of interconnectivity on their performance and price trajectory is amplified. Studying these phenomena is essential for understanding the processes driving the crypto-markets. In this paper, we propose seven different approaches to create a network of eighteen most important cryptocurrencies. The first three approaches discover correlations between cryptocurrencies based on their daily prices, daily returns, and sentiment extracted from Reddit data. The following two approaches offer insights from the frequency of joint appearance of cryptocurrencies in Google news and Reddit data. The remaining two approaches determine each cryptocurrency’s impact over the others when forecasting prices and returns. Furthermore, we explore the networks’ interdependencies to explore the similarities of the cryptocurrency networks generated by different approaches. The proposed methodology allows us to understand the dynamics in the cryptocurrency markets and the different processes that influence their performance."
IRENA VODENSKA,Challenges and opportunities in ESG investments,
IRENA VODENSKA,Examining mental illness trends in the United States from 2006 to 2019,We investigate the characteristics of medical expenditures associated with mental illness hospitalizations using the Truven Health MarketScan Database. We focus on the inpatient admissions due to mental illness of adults aged 1S to 64 between 2006 to 2019. We aim to answer the following questions: (1) Did the financial crisis of 2008 impact mental health in the U.S.?(2) What are the other macro-level (socioeconomic and regulartory) and micro-level (individualpatient related) factors that affect the cost of inpatient care due to mental illness; (3) Did mental illness affect men and women differently? (4) How were different regions within the U.S. affected by mental illness?
IRENA VODENSKA,Partial correlation analysis: applications for financial markets,"The presence of significant cross-correlations between the synchronous time evolution of a pair of equity returns is a well-known empirical fact. The Pearson correlation is commonly used to indicate the level of similarity in the price changes for a given pair of stocks, but it does not measure whether other stocks influence the relationship between them. To explore the influence of a third stock on the relationship between two stocks, we use a partial correlation measurement to determine the underlying relationships between financial assets. Building on previous work, we present a statistically robust approach to extract the underlying relationships between stocks from four different financial markets: the United States, the United Kingdom, Japan, and India. This methodology provides new insights into financial market dynamics and uncovers implicit influences in play between stocks. To demonstrate the capabilities of this methodology, we (i) quantify the influence of different companies and, by studying market similarity across time, present new insights into market structure and market stability, and (ii) we present a practical application, which provides information on the how a company is influenced by different economic sectors, and how the sectors interact with each other. These examples demonstrate the effectiveness of this methodology in uncovering information valuable for a range of individuals, including not only investors and traders but also regulators and policy makers."
IRENA VODENSKA,Systemic stress test model for shared portfolio networks,"We propose a dynamic model for systemic risk using a bipartite network of banks and assets in which the weight of links and node attributes vary over time. Using market data and bank asset holdings, we are able to estimate a single parameter as an indicator of the stability of the financial system. We apply the model to the European sovereign debt crisis and observe that the results closely match real-world events (e.g., the high risk of Greek sovereign bonds and the distress of Greek banks). Our model could become complementary to existing stress tests, incorporating the contribution of interconnectivity of the banks to systemic risk in time-dependent networks. Additionally, we propose an institutional systemic importance ranking, BankRank, for the financial institutions analyzed in this study to assess the contribution of individual banks to the overall systemic risk."
IRENA VODENSKA,Relationship between macroeconomic indicators and economic cycles in U.S.,"We analyze monthly time series of 57 US macroeconomic indicators (18 leading, 30 coincidental, and 9 lagging) and 5 other trade/money indexes. Using novel methods, we confirm statistically significant co-movements among these time series and identify noteworthy economic events. The methods we use are Complex Hilbert Principal Component Analysis (CHPCA) and Rotational Random Shuffling (RRS). We obtain significant complex correlations among the US economic indicators with leads/lags. We then use the Hodge decomposition to obtain the hierarchical order of each time series. The Hodge potential allows us to better understand the lead/lag relationships. Using both CHPCA and Hodge decomposition approaches, we obtain a new lead/lag order of the macroeconomic indicators and perform clustering analysis for positively serially correlated positive and negative changes of the analyzed indicators. We identify collective negative co-movements around the Dot.com bubble in 2001 as well as the Global Financial Crisis (GFC) in October 2008. We also identify important events such as the Hurricane Katrina in August 2005 and the Oil Price Crisis in July 2008. Additionally, we demonstrate that some coincidental and lagging indicators actually show leading indicator characteristics. This suggests that there is a room for existing indicators to be improved."
IRENA VODENSKA,Cohesiveness in financial news and its relation to market volatility,"Motivated by recent financial crises, significant research efforts have been put into studying contagion effects and herding behaviour in financial markets. Much less has been said regarding the influence of financial news on financial markets. We propose a novel measure of collective behaviour based on financial news on the Web, the News Cohesiveness Index (NCI), and we demonstrate that the index can be used as a financial market volatility indicator. We evaluate the NCI using financial documents from large Web news sources on a daily basis from October 2011 to July 2013 and analyse the interplay between financial markets and finance-related news. We hypothesise that strong cohesion in financial news reflects movements in the financial markets. Our results indicate that cohesiveness in financial news is highly correlated with and driven by volatility in financial markets."
IRENA VODENSKA,New measures of journal impact based on the number of citations and PageRank,"The number of citations has been used for measuring the significance of a paper. Moreover, we have the following question: which paper is the most important if there are some papers with the same number of citations? Some measures have been introduced to answer this question: one of them is PageRank. We use the Science Citation Index Expanded from 1981 to 2015 to calculate the number of citations and the Google number in the citation network consisting of 34,666,719 papers and 591,321,826 citations. We clarify the positive linear relationship between the number of citations and the Google number, as well as extract some outliers from this positive linear relationship. These outliers are considered to be extremely prestigious papers. Furthermore, we calculate the mean values of the number of citations and the Google number for all journals, construct a new measure of journal influence, and extract extremely prestigious journals. This new measure has a positive and medium correlation with the impact factor, Eigenfactor score, and SCImago Journal Rank."
IRENA VODENSKA,Socio-economic impact of the Covid-19 pandemic,"This paper proposes a dynamic cascade model to investigate the systemic risk posed by sector-level industries within the U.S. inter-industry network. We then use this model to study the effect of the disruptions presented by Covid-19 on the U.S. economy. We construct a weighted digraph G = (V,E,W) using the industry-by-industry total requirements table for 2018, provided by the Bureau of Economic Analysis (BEA). We impose an initial shock that disrupts the production capacity of one or more industries, and we calculate the propagation of production shortages with a modified Cobb–Douglas production function. For the Covid-19 case, we model the initial shock based on the loss of labor between March and April 2020 as reported by the Bureau of Labor Statistics (BLS). The industries within the network are assigned a resilience that determines the ability of an industry to absorb input losses, such that if the rate of input loss exceeds the resilience, the industry fails, and its outputs go to zero. We observed a critical resilience, such that, below this critical value, the network experienced a catastrophic cascade resulting in total network collapse. Lastly, we model the economic recovery from June 2020 through March 2021 using BLS data."
IRENA VODENSKA,Multiple tipping points and optimal repairing in interacting networks,"Systems composed of many interacting dynamical networks—such as the human body with its biological networks or the global economic network consisting of regional clusters—often exhibit complicated collective dynamics. Three fundamental processes that are typically present are failure, damage spread and recovery. Here we develop a model for such systems and find a very rich phase diagram that becomes increasingly more complex as the number of interacting networks increases. In the simplest example of two interacting networks we find two critical points, four triple points, ten allowed transitions and two ‘forbidden’ transitions, as well as complex hysteresis loops. Remarkably, we find that triple points play the dominant role in constructing the optimal repairing strategy in damaged interacting systems. To test our model, we analyse an example of real interacting financial networks and find evidence of rapid dynamical transitions between well-defined states, in agreement with the predictions of our model."
IRENA VODENSKA,Inferring short-term volatility indicators from the Bitcoin blockchain,"In this paper, we study the possibility of inferring early warning indicators (EWIs) for periods of extreme bitcoin price volatility using features obtained from Bitcoin daily transaction graphs. We infer the low-dimensional representations of transaction graphs in the time period from 2012 to 2017 using Bitcoin blockchain, and demonstrate how these representations can be used to predict extreme price volatility events. Our EWI, which is obtained with a non-negative decomposition, contains more predictive information than those obtained with singular value decomposition or scalar value of the total Bitcoin transaction volume."
IRENA VODENSKA,Enhanced news sentiment analysis using deep learning methods,"We explore the predictive power of historical news sentiments based on financial market performance to forecast financial news sentiments. We define news sentiments based on stock price returns averaged over one minute right after a news article has been released. If the stock price exhibits positive (negative) return, we classify the news article released just prior to the observed stock return as positive (negative). We use Wikipedia and Gigaword five corpus articles from 2014 and we apply the global vectors for word representation method to this corpus to create word vectors to use as inputs into the deep learning TensorFlow network. We analyze high-frequency (intraday) Thompson Reuters News Archive as well as the high-frequency price tick history of the Dow Jones Industrial Average (DJIA 30) Index individual stocks for the period between 1/1/2003 and 12/30/2013. We apply a combination of deep learning methodologies of recurrent neural network with long short-term memory units to train the Thompson Reuters News Archive Data from 2003 to 2012, and we test the forecasting power of our method on 2013 News Archive data. We find that the forecasting accuracy of our methodology improves when we switch from random selection of positive and negative news to selecting the news with highest positive scores as positive news and news with highest negative scores as negative news to create our training data set."
IRENA VODENSKA,Classical mechanics of economic networks,"Financial networks are dynamic. To assess their systemic importance to the world-wide economic network and avert losses we need models that take the time variations of the links and nodes into account. Using the methodology of classical mechanics and Laplacian determinism we develop a model that can predict the response of the financial network to a shock. We also propose a way of measuring the systemic importance of the banks, which we call BankRank. Using European Bank Authority 2011 stress test exposure data, we apply our model to the bipartite network connecting the largest institutional debt holders of the troubled European countries (Greece, Italy, Portugal, Spain, and Ireland). From simulating our model we can determine whether a network is in a ""stable"" state in which shocks do not cause major losses, or a ""unstable"" state in which devastating damages occur. Fitting the parameters of the model, which play the role of physical coupling constants, to Eurozone crisis data shows that before the Eurozone crisis the system was mostly in a ""stable"" regime, and that during the crisis it transitioned into an ""unstable"" regime. The numerical solutions produced by our model match closely the actual time-line of events of the crisis. We also find that, while the largest holders are usually more important, in the unstable regime smaller holders also exhibit systemic importance. Our model also proves useful for determining the vulnerability of banks and assets to shocks. This suggests that our model may be a useful tool for simulating the response dynamics of shared portfolio networks."
IRENA VODENSKA,Predicting companies stock price direction by using sentiment analysis of news articles,"This paper summarizes our experience teaching several courses at Metropolitan College of Boston University Computer Science department over five years. A number of innovative teaching techniques are presented in this paper. We specifically address the role of a project archive, when designing a course. This research paper explores survey results from every running of courses, from 2014 to 2019. During each class, students participated in two distinct surveys: first, dealing with key learning outcomes, and, second, with teaching techniques used. This paper makes several practical recommendations based on the analysis of collected data. The research validates the value of a sound repository of technical term projects and the role such repository plays in effective teaching and learning of computer science courses."
IRENA VODENSKA,Cascading failures in bi-partite graphs: model for systemic risk propagation,"As economic entities become increasingly interconnected, a shock in a financial network can provoke significant cascading failures throughout the system. To study the systemic risk of financial systems, we create a bi-partite banking network model composed of banks and bank assets and propose a cascading failure model to describe the risk propagation process during crises. We empirically test the model with 2007 US commercial banks balance sheet data and compare the model prediction of the failed banks with the real failed banks after 2007. We find that our model efficiently identifies a significant portion of the actual failed banks reported by Federal Deposit Insurance Corporation. The results suggest that this model could be useful for systemic risk stress testing for financial systems. The model also identifies that commercial rather than residential real estate assets are major culprits for the failure of over 350 US commercial banks during 2008–2011."
IRENA VODENSKA,From stress testing to systemic stress testing: the importance of macroprudential regulation,"Stability of the banking system and macroprudential regulation are essential for healthy economic growth. In this paper we study the European bank network and its vulnerability to stressing different bank assets. The importance of macroprudential policy is emphasized by the inherent vulnerability of the financial system, high level of leverage, interconnectivity of system's entities, similar risk exposure of financial institutions, and susceptibility for systemic crisis propagation through the system. Current stress tests conducted by the European Banking Authority do not take in consideration the connectivity of the banks and the potential of one bank vulnerability spilling over to the rest of the system. We create a bipartite network with bank nodes on one hand and asset nodes on the other with weighted links between the two layers based on the level of different countries’ sovereign debt holdings by each bank. We propose a model for systemic risk propagation based on common bank exposures to specific asset classes. We introduce the similarity in asset distribution among the banks as a measure of bank closeness. We link the closeness of asset distributions to the likelihood that banks will experience a similar level and type of distress in a given adverse scenario. We analyze the dynamics of tier 1 capital ratio after stressing the bank network and find that while the system is able to withstand shocks for a wide range of parameters, we identify a critical threshold for both asset risk and bank response to a shock beyond which the system transitions from stable to unstable."
IRENA VODENSKA,Confidence and self-attribution bias in an artificial stock market,"Using an agent-based model we examine the dynamics of stock price fluctuations and their rates of return in an artificial financial market composed of fundamentalist and chartist agents with and without confidence. We find that chartist agents who are confident generate higher price and rate of return volatilities than those who are not. We also find that kurtosis and skewness are lower in our simulation study of agents who are not confident. We show that the stock price and confidence index—both generated by our model—are cointegrated and that stock price affects confidence index but confidence index does not affect stock price. We next compare the results of our model with the S&P 500 index and its respective stock market confidence index using cointegration and Granger tests. As in our model, we find that stock prices drive their respective confidence indices, but that the opposite relationship, i.e., the assumption that confidence indices drive stock prices, is not significant."
IRENA VODENSKA,Economic and political effects on currency clustering,"We propose a new measure named the symbolic performance to better understand the structure of foreign exchange markets. Instead of considering currency pairs, we isolate a quantity that describes each currency’s position in the market, independent of a base currency. We apply the k-means++ clustering algorithm to analyze how the roles of currencies change over time, from reference status or minimal apprecia- tions and depreciations with respect to other currencies to large appreciations and depreciations. We show how diﬀerent central bank interventions and economic and political developments, such as the cap on the Swiss franc to the euro enforced by the Swiss National Bank or the Brexit vote, aﬀect the position of a currency in the global foreign exchange market."
IRENA VODENSKA,Bitcoin price prediction using transfer learning on financial micro-blogs,"We present a methodology for predicting the price of Bitcoin using Twitter data and historical Bitcoin prices. Bitcoin is the largest cryptocurrency that, in terms of market capitalization, represents over 110 billion dollars. The news volume is rapidly growing, and Twitter is increasingly used as a news source influencing purchase decisions by informing users of the currency and its popularity. Using modern Natural Language Processing models for transfer learning, we analyze tweets’ meaning and calculate sentiment using the NLP transformers. We combine the daily historical Bitcoin price data with the daily sentiment and predict the next day’s price using auto-regressive models for time-series forecasting. The results show that modern approaches for sentiment analysis, time-series forecasting, and transfer-learning are applicable for predicting Bitcoin price when we include sentiment extracted from financial micro-blogs as input. The results show improvement when compared to the old approaches using only historical price data. Additionally, we show that the NLP models based on transfer-learning methodologies improve the efficiency in sentiment extraction in financial micro-blogs compared to standard sentiment extraction methods."
IRENA VODENSKA,Comparing the performance of ChatGPT and state-of-the-art climate NLP models on climate-related text classification tasks,"Recently, there has been a surge in general-purpose language models, with ChatGPT being the most advanced model to date. These models are primarily used for generating text in response to user prompts on various topics. It needs to be validated how accurate and relevant the generated text from ChatGPT is on the specific topics, as it is designed for general conversation and not for context-specific purposes. This study explores how ChatGPT, as a general-purpose model, performs in the context of a real-world challenge such as climate change compared to ClimateBert, a state-of-the-art language model specifically trained on climate-related data from various sources, including texts, news, and papers. ClimateBert is fine-tuned on five different NLP classification tasks, making it a valuable benchmark for comparison with the ChatGPT on various NLP tasks. The main results show that for climate-specific NLP tasks, ClimateBert outperforms ChatGPT."
IRENA VODENSKA,Using NLP transformer models to evaluate the relationship between ethical principles in finance and machine learning,
IRENA VODENSKA,Post-COVID-19 depression prediction using Twitter data,
IRENA VODENSKA,Mitigation of cascading failures in complex networks,"Cascading failures in many systems such as infrastructures or financial networks can lead to catastrophic system collapse. We develop here an intuitive, powerful and simple-to-implement approach for mitigation of cascading failures on complex networks based on local network structure. We offer an algorithm to select critical nodes, the protection of which ensures better survival of the network. We demonstrate the strength of our approach compared to various standard mitigation techniques. We show the efficacy of our method on various network structures and failure mechanisms, and finally demonstrate its merit on an example of a real network of financial holdings."
IRENA VODENSKA,Crises and physical phases of a bipartite market model,"We analyze the linear response of a market network to shocks based on the bipartite market model we introduced in an earlier paper, which we claimed to be able to identify the time-line of the 2009-2011 Eurozone crisis correctly. We show that this model has three distinct phases that can broadly be categorized as ""stable"" and ""unstable"". Based on the interpretation of our behavioral parameters, the stable phase describes periods where investors and traders have confidence in the market (e.g. predict that the market rebounds from a loss). We show that the unstable phase happens when there is a lack of confidence and seems to describe ""boom-bust"" periods in which changes in prices are exponential. We analytically derive these phases and where the phase transition happens using a mean field approximation of the model. We show that the condition for stability is αβ<1 with α being the inverse of the ""price elasticity"" and β the ""income elasticity of demand"", which measures how rash the investors make decisions. We also show that in the mean-field limit this model reduces to the Langevin model by Bouchaud et al. for price returns."
STEPHANIE CURENTON-JOLLY,"Conversations in early childhood classrooms: review of literature, preliminary findings from a professional development intervention, and policy suggestions","This chapter focuses on classroom conversations during the early childhood years, before formal schooling. We argue that encouraging high-quality conversations between teachers and young children is consistent with empirical findings and professional wisdom demonstrating that such conversations are positively related to children’s language outcomes. Classroom discourse is at the core of strong pedagogy and practice. Yet, few professional development trainings and/or college courses specifically focus on how conversation can be used as an instructional tool. Analyses throughout the article point to the urgency for policy makers to invest in the education and training of future early childhood teachers (both pre-service and in-service) around this issue, especially for those teaching culturally and linguistically diverse learners."
MICHELLE HENSHAW,Assessing the Oral Health Needs of Public Housing Residents,"Objectives: “Tooth Smart Healthy Start” is a randomized clinical trial which aims to reduce the incidence of early childhood caries (ECC) in Boston public housing residents as part of the NIH funded Northeast Center for Research to Evaluate and Eliminate Dental Disparities. The purpose of this project was to assess public housing stakeholders' perception of the oral health needs of public housing residents and their interest in replicating “Tooth Smart Healthy Start” in other public housing sites across the nation. Methods: The target population was the 180 attendees of the 2010 meeting of the Health Care for Residents of Public Housing National Conference. A ten question survey which assessed conference attendees' beliefs about oral health and its importance to public housing residents was distributed. Data was analyzed using SAS 9.1. Descriptive statistics were calculated for each variable and results were stratified by participants' roles. Results: Thirty percent of conference attendees completed the survey. The participants consisted of residents, agency representatives, and housing authority personnel. When asked to rank health issues facing public housing residents, oral health was rated as most important (42%) or top three (16%) by residents. The agency representatives and housing authority personnel rated oral health among the top three (33% and 58% respectively) and top five (36% and 25% respectively). When participants ranked the three greatest resident health needs out of eight choices, oral health was the most common response. Majority of the participants expressed interest in replicating the “Tooth Smart Healthy Start” program at their sites. Conclusion: All stakeholder groups identified oral health as one of the greatest health needs of residents in public housing. Furthermore, if shown to reduce ECC, there is significant interest in implementing the program amongst key public housing stakeholders across the nation."
NINA S SILBER,"IMPACT: The Journal of the Center for Interdisciplinary Teaching and Learning. Volume 7, Issue 2, Summer 2018","In the weeks and months following August 12, 2017, members of the Boston University community struggled — like Americans everywhere — to comprehend the series of troubling, and tragic, events which would come, almost immediately, to be denoted in the national imagination by the metonym “Charlottesville.” This special issue of Impact: The Journal of the Center for Interdisciplinary Teaching & Learning comprises a series of responses to these events and their aftermath, as well as the conditions which enabled them, by faculty members from across the BU campus."
NINA S SILBER,Reflections on Charlottesville,"Historical context surrounding the confrontation in Charlottesville, Virginia, in August, 2017, regarding confederate monuments."
NINA S SILBER,"""Civil War Cinema in New Deal America""","During the early decades of the 20th century, Hollywood filmmakers both shaped and reflected the popular understanding of the Confederacy, slavery, and Abraham Lincoln."
NINA S SILBER,Revisiting Mary Massey’s Bonnet Brigades,
MICHAEL C CARAMANIS,Shift factor-based SCOPF topology control MIP formulations with substation configurations,"Topology control (TC) is an effective tool for managing congestion, contingency events, and overload control. The majority of TC research has focused on line and transformer switching. Substation reconfiguration is an additional TC action, which consists of opening or closing breakers not in series with lines or transformers. Some reconfiguration actions can be simpler to implement than branch opening, seen as a less invasive action. This paper introduces two formulations that incorporate substation reconfiguration with branch opening in a unified TC framework. The first method starts from a topology with all candidate breakers open, and breaker closing is emulated and optimized using virtual transactions. The second method takes the opposite approach, starting from a fully closed topology and optimizing breaker openings. We provide a theoretical framework for both methods and formulate security-constrained shift factor MIP TC formulations that incorporate both breaker and branch switching. By maintaining the shift factor formulation, we take advantage of its compactness, especially in the context of contingency constraints, and by focusing on reconfiguring substations, we hope to provide system operators additional flexibility in their TC decision processes. Simulation results on a subarea of PJM illustrate the application of the two formulations to realistic systems."
MICHAEL C CARAMANIS,Learning from past bids to participate strategically in day-ahead electricity markets,"We consider the process of bidding by electricity suppliers in a day-ahead market context, where each supplier bids a linear non-decreasing function of her generating capacity with the goal of maximizing her individual profit given other competing suppliers' bids. Based on the submitted bids, the market operator schedules suppliers to meet demand during each hour and determines hourly market clearing prices. Eventually, this game-theoretic process reaches a Nash equilibrium when no supplier is motivated to modify her bid. However, solving the individual profit maximization problem requires information of rivals' bids, which are typically not available. To address this issue, we develop an inverse optimization approach for estimating rivals' production cost functions given historical market clearing prices and production levels. We then use these functions to bid strategically and compute Nash equilibrium bids. We present numerical experiments illustrating our methodology, showing good agreement between bids based on the estimated production cost functions with the bids based on the true cost functions. We discuss an extension of our approach that takes into account network congestion resulting in location-dependent prices"
MICHAEL C CARAMANIS,An optimal transmission line switching and bus splitting heuristic incorporating AC and N-1 contingency constraints,"Optimal transmission line switching and/or bus splitting is shown to contribute in relieving congestion and reducing the operation cost by rerouting power flows throughout the network. Although bus splitting may be as powerful as line switching in congestion mitigation and is typically considered a smaller disturbance compared with line switching, it has received less attention in the literature in part due to the more complicated node-breaker modeling requirement. In this paper, an optimal transmission line switching and bus splitting heuristic is presented to minimize the operation cost while respecting AC and N-1 contingency constraints. We present a two-level solution method where switching decisions are made in the upper level problem formulated as a mixed integer second order cone programming master problem, while the resulting network topology is checked against AC and N-1 contingency constraints in lower level subproblems. Line switching and bus splitting are modeled as switching actions assuming double-bus double-breaker substation arrangements where all elements at a substation, including generators, loads, lines and shunt elements, are given switches to connect to either of the busbars if the respective substation is split. We also introduce additional constraints to model a breaker-and-a-half substation scheme. Furthermore, a pre-screening step is presented to limit the search space of the problem, thus accelerating the solution process. We demonstrate the application of the proposed method on IEEE standard test systems."
MICHAEL C CARAMANIS,Computation of Convex Hull prices in electricity markets with non-convexities using Dantzig-Wolfe decomposition,"The presence of non-convexities in electricity markets has been an active research area for about two decades. The — inevitable under current marginal cost pricing — problem of guaranteeing that no market participant incurs losses in the day-ahead market is addressed in current practice through make-whole payments a.k.a. uplift. Alternative pricing rules have been studied to deal with this problem. Among them, Convex Hull (CH) prices associated with minimum uplift have attracted significant attention. Several US Independent System Operators (ISOs) have considered CH prices but resorted to approximations, mainly because determining exact CH prices is computationally challenging, while providing little intuition about the price formation rationale. In this paper, we describe the CH price estimation problem by relying on Dantzig-Wolfe decomposition and Column Generation, as a tractable, highly paralellizable, and exact method — i.e., yielding exact, not approximate, CH prices — with guaranteed finite convergence. Moreover, the approach provides intuition on the underlying price formation rationale. A test bed of stylized examples provide an exposition of the intuition in the CH price formation. In addition, a realistic ISO dataset is used to support scalability and validate the proof-of-concept."
MICHAEL C CARAMANIS,A Riemannian augmented Lagrangian method for the optimal power flow problem in radial distribution networks,
EMANUEL KATZ,Indicators of conformal field theory: Entanglement entropy and multiple-point correlators,"The entanglement entropy (EE) of quantum systems is often used as a test of low-energy descriptions by conformal field theory (CFT). Here we point out that this is not a reliable indicator, as the EE often shows the same behavior even when a CFT description is not correct (as long as the system is asymptotically scale-invariant). We use constraints on the scaling dimension given by the CFT with SU(2) symmetry to provide alternative tests with two- and four-point correlation functions, showing examples for quantum spin models in 1+1 dimensions. In the case of a critical amplitude-product state expressed in the valence-bond basis (where the amplitudes decay as a power law of the bond length and the wave function is the product of all bond amplitudes), we show that even though the EE exhibits the expected CFT behavior, there is no CFT description of this state. We provide numerical tests of the behavior predicted by CFT for the correlation functions in the critical transverse-field Ising chain and the J−Q spin chain, where the conformal structure is well understood. That behavior is not reproduced in the amplitude-product state."
EMANUEL KATZ,Numerical investigations of SO(4) emergent extended symmetry in spin-1/2 Heisenberg antiferromagnetic chains,"The antiferromagnetic Heisenberg chain is expected to have an extended symmetry, [SU(2)×SU(2)]/Z2, in the infrared limit whose physical interpretation is that the spin and dimer order parameters form the components of a common four-dimensional pseudovector. Here we numerically investigate this emergent symmetry using quantum Monte Carlo simulations of a modified Heisenberg chain (the J−Q model) in which the logarithmic scaling corrections of the conventional Heisenberg chain can be avoided. We show how the two- and three-point spin and dimer correlation functions approach their forms constrained by conformal field theory as the system size increases and numerically confirm the expected effects of the extended symmetry on various correlation functions. We stress that sometimes the leading power laws of three-point (and higher) correlations are not given simply by the scaling dimensions of the lattice operators involved but can be faster decaying because of exact cancellations of contributions from the fields and currents under conformal symmetry."
EMANUEL KATZ,Nonperturbative matching between equal-time and lightcone quantization,"We investigate the nonperturbative relation between lightcone (LC) and standard equal-time (ET) quantization in the context of λϕ4 theory in d = 2. We discuss the perturbative matching between bare parameters and the failure of its naive nonperturbative extension. We argue that they are nevertheless the same theory nonperturbatively, and that furthermore the nonperturbative map between bare parameters can be extracted from ET perturbation theory via Borel resummation of the mass gap. We test this map by using it to compare physical quantities computed using numerical Hamiltonian truncation methods in ET and LC."
EMANUEL KATZ,Dynamic trapping near a quantum critical point,"The study of dynamics in closed quantum systems has been revitalized by the emergence of experimental systems that are well-isolated from their environment. In this paper, we consider the closed-system dynamics of an archetypal model: spins driven across a second-order quantum critical point, which are traditionally described by the Kibble-Zurek mechanism. Imbuing the driving field with Newtonian dynamics, we find that the full closed system exhibits a robust new phenomenon—dynamic critical trapping—in which the system is self-trapped near the critical point due to efficient absorption of field kinetic energy by heating the quantum spins. We quantify limits in which this phenomenon can be observed and generalize these results by developing a Kibble-Zurek scaling theory that incorporates the dynamic field. Our findings can potentially be interesting in the context of early universe physics, where the role of the driving field is played by the inflaton or a modulus field."
EMANUEL KATZ,Introduction to lightcone conformal truncation: QFT dynamics from CFT data,"We both review and augment the lightcone conformal truncation (LCT) method. LCT is a Hamiltonian truncation method for calculating dynamical quantities in QFT in infinite volume. This document is a self-contained, pedagogical introduction and ""how-to"" manual for LCT. We focus on 2D QFTs which have UV descriptions as free CFTs containing scalars, fermions, and gauge fields, providing a rich starting arena for LCT applications. Along our way, we develop several new techniques and innovations that greatly enhance the efficiency and applicability of LCT. These include the development of CFT radial quantization methods for computing Hamiltonian matrix elements and a new SUSY-inspired way of avoiding state-dependent counterterms and maintaining chiral symmetry. We walk readers through the construction of their own basic LCT code, sufficient for small truncation cutoffs. We also provide a more sophisticated and comprehensive set of Mathematica packages and demonstrations that can be used to study a variety of 2D models. We guide the reader through these packages with several examples and illustrate how to obtain QFT observables, such as spectral densities and the Zamolodchikov $C$-function. Specific models considered are finite $N_c$ QCD, scalar $\phi^4$ theory, and Yukawa theory."
EMANUEL KATZ,Solving the 2D SUSY Gross-Neveu-Yukawa Model with Conformal Truncation,"We use Lightcone Conformal Truncation to analyze the RG flow of the two-dimensional supersymmetric Gross-Neveu-Yukawa theory, i.e. the theory of a real scalar superfield with a $\mathbb{Z}_2$-symmetric cubic superpotential. The theory depends on a single dimensionless coupling $\bar{g}$, and is expected to have a critical point at a tuned value $\bar{g}_*$ where it flows in the IR to the Tricritical Ising Model (TIM); the theory spontaneously breaks the $\mathbb{Z}_2$ symmetry on one side of this phase transition, and breaks SUSY on the other side. We calculate the spectrum of energies as a function of $\bar{g}$ and see the gap close as the critical point is approached, and numerically read off the critical exponent $\nu$ in TIM. Beyond the critical point, the gap remains nearly zero, in agreement with the expectation of a massless Goldstino. We also study spectral functions of local operators on both sides of the phase transition and compare to analytic predictions where possible. In particular, we use the Zamolodchikov $C$-function to map the entire phase diagram of the theory. Crucial to this analysis is the fact that our truncation is able to preserve supersymmetry sufficiently to avoid any additional fine tuning."
EMANUEL KATZ,Lightcone Hamiltonian for Ising Field Theory I: T < T_c,
DAVID I WALKER,"BMQ : Boston medical quarterly: v. 14, no. 1-4",
CHRISTOPHE CHAMLEY,Asientos as sinews of war in the composite superpower of the 16th century,"The full analysis of the text of a contract, asiento, between Philip II of Spain and a Genoese merchant–banker details how in this pre-modern composite state, merchant–bankers acted as agents of the Crown who gathered many scattered sources of income to the Crown and transformed them into large and regular cash flows, mesadas, for the army. Because of the uncertain availability of these sources, the contract provided flexibility to both parties and legal assistance to the banker who reported to accountants for audit and, if necessary, the charge of an interest at about 1 percent per month."
SIDDHARTH RAMACHANDRAN,Long-Range Fiber Transmission of Optical Vortices,"We use specialty fiber (“vortex fiber”), to create and propagate orbital angular momentum states over ~kilometer lengths in telecom band (~1550nm). The spiral phase structure of the vortex beams was confirmed by interference with a Gaussian reference. This result is an important step toward achieving long-range classical and quantum communication links using orbital angular momentum of light."
JOANNE MURABITO,Genome-Wide Association Study for Subclinical Atherosclerosis in Major Arterial Territories in the NHLBI's Framingham Heart Study,"INTRODUCTION: Subclinical atherosclerosis (SCA) measures in multiple arterial beds are heritable phenotypes that are associated with increased incidence of cardiovascular disease. We conducted a genome-wide association study (GWAS) for SCA measurements in the community-based Framingham Heart Study. METHODS: Over 100,000 single nucleotide polymorphisms (SNPs) were genotyped (Human 100K GeneChip, Affymetrix) in 1345 subjects from 310 families. We calculated sex-specific age-adjusted and multivariable-adjusted residuals in subjects tested for quantitative SCA phenotypes, including ankle-brachial index, coronary artery calcification and abdominal aortic calcification using multi-detector computed tomography, and carotid intimal medial thickness (IMT) using carotid ultrasonography. We evaluated associations of these phenotypes with 70,987 autosomal SNPs with minor allele frequency ≥ 0.10, call rate ≥ 80%, and Hardy-Weinberg p-value ≥ 0.001 in samples ranging from 673 to 984 subjects, using linear regression with generalized estimating equations (GEE) methodology and family-based association testing (FBAT). Variance components LOD scores were also calculated. RESULTS. There was no association result meeting criteria for genome-wide significance, but our methods identified 11 SNPs with p < 10-5 by GEE and five SNPs with p < 10-5 by FBAT for multivariable-adjusted phenotypes. Among the associated variants were SNPs in or near genes that may be considered candidates for further study, such as rs1376877 (GEE p < 0.000001, located in ABI2) for maximum internal carotid artery IMT and rs4814615 (FBAT p = 0.000003, located in PCSK2) for maximum common carotid artery IMT. Modest significant associations were noted with various SCA phenotypes for variants in previously reported atherosclerosis candidate genes, including NOS3 and ESR1. Associations were also noted of a region on chromosome 9p21 with CAC phenotypes that confirm associations with coronary heart disease and CAC in two recently reported genome-wide association studies. In linkage analyses, several regions of genome-wide linkage were noted, confirming previously reported linkage of internal carotid artery IMT on chromosome 12. All GEE, FBAT and linkage results are provided as an open-access results resource at. CONCLUSION: The results from this GWAS generate hypotheses regarding several SNPs that may be associated with SCA phenotypes in multiple arterial beds. Given the number of tests conducted, subsequent independent replication in a staged approach is essential to identify genetic variants that may be implicated in atherosclerosis."
JOANNE MURABITO,The Framingham Heart Study 100K SNP Genome-Wide Association Study Resource: Overview of 17 Phenotype Working Group Reports,"BACKGROUND: The Framingham Heart Study (FHS), founded in 1948 to examine the epidemiology of cardiovascular disease, is among the most comprehensively characterized multi-generational studies in the world. Many collected phenotypes have substantial genetic contributors; yet most genetic determinants remain to be identified. Using single nucleotide polymorphisms (SNPs) from a 100K genome-wide scan, we examine the associations of common polymorphisms with phenotypic variation in this community-based cohort and provide a full-disclosure, web-based resource of results for future replication studies. METHODS: Adult participants (n = 1345) of the largest 310 pedigrees in the FHS, many biologically related, were genotyped with the 100K Affymetrix GeneChip. These genotypes were used to assess their contribution to 987 phenotypes collected in FHS over 56 years of follow up, including: cardiovascular risk factors and biomarkers; subclinical and clinical cardiovascular disease; cancer and longevity traits; and traits in pulmonary, sleep, neurology, renal, and bone domains. We conducted genome-wide variance components linkage and population-based and family-based association tests. RESULTS: The participants were white of European descent and from the FHS Original and Offspring Cohorts (examination 1 Offspring mean age 32 ± 9 years, 54% women). This overview summarizes the methods, selected findings and limitations of the results presented in the accompanying series of 17 manuscripts. The presented association results are based on 70,897 autosomal SNPs meeting the following criteria: minor allele frequency ≥ 10%, genotype call rate ≥ 80%, Hardy-Weinberg equilibrium p-value ≥ 0.001, and satisfying Mendelian consistency. Linkage analyses are based on 11,200 SNPs and short-tandem repeats. Results of phenotype-genotype linkages and associations for all autosomal SNPs are posted on the NCBI dbGaP website at. CONCLUSION: We have created a full-disclosure resource of results, posted on the dbGaP website, from a genome-wide association study in the FHS. Because we used three analytical approaches to examine the association and linkage of 987 phenotypes with thousands of SNPs, our results must be considered hypothesis-generating and need to be replicated. Results from the FHS 100K project with NCBI web posting provides a resource for investigators to identify high priority findings for replication."
JOANNE MURABITO,Genetic Correlates of Longevity and Selected Age-Related Phenotypes: A Genome-Wide Association Study in the Framingham Study,"BACKGROUND: Family studies and heritability estimates provide evidence for a genetic contribution to variation in the human life span. METHODS: We conducted a genome wide association study (Affymetrix 100K SNP GeneChip) for longevity-related traits in a community-based sample. We report on 5 longevity and aging traits in up to 1345 Framingham Study participants from 330 families. Multivariable-adjusted residuals were computed using appropriate models (Cox proportional hazards, logistic, or linear regression) and the residuals from these models were used to test for association with qualifying SNPs (70, 987 autosomal SNPs with genotypic call rate ≥80%, minor allele frequency ≥10%, Hardy-Weinberg test p ≥ 0.001). RESULTS: In family-based association test (FBAT) models, 8 SNPs in two regions approximately 500 kb apart on chromosome 1 (physical positions 73,091,610 and 73, 527,652) were associated with age at death (p-value < 10-5). The two sets of SNPs were in high linkage disequilibrium (minimum r2 = 0.58). The top 30 SNPs for generalized estimating equation (GEE) tests of association with age at death included rs10507486 (p = 0.0001) and rs4943794 (p = 0.0002), SNPs intronic to FOXO1A, a gene implicated in lifespan extension in animal models. FBAT models identified 7 SNPs and GEE models identified 9 SNPs associated with both age at death and morbidity-free survival at age 65 including rs2374983 near PON1. In the analysis of selected candidate genes, SNP associations (FBAT or GEE p-value < 0.01) were identified for age at death in or near the following genes: FOXO1A, GAPDH, KL, LEPR, PON1, PSEN1, SOD2, and WRN. Top ranked SNP associations in the GEE model for age at natural menopause included rs6910534 (p = 0.00003) near FOXO3a and rs3751591 (p = 0.00006) in CYP19A1. Results of all longevity phenotype-genotype associations for all autosomal SNPs are web posted at . CONCLUSION. Longevity and aging traits are associated with SNPs on the Affymetrix 100K GeneChip. None of the associations achieved genome-wide significance. These data generate hypotheses and serve as a resource for replication as more genes and biologic pathways are proposed as contributing to longevity and healthy aging."
JOANNE MURABITO,Framingham Heart Study 100K Project: Genome-Wide Associations for Cardiovascular Disease Outcomes,"BACKGROUND: Cardiovascular disease (CVD) and its most common manifestations – including coronary heart disease (CHD), stroke, heart failure (HF), and atrial fibrillation (AF) – are major causes of morbidity and mortality. In many industrialized countries, cardiovascular disease (CVD) claims more lives each year than any other disease. Heart disease and stroke are the first and third leading causes of death in the United States. Prior investigations have reported several single gene variants associated with CHD, stroke, HF, and AF. We report a community-based genome-wide association study of major CVD outcomes. METHODS: In 1345 Framingham Heart Study participants from the largest 310 pedigrees (54% women, mean age 33 years at entry), we analyzed associations of 70,987 qualifying SNPs (Affymetrix 100K GeneChip) to four major CVD outcomes: major atherosclerotic CVD (n = 142; myocardial infarction, stroke, CHD death), major CHD (n = 118; myocardial infarction, CHD death), AF (n = 151), and HF (n = 73). Participants free of the condition at entry were included in proportional hazards models. We analyzed model-based deviance residuals using generalized estimating equations to test associations between SNP genotypes and traits in additive genetic models restricted to autosomal SNPs with minor allele frequency ≥0.10, genotype call rate ≥0.80, and Hardy-Weinberg equilibrium p-value ≥ 0.001. RESULTS: Six associations yielded p < 10-5. The lowest p-values for each CVD trait were as follows: major CVD, rs499818, p = 6.6 × 10-6; major CHD, rs2549513, p = 9.7 × 10-6; AF, rs958546, p = 4.8 × 10-6; HF: rs740363, p = 8.8 × 10-6. Of note, we found associations of a 13 Kb region on chromosome 9p21 with major CVD (p 1.7 – 1.9 × 10-5) and major CHD (p 2.5 – 3.5 × 10-4) that confirm associations with CHD in two recently reported genome-wide association studies. Also, rs10501920 in CNTN5 was associated with AF (p = 9.4 × 10-6) and HF (p = 1.2 × 10-4). Complete results for these phenotypes can be found at the dbgap website. CONCLUSION: No association attained genome-wide significance, but several intriguing findings emerged. Notably, we replicated associations of chromosome 9p21 with major CVD. Additional studies are needed to validate these results. Finding genetic variants associated with CVD may point to novel disease pathways and identify potential targeted preventive therapies."
JOANNE MURABITO,Genome-wide association with bone mass and geometry in the Framingham Heart Study,"BACKGROUND:Osteoporosis is characterized by low bone mass and compromised bone structure, heritable traits that contribute to fracture risk. There have been no genome-wide association and linkage studies for these traits using high-density genotyping platforms.METHODS:We used the Affymetrix 100K SNP GeneChip marker set in the Framingham Heart Study (FHS) to examine genetic associations with ten primary quantitative traits: bone mineral density (BMD), calcaneal ultrasound, and geometric indices of the hip. To test associations with multivariable-adjusted residual trait values, we used additive generalized estimating equation (GEE) and family-based association tests (FBAT) models within each sex as well as sexes combined. We evaluated 70,987 autosomal SNPs with genotypic call rates [greater than or equal to]80%, HWE p [greater than or equal to] 0.001, and MAF [greater than or equal to]10% in up to 1141 phenotyped individuals (495 men and 646 women, mean age 62.5 yrs). Variance component linkage analysis was performed using 11,200 markers.RESULTS:Heritability estimates for all bone phenotypes were 30-66%. LOD scores [greater than or equal to]3.0 were found on chromosomes 15 (1.5 LOD confidence interval: 51,336,679-58,934,236 bp) and 22 (35,890,398-48,603,847 bp) for femoral shaft section modulus. The ten primary phenotypes had 12 associations with 100K SNPs in GEE models at p < 0.000001 and 2 associations in FBAT models at p < 0.000001. The 25 most significant p-values for GEE and FBAT were all less than 3.5 x 10-6 and 2.5 x 10-5, respectively. Of the 40 top SNPs with the greatest numbers of significantly associated BMD traits (including femoral neck, trochanter, and lumbar spine), one half to two-thirds were in or near genes that have not previously been studied for osteoporosis. Notably, pleiotropic associations between BMD and bone geometric traits were uncommon. Evidence for association (FBAT or GEE p < 0.05) was observed for several SNPs in candidate genes for osteoporosis, such as rs1801133 in MTHFR; rs1884052 and rs3778099 in ESR1; rs4988300 in LRP5; rs2189480 in VDR; rs2075555 in COLIA1; rs10519297 and rs2008691 in CYP19, as well as SNPs in PPARG (rs10510418 and rs2938392) and ANKH (rs2454873 and rs379016). All GEE, FBAT and linkage results are provided as an open-access results resource at http://www.ncbi.nlm.nih.gov/projects/gap/cgi-bin/study.cgi?id=phs000007.CONCLUSION:The FHS 100K SNP project offers an unbiased genome-wide strategy to identify new candidate loci and to replicate previously suggested candidate genes for osteoporosis."
JOANNE MURABITO,The Framingham Heart Study 100K SNP Genome-Wide Association Study Resource: Overview of 17 Phenotype Working Group Reports,"BACKGROUND: The Framingham Heart Study (FHS), founded in 1948 to examine the epidemiology of cardiovascular disease, is among the most comprehensively characterized multi-generational studies in the world. Many collected phenotypes have substantial genetic contributors; yet most genetic determinants remain to be identified. Using single nucleotide polymorphisms (SNPs) from a 100K genome-wide scan, we examine the associations of common polymorphisms with phenotypic variation in this community-based cohort and provide a full-disclosure, web-based resource of results for future replication studies. METHODS: Adult participants (n = 1345) of the largest 310 pedigrees in the FHS, many biologically related, were genotyped with the 100K Affymetrix GeneChip. These genotypes were used to assess their contribution to 987 phenotypes collected in FHS over 56 years of follow up, including: cardiovascular risk factors and biomarkers; subclinical and clinical cardiovascular disease; cancer and longevity traits; and traits in pulmonary, sleep, neurology, renal, and bone domains. We conducted genome-wide variance components linkage and population-based and family-based association tests. RESULTS: The participants were white of European descent and from the FHS Original and Offspring Cohorts (examination 1 Offspring mean age 32 ± 9 years, 54% women). This overview summarizes the methods, selected findings and limitations of the results presented in the accompanying series of 17 manuscripts. The presented association results are based on 70,897 autosomal SNPs meeting the following criteria: minor allele frequency ≥ 10%, genotype call rate ≥ 80%, Hardy-Weinberg equilibrium p-value ≥ 0.001, and satisfying Mendelian consistency. Linkage analyses are based on 11,200 SNPs and short-tandem repeats. Results of phenotype-genotype linkages and associations for all autosomal SNPs are posted on the NCBI dbGaP website at. CONCLUSION: We have created a full-disclosure resource of results, posted on the dbGaP website, from a genome-wide association study in the FHS. Because we used three analytical approaches to examine the association and linkage of 987 phenotypes with thousands of SNPs, our results must be considered hypothesis-generating and need to be replicated. Results from the FHS 100K project with NCBI web posting provides a resource for investigators to identify high priority findings for replication."
JOANNE MURABITO,Genome-wide association study for subclinical atherosclerosis in major arterial territories in the NHLBI's Framingham Heart Study,"INTRODUCTION:Subclinical atherosclerosis (SCA) measures in multiple arterial beds are heritable phenotypes that are associated with increased incidence of cardiovascular disease. We conducted a genome-wide association study (GWAS) for SCA measurements in the community-based Framingham Heart Study.METHODS:Over 100,000 single nucleotide polymorphisms (SNPs) were genotyped (Human 100K GeneChip, Affymetrix) in 1345 subjects from 310 families. We calculated sex-specific age-adjusted and multivariable-adjusted residuals in subjects tested for quantitative SCA phenotypes, including ankle-brachial index, coronary artery calcification and abdominal aortic calcification using multi-detector computed tomography, and carotid intimal medial thickness (IMT) using carotid ultrasonography. We evaluated associations of these phenotypes with 70,987 autosomal SNPs with minor allele frequency [greater than or equal to] 0.10, call rate [greater than or equal to] 80%, and Hardy-Weinberg p-value [greater than or equal to] 0.001 in samples ranging from 673 to 984 subjects, using linear regression with generalized estimating equations (GEE) methodology and family-based association testing (FBAT). Variance components LOD scores were also calculated.RESULTS:There was no association result meeting criteria for genome-wide significance, but our methods identified 11 SNPs with p < 10-5 by GEE and five SNPs with p < 10-5 by FBAT for multivariable-adjusted phenotypes. Among the associated variants were SNPs in or near genes that may be considered candidates for further study, such as rs1376877 (GEE p < 0.000001, located in ABI2) for maximum internal carotid artery IMT and rs4814615 (FBAT p = 0.000003, located in PCSK2) for maximum common carotid artery IMT. Modest significant associations were noted with various SCA phenotypes for variants in previously reported atherosclerosis candidate genes, including NOS3 and ESR1. Associations were also noted of a region on chromosome 9p21 with CAC phenotypes that confirm associations with coronary heart disease and CAC in two recently reported genome-wide association studies. In linkage analyses, several regions of genome-wide linkage were noted, confirming previously reported linkage of internal carotid artery IMT on chromosome 12. All GEE, FBAT and linkage results are provided as an open-access results resource at http://www.ncbi.nlm.nih.gov/projects/gap/cgi-bin/study.cgi?id=phs000007.CONCLUSION:The results from this GWAS generate hypotheses regarding several SNPs that may be associated with SCA phenotypes in multiple arterial beds. Given the number of tests conducted, subsequent independent replication in a staged approach is essential to identify genetic variants that may be implicated in atherosclerosis."
JOANNE MURABITO,Genome-wide association with bone mass and geometry in the Framingham Heart Study,"BACKGROUND: Osteoporosis is characterized by low bone mass and compromised bone structure, heritable traits that contribute to fracture risk. There have been no genome-wide association and linkage studies for these traits using high-density genotyping platforms. METHODS: We used the Affymetrix 100K SNP GeneChip marker set in the Framingham Heart Study (FHS) to examine genetic associations with ten primary quantitative traits: bone mineral density (BMD), calcaneal ultrasound, and geometric indices of the hip. To test associations with multivariable-adjusted residual trait values, we used additive generalized estimating equation (GEE) and family-based association tests (FBAT) models within each sex as well as sexes combined. We evaluated 70,987 autosomal SNPs with genotypic call rates ≥80%, HWE p ≥ 0.001, and MAF ≥10% in up to 1141 phenotyped individuals (495 men and 646 women, mean age 62.5 yrs). Variance component linkage analysis was performed using 11,200 markers. RESULTS: Heritability estimates for all bone phenotypes were 30–66%. LOD scores ≥3.0 were found on chromosomes 15 (1.5 LOD confidence interval: 51,336,679–58,934,236 bp) and 22 (35,890,398–48,603,847 bp) for femoral shaft section modulus. The ten primary phenotypes had 12 associations with 100K SNPs in GEE models at p < 0.000001 and 2 associations in FBAT models at p < 0.000001. The 25 most significant p-values for GEE and FBAT were all less than 3.5 × 10-6 and 2.5 × 10-5, respectively. Of the 40 top SNPs with the greatest numbers of significantly associated BMD traits (including femoral neck, trochanter, and lumbar spine), one half to two-thirds were in or near genes that have not previously been studied for osteoporosis. Notably, pleiotropic associations between BMD and bone geometric traits were uncommon. Evidence for association (FBAT or GEE p < 0.05) was observed for several SNPs in candidate genes for osteoporosis, such as rs1801133 in MTHFR; rs1884052 and rs3778099 in ESR1; rs4988300 in LRP5; rs2189480 in VDR; rs2075555 in COLIA1; rs10519297 and rs2008691 in CYP19, as well as SNPs in PPARG (rs10510418 and rs2938392) and ANKH (rs2454873 and rs379016). All GEE, FBAT and linkage results are provided as an open-access results resource at. CONCLUSION: The FHS 100K SNP project offers an unbiased genome-wide strategy to identify new candidate loci and to replicate previously suggested candidate genes for osteoporosis."
JOANNE MURABITO,A genome-wide association study of breast and prostate cancer in the NHLBI's Framingham Heart Study,"BACKGROUND: Breast and prostate cancer are two commonly diagnosed cancers in the United States. Prior work suggests that cancer causing genes and cancer susceptibility genes can be identified. METHODS: We conducted a genome-wide association study (Affymetrix 100K SNP GeneChip) of cancer in the community-based Framingham Heart Study. We report on 2 cancer traits – prostate cancer and breast cancer – in up to 1335 participants from 330 families (54% women, mean entry age 33 years). Multivariable-adjusted residuals, computed using Cox proportional hazards models, were tested for association with qualifying SNPs (70, 987 autosomal SNPs with genotypic call rate ≥80%, minor allele frequency ≥10%, Hardy-Weinberg test p ≥ 0.001) using generalized estimating equations (GEE) models and family based association tests (FBAT). RESULTS: There were 58 women with breast cancer and 59 men with prostate cancer. No SNP associations attained genome-wide significance. The top SNP associations in GEE models for each trait were as follows: breast cancer, rs2075555, p = 8.0 × 10-8 in COL1A1; and prostate cancer, rs9311171, p = 1.75 × 10-6 in CTDSPL. In analysis of selected candidate cancer susceptibility genes, two MSR1 SNPs (rs9325782, GEE p = 0.008 and rs2410373, FBAT p = 0.021) were associated with prostate cancer and three ERBB4 SNPs (rs905883 GEE p = 0.0002, rs7564590 GEE p = 0.003, rs7558615 GEE p = 0.0078) were associated with breast cancer. The previously reported risk SNP for prostate cancer, rs1447295, was not included on the 100K chip. Results of cancer phenotype-genotype associations for all autosomal SNPs are web posted at. CONCLUSION: Although no association attained genome-wide significance, several interesting associations emerged for breast and prostate cancer. These findings can serve as a resource for replication in other populations to identify novel biologic pathways contributing to cancer susceptibility."
JOANNE MURABITO,Evaluation of association of HNF1B variants with diverse cancers: collaborative analysis of data from 19 genome-wide association studies,"BACKGROUND. Genome-wide association studies have found type 2 diabetes-associated variants in the HNF1B gene to exhibit reciprocal associations with prostate cancer risk. We aimed to identify whether these variants may have an effect on cancer risk in general versus a specific effect on prostate cancer only. METHODOLOGY/PRINCIPAL FINDINGS. In a collaborative analysis, we collected data from GWAS of cancer phenotypes for the frequently reported variants of HNF1B, rs4430796 and rs7501939, which are in linkage disequilibrium (r2=0.76, HapMap CEU). Overall, the analysis included 16 datasets on rs4430796 with 19,640 cancer cases and 21,929 controls; and 21 datasets on rs7501939 with 26,923 cases and 49,085 controls. Malignancies other than prostate cancer included colorectal, breast, lung and pancreatic cancers, and melanoma. Meta-analysis showed large between-dataset heterogeneity that was driven by different effects in prostate cancer and other cancers. The per-T2D-risk-allele odds ratios (95% confidence intervals) for rs4430796 were 0.79 (0.76, 0.83)] per G allele for prostate cancer (p<10-15 for both); and 1.03 (0.99, 1.07) for all other cancers. Similarly for rs7501939 the per-T2D-risk-allele odds ratios (95% confidence intervals) were 0.80 (0.77, 0.83) per T allele for prostate cancer (p<10-15 for both); and 1.00 (0.97, 1.04) for all other cancers. No malignancy other than prostate cancer had a nominally statistically significant association. CONCLUSIONS/SIGNIFICANCE. The examined HNF1B variants have a highly specific effect on prostate cancer risk with no apparent association with any of the other studied cancer types."
RAYMOND FISMAN,Experimental evidence of physician social preferences,"Physicians' professional ethics require that they put patients' interests ahead of their own and that they should allocate limited medical resources efficiently. Understanding physicians' extent of adherence to these principles requires understanding the social preferences that lie behind them. These social preferences may be divided into two qualitatively different trade-offs: the trade-off between self and other (altruism) and the trade-off between reducing differences in payoffs (equality) and increasing total payoffs (efficiency). We experimentally measure social preferences among a nationwide sample of practicing physicians in the United States. Our design allows us to distinguish empirically between altruism and equality-efficiency orientation and to accurately measure both trade-offs at the level of the individual subject. We further compare the experimentally measured social preferences of physicians with those of a representative sample of Americans, an ""elite"" subsample of Americans, and a nationwide sample of medical students. We find that physicians' altruism stands out. Although most physicians place a greater weight on self than on other, the share of physicians who place a greater weight on other than on self is twice as large as for all other samples-32% as compared with 15 to 17%. Subjects in the general population are the closest to physicians in terms of altruism. The higher altruism among physicians compared with the other samples cannot be explained by income or age differences. By contrast, physicians' preferences regarding equality-efficiency orientation are not meaningfully different from those of the general sample and elite subsample and are less efficiency oriented than medical students."
RAYMOND FISMAN,Reference points and redistributive preferences: experimental evidence,
RAYMOND FISMAN,Distributional preferences in larger groups: keeping up with the Joneses and keeping track of the tails,"We study distributional preferences in larger “societies”. We conduct experiments via Mechanical Turk, in which subjects choose between two income distributions, each with seven (or more) individuals, with hypothetical incomes that aim to approximate the actual distribution of income in the United States. In contrast to prior work, our design allows us to flexibly capture the particular distributional concerns of subjects. Consistent with standard maximin (Rawlsian) preferences, subjects select distributions in which the bottom individual’s income is higher (but show little regard for lower incomes above the bottom ranking). In contrast to standard models, however, we find that subjects select distributions that lower the top individual’s income, but not other high incomes. Finally, we provide evidence of “locally competitive” preferences—in most experimental sessions, subjects select distributions that lower the income of the individual directly above them, whereas the income of the individual two positions above has little effect on subjects’ decisions. Our findings suggest that theories of inequality aversion should be adapted to account for individuals’ aversion to “topmost” and “local” disadvantageous inequality."
RAYMOND FISMAN,Experience of communal conflicts and inter-group lending,"We provide micro economic evidence on the link between ethnic frictions and market efficiency, using dyadic data on managers and borrowers from a large Indian bank. Our analysis builds on the idea that exposure to religion-based communal violence may intensify branch managers’ same-group preferences, and thus result in lending decisions that are more sensitive to a borrower’s religion. We find that,in our sample of Hindu loan officers, those with substantial riot exposure prior to joining the bank lend relatively less to Muslim borrowers. Riot-exposed officers’ loans to Muslims are also less likely to default, suggesting that the lower lending rate for Muslims is driven by taste-based discrimination. This bias is persistent across a bank officer’s tenure, suggesting that the economic costs of ethnic conflict are long-lasting, potentially spanning across generations."
RAYMOND FISMAN,An Event Long-Short Index: theory and applications,"We propose a stock market-based measure to capture initial beliefs about an event's effect on firm profits, which may be used to measure whether initial expectations are subsequently realized. Our ""Event Long-Short Index"" is the difference in market-capitalization-weighted returns of firms that outperform versus underperform the market on the event date. We use post-event index returns to measure whether initial beliefs are reinforced or attenuated. We apply our approach to the 2016 US presidential election and Brexit referendum to illustrate the index and its interpretation and to validate it, showing that it moves as expected following subsequent political and business news."
RAYMOND FISMAN,Gender and bureaucratic corruption: evidence from two countries,"We examine the correlation between gender and bureaucratic corruption using two distinct datasets, from Italy and from China. In each case, we find that women are far less likely to be investigated for corruption than men. In our Italian data, female procurement officials are 22% less likely than men to be investigated for corruption by enforcement authorities; in China, female prefectural leaders are 81% less likely to be arrested for corruption than men. While these represent correlations (rather than definitive causal effects), both are very robust relationships, which survive the inclusion of fine-grained individual and geographic controls, and based on Oster’s (2019. “Unobservable Selection and Coefficient Stability: Theory and Evidence,” 37 Journal of Business &amp; Economic Statistics 187–204.) test unlikely to be driven by unobservables. Using data from a survey of Italian procurement officials, we present tentative evidence on mechanism: the gender gap is partly due to women acting more “defensively” in administering their duties."
RAYMOND FISMAN,Social ties and the selection of China's political elite,"We study how sharing a hometown or college connection with an incumbent member of China's Politburo affects a candidate's likelihood of selection as a new member. In specifications that include fixed effects to absorb quality differences across cities and colleges, we find that hometown and college connections are each associated with 5–9 percentage point reductions in selection probability. This ""connections penalty"" is equally strong for retiring Politburo members, arguing against quota-based explanations, and it is much stronger for junior Politburo members, consistent with a role for intra-factional competition. Our findings differ from earlier work because of our emphasis on within-group variation, and our focus on shared hometown and college, rather than shared workplace, connections."
RAYMOND FISMAN,Hometown ties and the quality of government monitoring: evidence from rotation of Chinese auditors,"Audits are a standard mechanism for reducing corruption in government investments. The quality of audits themselves, however, may be affected by relationships between auditor and target. We study whether provincial chief auditors in China show greater leniency in evaluating prefecture governments in their hometowns. In city-fixed-effect specifications―in which the role of shared background is identified from auditor turnover―we show that hometown auditors find 38 percent less in questionable monies. This hometown effect is similar throughout the auditor's tenure and is diminished for audits ordered by the provincial Organization Department as a result of the departure of top city officials. We argue that our findings are most readily explained by leniency toward local officials rather than an endogenous response to concerns of better enforcement by hometown auditors. We complement these city-level findings with firm-level analyses of earnings manipulation by state-owned enterprises (SOE) via real activity manipulation (a standard measure from the accounting literature), which we show is higher under hometown auditors."
RAYMOND FISMAN,SOEs and soft incentive constraints in state bank lending,"We study how Chinese state bank managers’ lending incentives impact lending to state-owned enterprises (SOEs). We show lending quantity increases and quality decreases at month’s end, indicating monthly lending targets that decrease lending standards. Increased quantity comes from both SOEs and private lending, whereas decreased quality is from only SOEs, which continue to receive loans even after prior defaults (particularly at month’s end). We suggest that SOE lending may thus be beneficial for state bank managers, who lend to delinquent state enterprises to meet targets, which in turn may exacerbate SOEs’ soft budget constraints. (JEL G21, G28, L32, O16, P34)."
DANIEL P ALFORD,A transitional opioid program to engage hospitalized drug users,"BACKGROUND: Many opioid-dependent patients do not receive care for addiction issues when hospitalized for other medical problems. Based on 3 years of clinical practice, we report the Transitional Opioid Program (TOP) experience using hospitalization as a ""reachable moment"" to identify and link opioid-dependent persons to addiction treatment from medical care. METHODS: A program nurse identified, assessed, and enrolled hospitalized, out-of-treatment opioid-dependent drug users based on their receipt of methadone during hospitalization. At discharge, patients transitioned to an outpatient interim opioid agonist program providing 30-day stabilization followed by 60-day taper. The nurse provided case management emphasizing HIV risk reduction, health education, counseling, and medical follow-up. Treatment outcomes included opioid agonist stabilization then taper or transfer to long-term opioid agonist treatment. RESULTS: From January 2002 to January 2005, 362 unique hospitalized, opioid-dependent drug users were screened; 56% (n = 203) met eligibility criteria and enrolled into the program. Subsequently, 82% (167/203) presented to the program clinic post-hospital discharge; for 59% (119/203) treatment was provided, for 26% (52/203) treatment was not provided, and for 16% (32/203) treatment was not possible (pursuit of TOP objectives precluded by medical problems, psychiatric issues, or incarceration). Program patients adhered to a spectrum of medical recommendations (e.g., obtaining prescription medications, medical follow-up). CONCLUSIONS: The Transitional Opioid Program (TOP) identified at-risk hospitalized, out-of-treatment opioid-dependent drug users and, by offering a range of treatment intensity options, engaged a majority into addiction treatment. Hospitalization can be a ""reachable moment"" to engage and link drug users into addiction treatment."
DANIEL P ALFORD,A Web-Based Alcohol Clinical Training (ACT) Curriculum: Is In-Person Faculty Development Necessary to Affect Teaching?,"BACKGROUND: Physicians receive little education about unhealthy alcohol use and as a result patients often do not receive efficacious interventions. The objective of this study is to evaluate whether a free web-based alcohol curriculum would be used by physician educators and whether in-person faculty development would increase its use, confidence in teaching and teaching itself. METHODS: Subjects were physician educators who applied to attend a workshop on the use of a web-based curriculum about alcohol screening and brief intervention and cross-cultural efficacy. All physicians were provided the curriculum web address. Intervention subjects attended a 3-hour workshop including demonstration of the website, modeling of teaching, and development of a plan for using the curriculum. All subjects completed a survey prior to and 3 months after the workshop. RESULTS: Of 20 intervention and 13 control subjects, 19 (95%) and 10 (77%), respectively, completed follow-up. Compared to controls, intervention subjects had greater increases in confidence in teaching alcohol screening, and in the frequency of two teaching practices – teaching about screening and eliciting patient health beliefs. Teaching confidence and teaching practices improved significantly in 9 of 10 comparisons for intervention, and in 0 comparisons for control subjects. At follow-up 79% of intervention but only 50% of control subjects reported using any part of the curriculum (p = 0.20). CONCLUSION: In-person training for physician educators on the use of a web-based alcohol curriculum can increase teaching confidence and practices. Although the web is frequently used for disemination, in-person training may be preferable to effect widespread teaching of clinical skills like alcohol screening and brief intervention."
DANIEL P ALFORD,Internal Medicine Residency Training for Unhealthy Alcohol and Other Drug Use: Recommendations for Curriculum Design,"BACKGROUND: Unhealthy substance use is the spectrum from use that risks harm, to use associated with problems, to the diagnosable conditions of substance abuse and dependence, often referred to as substance abuse disorders. Despite the prevalence and impact of unhealthy substance use, medical education in this area remains lacking, not providing physicians with the necessary expertise to effectively address one of the most common and costly health conditions. Medical educators have begun to address the need for physician training in unhealthy substance use, and formal curricula have been developed and evaluated, though broad integration into busy residency curricula remains a challenge. DISCUSSION: We review the development of unhealthy substance use related competencies, and describe a curriculum in unhealthy substance use that integrates these competencies into internal medicine resident physician training. We outline strategies to facilitate adoption of such curricula by the residency programs. This paper provides an outline for the actual implementation of the curriculum within the structure of a training program, with examples using common teaching venues. We describe and link the content to the core competencies mandated by the Accreditation Council for Graduate Medical Education, the formal accrediting body for residency training programs in the United States. Specific topics are recommended, with suggestions on how to integrate such teaching into existing internal medicine residency training program curricula. SUMMARY: Given the burden of disease and effective interventions available that can be delivered by internal medicine physicians, teaching about unhealthy substance use must be incorporated into internal medicine residency training, and can be done within existing teaching venues."
DANIEL P ALFORD,Treating Homeless Opioid Dependent Patients with Buprenorphine in an Office-Based Setting,"CONTEXT Although office-based opioid treatment with buprenorphine (OBOT-B) has been successfully implemented in primary care settings in the US, its use has not been reported in homeless patients. OBJECTIVE To characterize the feasibility of OBOT-B in homeless relative to housed patients. DESIGN A retrospective record review examining treatment failure, drug use, utilization of substance abuse treatment services, and intensity of clinical support by a nurse care manager (NCM) among homeless and housed patients in an OBOT-B program between August 2003 and October 2004. Treatment failure was defined as elopement before completing medication induction, discharge after medication induction due to ongoing drug use with concurrent nonadherence with intensified treatment, or discharge due to disruptive behavior. RESULTS Of 44 homeless and 41 housed patients enrolled over 12 months, homeless patients were more likely to be older, nonwhite, unemployed, infected with HIV and hepatitis C, and report a psychiatric illness. Homeless patients had fewer social supports and more chronic substance abuse histories with a 3- to 6-fold greater number of years of drug use, number of detoxification attempts and percentage with a history of methadone maintenance treatment. The proportion of subjects with treatment failure for the homeless (21%) and housed (22%) did not differ (P=.94). At 12 months, both groups had similar proportions with illicit opioid use [Odds ratio (OR), 0.9 (95% CI, 0.5–1.7) P=.8], utilization of counseling (homeless, 46%; housed, 49%; P=.95), and participation in mutual-help groups (homeless, 25%; housed, 29%; P=.96). At 12 months, 36% of the homeless group was no longer homeless. During the first month of treatment, homeless patients required more clinical support from the NCM than housed patients. CONCLUSIONS Despite homeless opioid dependent patients' social instability, greater comorbidities, and more chronic drug use, office-based opioid treatment with buprenorphine was effectively implemented in this population comparable to outcomes in housed patients with respect to treatment failure, illicit opioid use, and utilization of substance abuse treatment."
ROBERT A MARGO,“Automation” of manufacturing in the late nineteenth century: the hand and machine labor study,"Recent advances in artificial intelligence and robotics have generated a robust debate about the future of work. An analogous debate occurred in the late nineteenth century when mechanization first transformed manufacturing. We analyze an extraordinary dataset from the late nineteenth century, the Hand and Machine Labor study carried out by the US Department of Labor in the mid-1890s. We focus on transitions at the task level from hand to machine production, and on the impact of inanimate power, especially of steam power, on labor productivity. Our analysis sheds light on the ability of modern task-based models to account for the effects of historical mechanization."
ROBERT A MARGO,"Gallman revisited: Blacksmithing and American manufacturing, 1850-1870","In nineteenth-century America, blacksmiths were a fixture in every village, town, and city, producing a diverse range of products from axes to wheels and services from repairs to horse shoeing. In constructing his historical GNP accounts, Gallman opted to exclude these “jacks-of-all-trades” from the manufacturing sector, classifying them instead as part of the service sector. However, using establishment-level data for blacksmiths from the federal censuses of manufactures for 1850, 1860, and 1870, we re-examine that choice and show that blacksmiths were an important, if declining, source of manufactured goods. Moreover, as quintessential artisan shops, a close analysis of their structure and operation helps resolve several key puzzles regarding industrialization in the nineteenth century. As “jacks-of-all-trades,” they were generally masters of none (except for their service activities). Moreover, the historical record reveals that several of those who managed to achieve mastery moved on to become specialized manufacturers of that specific product. Such specialized producers had higher productivity levels than those calling themselves blacksmiths producing the same goods, explaining changes in industry mix and the decline of the blacksmith in manufacturing."
ROBERT A MARGO,The integration of economic history into economics,"In the USA today the academic field of economic history is much closer to economics than it is to history in terms of professional behavior, a stylized fact that I call the “integration of economic history into economics.” I document this using two types of evidence—use of econometric language in articles appearing in academic journals of economic history and economics; and publication histories of successive cohorts of Ph.D.s in the first decade since receiving the doctorate. Over time, economic history became more like economics in its use of econometrics and in the likelihood of scholars publishing in economics, as opposed to, say, economic history journals. But the pace of change was slower in economic history than in labor economics, another subfield of economics that underwent profound intellectual change in the 1950s and 1960s, and there was also a structural break evident for post-2000 Ph.D. cohorts. To account for these features of the data, I sketch a simple, overlapping generations model of the academic labor market in which junior scholars have to convince senior scholars of the merits of their work in order to gain tenure. I argue that the early cliometricians—most notably, Robert Fogel and Douglass North—conceived of a scholarly identity for economic history that kept the field distinct from economics proper in various ways, until after 2000 when their influence had waned."
ROBERT A MARGO,"Obama, Katrina, and the persistence of racial inequality","New benchmark estimates of Black-White income ratios for 1870, 1900, and 1940 are combined with standard post-World War census data. The resulting time series reveals that the pace of racial income convergence has generally been steady but slow, quickening only during the 1940s and the modern Civil Rights era. I explore the interpretation of the time series with a model of intergenerational transmission of inequality in which racial differences in causal factors that determine income are very large just after the Civil War and which erode slowly across subsequent generations."
ROBERT A MARGO,“Mechanization takes command?”: Powered machinery and production times in late nineteenth-century American manufacturing,"During the nineteenth century, U.S. manufacturers shifted away from the “hand labor” mode of production, characteristic of artisan shops, to “machine labor,” which was increasingly concentrated in steam-powered factories. This transition fundamentally changed production tasks, jobs, and job requirements. This paper uses digitized data on these two production modes from an 1899 U.S. Commissioner of Labor report to estimate the frequency and impact of the use of inanimate power on production operation times. About half of production operations were mechanized; the use of inanimate power raised productivity, accounting for about one-quarter to one-third of the overall productivity advantage of machine labor. However, additional factors, such as the increased division of labor and adoption of high-volume production, also played quantitatively important roles in raising productivity in machine production versus by hand."
ROBERT A MARGO,Industrialization and urbanization in nineteenth century America,"During the nineteenth century manufacturing increased its share of the labor force in the United States, and manufacturing became more urban, as did the population. Our survey of the literature and analyses of census data suggests that a key reason was the development of a nationwide transportation system, especially the railroad. Coupled with changes in manufacturing technology and organizational form, the “transportation revolution” increased demand for manufacturing labor in urban locations. Labor supply responded and because of agglomeration economies, population density and the size and number of urban places increased. Although our focus is on the US experience, a causal role for transportation is likely for other economies that experienced historical industrialization and urbanization."
ROBERT A MARGO,Digitizing Carroll D. Wright’s “Hand and Machine Labor” study,"We describe our digitization of a uniquely detailed study of 19th century production methods assembled by the United States Department of Labor (1899). The staff spent five years collecting and assembling data on the production of hundreds of highly specific products (as well as some services) at the production operations level using traditional artisanal (“hand”) methods and by the (then) most modern “machine” methods, measuring productivity in terms of the time taken to complete a specific task or set of tasks. The data proved too complex and voluminous to use, except as a source of anecdotes, until now. We describe how we have made these invaluable data from the first industrial revolution tractable to modern analysis and how they might be used to provide insight and perspective into the effects of robotics and artificial intelligence on labor during the third industrial revolution."
JODI CRANSTON,"Mapping paintings, or how to breathe life into provenance","The Isabella Stewart Gardner Museum in Boston houses the extraordinary collection of Mrs. Gardner in a purpose-built Venetian-style palace. Observing the artworks in exactly the same arrangement as Mrs. Gardner experienced them allows visitors to situate themselves physically in the early twentieth century and to imagine a kind of kinship with the world of Fenway Court, as Mrs. Gardner referred to her house. This type of house museum arguably nudges us into the edges of history a bit more easily and effectively than the galleries of a traditional purpose-built museum: the wallpaper, the furnishings, the absence of wall labels. But how much of the lives of these artworks still remains lost to us as we stand before them? How much of the life of any artwork remains lost to us when we look at it in a museum?"
KENNETH J ROTHMAN,Exposure to Fumonisins and the Occurrence of Neural Tube Defects along the Texas-Mexico Border,"Along the Texas-Mexico border, the prevalence of neural tube defects (NTDs) among Mexican-American women doubled during 1990-1991. The human outbreak began during the same crop year as epizootics attributed to exposure to fumonisin, a mycotoxin that often contaminates corn. Because Mexican Americans in Texas consume large quantities of corn, primarily in the form of tortillas, they may be exposed to high levels of fumonisins. We examined whether or not maternal exposure to fumonisins increases the risk of NTDs in offspring using a population-based case-control study. We estimated fumonisin exposure from a postpartum sphinganine:sphingosine (sa:so) ratio, a biomarker for fumonisin exposure measured in maternal serum, and from maternal recall of periconceptional corn tortilla intake. After adjusting for confounders, moderate (301-400) compared with low (≤100) consumption of tortillas during the first trimester was associated with increased odds ratios (ORs) of having an NTD-affected pregnancy (OR = 2.4; 95% confidence interval, 1.1-5.3). No increased risks were observed at intakes higher than 400 tortillas (OR = 0.8 for 401-800, OR = 1.0 for > 800). Based on the postpartum sa:so ratio, increasing levels of fumonisin exposure were associated with increasing ORs for NTD occurrences, except for the highest exposure category (sa:so > 0.35). Our findings suggest that fumonisin exposure increases the risk of NTD, proportionate to dose, up to a threshold level, at which point fetal death may be more likely to occur. These results also call for population studies that can more directly measure individual fumonisin intakes and assess effects on the developing embryo."
KENNETH J ROTHMAN,The Relation between Amyotrophic Lateral Sclerosis and Inorganic Selenium in Drinking Water: A Population-Based Case-Control Study,"BACKGROUND: A community in northern Italy was previously reported to have an excess incidence of amyotrophic lateral sclerosis among residents exposed to high levels of inorganic selenium in their drinking water. METHODS: To assess the extent to which such association persisted in the decade following its initial observation, we conducted a population-based case-control study encompassing forty-one newly-diagnosed cases of amyotrophic lateral sclerosis and eighty-two age- and sex-matched controls. We measured long-term intake of inorganic selenium along with other potentially neurotoxic trace elements. RESULTS: We found that consumption of drinking water containing ≥ 1 μg/l of inorganic selenium was associated with a relative risk for amyotrophic lateral sclerosis of 5.4 (95% confidence interval 1.1-26) after adjustment for confounding factors. Greater amounts of cumulative inorganic selenium intake were associated with progressively increasing effects, with a relative risk of 2.1 (95% confidence interval 0.5-9.1) for intermediate levels of cumulative intake and 6.4 (95% confidence interval 1.3-31) for high intake. CONCLUSION: Based on these results, coupled with other epidemiologic data and with findings from animal studies that show specific toxicity of the trace element on motor neurons, we hypothesize that dietary intake of inorganic selenium through drinking water increases the risk for amyotrophic lateral sclerosis."
KENNETH J ROTHMAN,Association of Apgar Score at Five Minutes with Long-Term Neurologic Disability and Cognitive Function in a Prevalence Study of Danish Conscripts,"BACKGROUND. Apgar score is used for rapid assessment of newborns. Low five-minute Apgar score has been associated with increased risk of severe neurologic outcome, but data on milder outcomes, particularly in the long term, are limited. We aimed to examine the association of five-minute Apgar score with prevalence of neurologic disability and with cognitive function in early adulthood. METHODS. We conducted a prevalence study among draft-liable men born in Denmark in 1978–1983 and presenting for the mandatory army evaluation in a northern Danish conscription district. We linked records of this evaluation, which includes medical exam and intelligence testing, with the conscripts' records in the Medical Birth Registry, containing perinatal data. We examined prevalence of neurologic disability and of low cognitive function according to five-minute Apgar score. RESULTS. Less than 1% (136/19,559) of the conscripts had 5-minute Apgar scores <7. Prevalence of neurologic disability was 2.2% (435/19,559) overall; among conscripts with Apgar scores <7, 7–9, and 10 (reference), it was 8.8%, 2.5%, and 2.2% respectively. The corresponding prevalences of low cognitive function (intelligence test score in the bottom quartile) were 34.9%, 27.2%, and 25.0%. The outcomes were more prevalent if Apgar score <7 was accompanied by certain fetal or obstetric adversities. After accounting for perinatal characteristics, 5-mintue Apgar score <7 was associated with prevalence ratios of 4.02 (95% confidence interval: 2.24; 7.24) for neurologic disability and 1.33 (0.94; 1.88) for low cognitive function. CONCLUSION. A five-minute Apgar score <7 has a consistent association with prevalence of neurologic disability and with low cognitive function in early adulthood."
KENNETH J ROTHMAN,"Migraine, Fibromyalgia, and Depression among People with IBS: A Prevalence Study","BACKGROUND. Case descriptions suggest IBS patients are more likely to have other disorders, including migraine, fibromyalgia, and depression. We sought to examine the prevalence of these conditions in cohorts of people with and without IBS. METHODS. The source of data was a large U.S. health plan from January 1, 1996 though June 30, 2002. We identified all people with a medical claim associated with an ICD-9 code for IBS. A non-IBS cohort was a random sample of people with an ICD-9 code for routine medical care. In the cohorts, we identified all claims for migraine, depression, and fibromyalgia. We estimated the prevalence odds ratios (PORs) of each of the three conditions using the Mantel-Haenszel method. We conducted quantitative sensitivity analyses to quantify the impact of residual confounding and in differential outcome identification. RESULTS. We identified 97,593 people in the IBS cohort, and a random sample of 27,402 people to compose the non-IBS comparison cohort. With adjustment, there was a 60% higher odds in the IBS cohort of having any one of the three disorders relative to the comparison cohort (POR 1.6, 95% CI 1.5 – 1.7). There was a 40% higher odds of depression in the IBS cohort (POR 1.4, 95% CI 1.3 – 1.4). The PORs for fibromyalgia and migraine were similar (POR for fibromyalgia 1.8, 95% CI 1.7 – 1.9; POR for migraine 1.6, 95% CI 1.4 – 1.7). Differential prevalence of an unmeasured confounder, or imperfect sensitivity or specificity of outcome detection would have impacted the observed results. CONCLUSION. People in the IBS cohort had a 40% to 80% higher prevalence odds of migraine, fibromyalgia, and depression."
KENNETH J ROTHMAN,Apgar Score and Hospitalization for Epilepsy in Childhood: A Registry-Based Cohort Study,"BACKGROUND. A depressed Apgar score at 5 minutes is a marker for perinatal insults, including neurologic damage. We examined the association between 5-minute Apgar score and the risk of epilepsy hospitalization in childhood. METHODS. Using records linked from population registries, we conducted a cohort study among singleton children born alive in the period 1978–2001 in North Jutland County, Denmark. The first hospital discharge diagnosis of epilepsy during the follow-up time was the main outcome. We followed each child for up to 12 years, calculated absolute risks and risk differences, and used a Poisson regression model to estimate risk ratios for epilepsy hospitalization. We adjusted risk ratio estimates for birth weight, gestational age, mode of delivery, birth presentation, mother's age at delivery, and birth defects. RESULTS. One percent of the 131,853 eligible newborns had a 5-minute Apgar score <7. These children were more likely to be hospitalized with epilepsy during the follow-up than were children with an Apgar score of 7 or greater. The crude risk difference for epilepsy hospitalization was 2.5 cases per 100 (95% confidence interval [CI] 1.3 to 3.8). The risk difference estimates were greater in the presence of other perinatal risk factors. The adjusted risk ratio was 2.4 (95% CI 1.5 to 3.8). Half of the 12-year risk for epilepsy hospitalization in those with a depressed Apgar score occurred during the first year of life. The risk ratio during the first year of life was 4.9 (95% CI 2.0 to 12.3). CONCLUSION. An Apgar score <7 at five minutes predicts an increase in the subsequent risk of epilepsy hospitalization. This association is amplified by other perinatal risk factors."
ASSAF J KFOURY,Systematic Verification of Safety Properties of Arbitrary Network Protocol Compositions Using CHAIN,"Formal correctness of complex multi-party network protocols can be difficult to verify. While models of specific fixed compositions of agents can be checked against design constraints, protocols which lend themselves to arbitrarily many compositions of agents-such as the chaining of proxies or the peering of routers-are more difficult to verify because they represent potentially infinite state spaces and may exhibit emergent behaviors which may not materialize under particular fixed compositions. We address this challenge by developing an algebraic approach that enables us to reduce arbitrary compositions of network agents into a behaviorally-equivalent (with respect to some correctness property) compact, canonical representation, which is amenable to mechanical verification. Our approach consists of an algebra and a set of property-preserving rewrite rules for the Canonical Homomorphic Abstraction of Infinite Network protocol compositions (CHAIN). Using CHAIN, an expression over our algebra (i.e., a set of configurations of network protocol agents) can be reduced to another behaviorally-equivalent expression (i.e., a smaller set of configurations). Repeated applications of such rewrite rules produces a canonical expression which can be checked mechanically. We demonstrate our approach by characterizing deadlock-prone configurations of HTTP agents, as well as establishing useful properties of an overlay protocol for scheduling MPEG frames, and of a protocol for Web intra-cache consistency."
ASSAF J KFOURY,Integrating Sensor-Network Research and Development into a Software Engineering Curriculum,"The emergence of a sensor-networked world produces a clear and urgent need for well-planned, safe and secure software engineering. It is the role of universities to prepare graduates with the knowledge and experience to enter the work-force with a clear understanding of software design and its application to the future safety of computing. The snBench (Sensor Network WorkBench) project aims to provide support to the programming and deployment of Sensor Network Applications, enabling shared sensor embedded spaces to be easily tasked with various sensory applications by different users for simultaneous execution. In this report we discus our experience using the snBench research project as the foundation for semester-long project in a graduate level software engineering class at Boston University (CS511)."
ASSAF J KFOURY,Towards Formalizing Java's Weak References,"Weak references provide the programmer with limited control over the process of memory management. By using them, a programmer can make decisions based on previous actions that are taken by the garbage collector. Although this is often helpful, the outcome of a program using weak references is less predictable due to the nondeterminism they introduce in program evaluation. It is therefore desirable to have a framework of formal tools to reason about weak references and programs that use them. We present several calculi that formalize various aspects of weak references, inspired by their implementation in Java. We provide a calculus to model multiple levels of non-strong references, where a different garbage collection policy is applied to each level. We consider different collection policies such as eager collection and lazy collection. Similar to the way they are implemented in Java, we give the semantics of eager collection to weak references and the semantics of lazy collection to soft references. Moreover, we condition garbage collection on the availability of time and space resources. While time constraints are used in order to restrict garbage collection, space constraints are used in order to trigger it. Finalizers are a problematic feature in Java, especially when they interact with weak references. We provide a calculus to model finalizer evaluation. Since finalizers have little meaning in a language without side-effect, we introduce a limited form of side effect into the calculus. We discuss determinism and the separate notion of uniqueness of (evaluation) outcome. We show that in our calculus, finalizer evaluation does not affect uniqueness of outcome."
ASSAF J KFOURY,Some Considerations on a Calculus with Weak References,"Weak references are references that do not prevent the object they point to from being garbage collected. Most realistic languages, including Java, SML/NJ, and OCaml to name a few, have some facility for programming with weak references. Weak references are used in implementing idioms like memoizing functions and hash-consing in order to avoid potential memory leaks. However, the semantics of weak references in many languages are not clearly specified. Without a formal semantics for weak references it becomes impossible to prove the correctness of implementations making use of this feature. Previous work by Hallett and Kfoury extends λgc, a language for modeling garbage collection, to λweak, a similar language with weak references. Using this previously formalized semantics for weak references, we consider two issues related to well-behavedness of programs. Firstly, we provide a new, simpler proof of the well-behavedness of the syntactically restricted fragment of λweak defined previously. Secondly, we give a natural semantic criterion for well-behavedness much broader than the syntactic restriction, which is useful as principle for programming with weak references. Furthermore we extend the result, proved in previously of λgc, which allows one to use type-inference to collect some reachable objects that are never used. We prove that this result holds of our language, and we extend this result to allow the collection of weakly-referenced reachable garbage without incurring the computational overhead sometimes associated with collecting weak bindings (e.g. the need to recompute a memoized function). Lastly we use extend the semantic framework to model the key/value weak references found in Haskell and we prove the Haskell is semantics equivalent to a simpler semantics due to the lack of side-effects in our language."
ASSAF J KFOURY,Type Systems for a Network Specification Language With Multiple-Choice Let,"When analysing the behavior of complex networked systems, it is often the case that some components within that network are only known to the extent that they belong to one of a set of possible ""implementations"" – e.g., versions of a specific protocol, class of schedulers, etc. In this report we augment the specification language considered in BUCSTR-2004-021, BUCS-TR-2005-014, BUCS-TR-2005-015, and BUCS-TR-2005-033, to include a non-deterministic multiple-choice let-binding, which allows us to consider compositions of networking subsystems that allow for looser component specifications."
ASSAF J KFOURY,Safe Compositional Specification of Networking Systems: A Compositional Analysis Approach,"We present a type inference algorithm, in the style of compositional analysis, for the language TRAFFIC—a specification language for flow composition applications proposed in [2]—and prove that this algorithm is correct: the typings it infers are principal typings, and the typings agree with syntax-directed type checking on closed flow specifications. This algorithm is capable of verifying partial flow specifications, which is a significant improvement over syntax-directed type checking algorithm presented in [3]. We also show that this algorithm runs efficiently, i.e., in low-degree polynomial time."
ASSAF J KFOURY,Safe Compositional Specification of Networking Systems: TRAFFIC The Language and Its Type Checking,"This paper formally defines the operational semantic for TRAFFIC, a specification language for flow composition applications proposed in BUCS-TR-2005-014, and presents a type system based on desired safety assurance. We provide proofs on reduction (weak-confluence, strong-normalization and unique normal form), on soundness and completeness of type system with respect to reduction, and on equivalence classes of flow specifications. Finally, we provide a pseudo-code listing of a syntax-directed type checking algorithm implementing rules of the type system capable of inferring the type of a closed flow specification."
ASSAF J KFOURY,Computational Properties of SNAFU,"Sensor applications in Sensoria [1] are expressed using STEP (Sensorium Task Execution Plan). SNAFU (Sensor-Net Applications as Functional Units) serves as a high-level sensor-programming language, which is compiled into STEP. In SNAFU’s current form, its differences with STEP are relatively minor, as they are limited to shorthands and macros not available in STEP. We show that, however restrictive it may seem, SNAFU has in fact universal power; technically, it is a Turing-complete language, i.e., any Turing program can be written in SNAFU (though not always conveniently). Although STEP may be allowed to have universal power, as a low-level language not directly available to Sensorium users, SNAFU programmers may use this power for malicious purposes or inadvertently introduce errors with destructive consequences. In future developments of SNAFU, we plan to introduce restrictions and highlevel features with safety guards, such as those provided by a type system, which will make SNAFU programming safer."
ASSAF J KFOURY,What Are Polymorphically-Typed Ambients?,"Abstract: The Ambient Calculus was developed by Cardelli and Gordon as a formal framework to study issues of mobility and migrant code. We consider an Ambient Calculus where ambients transport and exchange programs rather that just inert data. We propose different senses in which such a calculus can be said to be polymorphically typed, and design accordingly a polymorphic type system for it. Our type system assigns types to embedded programs and what we call behaviors to processes; a denotational semantics of behaviors is then proposed, here called trace semantics, underlying much of the remaining analysis. We state and prove a Subject Reduction property for our polymorphically typed calculus. Based on techniques borrowed from finite automata theory, type-checking of fully type-annotated processes is shown to be decidable; the time complexity of our decision procedure is exponential (this is a worst-case in theory, arguably not encountered in practice). Our polymorphically-typed calculus is a conservative extension of the typed Ambient Calculus originally proposed by Cardelli and Gordon."
ASSAF J KFOURY,Type Inference For Recursive Definitions,"We consider type systems that combine universal types, recursive types, and object types. We study type inference in these systems under a rank restriction, following Leivant's notion of rank. To motivate our work, we present several examples showing how our systems can be used to type programs encountered in practice. We show that type inference in the rank-k system is decidable for k ≤ 2 and undecidable for k ≥ 3. (Similar results based on different techniques are known to hold for System F, without recursive types and object types.) Our undecidability result is obtained by a reduction from a particular adaptation (which we call ""regular"") of the semi-unification problem and whose undecidability is, interestingly, obtained by methods totally different from those used in the case of standard (or finite) semi-unification."
ASSAF J KFOURY,Typed Abstraction of Complex Network Compositions,"The heterogeneity and open nature of network systems make analysis of compositions of components quite challenging, making the design and implementation of robust network services largely inaccessible to the average programmer. We propose the development of a novel type system and practical type spaces which reflect simplified representations of the results and conclusions which can be derived from complex compositional theories in more accessible ways, essentially allowing the system architect or programmer to be exposed only to the inputs and output of compositional analysis without having to be familiar with the ins and outs of its internals. Toward this end we present the TRAFFIC (Typed Representation and Analysis of Flows For Interoperability Checks) framework, a simple flow-composition and typing language with corresponding type system. We then discuss and demonstrate the expressive power of a type space for TRAFFIC derived from the network calculus, allowing us to reason about and infer such properties as data arrival, transit, and loss rates in large composite network applications."
ASSAF J KFOURY,StaXML: Static Typing of XML Document Fragments for Imperative Web Scripting Languages,"We present a type system, StaXML, which employs the stacked type syntax to represent essential aspects of the potential roles of XML fragments to the structure of complete XML documents. The simplest application of this system is to enforce well-formedness upon the construction of XML documents without requiring the use of templates or balanced ""gap plugging"" operators; this allows it to be applied to programs written according to common imperative web scripting idioms, particularly the echoing of unbalanced XML fragments to an output buffer. The system can be extended to verify particular XML applications such as XHTML and identifying individual XML tags constructed from their lexical components. We also present StaXML for PHP, a prototype precompiler for the PHP4 scripting language which infers StaXML types for expressions without assistance from the programmer."
ASSAF J KFOURY,Safe Composition of Web Communication Protocols for Extensible Edge Services,"As new multi-party edge services are deployed on the Internet, application-layer protocols with complex communication models and event dependencies are increasingly being specified and adopted. To ensure that such protocols (and compositions thereof with existing protocols) do not result in undesirable behaviors (e.g., livelocks) there needs to be a methodology for the automated checking of the ""safety"" of these protocols. In this paper, we present ingredients of such a methodology. Specifically, we show how SPIN, a tool from the formal systems verification community, can be used to quickly identify problematic behaviors of application-layer protocols with non-trivial communication models—such as HTTP with the addition of the ""100 Continue"" mechanism. As a case study, we examine several versions of the specification for the Continue mechanism; our experiments mechanically uncovered multi-version interoperability problems, including some which motivated revisions of HTTP/1.1 and some which persist even with the current version of the protocol. One such problem resembles a classic degradation-of-service attack, but can arise between well-meaning peers. We also discuss how the methods we employ can be used to make explicit the requirements for hardening a protocol's implementation against potentially malicious peers, and for verifying an implementation's interoperability with the full range of allowable peer behaviors."
ASSAF J KFOURY,Using Alloy to formally model and reason about an OpenFlow network switch,"Openflow provides a standard interface for partitioning a network into a data plane and a programmatic control plane. While providing easy network reconfiguration, Openflow introduces the potential for programming bugs, causing network deficiency. To study the behavior of OpenFlow switchs, we used Alloy to create a software abstraction, describing the internal state of a network and its OpenFlow switches. Hence, this work is an attempt to model the static and dynamic behaviour of networks configured using OpenFlow switches."
ASSAF J KFOURY,Safe Compositional Specification of Networking Systems,"The Science of Network Service Composition has clearly emerged as one of the grand themes driving many of our research questions in the networking field today [NeXtworking 2003]. This driving force stems from the rise of sophisticated applications and new networking paradigms. By ""service composition"" we mean that the performance and correctness properties local to the various constituent components of a service can be readily composed into global (end-to-end) properties without re-analyzing any of the constituent components in isolation, or as part of the whole composite service. The set of laws that would govern such composition is what will constitute that new science of composition. The combined heterogeneity and dynamic open nature of network systems makes composition quite challenging, and thus programming network services has been largely inaccessible to the average user. We identify (and outline) a research agenda in which we aim to develop a specification language that is expressive enough to describe different components of a network service, and that will include type hierarchies inspired by type systems in general programming languages that enable the safe composition of software components. We envision this new science of composition to be built upon several theories (e.g., control theory, game theory, network calculus, percolation theory, economics, queuing theory). In essence, different theories may provide different languages by which certain properties of system components can be expressed and composed into larger systems. We then seek to lift these lower-level specifications to a higher level by abstracting away details that are irrelevant for safe composition at the higher level, thus making theories scalable and useful to the average user. In this paper we focus on services built upon an overlay management architecture, and we use control theory and QoS theory as example theories from which we lift up compositional specifications."
ASSAF J KFOURY,Validating Arbitrarily Large Network Protocol Compositions with Finite Computation,"Formal tools like finite-state model checkers have proven useful in verifying the correctness of systems of bounded size and for hardening single system components against arbitrary inputs. However, conventional applications of these techniques are not well suited to characterizing emergent behaviors of large compositions of processes. In this paper, we present a methodology by which arbitrarily large compositions of components can, if sufficient conditions are proven concerning properties of small compositions, be modeled and completely verified by performing formal verifications upon only a finite set of compositions. The sufficient conditions take the form of reductions, which are claims that particular sequences of components will be causally indistinguishable from other shorter sequences of components. We show how this methodology can be applied to a variety of network protocol applications, including two features of the HTTP protocol, a simple active networking applet, and a proposed web cache consistency algorithm. We also doing discuss its applicability to framing protocol design goals and to representing systems which employ non-model-checking verification methodologies. Finally, we brieﬂy discuss how we hope to broaden this methodology to more general topological compositions of network applications."
ASSAF J KFOURY,Mathematical logic in computer science,
ASSAF J KFOURY,Efficient reassembling of three-regular planar graphs,
ASSAF J KFOURY,A fixed-parameter linear-time algorithm for maximum flow in planar flow networks,
NOYAN GOKCE,B Cell Activation in Insulin Resistance and Obesity,"Our group has demonstrated that inflammatory diseases such as type 2 diabetes (DM), inflammatory bowel disease (IBD), and periodontal disease (PD) are associated with altered B cell function that may contribute to disease pathogenesis. B cells were found to be highly activated with characteristics of inflammatory cells. Obesity is a pre-disease state for cardiovascular disease and type 2 diabetes and is considered a state of chronic inflammation. Therefore, we sought to better characterize B cell function and phenotype in obese patients. We demonstrate that (Toll-like receptor) TLR4 and CD36 expression by B cells is elevated in obese subjects, suggesting increased sensing of lipopolysaccharide (LPS) and other TLR ligands. These ligands may be of microbial, from translocation from a leaky gut, or host origin. To better assess microbial ligand burden and host response in the bloodstream, we measured LPS binding protein (LBP), bacterial/permeability increasing protein (BPI), and high mobility group box 1 (HMGB1). Thus far, our data demonstrate an increase in LBP in DM and obesity indicating increased responses to TLR ligands in the blood. Interestingly, B cells responded to certain types of LPS by phosphorylating extracellular-signal-regulated kinases (ERK) 1/2. A better understanding of the immunological state of obesity and the microbial and endogenous TLR ligands that may be activating B cells will help identify novel therapeutics to reduce the risk of more dangerous conditions, such as cardiovascular disease."
KATHLEEN CORRIVEAU,The theoretical and methodological opportunities afforded by guided play with young children,"For infants and young children, learning takes place all the time and everywhere. How children learn best both in and out of school has been a long-standing topic of debate in education, cognitive development, and cognitive science. Recently, guided play has been proposed as an integrative approach for thinking about learning as a child-led, adult-assisted playful activity. The interactive and dynamic nature of guided play presents theoretical and methodological challenges and opportunities. Drawing upon research from multiple disciplines, we discuss the integration of cutting-edge computational modeling and data science tools to address some of these challenges, and highlight avenues toward an empirically grounded, computationally precise and ecologically valid framework of guided play in early education."
KATHLEEN CORRIVEAU,Embedding scientific explanations into storybooks impacts children's scientific discourse and learning,"Children's understanding of unobservable scientific entities largely depends on testimony from others, especially through parental explanations that highlight the mechanism underlying a scientific entity. Mechanistic explanations are particularly helpful in promoting children's conceptual understanding, yet they are relatively rare in parent-child conversations. The current study aimed to increase parent-child use of mechanistic conversation by modeling this language in a storybook about the mechanism of electrical circuits. We also examined whether an increase in mechanistic conversation was associated with science learning outcomes, measured at both the dyadic- and child-level. In the current study, parents and their 4- to 5-year-old children (N = 60) were randomly assigned to read a book containing mechanistic explanations (n = 32) or one containing non-mechanistic explanations (n = 28). After reading the book together, parent-child joint understanding of electricity's mechanism was tested by asking the dyad to assemble electrical components of a circuit toy so that a light would turn on. Finally, child science learning outcomes were examined by asking children to assemble a novel circuit toy and answer comprehension questions to gauge their understanding of electricity's mechanism. Results indicate that dyads who read storybooks containing mechanistic explanations were (1) more successful at completing the circuit (putting the pieces together to make the light turn on) and (2) used more mechanistic language than dyads assigned to the non-mechanistic condition. Children in the mechanistic condition also had better learning outcomes, but only if they engaged in more mechanistic discourse with their parent. We discuss these results using a social interactionist framework to highlight the role of input and interaction for learning. We also highlight how these results implicate everyday routines such as book reading in supporting children's scientific discourse and understanding."
KATHLEEN CORRIVEAU,Investigating science together: inquiry-based training promotes scientific conversations in parent-child interactions,"This study examined the effects of two pedagogical training approaches on parent-child dyads' discussion of scientific content in an informal museum setting. Forty-seven children (mean age = 5.43) and their parents were randomly assigned to training conditions where an experimenter modeled one of two different pedagogical approaches when interacting with the child and a science-based activity: (1) a scientific inquiry-based process or (2) a scientific statement-sharing method. Both approaches provided the same information about scientific mechanisms but differed in the process through which that content was delivered. Immediately following the training, parents were invited to model the same approach with their child with a novel science-based activity. Results indicated significant differences in the process through which parents prompted discussion of the targeted information content: when talking about causal scientific concepts, parents in the scientific inquiry condition were significantly more likely to pose questions to their child than parents in the scientific statements condition. Moreover, children in the scientific inquiry condition were marginally more responsive to parental causal talk and provided significantly more scientific content in response. These findings provide initial evidence that training parents to guide their children using scientific inquiry-based approaches in informal learning settings can encourage children to participate in more joint scientific conversations."
KATHLEEN CORRIVEAU,‘We practise every day’: parents’ attitudes towards early science learning and education among a sample of urban families in Ireland,"Educational policies increasingly emphasise early childhood science engagement. As key influencers in children’s early learning, parents (n = 85) attending a science workshop in three urban schools in Ireland were surveyed regarding their attitudes towards science. Seventy per cent of parents believed that science education should begin in the pre-school years, before the age of four. Despite high levels of education, at least half of the parents expressed some lack of confidence in talking about, and in doing science with, their young children. Parents who reported less confidence in doing science activities with their children also reported reduced frequency of activities for five out of the seven science learning opportunities listed. Mothers, compared to fathers, reported less confidence in doing science activities with their children. Findings indicate that parents’ confidence in science may impact early science experiences and highlight parents as a key support for increasing early science engagement."
DAVID BISHOP,Searching for the Casimir Energy,"In this article, we present a nano-electromechanical system (NEMS) designed to detect changes in the Casimir Energy. The Casimir effect is a result of the appearance of quantum fluctuations in the electromagnetic vacuum. Previous experiments have used nano- or micro- scale parallel plate capacitors to detect the Casimir force by measuring the small attractive force these fluctuations exert between the two surfaces. In this new set of experiments, we aim to directly detect shifts in the Casimir energy in the vacuum due to the presence of metallic parallel plates, one of which is a superconductor. A change in the Casimir energy of this configuration is predicted to shift the superconducting transition temperature (T_c) because of an interaction between it and the superconducting condensation energy. The experiment we discuss consists of taking a superconducting film, carefully measuring its transition temperature, bringing a conducting plate close to the film, creating a Casimir cavity, and then measuring the transition temperature again. The expected shifts will be small, comparable to the normal shifts one sees in cycling superconducting films to cryogenic temperatures and so using a NEMS resonator and doing this in situ is the only practical way to obtain accurate, reproducible data. Using a thin Pb film and opposing Au surface, we observe no shift in T_c greater than 12 𝜇K down to a minimum spacing of approximately 70 nm."
DAVID BISHOP,Mems device with large out-of-plane actuation and low-resistance interconnect and methods of use,"The present application is directed to a MEMS device. The MEMS device includes a substrate having a first end and a second end extending along a longitudinal axis, the Substrate including an electrostatic actuator. The device also includes a movable plate having a first end and a second end. The device also includes a thermal actuator having a first end coupled to the first end of the substrate and a second end coupled to the first end of the plate. The actuator moves the plate in relation to the substrate. Further, the device includes a power source electrically coupled to the thermal actuator and the Substrate. The application is also directed to a method for operating a MEMS device."
DAVID BISHOP,"Atomic calligraphy, a fab on a chip approach to lithography","The traditional approach to keeping pace with Moore's Law has largely been improving conventional semiconductor foundry technology. Here we present an alternative approach called Fab-On-a-Chip where conventional foundry machines are used to create nano-fabrication machines, including deposition, sensing, and lithography tools. Chip based versions of all these tools will be touched on, but the focus of the talk will be a lithography technique called atomic calligraphy. Atomic calligraphy uses MEMS based nano-positioners to move a plate with potentially sub-nanometer resolution. Single apertures, arrays of apertures, or complex designs can be milled into the plate and used to mask depositions. Though serial, atomic calligraphy can achieving large throughputs because it can be scaled to tens of thousands of devices fabricating in parallel. We will show examples of nanoscale structures made with atomic calligraphy including arrays of structures and structures covering a large area. We will also discuss methods used to prevent the clogging of apertures, to push the resolution limits to sub-nanometer, and to array devices to achieve large throughput. (Abstract from: http://meetings.aps.org/link/BAPS.2018.MAR.L15.2)"
DAVID BISHOP,Casimir driven parametric amplification of a MEMS accelerometer,"In this work, we use the gradient of the attractive Casimir interaction existing between an Au-coated silicon plate and an Au-coated microsphere bonded to the proof mass of a commercial MEMS capacitive accelerometer to modulate the effective spring constant of the MEMS proof mass. By modulating this system parameter, the accelerometer output at resonance can be greatly amplified or de-amplified depending on the frequency and phase of modulation. The extraordinary distance dependence of the Casimir force (1/r3 for a sphere-plane geometry) leveraged with the parametric amplification results in a system capable of resolving sub pm changes in sphere-plate separation or sub pN changes in force. On top of that, the robust built-in signal processing in the MEMS circuitry provides low noise density of ~fN/√Hz. Such a highly sensitive device allows for further investigation into the not yet fully understood physics of Casimir cavities and also provides a versatile platform for conducting a variety of quantum metrology experiments."
DAVID BISHOP,Feedforward control algorithms for MEMS galvos and scanners,
DAVID BISHOP,Zeptometer metrology using the Casimir effect,"In this paper, we discuss using the Casimir force in conjunction with a MEMS parametric amplifier to construct a quantum displacement amplifier. Such a mechanical amplifier converts DC displacements into much larger AC oscillations via the quantum gain of the system which, in some cases, can be a factor of a million or more. This would allow one to build chip scale metrology systems with zeptometer positional resolution. This approach leverages quantum fluctuations to build a device with a sensitivity that can’t be obtained with classical systems."
DAVID BISHOP,Analysis of a Casimir-driven parametric amplifier with resilience to Casimir pull-in for MEMS single-point magnetic gradiometry,"The Casimir force, a quantum mechanical effect, has been observed in several microelectromechanical system (MEMS) platforms. Due to its extreme sensitivity to the separation of two objects, the Casimir force has been proposed as an excellent avenue for quantum metrology. Practical application, however, is challenging due to attractive forces leading to stiction and device failure, called Casimir pull-in. In this work, we design and simulate a Casimir-driven metrology platform, where a time-delay-based parametric amplification technique is developed to achieve a steady-state and avoid pull-in. We apply the design to the detection of weak, low-frequency, gradient magnetic fields similar to those emanating from ionic currents in the heart and brain. Simulation parameters are selected from recent experimental platforms developed for Casimir metrology and magnetic gradiometry, both on MEMS platforms. While a MEMS offers many advantages to such an application, the detected signal must typically be at the resonant frequency of the device, with diminished sensitivity in the low frequency regime of biomagnetic fields. Using a Casimir-driven parametric amplifier, we report a 10,000-fold improvement in the best-case resolution of MEMS single-point gradiometers, with a maximum sensitivity of 6 Hz/(pT/cm) at 1 Hz. Further development of the proposed design has the potential to revolutionize metrology and may specifically enable the unshielded monitoring of biomagnetic fields in ambient conditions."
DAVID BISHOP,Renal mechanisms for the conservation of base,
DAVID BISHOP,Bone Marrow Lesions from Osteoarthritis Knees Are Characterized by Sclerotic Bone that Is Less Well Mineralized,"INTRODUCTION. Although the presence of bone marrow lesions (BMLs) on magnetic resonance images is strongly associated with osteoarthritis progression and pain, the underlying pathology is not well established. The aim of the present study was to evaluate the architecture of subchondral bone in regions with and without BMLs from the same individual using bone histomorphometry. METHODS. Postmenopausal female subjects (n = 6, age 48 to 90 years) with predominantly medial compartment osteoarthritis and on a waiting list for total knee replacement were recruited. To identify the location of the BMLs, subjects had a magnetic resonance imaging scan performed on their study knee prior to total knee replacement using a GE 1.5 T scanner with a dedicated extremity coil. An axial map of the tibial plateau was made, delineating the precise location of the BML. After surgical removal of the tibial plateau, the BML was localized using the axial map from the magnetic resonance image and the lesion excised along with a comparably sized bone specimen adjacent to the BML and from the contralateral compartment without a BML. Cores were imaged via microcomputed tomography, and the bone volume fraction and tissue mineral density were calculated for each core. In addition, the thickness of the subchondral plate was measured, and the following quantitative metrics of trabecular structure were calculated for the subchondral trabecular bone in each core: trabecular number, thickness, and spacing, structure model index, connectivity density, and degree of anisotropy. We computed the mean and standard deviation for each parameter, and the unaffected bone from the medial tibial plateau and the bone from the lateral tibial plateau were compared with the affected BML region in the medial tibial plateau. RESULTS. Cores from the lesion area displayed increased bone volume fraction but reduced tissue mineral density. The samples from the subchondral trabecular lesion area exhibited increased trabecular thickness and were also markedly more plate-like than the bone in the other three locations, as evidenced by the lower value of the structural model index. Other differences in structure that were noted were increased trabecular spacing and a trend towards decreased trabecular number in the cores from the medial location as compared with the contralateral location. CONCLUSIONS. Our preliminary data localize specific changes in bone mineralization, remodeling and defects within BMLs features that are adjacent to the subchondral plate. These BMLs appear to be sclerotic compared with unaffected regions from the same individual based on the increased bone volume fraction and increased trabecular thickness. The mineral density in these lesions, however, is reduced and may render this area to be mechanically compromised, and thus susceptible to attrition."
DAVID BISHOP,A system for probing Casimir energy corrections to the condensation energy,"In this article, we present a nanoelectromechanical system (NEMS) designed to detect changes in the Casimir energy. The Casimir effect is a result of the appearance of quantum fluctuations in an electromagnetic vacuum. Previous experiments have used nano- or microscale parallel plate capacitors to detect the Casimir force by measuring the small attractive force these fluctuations exert between the two surfaces. In this new set of experiments, we aim to directly detect the shifts in the Casimir energy in a vacuum due to the presence of the metallic parallel plates, one of which is a superconductor. A change in the Casimir energy of this configuration is predicted to shift the superconducting transition temperature (Tc) because of the interaction between it and the superconducting condensation energy. In our experiment, we take a superconducting film, carefully measure its transition temperature, bring a conducting plate close to the film, create a Casimir cavity, and then measure the transition temperature again. The expected shifts are smaller than the normal shifts one sees in cycling superconducting films to cryogenic temperatures, so using a NEMS resonator in situ is the only practical way to obtain accurate, reproducible data. Using a thin Pb film and opposing Au surface, we observe no shift in Tc >12 µK down to a minimum spacing of ~70 nm at zero applied magnetic field."
DAVID BISHOP,100 pT/cm single-point MEMS magnetic gradiometer from a commercial accelerometer,"Magnetic sensing is present in our everyday interactions with consumer electronics and demonstrates the potential for the measurement of extremely weak biomagnetic fields, such as those of the heart and brain. In this work, we leverage the many benefits of microelectromechanical system (MEMS) devices to fabricate a small, low-power, and inexpensive sensor whose resolution is in the range of biomagnetic fields. At present, biomagnetic fields are measured only by expensive mechanisms such as optical pumping and superconducting quantum interference devices (SQUIDs), suggesting a large opportunity for MEMS technology in this work. The prototype fabrication is achieved by assembling micro-objects, including a permanent micromagnet, onto a postrelease commercial MEMS accelerometer using a pick-and-place technique. With this system, we demonstrate a room-temperature MEMS magnetic gradiometer. In air, the sensor's response is linear, with a resolution of 1.1 nT cm-1, spans over 3 decades of dynamic range to 4.6 µT cm-1, and is capable of off-resonance measurements at low frequencies. In a 1 mTorr vacuum with 20 dB magnetic shielding, the sensor achieves a 100 pT cm-1 resolution at resonance. This resolution represents a 30-fold improvement compared with that of MEMS magnetometer technology and a 1000-fold improvement compared with that of MEMS gradiometer technology. The sensor is capable of a small spatial resolution with a magnetic sensing element of 0.25 mm along its sensitive axis, a >4-fold improvement compared with that of MEMS gradiometer technology. The calculated noise floor of this platform is 110 fT cm-1 Hz-1/2, and thus, these devices hold promise for both magnetocardiography (MCG) and magnetoencephalography (MEG) applications."
DAVID BISHOP,Probing the subcellular nanostructure of engineered human cardiomyocytes in 3D tissue,"The structural and functional maturation of human induced pluripotent stem cell-derived cardiomyocytes (hiPSC-CMs) is essential for pharmaceutical testing, disease modeling, and ultimately therapeutic use. Multicellular 3D-tissue platforms have improved the functional maturation of hiPSC-CMs, but probing cardiac contractile properties in a 3D environment remains challenging, especially at depth and in live tissues. Using small-angle X-ray scattering (SAXS) imaging, we show that hiPSC-CMs matured and examined in a 3D environment exhibit a periodic spatial arrangement of the myofilament lattice, which has not been previously detected in hiPSC-CMs. The contractile force is found to correlate with both the scattering intensity (R 2 = 0.44) and lattice spacing (R 2 = 0.46). The scattering intensity also correlates with lattice spacing (R 2 = 0.81), suggestive of lower noise in our structural measurement than in the functional measurement. Notably, we observed decreased myofilament ordering in tissues with a myofilament mutation known to lead to hypertrophic cardiomyopathy (HCM). Our results highlight the progress of human cardiac tissue engineering and enable unprecedented study of structural maturation in hiPSC-CMs."
DAVID BISHOP,Building a Casimir metrology platform with a commercial MEMS sensor,"The Casimir Effect is a physical manifestation of quantum fluctuations of the electromagnetic vacuum. When two metal plates are placed close together, typically much less than a micron, the long wavelength modes between them are frozen out, giving rise to a net attractive force between the plates, scaling as d -4 (or d -3 for a spherical-planar geometry) even when they are not electrically charged. In this paper, we observe the Casimir Effect in ambient conditions using a modified capacitive micro-electromechanical system (MEMS) sensor. Using a feedback-assisted pick-and-place assembly process, we are able to attach various microstructures onto the post-release MEMS, converting it from an inertial force sensor to a direct force measurement platform with pN (piconewton) resolution. With this system we are able to directly measure the Casimir force between a silver-coated microsphere and gold-coated silicon plate. This device is a step towards leveraging the Casimir Effect for cheap, sensitive, room temperature quantum metrology."
DAVID BISHOP,Direct laser writing for cardiac tissue engineering: a microfluidic heart on a chip with integrated transducers,"We have developed a microfluidic platform for engineering cardiac microtissues in highly-controlled microenvironments. The platform is fabricated using direct laser writing (DLW) lithography and soft lithography, and contains four separate devices. Each individual device houses a cardiac microtissue and is equipped with an integrated strain actuator and a force sensor. Application of external pressure waves to the platform results in controllable time-dependent forces on the microtissues. Conversely, oscillatory forces generated by the microtissues are transduced into measurable electrical outputs. We demonstrate the capabilities of this platform by studying the response of cardiac microtissues derived from human induced pluripotent stem cells (hiPSC) under prescribed mechanical loading and pacing. This platform will be used for fundamental studies and drug screening on cardiac microtissues."
DAVID BISHOP,The integration of optical stimulation in a mechanically dynamic cell culture substrate,"A cell culture well with integrated mechanical and optical stimulation is presented. This is achieved by combining dielectric elastomer soft actuators, also known as artificial muscles, and a varifocal micro-electromechanical mirror that couples light from an optical fiber and focuses it onto the transparent cell substrate. The device enables unprecedented control of in vitro cell cultures by allowing the experimenter to tune and synchronize mechanical and optical stimuli, thereby enabling new experimental assays in optogenetics, fluorescent microscopy, or laser stimulation that include dynamic mechanical strain as a controlled input parameter."
DAVID BISHOP,Monolayer MoS2 strained to 1.3% with a microelectromechanical system,"We report on a modified transfer technique for atomically thin materials integrated onto microelectromechanical systems (MEMS) for studying strain physics and creating strain-based devices. Our method tolerates the non-planar structures and fragility of MEMS, while still providing precise positioning and crack free transfer of flakes. Further, our method used the transfer polymer to anchor the 2D crystal to the MEMS, which reduces the fabrication time, increases the yield, and allowed us to exploit the strong mechanical coupling between 2D crystal and polymer to strain the atomically thin system. We successfully strained single atomic layers of molybdenum disulfide (MoS2) with MEMS devices for the first time and achieved greater than 1.3% strain, marking a major milestone for incorporating 2D materials with MEMS We used the established strain response of MoS2 Raman and Photoluminescence spectra to deduce the strain in our crystals and provide a consistency check. We found good comparison between our experiment and literature."
LAURA F WHITE,Endemicity is not a victory: the unmitigated downside risks of widespread SARS-CoV-2 transmission,"The strategy of relying solely on current SARS-CoV-2 vaccines to halt SARS-CoV-2 transmission has proven infeasible. In response, many public-health authorities have advocated for using vaccines to limit mortality while permitting unchecked SARS-CoV-2 spread (“learning to live with the disease”). The feasibility of this strategy critically depends on the infection fatality rate (IFR) of SARS-CoV-2. An expectation exists that the IFR will decrease due to selection against virulence. In this work, we perform a viral fitness estimation to examine the basis for this expectation. Our findings suggest large increases in virulence for SARS-CoV-2 would result in minimal loss of transmissibility, implying that the IFR may vary freely under neutral evolutionary drift. We use an SEIRS model framework to examine the effect of hypothetical changes in the IFR on steady-state death tolls under COVID-19 endemicity. Our modeling suggests that endemic SARS-CoV-2 implies vast transmission resulting in yearly US COVID-19 death tolls numbering in the hundreds of thousands under many plausible scenarios, with even modest increases in the IFR leading to unsustainable mortality burdens. Our findings highlight the importance of enacting a concerted strategy and continued development of biomedical interventions to suppress SARS-CoV-2 transmission and slow its evolution."
LAURA F WHITE,No magic bullet: limiting in-school transmission in the face of variable SARS-CoV-2 viral loads,"In the face of a long-running pandemic, understanding the drivers of ongoing SARS-CoV-2 transmission is crucial for the rational management of COVID-19 disease burden. Keeping schools open has emerged as a vital societal imperative during the pandemic, but in-school transmission of SARS-CoV-2 can contribute to further prolonging the pandemic. In this context, the role of schools in driving SARS-CoV-2 transmission acquires critical importance. Here we model in-school transmission from first principles to investigate the effectiveness of layered mitigation strategies on limiting in-school spread. We examined the effect of masks and air quality (ventilation, filtration and ionizers) on steady-state viral load in classrooms, as well as on the number of particles inhaled by an uninfected person. The effectiveness of these measures in limiting viral transmission was assessed for variants with different levels of mean viral load (ancestral, Delta, Omicron). Our results suggest that a layered mitigation strategy can be used effectively to limit in-school transmission, with certain limitations. First, poorly designed strategies (insufficient ventilation, no masks, staying open under high levels of community transmission) will permit in-school spread even if some level of mitigation is present. Second, for viral variants that are sufficiently contagious, it may be difficult to construct any set of interventions capable of blocking transmission once an infected individual is present, underscoring the importance of other measures. Our findings provide practical recommendations; in particular, the use of a layered mitigation strategy that is designed to limit transmission, with other measures such as frequent surveillance testing and smaller class sizes (such as by offering remote schooling options to those who prefer it) as needed."
LAURA F WHITE,The long-baseline neutrino experiment: exploring fundamental symmetries of the universe,"The preponderance of matter over antimatter in the early Universe, the dynamics of the supernova bursts that produced the heavy elements necessary for life and whether protons eventually decay --- these mysteries at the forefront of particle physics and astrophysics are key to understanding the early evolution of our Universe, its current state and its eventual fate. The Long-Baseline Neutrino Experiment (LBNE) represents an extensively developed plan for a world-class experiment dedicated to addressing these questions. LBNE is conceived around three central components: (1) a new, high-intensity neutrino source generated from a megawatt-class proton accelerator at Fermi National Accelerator Laboratory, (2) a near neutrino detector just downstream of the source, and (3) a massive liquid argon time-projection chamber deployed as a far detector deep underground at the Sanford Underground Research Facility. This facility, located at the site of the former Homestake Mine in Lead, South Dakota, is approximately 1,300 km from the neutrino source at Fermilab -- a distance (baseline) that delivers optimal sensitivity to neutrino charge-parity symmetry violation and mass ordering effects. This ambitious yet cost-effective design incorporates scalability and flexibility and can accommodate a variety of upgrades and contributions. With its exceptional combination of experimental configuration, technical capabilities, and potential for transformative discoveries, LBNE promises to be a vital facility for the field of particle physics worldwide, providing physicists from around the globe with opportunities to collaborate in a twenty to thirty year program of exciting science. In this document we provide a comprehensive overview of LBNE's scientific objectives, its place in the landscape of neutrino physics worldwide, the technologies it will incorporate and the capabilities it will possess."
ANDREI E RUCKENSTEIN,"Transition temperature of dilute, weakly repulsive Bose gas","Within a quasiparticle framework, we reconsider the issue of computing the Bose-Einstein condensation temperature (T_c) in a weakly non-ideal Bose gas. The main result of this and previous investigations is that T_c increases with the scattering length 𝑎, with the leading dependence being either linear or log-linear in 𝑎. The calculation of T_c reduces to that of computing the excitation spectrum near the transition. We report two approaches to regularizing the infrared divergence at the transition point. One leads to a 𝑎√|ln 𝑎|-like shift in T_c, and the other allows numerical calculations for the shift."
ANDREI E RUCKENSTEIN,Squeezing in the weakly interacting uniform Bose-Einstein condensate,"We investigate the presence of squeezing in the weakly repulsive uniform Bose gas, in both the condensate mode and in the nonzero opposite-momenta mode pairs, using two different variational formulations. We explore the U(1) symmetry breaking and Goldstone’s theorem in the context of a squeezed coherent variational wave function and present the associated Ward identity. We show that squeezing of the condensate mode is absent at the mean field Hartree-Fock-Bogoliubov level and emerges as a result of fluctuations about mean field as a finite volume effect, which vanishes in the thermodynamic limit. On the other hand, the squeezing of the excitations about the condensate survives the thermodynamic limit and is interpreted in terms of density-phase variables using a number-conserving formulation of the interacting Bose gas."
ANDREI E RUCKENSTEIN,Tensor network method for reversible classical computation,"We develop a tensor network technique that can solve universal reversible classical computational problems, formulated as vertex models on a square lattice [Nat. Commun. 8, 15303 (2017)]. By encoding the truth table of each vertex constraint in a tensor, the total number of solutions compatible with partial inputs and outputs at the boundary can be represented as the full contraction of a tensor network. We introduce an iterative compression-decimation (ICD) scheme that performs this contraction efficiently. The ICD algorithm first propagates local constraints to longer ranges via repeated contraction-decomposition sweeps over all lattice bonds, thus achieving compression on a given length scale. It then decimates the lattice via coarse-graining tensor contractions. Repeated iterations of these two steps gradually collapse the tensor network and ultimately yield the exact tensor trace for large systems, without the need for manual control of tensor dimensions. Our protocol allows us to obtain the exact number of solutions for computations where a naive enumeration would take astronomically long times."
CHRISTOPHER EVANS,Architects of change: professionalizing the Islamic scholar in the United Kingdom and Germany,"This dissertation examines two recent programs for post-secondary Islamic theological training in Europe that aim to produce a new class of professional Islamic scholars for emerging roles within European society. Graduates can use their training and new qualifications to secure advanced professional roles and leadership positions within the Muslim community and the broader society and state. In the process, these graduates develop and define an emergent institutional role for Islamic knowledge and authority in Europe. This study is based in seven months of fieldwork research in 2017 at two centers for higher Islamic education, including participant observation within classrooms and interviews with students, faculty and alumni. Founded in 2009, the Cambridge Muslim College in Cambridge, England is a small private school that provides professional training for about a dozen graduates of the many Islamic seminaries in the UK. Founded in 2012 with support from the German state, the Center for Islamic Theology at the University of Tübingen provides Islamic theological training to hundreds of undergraduate and graduate students each year, many of whom have received no prior formal Islamic education. In addition to the institutional differences between the schools, their graduates enter into different job markets. Where the British graduates must develop new entrepreneurial roles for Islamic leaders in the UK, the German graduates become the skilled workforce to meet existing demand for public school Islam teachers, academic theologians and professional chaplains. Comparing these two educational programs—one private, the other public—this dissertation explores how the position of each school vis-á-vis the Muslim community and the state shapes the construction of scholarly authority and the professional outcomes of the graduates. It finds that students at each school leverage their new authority to formulate creative programs of Islamic reform that justify and promote new roles for professional Islamic scholars within both the Muslim community and the larger society. Drawing upon current scholarship about Muslim identity, Islamic authority and secularism in Europe, this study considers how prevailing national discourses that marginalize Muslims in Europe shape students’ creative programs of reform and so also the future institution of Islamic knowledge in Europe."
CHRISTOPHER EVANS,The single-case reporting guideline In behavioural interventions (SCRIBE) 2016 statement,"We developed a reporting guideline to provide authors with guidance about what should be reported when writing a paper for publication in a scientific journal using a particular type of research design: the single-case experimental design. This report describes the methods used to develop the Single-Case Reporting guideline In BEhavioural interventions (SCRIBE) 2016. As a result of 2 online surveys and a 2-day meeting of experts, the SCRIBE 2016 checklist was developed, which is a set of 26 items that authors need to address when writing about single-case research. This article complements the more detailed SCRIBE 2016 Explanation and Elaboration article (Tate et al., 2016) that provides a rationale for each of the items and examples of adequate reporting from the literature. Both these resources will assist authors to prepare reports of single-case research with clarity, completeness, accuracy, and transparency. They will also provide journal reviewers and editors with a practical checklist against which such reports may be critically evaluated. We recommend that the SCRIBE 2016 is used by authors preparing manuscripts describing single-case research for publication, as well as journal reviewers and editors who are evaluating such manuscripts. SCIENTIFIC ABSTRACT: Reporting guidelines, such as the Consolidated Standards of Reporting Trials (CONSORT) Statement, improve the reporting of research in the medical literature (Turner et al., 2012). Many such guidelines exist and the CONSORT Extension to Nonpharmacological Trials (Boutron et al., 2008) provides suitable guidance for reporting between-groups intervention studies in the behavioral sciences. The CONSORT Extension for N-of-1 Trials (CENT 2015) was developed for multiple crossover trials with single individuals in the medical sciences (Shamseer et al., 2015; Vohra et al., 2015), but there is no reporting guideline in the CONSORT tradition for single-case research used in the behavioral sciences. We developed the Single-Case Reporting guideline In BEhavioural interventions (SCRIBE) 2016 to meet this need. This Statement article describes the methodology of the development of the SCRIBE 2016, along with the outcome of 2 Delphi surveys and a consensus meeting of experts. We present the resulting 26-item SCRIBE 2016 checklist. The article complements the more detailed SCRIBE 2016 Explanation and Elaboration article (Tate et al., 2016) that provides a rationale for each of the items and examples of adequate reporting from the literature. Both these resources will assist authors to prepare reports of single-case research with clarity, completeness, accuracy, and transparency. They will also provide journal reviewers and editors with a practical checklist against which such reports may be critically evaluated."
CHRISTOPHER EVANS,The Single-Case Reporting Guideline In BEhavioural Interventions (SCRIBE) 2016 statement,"We developed a reporting guideline to provide authors with guidance about what should be reported when writing a paper for publication in a scientific journal using a particular type of research design: the single-case experimental design. This report describes the methods used to develop the Single-Case Reporting guideline In BEhavioural interventions (SCRIBE) 2016. As a result of 2 online surveys and a 2-day meeting of experts, the SCRIBE 2016 checklist was developed, which is a set of 26 items that authors need to address when writing about single-case research. This article complements the more detailed SCRIBE 2016 Explanation and Elaboration article (Tate et al., 2016) that provides a rationale for each of the items and examples of adequate reporting from the literature. Both these resources will assist authors to prepare reports of single-case research with clarity, completeness, accuracy, and transparency. They will also provide journal reviewers and editors with a practical checklist against which such reports may be critically evaluated. We recommend that the SCRIBE 2016 is used by authors preparing manuscripts describing single-case research for publication, as well as journal reviewers and editors who are evaluating such manuscripts."
CHRISTOPHER EVANS,The Single-Case Reporting Guideline In BEhavioural Interventions (SCRIBE) 2016 Statement,"We developed a reporting guideline to provide authors with guidance about what should be reported when writing a paper for publication in a scientific journal using a particular type of research design: the single-case experimental design. This report describes the methods used to develop the Single-Case Reporting guideline In BEhavioural interventions (SCRIBE) 2016. As a result of 2 online surveys and a 2-day meeting of experts, the SCRIBE 2016 checklist was developed, which is a set of 26 items that authors need to address when writing about single-case research. This article complements the more detailed SCRIBE 2016 Explanation and Elaboration article (Tate et al., 2016) that provides a rationale for each of the items and examples of adequate reporting from the literature. Both these resources will assist authors to prepare reports of single-case research with clarity, completeness, accuracy, and transparency. They will also provide journal reviewers and editors with a practical checklist against which such reports may be critically evaluated. We recommend that the SCRIBE 2016 is used by authors preparing manuscripts describing single-case research for publication, as well as journal reviewers and editors who are evaluating such manuscripts."
CHRISTOPHER EVANS,The Single-Case Reporting Guideline In BEhavioural Interventions (SCRIBE) 2016 statement,"We developed a reporting guideline to provide authors with guidance about what should be reported when writing a paper for publication in a scientific journal using a particular type of research design: the single-case experimental design. This report describes the methods used to develop the Single-Case Reporting guideline In BEhavioural interventions (SCRIBE) 2016. As a result of 2 online surveys and a 2-day meeting of experts, the SCRIBE 2016 checklist was developed, which is a set of 26 items that authors need to address when writing about single-case research. This article complements the more detailed SCRIBE 2016 Explanation and Elaboration article (Tate et al., 2016) that provides a rationale for each of the items and examples of adequate reporting from the literature. Both these resources will assist authors to prepare reports of single-case research with clarity, completeness, accuracy, and transparency. They will also provide journal reviewers and editors with a practical checklist against which such reports may be critically evaluated. We recommend that the SCRIBE 2016 is used by authors preparing manuscripts describing single-case research for publication, as well as journal reviewers and editors who are evaluating such manuscripts."
CHRISTOPHER EVANS,The Single-Case Reporting guideline In BEhavioural Interventions (SCRIBE) 2016 statement,"We developed a reporting guideline to provide authors with guidance about what should be reported when writing a paper for publication in a scientific journal using a particular type of research design: the single-case experimental design. This report describes the methods used to develop the Single-Case Reporting guideline In BEhavioural interventions (SCRIBE) 2016. As a result of 2 online surveys and a 2-day meeting of experts, the SCRIBE 2016 checklist was developed, which is a set of 26 items that authors need to address when writing about single-case research. This article complements the more detailed SCRIBE 2016 Explanation and Elaboration article (Tate et al., 2016) that provides a rationale for each of the items and examples of adequate reporting from the literature. Both these resources will assist authors to prepare reports of single-case research with clarity, completeness, accuracy, and transparency. They will also provide journal reviewers and editors with a practical checklist against which such reports may be critically evaluated. We recommend that the SCRIBE 2016 is used by authors preparing manuscripts describing single-case research for publication, as well as journal reviewers and editors who are evaluating such manuscripts."
CHRISTOPHER EVANS,The single-case reporting guideline In behavioural interventions (SCRIBE) 2016 statement,"We developed a reporting guideline to provide authors with guidance about what should be reported when writing a paper for publication in a scientific journal using a particular type of research design: the single-case experimental design. This report describes the methods used to develop the Single-Case Reporting guideline In BEhavioural interventions (SCRIBE) 2016. As a result of 2 online surveys and a 2-day meeting of experts, the SCRIBE 2016 checklist was developed, which is a set of 26 items that authors need to address when writing about single-case research. This article complements the more detailed SCRIBE 2016 Explanation and Elaboration article (Tate et al., 2016) that provides a rationale for each of the items and examples of adequate reporting from the literature. Both these resources will assist authors to prepare reports of single-case research with clarity, completeness, accuracy, and transparency. They will also provide journal reviewers and editors with a practical checklist against which such reports may be critically evaluated. We recommend that the SCRIBE 2016 is used by authors preparing manuscripts describing single-case research for publication, as well as journal reviewers and editors who are evaluating such manuscripts. SCIENTIFIC ABSTRACT: Reporting guidelines, such as the Consolidated Standards of Reporting Trials (CONSORT) Statement, improve the reporting of research in the medical literature (Turner et al., 2012). Many such guidelines exist and the CONSORT Extension to Nonpharmacological Trials (Boutron et al., 2008) provides suitable guidance for reporting between-groups intervention studies in the behavioral sciences. The CONSORT Extension for N-of-1 Trials (CENT 2015) was developed for multiple crossover trials with single individuals in the medical sciences (Shamseer et al., 2015; Vohra et al., 2015), but there is no reporting guideline in the CONSORT tradition for single-case research used in the behavioral sciences. We developed the Single-Case Reporting guideline In BEhavioural interventions (SCRIBE) 2016 to meet this need. This Statement article describes the methodology of the development of the SCRIBE 2016, along with the outcome of 2 Delphi surveys and a consensus meeting of experts. We present the resulting 26-item SCRIBE 2016 checklist. The article complements the more detailed SCRIBE 2016 Explanation and Elaboration article (Tate et al., 2016) that provides a rationale for each of the items and examples of adequate reporting from the literature. Both these resources will assist authors to prepare reports of single-case research with clarity, completeness, accuracy, and transparency. They will also provide journal reviewers and editors with a practical checklist against which such reports may be critically evaluated."
CHRISTOPHER EVANS,ICAT: a novel algorithm to robustly identify cell states following perturbations in single cell transcriptomes,
CHRISTOPHER EVANS,ICAT: a novel algorithm to robustly identify cell states following perturbations in single-cell transcriptomes,"MOTIVATION: The detection of distinct cellular identities is central to the analysis of single-cell RNA sequencing (scRNA-seq) experiments. However, in perturbation experiments, current methods typically fail to correctly match cell states between conditions or erroneously remove population substructure. Here, we present the novel, unsupervised algorithm Identify Cell states Across Treatments (ICAT) that employs self-supervised feature weighting and control-guided clustering to accurately resolve cell states across heterogeneous conditions. RESULTS: Using simulated and real datasets, we show ICAT is superior in identifying and resolving cell states compared with current integration workflows. While requiring no a priori knowledge of extant cell states or discriminatory marker genes, ICAT is robust to low signal strength, high perturbation severity, and disparate cell type proportions. We empirically validate ICAT in a developmental model and find that only ICAT identifies a perturbation-unique cellular response. Taken together, our results demonstrate that ICAT offers a significant improvement in defining cellular responses to perturbation in scRNA-seq data. AVAILABILITY AND IMPLEMENTATION: https://github.com/BradhamLab/icat."
CHRISTOPHER EVANS,Prospects for beyond the standard model physics searches at the deep underground neutrino experiment: DUNE collaboration,"The Deep Underground Neutrino Experiment (DUNE) will be a powerful tool for a variety of physics topics. The high-intensity proton beams provide a large neutrino flux, sampled by a near detector system consisting of a combination of capable precision detectors, and by the massive far detector system located deep underground. This configuration sets up DUNE as a machine for discovery, as it enables opportunities not only to perform precision neutrino measurements that may uncover deviations from the present three-flavor mixing paradigm, but also to discover new particles and unveil new interactions and symmetries beyond those predicted in the Standard Model (SM). Of the many potential beyond the Standard Model (BSM) topics DUNE will probe, this paper presents a selection of studies quantifying DUNE's sensitivities to sterile neutrino mixing, heavy neutral leptons, non-standard interactions, CPT symmetry violation, Lorentz invariance violation, neutrino trident production, dark matter from both beam induced and cosmogenic sources, baryon number violation, and other new physics topics that complement those at high-energy colliders and significantly extend the present reach."
CHRISTOPHER EVANS,Identification and reconstruction of low-energy electrons in the ProtoDUNE-SP detector,
CHRISTOPHER EVANS,The long-baseline neutrino experiment: exploring fundamental symmetries of the universe,"The preponderance of matter over antimatter in the early Universe, the dynamics of the supernova bursts that produced the heavy elements necessary for life and whether protons eventually decay --- these mysteries at the forefront of particle physics and astrophysics are key to understanding the early evolution of our Universe, its current state and its eventual fate. The Long-Baseline Neutrino Experiment (LBNE) represents an extensively developed plan for a world-class experiment dedicated to addressing these questions. LBNE is conceived around three central components: (1) a new, high-intensity neutrino source generated from a megawatt-class proton accelerator at Fermi National Accelerator Laboratory, (2) a near neutrino detector just downstream of the source, and (3) a massive liquid argon time-projection chamber deployed as a far detector deep underground at the Sanford Underground Research Facility. This facility, located at the site of the former Homestake Mine in Lead, South Dakota, is approximately 1,300 km from the neutrino source at Fermilab -- a distance (baseline) that delivers optimal sensitivity to neutrino charge-parity symmetry violation and mass ordering effects. This ambitious yet cost-effective design incorporates scalability and flexibility and can accommodate a variety of upgrades and contributions. With its exceptional combination of experimental configuration, technical capabilities, and potential for transformative discoveries, LBNE promises to be a vital facility for the field of particle physics worldwide, providing physicists from around the globe with opportunities to collaborate in a twenty to thirty year program of exciting science. In this document we provide a comprehensive overview of LBNE's scientific objectives, its place in the landscape of neutrino physics worldwide, the technologies it will incorporate and the capabilities it will possess."
CHRISTOPHER EVANS,Scintillation light detection in the 6-m drift-length ProtoDUNE Dual Phase liquid argon TPC,"DUNE is a dual-site experiment for long-baseline neutrino oscillation studies, neutrino astrophysics and nucleon decay searches. ProtoDUNE Dual Phase (DP) is a 6  ×  6  ×  6 m 3 liquid argon time-projection-chamber (LArTPC) that recorded cosmic-muon data at the CERN Neutrino Platform in 2019-2020 as a prototype of the DUNE Far Detector. Charged particles propagating through the LArTPC produce ionization and scintillation light. The scintillation light signal in these detectors can provide the trigger for non-beam events. In addition, it adds precise timing capabilities and improves the calorimetry measurements. In ProtoDUNE-DP, scintillation and electroluminescence light produced by cosmic muons in the LArTPC is collected by photomultiplier tubes placed up to 7 m away from the ionizing track. In this paper, the ProtoDUNE-DP photon detection system performance is evaluated with a particular focus on the different wavelength shifters, such as PEN and TPB, and the use of Xe-doped LAr, considering its future use in giant LArTPCs. The scintillation light production and propagation processes are analyzed and a comparison of simulation to data is performed, improving understanding of the liquid argon properties."
CHRISTOPHER EVANS,A review on mechanics and mechanical properties of 2D materials—graphene and beyond,"Since the first successful synthesis of graphene just over a decade ago, a variety of two-dimensional (2D) materials (e.g., transition metal-dichalcogenides, hexagonal boron-nitride, etc.) have been discovered. Among the many unique and attractive properties of 2D materials, mechanical properties play important roles in manufacturing, integration and performance for their potential applications. Mechanics is indispensable in the study of mechanical properties, both experimentally and theoretically. The coupling between the mechanical and other physical properties (thermal, electronic, optical) is also of great interest in exploring novel applications, where mechanics has to be combined with condensed matter physics to establish a scalable theoretical framework. Moreover, mechanical interactions between 2D materials and various substrate materials are essential for integrated device applications of 2D materials, for which the mechanics of interfaces (adhesion and friction) has to be developed for the 2D materials. Here we review recent theoretical and experimental works related to mechanics and mechanical properties of 2D materials. While graphene is the most studied 2D material to date, we expect continual growth of interest in the mechanics of other 2D materials beyond graphene."
KATHLEEN CAREY,Using near-term forecasts and uncertainty partitioning to improve predictions of low-frequency cyanobacterial events,"Near-term ecological forecasts provide resource managers advance notice of changes in ecosystem services, such as fisheries stocks, timber yields, or water and air quality. Importantly, ecological forecasts can identify where uncertainty enters the forecasting system, which is necessary to refine and improve forecast skill and guide interpretation of forecast results. Uncertainty partitioning identifies the relative contributions to total forecast variance (uncertainty) introduced by different sources, including specification of the model structure, errors in driver data, and estimation of initial state conditions. Uncertainty partitioning could be particularly useful in improving forecasts of high-density cyanobacterial events, which are difficult to predict and present a persistent challenge for lake managers. Cyanobacteria can produce toxic or unsightly surface scums and advance warning of these events could help managers mitigate water quality issues. Here, we calibrate fourteen Bayesian state-space models to evaluate different hypotheses about cyanobacterial growth using data from eight summers of weekly cyanobacteria density samples in an oligotrophic (low nutrient) lake that experiences sporadic surface scums of the toxin-producing cyanobacterium, Gloeotrichia echinulata. We identify dominant sources of uncertainty for near-term (one-week to four-week) forecasts of G. echinulata densities over two years. Water temperature was an important predictor in calibration and at the four-week forecast horizon. However, no environmental covariates improved over a simple autoregressive (AR) model at the one-week horizon. Even the best fit models exhibited large variance in forecasted cyanobacterial densities and often did not capture rare peak density occurrences, indicating that significant explanatory variables in calibration are not always effective for near-term forecasting of low-frequency events. Uncertainty partitioning revealed that model process specification and initial conditions uncertainty dominated forecasts at both time horizons. These findings suggest that observed densities result from both growth and movement of G. echinulata, and that imperfect observations as well as spatial misalignment of environmental data and cyanobacteria observations affect forecast skill. Future research efforts should prioritize long-term studies to refine process understanding and increased sampling frequency and replication to better define initial conditions. Our results emphasize the importance of ecological forecasting principles and uncertainty partitioning to refine and understand predictive capacity across ecosystems."
KATHLEEN CAREY,Training macrosystems scientists requires both interpersonal and technical skills,"Macrosystems science strives to integrate patterns and processes that span regional to continental scales. The scope of such research often necessitates the involvement of large interdisciplinary and/or multi-institutional teams composed of scientists across a range of career stages, a diversity that requires researchers to hone both technical and interpersonal skills. We surveyed participants in macrosystems projects funded by the US National Science Foundation to assess the perceived importance of different skills needed in their research, as well as the types of training they received. Survey results revealed a mismatch between the skills participants perceive as important and the training they received, particularly for interpersonal and management skills. We highlight lessons learned from macrosystems training case studies, explore avenues for further improvement of undergraduate and graduate education, and discuss other training opportunities for macrosystems scientists. Given the trend toward interdisciplinary research beyond the macrosystems community, these insights are broadly applicable for scientists involved in diverse, collaborative projects."
ANN ASCHENGRAU,Participant experiences in a breastmilk biomonitoring study: A qualitative assessment.,"BACKGROUND: Biomonitoring studies can provide information about individual and population-wide exposure. However they must be designed in a way that protects the rights and welfare of participants. This descriptive qualitative study was conducted as a follow-up to a breastmilk biomonitoring study. The primary objectives were to assess participants' experiences in the study, including the report-back of individual body burden results, and to determine if participation in the study negatively affected breastfeeding rates or duration. METHODS: Participants of the Greater Boston PBDE Breastmilk Biomonitoring Study were contacted and asked about their experiences in the study: the impact of study recruitment materials on attitudes towards breastfeeding; if participants had wanted individual biomonitoring results; if the protocol by which individual results were distributed met participants' needs; and the impact of individual results on attitudes towards breastfeeding. RESULTS: No participants reported reducing the duration of breastfeeding because of the biomonitoring study, but some responses suggested that breastmilk biomonitoring studies have the potential to raise anxieties about breastfeeding. Almost all participants wished to obtain individual results. Although several reported some concern about individual body burden, none reported reducing the duration of breastfeeding because of biomonitoring results. The study literature and report-back method were found to mitigate potential negative impacts. CONCLUSION: Biomonitoring study design, including clear communication about the benefits of breastfeeding and the manner in which individual results are distributed, can prevent negative impacts of biomonitoring on breastfeeding. Adoption of more specific standards for biomonitoring studies and continued study of risk communication issues related to biomonitoring will help protect participants from harm."
ANN ASCHENGRAU,"Perchloroethylene-contaminated drinking water and the risk of breast cancer: additional results from Cape Cod, Massachusetts, USA",
ANN ASCHENGRAU,Breast Cancer Risk and Drinking Water Contaminated by Wastewater: A Case Control Study,"BACKGROUND: Drinking water contaminated by wastewater is a potential source of exposure to mammary carcinogens and endocrine disrupting compounds from commercial products and excreted natural and pharmaceutical hormones. These contaminants are hypothesized to increase breast cancer risk. Cape Cod, Massachusetts, has a history of wastewater contamination in many, but not all, of its public water supplies; and the region has a history of higher breast cancer incidence that is unexplained by the population's age, in-migration, mammography use, or established breast cancer risk factors. We conducted a case-control study to investigate whether exposure to drinking water contaminated by wastewater increases the risk of breast cancer. METHODS: Participants were 824 Cape Cod women diagnosed with breast cancer in 1988–1995 and 745 controls who lived in homes served by public drinking water supplies and never lived in a home served by a Cape Cod private well. We assessed each woman's exposure yearly since 1972 at each of her Cape Cod addresses, using nitrate nitrogen (nitrate-N) levels measured in public wells and pumping volumes for the wells. Nitrate-N is an established wastewater indicator in the region. As an alternative drinking water quality indicator, we calculated the fraction of recharge zones in residential, commercial, and pesticide land use areas. RESULTS: After controlling for established breast cancer risk factors, mammography, and length of residence on Cape Cod, results showed no consistent association between breast cancer and average annual nitrate-N (OR = 1.8; 95% CI 0.6 – 5.0 for ≥ 1.2 vs. < .3 mg/L), the sum of annual nitrate-N concentrations (OR = 0.9; 95% CI 0.6 – 1.5 for ≥ 10 vs. 1 to < 10 mg/L), or the number of years exposed to nitrate-N over 1 mg/L (OR = 0.9; 95% CI 0.5 – 1.5 for ≥ 8 vs. 0 years). Variation in exposure levels was limited, with 99% of women receiving some of their water from supplies with nitrate-N levels in excess of background. The total fraction of residential, commercial, and pesticide use land in recharge zones of public supply wells was associated with a small statistically unstable higher breast cancer incidence (OR = 1.4; 95% CI 0.8–2.4 for highest compared with lowest land use), but risk did not increase for increasing land use fractions. CONCLUSION: Results did not provide evidence of an association between breast cancer and drinking water contaminated by wastewater. The computer mapping methods used in this study to link routine measurements required by the Safe Drinking Water Act with interview data can enhance individual-level epidemiologic studies of multiple health outcomes, including diseases with substantial latency."
ANN ASCHENGRAU,"Spatial Analysis of Lung, Colorectal, and Breast Cancer on Cape Cod: An Application of Generalized Additive Models to Case-Control Data","BACKGROUND: The availability of geographic information from cancer and birth defect registries has increased public demands for investigation of perceived disease clusters. Many neighborhood-level cluster investigations are methodologically problematic, while maps made from registry data often ignore latency and many known risk factors. Population-based case-control and cohort studies provide a stronger foundation for spatial epidemiology because potential confounders and disease latency can be addressed. METHODS: We investigated the association between residence and colorectal, lung, and breast cancer on upper Cape Cod, Massachusetts (USA) using extensive data on covariates and residential history from two case-control studies for 1983–1993. We generated maps using generalized additive models, smoothing on longitude and latitude while adjusting for covariates. The resulting continuous surface estimates disease rates relative to the whole study area. We used permutation tests to examine the overall importance of location in the model and identify areas of increased and decreased risk. RESULTS: Maps of colorectal cancer were relatively flat. Assuming 15 years of latency, lung cancer was significantly elevated just northeast of the Massachusetts Military Reservation, although the result did not hold when we restricted to residences of longest duration. Earlier non-spatial epidemiology had found a weak association between lung cancer and proximity to gun and mortar positions on the reservation. Breast cancer hot spots tended to increase in magnitude as we increased latency and adjusted for covariates, indicating that confounders were partly hiding these areas. Significant breast cancer hot spots were located near known groundwater plumes and the Massachusetts Military Reservation. DISCUSSION: Spatial epidemiology of population-based case-control studies addresses many methodological criticisms of cluster studies and generates new exposure hypotheses. Our results provide evidence for spatial clustering of breast cancer on upper Cape Cod. The analysis suggests further investigation of the potential association between breast cancer and pollution plumes based on detailed exposure modeling."
ANN ASCHENGRAU,Cluster Detection Methods Applied to the Upper Cape Cod Cancer Data,"BACKGROUND: A variety of statistical methods have been suggested to assess the degree and/or the location of spatial clustering of disease cases. However, there is relatively little in the literature devoted to comparison and critique of different methods. Most of the available comparative studies rely on simulated data rather than real data sets. METHODS: We have chosen three methods currently used for examining spatial disease patterns: the M-statistic of Bonetti and Pagano; the Generalized Additive Model (GAM) method as applied by Webster; and Kulldorff's spatial scan statistic. We apply these statistics to analyze breast cancer data from the Upper Cape Cancer Incidence Study using three different latency assumptions. RESULTS: The three different latency assumptions produced three different spatial patterns of cases and controls. For 20 year latency, all three methods generally concur. However, for 15 year latency and no latency assumptions, the methods produce different results when testing for global clustering. CONCLUSION: The comparative analyses of real data sets by different statistical methods provides insight into directions for further research. We suggest a research program designed around examining real data sets to guide focused investigation of relevant features using simulated data, for the purpose of understanding how to interpret statistical methods applied to epidemiological data with a spatial component."
ANN ASCHENGRAU,"Spatial Analysis of Bladder, Kidney, and Pancreatic Cancer on Upper Cape Cod: An Application of Generalized Additive Models to Case-Control Data","BACKGROUND: In 1988, elevated cancer incidence in upper Cape Cod, Massachusetts prompted a large epidemiological study of nine cancers to investigate possible environmental risk factors. Positive associations were observed, but explained only a portion of the excess cancer incidence. This case-control study provided detailed information on individual-level covariates and residential history that can be spatially analyzed using generalized additive models (GAMs) and geographical information systems (GIS). METHODS: We investigated the association between residence and bladder, kidney, and pancreatic cancer on upper Cape Cod. We estimated adjusted odds ratios using GAMs, smoothing on location. A 40-year residential history allowed for latency restrictions. We mapped spatially continuous odds ratios using GIS and identified statistically significant clusters using permutation tests. RESULTS: Maps of bladder cancer are essentially flat ignoring latency, but show a statistically significant hot spot near known Massachusetts Military Reservation (MMR) groundwater plumes when 15 years latency is assumed. The kidney cancer map shows significantly increased ORs in the south of the study area and decreased ORs in the north. CONCLUSION: Spatial epidemiology using individual level data from population-based studies addresses many methodological criticisms of cluster studies and generates new exposure hypotheses. Our results provide evidence for spatial clustering of bladder cancer near MMR plumes that suggest further investigation using detailed exposure modeling."
ANN ASCHENGRAU,Prenatal Exposure to Tetrachloroethylene-Contaminated Drinking Water and the Risk of Congenital Anomalies: A Retrospective Cohort Study,"BACKGROUND: Prior animal and human studies of prenatal exposure to solvents including tetrachloroethylene (PCE) have shown increases in the risk of certain congenital anomalies among exposed offspring. OBJECTIVES: This retrospective cohort study examined whether PCE contamination of public drinking water supplies in Massachusetts influenced the occurrence of congenital anomalies among children whose mothers were exposed around the time of conception. METHODS: The study included 1,658 children whose mothers were exposed to PCE-contaminated drinking water and a comparable group of 2,999 children of unexposed mothers. Mothers completed a self-administered questionnaire to gather information on all of their prior births, including the presence of anomalies, residential histories and confounding variables. PCE exposure was estimated using EPANET water distribution system modeling software that incorporated a fate and transport model. RESULTS: Children whose mothers had high exposure levels around the time of conception had an increased risk of congenital anomalies. The adjusted odds ratio of all anomalies combined among children with prenatal exposure in the uppermost quartile was 1.5 (95% CI: 0.9, 2.5). No meaningful increases in the risk were seen for lower exposure levels. Increases were also observed in the risk of neural tube defects (OR: 3.5, 95% CI: 0.8, 14.0) and oral clefts (OR 3.2, 95% CI: 0.7, 15.0) among offspring with any prenatal exposure. CONCLUSION: The results of this study suggest that the risk of certain congenital anomalies is increased among the offspring of women who were exposed to PCE-contaminated drinking water around the time of conception. Because these results are limited by the small number of children with congenital anomalies that were based on maternal reports, a follow-up investigation should be conducted with a larger number of affected children who are identified by independent records."
ANN ASCHENGRAU,Prenatal Exposure to Tetrachloroethylene-Contaminated Drinking Water and the Risk of Adverse Birth Outcomes,"BACKGROUND. Prior studies of prenatal exposure to tetrachloroethylene (PCE) have shown mixed results regarding its effect on birth weight and gestational age. OBJECTIVES. In this retrospective cohort study we examined whether PCE contamination of public drinking-water supplies in Massachusetts influenced the birth weight and gestational duration of children whose mothers were exposed before the child's delivery. METHODS. The study included 1,353 children whose mothers were exposed to PCE-contaminated drinking water and a comparable group of 772 children of unexposed mothers. Birth records were used to identify subjects and provide information on the outcomes. Mothers completed a questionnaire to gather information on residential histories and confounding variables. PCE exposure was estimated using EPANET water distribution system modeling software that incorporated a fate and transport model. RESULTS. We found no meaningful associations between PCE exposure and birth weight or gestational duration. Compared with children whose mothers were unexposed during the year of the last menstrual period (LMP), adjusted mean differences in birth weight were 20.9, 6.2, 30.1, and 15.2 g for children whose mothers' average monthly exposure during the LMP year ranged from the lowest to highest quartile. Similarly, compared with unexposed children, adjusted mean differences in gestational age were -0.2, 0.1, -0.1, and -0.2 weeks for children whose mothers' average monthly exposure ranged from the lowest to highest quartile. Similar results were observed for two other measures of prenatal exposure. CONCLUSIONS. These results suggest that prenatal PCE exposure does not have an adverse effect on these birth outcomes at the exposure levels experienced by this population."
ANN ASCHENGRAU,Using Residential History and Groundwater Modeling to Examine Drinking Water Exposure and Breast Cancer,"BACKGROUND. Spatial analyses of case-control data have suggested a possible link between breast cancer and groundwater plumes in upper Cape Cod, Massachusetts. OBJECTIVE. We integrated residential histories, public water distribution systems, and groundwater modeling within geographic information systems (GIS) to examine the association between exposure to drinking water that has been contaminated by wastewater effluent and breast cancer. METHODS. Exposure was assessed from 1947 to 1993 for 638 breast cancer cases who were diagnosed from 1983 to 1993 and 842 controls; we took into account residential mobility and drinking water source. To estimate the historical impact of effluent on drinking water wells, we modified a modular three-dimensional finite-difference groundwater model (MODFLOW) from the U.S. Geological Survey. The analyses included latency and exposure duration. RESULTS. Wastewater effluent impacted the drinking water wells of study participants as early as 1966. For > 0-5 years of exposure (versus no exposure), associations were generally null. Adjusted odds ratios (AORs) for > 10 years of exposure were slightly increased, assuming latency periods of 0 or 10 years [AOR = 1.3; 95% confidence interval (CI), 0.9-1.9 and AOR = 1.6; 95% CI, 0.8-3.2, respectively]. Statistically significant associations were estimated for ever-exposed versus never-exposed women when a 20-year latency period was assumed (AOR = 1.9; 95% CI, 1.0-3.4). A sensitivity analysis that classified exposures assuming lower well-pumping rates showed similar results. CONCLUSION. We investigated the hypothesis generated by earlier spatial analyses that exposure to drinking water contaminated by wastewater effluent may be associated with breast cancer. Using a detailed exposure assessment, we found an association with breast cancer that increased with longer latency and greater exposure duration."
ANN ASCHENGRAU,"Perchloroethylene-Contaminated Drinking Water and the Risk of Breast Cancer: Additional Results from Cape Cod, Massachusetts, USA","In 1998 we published the results of a study suggesting an association between breast cancer and perchloroethylene (PCE; also called tetrachloroethylene) exposure from public drinking water. The present case-control study was undertaken to evaluate this association further. The cases were composed of female residents of eight towns in the Cape Cod region of Massachusetts who had been diagnosed with breast cancer from 1987 through 1993 (n = 672). Controls were composed of demographically similar women from the same towns (n = 616). Women were exposed to PCE when it leached from the vinyl lining of water distribution pipes from the late 1960s through the early 1980s. A relative delivered dose of PCE that entered a home was estimated using an algorithm that took into account residential history, water flow, and pipe characteristics. Small to moderate elevations in risk were seen among women whose exposure levels were above the 75th and 90th percentiles when 0-15 years of latency were considered (adjusted odds ratios, 1.5-1.9 for > 75th percentile, 1.3-2.8 for > 90th percentile). When data from the present and prior studies were combined, small to moderate increases in risk were also seen among women whose exposure levels were above the 75th and 90th percentiles when 0-15 years of latency were considered (adjusted odds ratios, 1.6-1.9 for > 75th percentile, 1.3-1.9 for > 90th percentile). The results of the present study confirm those of the previous one and suggest that women with the highest PCE exposure levels have a small to moderate increased risk of breast cancer."
ANN ASCHENGRAU,Exploring Associations Between Residential Location and Breast Cancer Incidence in a Case-Control Study,"Locating geographic hot spots of cancer may lead to new causal hypotheses and ultimately to new knowledge of cancer-causing factors. The Cape Cod region of Massachusetts has experienced elevated incidence of breast cancer compared with statewide averages. The origins of the excess remain largely unexplained, even after the Upper Cape Cod Cancer Incidence Study investigated numerous potential environmental exposures. Using case-control data from this study (258 cases and 686 controls), we developed an exploratory approach for measuring associations between residential location and breast cancer incidence, adjusting for individual-level risk factors. We measured crude and adjusted odds ratios over the study region using fixed-scale grids and a smoothing algorithm of overlapping circular units. Polycircular hot spot regions, derived from the peak values of the smoothed odds ratios, delineated geographic areas wherein residence was associated with 60% [odds ratio (OR), 1.6; 95% confidence interval (CI), 0.8-3.2] to 210% (OR, 3.1; 95% CI, 1.3-7.2) increased incidence relative to the remainder of the study population. The findings suggest several directions for further research, including the identification of potential environmental exposures that may be assessed in forthcoming case-control studies."
ANN ASCHENGRAU,"Community- and Individual-Level Socioeconomic Status and Breast Cancer Risk: Multilevel Modeling on Cape Cod, Massachusetts","BACKGROUND. Previous research demonstrated increased risk of breast cancer associated with higher socioeconomic status (SES) measured at both the individual and community levels. However, little attention has been paid to simultaneously examining both measures. OBJECTIVES. We evaluated the independent influences of individual and community SES on the risk of breast cancer using case-control data. Because our previous work suggests that associations may be stronger after including a latency period, we also assessed the effect of community-level SES assuming a 10-year latency period. METHODS. We obtained individual education for cases and matched controls diagnosed between 1987 and 1993 on Cape Cod, Massachusetts (USA). We acquired community-level SES from census data for 1980 and 1990. Using SES data at diagnosis and 10 years earlier, we constructed models for breast cancer risk using individual-level SES only, community-level SES only, and a multilevel analysis including both. We adjusted models for other individual-level risk factors. RESULTS. Women with the highest education were at greater risk of developing breast cancer in both 1980 and 1990 [odds ratio (OR) = 1.17 and 1.19, respectively]. Similarly, women living in the highest-SES communities in 1990 had greater risk (OR = 1.30). Results were stronger in the analyses considering a latency period (OR = 1.69). Adjusting for intragroup correlation had little effect on the analyses. CONCLUSIONS. Models including individual- or community-level measures of SES produced associations similar to those observed in previous research. Results for models including both measures are consistent with a contextual effect of SES on risk of breast cancer independent of individual SES."
ANN ASCHENGRAU,Residential History and Groundwater Modeling: Gallagher et al. Respond,
ANN ASCHENGRAU,Tetrachloroethylene-contaminated drinking water and the risk of breast cancer,
ANN ASCHENGRAU,Exploring associations between residential location and breast cancer incidence in a case-control study,
ANN ASCHENGRAU,Evaluation of the Webler-Brown Model for Estimating Tetrachloroethylene Exposure from Vinyl-Lined Asbestos-Cement Pipes,"BACKGROUND: From May 1968 through March 1980, vinyl-lined asbestos-cement (VL/AC) water distribution pipes were installed in New England to avoid taste and odor problems associated with asbestos-cement pipes. The vinyl resin was applied to the inner pipe surface in a solution of tetrachloroethylene (perchloroethylene, PCE). Substantial amounts of PCE remained in the liner and subsequently leached into public drinking water supplies. METHODS: Once aware of the leaching problem and prior to remediation (April-November 1980), Massachusetts regulators collected drinking water samples from VL/AC pipes to determine the extent and severity of the PCE contamination. This study compares newly obtained historical records of PCE concentrations in water samples (n = 88) with concentrations estimated using an exposure model employed in epidemiologic studies on the cancer risk associated with PCE-contaminated drinking water. The exposure model was developed by Webler and Brown to estimate the mass of PCE delivered to subjects' residences. RESULTS: The mean and median measured PCE concentrations in the water samples were 66 and 0.5 μg/L, respectively, and the range extended from non-detectable to 2432 μg/L. The model-generated concentration estimates and water sample concentrations were moderately correlated (Spearman rank correlation coefficient = 0.48, p < 0.0001). Correlations were higher in samples taken at taps and spigots vs. hydrants (ρ = 0.84 vs. 0.34), in areas with simple vs. complex geometry (ρ = 0.51 vs. 0.38), and near pipes installed in 1973–1976 vs. other years (ρ = 0.56 vs. 0.42 for 1968–1972 and 0.37 for 1977–1980). Overall, 24% of the variance in measured PCE concentrations was explained by the model-generated concentration estimates (p < 0.0001). Almost half of the water samples had undetectable concentrations of PCE. Undetectable levels were more common in areas with the earliest installed VL/AC pipes, at the beginning and middle of VL/AC pipes, at hydrants, and in complex pipe configurations. CONCLUSION: PCE concentration estimates generated using the Webler-Brown model were moderately correlated with measured water concentrations. The present analysis suggests that the exposure assessment process used in prior epidemiological studies could be improved with more accurate characterization of water flow. This study illustrates one method of validating an exposure model in an epidemiological study when historical measurements are not available."
ANN ASCHENGRAU,Associations between Maternal Thyroid Function in Pregnancy and Obstetric and Perinatal Outcomes - Supplemental Table 1,
ANN ASCHENGRAU,Renal Hyperfiltration and the Development of Microalbuminuria in Type 1 Diabetes,"OBJECTIVE: The purpose of this study was to examine prospectively whether renal hyperfiltration is associated with the development of microalbuminuria in patients with type 1 diabetes, after taking into account known risk factors. RESEARCH DESIGN AND METHODS: The study group comprised 426 participants with normoalbuminuria from the First Joslin Kidney Study, followed for 15 years. Glomerular filtration rate was estimated by serum cystatin C, and hyperfiltration was defined as exceeding the 97.5th percentile of the sex-specific distribution of a similarly aged, nondiabetic population (134 and 149 ml/min per 1.73 m2 for men and women, respectively). The outcome was time to microalbuminuria development (multiple albumin excretion rate >30 μg/min). Hazard ratios (HRs) for microalbuminuria were calculated at 5, 10, and 15 years. RESULTS: Renal hyperfiltration was present in 24% of the study group and did not increase the risk of developing microalbuminuria. The unadjusted HR for microalbuminuria comparing those with and without hyperfiltration at baseline was 0.8 (95% CI 0.4–1.7) during the first 5 years, 1.0 (0.6–1.7) during the first 10 years, and 0.8 (0.5–1.4) during 15 years of follow-up. The model adjusted for baseline known risk factors including A1C, age at diagnosis of diabetes, diabetes duration, and cigarette smoking resulted in similar HRs. In addition, incorporating changes in hyperfiltration status during follow-up had minimal impact on the HRs for microalbuminuria. CONCLUSION;S Renal hyperfiltration does not have an impact on the development of microalbuminuria in type 1 diabetes during 5, 10, or 15 years of follow-up."
ANN ASCHENGRAU,Participant experiences in a breastmilk biomonitoring study: a qualitative assessment,"BACKGROUND: Biomonitoring studies can provide information about individual and population-wide exposure. However they must be designed in a way that protects the rights and welfare of participants. This descriptive qualitative study was conducted as a follow-up to a breastmilk biomonitoring study. The primary objectives were to assess participants' experiences in the study, including the report-back of individual body burden results, and to determine if participation in the study negatively affected breastfeeding rates or duration. METHODS: Participants of the Greater Boston PBDE Breastmilk Biomonitoring Study were contacted and asked about their experiences in the study: the impact of study recruitment materials on attitudes towards breastfeeding; if participants had wanted individual biomonitoring results; if the protocol by which individual results were distributed met participants' needs; and the impact of individual results on attitudes towards breastfeeding. RESULTS: No participants reported reducing the duration of breastfeeding because of the biomonitoring study, but some responses suggested that breastmilk biomonitoring studies have the potential to raise anxieties about breastfeeding. Almost all participants wished to obtain individual results. Although several reported some concern about individual body burden, none reported reducing the duration of breastfeeding because of biomonitoring results. The study literature and report-back method were found to mitigate potential negative impacts. CONCLUSION: Biomonitoring study design, including clear communication about the benefits of breastfeeding and the manner in which individual results are distributed, can prevent negative impacts of biomonitoring on breastfeeding. Adoption of more specific standards for biomonitoring studies and continued study of risk communication issues related to biomonitoring will help protect participants from harm."
ANN ASCHENGRAU,Impact of Tetrachloroethylene-Contaminated Drinking Water on the Risk of Breast Cancer: Using a Dose Model to Assess Exposure in a Case-Control Study,"BACKGROUND: A population-based case-control study was undertaken in 1997 to investigate the association between tetrachloroethylene (PCE) exposure from public drinking water and breast cancer among permanent residents of the Cape Cod region of Massachusetts. PCE, a volatile organic chemical, leached from the vinyl lining of certain water distribution pipes into drinking water from the late 1960s through the early 1980s. The measure of exposure in the original study, referred to as the relative delivered dose (RDD), was based on an amount of PCE in the tap water entering the home and estimated with a mathematical model that involved only characteristics of the distribution system. METHODS: In the current analysis, we constructed a personal delivered dose (PDD) model that included personal information on tap water consumption and bathing habits so that inhalation, ingestion, and dermal absorption were also considered. We reanalyzed the association between PCE and breast cancer and compared the results to the original RDD analysis of subjects with complete data. RESULTS: The PDD model produced higher adjusted odds ratios than the RDD model for exposures > 50th and >75th percentile when shorter latency periods were considered, and for exposures < 50th and >90th percentile when longer latency periods were considered. Overall, however, the results from the PDD analysis did not differ greatly from the RDD analysis. CONCLUSION: The inputs that most heavily influenced the PDD model were initial water concentration and duration of exposure. These variables were also included in the RDD model. In this study population, personal factors like bath and shower temperature, bathing frequencies and durations, and water consumption did not differ greatly among subjects, so including this information in the model did not significantly change subjects' exposure classification."
ANN ASCHENGRAU,"Self-Reported Chemicals Exposure, Beliefs About Disease Causation, and Risk of Breast Cancer in the Cape Cod Breast Cancer and Environment Study: A Case-Control Study","BACKGROUND: Household cleaning and pesticide products may contribute to breast cancer because many contain endocrine disrupting chemicals or mammary gland carcinogens. This population-based case-control study investigated whether use of household cleaners and pesticides increases breast cancer risk. METHODS: Participants were 787 Cape Cod, Massachusetts, women diagnosed with breast cancer between 1988 and 1995 and 721 controls. Telephone interviews asked about product use, beliefs about breast cancer etiology, and established and suspected breast cancer risk factors. To evaluate potential recall bias, we stratified product-use odds ratios by beliefs about whether chemicals and pollutants contribute to breast cancer; we compared these results with odds ratios for family history (which are less subject to recall bias) stratified by beliefs about heredity. RESULTS: Breast cancer risk increased two-fold in the highest compared with lowest quartile of self-reported combined cleaning product use (Adjusted OR = 2.1, 95% CI: 1.4, 3.3) and combined air freshener use (Adjusted OR = 1.9, 95% CI: 1.2, 3.0). Little association was observed with pesticide use. In stratified analyses, cleaning products odds ratios were more elevated among participants who believed pollutants contribute ""a lot"" to breast cancer and moved towards the null among the other participants. In comparison, the odds ratio for breast cancer and family history was markedly higher among women who believed that heredity contributes ""a lot"" (OR = 2.6, 95% CI: 1.9, 3.6) and not elevated among others (OR = 0.7, 95% CI: 0.5, 1.1). CONCLUSIONS: Results of this study suggest that cleaning product use contributes to increased breast cancer risk. However, results also highlight the difficulty of distinguishing in retrospective self-report studies between valid associations and the influence of recall bias. Recall bias may influence higher odds ratios for product use among participants who believed that chemicals and pollutants contribute to breast cancer. Alternatively, the influence of experience on beliefs is another explanation, illustrated by the protective odds ratio for family history among women who do not believe heredity contributes ""a lot."" Because exposure to chemicals from household cleaning products is a biologically plausible cause of breast cancer and avoidable, associations reported here should be further examined prospectively."
ANN ASCHENGRAU,A Case-Only Analysis of the Interaction between N-Acetyltransferase 2 Haplotypes and Tobacco Smoke on Breast Cancer Etiology,"INTRODUCTION. N-acetyltransferase 2 is a polymorphic enzyme in humans. Women who possess homozygous polymorphic alleles have a slower rate of metabolic activation of aryl aromatic amines, one of the constituents of tobacco smoke that has been identified as carcinogenic. We hypothesized that women with breast cancer who were slow acetylators would be at increased risk of breast cancer associated with active and passive exposure to tobacco smoke. METHODS. We used a case-only study design to evaluate departure from multiplicativity between acetylation status and smoking status. We extracted DNA from buccal cell samples collected from 502 women with incident primary breast cancer and assigned acetylation status by genotyping ten single-nucleotide polymorphisms. Information on tobacco use and breast cancer risk factors was obtained by structured interviews. RESULTS. We observed no substantial departure from multiplicativity between acetylation status and history of ever having been an active smoking (adjusted odds ratio estimate of departure from multiplicativity = 0.9, 95% confidence interval 0.5 to 1.7) or ever having had passive residential exposure to tobacco smoke (adjusted odds ratio = 0.7, 95% confidence interval 0.4 to 1.5). The estimates for departure from multiplicativity between acetylation status and various measures of intensity, duration, and timing of active and passive tobacco exposure lacked consistency and were generally not supportive of the idea of a gene–environment interaction. CONCLUSION. In this, the largest case-only study to evaluate the interaction between acetylation status and active or passive exposure to tobacco smoke, we found little evidence to support the idea of a departure from multiplicativity."
ANN ASCHENGRAU,Departure from multiplicative interaction for catechol-O-methyltransferase genotype and active/passive exposure to tobacco smoke among women with breast cancer,"BACKGROUND Women with homozygous polymorphic alleles of catechol-O-methyltransferase (COMT-LL) metabolize 2-hydroxylated estradiol, a suspected anticarcinogenic metabolite of estrogen, at a four-fold lower rate than women with no polymorphic alleles (COMT-HH) or heterozygous women (COMT-HL). We hypothesized that COMT-LL women exposed actively or passively to tobacco smoke would have higher exposure to 2-hydroxylated estradiol than never-active/never passive exposed women, and should therefore have a lower risk of breast cancer than women exposed to tobacco smoke or with higher COMT activity. METHODS We used a case-only design to evaluate departure from multiplicative interaction between COMT genotype and smoking status. We identified 502 cases of invasive incident breast cancer and characterized COMT genotype. Information on tobacco use and other potential breast cancer risk factors were obtained by structured interviews. RESULTS We observed moderate departure from multiplicative interaction for COMT-HL genotype and history of ever-active smoking (adjusted odds ratio [aOR] = 1.6, 95% confidence interval [CI]: 0.7, 3.8) and more pronounced departure for women who smoked 40 or more years (aOR = 2.3, 95% CI: 0.8, 7.0). We observed considerable departure from multiplicative interaction for COMT-HL genotype and history of ever-passive smoking (aOR = 2.0, 95% CI: 0.8, 5.2) or for having lived with a smoker after age 20 (aOR = 2.8, 95% CI: 0.8, 10). CONCLUSION With greater control over potential misclassification errors and a large case-only population, we found evidence to support an interaction between COMT genotype and tobacco smoke exposure in breast cancer etiology."
ANN ASCHENGRAU,A multilevel non-hierarchical study of birth weight and socioeconomic status,"BACKGROUND. It is unclear whether the socioeconomic status (SES) of the community of residence has a substantial association with infant birth weight. We used multilevel models to examine associations of birth weight with family- and community-level SES in the Cape Cod Family Health Study. Data were collected retrospectively on births to women between 1969 and 1983 living on Cape Cod, Massachusetts. The sample included siblings born in different residences with differing community-level SES. METHODS. We used cross-classified models to account for multiple levels of correlation in a non-hierarchical data structure. We accounted for clustering at family- and community-levels. Models included extensive individual- and family-level covariates. SES variables of interest were maternal education; paternal occupation; percent adults living in poverty; percent adults with a four year college degree; community mean family income; and percent adult unemployment. RESULTS. Residual correlation was detected at the family- but not the community-level. Substantial effects sizes were observed for family-level SES while smaller magnitudes were observed for community-level SES. Overall, higher SES corresponded to increased birth weight though neither family- nor community-level variables had significant associations with the outcome. In a model applied to a reduced sample that included a single child per family, enforcing a hierarchical data structure, paternal occupation was found to have a significant association with birth weight (p = 0.033). Larger effect sizes for community SES appeared in models applied to the full sample that contained limited covariates, such as those typically found on birth certificates. CONCLUSIONS. Cross-classified models allowed us to include more than one child per family even when families moved between births. There was evidence of mild associations between family SES and birth weight. Stronger associations between paternal occupation and birth weight were observed in models applied to reduced samples with hierarchical data structures, illustrating consequences of excluding observations from the cross-classified analysis. Models with limited covariates showed associations of birth weight with community SES. In models adjusting for a complete set of individual- and family-level covariates, community SES was not as important."
ANN ASCHENGRAU,Method for Mapping Population-Based Case-Control Studies: An Application Using Generalized Additive Models,"BACKGROUND. Mapping spatial distributions of disease occurrence and risk can serve as a useful tool for identifying exposures of public health concern. Disease registry data are often mapped by town or county of diagnosis and contain limited data on covariates. These maps often possess poor spatial resolution, the potential for spatial confounding, and the inability to consider latency. Population-based case-control studies can provide detailed information on residential history and covariates. RESULTS. Generalized additive models (GAMs) provide a useful framework for mapping point-based epidemiologic data. Smoothing on location while controlling for covariates produces adjusted maps. We generate maps of odds ratios using the entire study area as a reference. We smooth using a locally weighted regression smoother (loess), a method that combines the advantages of nearest neighbor and kernel methods. We choose an optimal degree of smoothing by minimizing Akaike's Information Criterion. We use a deviance-based test to assess the overall importance of location in the model and pointwise permutation tests to locate regions of significantly increased or decreased risk. The method is illustrated with synthetic data and data from a population-based case-control study, using S-Plus and ArcView software. CONCLUSION. Our goal is to develop practical methods for mapping population-based case-control and cohort studies. The method described here performs well for our synthetic data, reproducing important features of the data and adequately controlling the covariate. When applied to the population-based case-control data set, the method suggests spatial confounding and identifies statistically significant areas of increased and decreased odds ratios."
ANN ASCHENGRAU,"Spatial analysis of learning and developmental disorders in upper Cape Cod, Massachusetts using generalized additive models","The spatial variability of three indicators of learning and developmental disability (LDD) was assessed for Cape Cod, Massachusetts. Maternal reports of receiving special education services, attention deficit hyperactivity disorder, and educational attainment were available for a birth cohort from 1969-1983. Using generalized additive models and residential history, maps of the odds of LDD were produced that also controlled for known risk factors. While results were not statistically significant, they suggest that children living in certain parts of Cape Cod were more likely to have a LDD. The spatial variation may be due to variation in the physical and social environment."
ANN ASCHENGRAU,"Spatial-temporal analysis of breast cancer in upper Cape Cod, Massachusetts","INTRODUCTION. The reasons for elevated breast cancer rates in the upper Cape Cod area of Massachusetts remain unknown despite several epidemiological studies that investigated possible environmental risk factors. Data from two of these population-based case-control studies provide geocoded residential histories and information on confounders, creating an invaluable dataset for spatial-temporal analysis of participants' residency over five decades. METHODS. The combination of statistical modeling and mapping is a powerful tool for visualizing disease risk in a spatial-temporal analysis. Advances in geographic information systems (GIS) enable spatial analytic techniques in public health studies previously not feasible. Generalized additive models (GAMs) are an effective approach for modeling spatial and temporal distributions of data, combining a number of desirable features including smoothing of geographical location, residency duration, or calendar years; the ability to estimate odds ratios (ORs) while adjusting for confounders; selection of optimum degree of smoothing (span size); hypothesis testing; and use of standard software. We conducted a spatial-temporal analysis of breast cancer case-control data using GAMs and GIS to determine the association between participants' residential history during 1947–1993 and the risk of breast cancer diagnosis during 1983–1993. We considered geographic location alone in a two-dimensional space-only analysis. Calendar year, represented by the earliest year a participant lived in the study area, and residency duration in the study area were modeled individually in one-dimensional time-only analyses, and together in a two-dimensional time-only analysis. We also analyzed space and time together by applying a two-dimensional GAM for location to datasets of overlapping calendar years. The resulting series of maps created a movie which allowed us to visualize changes in magnitude, geographic size, and location of elevated breast cancer risk for the 40 years of residential history that was smoothed over space and time. RESULTS. The space-only analysis showed statistically significant increased areas of breast cancer risk in the northern part of upper Cape Cod and decreased areas of breast cancer risk in the southern part (p-value = 0.04; ORs: 0.90–1.40). There was also a significant association between breast cancer risk and calendar year (p-value = 0.05; ORs: 0.53–1.38), with earlier calendar years resulting in higher risk. The results of the one-dimensional analysis of residency duration and the two-dimensional analysis of calendar year and duration showed that the risk of breast cancer increased with increasing residency duration, but results were not statistically significant. When we considered space and time together, the maps showed a large area of statistically significant elevated risk for breast cancer near the Massachusetts Military Reservation (p-value range:0.02–0.05; ORs range: 0.25–2.5). This increased risk began with residences in the late 1940s and remained consistent in size and location through the late 1950s. CONCLUSION. Spatial-temporal analysis of the breast cancer data may help identify new exposure hypotheses that warrant future epidemiologic investigations with detailed exposure models. Our methods allow us to visualize breast cancer risk, adjust for known confounders including age at diagnosis or index year, family history of breast cancer, parity and age at first live- or stillbirth, and test for the statistical significance of location and time. Despite the advantages of GAMs, analyses are for exploratory purposes and there are still methodological issues that warrant further research. This paper illustrates that GAM methods are a suitable alternative to widely-used cluster detection methods and may be preferable when residential histories from existing epidemiological studies are available."
ANN ASCHENGRAU,"Tetrachloroethylene-Contaminated Drinking Water in Massachusetts and the Risk of Colon-rectum, Lung, and Other Cancers","We conducted a population-based case-control study to evaluate the relationship between cancer of the colon-rectum (n = 326), lung (n = 252), brain (n = 37), and pancreas (n = 37), and exposure to tetrachloroethylene (PCE) from public drinking water. Subjects were exposed to PCE when it leached from the vinyl lining of drinking-water distribution pipes. Relative delivered dose of PCE was estimated using a model that took into account residential location, years of residence, water flow, and pipe characteristics. Adjusted odds ratios (ORs) for lung cancer were moderately elevated among subjects whose exposure level was above the 90th percentile whether or not a latent period was assumed [ORs and 95% confidence intervals (CIs), 3.7 (1.0-11.7), 3.3 (0.6-13.4), 6.2 (1.1-31.6), and 19.3 (2.5-141.7) for 0, 5, 7, and 9 years of latency, respectively]. The adjusted ORs for colon-rectum cancer were modestly elevated among ever-exposed subjects as more years of latency were assumed [OR and CI, 1.7 (0.8-3.8) and 2.0 (0.6-5.8) for 11 and 13 years of latency, respectively]. These elevated ORs stemmed mainly from associations with rectal cancer. Adjusted ORs for rectal cancer among ever-exposed subjects were more elevated [OR and CI, 2.6 (0. 8-6.7) and 3.1 (0.7-10.9) for 11 and 13 years of latency, respectively] than were corresponding estimates for colon cancer [OR and CI, 1.3 (0.5-3.5) and 1.5 (0.3-5.8) for 11 and 13 years of latency, respectively]. These results provide evidence for an association between PCE-contaminated public drinking water and cancer of the lung and, possibly, cancer of the colon-rectum."
ANN ASCHENGRAU,"Tetrachloroethylene-contaminated drinking water in Massachusetts and the risk of colon-rectum, lung, and other cancers",
FREDERICK L RUBERG,Persistent Left Superior Vena Cava: A Case Report and Review of Literature,"Persistent left superior vena cava is rare but important congenital vascular anomaly. It results when the left superior cardinal vein caudal to the innominate vein fails to regress. It is most commonly observed in isolation but can be associated with other cardiovascular abnormalities including atrial septal defect, bicuspid aortic valve, coarctation of aorta, coronary sinus ostial atresia, and cor triatriatum. The presence of PLSVC can render access to the right side of heart challenging via the left subclavian approach, which is a common site of access utilized when placing pacemakers and Swan-Ganz catheters. Incidental notation of a dilated coronary sinus on echocardiography should raise the suspicion of PLSVC. The diagnosis should be confirmed by saline contrast echocardiography."
JEFFREY H SPIEGEL,"The back pain consortium (BACPAC) research program: structure, research priorities, and methods","In 2019, the National Health Interview survey found that nearly 59% of adults reported pain some, most, or every day in the past 3 months, with 39% reporting back pain, making back pain the most prevalent source of pain, and a significant issue among adults. Often, identifying a direct, treatable cause for back pain is challenging, especially as it is often attributed to complex, multifaceted issues involving biological, psychological, and social components. Due to the difficulty in treating the true cause of chronic low back pain (cLBP), an over-reliance on opioid pain medications among cLBP patients has developed, which is associated with increased prevalence of opioid use disorder and increased risk of death. To combat the rise of opioid-related deaths, the National Institutes of Health (NIH) initiated the Helping to End Addiction Long-TermSM (HEAL) initiative, whose goal is to address the causes and treatment of opioid use disorder while also seeking to better understand, diagnose, and treat chronic pain. The NIH Back Pain Consortium (BACPAC) Research Program, a network of 14 funded entities, was launched as a part of the HEAL initiative to help address limitations surrounding the diagnosis and treatment of cLBP. This paper provides an overview of the BACPAC research program's goals and overall structure, and describes the harmonization efforts across the consortium, define its research agenda, and develop a collaborative project which utilizes the strengths of the network. The purpose of this paper is to serve as a blueprint for other consortia tasked with the advancement of pain related science."
MARC S RYSMAN,Moment inequalities in the context of simulated and predicted variables,"This paper explores the effects of simulated moments on the performance of inference methods based on moment inequalities. Commonly used confidence sets for parameters are level sets of criterion functions whose boundary points may depend on sample moments in an irregular manner. Due to this feature, simulation errors can affect the performance of inference in non-standard ways. In particular, a (first-order) bias due to the simulation errors may remain in the estimated boundary of the confidence set. We demonstrate, through Monte Carlo experiments, that simulation errors can significantly reduce the coverage probabilities of confidence sets in small samples. The size distortion is particularly severe when the number of inequality restrictions is large. These results highlight the danger of ignoring the sampling variations due to the simulation errors in moment inequality models. Similar issues arise when using predicted variables in moment inequalities models. We propose a method for properly correcting for these variations based on regularizing the intersection of moments in parameter space, and we show that our proposed method performs well theoretically and in practice."
KEVIN LANG,Ben-Porath meets Lazear: lifetime skill investment and occupation choice with multiple skills,"We develop a fairly general and tractable model of investment when workers can invest in multiple skills and different jobs put different weights on those skills. In addition to expected findings such as that younger workers are more likely than older workers to respond to a demand shock by investing in skills whose value unexpectedly increases, we derive some less obvious results. Credit constraints may affect investment even when they do not bind it equilibrium. If there are mobility costs, firms will generally have an incentive to invest in some of their workers' skills even when there are a large number of similar competitors, and, in equilibrium, there can be overinvestment in all skills. Worker skill accumulation resembles learning by doing even in its absence. We demonstrate how the model can be simulated to show the effect of a shock to the price of individual skills."
KEVIN LANG,"School entry, educational attainment, and quarter of birth: a cautionary tale of a local average treatment effect","Studies of the effects of school entry age on short-run and long-run outcomes generally fail to capture the parameter of policy interest and/or are inconsistent because the instrument they use violates monotonicity, required for identification of a local average treatment effect. Our instrument addresses both problems and shows no effect of entry age on the educational attainment of children born in the fourth quarter who delay enrollment only because they are constrained by the law. We provide suggestive evidence that a waiver policy allowing some children to enter before the legally permissible age increases average educational attainment."
KEVIN LANG,Educational homogamy and assortative mating have not increased,"Some economists have argued that assortative mating between men and women has increased over the last several decades, thereby contributing to increased family income inequality. Sociologists have argued that educational homogamy has increased. We clarify the relation between the two and, using both the Current Population Surveys and the decennial Censuses/American Community Survey, show that neither is correct. The former is based on the use of inappropriate statistical techniques. Both are sensitive to how educational categories are chosen. We also find no evidence that the correlation between spouses' potential earnings has changed dramatically."
KEVIN LANG,"Perspective on ""Ageing and the skill portfolio: Evidence from job based skill measures"" by Audra Bowlus, Hiroaki Mori and Chris Robinson",
KEVIN LANG,Monitoring for worker quality,"Much nonmanagerial work is routine, with all workers having similar output most of the time. However, failure to address occasional challenges can be very costly, and consequently easily detected, while challenges handled well pass unnoticed. We analyze job assignment and worker monitoring for such “guardian” jobs. If monitoring costs are positive but small, monitoring is nonmonotonic in the firm’s belief about the probability that a worker is good. The model explains several empirical regularities regarding nonmanagerial internal labor markets: low use of performance pay, seniority pay, rare demotions, wage ceilings within grade, and wage jumps at promotion."
KEVIN LANG,The promise and pitfalls of differences-in-differences: reflections on '16 and Pregnant' and other applications,"We use the exchange between Kearney/Levine and Jaeger/Joyce/Kaestner on `16 and Pregnant' to reexamine the use of DiD as a response to the failure of nature to properly design an experiment for us. We argue that 1) any DiD paper should address why the original levels of the experimental and control groups differed, and why this would not impact trends, 2) the parallel trends argument requires a justification of the chosen functional form and that the use of the interaction coefficients in probit and logit may be justified in some cases, and 3) parallel trends in the period prior to treatment is suggestive of counterfactual parallel trends, but parallel pre-trends is neither necessary nor sufficient for the parallel counterfactual trends condition to hold. Importantly, the purely statistical approach uses pretesting and thus generates the wrong standard errors. Moreover, we underline the dangers of implicitly or explicitly accepting the null hypothesis when failing to reject the absence of a differential pre-trend."
KEVIN LANG,Is intervention fadeout a scaling artefact?,"To determine whether scaling decisions might account for fadeout of impacts in early education interventions, we reanalyze data from a well-known early mathematics RCT intervention that showed substantial fadeout in the two years after the intervention ended. We examine how various order-preserving transformations of the scale affect the relative mathematics achievement of the control and experimental groups by age. Although fadeout was robust to most transformations, we were able to eliminate or even reverse fadeout by emphasizing differences in scores near typical levels of first-graders while treating differences elsewhere as unimportant. Such a transformation lowers treatment effects at preschool age and raises them in first grade, relative to the original scale. The findings suggest substantial implications for interpreting the effects of educational interventions."
KEVIN LANG,Effort and wages: evidence from the payroll tax,"I show that under a canonical efficiency wage model, a per capita employment tax levied on the employer raises the wage. In contrast, under market-clearing, wages fall regardless of whether effort is contractible. I examine the effect of increases in the earnings base for the payroll tax in the United States on wages of high-wage workers for whom the change represents an increase in a per capita tax. In most specifications, the results suggest that wages rose, consistent with the efficiency wage model, but they are generally too imprecise to rule out large effects of wages on noncontractible productivity that are insufficient to prevent market-clearing. Provided labour demand is inelastic, the results are inconsistent with a model of contractible effort."
KEVIN LANG,Social ties and the job search of recent immigrants,"This article highlights a specific mechanism through which social networks help in job search. The authors characterize the strength of a network by its likelihood of providing a job offer. Using a theoretical model, they show that the difference between wages in jobs found using networks versus those found using formal channels decreases as the network becomes stronger. The authors verify this result for recent immigrants to Canada for whom a strong network is captured by the presence of a “close tie.” Furthermore, structural estimates confirm that the presence of a close tie operates by increasing the likelihood of generating a job offer from the network rather than by altering the network wage distribution."
KEVIN LANG,The black-white education-scaled test-score gap in grades k-7,"We measure the black-white achievement gap from kindergarten through seventh grade on an interval scale created by tying each grade/test score combination to average eventual education. After correcting for various sources of test measurement error, some of which are unique to forward-looking scales, we find no racial component in the evolution of the achievement gap through the first eight years of schooling. Further, most, if not all, of the gap can be explained by socioeconomic differences. Our results suggest that the rising racial test gap in previous studies probably reflects excessive measurement error in testing in the early grades."
KEVIN LANG,Educational homogamy and assortative mating have not increased,"Some economists have argued that assortative mating between men and women has increased over the last several decades. Sociologists have argued that educational homogamy has increased. The two are conceptually distinct but often confused. We clarify the relation between the two and, using both the Current Population Surveys and the decennial Censuses/American Community Survey, show that neither conclusion is correct. Both are sensitive to how educational categories are chosen. The former is based on the use of inappropriate statistical techniques."
KEVIN LANG,How discrimination and bias shape outcomes,"In this article, economists Kevin Lang and Ariella Kahn-Lang Spitzer take up the expansive issue of discrimination, examining specifically how discrimination and bias shape people’s outcomes. The authors focus primarily on discrimination by race, while acknowledging that discrimination exists along many other dimensions as well, including gender, sexual orientation, religion, and ethnicity. They describe evidence of substantial racial disparities in the labor market, education, criminal justice, health, and housing, and they show that in each of these domains, such disparities at least partially reflect discrimination. Lang and Kahn-Lang Spitzer note that the disparities we see are both causes and results of discrimination, and that they reinforce each other. For instance, harsher treatment from the criminal justice system makes it more difficult for black people to get good jobs, which makes it more likely they’ll live in poor neighborhoods and that their children will attend inferior schools. The authors argue that simply prohibiting discrimination isn’t effective, partly because it’s hard to prevent discrimination along dimensions that are correlated with race. Rather, they write, policies are more likely to be successful if they aim to eliminate the statistical association between race and many other social and economic characteristics and to decrease the social distance between people of different races."
KEVIN LANG,Detailed studies of ^100Mo two-neutrino double beta decay in NEMO-3,"The full data set of the NEMO-3 experiment has been used to measure the half-life of the two-neutrino double beta decay of ^100Mo to the ground state of ^100Ru, 𝑇_1/2=[6.81±0.01( stat )^+0.38_−0.40( syst )]×10^18 year. The two-electron energy sum, single electron energy spectra and distribution of the angle between the electrons are presented with an unprecedented statistics of 5×10^5 events and a signal-to-background ratio of ∼ 80. Clear evidence for the Single State Dominance model is found for this nuclear transition. Limits on Majoron emitting neutrinoless double beta decay modes with spectral indices of n=2,3,7, as well as constraints on Lorentz invariance violation and on the bosonic neutrino contribution to the two-neutrino double beta decay mode are obtained."
KEVIN LANG,The long-baseline neutrino experiment: exploring fundamental symmetries of the universe,"The preponderance of matter over antimatter in the early Universe, the dynamics of the supernova bursts that produced the heavy elements necessary for life and whether protons eventually decay --- these mysteries at the forefront of particle physics and astrophysics are key to understanding the early evolution of our Universe, its current state and its eventual fate. The Long-Baseline Neutrino Experiment (LBNE) represents an extensively developed plan for a world-class experiment dedicated to addressing these questions. LBNE is conceived around three central components: (1) a new, high-intensity neutrino source generated from a megawatt-class proton accelerator at Fermi National Accelerator Laboratory, (2) a near neutrino detector just downstream of the source, and (3) a massive liquid argon time-projection chamber deployed as a far detector deep underground at the Sanford Underground Research Facility. This facility, located at the site of the former Homestake Mine in Lead, South Dakota, is approximately 1,300 km from the neutrino source at Fermilab -- a distance (baseline) that delivers optimal sensitivity to neutrino charge-parity symmetry violation and mass ordering effects. This ambitious yet cost-effective design incorporates scalability and flexibility and can accommodate a variety of upgrades and contributions. With its exceptional combination of experimental configuration, technical capabilities, and potential for transformative discoveries, LBNE promises to be a vital facility for the field of particle physics worldwide, providing physicists from around the globe with opportunities to collaborate in a twenty to thirty year program of exciting science. In this document we provide a comprehensive overview of LBNE's scientific objectives, its place in the landscape of neutrino physics worldwide, the technologies it will incorporate and the capabilities it will possess."
KEVIN LANG,Final results on Se-82 double beta decay to the ground state of Kr-82 from the NEMO-3 experiment,"Using data from the NEMO-3 experiment, we have measured the two-neutrino double beta decay (2𝜈𝛽𝛽) half-life of ^82 Se as 𝑇2𝜈1/2=[9.39±0.17( stat )±0.58( syst )]×10^19 y under the single-state dominance hypothesis for this nuclear transition. The corresponding nuclear matrix element is ∣∣𝑀^2𝜈∣∣=0.0498±0.0016. In addition, a search for neutrinoless double beta decay (0𝜈𝛽𝛽) using 0.93 kg of ^82 Se observed for a total of 5.25 y has been conducted and no evidence for a signal has been found. The resulting half-life limit of 𝑇^0𝜈1/2>2.5×10^23 y (90% C.L. ) for the light neutrino exchange mechanism leads to a constraint on the effective Majorana neutrino mass of ⟨𝑚𝜈⟩<(1.2−3.0) eV , where the range reflects 0𝜈𝛽𝛽 nuclear matrix element values from different calculations. Furthermore, constraints on lepton number violating parameters for other 0𝜈𝛽𝛽 mechanisms, such as right-handed currents, majoron emission and R-parity violating supersymmetry modes have been set."
KEVIN LANG,Borrowing in an illegal market: contracting with loan sharks,"Using over 11,000 unlicensed loans to over 1,000 borrowers in Singapore, we provide basic information about an understudied market: illegal moneylending. Borrowers and lenders interact frequently and primarily rely on relational contracts to enforce their agreements. Borrowers have high discount rates, often have gambling and/or substance abuse problems, and often repay late. While lenders sometimes resort to nonfinancial punishments, the primary cost of late repayment is the compounding of a very high interest rate. Consistent with our view that lenders cannot extract all surplus, a crackdown on illegal lending raised interest rates and lowered the size of loans."
KEVIN LANG,The sad truth about happiness scales,"Happiness is reported in ordered intervals (e.g. very, pretty, not too happy). We review and apply standard statistical results to determine when such data permit identification of two groups' relative average happiness. The necessary conditions for nonparametric identification are strong and unlikely to be ever satisfied. Standard parametric approaches cannot identify this ranking unless the variances are exactly equal. If not, ordered probit findings can be reversed by lognormal transformations. For nine prominent happiness research areas, conditions for nonparametric identification are rejected and standard parametric results are reversed using plausible transformations. Tests for a common reporting function consistently reject."
DANA L ROBERT,"Journal of African Christian Biography: v. 8, no. 4","[This issue of the Journal of African Christian Biography highlights some of the entries in the DACB that profile participants in the twentieth-century ecumenical movement in southern Africa. The overwhelming impression one gets of this subject is that of gaps: there is urgent need for more entries that address the myriad ways in which African Christian leaders engaged the ecumenical movement as a network through which to build social capital during the critical period after the Second World War. As African nations became independent of European colonial control, church-educated leaders often acted as global spokesmen for postcolonial visions of society. They cultivated international support structures and led regional independence movements. Ecumenical networks played crucial roles in maintaining structures for education and peace-building in conflictive situations. Nelson Mandela himself, for example, attended Healdtown, a Methodist mission that became the largest high school in the country and educated many of the most important black nationalist leaders at mid century. The entries highlighted in this issue are the tip of the iceberg of what needs to be researched and written. This issue, then, appeals for scholars and church leaders to step up and to provide biographies of “ecumenists”—those who located their commitment to the Body of Christ in an international vision of peace, equality, and justice, in collaboration with other Christians from across Africa and around the world, as well as those who worked at the local level of cooperative church movements.]"
DANA L ROBERT,Testimonies and Truth-tellings: Women in the United Methodist Tradition.,
DANA L ROBERT,"Journal of African Christian Biography: v. 3, no. 2",
DANA L ROBERT,"Journal of African Christian Biography: v. 9, no. 1","[African Christians from the DACB collection are showcased here as well. We also included a short section on the Gospel writer John Mark later in the issue. This excerpt from Oden’s 2011 book the African Memory of Mark: Reassessing Early Church Tradition (InterVarsity Press.) also includes an important historiographical concept—that of African memory and how it contrasts with Western memory. This volume includes three resources made available by CEAC to JACB readers: the abovementioned lecture by Andrews Walls, a transcript of an interview with Lamin Sanneh, and a selection from We Believe, an Early African commentary on the Nicene Creed. This work, written by Christopher Hall and commissioned by Oden, illustrates the intellectual and spiritual wisdom of the early African church. These resources affirm and complement Oden’s historiographical legacy.]"
DANA L ROBERT,"Introduction for John T. McGreevy, Catholicism: a global history from the French Revolution to Pope Francis (2022)","Approximately one third of the world is Christian, and half of those are Roman Catholics. The demographics alone make writing a global history of Catholicism a mammoth task. To attempt the impossible, Professor John T. McGreevy of Notre Dame University has tackled a theme that plays itself out over multiple centuries and diverse cultural settings—the conversation, negotiation, tension, and conflict between traditionalism and modernization. Given that conservatism and progressivism shift meanings according to historical context, the implications of each position in situ are complex and surprising. Only a scholar of McGreevy's maturity and erudition could hope to succeed in such a bold enterprise."
DANA L ROBERT,"Journal of African Christian Biography: v. 4, no. 1",
VIVIEN SCHMIDT,The discursive construction of discontent: varieties of populist anti-system ideas and discursive interactions in Europe,"This article builds on existing scholarship on populism while shifting the lens to focus on the ideational and discursive dynamics of populist power. It defines populism as, at its core, the discursive construction of discontent, as charismatic leaders claiming to speak for the people against the elites use post-truth language to give expression to peoples’ grievances, to mobilize them via real and virtual networks of support, and to disseminate their ideas via social and traditional media in order to win elections and then to govern differently. This article deploys the discursive institutionalist framework of analysis to consider the four main features of the discursive construction of populism – the message, the messenger, the medium, and the milieu. Throughout, the article illustrates by considering not only the rise of populist anti-system parties in European countries but also the special challenges this poses for the EU."
VIVIEN SCHMIDT,Politicization in the EU: between national politics and EU political dynamics,"The EU has become increasingly politicized not only at the bottom, due to polarized debates, divided electorates, declining mainstream parties, and rising Euroskeptic populism; or from the bottom up, as national politics permeates member-state leaders’ positions in the Council. It has also emerged purely at the top, in the increasingly politically charged dynamics of interaction within and among EU actors. Such politicization involves struggles for power and influence that are ideational as much as institutional and coercive. Current theorists of EU integration, because of their tendency to focus on only one or another EU actor have overlooked the EU’s politicized dynamics, even though their accounts, taken together, provide ample evidence of it. The article shows that the EU has gone from what was once metaphorically described with the catchphrase of ‘politics without policy’ at the national level to ‘politics against policy’ in more contentious areas, whereas at the EU level it has moved from ‘policy without politics’ to ‘politics with policy’. The paper illustrates with the cases of the Council and the Commission in the Eurozone crisis."
VIVIEN SCHMIDT,The future of European integration,"The future of Europe is one of differentiated integration, in which the overlapping participation of a large majority of member-states in the EU’s many policy communities makes for a soft core EU, as opposed to the often-evoked hard core around the Eurozone. The paper argues that this multi-clustered Europe is the only possible future, once one takes stock of the challenges facing the EU from its many crises, its problems of governance, and the questions raised about its legitimacy against a background of increasing EU politicization and the rise of populist Euroskepticism."
VIVIEN SCHMIDT,Power and changing modes of governance in the euro crisis,"Which European Union actors are most powerful in the governance of the euro crisis? The euro crisis has reignited the classic debate between intergovernmentalists, who tend to stress the coercive power of dominant member states in the European Council, and supranationalists, who maintain that through the use of institutional power, the Commission, and the European Central Bank turned out the “winners” of the crisis. This article argues that euro crisis governance is best understood not just in terms of one form of power but instead as evolving through different constellations of coercive, institutional, and ideational power that favored different EU actors over the course of the crisis, from the initial fast-burning phase (2010–2012), where the coercive and ideational power of Northern European member states in the European Council was strongest, to the slow-burning phase (2012–2016), when greater influence was afforded supranational actors through the use of ideational and institutional power."
VIVIEN SCHMIDT,European emergency politics and the question of legitimacy,"Emergency politics raises theoretical questions about the legitimacy of executive authorities’ governing activities in times of crisis, and in particular whether ensuring effective outcomes (output) can make up for the temporary suspension of political responsiveness (input) and accountable procedures (throughput). Answers depend not only on the specifics of the emergency actions but also on executives’ rhetorical power to legitimize such actions via ideational/discursive coercion, structuring, or persuasion. After outlining the theoretical issues involved, this contribution considers the legitimacy and rhetorical power of political executives in multilateral emergency politics and technical executives in supranational emergency politics. It uses the cases of the Council and the ECB in the Eurozone and Covid-19 crises in illustration, considering their legitimacy over time, between a crisis’ fast-burning phase of emergency politics and its slow-burning phase of legitimizing normalization or delegitimation."
VIVIEN SCHMIDT,The European framework for economic governance: key issues to assess its recent past and desirable evolution,
VIVIEN SCHMIDT,Ideas and power in employment relations studies,"Motivated by the efforts to understand shifting dynamics of change and stability in employment relations—not least ones brought on by a decade of crisis in what was a neoliberal consensus—scholars increasingly focus on the role of ideas, discourses, and identities. This paper argues for the potential of continuing down this path of employing ideational explanations in employment relations. First, it highlights four key weaknesses of employing more pure materialist–institutionalist approaches that have traditionally dominated employment relations scholarship. Second, it argues that to recognize and build on existing efforts to bring in ideas to employment relations, it is useful to place these on the macro-, meso-, and micro levels. Third, to further advance an ideational perspective on employment relations, it proposes to place more centrally the concept of ideational power. Fourth, it presents key insights from the papers that make up the Special Issue and fleshes out how the individual papers of the Special Issue contribute to this agenda."
VIVIEN SCHMIDT,"Rethinking European economic governance: more effective, more flexible, and more democratic",
VIVIEN SCHMIDT,"Conceptualizing throughput legitimacy: procedural mechanisms of accountability, transparency, inclusiveness and openness in EU governance","This symposium demonstrates the potential for throughput legitimacy as a concept for shedding empirical light on the strengths and weaknesses of multi-level governance, as well as challenging the concept theoretically. This article introduces the symposium by conceptualizing throughput legitimacy as an ‘umbrella concept’, encompassing a constellation of normative criteria not necessarily empirically interrelated. It argues that in order to interrogate multi-level governance processes in all their complexity, it makes sense for us to develop normative standards that are not naïve about the empirical realities of how power is exercised within multilevel governance, or how it may interact with legitimacy. We argue that while throughput legitimacy has its normative limits, it can be substantively useful for these purposes. While being no replacement for input and output legitimacy, throughput legitimacy offers distinctive normative criteria— accountability, transparency, inclusiveness and openness— and points towards substantive institutional reforms."
VIVIEN SCHMIDT,Cautious optimism for EU economic governance and democracy in 2022,
VIVIEN SCHMIDT,"Reconsidering the EU’s economic ideas on markets and law: towards greater effectiveness, accountability and democracy",
VIVIEN SCHMIDT,Economic crisis management in the EU: from past Eurozone mistakes to future promise beyond the COVID-19 pandemic,
VIVIEN SCHMIDT,"Power through, over and in ideas: conceptualizing ideational power in discursive institutionalism","Owing to the tendency of discursive institutionalists to conflate the notion that ‘ideas matter' for policy-making with the ‘power of ideas’, little has been done to explicitly theorize ideational power. To fill this lacuna, the contribution defines ideational power as the capacity of actors (whether individual or collective) to influence other actors’ normative and cognitive beliefs through the use of ideational elements, and – based on insights from the discursive institutionalist literature – suggests three different types of ideational power: power through ideas, understood as the capacity of actors to persuade other actors to accept and adopt their views through the use of ideational elements; power over ideas, meaning the imposition of ideas and the power to resist the inclusion of alternative ideas into the policy-making arena; and power in ideas, which takes place through the establishing of hegemony or institutions imposing constraints on what ideas are considered."
VIVIEN SCHMIDT,Interpretivism in motion: discursive institutionalism as the fourth ‘new’ institutionalism,"Vivienne Schmidt discusses the Rudolph’s development of their interpretative approach in the context of the spirited debates about epistemology and social science inquiry. She builds upon the Rudolphs’ approach to elaborate ‘discursive institutionalism’, a mode of analysis that theorizes the nature of discourse and how discursive exchange contributes to political action and institutional change. Schmidt’s chapter advances beyond most discursive analyses by theorizing ‘ideational power’, or the capacity of actors to use ideas to: influence other actors’ normative and cognitive beliefs; control the meaning and normative value of ideas; and structure discourse by controlling its agenda. Ultimately, Schmidt makes a cogent case for methodological pluralism in the study of ideas, one that can engage and even synthesize a range of analytical approaches."
VIVIEN SCHMIDT,"Reinterpreting the rules ""by stealth' in times of crisis: a discursive institutionalist analysis of the European Central Bank and the European Commission","This article examines the ways in which EU actors have engaged in incremental changes to the eurozone rules ‘by stealth’ ‒ that is, by reinterpreting the rules and recalibrating the numbers without admitting it in their public discourse. Using the methodological framework of discursive institutionalism to focus on agents’ ideas and discursive interactions in institutional context, the article links EU actors’ reinterpretation of rules to their efforts to ensure greater legitimacy in terms of policy performance and governance processes as well as citizen politics. Using the normative theoretical framework of EU democratic systems theory, it analyses EU actors’ considerations of legitimacy not only in terms of their policies’ ‘output’ performance and citizens’ political ‘input’ but also the ‘throughput’ quality of their governance processes. The article illustrates this by elaborating on the different pathways to legitimation of the European Central Bank and the European Commission."
VIVIEN SCHMIDT,Conceptualizing capitalism in the 21st century: the BRICs and the European periphery,"The paper provides a conceptual framework of the Special Issue by theorising and analysing the evolving nature of capitalism in emerging economies and the European periphery today. The conventional conceptual tools of comparative capitalism provide a useful structure for the analysis but have not been up to the task of capturing the changing nature of capitalism in the globalised world, in particular the rise of the emerging powers as well as their current crises. Neither do the conventional tools allow one to capture the developments in the political economies of the European periphery since the Eurozone crisis and the advent of austerity policy. In order to adequately assess these developments we need to rethink the comparative capitalism framework in the global context. To do so, we propose a more dynamic conceptual approach better suited to capturing the changing nature of political economies in and beyond the OECD world. We build upon Uwe Becker's open Varieties of Capitalism framework, among other approaches. Our theoretical conceptualization highlights five aspects: 1) the role of the state as a central force organising the market 2) patrimonialism, understood as a distinct structural mode of political-economic organisation based on clientelism, patronage and informal relations between actors 3) policies and political institutions 4) the role of ideas, and 5) pressures from globalisation and economic crises."
VIVIEN SCHMIDT,The future of differentiated integration: A ‘soft-core’ multi-clustered Europe of overlapping policy communities,"In lieu of a conclusion to the Special Issue, this article discusses the future of Europe as one of differentiated integration. It argues that this future takes the form of member-states’ overlapping participation in the EU’s many policy communities, making for a soft-core Europe, as an alternative option to the hard-core around the Eurozone. The article contends that this multi-clustered Europe is the only feasible future, given the challenges facing the EU from its many crises, its problems of governance, and the difficulties of decision-making against a background of increasing politicization. But such differentiation is not without its problems, given EU decision-rules, the interconnectedness of policy arenas that can spell problems of spillover, and the need for deeper integration in some policy areas (e.g., migration) while others many benefit from less or more highly differentiated integration (e.g., Eurozone). Institutional reforms would also be necessary to ensure a positive future of differentiated integration: while the EU would continue to require a single set of institutions, it would need modified decision-rules to allow for more (and less) differentiation depending upon the area."
VIVIEN SCHMIDT,L’invention d’un nouvel avenir pour l’Europe,"Ces dernières années, l’Union européenne s’est trouvée confrontée à toute une série de crises : tout d’abord la crise de la zone euro, ensuite la crise des réfugiés, puis le Brexit, sans parler des hauts et des bas de la crise de la sécurité lors de chaque attaque terroriste, ou de l’élection récente aux États-Unis de Donald Trump, avec toutes les incertitudes qu’implique celle-ci quant aux relations transatlantiques et à la sécurité. Potentiellement, chacune de ces crises représente pour l’Union européenne un défi existentiel. Toutes nécessitent une réaction concertée de l’Union européenne, ce qui implique une plus grande coordination, d’une façon ou d’une autre, et par conséquent une intégration européenne approfondie. Mais il n’y a que dans un cas, celui de la crise de la zone euro, que l’on ait essayé d’approfondir l’intégration, et ce ne fut pas une grande réussite. Même si l’euro fut effectivement sauvé, la zone euro continue à vivre sous la menace de la déflation, d’une croissance basse, de la montée de la pauvreté et d’un chômage élevé, en particulier dans l’Europe du Sud. On peut pour une bonne part attribuer cela à des politiques contestables liées à une intégration approfondie, laquelle repose sur le principe « gouverner par les règles et réguler par les chiffres » à l’intérieur de la zone euro. En réponse à la crise, plutôt que de proposer une forme d’effacement de la dette ou de mutualisation de celle-ci, accompagnée d’une plus grande stimulation de l’investissement, l’Union européenne a renforcé les règles macroéconomiques qui préconisent une inflation basse, des déficits bas et une dette également basse, en échange de quoi les États membres ont accepté plus de surveillance de leurs budgets nationaux…"
VIVIEN SCHMIDT,Britain-out and Trump-in: a discursive institutionalist analysis of the British referendum on the EU and the US presidential election,"Adding discursive institutionalism to the political science toolkit is key to understanding the victory of the forces pushing the UK to exit from the EU and for Trump's election in the US. The contextualized analysis of the substantive content of agents' ideas enables us to explore the ideational root causes of discontent, including economic neo-liberalism, social liberalism, and political mistrust. The examination of the discursive dynamics of policy coordination and political communication calls attention to agents' rhetorical strategies, the circulation of ideas in discursive communities, and the role of ideational leaders along with that of the public and the media in a post-truth era. Discursive institutionalism also lends insight into questions of power, including how ideational agents have been able to use their persuasive power through ideas to channel people's anger while challenging experts' power over ideas as they upended the long-standing power in ideas of the liberal order."
VIVIEN SCHMIDT,Theorizing institutional change and governance in European responses to the Covid-19 pandemic,
VIVIEN SCHMIDT,Politics shakes up EU governance,"EU governance, which was long apolitical and technocratic, with disagreements handled in private and deals made behind closed doors, has become more politically charged."
VIVIEN SCHMIDT,"Theorizing ideas and discourse in political science: intersubjectivity, neo-institutionalisms, and the power of ideas","Oscar Larsson’s (2015) essay condemns discursive institutionalism for the “sin” of subjectivism. In reality, however, discursive institutionalism emphasizes the intersubjective nature of ideas through its theorization of agents’ “background ideational abilities” and “foreground discursive abilities.” It also avoids relativism by means of Wittgenstein’s distinction between experiences of everyday life and pictures of the world. Contrary to Larsson, what truly separates post-structuralism from discursive institutionalism is the respective approaches’ theorization of the relationship of power to ideas, with discursive institutionalists mainly focused on persuasive power through ideas, while post-structuralists focus on the structural power in ideas or on coercive power over ideas."
VIVIEN SCHMIDT,Where is the European Union today? Will it survive? Can it thrive?,"Over the past few years, the European Union has suffered through a cascading set of crises. The sovereign debt crisis for countries in the Euro began in 2010 with the buildup to the bailout of Greece. This was quickly followed by the refugee crisis, which hit the headlines in 2012, as thousands fled conflict in Syria and poverty in Africa. All the while, the security crisis continued, whether as a result of terrorist attacks, in particular since the Charlie Hebdo massacre of January 2015, or trouble in the neighborhood, especially with Russian incursions in the Ukraine since 2014 and the unending civil wars in the Middle East. And then there was the British vote on exit from the EU in June 2016, raising questions not only about the future of the UK but also about the EU. Finally, let us not forget the most recent potential crisis, the election of US President Donald Trump, with all the uncertainties it brings for the EU in terms of transatlantic relations, trade, and security."
VIVIEN SCHMIDT,The UK and the EU after Brexit—how hard or soft a landing?,
VIVIEN SCHMIDT,Europe’s (Euro) crisis of legitimacy,"This chapter is dedicated to Loukas Tsoukalis, a major intellectual force through his thoughtful scholarly work and insightful policy analysis on Europe, and a great friend. Years ago, he wrote a wonderful textbook on European political economy that I used year after year in my courses on European integration, until it was no longer available. Loukas’ contributions go beyond his writings and role as a public intellectual, however, to include his organizational skills as founding director of Greece’s premier think-tank on Europe, Eliamep, with its stimulating conferences in picturesque villages and on dreamy islands."
VIVIEN SCHMIDT,Europe’s ‘soft-core’ future of differentiated integration,
VIVIEN SCHMIDT,"Differentiated integration through more integration, decentralization, and democracy","The EU is going nowhere unless and until it successfully responds to its multiple crises – of policy, politics, and polity – in ways that enable it to ensure the best possible future for its member states and non-members alike (Fabbrini & Schmidt 2019). Europe’s policy crises are in key areas such as money (how to move forwards in the eurozone), borders (what to do about refugees and migrants), security (how to develop effective security and defense cooperation), the integrity of the EU (how to manage British exit from the EU), and “rule of law” (what to do about the “democratic..."
VIVIEN SCHMIDT,Differentiated European integration and a future ‘soft core’ Europe,
VIVIEN SCHMIDT,Power and changing modes of governance in the Euro Crisis,"Which European Union actors are most powerful in the governance of the euro crisis? The euro crisis has reignited the classic debate between intergovernmentalists, who tend to stress the coercive power of dominant member states in the European Council, and supranationalists, who maintain that through the use of institutional power, the Commission and the European Central Bank turned out the ‘winners’ of the crisis. This paper argues that euro crisis governance is best understood not just in terms of one form of power but instead as evolving through different constellations of coercive, institutional and ideational power that favored different EU actors over the course of the crisis, from the initial fastburning phase (2010-2012), where the coercive and ideational power of Northern European member states in the European Council was strongest, to the slow-burning phase (2012-2016), when greater influence was afforded supranational actors through the use of ideational and institutional power.
 Panel on ‘The coordination of macroeconomic policies at the EU level: unintended shifts in the modes of governance, institutions and power relations’. Paper to be presented at the Council for European Studies Meetings, Chicago (March 28-30, 2018. Published in Governance (2018) https://doi.org/10.1111/gove.12318
 "
VIVIEN SCHMIDT,The past decade and the future of governance and democracy: populist challenges to liberal democracy,
VIVIEN SCHMIDT,Is there a deficit of throughput legitimacy in the EU?,
VIVIEN SCHMIDT,“Is there a deficit of throughput legitimacy in the EU?”,"Theoretical questions regarding how to legitimate the European Union’s supranational governance have turned the spotlight on procedural legitimacy and its relationship to political and performance legitimacy. In EU studies, such questions translate into a focus on ‘throughput’ legitimacy’ and its relationship to ‘input’ and ‘output’ legitimacy. This article defines the terms, discusses their interrelationships, then elaborates on the five criteria encompassed by throughput—efficacy, accountability, transparency, inclusiveness and openness. It illustrates with examples from EU governance, ending with the throughput legitimacy problems of EU institutional actors during the Eurozone crisis. The article argues that although throughput legitimacy is no substitute for input or output legitimacy, it is nonetheless an indispensable component of legitimacy. The interrelationships of its different components as well as with output and input legitimacy are at the center of the dilemmas of EU governance today."
VIVIEN SCHMIDT,Ideational power and pathways to legitimation in the euro crisis,"How have European Union institutional actors sought to build, defend or undermine the legitimacy of crisis management during the euro crisis? Scholars have tended to investigate the euro crisis from either a pragmatic and prescriptive perspective – asking which reforms are necessary to build legitimacy in the governance structure of the Eurozone – or an analytical perspective focused on the power wielding of actors useful for understanding what actors have done and why they have been influential or not. The paper argues that rather than bifurcating the issues of legitimacy and power politics, much may be gained by investigating the relationship between legitimacy and power. Specifically, the paper employs the concept of ideational power to analyze the strategies through which actors have sought to defend their claims to three constitutive dimensions of legitimacy – input, output and throughput legitimacy – and proposes a matrix of nine pathways to legitimation that played into processes of legitimacy battles in the Eurozone crisis."
VIVIEN SCHMIDT,The roots of neo-liberal resilience: Explaining continuity and change in background ideas in Europe's political economy,"Neo-liberalism has come to constitute the core background idea of European political economies, as the unquestioned set of beliefs, understandings, or core philosophy exercising a seemingly incontrovertible hold since the 1980s in Europe. Using a discursive institutionalist framework, this article defines background ideas; describes their different forms, levels, and types; theorizes about the nature of continuity and change in such ideas; and considers the agents and discursive processes through which such ideas are constructed and disseminated. It illustrates throughout with examples of neo-liberalism, from the philosophical origins through its many different permutations in different institutional contexts over time. This article concludes that although ‘background ideas’ as a concept remains somewhat elusive, it is nonetheless useful as a way of understanding how neo-liberalism has managed to infuse people’s deepest assumptions about the possible and thereby to set the limits of the imaginable with regard to political economic action."
VIVIEN SCHMIDT,Rethinking EU governance: from ‘old’ to ‘new’ approaches to who steers integration*,"EU scholars have long been divided on the main drivers of European integration. The original approaches were at odds on whether EU level intergovernmental actors or supranational actors were better able to exercise coercive or institutional power to pursue their interests, with Andrew Moravcsik's liberal intergovernmentalism serving as a baseline for one side of those debates. Newer approaches are similarly divided, but see power in terms of ideational innovation and consensus‐focused deliberation. The one thing old and new approaches have in common is that they ignore the parliamentarists, new and old. What all sides to the debates have failed to recognize is the reality of a ‘new’ EU governance of more politically charged dynamics among all three main EU actors exercising different kinds of power. This has roots not only in the national level's increasing ‘politics against policy’ and its bottom up effects on the EU level. It also stems from EU institutional interactions at the top, and its ‘policy with politics’."
VIVIEN SCHMIDT,"Philippe Zittoun, Frank Fischer, and Nikolaos Zahariadis, eds., Political Formulation of Policy Solutions Bristol: Bristol University Press, 2021",
VIVIEN SCHMIDT,"Economic development, human development, and the pursuit of happiness, April 1, 2, and 3, 2004","The conference asks the questions, how can we make sure that the benefits of economic growth flow into health, education, welfare, and other aspects of human development; and what is the relationship between human development and economic development? Speakers and participants discuss the role that culture, legal and political institutions, the UN Developmental Goals, the level of decision-making, and ethics, play in development."
VIVIEN SCHMIDT,Between power and powerlessness in the euro zone crisis and thereafter,"Employing a multidimensional conception of power shows how interaction between EU institutional actors is structured by different kinds of power – coercive, institutional and ideational – and that none of these are sufficient on their own for actors to successfully drive the reform process. We ask not just who leads the euro zone, but how interactions between actors enable the polity to achieve (or not) its goals. This requires thinking of power in terms of both zero-sum and positive-sum outcomes, which reveals the weakness of the polity as a whole, whatever the power of different institutional actors. In this view, key for the long-term sustainability of the euro zone is an economic and political rebalancing among its members borne out at the level of common ideas and institutions and the leadership of the most resourceful member states. Despite significant reforms in wake of the crisis, such rebalancing seems far beyond the horizon."
VIVIEN SCHMIDT,"Looking ahead: forecasting and planning for the longer-range future, April 1, 2, and 3, 2005","The conference allowed for many highly esteemed scholars and professionals from a broad range of fields to come together to discuss strategies designed for the 21st century and beyond. The speakers and discussants covered a broad range of subjects including: long-term policy analysis, forecasting for business and investment, the National Intelligence Council Global Trends 2020 report, Europe’s transition from the Marshal plan to the EU, forecasting global transitions, foreign policy planning, and forecasting for defense."
SANDOR VAJDA,Protein Docking by the Underestimation of Free Energy Funnels in the Space of Encounter Complexes,"Similarly to protein folding, the association of two proteins is driven by a free energy funnel, determined by favorable interactions in some neighborhood of the native state. We describe a docking method based on stochastic global minimization of funnel-shaped energy functions in the space of rigid body motions (SE(3)) while accounting for flexibility of the interface side chains. The method, called semi-definite programming-based underestimation (SDU), employs a general quadratic function to underestimate a set of local energy minima and uses the resulting underestimator to bias further sampling. While SDU effectively minimizes functions with funnel-shaped basins, its application to docking in the rotational and translational space SE(3) is not straightforward due to the geometry of that space. We introduce a strategy that uses separate independent variables for side-chain optimization, center-to-center distance of the two proteins, and five angular descriptors of the relative orientations of the molecules. The removal of the center-to-center distance turns out to vastly improve the efficiency of the search, because the five-dimensional space now exhibits a well-behaved energy surface suitable for underestimation. This algorithm explores the free energy surface spanned by encounter complexes that correspond to local free energy minima and shows similarity to the model of macromolecular association that proceeds through a series of collisions. Results for standard protein docking benchmarks establish that in this space the free energy landscape is a funnel in a reasonably broad neighborhood of the native state and that the SDU strategy can generate docking predictions with less than 5 � ligand interface Ca root-mean-square deviation while achieving an approximately 20-fold efficiency gain compared to Monte Carlo methods."
SANDOR VAJDA,Protein docking by the underestimation of free energy funnels in the space of encounter complexes,"Similarly to protein folding, the association of two proteins is driven by a free energy funnel, determined by favorable interactions in some neighborhood of the native state. We describe a docking method based on stochastic global minimization of funnel-shaped energy functions in the space of rigid body motions (SE(3)) while accounting for flexibility of the interface side chains. The method, called semi-definite programming-based underestimation (SDU), employs a general quadratic function to underestimate a set of local energy minima and uses the resulting underestimator to bias further sampling. While SDU effectively minimizes functions with funnel-shaped basins, its application to docking in the rotational and translational space SE(3) is not straightforward due to the geometry of that space. We introduce a strategy that uses separate independent variables for side-chain optimization, center-to-center distance of the two proteins, and five angular descriptors of the relative orientations of the molecules. The removal of the center-to-center distance turns out to vastly improve the efficiency of the search, because the five-dimensional space now exhibits a well-behaved energy surface suitable for underestimation. This algorithm explores the free energy surface spanned by encounter complexes that correspond to local free energy minima and shows similarity to the model of macromolecular association that proceeds through a series of collisions. Results for standard protein docking benchmarks establish that in this space the free energy landscape is a funnel in a reasonably broad neighborhood of the native state and that the SDU strategy can generate docking predictions with less than 5 A ligand interface C(alpha) root-mean-square deviation while achieving an approximately 20-fold efficiency gain compared to Monte Carlo methods."
SANDOR VAJDA,Efficient maintenance and update of nonbonded lists in macromolecular simulations,"Molecular mechanics and dynamics simulations use distance based cutoff approximations for faster computation of pairwise van der Waals and electrostatic energy terms. These approximations traditionally use a precalculated and periodically updated list of interacting atom pairs, known as the “nonbonded neighborhood lists” or nblists, in order to reduce the overhead of finding atom pairs that are within distance cutoff. The size of nblists grows linearly with the number of atoms in the system and superlinearly with the distance cutoff, and as a result, they require significant amount of memory for large molecular systems. The high space usage leads to poor cache performance, which slows computation for large distance cutoffs. Also, the high cost of updates means that one cannot afford to keep the data structure always synchronized with the configuration of the molecules when efficiency is at stake. We propose a dynamic octree data structure for implicit maintenance of nblists using space linear in the number of atoms but independent of the distance cutoff. The list can be updated very efficiently as the coordinates of atoms change during the simulation. Unlike explicit nblists, a single octree works for all distance cutoffs. In addition, octree is a cache-friendly data structure, and hence, it is less prone to cache miss slowdowns on modern memory hierarchies than nblists. Octrees use almost 2 orders of magnitude less memory, which is crucial for simulation of large systems, and while they are comparable in performance to nblists when the distance cutoff is small, they outperform nblists for larger systems and large cutoffs. Our tests show that octree implementation is approximately 1.5 times faster in practical use case scenarios as compared to nblists."
SANDOR VAJDA,Fragments and hot spots in drug discovery,
SANDOR VAJDA,Amidino-rocaglates: a potent class of eIF4A inhibitors,"Rocaglates share a common cyclopenta[b]benzofuran core that inhibits eukaryotic translation initiation by modifying the behavior of the RNA helicase, eIF4A. Working as interfacial inhibitors, rocaglates stabilize the association between eIF4A and RNA, which can lead to the formation of steric barriers that block initiating ribosomes. There is significant interest in the development and expansion of rocaglate derivatives, as several members of this family have been shown to possess potent anti-neoplastic activity in vitro and in vivo. To further our understanding of rocaglate diversity and drug design, herein we explore the RNA clamping activity of >200 unique rocaglate derivatives. Through this, we report on the identification and characterization of a potent class of synthetic rocaglates called amidino-rocaglates. These compounds are among the most potent rocaglates documented to date and, taken together, this work offers important information that will guide the future design of rocaglates with improved biological properties."
SANDOR VAJDA,Discovery of macrocyclic inhibitors of apurinic/apyrimidinic endonuclease 1,"Apurinic/apyrimidinic endonuclease 1 (APE1) is an essential base excision repair enzyme that is upregulated in a number of cancers, contributes to resistance of tumors treated with DNA-alkylating or -oxidizing agents, and has recently been identified as an important therapeutic target. In this work, we identified hot spots for binding of small organic molecules experimentally in high resolution crystal structures of APE1 and computationally through the use of FTMAP analysis (http://ftmap.bu.edu). Guided by these hot spots, a library of drug-like macrocycles was docked and then screened for inhibition of APE1 endonuclease activity. In an iterative process, hot-spot-guided docking, characterization of inhibition of APE1 endonuclease, and cytotoxicity of cancer cells were used to design next generation macrocycles. To assess target selectivity in cells, selected macrocycles were analyzed for modulation of DNA damage. Taken together, our studies suggest that macrocycles represent a promising class of compounds for inhibition of APE1 in cancer cells."
SANDOR VAJDA,How proteins bind macrocycles,"The potential utility of synthetic macrocycles (MCs) as drugs, particularly against low-druggability targets such as protein-protein interactions, has been widely discussed. There is little information, however, to guide the design of MCs for good target protein-binding activity or bioavailability. To address this knowledge gap, we analyze the binding modes of a representative set of MC-protein complexes. The results, combined with consideration of the physicochemical properties of approved macrocyclic drugs, allow us to propose specific guidelines for the design of synthetic MC libraries with structural and physicochemical features likely to favor strong binding to protein targets as well as good bioavailability. We additionally provide evidence that large, natural product-derived MCs can bind targets that are not druggable by conventional, drug-like compounds, supporting the notion that natural product-inspired synthetic MCs can expand the number of proteins that are druggable by synthetic small molecules."
SANDOR VAJDA,Protein docking refinement by convex underestimation in the low-dimensional subspace of encounter complexes,"We propose a novel stochastic global optimization algorithm with applications to the refinement stage of protein docking prediction methods. Our approach can process conformations sampled from multiple clusters, each roughly corresponding to a different binding energy funnel. These clusters are obtained using a density-based clustering method. In each cluster, we identify a smooth “permissive” subspace which avoids high-energy barriers and then underestimate the binding energy function using general convex polynomials in this subspace. We use the underestimator to bias sampling towards its global minimum. Sampling and subspace underestimation are repeated several times and the conformations sampled at the last iteration form a refined ensemble. We report computational results on a comprehensive benchmark of 224 protein complexes, establishing that our refined ensemble significantly improves the quality of the conformations of the original set given to the algorithm. We also devise a method to enhance the ensemble from which near-native models are selected."
SANDOR VAJDA,Novel Druggable Hot Spots in Avian Influenza Neuraminidase H5N1 Revealed by Computational Solvent Mapping of a Reduced and Representative Receptor Ensemble,"The influenza virus subtype H5N1 has raised concerns of a possible human pandemic threat because of its high virulence and mutation rate. Although several approved anti-influenza drugs effectively target the neuraminidase, some strains have already acquired resistance to the currently available anti-influenza drugs. In this study, we present the synergistic application of extended explicit solvent molecular dynamics (MD) and computational solvent mapping (CS-Map) to identify putative 'hot spots' within flexible binding regions of N1 neuraminidase. Using representative conformations of the N1 binding region extracted from a clustering analysis of four concatenated 40-ns MD simulations, CS-Map was utilized to assess the ability of small, solvent-sized molecules to bind within close proximity to the sialic acid binding region. Mapping analyses of the dominant MD conformations reveal the presence of additional hot spot regions in the 150- and 430-loop regions. Our hot spot analysis provides further support for the feasibility of developing high-affinity inhibitors capable of binding these regions, which appear to be unique to the N1 strain."
SANDOR VAJDA,PRECISE: A Database of Predicted and Consensus Interaction Sites in Enzymes,"PRECISE (Predicted and Consensus Interaction Sites in Enzymes) is a database of interactions between the amino acid residues of an enzyme and its ligands (substrate and transition state analogs, cofactors, inhibitors and products). It is available online at http://precise.bu.edu. In the current version, all information on interactions is extracted from the enzyme–ligand complexes in the Protein Data Bank (PDB) by performing the following steps: (i) clustering homologous enzyme chains such that, in each cluster, the proteins have the same EC number and all sequences are similar; (ii) selecting a representative chain for each cluster; (iii) selecting ligand types; (iv) finding non-bonded interactions and hydrogen bonds; and (v) summing the interactions for all chains within the cluster. The output of the search is the color-coded sequence of the representative. The colors indicate the total number of interactions found at each amino acid position in all chains of the cluster. Clicking on a residue displays a detailed list of interactions for that residue. Optional filters allow restricting the output to selected chains in the cluster, to non-bonded or hydrogen bonding interactions, and to selected ligand types. The binding site information is essential for understanding and altering substrate specificity and for the design of enzyme inhibitors."
SANDOR VAJDA,Protein Docking by the Underestimation of Free Energy Funnels in the Space of Encounter Complexes,"Similarly to protein folding, the association of two proteins is driven by a free energy funnel, determined by favorable interactions in some neighborhood of the native state. We describe a docking method based on stochastic global minimization of funnel-shaped energy functions in the space of rigid body motions (SE(3)) while accounting for flexibility of the interface side chains. The method, called semi-definite programming-based underestimation (SDU), employs a general quadratic function to underestimate a set of local energy minima and uses the resulting underestimator to bias further sampling. While SDU effectively minimizes functions with funnel-shaped basins, its application to docking in the rotational and translational space SE(3) is not straightforward due to the geometry of that space. We introduce a strategy that uses separate independent variables for side-chain optimization, center-to-center distance of the two proteins, and five angular descriptors of the relative orientations of the molecules. The removal of the center-to-center distance turns out to vastly improve the efficiency of the search, because the five-dimensional space now exhibits a well-behaved energy surface suitable for underestimation. This algorithm explores the free energy surface spanned by encounter complexes that correspond to local free energy minima and shows similarity to the model of macromolecular association that proceeds through a series of collisions. Results for standard protein docking benchmarks establish that in this space the free energy landscape is a funnel in a reasonably broad neighborhood of the native state and that the SDU strategy can generate docking predictions with less than 5 Å ligand interface Cα root-mean-square deviation while achieving an approximately 20-fold efficiency gain compared to Monte Carlo methods. Author SummaryProtein–protein interactions play a central role in various aspects of the structural and functional organization of the cell, and their elucidation is crucial for a better understanding of processes such as metabolic control, signal transduction, and gene regulation. Genomewide proteomics studies, primarily yeast two-hybrid assays, will provide an increasing list of interacting proteins, but only a small fraction of the potential complexes will be amenable to direct experimental analysis. Thus, it is important to develop computational docking methods that can elucidate the details of specific interactions at the atomic level. Protein–protein docking generally starts with a rigid body search that generates a large number of docked conformations with good shape, electrostatic, and chemical complementarity. The conformations are clustered to obtain a manageable number of models, but the current methods are unable to select the most likely structure among these models. Here we describe a refinement algorithm that, applied to the individual clusters, improves the quality of the models. The better models are suitable for higher-accuracy energy calculation, thereby increasing the chances that near-native structures can be identified, and thus the refinement increases the reliability of the entire docking algorithm."
SANDOR VAJDA,Improved prediction of MHC-peptide binding using protein language models,"Major histocompatibility complex Class I (MHC-I) molecules bind to peptides derived from intracellular antigens and present them on the surface of cells, allowing the immune system (T cells) to detect them. Elucidating the process of this presentation is essential for regulation and potential manipulation of the cellular immune system. Predicting whether a given peptide binds to an MHC molecule is an important step in the above process and has motivated the introduction of many computational approaches to address this problem. NetMHCPan, a pan-specific model for predicting binding of peptides to any MHC molecule, is one of the most widely used methods which focuses on solving this binary classification problem using shallow neural networks. The recent successful results of Deep Learning (DL) methods, especially Natural Language Processing (NLP-based) pretrained models in various applications, including protein structure determination, motivated us to explore their use in this problem. Specifically, we consider the application of deep learning models pretrained on large datasets of protein sequences to predict MHC Class I-peptide binding. Using the standard performance metrics in this area, and the same training and test sets, we show that our models outperform NetMHCpan4.1, currently considered as the-state-of-the-art."
GEORGE J ANNAS,"Centerscope: v. 16, no. 1-2",
GEORGE J ANNAS,"Centerscope: v. 15, no. 1-3",
GEORGE J ANNAS,Genetic variation and gene expression across multiple tissues and developmental stages in a nonhuman primate,"By analyzing multitissue gene expression and genome-wide genetic variation data in samples from a vervet monkey pedigree, we generated a transcriptome resource and produced the first catalog of expression quantitative trait loci (eQTLs) in a nonhuman primate model. This catalog contains more genome-wide significant eQTLs per sample than comparable human resources and identifies sex- and age-related expression patterns. Findings include a master regulatory locus that likely has a role in immune function and a locus regulating hippocampal long noncoding RNAs (lncRNAs), whose expression correlates with hippocampal volume. This resource will facilitate genetic investigation of quantitative traits, including brain and behavioral phenotypes relevant to neuropsychiatric disorders."
GEORGE J ANNAS,"Centerscope: v. 5, no. 1-5",
GEORGE J ANNAS,"Centerscope: v. 8, no. 1-4",
GEORGE J ANNAS,Transmission of Staphylococcus aureus from humans to green monkeys in The Gambia as revealed by whole-genome sequencing,"Staphylococcus aureus is an important pathogen of humans and animals. We genome sequenced 90 S. aureus isolates from The Gambia: 46 isolates from invasive disease in humans, 13 human carriage isolates, and 31 monkey carriage isolates. We inferred multiple anthroponotic transmissions of S. aureus from humans to green monkeys (Chlorocebus sabaeus) in The Gambia over different time scales. We report a novel monkey-associated clade of S. aureus that emerged from a human-to-monkey switch estimated to have occurred 2,700 years ago. Adaptation of this lineage to the monkey host is accompanied by the loss of phage-carrying genes that are known to play an important role in human colonization. We also report recent anthroponotic transmission of the well-characterized human lineages sequence type 6 (ST6) and ST15 to monkeys, probably because of steadily increasing encroachment of humans into the monkeys' habitat. Although we have found no evidence of transmission of S. aureus from monkeys to humans, as the two species come into ever-closer contact, there might be an increased risk of additional interspecies exchanges of potential pathogens. IMPORTANCE: The population structures of Staphylococcus aureus in humans and monkeys in sub-Saharan Africa have been previously described using multilocus sequence typing (MLST). However, these data lack the power to accurately infer details regarding the origin and maintenance of new adaptive lineages. Here, we describe the use of whole-genome sequencing to detect transmission of S. aureus between humans and nonhuman primates and to document the genetic changes accompanying host adaptation. We note that human-to-monkey switches tend to be more common than the reverse and that a novel monkey-associated clade is likely to have emerged from such a switch approximately 2,700 years ago. Moreover, analysis of the accessory genome provides important clues as to the genetic changes underpinning host adaptation and, in particular, shows that human-to-monkey switches tend to be associated with the loss of genes known to confer adaptation to the human host."
GEORGE J ANNAS,"Genome-wide association studies of serum magnesium, potassium, and sodium concentrations identify six loci influencing serum magnesium levels","Magnesium, potassium, and sodium, cations commonly measured in serum, are involved in many physiological processes including energy metabolism, nerve and muscle function, signal transduction, and fluid and blood pressure regulation. To evaluate the contribution of common genetic variation to normal physiologic variation in serum concentrations of these cations, we conducted genome-wide association studies of serum magnesium, potassium, and sodium concentrations using ∼2.5 million genotyped and imputed common single nucleotide polymorphisms (SNPs) in 15,366 participants of European descent from the international CHARGE Consortium. Study-specific results were combined using fixed-effects inverse-variance weighted meta-analysis. SNPs demonstrating genome-wide significant (p<5×10−8) or suggestive associations (p<4×10−7) were evaluated for replication in an additional 8,463 subjects of European descent. The association of common variants at six genomic regions (in or near MUC1, ATP2B1, DCDC5, TRPM6, SHROOM3, and MDS1) with serum magnesium levels was genome-wide significant when meta-analyzed with the replication dataset. All initially significant SNPs from the CHARGE Consortium showed nominal association with clinically defined hypomagnesemia, two showed association with kidney function, two with bone mineral density, and one of these also associated with fasting glucose levels. Common variants in CNNM2, a magnesium transporter studied only in model systems to date, as well as in CNNM3 and CNNM4, were also associated with magnesium concentrations in this study. We observed no associations with serum sodium or potassium levels exceeding p<4×10−7. Follow-up studies of newly implicated genomic loci may provide additional insights into the regulation and homeostasis of human serum magnesium levels. Author Summary Magnesium, potassium, and sodium are involved in important physiological processes. To better understand how common genetic variation may contribute to inter-individual differences in serum concentrations of these electrolytes, we evaluated single nucleotide polymorphisms (SNPs) across the genome in association with serum magnesium, potassium, and sodium levels in 15,366 participants of European descent from the CHARGE Consortium. We then verified the associations in an additional 8,463 study participants. Six different genomic regions contain variants that are reproducibly associated with serum magnesium levels, and only one of the regions had been previously known to influence serum magnesium concentrations in humans. The identified SNPs also show association with clinically defined hypomagnesemia, and some of them with traits that have been linked to serum magnesium levels, including kidney function, fasting glucose, and bone mineral density. We further provide evidence for a physiological role of magnesium transporters in humans which have previously only been studied in model systems. None of the SNPs evaluated in our study are significantly associated with serum levels of sodium or potassium. Additional studies are needed to investigate the underlying molecular mechanisms in order to help us understand the contribution of these newly identified regions to magnesium homeostasis."
GEORGE J ANNAS,"Boston University medicine: v. 5, no. 1-2",
GEORGE J ANNAS,Ancient hybridization and strong adaptation to viruses across African vervet monkey populations,"Vervet monkeys are among the most widely distributed nonhuman primates, show considerable phenotypic diversity, and have long been an important biomedical model for a variety of human diseases and in vaccine research. Using whole-genome sequencing data from 163 vervets sampled from across Africa and the Caribbean, we find high diversity within and between taxa and clear evidence that taxonomic divergence was reticulate rather than following a simple branching pattern. A scan for diversifying selection across taxa identifies strong and highly polygenic selection signals affecting viral processes. Furthermore, selection scores are elevated in genes whose human orthologs interact with HIV and in genes that show a response to experimental simian immunodeficiency virus (SIV) infection in vervet monkeys but not in rhesus macaques, suggesting that part of the signal reflects taxon-specific adaptation to SIV."
GEORGE J ANNAS,"Centerscope: v. 9, no. 1-4",
GEORGE J ANNAS,"Centerscope: v. 7, no. 1-4",
GEORGE J ANNAS,"Centerscope: v. 10, no. 1-4",
GEORGE J ANNAS,(Re)criminalizing abortion: returning to the political with stories,"Abortion stories have always played a powerful role in advancing women’s rights. In the abortion sphere particularly, the personal is political. Following the Court’s reversal of Roe v. Wade, abortion politics, and abortion storytelling, take on an even deeper political role in challenging the bloodless judicial language of Dobbs with the lived experience of women."
GEORGE J ANNAS,"Trust, brutality, and human dignity: how “partial birth abortion” helps shape American biopolitics","In this Article, I explore how nearly continuous public rhetorical challenges to abortion in the political realm first led the public and the courts to turn away from a particular abortion procedure (intact dilation and extraction, also known as partial-birth abortion) which political agitators labeled as “barbaric” and then to view physicians who performed abortions not as legitimate professionals, but simply as “abortionists,” and sometimes as evil “Frankensteins.” “Abortionists” use no “medical judgment” and are unworthy of deference by state legislatures, Congress, or the courts when deciding how or when to perform an abortion. The concentration on the welfare of fetuses and the actions of physicians permitted the abortion debate to bypass discussion of both the rights and welfare of pregnant patients, including their right to health, and to virtually never mention that abortion restrictions primarily affect people in poverty who cannot afford to seek reproductive health care, including an abortion, by traveling to a nonrestrictive state. Understanding the power of extreme rhetoric, including the use of social media in political campaigns and the use and misuse of concrete terms such as murder, infanticide, brutality, and dismemberment, and abstract concepts such as “human dignity,” can help us plot a post-Dobbs way forward. Perhaps the demise of Roe can lead to a birth of a new rhetoric on abortion, one that concentrates on the right to health of everyone, including the right to make reproductive decisions, and requires moving abortion back into the realm of contemporary medicine, complete with a meaningful doctor-patient relationship protected by privacy and financed in a way that is accessible to all pregnant patients."
GEORGE J ANNAS,The long-baseline neutrino experiment: exploring fundamental symmetries of the universe,"The preponderance of matter over antimatter in the early Universe, the dynamics of the supernova bursts that produced the heavy elements necessary for life and whether protons eventually decay --- these mysteries at the forefront of particle physics and astrophysics are key to understanding the early evolution of our Universe, its current state and its eventual fate. The Long-Baseline Neutrino Experiment (LBNE) represents an extensively developed plan for a world-class experiment dedicated to addressing these questions. LBNE is conceived around three central components: (1) a new, high-intensity neutrino source generated from a megawatt-class proton accelerator at Fermi National Accelerator Laboratory, (2) a near neutrino detector just downstream of the source, and (3) a massive liquid argon time-projection chamber deployed as a far detector deep underground at the Sanford Underground Research Facility. This facility, located at the site of the former Homestake Mine in Lead, South Dakota, is approximately 1,300 km from the neutrino source at Fermilab -- a distance (baseline) that delivers optimal sensitivity to neutrino charge-parity symmetry violation and mass ordering effects. This ambitious yet cost-effective design incorporates scalability and flexibility and can accommodate a variety of upgrades and contributions. With its exceptional combination of experimental configuration, technical capabilities, and potential for transformative discoveries, LBNE promises to be a vital facility for the field of particle physics worldwide, providing physicists from around the globe with opportunities to collaborate in a twenty to thirty year program of exciting science. In this document we provide a comprehensive overview of LBNE's scientific objectives, its place in the landscape of neutrino physics worldwide, the technologies it will incorporate and the capabilities it will possess."
GEORGE J ANNAS,The genome of the vervet ( Chlorocebus aethiops sabaeus ),"We describe a genome reference of the African green monkey or vervet (Chlorocebus aethiops). This member of the Old World monkey (OWM) superfamily is uniquely valuable for genetic investigations of simian immunodeficiency virus (SIV), for which it is the most abundant natural host species, and of a wide range of health-related phenotypes assessed in Caribbean vervets (C. a. sabaeus), whose numbers have expanded dramatically since Europeans introduced small numbers of their ancestors from West Africa during the colonial era. We use the reference to characterize the genomic relationship between vervets and other primates, the intra-generic phylogeny of vervet subspecies, and genome-wide structural variations of a pedigreed C. a. sabaeus population. Through comparative analyseswith human and rhesus macaque, we characterize at high resolution the unique chromosomal fission events that differentiate the vervets and their close relatives from most other catarrhine primates, in whom karyotype is highly conserved. We also provide a summary of transposable elements and contrast these with the rhesus macaque and human. Analysis of sequenced genomes representing each of the main vervet subspecies supports previously hypothesized relationships between these populations, which range across most of sub-Saharan Africa, while uncovering high levels of genetic diversity within each. Sequence-based analyses of major histocompatibility complex (MHC) polymorphisms reveal extremely low diversity in Caribbean C. a. sabaeus vervets, compared to vervets from putatively ancestral West African regions. In the C. a. sabaeus research population, we discover the first structural variations that are, in some cases, predicted to have a deleterious effect; future studies will determine the phenotypic impact of these variations."
GEORGE J ANNAS,X-Ray Polarization Observations of BL Lacertae,"Blazars are a class of jet-dominated active galactic nuclei with a typical double-humped spectral energy distribution. It is of common consensus that the synchrotron emission is responsible for the low frequency peak, while the origin of the high frequency hump is still debated. The analysis of X-rays and their polarization can provide a valuable tool to understand the physical mechanisms responsible for the origin of high-energy emission of blazars. We report the first observations of BL Lacertae (BL Lac) performed with the Imaging X-ray Polarimetry Explorer, from which an upper limit to the polarization degree Π X &lt; 12.6% was found in the 2–8 keV band. We contemporaneously measured the polarization in radio, infrared, and optical wavelengths. Our multiwavelength polarization analysis disfavors a significant contribution of proton-synchrotron radiation to the X-ray emission at these epochs. Instead, it supports a leptonic origin for the X-ray emission in BL Lac."
GEORGE J ANNAS,"The future of human nature: a symposium on the promises and challenges of the revolutions in genomics and computer science, April 10, 11, and 12, 2003","This conference focused on scientific and technological advances in genetics, computer science, and their convergence during the next 35 to 250 years. In particular, it focused on directed evolution, the futures it allows, the shape of society in those futures, and the robustness of human nature against technological change at the level of individuals, groups, and societies. It is taken as a premise that biotechnology and computer science will mature and will reinforce one another. During the period of interest, human cloning, germ-line genetic engineering, and an array of reproductive technologies will become feasible and safe. Early in this period, we can reasonably expect the processing power of a laptop computer to exceed the collective processing power of every human brain on the planet; later in the period human/machine interfaces will begin to emerge. Whether such technologies will take hold is not known. But if they do, human evolution is likely to proceed at a greatly accelerated rate; human nature as we know it may change markedly, if it does not disappear altogether, and new intelligent species may well be created."
ALICIA BORINSKY,Interlocución y aporía: notas a propósito de Alberto Girri y Juan Gelman,
ALICIA BORINSKY,Macedonio y el humor de Julio Cortazar,
ALICIA BORINSKY,?Que leemos cuando leemos?,
ALICIA BORINSKY,Correspondencia de Macedonio Fernandez a Gomez de la Serna,
ALICIA BORINSKY,Castracion y lujos: la escritura de Manuel Puig,
ALICIA BORINSKY,Memoria del vacío: una nota personal en torno a la escritura y las raíces judías,
ALICIA BORINSKY,Borges en nuestra biblioteca,
ALICIA BORINSKY,Estridencias y silencios femeninos: entre princesas y vírgenes,
ALICIA BORINSKY,José Emilio Pacheco: relecturas e historia,
ALICIA BORINSKY,José Donoso: el otro coloquio de los perros,
ALICIA BORINSKY,Lecturas y traducción: Dormir al sol de Adolfo Bioy Casares,
ALICIA BORINSKY,Altazor: entierros y comienzos,
ALICIA BORINSKY,Saul Yurkievich - in memoriam,
KAREN N ALLEN,Disulfide-mediated stabilization of the IκB kinase binding domain of NF-κB essential modulator (NEMO),"Human NEMO (NF-κB essential modulator) is a 419 residue scaffolding protein that, together with catalytic subunits IKKα and IKKβ, forms the IκB kinase (IKK) complex, a key regulator of NF-κB pathway signaling. NEMO is an elongated homodimer comprising mostly α-helix. It has been shown that a NEMO fragment spanning residues 44-111, which contains the IKKα/β binding site, is structurally disordered in the absence of bound IKKβ. Herein we show that enforcing dimerization of NEMO1-120 or NEMO44-111 constructs through introduction of one or two interchain disulfide bonds, through oxidation of the native Cys54 residue and/or at position 107 through a Leu107Cys mutation, induces a stable α-helical coiled-coil structure that is preorganized to bind IKKβ with high affinity. Chemical and thermal denaturation studies showed that, in the context of a covalent dimer, the ordered structure was stabilized relative to the denatured state by up to 3 kcal/mol. A full-length NEMO-L107C protein formed covalent dimers upon treatment of mammalian cells with H2O2. Furthermore, NEMO-L107C bound endogenous IKKβ in A293T cells, reconstituted TNF-induced NF-κB signaling in NEMO-deficient cells, and interacted with TRAF6. Our results indicate that the IKKβ binding domain of NEMO possesses an ordered structure in the unbound state, provided that it is constrained within a dimer as is the case in the constitutively dimeric full-length NEMO protein. The stability of the NEMO coiled coil is maintained by strong interhelix interactions in the region centered on residue 54. The disulfide-linked constructs we describe herein may be useful for crystallization of NEMO's IKKβ binding domain in the absence of bound IKKβ, thereby facilitating the structural characterization of small-molecule inhibitors."
KAREN N ALLEN,"Structure of a rabbit muscle fructose-1,6-bisphosphate aldolase A dimer variant","Fructose-1,6-bisphosphate aldolase (aldolase) is an essential enzyme in glycolysis and gluconeogenesis. In addition to this primary function, aldolase is also known to bind to a variety of other proteins, a property that may allow it to perform 'moonlighting' roles in the cell. Although monomeric and dimeric aldolases possess full catalytic activity, the enzyme occurs as an unusually stable tetramer, suggesting a possible link between the oligomeric state and these noncatalytic cellular roles. Here, the first high-resolution X-ray crystal structure of rabbit muscle D128V aldolase, a dimeric form of aldolase mimicking the clinically important D128G mutation in humans associated with hemolytic anemia, is presented. The structure of the dimer was determined to 1.7 angstroms resolution with the product DHAP bound in the active site. The turnover of substrate to produce the product ligand demonstrates the retention of catalytic activity by the dimeric aldolase. The D128V mutation causes aldolase to lose intermolecular contacts with the neighboring subunit at one of the two interfaces of the tetramer. The tertiary structure of the dimer does not significantly differ from the structure of half of the tetramer. Analytical ultracentrifugation confirms the occurrence of the enzyme as a dimer in solution. The highly stable structure of aldolase with an independent active site is consistent with a model in which aldolase has evolved as a multimeric scaffold to perform other noncatalytic functions."
KAREN N ALLEN,"Structure of a Rabbit Muscle Fructose-1,6-Bisphosphate Aldolase A Dimer Variant","The X-ray crystallographic structure of a dimer variant of fructose-1,6-bisphosphate aldolase demonstrates a stable oligomer that mirrors half of the native tetramer. The presence of product demonstrates that this is an active form. Fructose-1,6-bisphosphate aldolase (aldolase) is an essential enzyme in glycolysis and gluconeogenesis. In addition to this primary function, aldolase is also known to bind to a variety of other proteins, a property that may allow it to perform 'moonlighting' roles in the cell. Although monomeric and dimeric aldolases possess full catalytic activity, the enzyme occurs as an unusually stable tetramer, suggesting a possible link between the oligomeric state and these noncatalytic cellular roles. Here, the first high-resolution X-ray crystal structure of rabbit muscle D128V aldolase, a dimeric form of aldolase mimicking the clinically important D128G mutation in humans associated with hemolytic anemia, is presented. The structure of the dimer was determined to 1.7 Å resolution with the product DHAP bound in the active site. The turnover of substrate to produce the product ligand demonstrates the retention of catalytic activity by the dimeric aldolase. The D128V mutation causes aldolase to lose intermolecular contacts with the neighboring subunit at one of the two interfaces of the tetramer. The tertiary structure of the dimer does not significantly differ from the structure of half of the tetramer. Analytical ultracentrifugation confirms the occurrence of the enzyme as a dimer in solution. The highly stable structure of aldolase with an independent active site is consistent with a model in which aldolase has evolved as a multimeric scaffold to perform other noncatalytic functions."
BRUCE ANDERSON,Persistent anomalies of the extratropical Northern Hemisphere wintertime circulation as an initiator of El Niño/Southern Oscillation events.,"Climates across both hemispheres are strongly influenced by tropical Pacific variability associated with the El Niño/Southern Oscillation (ENSO). Conversely, extratropical variability also can affect the tropics. In particular, seasonal-mean alterations of near-surface winds associated with the North Pacific Oscillation (NPO) serve as a significant extratropical forcing agent of ENSO. However, it is still unclear what dynamical processes give rise to year-to-year shifts in these long-lived NPO anomalies. Here we show that intraseasonal variability in boreal winter pressure patterns over the Central North Pacific (CNP) imparts a significant signature upon the seasonal-mean circulations characteristic of the NPO. Further we show that the seasonal-mean signature results in part from year-to-year variations in persistent, quasi-stationary low-pressure intrusions into the subtropics of the CNP, accompanied by the establishment of persistent, quasi-stationary high-pressure anomalies over high latitudes of the CNP. Overall, we find that the frequency of these persistent extratropical anomalies (PEAs) during a given winter serves as a key modulator of intraseasonal variability in extratropical North Pacific circulations and, through their influence on the seasonal-mean circulations in and around the southern lobe of the NPO, the state of the equatorial Pacific 9-12 months later."
BRUCE ANDERSON,Improving the characterization of ex vivo human brain optical properties using high numerical aperture optical coherence tomography by spatially constraining the confocal parameters,"SIGNIFICANCE: The optical properties of biological samples provide information about the structural characteristics of the tissue and any changes arising from pathological conditions. Optical coherence tomography (OCT) has proven to be capable of extracting tissue's optical properties using a model that combines the exponential decay due to tissue scattering and the axial point spread function that arises from the confocal nature of the detection system, particularly for higher numerical aperture (NA) measurements. A weakness in estimating the optical properties is the inter-parameter cross-talk between tissue scattering and the confocal parameters defined by the Rayleigh range and the focus depth. AIM: In this study, we develop a systematic method to improve the characterization of optical properties with high-NA OCT. APPROACH: We developed a method that spatially parameterizes the confocal parameters in a previously established model for estimating the optical properties from the depth profiles of high-NA OCT. RESULTS: The proposed parametrization model was first evaluated on a set of intralipid phantoms and then validated using a low-NA objective in which cross-talk from the confocal parameters is negligible. We then utilize our spatially parameterized model to characterize optical property changes introduced by a tissue index matching process using a simple immersion agent, 2,2'-thiodiethonal. CONCLUSIONS: Our approach improves the confidence of parameter estimation by reducing the degrees of freedom in the non-linear fitting model."
BRUCE ANDERSON,Selective Disruption of the Cerebral Neocortex in Alzheimer's Disease,"BACKGROUND. Alzheimer's disease (AD) and its transitional state mild cognitive impairment (MCI) are characterized by amyloid plaque and tau neurofibrillary tangle (NFT) deposition within the cerebral neocortex and neuronal loss within the hippocampal formation. However, the precise relationship between pathologic changes in neocortical regions and hippocampal atrophy is largely unknown. METHODOLOGY/PRINCIPAL FINDINGS. In this study, combining structural MRI scans and automated image analysis tools with reduced cerebrospinal fluid (CSF) Aß levels, a surrogate for intra-cranial amyloid plaques and elevated CSF phosphorylated tau (p-tau) levels, a surrogate for neocortical NFTs, we examined the relationship between the presence of Alzheimer's pathology, gray matter thickness of select neocortical regions, and hippocampal volume in cognitively normal older participants and individuals with MCI and AD (n=724). Amongst all 3 groups, only select heteromodal cortical regions significantly correlated with hippocampal volume. Amongst MCI and AD individuals, gray matter thickness of the entorhinal cortex and inferior temporal gyrus significantly predicted longitudinal hippocampal volume loss in both amyloid positive and p-tau positive individuals. Amongst cognitively normal older adults, thinning only within the medial portion of the orbital frontal cortex significantly differentiated amyloid positive from amyloid negative individuals whereas thinning only within the entorhinal cortex significantly discriminated p-tau positive from p-tau negative individuals. CONCLUSIONS/SIGNIFICANCE. Cortical Aß and tau pathology affects gray matter thinning within select neocortical regions and potentially contributes to downstream hippocampal degeneration. Neocortical Alzheimer's pathology is evident even amongst older asymptomatic individuals suggesting the existence of a preclinical phase of dementia."
BRUCE ANDERSON,"Concussion, microvascular injury, and early tauopathy in young athletes after impact head injury and an impact concussion mouse model","The mechanisms underpinning concussion, traumatic brain injury, and chronic traumatic encephalopathy, and the relationships between these disorders, are poorly understood. We examined post-mortem brains from teenage athletes in the acute-subacute period after mild closed-head impact injury and found astrocytosis, myelinated axonopathy, microvascular injury, perivascular neuroinflammation, and phosphorylated tau protein pathology. To investigate causal mechanisms, we developed a mouse model of lateral closed-head impact injury that uses momentum transfer to induce traumatic head acceleration. Unanaesthetized mice subjected to unilateral impact exhibited abrupt onset, transient course, and rapid resolution of a concussion-like syndrome characterized by altered arousal, contralateral hemiparesis, truncal ataxia, locomotor and balance impairments, and neurobehavioural deficits. Experimental impact injury was associated with axonopathy, blood–brain barrier disruption, astrocytosis, microgliosis (with activation of triggering receptor expressed on myeloid cells, TREM2), monocyte infiltration, and phosphorylated tauopathy in cerebral cortex ipsilateral and subjacent to impact. Phosphorylated tauopathy was detected in ipsilateral axons by 24 h, bilateral axons and soma by 2 weeks, and distant cortex bilaterally at 5.5 months post-injury. Impact pathologies co-localized with serum albumin extravasation in the brain that was diagnostically detectable in living mice by dynamic contrast-enhanced MRI. These pathologies were also accompanied by early, persistent, and bilateral impairment in axonal conduction velocity in the hippocampus and defective long-term potentiation of synaptic neurotransmission in the medial prefrontal cortex, brain regions distant from acute brain injury. Surprisingly, acute neurobehavioural deficits at the time of injury did not correlate with blood–brain barrier disruption, microgliosis, neuroinflammation, phosphorylated tauopathy, or electrophysiological dysfunction. Furthermore, concussion-like deficits were observed after impact injury, but not after blast exposure under experimental conditions matched for head kinematics. Computational modelling showed that impact injury generated focal point loading on the head and seven-fold greater peak shear stress in the brain compared to blast exposure. Moreover, intracerebral shear stress peaked before onset of gross head motion. By comparison, blast induced distributed force loading on the head and diffuse, lower magnitude shear stress in the brain. We conclude that force loading mechanics at the time of injury shape acute neurobehavioural responses, structural brain damage, and neuropathological sequelae triggered by neurotrauma. These results indicate that closed-head impact injuries, independent of concussive signs, can induce traumatic brain injury as well as early pathologies and functional sequelae associated with chronic traumatic encephalopathy. These results also shed light on the origins of concussion and relationship to traumatic brain injury and its aftermath."
BRUCE ANDERSON,Identification and reconstruction of low-energy electrons in the ProtoDUNE-SP detector,
BRUCE ANDERSON,The long-baseline neutrino experiment: exploring fundamental symmetries of the universe,"The preponderance of matter over antimatter in the early Universe, the dynamics of the supernova bursts that produced the heavy elements necessary for life and whether protons eventually decay --- these mysteries at the forefront of particle physics and astrophysics are key to understanding the early evolution of our Universe, its current state and its eventual fate. The Long-Baseline Neutrino Experiment (LBNE) represents an extensively developed plan for a world-class experiment dedicated to addressing these questions. LBNE is conceived around three central components: (1) a new, high-intensity neutrino source generated from a megawatt-class proton accelerator at Fermi National Accelerator Laboratory, (2) a near neutrino detector just downstream of the source, and (3) a massive liquid argon time-projection chamber deployed as a far detector deep underground at the Sanford Underground Research Facility. This facility, located at the site of the former Homestake Mine in Lead, South Dakota, is approximately 1,300 km from the neutrino source at Fermilab -- a distance (baseline) that delivers optimal sensitivity to neutrino charge-parity symmetry violation and mass ordering effects. This ambitious yet cost-effective design incorporates scalability and flexibility and can accommodate a variety of upgrades and contributions. With its exceptional combination of experimental configuration, technical capabilities, and potential for transformative discoveries, LBNE promises to be a vital facility for the field of particle physics worldwide, providing physicists from around the globe with opportunities to collaborate in a twenty to thirty year program of exciting science. In this document we provide a comprehensive overview of LBNE's scientific objectives, its place in the landscape of neutrino physics worldwide, the technologies it will incorporate and the capabilities it will possess."
BRUCE ANDERSON,Scintillation light detection in the 6-m drift-length ProtoDUNE Dual Phase liquid argon TPC,"DUNE is a dual-site experiment for long-baseline neutrino oscillation studies, neutrino astrophysics and nucleon decay searches. ProtoDUNE Dual Phase (DP) is a 6  ×  6  ×  6 m 3 liquid argon time-projection-chamber (LArTPC) that recorded cosmic-muon data at the CERN Neutrino Platform in 2019-2020 as a prototype of the DUNE Far Detector. Charged particles propagating through the LArTPC produce ionization and scintillation light. The scintillation light signal in these detectors can provide the trigger for non-beam events. In addition, it adds precise timing capabilities and improves the calorimetry measurements. In ProtoDUNE-DP, scintillation and electroluminescence light produced by cosmic muons in the LArTPC is collected by photomultiplier tubes placed up to 7 m away from the ionizing track. In this paper, the ProtoDUNE-DP photon detection system performance is evaluated with a particular focus on the different wavelength shifters, such as PEN and TPB, and the use of Xe-doped LAr, considering its future use in giant LArTPCs. The scintillation light production and propagation processes are analyzed and a comparison of simulation to data is performed, improving understanding of the liquid argon properties."
BRUCE ANDERSON,Northeast Pacific marine heatwaves linked to Kuroshio Extension variability,"Marine heatwave events in the Northeast Pacific Ocean from 2013-2015 and 2019-2020 have had significant impacts on ocean life and livelihoods in the region. Numerous studies have linked these marine heatwaves to known modes of climate variability. Here we show that the observed evolution of the 2013-2015 Northeast Pacific marine heatwave best correlates with the evolution of historical sea surface temperatures in response to variations in the Kuroshio Extension. By using ocean and atmospheric reanalysis data from 1981-2020 and ocean nutrient data from 1993-2020 from an ocean biogeochemistry model, we further report the physical and biogeochemical changes during this heat event and their relation to these same Kuroshio variations. Using these results, we propose an atmospheric teleconnection between Kuroshio Extension variations and Marine Heatwaves in the Northeast Pacific. This teleconnection’s influence further extends to the marine biogeochemistry and productivity in the Northeast Pacific region via Kuroshio-influenced modifications to mixed layer thickness."
BRUCE ANDERSON,Need and vision for global medium-resolution Landsat and Sentinel-2 data products,
MIN YE,Adapting or atrophying: China's Belt and Road after the Covid-19 pandemic,"From January to June 2020, China experienced the full cycle of Covid-19: outbreak, containment, economic reopening, and return of political life. The country’s policy discourse and activities during the cycle show that the BRI strategy maintains a high level of continuity after the pandemic. Specifically, motivations for the initiative today are similar to those at its launch in 2013, and interests developed during its implementation have grown following the Covid-19 outbreak, albeit with different emphases. Overall, Chinese “middle elites” will continue to help stabilize BRI and globalization during the Covid-19 pandemic and beyond."
MIN YE,U.S-China competition in the post-Covid world: globalization at the crossroad,"Covid 19 has severely challenged the geopolitical environment surrounding China and the United States. Both countries experienced the worst public health and economic crisis in a century. And yet, termination in international travels and policy exchange between China and the U.S have further intensified the bilateral rivalry and made global cooperation hopelessly difficult to attain, at a time when such cooperation was most needed."
MIN YE,The long-baseline neutrino experiment: exploring fundamental symmetries of the universe,"The preponderance of matter over antimatter in the early Universe, the dynamics of the supernova bursts that produced the heavy elements necessary for life and whether protons eventually decay --- these mysteries at the forefront of particle physics and astrophysics are key to understanding the early evolution of our Universe, its current state and its eventual fate. The Long-Baseline Neutrino Experiment (LBNE) represents an extensively developed plan for a world-class experiment dedicated to addressing these questions. LBNE is conceived around three central components: (1) a new, high-intensity neutrino source generated from a megawatt-class proton accelerator at Fermi National Accelerator Laboratory, (2) a near neutrino detector just downstream of the source, and (3) a massive liquid argon time-projection chamber deployed as a far detector deep underground at the Sanford Underground Research Facility. This facility, located at the site of the former Homestake Mine in Lead, South Dakota, is approximately 1,300 km from the neutrino source at Fermilab -- a distance (baseline) that delivers optimal sensitivity to neutrino charge-parity symmetry violation and mass ordering effects. This ambitious yet cost-effective design incorporates scalability and flexibility and can accommodate a variety of upgrades and contributions. With its exceptional combination of experimental configuration, technical capabilities, and potential for transformative discoveries, LBNE promises to be a vital facility for the field of particle physics worldwide, providing physicists from around the globe with opportunities to collaborate in a twenty to thirty year program of exciting science. In this document we provide a comprehensive overview of LBNE's scientific objectives, its place in the landscape of neutrino physics worldwide, the technologies it will incorporate and the capabilities it will possess."
MIN YE,Better than bipolar: US-China competition from TPP to BRI and FOIP,"Policy circles in the U.S. and China are full of skeptics of interdependence. Entering the intense stage of great-power contest, they have advocated decoupling as an acceptable--and even desirable--path forward. But, is interdependence truly that bad? Is it such a vulnerability to the warring states that decoupling should be actively pursued? Or, has interdependence also contributed to stability in the U.S-China rivalry? How? This chapter addresses the validity and limitations of three dominant bipolar perspectives--Thucydides’s Trap, Civilizational Clash, and Divided Peace. Then it presents the domestic processes of the Trans-Pacific Partnership, the Belt and Road Initiative, and the Free and Open Indo-Pacific Strategy. The analysis establishes that, while power and ideological conflicts are salient, these competing initiatives demonstrate ""complex competition"" embedded in an interdependent strategic environment. The interplay of domestic and transnational coalitions has worked to mitigate the great power rivalry in the recent decade."
STEPHEN G MARKS,Butterfly genome reveals promiscuous exchange of mimicry adaptations among species,"The evolutionary importance of hybridization and introgression has long been debated1. Hybrids are usually rare and unfit, but even infrequent hybridization can aid adaptation by transferring beneficial traits between species. Here we use genomic tools to investigate introgression in Heliconius, a rapidly radiating genus of neotropical butterflies widely used in studies of ecology, behaviour, mimicry and speciation2,3,4,5. We sequenced the genome of Heliconius melpomene and compared it with other taxa to investigate chromosomal evolution in Lepidoptera and gene flow among multiple Heliconius species and races. Among 12,669 predicted genes, biologically important expansions of families of chemosensory and Hox genes are particularly noteworthy. Chromosomal organization has remained broadly conserved since the Cretaceous period, when butterflies split from the Bombyx (silkmoth) lineage. Using genomic resequencing, we show hybrid exchange of genes between three co-mimics, Heliconius melpomene, Heliconius timareta and Heliconius elevatus, especially at two genomic regions that control mimicry pattern. We infer that closely related Heliconius species exchange protective colour-pattern genes promiscuously, implying that hybridization has an important role in adaptive radiation."
STEPHEN G MARKS,Multiple Independent Loci at Chromosome 15q25.1 Affect Smoking Quantity: a Meta-Analysis and Comparison with Lung Cancer and COPD,"Recently, genetic association findings for nicotine dependence, smoking behavior, and smoking-related diseases converged to implicate the chromosome 15q25.1 region, which includes the CHRNA5-CHRNA3-CHRNB4 cholinergic nicotinic receptor subunit genes. In particular, association with the nonsynonymous CHRNA5 SNP rs16969968 and correlates has been replicated in several independent studies. Extensive genotyping of this region has suggested additional statistically distinct signals for nicotine dependence, tagged by rs578776 and rs588765. One goal of the Consortium for the Genetic Analysis of Smoking Phenotypes (CGASP) is to elucidate the associations among these markers and dichotomous smoking quantity (heavy versus light smoking), lung cancer, and chronic obstructive pulmonary disease (COPD). We performed a meta-analysis across 34 datasets of European-ancestry subjects, including 38,617 smokers who were assessed for cigarettes-per-day, 7,700 lung cancer cases and 5,914 lung-cancer-free controls (all smokers), and 2,614 COPD cases and 3,568 COPD-free controls (all smokers). We demonstrate statistically independent associations of rs16969968 and rs588765 with smoking (mutually adjusted p-values<10−35 and >10−8 respectively). Because the risk alleles at these loci are negatively correlated, their association with smoking is stronger in the joint model than when each SNP is analyzed alone. Rs578776 also demonstrates association with smoking after adjustment for rs16969968 (p<10−6). In models adjusting for cigarettes-per-day, we confirm the association between rs16969968 and lung cancer (p<10−20) and observe a nominally significant association with COPD (p = 0.01); the other loci are not significantly associated with either lung cancer or COPD after adjusting for rs16969968. This study provides strong evidence that multiple statistically distinct loci in this region affect smoking behavior. This study is also the first report of association between rs588765 (and correlates) and smoking that achieves genome-wide significance; these SNPs have previously been associated with mRNA levels of CHRNA5 in brain and lung tissue. Author Summary Nicotine binds to cholinergic nicotinic receptors, which are composed of a variety of subunits. Genetic studies for smoking behavior and smoking-related diseases have implicated a genomic region that encodes the alpha5, alpha3, and beta4 subunits. We examined genetic data across this region for over 38,000 smokers, a subset of which had been assessed for lung cancer or chronic obstructive pulmonary disease. We demonstrate strong evidence that there are at least two statistically independent loci in this region that affect risk for heavy smoking. One of these loci represents a change in the protein structure of the alpha5 subunit. This work is also the first to report strong evidence of association between smoking and a group of genetic variants that are of biological interest because of their links to expression of the alpha5 cholinergic nicotinic receptor subunit gene. These advances in understanding the genetic influences on smoking behavior are important because of the profound public health burdens caused by smoking and nicotine addiction."
STEPHEN G MARKS,A super-earth and sub-neptune transiting the late-type M Dwarf LP 791-18,"Planets occur most frequently around cool dwarfs, but only a handful of specific examples are known to orbit the latest-type M stars. Using TESS photometry, we report the discovery of two planets transiting the low-mass star called LP 791-18 (identified by TESS as TOI 736). This star has spectral type M6V, effective temperature 2960 K, and radius 0.17 R ⊙, making it the third-coolest star known to host planets. The two planets straddle the radius gap seen for smaller exoplanets; they include a 1.1R ⊕ planet on a 0.95 day orbit and a 2.3R ⊕ planet on a 5 day orbit. Because the host star is small the decrease in light during these planets' transits is fairly large (0.4% and 1.7%). This has allowed us to detect both planets' transits from ground-based photometry, refining their radii and orbital ephemerides. In the future, radial velocity observations and transmission spectroscopy can both probe these planets' bulk interior and atmospheric compositions, and additional photometric monitoring would be sensitive to even smaller transiting planets."
STEPHEN G MARKS,"The L 98-59 system: three transiting, terrestrial-size planets orbiting a nearby M dwarf","We report the Transiting Exoplanet Survey Satellite (TESS) discovery of three terrestrial-size planets transiting L 98-59 (TOI-175, TIC 307210830)—a bright M dwarf at a distance of 10.6 pc. Using the Gaia-measured distance and broadband photometry, we find that the host star is an M3 dwarf. Combined with the TESS transits from three sectors, the corresponding stellar parameters yield planet radii ranging from 0.8 R ⊕ to 1.6 R ⊕. All three planets have short orbital periods, ranging from 2.25 to 7.45 days with the outer pair just wide of a 2:1 period resonance. Diagnostic tests produced by the TESS Data Validation Report and the vetting package DAVE rule out common false-positive sources. These analyses, along with dedicated follow-up and the multiplicity of the system, lend confidence that the observed signals are caused by planets transiting L 98-59 and are not associated with other sources in the field. The L 98-59 system is interesting for a number of reasons: the host star is bright (V = 11.7 mag, K = 7.1 mag) and the planets are prime targets for further follow-up observations including precision radial-velocity mass measurements and future transit spectroscopy with the James Webb Space Telescope; the near-resonant configuration makes the system a laboratory to study planetary system dynamical evolution; and three planets of relatively similar size in the same system present an opportunity to study terrestrial planets where other variables (age, metallicity, etc.) can be held constant. L 98-59 will be observed in four more TESS sectors, which will provide a wealth of information on the three currently known planets and have the potential to reveal additional planets in the system."
STEPHEN G MARKS,The revised TESS Input Catalog and candidate target list,"We describe the catalogs assembled and the algorithms used to populate the revised TESS Input Catalog (TIC), based on the incorporation of the Gaia second data release. We also describe a revised ranking system for prioritizing stars for 2 minute cadence observations, and we assemble a revised Candidate Target List (CTL) using that ranking. The TIC is available on the Mikulski Archive for Space Telescopes server, and an enhanced CTL is available through the Filtergraph data visualization portal system at http://filtergraph.vanderbilt.edu/tess_ctl."
STEPHEN G MARKS,The long-baseline neutrino experiment: exploring fundamental symmetries of the universe,"The preponderance of matter over antimatter in the early Universe, the dynamics of the supernova bursts that produced the heavy elements necessary for life and whether protons eventually decay --- these mysteries at the forefront of particle physics and astrophysics are key to understanding the early evolution of our Universe, its current state and its eventual fate. The Long-Baseline Neutrino Experiment (LBNE) represents an extensively developed plan for a world-class experiment dedicated to addressing these questions. LBNE is conceived around three central components: (1) a new, high-intensity neutrino source generated from a megawatt-class proton accelerator at Fermi National Accelerator Laboratory, (2) a near neutrino detector just downstream of the source, and (3) a massive liquid argon time-projection chamber deployed as a far detector deep underground at the Sanford Underground Research Facility. This facility, located at the site of the former Homestake Mine in Lead, South Dakota, is approximately 1,300 km from the neutrino source at Fermilab -- a distance (baseline) that delivers optimal sensitivity to neutrino charge-parity symmetry violation and mass ordering effects. This ambitious yet cost-effective design incorporates scalability and flexibility and can accommodate a variety of upgrades and contributions. With its exceptional combination of experimental configuration, technical capabilities, and potential for transformative discoveries, LBNE promises to be a vital facility for the field of particle physics worldwide, providing physicists from around the globe with opportunities to collaborate in a twenty to thirty year program of exciting science. In this document we provide a comprehensive overview of LBNE's scientific objectives, its place in the landscape of neutrino physics worldwide, the technologies it will incorporate and the capabilities it will possess."
STEPHEN G MARKS,Event Horizon Telescope imaging of the archetypal blazar 3C 279 at an extreme 20 microarcsecond resolution,"3C 279 is an archetypal blazar with a prominent radio jet that show broadband flux density variability across the entire electromagnetic spectrum. We use an ultra-high angular resolution technique – global Very Long Baseline Interferometry (VLBI) at 1.3 mm (230 GHz) – to resolve the innermost jet of 3C 279 in order to study its fine-scale morphology close to the jet base where highly variable γ-ray emission is thought to originate, according to various models. The source was observed during four days in April 2017 with the Event Horizon Telescope at 230 GHz, including the phased Atacama Large Millimeter/submillimeter Array (ALMA), at an angular resolution of ∼20 μas (at a redshift of z = 0.536 this corresponds to ∼0.13 pc  ∼ 1700 Schwarzschild radii with a black hole mass M<jats:sub>BH</jats:sub> = 8 × 10^8 M_⊙). Imaging and model-fitting techniques were applied to the data to parameterize the fine-scale source structure and its variation. We find a multicomponent inner jet morphology with the northernmost component elongated perpendicular to the direction of the jet, as imaged at longer wavelengths. The elongated nuclear structure is consistent on all four observing days and across different imaging methods and model-fitting techniques, and therefore appears robust. Owing to its compactness and brightness, we associate the northern nuclear structure as the VLBI “core”. This morphology can be interpreted as either a broad resolved jet base or a spatially bent jet. We also find significant day-to-day variations in the closure phases, which appear most pronounced on the triangles with the longest baselines. Our analysis shows that this variation is related to a systematic change of the source structure. Two inner jet components move non-radially at apparent speeds of ∼15 c and ∼20 c (∼1.3 and ∼1.7 μas day^−1, respectively), which more strongly supports the scenario of traveling shocks or instabilities in a bent, possibly rotating jet. The observed apparent speeds are also coincident with the 3C 279 large-scale jet kinematics observed at longer (cm) wavelengths, suggesting no significant jet acceleration between the 1.3 mm core and the outer jet. The intrinsic brightness temperature of the jet components are ≲10^10 K, a magnitude or more lower than typical values seen at ≥7 mm wavelengths. The low brightness temperature and morphological complexity suggest that the core region of 3C 279 becomes optically thin at short (mm) wavelengths."
STEPHEN G MARKS,First M87 Event Horizon Telescope results. II. Array and instrumentation,"The Event Horizon Telescope (EHT) is a very long baseline interferometry (VLBI) array that comprises millimeter- and submillimeter-wavelength telescopes separated by distances comparable to the diameter of the Earth. At a nominal operating wavelength of ~1.3 mm, EHT angular resolution (λ/D) is ~25 μas, which is sufficient to resolve nearby supermassive black hole candidates on spatial and temporal scales that correspond to their event horizons. With this capability, the EHT scientific goals are to probe general relativistic effects in the strong-field regime and to study accretion and relativistic jet formation near the black hole boundary. In this Letter we describe the system design of the EHT, detail the technology and instrumentation that enable observations, and provide measures of its performance. Meeting the EHT science objectives has required several key developments that have facilitated the robust extension of the VLBI technique to EHT observing wavelengths and the production of instrumentation that can be deployed on a heterogeneous array of existing telescopes and facilities. To meet sensitivity requirements, high-bandwidth digital systems were developed that process data at rates of 64 gigabit s−1, exceeding those of currently operating cm-wavelength VLBI arrays by more than an order of magnitude. Associated improvements include the development of phasing systems at array facilities, new receiver installation at several sites, and the deployment of hydrogen maser frequency standards to ensure coherent data capture across the array. These efforts led to the coordination and execution of the first Global EHT observations in 2017 April, and to event-horizon-scale imaging of the supermassive black hole candidate in M87."
STEPHEN G MARKS,First M87 Event Horizon Telescope results. I. The shadow of the supermassive black hole,"When surrounded by a transparent emission region, black holes are expected to reveal a dark shadow caused by gravitational light bending and photon capture at the event horizon. To image and study this phenomenon, we have assembled the Event Horizon Telescope, a global very long baseline interferometry array observing at a wavelength of 1.3 mm. This allows us to reconstruct event-horizon-scale images of the supermassive black hole candidate in the center of the giant elliptical galaxy M87. We have resolved the central compact radio source as an asymmetric bright emission ring with a diameter of 42 ± 3 μas, which is circular and encompasses a central depression in brightness with a flux ratio gsim10:1. The emission ring is recovered using different calibration and imaging schemes, with its diameter and width remaining stable over four different observations carried out in different days. Overall, the observed image is consistent with expectations for the shadow of a Kerr black hole as predicted by general relativity. The asymmetry in brightness in the ring can be explained in terms of relativistic beaming of the emission from a plasma rotating close to the speed of light around a black hole. We compare our images to an extensive library of ray-traced general-relativistic magnetohydrodynamic simulations of black holes and derive a central mass of M = (6.5 ± 0.7) × 10^9 M ⊙. Our radio-wave observations thus provide powerful evidence for the presence of supermassive black holes in centers of galaxies and as the central engines of active galactic nuclei. They also present a new tool to explore gravity in its most extreme limit and on a mass scale that was so far not accessible."
STEPHEN G MARKS,X-Ray Polarization Observations of BL Lacertae,"Blazars are a class of jet-dominated active galactic nuclei with a typical double-humped spectral energy distribution. It is of common consensus that the synchrotron emission is responsible for the low frequency peak, while the origin of the high frequency hump is still debated. The analysis of X-rays and their polarization can provide a valuable tool to understand the physical mechanisms responsible for the origin of high-energy emission of blazars. We report the first observations of BL Lacertae (BL Lac) performed with the Imaging X-ray Polarimetry Explorer, from which an upper limit to the polarization degree Π X &lt; 12.6% was found in the 2–8 keV band. We contemporaneously measured the polarization in radio, infrared, and optical wavelengths. Our multiwavelength polarization analysis disfavors a significant contribution of proton-synchrotron radiation to the X-ray emission at these epochs. Instead, it supports a leptonic origin for the X-ray emission in BL Lac."
STEPHEN G MARKS,Evaluation of association of HNF1B variants with diverse cancers: collaborative analysis of data from 19 genome-wide association studies,"BACKGROUND. Genome-wide association studies have found type 2 diabetes-associated variants in the HNF1B gene to exhibit reciprocal associations with prostate cancer risk. We aimed to identify whether these variants may have an effect on cancer risk in general versus a specific effect on prostate cancer only. METHODOLOGY/PRINCIPAL FINDINGS. In a collaborative analysis, we collected data from GWAS of cancer phenotypes for the frequently reported variants of HNF1B, rs4430796 and rs7501939, which are in linkage disequilibrium (r2=0.76, HapMap CEU). Overall, the analysis included 16 datasets on rs4430796 with 19,640 cancer cases and 21,929 controls; and 21 datasets on rs7501939 with 26,923 cases and 49,085 controls. Malignancies other than prostate cancer included colorectal, breast, lung and pancreatic cancers, and melanoma. Meta-analysis showed large between-dataset heterogeneity that was driven by different effects in prostate cancer and other cancers. The per-T2D-risk-allele odds ratios (95% confidence intervals) for rs4430796 were 0.79 (0.76, 0.83)] per G allele for prostate cancer (p<10-15 for both); and 1.03 (0.99, 1.07) for all other cancers. Similarly for rs7501939 the per-T2D-risk-allele odds ratios (95% confidence intervals) were 0.80 (0.77, 0.83) per T allele for prostate cancer (p<10-15 for both); and 1.00 (0.97, 1.04) for all other cancers. No malignancy other than prostate cancer had a nominally statistically significant association. CONCLUSIONS/SIGNIFICANCE. The examined HNF1B variants have a highly specific effect on prostate cancer risk with no apparent association with any of the other studied cancer types."
DANIEL ABRAMSON,Evidence and Narrative,
MARK W MILLER,"Caribbean Corals in Crisis: Record Thermal Stress, Bleaching, and Mortality in 2005","BACKGROUND. The rising temperature of the world's oceans has become a major threat to coral reefs globally as the severity and frequency of mass coral bleaching and mortality events increase. In 2005, high ocean temperatures in the tropical Atlantic and Caribbean resulted in the most severe bleaching event ever recorded in the basin. METHODOLOGY/PRINCIPAL FINDINGS. Satellite-based tools provided warnings for coral reef managers and scientists, guiding both the timing and location of researchers' field observations as anomalously warm conditions developed and spread across the greater Caribbean region from June to October 2005. Field surveys of bleaching and mortality exceeded prior efforts in detail and extent, and provided a new standard for documenting the effects of bleaching and for testing nowcast and forecast products. Collaborators from 22 countries undertook the most comprehensive documentation of basin-scale bleaching to date and found that over 80% of corals bleached and over 40% died at many sites. The most severe bleaching coincided with waters nearest a western Atlantic warm pool that was centered off the northern end of the Lesser Antilles. CONCLUSIONS/SIGNIFICANCE. Thermal stress during the 2005 event exceeded any observed from the Caribbean in the prior 20 years, and regionally-averaged temperatures were the warmest in over 150 years. Comparison of satellite data against field surveys demonstrated a significant predictive relationship between accumulated heat stress (measured using NOAA Coral Reef Watch's Degree Heating Weeks) and bleaching intensity. This severe, widespread bleaching and mortality will undoubtedly have long-term consequences for reef ecosystems and suggests a troubled future for tropical marine ecosystems under a warming climate."
MARK W MILLER,The long-baseline neutrino experiment: exploring fundamental symmetries of the universe,"The preponderance of matter over antimatter in the early Universe, the dynamics of the supernova bursts that produced the heavy elements necessary for life and whether protons eventually decay --- these mysteries at the forefront of particle physics and astrophysics are key to understanding the early evolution of our Universe, its current state and its eventual fate. The Long-Baseline Neutrino Experiment (LBNE) represents an extensively developed plan for a world-class experiment dedicated to addressing these questions. LBNE is conceived around three central components: (1) a new, high-intensity neutrino source generated from a megawatt-class proton accelerator at Fermi National Accelerator Laboratory, (2) a near neutrino detector just downstream of the source, and (3) a massive liquid argon time-projection chamber deployed as a far detector deep underground at the Sanford Underground Research Facility. This facility, located at the site of the former Homestake Mine in Lead, South Dakota, is approximately 1,300 km from the neutrino source at Fermilab -- a distance (baseline) that delivers optimal sensitivity to neutrino charge-parity symmetry violation and mass ordering effects. This ambitious yet cost-effective design incorporates scalability and flexibility and can accommodate a variety of upgrades and contributions. With its exceptional combination of experimental configuration, technical capabilities, and potential for transformative discoveries, LBNE promises to be a vital facility for the field of particle physics worldwide, providing physicists from around the globe with opportunities to collaborate in a twenty to thirty year program of exciting science. In this document we provide a comprehensive overview of LBNE's scientific objectives, its place in the landscape of neutrino physics worldwide, the technologies it will incorporate and the capabilities it will possess."
MICHAEL SALINGER,"""Self-Preferencing"" - the global antitrust institute report on the digital economy",
MICHAEL SALINGER,Price gouging and the COVID-19 crisis – this time is (a little) different,
MICHAEL SALINGER,The complicated simple economics of vertical mergers,"A common justification that economists have historically given for why competition authorities should generally tolerate vertical mergers is the successive monopoly model, in which a vertical merger results in a price reduction by eliminating double marginalization (EDM). That model does not include any rivals to one of the merging firms, so it assumes away both the possibility that a vertical merger can result in raising rivals’ costs (RRC) and or vertical upward pricing pressure. I extend the successive/complementary model to allow for differentiated duopoly in the sale of the final good. This structure is one of the two simplest possible settings to allow for EDM, RRC, and vertical upward pricing pressure (the other being duopoly upstream and monopoly downstream). Since this market structure leaves the competing downstream firm with no independent source of supply, it would seem to be the one most likely to give rise to anticompetitive pricing incentives. The model reveals, however, an additional competitive effect. Eliminating double marginalization not only removes a pricing distortion for the merging firm, but it can also increase competitive pressure on the rival and its input supplier (even if the merging firm is the input supplier). I consider a variety of functional forms for demand and allow for the stages to be either successive or complementary. RRC and an increase in one of the two consumer prices occurs in some cases, but the price the merged firm charges its downstream competitor does not increase (and, indeed, drops) in a surprisingly broad set of cases. The results suggest that any prediction of a price increase due to a vertical merger based on static pricing incentives will be sensitive to assumptions about the functional form of demand and the timing of decisions that may be hard to verify."
MICHAEL SALINGER,The new vertical merger guidelines: muddying the waters,"The new Department of Justice and Federal Trade Commission Vertical Merger Guidelines focus attention on how vertical mergers are likely to affect static pricing incentives. In contrast, the section on vertical mergers in the Department of Justice’s 1984 Merger Guidelines, which the new Guidelines replace, place more emphasis on potential competition as a rationale for blocking vertical mergers. Even allowing for the possibility of raising rivals’ costs (which the successive monopoly model ignores), economic theory predicts that vertical mergers can provide incentives to lower all prices. Because of RRC, price increases are another possible consequence of a vertical merger, but which of the possible outcomes occurs depends on details that are likely to be difficult to measure. Potential competition between firms remains a more compelling rationale for blocking vertical mergers."
MICHAEL SALINGER,Universal fluctuations in growth dynamics of economic systems,"The growth of business firms is an example of a system of complex interacting units that resembles complex interacting systems in nature such as earthquakes. Remarkably, work in econophysics has provided evidence that the statistical properties of the growth of business firms follow the same sorts of power laws that characterize physical systems near their critical points. Given how economies change over time, whether these statistical properties are persistent, robust, and universal like those of physical systems remains an open question. Here, we show that the scaling properties of firm growth previously demonstrated for publicly-traded U.S. manufacturing firms from 1974 to 1993 apply to the same sorts of firms from 1993 to 2015, to firms in other broad sectors (such as materials), and to firms in new sectors ( such as Internet services). We measure virtually the same scaling exponent for manufacturing for the 1993 to 2015 period as for the 1974 to 1993 period and virtually the same scaling exponent for other sectors as for manufacturing. Furthermore, we show that fluctuations of the growth rate for new industries self-organize into a power-law over relatively short time scales."
MICHAEL SALINGER,Power law scaling for a system of interacting units with complex internal structure,We study the dynamics of a system composed of interacting units each with a complex internal structure comprising many subunits and treat the case in which each subunit grows in a multiplicative manner. We propose a model for such systems in which the interaction among the units is treated in a mean field approximation and the interaction among subunits is nonlinear. We test the model and find agreement between our predictions and empirical results based on a large economics database spanning 20 years.
MICHAEL SALINGER,Self-preferencing,"When markets at successive or complementary stages are imperfectly competitive, firms that operate at multiple stages typically have a strong incentive to purchase from, sell to, or otherwise coordinate with its own operations at adjacent stations compared with those of individual firms. But, to the extent that such self-preferencing eliminates double marginalization that would occur under vertical separation or otherwise facilitates coordination that is difficult to accomplish between firms, competition rules that discourage self-preferencing, such as restrictions on vertical mergers or integration, can harm consumers. In 2020, the United States Department of Justice and Federal Trade Commission issue revised vertical merger guidelines that promised enforcement that more nearly resembles how they review horizontal mergers – that is, by predicting effects on static pricing incentives. But the economics of how vertical mergers on pricing incentives is more complicated than the economics of how horizontal mergers affect pricing incentives. If U.S. enforcement is based on a consumer welfare standard, the U.S. agencies will struggle to find a robust methodology for distinguishing anticompetitive from procompetitive vertical mergers. Self-preferencing has also played a key role in allegations of anticompetitive behavior in technologically advancing industries. A review of past allegations of anticompetitive product innovation against IBM, Microsoft, and Google reveals the challenges in designing rules that limit self-preferencing without acting as a drag on innovation."
MICHAEL SALINGER,The 2023 Merger Guidelines and the role of economics,"Relying heavily on legal analysis, the 2023 Merger Guidelines argue for a fundamental shift in antitrust enforcement that places more emphasis on protecting competitors and less on protecting the beneficiaries of competition (in most cases, consumers). It is up to courts, not economists, to ascertain whether this interpretation of antitrust law is correct. But economists can and should analyze the likely economic effects. Evidence that antitrust enforcement has not prevented some markets from becoming overly concentrated justify the tightening of horizontal merger enforcement signaled by these guidelines. The treatment of vertical mergers ignores the economics of vertical mergers. These guidelines will further damage the reputation of the DOJ and FTC among competition policy enforcers in other countries."
MICHAEL SALINGER,Product offering complexity: implications for tying doctrine,"Firms with multiple products that they can sell either separately or in bundles cannot, as a practical matter, offer every possible combination. I analyze bundling and tying by a firm that sells two products that it can sell individually and/or bundled. To capture the limiting factor on the number of distinct products, I assume a fixed product offering cost, with the individual items and the bundle each being a distinct product. The primary competitive constraint on the firm is the threat of entry. Potential entrants can be as efficient (no entry barriers) or less efficient (some entry barriers) than the incumbent. The model also allows for a potential entrant that is more efficient in the production of one of the two products. Pure bundling, which entails tying, can occur for two distinct reasons. One (exemplified by selling shoes in pairs) is that most people want both items and there are efficiencies in selling them as a bundle. The other (exemplified by newspapers) is that when the fixed cost of a product offering is high, a bundled product can meet the needs of a diverse set of customer preferences with a single offering. Mixed bundling permits each customer to buy exactly what she wants, but possibly at high prices. While pure bundling can appear to “force” customers to buy an item they do not want, the threat of entry by a firm selling individual goods restricts the price. In some cases, pure bundling makes all consumers better off. In others, customers who want just one of the items might pay more than they would if the seller offered the items separately; but those who want both items pay less, and aggregate consumer surplus is higher."
MICHAEL SALINGER,A simultaneous moves approach to the simple complicated economics of vertical mergers,"We use a simultaneous move approach to modeling a vertical merger between a monopolist over an input with one of two competing downstream duopolists for general demand curves. As with a successive stages model, one can decompose the changes in the merged firm’s two prices (the price of its final good and the price it charges its competitor for the input) into a change in the stand-alone profit-maximizing price and a cross-product effect that is a form of vertical pricing pressure. The model provides support for predicting downstream pricing effects with the difference between the elimination of double marginalization and downstream vertical pricing pressure (which we refer to as “Net Downstream Pricing Pressure” or “NDPP”). It also reveals why standard measures of upstream vertical pricing pressure overstate the incentive to raise rivals’ costs. We simulate the model for a wide set of parameters for four different functional forms of demand. Compared with the successive stages model, the fraction of cases that give rise to an increase in the Fisher Index is much smaller. In the simulations, the “NDPP” statistic is a strong predictor of the change in the Fisher Index."
JOHN THORNTON,In search of the 1619 African arrivals,
JOHN THORNTON,Butterfly genome reveals promiscuous exchange of mimicry adaptations among species,"The evolutionary importance of hybridization and introgression has long been debated1. Hybrids are usually rare and unfit, but even infrequent hybridization can aid adaptation by transferring beneficial traits between species. Here we use genomic tools to investigate introgression in Heliconius, a rapidly radiating genus of neotropical butterflies widely used in studies of ecology, behaviour, mimicry and speciation2,3,4,5. We sequenced the genome of Heliconius melpomene and compared it with other taxa to investigate chromosomal evolution in Lepidoptera and gene flow among multiple Heliconius species and races. Among 12,669 predicted genes, biologically important expansions of families of chemosensory and Hox genes are particularly noteworthy. Chromosomal organization has remained broadly conserved since the Cretaceous period, when butterflies split from the Bombyx (silkmoth) lineage. Using genomic resequencing, we show hybrid exchange of genes between three co-mimics, Heliconius melpomene, Heliconius timareta and Heliconius elevatus, especially at two genomic regions that control mimicry pattern. We infer that closely related Heliconius species exchange protective colour-pattern genes promiscuously, implying that hybridization has an important role in adaptive radiation."
JOHN THORNTON,Regional interests in U.S. tariff policy - The East North Central case,
JOHN THORNTON,"Amusements, Games, and Sports Mentioned in Six Novels of Jane Austen",
JOHN THORNTON,Wage inflation and union-management relations,
JOHN THORNTON,The Florentine relation: a newly discovered sixteenth-century description of the Kingdom of Kongo,"A newly discovered Spanish description of Kongo increases our knowledge of the country, joining Filippo Pigafetta’s famous account to expand our knowledge of Kongo in this early period. This research shows that the MS was written in 1587 or 1588 and was written by Carmelite Diego de la Encarnación. It adds details on the history of the country, daily life culture, and links to other Carmelite works, including an unpublished chapter of an account by Diego de Santissimo Santo and shows the author could have written a longer but well-known account in the Vatican Library. It includes extensive quotations from the new text."
JOHN THORNTON,Mwene Muji: a medieval empire in Central Africa?,"Although the Lower Kasai was identified by Jan Vansina as a likely center for highly complex societies, he failed to recognize that sixteenth-century sources had mentioned the Empire of Mwene Muji as a large polity in that region. Studying the well known and recently discovered literature on West Central Africa, as well as a critical study of oral tradition, shows considerable evidence for the antiquity and existence of Mwene Muji."
JOHN THORNTON,High-operating-temperature direct ink writing of mesoscale eutectic architectures,"High-operating-temperature direct ink writing (HOT-DIW) of mesoscale architectures that are composed of eutectic silver chloride–potassium chloride. The molten ink undergoes directional solidification upon printing on a cold substrate. The lamellar spacing of the printed features can be varied between approximately 100 nm and 2 µm, enabling the manipulation of light in the visible and infrared range."
LAURENCE KOTLIKOFF,Did the 2017 tax reform discriminate against blue-state voters?,"The Tax Cuts and Jobs Act of 2017 (TCJA) significantly changed federal income taxation, including limiting SALT (state and local property, income, and sales taxes) deductibility to $10,000. We estimate the TCJA’s differential effect on red- and blue-state taxpayers and the SALT limitation’s contribution to this differential. We find an average increase in remaining lifetime spending of 1.6 percent in red states versus 1.3 percent in blue states. Among the richest 10 percent of households, red states enjoyed a 2 percent increase compared to 1.2 percent in blue states, with the gap driven almost entirely by the SALT deduction limitation."
LAURENCE KOTLIKOFF,Did the 2017 tax reform discriminate against blue state voters?,"The Tax Cut and Jobs Act of 2017 (TCJA) made significant changes to corporate and personal federal income taxation, including limiting the SALT (state and local property, income and sales taxes) deductibility to $10,000. States with high SALT tend to vote Democratic. This paper estimates the differential effect of the TCJA on red- and blue-state taxpayers and investigates the importance of the SALT limitation to this differential. We calculate the effect of permanent implementation of the TCJA on households using The Fiscal Analyzer: a life-cycle, consumption-smoothing program incorporating all major federal and state fiscal policies. We find that the average percentage increase in remaining lifetime spending under the TCJA is 1.6 percent in red states versus 1.3 percent in blue states. Among the richest 10 percent of households, this differential is larger. Rich households in red states enjoyed a 2.0 percent increase compared to a 1.2 percent increase among the rich in blue-state households. This gap is driven almost entirely by the limitation on the SALT deduction. Excluding the SALT limitation from the TCJA results in a spending gain of 2.6 percent for rich red-state households compared to 2.7 percent for rich blue-state households."
LAURENCE KOTLIKOFF,Simulating Russia’s and other large economies’ challenging and interconnected transitions,"This paper develops a large-scale, dynamic life-cycle model to simulate Russia’s demographic and fiscal transition under favorable and unfavorable fossil-fuel price regimes. The model includes Russia, the U.S., China, India, the EU, and Japan+ (Japan plus Korea). The model predicts dramatic increases in tax rates in the U.S., EU, India, and Russia. Indeed, the increases are so large as to question their political feasibility let alone their actual collection given the potential for tax avoidance and tax evasion."
LAURENCE KOTLIKOFF,Assessing fiscal sustainability,"Every country faces an intertemporal budget constraint, which requires that its government's future expenditures, including servicing its outstanding official debt, be covered by its government's future receipts when measured in present value. The present value difference between a country's future expenditures and its future receipts is its fiscal gap. The US fiscal gap now stands at $205 trillion. This is 10.3 percent of the estimated present value of all future US GDP. The United States needs to raise taxes, cut spending, or engage in a combination of these policies by an amount equal to 10.3 percent of annual GDP to close its fiscal gap. Closing the gap via raising taxes would require an immediate and permanent 57 percent increase in all federal taxes. Closing the gap via spending cuts (apart from servicing official debt) would require an immediate and permanent 37 percent reduction in spending. This grave picture of America's fiscal position effectively constitutes a declaration of bankruptcy."
LAURENCE KOTLIKOFF,Leveraging posterity’s prosperity?,"We critically review studies by Blanchard (B) and Rachel and Summers (RS). By the standard fiscal-gap measure, the US government is in dire fiscal shape thanks to constantly enlarging its postwar, take-as-you-go Ponzi scheme. Yet B and RS seemingly rationalize its expansion. Their arguments rest on the safe rate being very low. But almost all households face high safe rates--the rates available from pre-paying their loans. We also question modeling assumptions that help drive key B and RS results and reference recent simulation studies, which reach strongly opposite conclusions to B's."
LAURENCE KOTLIKOFF,Making carbon taxation a generational win win,"Carbon taxation is mostly studied in social planner or inﬁnitely lived-agent models, which obscure carbon taxation’s potential to produce a generational win win. This article’s large-scale, dynamic 55-period, overlap- ping generations model calculates the carbon tax policy delivering the highest uniform welfare gain to all current and future generations. Our model features coal, oil, and gas, increasing extraction costs, clean energy, technical and demographic change, and Nordhaus’ carbon/temperature/damage functions. Assuming high-end carbon damages, the optimal carbon tax is $70, rising annually at 1.5%. This policy raises all generations’ welfare by almost 5%. However, doing so requires major intergenerational redistribution."
LAURENCE KOTLIKOFF,Valuing government obligations when markets are incomplete,"This paper posits and simulates a ten-period overlapping generations model with aggregate shocks to price safe and risky government obligations using consumption-asset pricing. Agents can’t trade with future generations to hedge the model’s productivity and depreciation shocks, and can only invest in one-period bonds and risky capital. We find that the pricing of short- and long-dated riskless obligations is anchored to the prevailing one-period risk-free return. The prices of obligations whose values are proportional to the prevailing wage are essentially identical to those of safe obligations, notwithstanding large macro shocks. On the contrary, government obligations provided in the form of options entail significant risk adjustment. We also show that the value of obligations to unborn generations depends on the nature of the compensating variation. Our model suggests the potential of CGE OLG models to price government obligations and non-marketed private securities in the presence of incomplete markets and macro shocks."
LAURENCE KOTLIKOFF,Banks as potentially crooked secret keepers,
LAURENCE KOTLIKOFF,The true cost of Social Security,"Implicit government obligations represent the lion's share of government liabilities in the U.S. and many other countries. Yet these liabilities are rarely measured let alone properly adjusted for their risk. This paper shows, by example, how modern asset pricing can be used to value implicit fiscal debts taking into account their risk properties. The example is the U.S. Social Security System's net liability to working-age Americans. Marking this debt to market makes a big difference. Based on our preferred estimate, its market value is 86 percent higher than the Social Security trustees' valuation method suggests. Our alternative APT specifications range from 74 to 115 percent higher than that of the Social Security Administration."
LAURENCE KOTLIKOFF,Closing America's enormous fiscal gap: who will pay?,"The US government has spent decades taxing current generations while also writing them huge IOUs for future benefits. This paper models the effect on everyday Americans of closing the true $210 trillion fiscal gap with an immediate and permanent 57 percent increase in all federal taxes or with a delayed increase of 69 percent. We examine the impact either method of smoothly closing the fiscal gap would have on five stylized households in three different cohorts. Raising the federal tax rate on all households, at all age and resource levels, increases each family's lifetime tax rate and decreases lifetime spending. The results show that delaying the tax increase lowers the burden on those now alive, particularly the elderly. Meanwhile, the burden on those left to pick up the tab grows larger with each passing year of congressional and presidential inaction."
THOMAS D TULLIUS,GBshape: a genome browser database for DNA shape annotations,"Many regulatory mechanisms require a high degree of specificity in protein-DNA binding. Nucleotide sequence does not provide an answer to the question of why a protein binds only to a small subset of the many putative binding sites in the genome that share the same core motif. Whereas higher-order effects, such as chromatin accessibility, cooperativity and cofactors, have been described, DNA shape recently gained attention as another feature that fine-tunes the DNA binding specificities of some transcription factor families. Our Genome Browser for DNA shape annotations (GBshape; freely available at http://rohslab.cmb.usc.edu/GBshape) provides minor groove width, propeller twist, roll, helix twist and hydroxyl radical cleavage predictions for the entire genomes of 94 organisms. Additional genomes can easily be added using the GBshape framework. GBshape can be used to visualize DNA shape annotations qualitatively in a genome browser track format, and to download quantitative values of DNA shape features as a function of genomic position at nucleotide resolution. As biological applications, we illustrate the periodicity of DNA shape features that are present in nucleosome-occupied sequences from human, fly and worm, and we demonstrate structural similarities between transcription start sites in the genomes of four Drosophila species."
THOMAS D TULLIUS,GBshape: a genome browser database for DNA shape annotations,"Many regulatory mechanisms require a high degree of specificity in protein-DNA binding. Nucleotide sequence does not provide an answer to the question of why a protein binds only to a small subset of the many putative binding sites in the genome that share the same core motif. Whereas higher-order effects, such as chromatin accessibility, cooperativity and cofactors, have been described, DNA shape recently gained attention as another feature that fine-tunes the DNA binding specificities of some transcription factor families. Our Genome Browser for DNA shape annotations (GBshape; freely available at http://rohslab.cmb.usc.edu/GBshape) provides minor groove width, propeller twist, roll, helix twist and hydroxyl radical cleavage predictions for the entire genomes of 94 organisms. Additional genomes can easily be added using the GBshape framework. GBshape can be used to visualize DNA shape annotations qualitatively in a genome browser track format, and to download quantitative values of DNA shape features as a function of genomic position at nucleotide resolution. As biological applications, we illustrate the periodicity of DNA shape features that are present in nucleosome-occupied sequences from human, fly and worm, and we demonstrate structural similarities between transcription start sites in the genomes of four Drosophila species."
THOMAS D TULLIUS,Experimental maps of DNA structure at nucleotide resolution distinguish intrinsic from protein-induced DNA deformations,"Recognition of DNA by proteins depends on DNA sequence and structure. Often unanswered is whether the structure of naked DNA persists in a protein–DNA complex, or whether protein binding changes DNA shape. While X-ray structures of protein–DNA complexes are numerous, the structure of naked cognate DNA is seldom available experimentally. We present here an experimental and computational analysis pipeline that uses hydroxyl radical cleavage to map, at single-nucleotide resolution, DNA minor groove width, a recognition feature widely exploited by proteins. For 11 protein–DNA complexes, we compared experimental maps of naked DNA minor groove width with minor groove width measured from X-ray co-crystal structures. Seven sites had similar minor groove widths as naked DNA and when bound to protein. For four sites, part of the DNA in the complex had the same structure as naked DNA, and part changed structure upon protein binding. We compared the experimental map with minor groove patterns of DNA predicted by two computational approaches, DNAshape and ORChID2, and found good but not perfect concordance with both. This experimental approach will be useful in mapping structures of DNA sequences for which high-resolution structural data are unavailable. This approach allows probing of protein family-dependent readout mechanisms."
THOMAS D TULLIUS,Chemical probing of RNA with the hydroxyl radical at single-atom resolution,"While hydroxyl radical cleavage is widely used to map RNA tertiary structure, lack of mechanistic understanding of strand break formation limits the degree of structural insight that can be obtained from this experiment. Here, we determine how individual ribose hydrogens of sarcin/ricin loop RNA participate in strand cleavage. We find that substituting deuterium for hydrogen at a ribose 5′-carbon produces a kinetic isotope effect on cleavage; the major cleavage product is an RNA strand terminated by a 5′-aldehyde. We conclude that hydroxyl radical abstracts a 5′-hydrogen atom, leading to RNA strand cleavage. We used this approach to obtain structural information for a GUA base triple, a common tertiary structural feature of RNA. Cleavage at U exhibits a large 5′ deuterium kinetic isotope effect, a potential signature of a base triple. Others had noted a ribose-phosphate hydrogen bond involving the G 2′-OH and the U phosphate of the GUA triple, and suggested that this hydrogen bond contributes to backbone rigidity. Substituting deoxyguanosine for G, to eliminate this hydrogen bond, results in a substantial decrease in cleavage at G and U of the triple. We conclude that this hydrogen bond is a linchpin of backbone structure around the triple."
THOMAS D TULLIUS,Chemical probing of RNA with the hydroxyl radical at single-atom resolution,"While hydroxyl radical cleavage is widely used to map RNA tertiary structure, lack of mechanistic understanding of strand break formation limits the degree of structural insight that can be obtained from this experiment. Here, we determine how individual ribose hydrogens of sarcin/ricin loop RNA participate in strand cleavage. We find that substituting deuterium for hydrogen at a ribose 5′-carbon produces a kinetic isotope effect on cleavage; the major cleavage product is an RNA strand terminated by a 5′-aldehyde. We conclude that hydroxyl radical abstracts a 5′-hydrogen atom, leading to RNA strand cleavage. We used this approach to obtain structural information for a GUA base triple, a common tertiary structural feature of RNA. Cleavage at U exhibits a large 5′ deuterium kinetic isotope effect, a potential signature of a base triple. Others had noted a ribose-phosphate hydrogen bond involving the G 2′-OH and the U phosphate of the GUA triple, and suggested that this hydrogen bond contributes to backbone rigidity. Substituting deoxyguanosine for G, to eliminate this hydrogen bond, results in a substantial decrease in cleavage at G and U of the triple. We conclude that this hydrogen bond is a linchpin of backbone structure around the triple."
FRANCOIS BROCHET,Say on pay laws and insider trading,"We examine whether mandatory adoption of say-on-pay increases executives’ incentives to engage in insider trading to offset the regulatory-induced increase in compensation risk. Our empirical design exploits the staggered adoption of say-on-pay laws across fourteen countries over the 2000-2015 period. We find that mandatory adoption of say-on-pay is associated with a material increase in insider trading profitability, especially in firms with excess pay and weaker governance. The increase in insider trading profits is mostly driven by more frequent and larger insider sales, consistent with executives’ desire to reduce their exposure to firm-specific risk and rebalance their portfolio. We also find some evidence that after the adoption of say-on-pay insider sales become more predictive of future returns and are more likely timed during information-sensitive windows. Overall, our results highlight the importance of considering potential effects on insider trading incentives when designing compensation reforms and when assessing their impact on executives’ incentives."
FRANCOIS BROCHET,Virtual shareholder meetings,
JAMES TRANIELLO,Origins of aminergic regulation of behavior in complex insect social systems,"Neuromodulators are conserved across insect taxa, but how biogenic amines and their receptors in ancestral solitary forms have been co-opted to control behaviors in derived socially complex species is largely unknown. Here we explore patterns associated with the functions of octopamine (OA), serotonin (5-HT) and dopamine (DA) in solitary ancestral insects and their derived functions in eusocial ants, bees, wasps and termites. Synthesizing current findings that reveal potential ancestral roles of monoamines in insects, we identify physiological processes and conserved behaviors under aminergic control, consider how biogenic amines may have evolved to modulate complex social behavior, and present focal research areas that warrant further study."
JAMES TRANIELLO,Group-wise 3D registration based templates to study the evolution of ant worker neuroanatomy,"The evolutionary success of ants and other social insects is considered to be intrinsically linked to division of labor and emergent collective intelligence. The role of the brains of individual ants in generating these processes, however, is poorly understood. One genus of ant of special interest is Pheidole, which includes more than a thousand species, most of which are dimorphic, i.e. their colonies contain two subcastes of workers: minors and majors. Using confocal imaging and manual annotations, it has been demonstrated that minor and major workers of different ages of three species of Pheidole have distinct patterns of brain size and subregion scaling. However, these studies require laborious effort to quantify brain region volumes and are subject to potential bias. To address these issues, we propose a group-wise 3D registration approach to build for the first time bias-free brain atlases of intra- and inter-subcaste individuals and automatize the segmentation of new individuals."
JAMES TRANIELLO,White paper: an integrated perspective on the causes of hypometric metabolic scaling in animals,"Larger animals studied during ontogeny, across populations, or across species, usually have lower mass-specific metabolic rates than smaller animals (hypometric scaling). This pattern is usually observed regardless of physiological state (e.g., basal, resting, field, and maximally active). The scaling of metabolism is usually highly correlated with the scaling of many life-history traits, behaviors, physiological variables, and cellular/molecular properties, making determination of the causation of this pattern challenging. For across-species comparisons of resting and locomoting animals (but less so for across populations or during ontogeny), the mechanisms at the physiological and cellular level are becoming clear. Lower mass-specific metabolic rates of larger species at rest are due to (a) lower contents of expensive tissues (brains, liver, and kidneys), and (b) slower ion leak across membranes at least partially due to membrane composition, with lower ion pump ATPase activities. Lower mass-specific costs of larger species during locomotion are due to lower costs for lower-frequency muscle activity, with slower myosin and Ca++ ATPase activities, and likely more elastic energy storage. The evolutionary explanation(s) for hypometric scaling remain(s) highly controversial. One subset of evolutionary hypotheses relies on constraints on larger animals due to changes in geometry with size; for example, lower surface-to-volume ratios of exchange surfaces may constrain nutrient or heat exchange, or lower cross-sectional areas of muscles and tendons relative to body mass ratios would make larger animals more fragile without compensation. Another subset of hypotheses suggests that hypometric scaling arises from biotic interactions and correlated selection, with larger animals experiencing less selection for mass-specific growth or neurolocomotor performance. An additional third type of explanation comes from population genetics. Larger animals with their lower effective population sizes and subsequent less effective selection relative to drift may have more deleterious mutations, reducing maximal performance and metabolic rates. Resolving the evolutionary explanation for the hypometric scaling of metabolism and associated variables is a major challenge for organismal and evolutionary biology. To aid progress, we identify some variation in terminology use that has impeded cross-field conversations on scaling. We also suggest that promising directions for the field to move forward include (1) studies examining the linkages between ontogenetic, population-level, and cross-species allometries; (2) studies linking scaling to ecological or phylogenetic context; (3) studies that consider multiple, possibly interacting hypotheses; and (4) obtaining better field data for metabolic rates and the life history correlates of metabolic rate such as lifespan, growth rate, and reproduction."
JAMES TRANIELLO,Species Diversity and Distribution Patterns of the Ants of Amazonian Ecuador,"Ants are among the most diverse, abundant and ecologically significant organisms on earth. Although their species richness appears to be greatest in the New World tropics, global patterns of ant diversity and distribution are not well understood. We comprehensively surveyed ant diversity in a lowland primary rainforest in Western Amazonia, Ecuador using canopy fogging, pitfall traps, baits, hand collecting, mini-Winkler devices and subterranean probes to sample ants. A total of 489 ant species comprising 64 genera in nine subfamilies were identified from samples collected in only 0.16 square kilometers. The most species-rich genera were Camponotus, Pheidole, Pseudomyrmex, Pachycondyla, Brachymyrmex, and Crematogaster. Camponotus and Pseudomyrmex were most diverse in the canopy, while Pheidole was most diverse on the ground. The three most abundant ground-dwelling ant genera were Pheidole, Solenopsis and Pyramica. Crematogaster carinata was the most abundant ant species in the canopy; Wasmannia auropunctata was most abundant on the ground, and the army ant Labidus coecus was the most abundant subterranean species. Ant species composition among strata was significantly different: 80% of species were found in only one stratum, 17% in two strata, and 3% in all three strata. Elevation and the number of logs and twigs available as nest sites were significant predictors of ground-dwelling ant species richness. Canopy species richness was not correlated with any ecological variable measured. Subterranean species richness was negatively correlated with depth in the soil. When ant species were categorized using a functional group matrix based on diet, nest-site preference and foraging ecology, the greatest diversity was found in Omnivorous Canopy Nesters. Our study indicates ant species richness is exceptionally high at Tiputini. We project 647-736 ant species in this global hotspot of biodiversity. Considering the relatively small area surveyed, this region of western Amazonia appears to support the most diverse ant fauna yet recorded."
JAMES TRANIELLO,"Social brain energetics: ergonomic efficiency, neurometabolic scaling, and metabolic polyphenism in ants","Metabolism, a metric of the energy cost of behavior, plays a significant role in social evolution. Body size and metabolic scaling are coupled, and a socioecological pattern of increased body size is associated with dietary change and the formation of larger and more complex groups. These consequences of the adaptive radiation of animal societies beg questions concerning energy expenses, a substantial portion of which may involve the metabolic rates of brains that process social information. Brain size scales with body size, but little is understood about brain metabolic scaling. Social insects such as ants show wide variation in worker body size and morphology that correlates with brain size, structure, and worker task performance, which is dependent on sensory inputs and information-processing ability to generate behavior. Elevated production and maintenance costs in workers may impose energetic constraints on body size and brain size that are reflected in patterns of metabolic scaling. Models of brain evolution do not clearly predict patterns of brain metabolic scaling, nor do they specify its relationship to task performance and worker ergonomic efficiency, two key elements of social evolution in ants. Brain metabolic rate is rarely recorded and, therefore, the conditions under which brain metabolism influences the evolution of brain size are unclear. We propose that studies of morphological evolution, colony social organization, and worker ergonomic efficiency should be integrated with analyses of species-specific patterns of brain metabolic scaling to advance our understanding of brain evolution in ants."
JAMES TRANIELLO,"Nesting ecology and cuticular microbial loads in dampwood (Zootermopsis Angusticollis) and drywood termites (Incisitermes Minor, I. Schwarzi, Cryptotermes Cavifrons)","Termites form one-piece nests in wood that can vary in their moisture content and degree of decomposition, and thus microbial richness. To estimate the microbial load of nests and the potential risk they pose for colony members, we quantified the number of microbes in the nest and on the cuticle of the dampwood termite, Zootermopsis angusticollis, and three drywood termites, Incisitermes minor, I. schwarzi, and Cryptotermes cavifrons. The number of colony forming units (CFUs) cultured from nest material samples and washes of the cuticle of larvae and nymphs were determined. CFUs recorded from nest material was low (fewer than 60 CFUs/g) in the drywood termites and comparatively high in the dampwood species, as more than 800 bacterial and fungal CFUs/g were cultured from the nest material of Z. angusticollis. Similarly, cuticular microbial loads were negligible in the drywood termites sampled, ranging from 0.5 to fewer than 16 CFUs/cm2, whereas approximately 200 CFUs/cm2 were cultured from Z. angusticollis. The nesting and feeding habits of these basal species likely influence colony microbial load and the degree of pathogen exposure, which in turn could favor adaptations to resist disease that vary with termite nesting biology."
ROBERT W HEFNER,Islam and institutional religious freedom in Indonesia,"By emphasizing that individual religious freedom depends for its realization on complex social embeddings, the concept of institutional religious freedom provides an important corrective to conventional, individualistic approaches to religious freedom. The concept also helpfully complicates the investigation of religious freedom by encouraging analysts to recognize that different societal and civilizational traditions define religion itself in significantly different ways. Tensions such as these between different social definitions of religion and between different manifestations of institutional religious freedom have been a chronic feature of religious life in Indonesia since the establishment of the republic in 1945. This paper examines these legacies in the context of contemporary Indonesia, especially in light of ongoing disputes over the legal and ethical status of spiritual traditions (kepercayaan) long barred from full state recognition. The essay also explores the theoretical and policy implications of the Indonesian example for the analysis of institutional religious freedom in the late modern world as a whole."
STEPHANIE L LEE,Imaging X-ray polarimetry explorer: prelaunch,"Launched on 2021 December 9, the Imaging X-ray Polarimetry Explorer (IXPE) is a NASA Small Explorer Mission in collaboration with the Italian Space Agency (ASI). The mission will open a new window of investigation—imaging x-ray polarimetry. The observatory features three identical telescopes, each consisting of a mirror module assembly with a polarization-sensitive imaging x-ray detector at the focus. A coilable boom, deployed on orbit, provides the necessary 4-m focal length. The observatory utilizes a three-axis-stabilized spacecraft, which provides services such as power, attitude determination and control, commanding, and telemetry to the ground. During its 2-year baseline mission, IXPE will conduct precise polarimetry for samples of multiple categories of x-ray sources, with follow-on observations of selected targets."
LINDSAY A FARRER,Nonsteroidal Anti-Inflammatory Drug Use and Alzheimer's Disease Risk: The MIRAGE Study,"BACKGROUND: Nonsteroidal anti-inflammatory drugs (NSAID) use may protect against Alzheimer's disease (AD) risk. We sought examine the association between NSAID use and risk of AD, and potential effect modification by APOE-ε4 carrier status and ethnicity. METHODS: The MIRAGE Study is a multi-center family study of genetic and environmental risk factors for AD. Subjects comprised 691 AD patients (probands) and 973 family members enrolled at 15 research centers between 1996 and 2002. The primary independent and dependent variables were prior NSAID use and AD case status, respectively. We stratified the dataset in order to evaluate whether the association between NSAID use and AD was similar in APOE-ε4 carriers and non-carriers. Ethnicity was similarly examined as an effect modifier. RESULTS: NSAID use was less frequent in cases compared to controls in the overall sample (adjusted OR = 0.64; 95% CI = 0.38–1.05). The benefit of NSAID use appeared more pronounced among APOE-ε4 carriers (adjusted OR = 0.49; 95% CI = 0.24–0.98) compared to non-carriers, although this association was not statistically significant. The pattern of association was similar in Caucasian and African Americans. CONCLUSIONS: NSAID use is inversely associated with AD and may be modified by APOE genotype. Prospective studies and clinical trials of sufficient power to detect effect modification by APOE-ε4 carrier status are needed."
LINDSAY A FARRER,Genetic mModifiers of Hb E/β0 Thalassemia Identified by a Two-Stage Genome-Wide Association Study,"BACKGROUND: Patients with Hb E/β0 thalassemia display remarkable variability in disease severity. To identify genetic modifiers influencing disease severity, we conducted a two-stage genome scan in groups of 207 mild and 305 severe unrelated patients from Thailand with Hb E/β0 thalassemia and normal α-globin genes. METHODS: First, we estimated and compared the allele frequencies of approximately 110,000 gene-based single nucleotide polymorphisms (SNPs) in pooled DNAs from different severity groups. The 756 SNPs that showed reproducible allelic differences at P < 0.02 by pooling were selected for individual genotyping. RESULTS: After adjustment for age, gender and geographic region, logistic regression models showed 50 SNPs significantly associated with disease severity (P < 0.05) after Bonferroni adjustment for multiple testing. Forty-one SNPs in a large LD block within the β-globin gene cluster had major alleles associated with severe disease. The most significant was bthal_bg200 (odds ratio (OR) = 5.56, P = 2.6 × 10-13). Seven SNPs in two distinct LD blocks within a region centromeric to the β-globin gene cluster that contains many olfactory receptor genes were also associated with disease severity; rs3886223 had the strongest association (OR = 3.03, P = 3.7 × 10-11). Several previously unreported SNPs were also significantly associated with disease severity. CONCLUSIONS: These results suggest that there may be an additional regulatory region centromeric to the β-globin gene cluster that affects disease severity by modulating fetal hemoglobin expression."
LINDSAY A FARRER,One for all and all for one: improving replication of genetic studies through network diffusion,"Improving accuracy in genetic studies would greatly accelerate understanding the genetic basis of complex diseases. One approach to achieve such an improvement for risk variants identified by the genome wide association study (GWAS) approach is to incorporate previously known biology when screening variants across the genome. We developed a simple approach for improving the prioritization of candidate disease genes that incorporates a network diffusion of scores from known disease genes using a protein network and a novel integration with GWAS risk scores, and tested this approach on a large Alzheimer disease (AD) GWAS dataset. Using a statistical bootstrap approach, we cross-validated the method and for the first time showed that a network approach improves the expected replication rates in GWAS studies. Several novel AD genes were predicted including CR2, SHARPIN, and PTPN2. Our re-prioritized results are enriched for established known AD-associated biological pathways including inflammation, immune response, and metabolism, whereas standard non-prioritized results were not. Our findings support a strategy of considering network information when investigating genetic risk factors."
LINDSAY A FARRER,Genome-Wide Linkage Analysis for Alcohol Dependence: A Comparison between Single-Nucleotide Polymorphism and Microsatellite Marker Assays,"Both theoretical and applied studies have proven that the utility of single nucleotide polymorphism (SNP) markers in linkage analysis is more powerful and cost-effective than current microsatellite marker assays. Here we performed a whole-genome scan on 115 White, non-Hispanic families segregating for alcohol dependence, using one 10.3-cM microsatellite marker set and two SNP data sets (0.33-cM, 0.78-cM spacing). Two definitions of alcohol dependence (ALDX1 and ALDX2) were used. Our multipoint nonparametric linkage analysis found alcoholism was nominal linked to 12 genomic regions. The linkage peaks obtained by using the microsatellite marker set and the two SNP sets had a high degree of correspondence in general, but the microsatellite marker set was insufficient to detect some nominal linkage peaks. The presence of linkage disequilibrium between markers did not significantly affect the results. Across the entire genome, SNP datasets had a much higher average linkage information content (0.33 cM: 0.93, 0.78 cM: 0.91) than did microsatellite marker set (0.57). The linkage peaks obtained through two SNP datasets were very similar with some minor differences. We conclude that genome-wide linkage analysis by using approximately 5,000 SNP markers evenly distributed across the human genome is sufficient and might be more powerful than current 10-cM microsatellite marker assays."
LINDSAY A FARRER,"Lack of Association between Angiotensin-Converting Enzyme and Dementia of the Alzheimer's Type in an Elderly Arab Population in Wadi Ara, Israel","The angiotensin-converting enzyme (ACE), a protease involved in blood pressure regulation, has been implicated as an important candidate gene for Alzheimer's disease (AD). This study investigated whether the ACE gene insertion–deletion (ID) polymorphism is associated with risk of developing dementia of Alzheimer's type (DAT) in an Arab–Israeli community, a unique genetic isolate where there is a high prevalence of DAT. In contrast to several other studies, we found no evidence of an association between this polymorphism and either DAT or age-related cognitive decline (ARCD)."
LINDSAY A FARRER,Multifactor-Dimensionality Reduction Versus Family-Based Association Tests in Detecting Susceptibility Loci in Discordant Sib-Pair Studies,"Complex diseases are generally thought to be under the influence of multiple, and possibly interacting, genes. Many association methods have been developed to identify susceptibility genes assuming a single-gene disease model, referred to as single-locus methods. Multilocus methods consider joint effects of multiple genes and environmental factors. One commonly used method for family-based association analysis is implemented in FBAT. The multifactor-dimensionality reduction method (MDR) is a multilocus method, which identifies multiple genetic loci associated with the occurrence of complex disease. Many studies of late onset complex diseases employ a discordant sib pairs design. We compared the FBAT and MDR in their ability to detect susceptibility loci using a discordant sib-pair dataset generated from the simulated data made available to participants in the Genetic Analysis Workshop 14. Using FBAT, we were able to identify the effect of one susceptibility locus. However, the finding was not statistically significant. We were not able to detect any of the interactions using this method. This is probably because the FBAT test is designed to find loci with major effects, not interactions. Using MDR, the best result we obtained identified two interactions. However, neither of these reached a level of statistical significance. This is mainly due to the heterogeneity of the disease trait and noise in the data."
LINDSAY A FARRER,Whole-Genome Variance Components Linkage Analysis Using Single-Nucleotide Polymorphisms Versus Microsatellites on Quantitative Traits of Derived Phenotypes grom Factor Analysis of Electroencephalogram Waves,"Alcohol dependence is a serious public health problem. We studied data from families participating in the Collaborative Study on the Genetics of Alcoholism (COGA) and made available to participants in the Genetic Analysis Workshop 14 (GAW14) in order to search for genes predisposing to alcohol dependence. Using factor analysis, we identified four factors (F1, F2, F3, F4) related to the electroencephalogram traits. We conducted variance components linkage analysis with each of the factors. Our results using the Affymetrix single-nucleotide polymorphism dataset showed significant evidence for a novel linkage of F3 (factor comprised of the three midline channel EEG measures from the target case of the Visual Oddball experiment ttdt2, 3, 4) to chromosome 18 (LOD = 3.45). This finding was confirmed by analyses of the microsatellite data (LOD = 2.73) and Illumina SNP data (LOD = 3.30). We also demonstrated that, in a sample like the COGA data, a dense single-nucleotide polymorphism map provides better linkage signals than low-resolution microsatellite map with quantitative traits."
LINDSAY A FARRER,Empirically Derived Phenotypic Subgroups – Qualitative and Quantitative Trait Analyses,"BACKGROUND. The Framingham Heart Study has contributed a great deal to advances in medicine. Most of the phenotypes investigated have been univariate traits (quantitative or qualitative). The aims of this study are to derive multivariate traits by identifying homogeneous groups of people and assigning both qualitative and quantitative trait scores; to assess the heritability of the derived traits; and to conduct both qualitative and quantitative linkage analysis on one of the heritable traits. METHODS. Multiple correspondence analysis, a nonparametric analogue of principal components analysis, was used for data reduction. Two-stage clustering, using both k-means and agglomerative hierarchical clustering, was used to cluster individuals based upon axes (factor) scores obtained from the data reduction. Probability of cluster membership was calculated using binary logistic regression. Heritability was calculated using SOLAR, which was also used for the quantitative trait analysis. GENEHUNTER-PLUS was used for the qualitative trait analysis. RESULTS. We found four phenotypically distinct groups. Membership in the smallest group was heritable (38%, p < 1 × 10-6) and had characteristics consistent with atherogenic dyslipidemia. We found both qualitative and quantitative LOD scores above 3 on chromosomes 11 and 14 (11q13, 14q23, 14q31). There were two Kong & Cox LOD scores above 1.0 on chromosome 6 (6p21) and chromosome 11 (11q23). CONCLUSION. This approach may be useful for the identification of genetic heterogeneity in complex phenotypes by clarifying the phenotype definition prior to linkage analysis. Some of our findings are in regions linked to elements of atherogenic dyslipidemia and related diagnoses, some may be novel, or may be false positives."
LINDSAY A FARRER,Search for Genetic Factors Predisposing to Atherogenic Dyslipidemia,"BACKGROUND. Atherogenic dyslipidemia (AD) is a common feature in persons with premature coronary heart disease. While several linkage studies have been carried out to dissect the genetic etiology of lipid levels, few have investigated the AD lipid triad comprising elevated serum triglyceride, small low density lipoprotein (LDL) particles, and reduced high density lipoprotein (HDL) cholesterol levels. Here we report the results of a whole-genome screen for AD using the Framingham Heart Study population. RESULTS. Our analyses provide some evidence for linkage to AD on chromosomes 1q31, 3q29, 10q26, 14p12, 14q13, 16q24, 18p11, and 19q13. CONCLUSION. AD susceptibility is modulated by multiple genes in different chromosomes. Our study confirms results from other populations and suggests new areas of potential importance."
LINDSAY A FARRER,Performance of Random Forest When SNPs Are in Linkage Disequilibrium,"BACKGROUND. Single nucleotide polymorphisms (SNPs) may be correlated due to linkage disequilibrium (LD). Association studies look for both direct and indirect associations with disease loci. In a Random Forest (RF) analysis, correlation between a true risk SNP and SNPs in LD may lead to diminished variable importance for the true risk SNP. One approach to address this problem is to select SNPs in linkage equilibrium (LE) for analysis. Here, we explore alternative methods for dealing with SNPs in LD: change the tree-building algorithm by building each tree in an RF only with SNPs in LE, modify the importance measure (IM), and use haplotypes instead of SNPs to build a RF. RESULTS. We evaluated the performance of our alternative methods by simulation of a spectrum of complex genetics models. When a haplotype rather than an individual SNP is the risk factor, we find that the original Random Forest method performed on SNPs provides good performance. When individual, genotyped SNPs are the risk factors, we find that the stronger the genetic effect, the stronger the effect LD has on the performance of the original RF. A revised importance measure used with the original RF is relatively robust to LD among SNPs; this revised importance measure used with the revised RF is sometimes inflated. Overall, we find that the revised importance measure used with the original RF is the best choice when the genetic model and the number of SNPs in LD with risk SNPs are unknown. For the haplotype-based method, under a multiplicative heterogeneity model, we observed a decrease in the performance of RF with increasing LD among the SNPs in the haplotype. CONCLUSION. Our results suggest that by strategically revising the Random Forest method tree-building or importance measure calculation, power can increase when LD exists between SNPs. We conclude that the revised Random Forest method performed on SNPs offers an advantage of not requiring genotype phase, making it a viable tool for use in the context of thousands of SNPs, such as candidate gene studies and follow-up of top candidates from genome wide association studies."
LINDSAY A FARRER,Genome-Wide Screen for Heavy Alcohol Consumption,"BACKGROUND. To find specific genes predisposing to heavy alcohol consumption (self-reported consumption of 24 grams or more of alcohol per day among men and 12 grams or more among women), we studied 330 families collected by the Framingham Heart Study made available to participants in the Genetic Analysis Workshop 13 (GAW13). RESULTS. Parametric and nonparametric methods of linkage analysis were used. No significant evidence of linkage was found; however, weak signals were identified in several chromosomal regions, including 1p22, 4q12, 4q25, and 11q24, which are in the vicinity of those reported in other similar studies. CONCLUSION. Our study did not reveal significant evidence of linkage to heavy alcohol use; however, we found weak confirmation of studies carried out in other populations."
LINDSAY A FARRER,Bulk brain tissue cell-type deconvolution with bias correction for single-nuclei RNA sequencing data using DeTREM,"BACKGROUND: Quantifying cell-type abundance in bulk tissue RNA-sequencing enables researchers to better understand complex systems. Newer deconvolution methodologies, such as MuSiC, use cell-type signatures derived from single-cell RNA-sequencing (scRNA-seq) data to make these calculations. Single-nuclei RNA-sequencing (snRNA-seq) reference data can be used instead of scRNA-seq data for tissues such as human brain where single-cell data are difficult to obtain, but accuracy suffers due to sequencing differences between the technologies. RESULTS: We propose a modification to MuSiC entitled 'DeTREM' which compensates for sequencing differences between the cell-type signature and bulk RNA-seq datasets in order to better predict cell-type fractions. We show DeTREM to be more accurate than MuSiC in simulated and real human brain bulk RNA-sequencing datasets with various cell-type abundance estimates. We also compare DeTREM to SCDC and CIBERSORTx, two recent deconvolution methods that use scRNA-seq cell-type signatures. We find that they perform well in simulated data but produce less accurate results than DeTREM when used to deconvolute human brain data. CONCLUSION: DeTREM improves the deconvolution accuracy of MuSiC and outperforms other deconvolution methods when applied to snRNA-seq data. DeTREM enables accurate cell-type deconvolution in situations where scRNA-seq data are not available. This modification improves characterization cell-type specific effects in brain tissue and identification of cell-type abundance differences under various conditions."
NANCY KOPELL,Differential contributions of synaptic and intrinsic inhibitory currents to speech segmentation via flexible phase-locking in neural oscillators,"Current hypotheses suggest that speech segmentation-the initial division and grouping of the speech stream into candidate phrases, syllables, and phonemes for further linguistic processing-is executed by a hierarchy of oscillators in auditory cortex. Theta (∼3-12 Hz) rhythms play a key role by phase-locking to recurring acoustic features marking syllable boundaries. Reliable synchronization to quasi-rhythmic inputs, whose variable frequency can dip below cortical theta frequencies (down to ∼1 Hz), requires ""flexible"" theta oscillators whose underlying neuronal mechanisms remain unknown. Using biophysical computational models, we found that the flexibility of phase-locking in neural oscillators depended on the types of hyperpolarizing currents that paced them. Simulated cortical theta oscillators flexibly phase-locked to slow inputs when these inputs caused both (i) spiking and (ii) the subsequent buildup of outward current sufficient to delay further spiking until the next input. The greatest flexibility in phase-locking arose from a synergistic interaction between intrinsic currents that was not replicated by synaptic currents at similar timescales. Flexibility in phase-locking enabled improved entrainment to speech input, optimal at mid-vocalic channels, which in turn supported syllabic-timescale segmentation through identification of vocalic nuclei. Our results suggest that synaptic and intrinsic inhibition contribute to frequency-restricted and -flexible phase-locking in neural oscillators, respectively. Their differential deployment may enable neural oscillators to play diverse roles, from reliable internal clocking to adaptive segmentation of quasi-regular sensory inputs like speech."
NANCY KOPELL,Neurosystems: brain rhythms and cognitive processing,"Neuronal rhythms are ubiquitous features of brain dynamics, and are highly correlated with cognitive processing. However, the relationship between the physiological mechanisms producing these rhythms and the functions associated with the rhythms remains mysterious. This article investigates the contributions of rhythms to basic cognitive computations (such as filtering signals by coherence and/or frequency) and to major cognitive functions (such as attention and multi-modal coordination). We offer support to the premise that the physiology underlying brain rhythms plays an essential role in how these rhythms facilitate some cognitive operations."
NANCY KOPELL,"Scalable, modular three-dimensional silicon microelectrode assembly via electroless plating","We devised a scalable, modular strategy for microfabricated 3-D neural probe synthesis. We constructed a 3-D probe out of individual 2-D components (arrays of shanks bearing close-packed electrodes) using mechanical self-locking and self-aligning techniques, followed by electroless nickel plating to establish electrical contact between the individual parts. We detail the fabrication and assembly process and demonstrate different 3-D probe designs bearing thousands of electrode sites. We find typical self-alignment accuracy between shanks of <0.2° and demonstrate orthogonal electrical connections of 40 µm pitch, with thousands of connections formed electrochemically in parallel. The fabrication methods introduced allow the design of scalable, modular electrodes for high-density 3-D neural recording. The combination of scalable 3-D design and close-packed recording sites may support a variety of large-scale neural recording strategies for the mammalian brain."
NANCY KOPELL,Layer and rhythm specificity for predictive routing,"In predictive coding, experience generates predictions that attenuate the feeding forward of predicted stimuli while passing forward unpredicted ""errors."" Different models have suggested distinct cortical layers, and rhythms implement predictive coding. We recorded spikes and local field potentials from laminar electrodes in five cortical areas (visual area 4 [V4], lateral intraparietal [LIP], posterior parietal area 7A, frontal eye field [FEF], and prefrontal cortex [PFC]) while monkeys performed a task that modulated visual stimulus predictability. During predictable blocks, there was enhanced alpha (8 to 14 Hz) or beta (15 to 30 Hz) power in all areas during stimulus processing and prestimulus beta (15 to 30 Hz) functional connectivity in deep layers of PFC to the other areas. Unpredictable stimuli were associated with increases in spiking and in gamma-band (40 to 90 Hz) power/connectivity that fed forward up the cortical hierarchy via superficial-layer cortex. Power and spiking modulation by predictability was stimulus specific. Alpha/beta power in LIP, FEF, and PFC inhibited spiking in deep layers of V4. Area 7A uniquely showed increases in high-beta (∼22 to 28 Hz) power/connectivity to unpredictable stimuli. These results motivate a conceptual model, predictive routing. It suggests that predictive coding may be implemented via lower-frequency alpha/beta rhythms that ""prepare"" pathways processing-predicted inputs by inhibiting feedforward gamma rhythms and associated spiking."
NANCY KOPELL,Rhythm generation through period concatenation in rat somatosensory cortex,"Rhythmic voltage oscillations resulting from the summed activity of neuronal populations occur in many nervous systems. Contemporary observations suggest that coexistent oscillations interact and, in time, may switch in dominance. We recently reported an example of these interactions recorded from in vitro preparations of rat somatosensory cortex. We found that following an initial interval of coexistent gamma (∼25 ms period) and beta2 (∼40 ms period) rhythms in the superficial and deep cortical layers, respectively, a transition to a synchronous beta1 (∼65 ms period) rhythm in all cortical layers occurred. We proposed that the switch to beta1 activity resulted from the novel mechanism of period concatenation of the faster rhythms: gamma period (25 ms)+beta2 period (40 ms) = beta1 period (65 ms). In this article, we investigate in greater detail the fundamental mechanisms of the beta1 rhythm. To do so we describe additional in vitro experiments that constrain a biologically realistic, yet simplified, computational model of the activity. We use the model to suggest that the dynamic building blocks (or motifs) of the gamma and beta2 rhythms combine to produce a beta1 oscillation that exhibits cross-frequency interactions. Through the combined approach of in vitro experiments and mathematical modeling we isolate the specific components that promote or destroy each rhythm. We propose that mechanisms vital to establishing the beta1 oscillation include strengthened connections between a population of deep layer intrinsically bursting cells and a transition from antidromic to orthodromic spike generation in these cells. We conclude that neural activity in the superficial and deep cortical layers may temporally combine to generate a slower oscillation."
NANCY KOPELL,Parietal low beta rhythm provides a dynamical substrate for a working memory buffer,"Working memory (WM) is a component of the brain's memory systems vital for interpretation of sequential sensory inputs and consequent decision making. Anatomically, WM is highly distributed over the prefrontal cortex (PFC) and the parietal cortex (PC). Here we present a biophysically detailed dynamical systems model for a WM buffer situated in the PC, making use of dynamical properties believed to be unique to this area. We show that the natural beta1 rhythm (12 to 20 Hz) of the PC provides a substrate for an episodic buffer that can synergistically combine executive commands (e.g., from PFC) and multimodal information into a flexible and updatable representation of recent sensory inputs. This representation is sensitive to distractors, it allows for a readout mechanism, and it can be readily terminated by executive input. The model provides a demonstration of how information can be usefully stored in the temporal patterns of activity in a neuronal network rather than just synaptic weights between the neurons in that network."
NANCY KOPELL,Rhythms of the nervous system: mathematical themes and variations,"The nervous system displays a variety of rhythms in both waking and sleep. These rhythms have been closely associated with different behavioral and cognitive states, but it is still unknown how the nervous system makes use of these rhythms to perform functionally important tasks. To address those questions, it is first useful to understood in a mechanistic way the origin of the rhythms, their interactions, the signals which create the transitions among rhythms, and the ways in which rhythms filter the signals to a network of neurons. This talk discusses how dynamical systems have been used to investigate the origin, properties and interactions of rhythms in the nervous system. It focuses on how the underlying physiology of the cells and synapses of the networks shape the dynamics of the network in different contexts, allowing the variety of dynamical behaviors to be displayed by the same network. The work is presented using a series of related case studies on different rhythms. These case studies are chosen to highlight mathematical issues, and suggest further mathematical work to be done. The topics include: different roles of excitation and inhibition in creating synchronous assemblies of cells, different kinds of building blocks for neural oscillations, and transitions among rhythms. The mathematical issues include reduction of large networks to low dimensional maps, role of noise, global bifurcations, use of probabilistic formulations."
NANCY KOPELL,"Excitable neurons, firing threshold manifolds and canards","We investigate firing threshold manifolds in a mathematical model of an excitable neuron. The model analyzed investigates the phenomenon of post-inhibitory rebound spiking due to propofol anesthesia and is adapted from McCarthy et al. (SIAM J. Appl. Dyn. Syst. 11(4):1674-1697, 2012). Propofol modulates the decay time-scale of an inhibitory GABAa synaptic current. Interestingly, this system gives rise to rebound spiking within a specific range of propofol doses. Using techniques from geometric singular perturbation theory, we identify geometric structures, known as canards of folded saddle-type, which form the firing threshold manifolds. We find that the position and orientation of the canard separatrix is propofol dependent. Thus, the speeds of relevant slow synaptic processes are encoded within this geometric structure. We show that this behavior cannot be understood using a static, inhibitory current step protocol, which can provide a single threshold for rebound spiking but cannot explain the observed cessation of spiking for higher propofol doses. We then compare the analyses of dynamic and static synaptic inhibition, showing how the firing threshold manifolds of each relate, and why a current step approach is unable to fully capture the behavior of this model."
NANCY KOPELL,Period concatenation underlies interactions between gamma and beta rhythms in neocortex,"The neocortex generates rhythmic electrical activity over a frequency range covering many decades. Specific cognitive and motor states are associated with oscillations in discrete frequency bands within this range, but it is not known whether interactions and transitions between distinct frequencies are of functional importance. When coexpressed rhythms have frequencies that differ by a factor of two or more interactions can be seen in terms of phase synchronization. Larger frequency differences can result in interactions in the form of nesting of faster frequencies within slower ones by a process of amplitude modulation. It is not known how coexpressed rhythms, whose frequencies differ by less than a factor of two may interact. Here we show that two frequencies (gamma – 40 Hz and beta2 – 25 Hz), coexpressed in superficial and deep cortical laminae with low temporal interaction, can combine to generate a third frequency (beta1 – 15 Hz) showing strong temporal interaction. The process occurs via period concatenation, with basic rhythm-generating microcircuits underlying gamma and beta2 rhythms forming the building blocks of the beta1 rhythm by a process of addition. The mean ratio of adjacent frequency components was a constant – approximately the golden mean – which served to both minimize temporal interactions, and permit multiple transitions, between frequencies. The resulting temporal landscape may provide a framework for multiplexing – parallel information processing on multiple temporal scales."
NANCY KOPELL,Are different rhythms good for different functions?,"This essay discusses the relationship between the physiology of rhythms and potential functional roles. We focus on how the biophysics underlying different rhythms can give rise to different abilities of a network to form and manipulate cell assemblies. We also discuss how changes in the modulatory setting of the rhythms can change the flow of information through cortical circuits, again tying physiology to computation. We suggest that diverse rhythms, or variations of a rhythm, can support different components of a cognitive act, with multiple rhythms potentially playing multiple roles."
NANCY KOPELL,"Approximate, not perfect synchrony maximizes the downstream effectiveness of excitatory neuronal ensembles","The most basic functional role commonly ascribed to synchrony in the brain is that of amplifying excitatory neuronal signals. The reasoning is straightforward: When positive charge is injected into a leaky target neuron over a time window of positive duration, some of it will have time to leak back out before an action potential is triggered in the target, and it will in that sense be wasted. If the goal is to elicit a firing response in the target using as little charge as possible, it seems best to deliver the charge all at once, i.e., in perfect synchrony. In this article, we show that this reasoning is correct only if one assumes that the input ceases when the target crosses the firing threshold, but before it actually fires. If the input ceases later-for instance, in response to a feedback signal triggered by the firing of the target-the ""most economical"" way of delivering input (the way that requires the least total amount of input) is no longer precisely synchronous, but merely approximately so. If the target is a heterogeneous network, as it always is in the brain, then ceasing the input ""when the target crosses the firing threshold"" is not an option, because there is no single moment when the firing threshold is crossed. In this sense, precise synchrony is never optimal in the brain."
NANCY KOPELL,Flexible resonance in prefrontal networks with strong feedback inhibition,"Oscillations are ubiquitous features of brain dynamics that undergo task-related changes in synchrony, power, and frequency. The impact of those changes on target networks is poorly understood. In this work, we used a biophysically detailed model of prefrontal cortex (PFC) to explore the effects of varying the spike rate, synchrony, and waveform of strong oscillatory inputs on the behavior of cortical networks driven by them. Interacting populations of excitatory and inhibitory neurons with strong feedback inhibition are inhibition-based network oscillators that exhibit resonance (i.e., larger responses to preferred input frequencies). We quantified network responses in terms of mean firing rates and the population frequency of network oscillation; and characterized their behavior in terms of the natural response to asynchronous input and the resonant response to oscillatory inputs. We show that strong feedback inhibition causes the PFC to generate internal (natural) oscillations in the beta/gamma frequency range (>15 Hz) and to maximize principal cell spiking in response to external oscillations at slightly higher frequencies. Importantly, we found that the fastest oscillation frequency that can be relayed by the network maximizes local inhibition and is equal to a frequency even higher than that which maximizes the firing rate of excitatory cells; we call this phenomenon population frequency resonance. This form of resonance is shown to determine the optimal driving frequency for suppressing responses to asynchronous activity. Lastly, we demonstrate that the natural and resonant frequencies can be tuned by changes in neuronal excitability, the duration of feedback inhibition, and dynamic properties of the input. Our results predict that PFC networks are tuned for generating and selectively responding to beta- and gamma-rhythmic signals due to the natural and resonant properties of inhibition-based oscillators. They also suggest strategies for optimizing transcranial stimulation and using oscillatory networks in neuromorphic engineering."
NANCY KOPELL,Neural sequence generation using spatiotemporal patterns of inhibition,"Stereotyped sequences of neural activity are thought to underlie reproducible behaviors and cognitive processes ranging from memory recall to arm movement. One of the most prominent theoretical models of neural sequence generation is the synfire chain, in which pulses of synchronized spiking activity propagate robustly along a chain of cells connected by highly redundant feedforward excitation. But recent experimental observations in the avian song production pathway during song generation have shown excitatory activity interacting strongly with the firing patterns of inhibitory neurons, suggesting a process of sequence generation more complex than feedforward excitation. Here we propose a model of sequence generation inspired by these observations in which a pulse travels along a spatially recurrent excitatory chain, passing repeatedly through zones of local feedback inhibition. In this model, synchrony and robust timing are maintained not through redundant excitatory connections, but rather through the interaction between the pulse and the spatiotemporal pattern of inhibition that it creates as it circulates the network. These results suggest that spatially and temporally structured inhibition may play a key role in sequence generation."
NANCY KOPELL,Hetereogeneity in neuronal intrinsic properties: a possible mechanism for hub-like properties of the rat anterior cingulate cortex during network activity,"The anterior cingulate cortex (ACC) is vital for a range of brain functions requiring cognitive control and has highly divergent inputs and outputs, thus manifesting as a hub in connectomic analyses. Studies show diverse functional interactions within the ACC are associated with network oscillations in the β (20-30 Hz) and γ (30-80 Hz) frequency range. Oscillations permit dynamic routing of information within cortex, a function that depends on bandpass filter-like behavior to selectively respond to specific inputs. However, a putative hub region such as ACC needs to be able to combine inputs from multiple sources rather than select a single input at the expense of others. To address this potential functional dichotomy, we modeled local ACC network dynamics in the rat in vitro. Modal peak oscillation frequencies in the β- and γ-frequency band corresponded to GABAAergic synaptic kinetics as seen in other regions; however, the intrinsic properties of ACC principal neurons were highly diverse. Computational modeling predicted that this neuronal response diversity broadened the bandwidth for filtering rhythmic inputs and supported combination-rather than selection-of different frequencies within the canonical γ and β electroencephalograph bands. These findings suggest that oscillating neuronal populations can support either response selection (routing) or combination, depending on the interplay between the kinetics of synaptic inhibition and the degree of heterogeneity of principal cell intrinsic conductances."
NANCY KOPELL,Cortical gamma rhythms modulate NMDAR-mediated spike timing dependent plasticity in a biophysical model,"Spike timing dependent plasticity (STDP) has been observed experimentally in vitro and is a widely studied neural algorithm for synaptic modification. While the functional role of STDP has been investigated extensively, the effect of rhythms on the precise timing of STDP has not been characterized as well. We use a simplified biophysical model of a cortical network that generates pyramidal interneuronal gamma rhythms (PING). Plasticity via STDP is investigated at the excitatory pyramidal cell synapse from a gamma frequency (30–90 Hz) input independent of the network gamma rhythm. The input may represent a corticocortical or an information-specific thalamocortical connection. This synapse is mediated by N-methyl-D-aspartate receptor mediated (NMDAR) currents. For distinct network and input frequencies, the model shows robust frequency regimes of potentiation and depression, providing a mechanism by which responses to certain inputs can potentiate while responses to other inputs depress. For potentiating regimes, the model suggests an optimal amount and duration of plasticity that can occur, which depends on the time course for the decay of the postsynaptic NMDAR current. Prolonging the duration of the input beyond this optimal time results in depression. Inserting pauses in the input can increase the total potentiation. The optimal pause length corresponds to the decay time of the NMDAR current. Thus, STDP in this model provides a mechanism for potentiation and depression depending on input frequency and suggests that the slow NMDAR current decay helps to regulate the optimal amplitude and duration of the plasticity. The optimal pause length is comparable to the time scale of the negative phase of a modulatory theta rhythm, which may pause gamma rhythm spiking. Our pause results may suggest a novel role for this theta rhythm in plasticity. Finally, we discuss our results in the context of auditory thalamocortical plasticity."
NANCY KOPELL,Frontal beta-theta network during REM sleep,"We lack detailed knowledge about the spatio-temporal physiological signatures of REM sleep, especially in humans. By analyzing intracranial electrode data from humans, we demonstrate for the first time that there are prominent beta (15–35 Hz) and theta (4–8 Hz) oscillations in both the anterior cingulate cortex (ACC) and the DLPFC during REM sleep. We further show that these theta and beta activities in the ACC and the DLPFC, two relatively distant but reciprocally connected regions, are coherent. These findings suggest that, counter to current prevailing thought, the DLPFC is active during REM sleep and likely interacting with other areas. Since the DLPFC and the ACC are implicated in memory and emotional regulation, and the ACC has motor areas and is thought to be important for error detection, the dialogue between these two areas could play a role in the regulation of emotions and in procedural motor and emotional memory consolidation."
NANCY KOPELL,Representation of time-varying stimuli by a network exhibiting oscillations on a faster time scale,"Sensory processing is associated with gamma frequency oscillations (30–80 Hz) in sensory cortices. This raises the question whether gamma oscillations can be directly involved in the representation of time-varying stimuli, including stimuli whose time scale is longer than a gamma cycle. We are interested in the ability of the system to reliably distinguish different stimuli while being robust to stimulus variations such as uniform time-warp. We address this issue with a dynamical model of spiking neurons and study the response to an asymmetric sawtooth input current over a range of shape parameters. These parameters describe how fast the input current rises and falls in time. Our network consists of inhibitory and excitatory populations that are sufficient for generating oscillations in the gamma range. The oscillations period is about one-third of the stimulus duration. Embedded in this network is a subpopulation of excitatory cells that respond to the sawtooth stimulus and a subpopulation of cells that respond to an onset cue. The intrinsic gamma oscillations generate a temporally sparse code for the external stimuli. In this code, an excitatory cell may fire a single spike during a gamma cycle, depending on its tuning properties and on the temporal structure of the specific input; the identity of the stimulus is coded by the list of excitatory cells that fire during each cycle. We quantify the properties of this representation in a series of simulations and show that the sparseness of the code makes it robust to uniform warping of the time scale. We find that resetting of the oscillation phase at stimulus onset is important for a reliable representation of the stimulus and that there is a tradeoff between the resolution of the neural representation of the stimulus and robustness to time-warp."
NANCY KOPELL,New dynamics in cerebellar purkinje cells: torus canards,"We describe a transition from bursting to rapid spiking in a reduced mathematical model of a cerebellar Purkinje cell. We perform a slow-fast analysis of the system and find that—after a saddle node bifurcation of limit cycles—the full model dynamics temporarily follow a repelling branch of limit cycles. We propose that the system exhibits a dynamical phenomenon new to realistic, biophysical applications: torus canards."
NANCY KOPELL,M-current expands the range of gamma frequency inputs to which a neuronal target entrains,"Theta (4–8 Hz) and gamma (30–80 Hz) rhythms in the brain are commonly associated with memory and learning (Kahana in J Neurosci 26:1669–1672, 2006; Quilichini et al. in J Neurosci 30:11128–11142, 2010). The precision of co-firing between neurons and incoming inputs is critical in these cognitive functions. We consider an inhibitory neuron model with M-current under forcing from gamma pulses and a sinusoidal current of theta frequency. The M-current has a long time constant (∼90 ms) and it has been shown to generate resonance at theta frequencies (Hutcheon and Yarom in Trends Neurosci 23:216–222, 2000; Hu et al. in J Physiol 545:783–805, 2002). We have found that this slow M-current contributes to the precise co-firing between the network and fast gamma pulses in the presence of a slow sinusoidal forcing. The M-current expands the phase-locking frequency range of the network, counteracts the slow theta forcing, and admits bistability in some parameter range. The effects of the M-current balancing the theta forcing are reduced if the sinusoidal current is faster than the theta frequency band. We characterize the dynamical mechanisms underlying the role of the M-current in enabling a network to be entrained to gamma frequency inputs using averaging methods, geometric singular perturbation theory, and bifurcation analysis."
NANCY KOPELL,Differential contributions of synaptic and intrinsic inhibitory currents to speech segmentation via flexible phase-locking in neural oscillators,"Current hypotheses suggest that speech segmentation – the initial division and grouping of the speech stream into candidate phrases, syllables, and phonemes for further linguistic processing – is executed by a hierarchy of oscillators in auditory cortex. Theta (~3-12 Hz) rhythms play a key role by phase-locking to recurring acoustic features marking syllable boundaries. Reliable synchronization to quasi-rhythmic inputs, whose variable frequency can dip below cortical theta frequencies (down to ~1 Hz), requires “flexible” theta oscillators whose underlying neuronal mechanisms remain unknown. Using biophysical computational models, we found that the flexibility of phase-locking in neural oscillators depended on the types of hyperpolarizing currents that paced them. Simulated cortical theta oscillators flexibly phase-locked to slow inputs when these inputs caused both (i) spiking and (ii) the subsequent buildup of outward current sufficient to delay further spiking until the next input. The greatest flexibility in phase-locking arose from a synergistic interaction between intrinsic currents that was not replicated by synaptic currents at similar timescales. Flexibility in phase-locking enabled improved entrainment to speech input, optimal at mid-vocalic channels, which in turn supported syllabic-timescale segmentation through identification of vocalic nuclei. Our results suggest that synaptic and intrinsic inhibition contribute to frequency-restricted and -flexible phase-locking in neural oscillators, respectively. Their differential deployment may enable neural oscillators to play diverse roles, from reliable internal clocking to adaptive segmentation of quasi-regular sensory inputs like speech. Author summary: Oscillatory activity in auditory cortex is believed to play an important role in auditory and speech processing. One suggested function of these rhythms is to divide the speech stream into candidate phonemes, syllables, words, and phrases, to be matched with learned linguistic templates. This requires brain rhythms to flexibly synchronize with regular acoustic features of the speech stream. How neuronal circuits implement this task remains unknown. In this study, we explored the contribution of inhibitory currents to flexible phase-locking in neuronal theta oscillators, believed to perform initial syllabic segmentation. We found that a combination of specific intrinsic inhibitory currents at multiple timescales, present in a large class of cortical neurons, enabled exceptionally flexible phase-locking, which could be used to precisely segment speech by identifying vowels at mid-syllable. This suggests that the cells exhibiting these currents are a key component in the brain’s auditory and speech processing architecture."
NANCY KOPELL,Thalamocortical control of propofol phase-amplitude coupling,"The anesthetic propofol elicits many different spectral properties on the EEG, including alpha oscillations (8-12 Hz), Slow Wave Oscillations (SWO, 0.1-1.5 Hz), and dose-dependent phase-amplitude coupling (PAC) between alpha and SWO. Propofol is known to increase GABAA inhibition and decrease H-current strength, but how it generates these rhythms and their interactions is still unknown. To investigate both generation of the alpha rhythm and its PAC to SWO, we simulate a Hodgkin-Huxley network model of a hyperpolarized thalamus and corticothalamic inputs. We find, for the first time, that the model thalamic network is capable of independently generating the sustained alpha seen in propofol, which may then be relayed to cortex and expressed on the EEG. This dose-dependent sustained alpha critically relies on propofol GABAA potentiation to alter the intrinsic spindling mechanisms of the thalamus. Furthermore, the H-current conductance and background excitation of these thalamic cells must be within specific ranges to exhibit any intrinsic oscillations, including sustained alpha. We also find that, under corticothalamic SWO UP and DOWN states, thalamocortical output can exhibit maximum alpha power at either the peak or trough of this SWO; this implies the thalamus may be the source of propofol-induced PAC. Hyperpolarization level is the main determinant of whether the thalamus exhibits trough-max PAC, which is associated with lower propofol dose, or peak-max PAC, associated with higher dose. These findings suggest: the thalamus generates a novel rhythm under GABAA potentiation such as under propofol, its hyperpolarization may determine whether a patient experiences trough-max or peak-max PAC, and the thalamus is a critical component of propofol-induced cortical spectral phenomena. Changes to the thalamus may be a critical part of how propofol accomplishes its effects, including unconsciousness."
NANCY KOPELL,Interactions of multiple rhythms in a biophysical network of neurons,"Neural oscillations, including rhythms in the beta1 band (12–20 Hz), are important in various cognitive functions. Often neural networks receive rhythmic input at frequencies different from their natural frequency, but very little is known about how such input affects the network’s behavior. We use a simplified, yet biophysical, model of a beta1 rhythm that occurs in the parietal cortex, in order to study its response to oscillatory inputs. We demonstrate that a cell has the ability to respond at the same time to two periodic stimuli of unrelated frequencies, firing in phase with one, but with a mean firing rate equal to that of the other. We show that this is a very general phenomenon, independent of the model used. We next show numerically that the behavior of a different cell, which is modeled as a high-dimensional dynamical system, can be described in a surprisingly simple way, owing to a reset that occurs in the state space when the cell fires. The interaction of the two cells leads to novel combinations of properties for neural dynamics, such as mode-locking to an input without phase-locking to it."
NANCY KOPELL,Representation of Time-Varying Stimuli by a Network Exhibiting Oscillations on a Faster Time Scale,"Sensory processing is associated with gamma frequency oscillations (30–80 Hz) in sensory cortices. This raises the question whether gamma oscillations can be directly involved in the representation of time-varying stimuli, including stimuli whose time scale is longer than a gamma cycle. We are interested in the ability of the system to reliably distinguish different stimuli while being robust to stimulus variations such as uniform time-warp. We address this issue with a dynamical model of spiking neurons and study the response to an asymmetric sawtooth input current over a range of shape parameters. These parameters describe how fast the input current rises and falls in time. Our network consists of inhibitory and excitatory populations that are sufficient for generating oscillations in the gamma range. The oscillations period is about one-third of the stimulus duration. Embedded in this network is a subpopulation of excitatory cells that respond to the sawtooth stimulus and a subpopulation of cells that respond to an onset cue. The intrinsic gamma oscillations generate a temporally sparse code for the external stimuli. In this code, an excitatory cell may fire a single spike during a gamma cycle, depending on its tuning properties and on the temporal structure of the specific input; the identity of the stimulus is coded by the list of excitatory cells that fire during each cycle. We quantify the properties of this representation in a series of simulations and show that the sparseness of the code makes it robust to uniform warping of the time scale. We find that resetting of the oscillation phase at stimulus onset is important for a reliable representation of the stimulus and that there is a tradeoff between the resolution of the neural representation of the stimulus and robustness to time-warp. Author Summary Sensory processing of time-varying stimuli, such as speech, is associated with high-frequency oscillatory cortical activity, the functional significance of which is still unknown. One possibility is that the oscillations are part of a stimulus-encoding mechanism. Here, we investigate a computational model of such a mechanism, a spiking neuronal network whose intrinsic oscillations interact with external input (waveforms simulating short speech segments in a single acoustic frequency band) to encode stimuli that extend over a time interval longer than the oscillation's period. The network implements a temporally sparse encoding, whose robustness to time warping and neuronal noise we quantify. To our knowledge, this study is the first to demonstrate that a biophysically plausible model of oscillations occurring in the processing of auditory input may generate a representation of signals that span multiple oscillation cycles."
NANCY KOPELL,Period Concatenation Underlies Interactions Between Gamma and Beta Rhythms in Neocortex,"The neocortex generates rhythmic electrical activity over a frequency range covering many decades. Specific cognitive and motor states are associated with oscillations in discrete frequency bands within this range, but it is not known whether interactions and transitions between distinct frequencies are of functional importance. When coexpressed rhythms have frequencies that differ by a factor of two or more interactions can be seen in terms of phase synchronization. Larger frequency differences can result in interactions in the form of nesting of faster frequencies within slower ones by a process of amplitude modulation. It is not known how coexpressed rhythms, whose frequencies differ by less than a factor of two may interact. Here we show that two frequencies (gamma - 40Hz and beta2 - 25Hz), coexpressed in superficial and deep cortical laminae with low temporal interaction, can combine to generate a third frequency (beta1 - 15Hz) showing strong temporal interaction. The process occurs via period concatenation, with basic rhythm-generating microcircuits underlying gamma and beta2 rhythms forming the building blocks of the beta1 rhythm by a process of addition. The mean ratio of adjacent frequency components was a constant - approximately the golden mean - which served to both minimize temporal interactions, and permit multiple transitions, between frequencies. The resulting temporal landscape may provide a framework for multiplexing - parallel information processing on multiple temporal scales."
NANCY KOPELL,Are Different Rhythms Good for Different Functions?,"This essay discusses the relationship between the physiology of rhythms and potential functional roles. We focus on how the biophysics underlying different rhythms can give rise to different abilities of a network to form and manipulate cell assemblies. We also discuss how changes in the modulatory setting of the rhythms can change the flow of information through cortical circuits, again tying physiology to computation. We suggest that diverse rhythms, or variations of a rhythm, can support different components of a cognitive act, with multiple rhythms potentially playing multiple roles."
NANCY KOPELL,Temporal Interactions Between Cortical Rhythms,"Multiple local neuronal circuits support different, discrete frequencies of network rhythm in neocortex. Relationships between different frequencies correspond to mechanisms designed to minimise interference, couple activity via stable phase interactions, and control the amplitude of one frequency relative to the phase of another. These mechanisms are proposed to form a framework for spectral information processing. Individual local circuits can also transform their frequency through changes in intrinsic neuronal properties and interactions with other oscillating microcircuits. Here we discuss a frequency transformation in which activity in two co-active local circuits may combine sequentially to generate a third frequency whose period is the concatenation sum of the original two. With such an interaction, the intrinsic periodicity in each component local circuit is preserved - alternate, single periods of each original rhythm form one period of a new frequency - suggesting a robust mechanism for combining information processed on multiple concurrent spatiotemporal scales."
NANCY KOPELL,Temporal interactions between cortical rhythms,"Multiple local neuronal circuits support different, discrete frequencies of network rhythm in neocortex. Relationships between different frequencies correspond to mechanisms designed to minimise interference, couple activity via stable phase interactions, and control the amplitude of one frequency relative to the phase of another. These mechanisms are proposed to form a framework for spectral information processing. Individual local circuits can also transform their frequency through changes in intrinsic neuronal properties and interactions with other oscillating microcircuits. Here we discuss a frequency transformation in which activity in two co-active local circuits may combine sequentially to generate a third frequency whose period is the concatenation sum of the original two. With such an interaction, the intrinsic periodicity in each component local circuit is preserved ÃƒÂ¢Ã‚Â€Ã‚Â“ alternate, single periods of each original rhythm form one period of a new frequency ÃƒÂ¢Ã‚Â€Ã‚Â“ suggesting a robust mechanism for combining information processed on multiple concurrent spatiotemporal scales."
NANCY KOPELL,Cortical Gamma Rhythms Modulate NMDAR-Mediated Spike Timing Dependent Plasticity in a Biophysical Model,"Spike timing dependent plasticity (STDP) has been observed experimentally in vitro and is a widely studied neural algorithm for synaptic modification. While the functional role of STDP has been investigated extensively, the effect of rhythms on the precise timing of STDP has not been characterized as well. We use a simplified biophysical model of a cortical network that generates pyramidal interneuronal gamma rhythms (PING). Plasticity via STDP is investigated at the excitatory pyramidal cell synapse from a gamma frequency (30–90 Hz) input independent of the network gamma rhythm. The input may represent a corticocortical or an information-specific thalamocortical connection. This synapse is mediated by N-methyl-D-aspartate receptor mediated (NMDAR) currents. For distinct network and input frequencies, the model shows robust frequency regimes of potentiation and depression, providing a mechanism by which responses to certain inputs can potentiate while responses to other inputs depress. For potentiating regimes, the model suggests an optimal amount and duration of plasticity that can occur, which depends on the time course for the decay of the postsynaptic NMDAR current. Prolonging the duration of the input beyond this optimal time results in depression. Inserting pauses in the input can increase the total potentiation. The optimal pause length corresponds to the decay time of the NMDAR current. Thus, STDP in this model provides a mechanism for potentiation and depression depending on input frequency and suggests that the slow NMDAR current decay helps to regulate the optimal amplitude and duration of the plasticity. The optimal pause length is comparable to the time scale of the negative phase of a modulatory theta rhythm, which may pause gamma rhythm spiking. Our pause results may suggest a novel role for this theta rhythm in plasticity. Finally, we discuss our results in the context of auditory thalamocortical plasticity. Author Summary Rhythms are well studied phenomena in many animal species. Brain rhythms in the gamma frequency range (30–90 Hz) are thought to play a role in attention and memory. In this paper, we are interested in how cortical gamma rhythms interact with information specific inputs that also have a significant gamma frequency component. The results from our computational model show that plasticity associated with learning depends on the specific frequencies of the input and cortical gamma rhythms. The results show a mechanism by which both increases and decreases in the strength of the input connection can occur, depending on the specific frequency of the input. A current mediated by NMDA receptors may be responsible for the temporal course of the plasticity seen in these brain regions. We discuss the implications of our results for conditioning paradigms applied to auditory learning."
NANCY KOPELL,Rhythm Generation through Period Concatenation in Rat Somatosensory Cortex,"Rhythmic voltage oscillations resulting from the summed activity of neuronal populations occur in many nervous systems. Contemporary observations suggest that coexistent oscillations interact and, in time, may switch in dominance. We recently reported an example of these interactions recorded from in vitro preparations of rat somatosensory cortex. We found that following an initial interval of coexistent gamma (∼25 ms period) and beta2 (∼40 ms period) rhythms in the superficial and deep cortical layers, respectively, a transition to a synchronous beta1 (∼65 ms period) rhythm in all cortical layers occurred. We proposed that the switch to beta1 activity resulted from the novel mechanism of period concatenation of the faster rhythms: gamma period (25 ms)+beta2 period (40 ms) = beta1 period (65 ms). In this article, we investigate in greater detail the fundamental mechanisms of the beta1 rhythm. To do so we describe additional in vitro experiments that constrain a biologically realistic, yet simplified, computational model of the activity. We use the model to suggest that the dynamic building blocks (or motifs) of the gamma and beta2 rhythms combine to produce a beta1 oscillation that exhibits cross-frequency interactions. Through the combined approach of in vitro experiments and mathematical modeling we isolate the specific components that promote or destroy each rhythm. We propose that mechanisms vital to establishing the beta1 oscillation include strengthened connections between a population of deep layer intrinsically bursting cells and a transition from antidromic to orthodromic spike generation in these cells. We conclude that neural activity in the superficial and deep cortical layers may temporally combine to generate a slower oscillation. Author SummarySince the late 19th century, rhythmic electrical activity has been observed in the mammalian brain. Although subject to intense scrutiny, only a handful of these rhythms are understood in terms of the biophysical elements that produce the oscillations. Even less understood are the mechanisms that underlie interactions between rhythms; how do rhythms of different frequencies coexist and affect one another in the dynamic environment of the brain? In this article, we consider a recent proposal for a novel mechanism of cortical rhythm generation: period concatenation, in which the periods of faster rhythms sum to produce a slower oscillation. To model this phenomenon, we implement simple—yet biophysical—computational models of the individual neurons that produce the brain's voltage activity. We utilize established models for the faster rhythms, and unite these in a particular way to generate a slower oscillation. Through the combined approach of experimental recordings (from thin sections of rat cortex) and mathematical modeling, we identify the cell types, synaptic connections, and ionic currents involved in rhythm generation through period concatenation. In this way the brain may generate new activity through the combination of preexisting elements."
NANCY KOPELL,Rapid synaptic and gamma rhythm signature of mouse critical period plasticity,
NANCY KOPELL,"Striatal cholinergic receptor activation causes a rapid, selective and state-dependent rise in cortico-striatal β activity",
NANCY KOPELL,A biophysical model of striatal microcircuits suggests delta/theta - rhythmically interleaved gamma and beta oscillations mediate periodicity in motor control,
NANCY KOPELL,DynaSim: a MATLAB toolbox for neural modeling and simulation,
NANCY KOPELL,Interacting rhythms enhance sensitivity of target detection in a fronto-parietal computational model of visual attention,"Even during sustained attention, enhanced processing of attended stimuli waxes and wanes rhythmically, with periods of enhanced and relatively diminished visual processing (and subsequent target detection) alternating at 4 or 8 Hz in a sustained visual attention task. These alternating attentional states occur alongside alternating dynamical states, in which lateral intraparietal cortex (LIP), the frontal eye field (FEF), and the mediodorsal pulvinar exhibit different activity and functional connectivity at α, β, and γ frequencies—rhythms associated with visual processing, working memory, and motor suppression. To assess whether and how these multiple interacting rhythms contribute to periodicity in attention, we propose a detailed computational model of FEF and LIP, which reproduced the rhythmic dynamics and behavioral consequences of observed attentional states when driven by θ-rhythmic inputs simulating experimentally-observed pulvinar activity. This model reveals that the frequencies and mechanisms of the observed rhythms allow for peak sensitivity in visual target detection while maintaining functional flexibility."
NANCY KOPELL,Deep brain stimulation in the subthalamic nucleus for Parkinson's disease can restore dynamics of striatal networks,"Deep brain stimulation (DBS) of the subthalamic nucleus (STN) is highly effective in alleviating movement disability in patients with Parkinson’s disease (PD). However, its therapeutic mechanism of action is unknown. The healthy striatum exhibits rich dynamics resulting from an interaction of beta, gamma and theta oscillations. These rhythms are at the heart of selection, initiation and execution of motor programs, and their loss or exaggeration due to dopamine (DA) depletion in PD is a major source of the behavioral deficits observed in PD patients. Interrupting abnormal rhythms and restoring the interaction of rhythms as observed in the healthy striatum may then be instrumental in the therapeutic action of DBS. We develop a biophysical networked model of a BG pathway to study how abnormal beta oscillations can emerge throughout the BG in PD, and how DBS can restore normal beta, gamma and theta striatal rhythms. Our model incorporates STN projections to the striatum, long known but understudied, that were recently shown to preferentially target fast spiking interneurons (FSI) in the striatum. We find that DBS in STN is able to normalize striatal medium spiny neuron (MSN) activity by recruiting FSI dynamics, and restoring the inhibitory potency of FSIs observed in normal condition. We also find that DBS allows the re-expression of gamma and theta rhythms, thought to be dependent on high DA levels and thus lost in PD, through cortical noise control. Our study shows how BG connectivity can amplify beta oscillations, and delineates the role of DBS in disrupting beta oscillations and providing corrective input to STN efferents to restore healthy striatal dynamics. It also suggests how gamma oscillations can be leveraged to enhance or supplement DBS treatment and improve its effectiveness."
NANCY KOPELL,Striatal cholinergic interneurons generate beta and gamma oscillations in the corticostriatal circuit and produce motor deficits,"Cortico-basal ganglia-thalamic (CBT) neural circuits are critical modulators of cognitive and motor function. When compromised, these circuits contribute to neurological and psychiatric disorders, such as Parkinson's disease (PD). In PD, motor deficits correlate with the emergence of exaggerated beta frequency (15-30 Hz) oscillations throughout the CBT network. However, little is known about how specific cell types within individual CBT brain regions support the generation, propagation, and interaction of oscillatory dynamics throughout the CBT circuit or how specific oscillatory dynamics are related to motor function. Here, we investigated the role of striatal cholinergic interneurons (SChIs) in generating beta and gamma oscillations in cortical-striatal circuits and in influencing movement behavior. We found that selective stimulation of SChIs via optogenetics in normal mice robustly and reversibly amplified beta and gamma oscillations that are supported by distinct mechanisms within striatal-cortical circuits. Whereas beta oscillations are supported robustly in the striatum and all layers of primary motor cortex (M1) through a muscarinic-receptor mediated mechanism, gamma oscillations are largely restricted to the striatum and the deeper layers of M1. Finally, SChI activation led to parkinsonian-like motor deficits in otherwise normal mice. These results highlight the important role of striatal cholinergic interneurons in supporting oscillations in the CBT network that are closely related to movement and parkinsonian motor symptoms."
NANCY KOPELL,A biophysical model of striatal microcircuits suggests gamma and beta oscillations interleaved at delta/theta frequencies mediate periodicity in motor control,"Striatal oscillatory activity is associated with movement, reward, and decision-making, and observed in several interacting frequency bands. Local field potential recordings in rodent striatum show dopamine- and reward-dependent transitions between two states: a ""spontaneous"" state involving β (∼15-30 Hz) and low γ (∼40-60 Hz), and a state involving θ (∼4-8 Hz) and high γ (∼60-100 Hz) in response to dopaminergic agonism and reward. The mechanisms underlying these rhythmic dynamics, their interactions, and their functional consequences are not well understood. In this paper, we propose a biophysical model of striatal microcircuits that comprehensively describes the generation and interaction of these rhythms, as well as their modulation by dopamine. Building on previous modeling and experimental work suggesting that striatal projection neurons (SPNs) are capable of generating β oscillations, we show that networks of striatal fast-spiking interneurons (FSIs) are capable of generating δ/θ (ie, 2 to 6 Hz) and γ rhythms. Under simulated low dopaminergic tone our model FSI network produces low γ band oscillations, while under high dopaminergic tone the FSI network produces high γ band activity nested within a δ/θ oscillation. SPN networks produce β rhythms in both conditions, but under high dopaminergic tone, this β oscillation is interrupted by δ/θ-periodic bursts of γ-frequency FSI inhibition. Thus, in the high dopamine state, packets of FSI γ and SPN β alternate at a δ/θ timescale. In addition to a mechanistic explanation for previously observed rhythmic interactions and transitions, our model suggests a hypothesis as to how the relationship between dopamine and rhythmicity impacts motor function. We hypothesize that high dopamine-induced periodic FSI γ-rhythmic inhibition enables switching between β-rhythmic SPN cell assemblies representing the currently active motor program, and thus that dopamine facilitates movement in part by allowing for rapid, periodic shifts in motor program execution."
NANCY KOPELL,Interacting rhythms enhance sensitivity of target detection in a fronto-parietal computational model of visual attention,"Even during sustained attention, enhanced processing of attended stimuli waxes and wanes rhythmically, with periods of enhanced and relatively diminished visual processing (and subsequent target detection) alternating at 4 or 8 Hz in a sustained visual attention task. These alternating attentional states occur alongside alternating dynamical states, in which lateral intraparietal cortex (LIP), the frontal eye field (FEF), and the mediodorsal pulvinar (mdPul) exhibit different activity and functional connectivity at α, β, and γ frequencies—rhythms associated with visual processing, working memory, and motor suppression. To assess whether and how these multiple interacting rhythms contribute to periodicity in attention, we propose a detailed computational model of FEF and LIP. When driven by θ-rhythmic inputs simulating experimentally-observed mdPul activity, this model reproduced the rhythmic dynamics and behavioral consequences of observed attentional states, revealing that the frequencies and mechanisms of the observed rhythms allow for peak sensitivity in visual target detection while maintaining functional flexibility."
NANCY KOPELL,Deep brain stimulation in the subthalamic nucleus for Parkinson's disease can restore dynamics of striatal networks,"Deep brain stimulation (DBS) of the subthalamic nucleus (STN) is highly effective in alleviating movement disability in patients with Parkinson’s disease (PD). However, its therapeutic mechanism of action is unknown. The healthy striatum exhibits rich dynamics resulting from an interaction of beta, gamma, and theta oscillations. These rhythms are essential to selection and execution of motor programs, and their loss or exaggeration due to dopamine (DA) depletion in PD is a major source of behavioral deficits. Restoring the natural rhythms may then be instrumental in the therapeutic action of DBS. We develop a biophysical networked model of a BG pathway to study how abnormal beta oscillations can emerge throughout the BG in PD and how DBS can restore normal beta, gamma, and theta striatal rhythms. Our model incorporates STN projections to the striatum, long known but understudied, found to preferentially target fast-spiking interneurons (FSI). We find that DBS in STN can normalize striatal medium spiny neuron activity by recruiting FSI dynamics and restoring the inhibitory potency of FSIs observed in normal conditions. We also find that DBS allows the reexpression of gamma and theta rhythms, thought to be dependent on high DA levels and thus lost in PD, through cortical noise control. Our study highlights that DBS effects can go beyond regularizing BG output dynamics to restoring normal internal BG dynamics and the ability to regulate them. It also suggests how gamma and theta oscillations can be leveraged to supplement DBS treatment and enhance its effectiveness."
NANCY KOPELL,Basolateral amygdala oscillations enable fear learning in a biophysical model,
SCOTT E SCHAUS,High-throughput screening in larval zebrafish identifies novel potent sedative-hypnotics,"BACKGROUND: Many general anesthetics were discovered empirically, but primary screens to find new sedative-hypnotics in drug libraries have not used animals, limiting the types of drugs discovered. The authors hypothesized that a sedative-hypnotic screening approach using zebrafish larvae responses to sensory stimuli would perform comparably to standard assays, and efficiently identify new active compounds. METHODS: The authors developed a binary outcome photomotor response assay for zebrafish larvae using a computerized system that tracked individual motions of up to 96 animals simultaneously. The assay was validated against tadpole loss of righting reflexes, using sedative-hypnotics of widely varying potencies that affect various molecular targets. A total of 374 representative compounds from a larger library were screened in zebrafish larvae for hypnotic activity at 10 µM. Molecular mechanisms of hits were explored in anesthetic-sensitive ion channels using electrophysiology, or in zebrafish using a specific reversal agent. RESULTS: Zebrafish larvae assays required far less drug, time, and effort than tadpoles. In validation experiments, zebrafish and tadpole screening for hypnotic activity agreed 100% (n = 11; P = 0.002), and potencies were very similar (Pearson correlation, r > 0.999). Two reversible and potent sedative-hypnotics were discovered in the library subset. CMLD003237 (EC50, ~11 µM) weakly modulated γ-aminobutyric acid type A receptors and inhibited neuronal nicotinic receptors. CMLD006025 (EC50, ~13 µM) inhibited both N-methyl-D-aspartate and neuronal nicotinic receptors. CONCLUSIONS: Photomotor response assays in zebrafish larvae are a mechanism-independent platform for high-throughput screening to identify novel sedative-hypnotics. The variety of chemotypes producing hypnosis is likely much larger than currently known."
SCOTT E SCHAUS,"Diastereoselective three-component synthesis of beta-amino carbonyl compounds using diazo compounds, boranes, and acyl imines under catalyst-free conditions","Diazo compounds, boranes, and acyl imines undergo a three-component Mannich condensation reaction under catalyst-free conditions to give the anti β-amino carbonyl compounds in high diastereoselectivity. The reaction tolerates a variety of functional groups, and an asymmetric variant was achieved using the (−)-phenylmenthol as chiral auxiliary in good yield and selectivity. These β-amino carbonyl compounds are valuable intermediates, which can be transformed to many potential bioactive molecules."
SCOTT E SCHAUS,Improvement of experimental testing and network training conditions with genome-wide microarrays for more accurate predictions of drug gene targets,"BACKGROUND: Genome-wide microarrays have been useful for predicting chemical-genetic interactions at the gene level. However, interpreting genome-wide microarray results can be overwhelming due to the vast output of gene expression data combined with off-target transcriptional responses many times induced by a drug treatment. This study demonstrates how experimental and computational methods can interact with each other, to arrive at more accurate predictions of drug-induced perturbations. We present a two-stage strategy that links microarray experimental testing and network training conditions to predict gene perturbations for a drug with a known mechanism of action in a well-studied organism. RESULTS: S. cerevisiae cells were treated with the antifungal, fluconazole, and expression profiling was conducted under different biological conditions using Affymetrix genome-wide microarrays. Transcripts were filtered with a formal network-based method, sparse simultaneous equation models and Lasso regression (SSEM-Lasso), under different network training conditions. Gene expression results were evaluated using both gene set and single gene target analyses, and the drug’s transcriptional effects were narrowed first by pathway and then by individual genes. Variables included: (i) Testing conditions – exposure time and concentration and (ii) Network training conditions – training compendium modifications. Two analyses of SSEM-Lasso output – gene set and single gene – were conducted to gain a better understanding of how SSEM-Lasso predicts perturbation targets. CONCLUSIONS: This study demonstrates that genome-wide microarrays can be optimized using a two-stage strategy for a more in-depth understanding of how a cell manifests biological reactions to a drug treatment at the transcription level. Additionally, a more detailed understanding of how the statistical model, SSEM-Lasso, propagates perturbations through a network of gene regulatory interactions is achieved."
SCOTT E SCHAUS,Asymmetric synthesis of griffipavixanthone employing a chiral phosphoric acid-catalyzed cycloaddition,"Asymmetric synthesis of the biologically active xanthone dimer griffipavixanthone is reported along with its absolute stereochemistry determination. Synthesis of the natural product is accomplished via dimerization of a p-quinone methide using a chiral phosphoric acid catalyst to afford a protected precursor in excellent diastereo- and enantioselectivity. Mechanistic studies, including an unbiased computational investigation of chiral ion-pairs using parallel tempering, were performed in order to probe the mode of asymmetric induction."
SCOTT E SCHAUS,"The microtubule-associated histone methyltransferase SET8, facilitated by transcription factor LSF, methylates α-tubulin","Microtubules are cytoskeletal structures critical for mitosis, cell motility, and protein and organelle transport and are a validated target for anticancer drugs. However, how tubulins are regulated and recruited to support these distinct cellular processes is incompletely understood. Posttranslational modifications of tubulins are proposed to regulate microtubule function and dynamics. Although many of these modifications have been investigated, only one prior study reports tubulin methylation and an enzyme responsible for this methylation. Here we used in vitro radiolabeling, MS, and immunoblotting approaches to monitor protein methylation and immunoprecipitation, immunofluorescence, and pulldown approaches to measure protein-protein interactions. We demonstrate that N-lysine methyltransferase 5A (KMT5A or SET8/PR-Set7), which methylates lysine 20 in histone H4, bound α-tubulin and methylated it at a specific lysine residue, Lys311 Furthermore, late SV40 factor (LSF)/CP2, a known transcription factor, bound both α-tubulin and SET8 and enhanced SET8-mediated α-tubulin methylation in vitro In addition, we found that the ability of LSF to facilitate this methylation is countered by factor quinolinone inhibitor 1 (FQI1), a specific small-molecule inhibitor of LSF. These findings suggest the general model that microtubule-associated proteins, including transcription factors, recruit or stimulate protein-modifying enzymes to target tubulins. Moreover, our results point to dual functions for SET8 and LSF not only in chromatin regulation but also in cytoskeletal modification."
SCOTT E SCHAUS,"Factor quinolinone inhibitors disrupt spindles and multiple LSF (TFCP2)-protein interactions in mitosis, including with microtubule-associated proteins","Factor quinolinone inhibitors (FQIs), a first-in-class set of small molecule inhibitors targeted to the transcription factor LSF (TFCP2), exhibit promising cancer chemotherapeutic properties. FQI1, the initial lead compound identified, unexpectedly induced a concentration-dependent delay in mitotic progression. Here, we show that FQI1 can rapidly and reversibly lead to mitotic arrest, even when added directly to mitotic cells, implying that FQI1-mediated mitotic defects are not transcriptionally based. Furthermore, treatment with FQIs resulted in a striking, concentration-dependent diminishment of spindle microtubules, accompanied by a concentration-dependent increase in multi-aster formation. Aberrant γ-tubulin localization was also observed. These phenotypes suggest that perturbation of spindle microtubules is the primary event leading to the mitotic delays upon FQI1 treatment. Previously, FQIs were shown to specifically inhibit not only LSF DNA-binding activity, which requires LSF oligomerization to tetramers, but also other specific LSF-protein interactions. Other transcription factors participate in mitosis through non-transcriptional means, and we recently reported that LSF directly binds α-tubulin and is present in purified cellular tubulin preparations. Consistent with a microtubule role for LSF, here we show that LSF enhanced the rate of tubulin polymerization in vitro, and FQI1 inhibited such polymerization. To probe whether the FQI1-mediated spindle abnormalities could result from inhibition of mitotic LSF-protein interactions, mass spectrometry was performed using as bait an inducible, tagged form of LSF that is biotinylated by endogenous enzymes. The global proteomics analysis yielded expected associations for a transcription factor, notably with RNA processing machinery, but also to nontranscriptional components. In particular, and consistent with spindle disruption due to FQI treatment, mitotic, FQI1-sensitive interactions were identified between the biotinylated LSF and microtubule-associated proteins that regulate spindle assembly, positioning, and dynamics, as well as centrosome-associated proteins. Probing the mitotic LSF interactome using small molecule inhibitors therefore supported a non-transcriptional role for LSF in mediating progression through mitosis."
SCOTT E SCHAUS,"Targeting the oncogene LSF with either the small molecule inhibitor FQI1 or siRNA causes mitotic delays with unaligned chromosomes, resulting in cell death or senescence","BACKGROUND: The oncogene LSF (encoded by TFCP2) has been proposed as a novel therapeutic target for multiple cancers. LSF overexpression in patient tumors correlates with poor prognosis in particular for both hepatocellular carcinoma and colorectal cancer. The limited treatment outcomes for these diseases and disappointing clinical results, in particular, for hepatocellular carcinoma in molecularly targeted therapies targeting cellular receptors and kinases, underscore the need for molecularly targeting novel mechanisms. LSF small molecule inhibitors, Factor Quinolinone Inhibitors (FQIs), have exhibited robust anti-tumor activity in multiple pre-clinical models, with no observable toxicity. METHODS: To understand how the LSF inhibitors impact cancer cell proliferation, we characterized the cellular phenotypes that result from loss of LSF activity. Cell proliferation and cell cycle progression were analyzed, using HeLa cells as a model cancer cell line responsive to FQI1. Cell cycle progression was studied either by time lapse microscopy or by bulk synchronization of cell populations to ensure accuracy in interpretation of the outcomes. In order to test for biological specificity of targeting LSF by FQI1, results were compared after treatment with either FQI1 or siRNA targeting LSF. RESULTS: Highly similar cellular phenotypes are observed upon treatments with FQI1 and siRNA targeting LSF. Along with similar effects on two cellular biomarkers, inhibition of LSF activity by either mechanism induced a strong delay or arrest prior to metaphase as cells progressed through mitosis, with condensed, but unaligned, chromosomes. This mitotic disruption in both cases resulted in improper cellular division leading to multiple outcomes: multi-nucleation, apoptosis, and cellular senescence. CONCLUSIONS: These data strongly support that cellular phenotypes observed upon FQI1 treatment are due specifically to the loss of LSF activity. Specific inhibition of LSF by either small molecules or siRNA results in severe mitotic defects, leading to cell death or senescence - consequences that are desirable in combating cancer. Taken together, these findings confirm that LSF is a promising target for cancer treatment. Furthermore, this study provides further support for developing FQIs or other LSF inhibitory strategies as treatment for LSF-related cancers with high unmet medical needs."
SCOTT E SCHAUS,Factor quinolinone inhibitors alter cell morphology and motility by destabilizing interphase microtubules,"Factor quinolinone inhibitors are promising anti-cancer compounds, initially characterized as specific inhibitors of the oncogenic transcription factor LSF (TFCP2). These compounds exert anti-proliferative activity at least in part by disrupting mitotic spindles. Herein, we report additional interphase consequences of the initial lead compound, FQI1, in two telomerase immortalized cell lines. Within minutes of FQI1 addition, the microtubule network is disrupted, resulting in a substantial, although not complete, depletion of microtubules as evidenced both by microtubule sedimentation assays and microscopy. Surprisingly, this microtubule breakdown is quickly followed by an increase in tubulin acetylation in the remaining microtubules. The sudden breakdown and partial depolymerization of the microtubule network precedes FQI1-induced morphological changes. These involve rapid reduction of cell spreading of interphase fetal hepatocytes and increase in circularity of retinal pigment epithelial cells. Microtubule depolymerization gives rise to FH-B cell compaction, as pretreatment with taxol prevents this morphological change. Finally, FQI1 decreases the rate and range of locomotion of interphase cells, supporting an impact of FQI1-induced microtubule breakdown on cell motility. Taken together, our results show that FQI1 interferes with microtubule-associated functions in interphase, specifically cell morphology and motility."
SCOTT E SCHAUS,Dihydropyrimidine-thiones and clioquinol synergize to target beta-amyloid cellular pathologies through a metal-dependent mechanism,"The lack of therapies for neurodegenerative diseases arises from our incomplete understanding of their underlying cellular toxicities and the limited number of predictive model systems. It is critical that we develop approaches to identify novel targets and lead compounds. Here, a phenotypic screen of yeast proteinopathy models identified dihydropyrimidine-thiones (DHPM-thiones) that selectively rescued the toxicity caused by β-amyloid (Aβ), the peptide implicated in Alzheimer’s disease. Rescue of Aβ toxicity by DHPM-thiones occurred through a metal-dependent mechanism of action. The bioactivity was distinct, however, from that of the 8-hydroxyquinoline clioquinol (CQ). These structurally dissimilar compounds strongly synergized at concentrations otherwise not competent to reduce toxicity. Cotreatment ameliorated Aβ toxicity by reducing Aβ levels and restoring functional vesicle trafficking. Notably, these low doses significantly reduced deleterious off-target effects caused by CQ on mitochondria at higher concentrations. Both single and combinatorial treatments also reduced death of neurons expressing Aβ in a nematode, indicating that DHPM-thiones target a conserved protective mechanism. Furthermore, this conserved activity suggests that expression of the Aβ peptide causes similar cellular pathologies from yeast to neurons. Our identification of a new cytoprotective scaffold that requires metal-binding underscores the critical role of metal phenomenology in mediating Aβ toxicity. Additionally, our findings demonstrate the valuable potential of synergistic compounds to enhance on-target activities, while mitigating deleterious off-target effects. The identification and prosecution of synergistic compounds could prove useful for developing AD therapeutics where combination therapies may be required to antagonize diverse pathologies."
SCOTT E SCHAUS,Transcription factor LSF-DNMT1 complex dissociation by FQI1 leads to aberrant DNA methylation and gene expression,"The transcription factor LSF is highly expressed in hepatocellular carcinoma (HCC) and promotes oncogenesis. Factor quinolinone inhibitor 1 (FQI1), inhibits LSF DNA-binding activity and exerts anti-proliferative activity. Here, we show that LSF binds directly to the maintenance DNA (cytosine-5) methyltransferase 1 (DNMT1) and its accessory protein UHRF1 both in vivo and in vitro. Binding of LSF to DNMT1 stimulated DNMT1 activity and FQI1 negated the methyltransferase activation. Addition of FQI1 to the cell culture disrupted LSF bound DNMT1 and UHRF1 complexes, resulting in global aberrant CpG methylation. Differentially methylated regions (DMR) containing at least 3 CpGs, were significantly altered by FQI1 compared to control cells. The DMRs were mostly concentrated in CpG islands, proximal to transcription start sites, and in introns and known genes. These DMRs represented both hypo and hypermethylation, correlating with altered gene expression. FQI1 treatment elicits a cascade of effects promoting altered cell cycle progression. These findings demonstrate a novel mechanism of FQI1 mediated alteration of the epigenome by DNMT1-LSF complex disruption, leading to aberrant DNA methylation and gene expression."
SCOTT E SCHAUS,Small molecule inhibitors of Late SV40 Factor (LSF) abrogate hepatocellular carcinoma (HCC): evaluation using an endogenous HCC model,"Hepatocellular carcinoma (HCC) is a lethal malignancy with high mortality and poor prognosis. Oncogenic transcription factor Late SV40 Factor (LSF) plays an important role in promoting HCC. A small molecule inhibitor of LSF, Factor Quinolinone Inhibitor 1 (FQI1), significantly inhibited human HCC xenografts in nude mice without harming normal cells. Here we evaluated the efficacy of FQI1 and another inhibitor, FQI2, in inhibiting endogenous hepatocarcinogenesis. HCC was induced in a transgenic mouse with hepatocyte-specific overexpression of c-myc (Alb/c-myc) by injecting N-nitrosodiethylamine (DEN) followed by FQI1 or FQI2 treatment after tumor development. LSF inhibitors markedly decreased tumor burden in Alb/c-myc mice with a corresponding decrease in proliferation and angiogenesis. Interestingly, in vitro treatment of human HCC cells with LSF inhibitors resulted in mitotic arrest with an accompanying increase in CyclinB1. Inhibition of CyclinB1 induction by Cycloheximide or CDK1 activity by Roscovitine significantly prevented FQI-induced mitotic arrest. A significant induction of apoptosis was also observed upon treatment with FQI. These effects of LSF inhibition, mitotic arrest and induction of apoptosis by FQI1s provide multiple avenues by which these inhibitors eliminate HCC cells. LSF inhibitors might be highly potent and effective therapeutics for HCC either alone or in combination with currently existing therapies."
SCOTT E SCHAUS,"Enantioselective multicomponent condensation reactions of phenols, aldehydes, and boronates catalyzed by chiral biphenols","Chiral diols and biphenols catalyze the multicomponent condensation reaction of phenols, aldehydes, and alkenyl or aryl boronates. The condensation products are formed in good yields and enantioselectivities. The reaction proceeds via an initial Friedel-Crafts alkylation of the aldehyde and phenol to yield an ortho-quinone methide that undergoes an enantioselective boronate addition. A cyclization pathway was discovered while exploring the scope of the reaction that provides access to chiral 2,4-diaryl chroman products, the core of which is a structural motif found in natural products."
SCOTT E SCHAUS,"Fluorescent dendritic micro-hydrogels: synthesis, analysis and use in single-cell detection","Hydrogels are of keen interest for a wide range of medical and biotechnological applications including as 3D substrate structures for the detection of proteins, nucleic acids, and cells. Hydrogel parameters such as polymer wt % and crosslink density are typically altered for a specific application; now, fluorescence can be incorporated into such criteria by specific macromonomer selection. Intrinsic fluorescence was observed at λmax 445 nm from hydrogels polymerized from lysine and aldehyde- terminated poly(ethylene glycol) macromonomers upon excitation with visible light. The hydrogel’s photochemical properties are consistent with formation of a nitrone functionality. Printed hydrogels of 150 μm were used to detect individual cell adherence via a decreased in fluorescence. The use of such intrinsically fluorescent hydrogels as a platform for cell sorting and detection expands the current repertoire of tools available."
SCOTT E SCHAUS,Multicomponent condensation reactions via ortho-Quinone Methides,"Iron(III) salts promote the condensation of aldehydes or acetals with electron-rich phenols to generate ortho-quinone methides that undergo Diels-Alder condensations with alkenes. The reaction sequence occurs in a single vessel to afford benzopyrans in up to 95% yield. The reaction was discovered while investigating a two-component strategy using 2-(hydroxy(phenyl)methyl)phenols to access the desired ortho-quinone methide in a Diels-Alder condensation. The two-component condensation also afforded the corresponding benzopyran products in yields up to 97%. Taken together, the two- and three-component strategies using ortho-quinone methide intermediates provide efficient access to benzopyrans in good yields and selectivities."
MUHAMMAD H ZAMAN,Compressive remodeling alters fluid transport properties of collagen networks - implications for tumor growth,"Biomechanical alterations to the tumor microenvironment include accumulation of solid stresses, extracellular matrix (ECM) stiffening and increased fluid pressure in both interstitial and peri-tumoral spaces. The relationship between interstitial fluid pressurization and ECM remodeling in vascularized tumors is well characterized, while earlier biomechanical changes occurring during avascular tumor growth within the peri-tumoral ECM remain poorly understood. Type I collagen, the primary fibrous ECM constituent, bears load in tension while it buckles under compression. We hypothesized that tumor-generated compressive forces cause collagen remodeling via densification which in turn creates a barrier to convective fluid transport and may play a role in tumor progression and malignancy. To better understand this process, we characterized the structure-function relationship of collagen networks under compression both experimentally and computationally. Here we show that growth of epithelial cancers induces compressive remodeling of the ECM, documented in the literature as a TACS-2 phenotype, which represents a localized densification and tangential alignment of peri-tumoral collagen. Such compressive remodeling is caused by the unique features of collagen network mechanics, such as fiber buckling and cross-link rupture, and reduces the overall hydraulic permeability of the matrix."
MARTIN SCHMALTZ,Cannibal dark matter and large scale structure,"Cannibals are dark matter particles with a scattering process that allows three particles to annihilate to two. This exothermic process keeps the gas of the remaining particles warm long after they become nonrelativistic. A cannibalizing dark sector which is decoupled from the standard model naturally arises from a pure-glue confining hidden sector. It has an effective field theory description with a single massive interacting real scalar field, the lightest glueball. Since warm dark matter strongly suppresses the growth of structure, cannibals cannot be all of the dark matter. Thus, we propose a scenario where most dark matter is noninteracting and cold but about 1 percent is cannibalistic. We review the cannibals’ unusual scaling of the temperature and energy and number densities with redshift and generalize the equations for the growth of matter density perturbations to the case of cannibals. We solve the equations numerically to predict the scaling of the Hubble parameter and the characteristic shape of the linear matter power spectrum as a function of model parameters. Our results may have implications for the σ8 and H0 problems."
MARTIN SCHMALTZ,A portalino to the dark sector,"“Portal” models that connect the Standard Model to a Dark Sector allow for a wide variety of scenarios beyond the simplest WIMP models. Kinetic mixing of gauge fields in particular has allowed a broad range of new ideas. However, the models that evade CMB constraints are often non-generic, with new mass scales and operators to split states and suppress indirect detection signals. Models with a “portalino”, a neutral fermion that marries a linear combination of a standard model neutrino and dark sector fermion and carries a conserved quantum number, can be simpler. This is especially interesting for interacting dark sectors; then the unmarried linear combination which we identify as the standard model neutrino inherits these interactions too, and provides a new, effective interaction between the dark sector and the standard model. These interactions can be simple Z′ type interactions or lepton-flavor changing. Dark matter freezes out into neutrinos, thereby evading CMB constraints, and conventional direct detection signals are largely absent. The model offers different signals, however. The “portalino” mechanism itself predicts small corrections to the standard model neutrino couplings as well as the possibility of discovering the portalino particle in collider experiments. Possible cosmological and astroparticle signatures include monochromatic neutrino signals from annihilation, spectral features in high energy CR neutrinos as well as conventional signals of additional light species and dark matter interactions."
MARTIN SCHMALTZ,The leptoquark Hunter's guide: large coupling,"Leptoquarks have recently received much attention especially because they may provide an explanation to the 𝑅_𝐷(∗)and𝑅_𝐾(∗) anomalies in rare B meson decays. In a previous paper we proposed a systematic search strategy for all possible leptoquark flavors by focusing on leptoquark pair production. In this paper, we extend this strategy to large (order unity) leptoquark couplings which offer new search opportunities: single leptoquark production and t-channel leptoquark exchange with dilepton final states. We discuss the unique features of the different search channels and show that they cover complementary regions of parameter space. We collect and update all currently available bounds for the different flavor final states from LHC searches and from atomic parity violation measurements. As an application of our analysis, we find that current limits do not exclude a leptoquark explanation of the B physics anomalies but that the high luminosity run of the LHC will reach the most interesting parameter space."
MARTIN SCHMALTZ,A step in understanding the Hubble tension,"As cosmological data have improved, tensions have arisen. One such tension is the difference between the locally measured Hubble constant H 0 and the value inferred from the cosmic microwave background (CMB). Interacting radiation has been suggested as a solution, but studies show that conventional models are precluded by high- ℓ CMB polarization data. It seems at least plausible that a solution may be provided by related models that distinguish between high- and low- ℓ multipoles. When interactions of strongly-coupled radiation are mediated by a force carrier that becomes nonrelativistic, the dark radiation undergoes a “step” in which its relative energy density increases as the mediator deposits its entropy into the lighter species. If this transition occurs while CMB-observable modes are inside the horizon, high- and low- ℓ peaks are impacted differently, corresponding to modes that enter the horizon before or after the step. These dynamics are naturally packaged into the simplest supersymmetric theory, the Wess-Zumino model, with the mass of the scalar mediator near the eV scale. We investigate the cosmological signatures of such Wess-Zumino dark radiation (WZDR) and find that it provides an improved fit to the CMB alone, favoring larger values of H 0 . If supernovae measurements from the SH0ES Collaboration are also included in the analysis, the inferred value of H 0 is yet larger, but the preference for dark radiation and the location of the transition is left nearly unchanged. Utilizing a standardized set of measures, we compare to other models and find that WZDR is among the most successful at addressing the H 0 tension and is the best of those with a Lagrangian formulation."
MARTIN SCHMALTZ,Non-Abelian dark matter and dark radiation,"We propose a new class of dark matter models with unusual phenomenology. What is ordinary about our models is that dark matter particles are weakly interacting massive particles; they are weakly coupled to the standard model and have weak scale masses. What is unusual is that they come in multiplets of a new dark non-Abelian gauge group with milliweak coupling. The massless dark gluons of this dark gauge group contribute to the energy density of the Universe as a form of weakly self-interacting dark radiation. In this paper we explore the consequences of having (i) dark matter in multiplets, (ii) self-interacting dark radiation, and (iii) dark matter which is weakly coupled to dark radiation. We find that (i) dark matter cross sections are modified by multiplicity factors which have significant consequences for collider searches and indirect detection, and (ii) dark gluons have thermal abundances which affect the cosmic microwave background (CMB) as dark radiation. Unlike additional massless neutrino species the dark gluons are interacting and have vanishing viscosity and (iii) the coupling of dark radiation to dark matter represents a new mechanism for damping the large scale structure power spectrum. A combination of additional radiation and slightly damped structure is interesting because it can remove tensions between global ΛCDM fits from the CMB and direct measurements of the Hubble expansion rate (H0) and large scale structure (σ8)."
MARTIN SCHMALTZ,Evidence for dark matter interactions in cosmological precision data?,"We study a two-parameter extension of the cosmological standard model ΛCDM in which cold dark matter interacts with a new form of dark radiation. The two parameters correspond to the energy density in the dark radiation fluid ΔNfluid and the interaction strength between dark matter and dark radiation. The interactions give rise to a very weak ``dark matter drag'' which damps the growth of matter density perturbations throughout radiation domination, allowing to reconcile the tension between predictions of large scale structure from the CMB and direct measurements of σ8. We perform a precision fit to Planck CMB data, BAO, large scale structure, and direct measurements of the expansion rate of the universe today. Our model lowers the χ-squared relative to ΛCDM by about 12, corresponding to a preference for non-zero dark matter drag by more than 3σ. Particle physics models which naturally produce a dark matter drag of the required form include the recently proposed non-Abelian dark matter model in which the dark radiation corresponds to massless dark gluons."
MARTIN SCHMALTZ,Interacting dark sector and precision cosmology,"We consider a recently proposed model in which dark matter interacts with a thermal background of dark radiation. Dark radiation consists of relativistic degrees of freedom which allow larger values of the expansion rate of the universe today to be consistent with CMB data (H0-problem). Scattering between dark matter and radiation suppresses the matter power spectrum at small scales and can explain the apparent discrepancies between ΛCDM predictions of the matter power spectrum and direct measurements of Large Scale Structure LSS (σ8-problem). We go beyond previous work in two ways: 1. we enlarge the parameter space of our previous model and allow for an arbitrary fraction of the dark matter to be interacting and 2. we update the data sets used in our fits, most importantly we include LSS data with full k-dependence to explore the sensitivity of current data to the shape of the matter power spectrum. We find that LSS data prefer models with overall suppressed matter clustering due to dark matter - dark radiation interactions over ΛCDM at 3–4 σ. However recent weak lensing measurements of the power spectrum are not yet precise enough to clearly distinguish two limits of the model with different predicted shapes for the linear matter power spectrum. In two appendices we give a derivation of the coupled dark matter and dark radiation perturbation equations from the Boltzmann equation in order to clarify a confusion in the recent literature, and we derive analytic approximations to the solutions of the perturbation equations in the two physically interesting limits of all dark matter weakly interacting or a small fraction of dark matter strongly interacting."
MARTIN SCHMALTZ,The leptoquark Hunter’s guide: pair production,"Leptoquarks occur in many new physics scenarios and could be the next big discovery at the LHC. The purpose of this paper is to point out that a model-independent search strategy covering all possible leptoquarks is possible and has not yet been fully exploited. To be systematic we organize the possible leptoquark final states according to a leptoquark matrix with entries corresponding to nine experimentally distinguishable leptoquark decays: any of {light-jet, b-jet, top} with any of {neutrino, e/μ, τ}. The 9 possibilities can be explored in a largely model-independent fashion with pair-production of leptoquarks at the LHC. We review the status of experimental searches for the 9 components of the leptoquark matrix, pointing out which 3 have not been adequately covered. We plead that experimenters publish bounds on leptoquark cross sections as functions of mass for as wide a range of leptoquark masses as possible. Such bounds are essential for reliable recasts to general leptoquark models. To demonstrate the utility of the leptoquark matrix approach we collect and summarize searches with the same final states as leptoquark pair production and use them to derive bounds on a complete set of Minimal Leptoquark models which span all possible flavor and gauge representations for scalar and vector leptoquarks."
MARTIN SCHMALTZ,A step in understanding the S8 tension,
MARTIN SCHMALTZ,"Cosmology intertwined: a review of the particle physics, astrophysics, and cosmology associated with the cosmological tensions and anomalies",
MARTIN SCHMALTZ,Dark radiation from neutrino mixing after Big Bang Nucleosynthesis,
MARTIN SCHMALTZ,A step in understanding the 𝑆₈ tension,"Models of dark sectors with a mass threshold can have important cosmological signatures. If, in the era prior to recombination, a relativistic species becomes nonrelativistic and is then depopulated in equilibrium, there can be measurable impacts on the cosmic microwave background as the entropy is transferred to lighter relativistic particles. In particular, if this “step” occurs near z∼20, 000, the model can naturally accommodate larger values of H₀. If this stepped radiation is additionally coupled to dark matter, there can be a meaningful impact on the matter power spectrum as dark matter can be coupled via a species that becomes nonrelativistic and depleted. This can naturally lead to suppressed power at scales inside the sound horizon before the step, while leaving conventional cold dark matter signatures for power outside the sound horizon. We study these effects and show such models can naturally provide lower values of 𝑆₈ than scenarios without a step. This suggests these models may provide an interesting framework to address the 𝑆₈ tension, both in concert with the H₀ tension and without."
MARTIN SCHMALTZ,A model for the LHC diboson excess,
PAUL E BARBONE,Shear waves in prestrained poroelastic media,"Shear wave elastography measures shear wave speed in soft tissues for diagnostic purposes. In [1], shear wave speed was shown to depend on prestrain, but not necessarily prestress, in a perfused canine liver. We model this phenomenon by examining incremental waves in a pressurized poroelastic medium with incompressible phases. The analysis suggests novel restrictions on the strain energy functions for soft tissues."
PAUL E BARBONE,Elastic waves in a soft electrically conducting solid in a strong magnetic field,"Shear wave motion of a soft, electrically-conducting solid in the presence of a strong magnetic field excites eddy currents in the solid. These, in turn, give rise to Lorentz forces that resist the wave motion. We derive a mathematical model for linear elastic wave propagation in a soft electrically conducting solid in the presence of a strong magnetic field. The model reduces to an effective anisotropic dissipation term resembling an anisotropic viscous foundation. The application to magnetic resonance elastography, which uses strong magnetic fields to measure shear wave speed in soft tissues for diagnostic purposes, is considered."
RICHARD PRIMACK,"Bostonia: 1999-2000, no. 1-4",
RICHARD PRIMACK,Confronting ethical challenges in long-term research programs in the tropics,"Ecologists and conservation biologists conducting long-term research programs in the tropics must confront serious ethical challenges that revolve around economic inequalities, cultural differences, supporting the local communities as much as possible, and sharing the knowledge produced by the research. In this collective article, researchers share their experiences and perspectives in dealing with the ethical issues that arise during research activities and cannot be ignored."
RICHARD PRIMACK,Effects of warming temperatures on winning times in the Boston Marathon,"It is not known whether global warming will affect winning times in endurance events, and counterbalance improvements in race performances that have occurred over the past century. We examined a time series (1933–2004) from the Boston Marathon to test for an effect of warming on winning times by men and women. We found that warmer temperatures and headwinds on the day of the race slow winning times. However, 1.6°C warming in annual temperatures in Boston between 1933 and 2004 did not consistently slow winning times because of high variability in temperatures on race day. Starting times for the race changed to earlier in the day beginning in 2006, making it difficult to anticipate effects of future warming on winning times. However, our models indicate that if race starting times had not changed and average race day temperatures had warmed by 0.058°C/yr, a high-end estimate, we would have had a 95% chance of detecting a consistent slowing of winning marathon times by 2100. If average race day temperatures had warmed by 0.028°C/yr, a mid-range estimate, we would have had a 64% chance of detecting a consistent slowing of winning times by 2100."
RICHARD PRIMACK,Favorable Climate Change Response Explains Non-Native Species' Success in Thoreau's Woods,"Invasive species have tremendous detrimental ecological and economic impacts. Climate change may exacerbate species invasions across communities if non-native species are better able to respond to climate changes than native species. Recent evidence indicates that species that respond to climate change by adjusting their phenology (i.e., the timing of seasonal activities, such as flowering) have historically increased in abundance. The extent to which non-native species success is similarly linked to a favorable climate change response, however, remains untested. We analyzed a dataset initiated by the conservationist Henry David Thoreau that documents the long-term phenological response of native and non-native plant species over the last 150 years from Concord, Massachusetts (USA). Our results demonstrate that non-native species, and invasive species in particular, have been far better able to respond to recent climate change by adjusting their flowering time. This demonstrates that climate change has likely played, and may continue to play, an important role in facilitating non-native species naturalization and invasion at the community level."
RICHARD PRIMACK,The growing and vital role of botanical gardens in climate change research.,"Botanical gardens make unique contributions to climate change research, conservation, and public engagement. They host unique resources, including diverse collections of plant species growing in natural conditions, historical records, and expert staff, and attract large numbers of visitors and volunteers. Networks of botanical gardens spanning biomes and continents can expand the value of these resources. Over the past decade, research at botanical gardens has advanced our understanding of climate change impacts on plant phenology, physiology, anatomy, and conservation. For example, researchers have utilized botanical garden networks to assess anatomical and functional traits associated with phenological responses to climate change. New methods have enhanced the pace and impact of this research, including phylogenetic and comparative methods, and online databases of herbarium specimens and photographs that allow studies to expand geographically, temporally, and taxonomically in scope. Botanical gardens have grown their community and citizen science programs, informing the public about climate change and monitoring plants more intensively than is possible with garden staff alone. Despite these advances, botanical gardens are still underutilized in climate change research. To address this, we review recent progress and describe promising future directions for research and public engagement at botanical gardens."
RICHARD PRIMACK,Siberian plants shift their phenology in response to climate change,"Siberia has undergone dramatic climatic changes due to global warming in recent decades. Yet, the ecological responses to these climatic changes are still poorly understood due to a lack of data. Here, we use a unique data set from the Russian 'Chronicles of Nature' network to analyse the long-term (1976-2018) phenological shifts in leaf out, flowering, fruiting and senescence of 67 common Siberian plant species. We find that Siberian boreal forest plants advanced their early season (leaf out and flowering) and mid-season (fruiting) phenology by -2.2, -0.7 and -1.6 days/decade, and delayed the onset of senescence by 1.6 days/decade during this period. These mean values, however, are subject to substantial intraspecific variability, which is partly explained by the plants' growth forms. Trees and shrubs advanced leaf out and flowering (-3.1 and -3.3. days/decade) faster than herbs (-1 day/decade), presumably due to the more direct exposure of leaf and flower buds to ambient air for the woody vegetation. For senescence, we detected a reverse pattern: stronger delays in herbs (2.1 days/decade) than in woody plants (1.0-1.2 days/decade), presumably due to the stronger effects of autumn frosts on the leaves of herbs. Interestingly, the timing of fruiting in all four growth forms advanced at similar paces, from 1.4 days/decade in shrubs to 1.7 days/decade in trees and herbs. Our findings point to a strong, yet heterogeneous, response of Siberian plant phenology to recent global warming. Furthermore, the results highlight that species- and growth form-specific differences among study species could be used to identify plants particularly at risk of decline due to their low adaptive capacity or a loss of synchronization with important interaction partners."
RICHARD PRIMACK,The conservation and ecological impacts of the COVID-19 pandemic,
RICHARD PRIMACK,Biodiversity science blossoms in China,"[Over the past 35 years, China has been transformed by an economic miracle unlike anything seen in the history of the world. Hundreds of millions of people have emerged from rural poverty, cities have been re-built, cutting-edge industries have been established and a modern transportation network now knits together the country. This transformation has come at a significant cost to the environment, in terms of air pollution, water pollution, the loss and fragmentation of habitats, and threats of species extinction. Yet, as the review by Mi et al. (2021) points out, China is also now emerging as a leader in biodiversity conservation and research [1]. This focus on finding a balance between biodiversity conservation and socio-economic development, sometimes referred to as ‘ecological civilization,’ is a high priority in China because of the dependence of its enormous human population on ecosystem services and because of its astonishing richness of species.]"
RICHARD PRIMACK,Women and Global South strikingly underrepresented among top‐publishing ecologists,"The global scientific community has become increasingly diverse over recent decades, but is this ongoing development also reflected among top-publishing authors and potential scientific leaders? We surveyed 13 leading journals in ecology, evolution, and conservation to investigate the diversity of the 100 top-publishing authors in each journal between 1945 and 2019. Out of 1051 individual top-publishing authors, only 11% are women. The United States, the United Kingdom, Australia, Germany, and Canada account for more than 75% of top-publishing authors, while countries of the Global South (as well as Russia, Japan, and South Korea) were strikingly underrepresented. The number of top-publishing authors who are women and/or are from the Global South is increasing only slowly over time. We outline transformative actions that scientific communities can take to enhance diversity, equity and inclusion at author, leadership, and society level. The resulting promotion of scientific innovation and productivity is essential for the development of global solutions in conservation science."
RICHARD PRIMACK,Taking the pulse of Earth's tropical forests using networks of highly distributed plots,"Tropical forests are the most diverse and productive ecosystems on Earth. While better understanding of these forests is critical for our collective future, until quite recently efforts to measure and monitor them have been largely disconnected. Networking is essential to discover the answers to questions that transcend borders and the horizons of funding agencies. Here we show how a global community is responding to the challenges of tropical ecosystem research with diverse teams measuring forests tree-by-tree in thousands of long-term plots. We review the major scientific discoveries of this work and show how this process is changing tropical forest science. Our core approach involves linking long-term grassroots initiatives with standardized protocols and data management to generate robust scaled-up results. By connecting tropical researchers and elevating their status, our Social Research Network model recognises the key role of the data originator in scientific discovery. Conceived in 1999 with RAINFOR (South America), our permanent plot networks have been adapted to Africa (AfriTRON) and Southeast Asia (T-FORCES) and widely emulated worldwide. Now these multiple initiatives are integrated via ForestPlots.net cyber-infrastructure, linking colleagues from 54 countries across 24 plot networks. Collectively these are transforming understanding of tropical forests and their biospheric role. Together we have discovered how, where and why forest carbon and biodiversity are responding to climate change, and how they feedback on it. This long-term pan-tropical collaboration has revealed a large long-term carbon sink and its trends, as well as making clear which drivers are most important, which forest processes are affected, where they are changing, what the lags are, and the likely future responses of tropical forests as the climate continues to change. By leveraging a remarkably old technology, plot networks are sparking a very modern revolution in tropical forest science. In the future, humanity can benefit greatly by nurturing the grassroots communities now collectively capable of generating unique, long-term understanding of Earth's most precious forests."
THOMAS M BANIA,Green Bank Telescope observations of ³He⁺: planetary nebulae,"We use the Green Bank Telescope to search for ³He⁺ emission from a sample of four Galactic planetary nebulae: NGC 3242, NGC 6543, NGC 6826, and NGC 7009. During the era of primordial nucleosynthesis the light elements ²H, ³He, ⁴He, and ⁷Li were produced in significant amounts and these abundances have since been modified primarily by stars. Observations of ³He⁺ in H II regions located throughout the Milky Way disk reveal very little variation in the ³He/H abundance ratio -- the ""³He Plateau"" -- indicating that the net effect of ³He production in stars is negligible. This is in contrast to much higher ³He/H abundance ratios reported for some planetary nebulae. This discrepancy is known as the ""³He Problem"". We use radio recombination lines observed simultaneously with the ³He⁺ transition to make a robust assessment of the spectral sensitivity that these observations achieve. We detect spectral lines at ∼ 1 -- 2 mK intensities, but at these levels instrumental effects compromise our ability to measure accurate spectral line parameters. We do not confirm reports of previous detections of ³He⁺ in NGC 3242 nor do we detect ³He⁺ emission from any of our sources. This result calls into question all reported detections of ³He⁺ emission from any planetary nebula. The ³He/H abundance upper limit we derive here for NGC 3242 is inconsistent with standard stellar production of ³He and thus requires that some type of extra mixing process operates in low-mass stars."
THOMAS M BANIA,Green Bank Telescope Observations of ³He⁺: H II Regions,"During the era of primordial nucleosynthesis, the light elements ^2H, ^3He, ^4He, and ^7Li were produced in significant amounts, and these abundances have since been modified primarily by stars. Observations of ^3He⁺ in H ii regions located throughout the Milky Way disk reveal very little variation in the ^3He/H abundance ratio—the ""^3He Plateau""—indicating that the net effect of ^3He production in stars is negligible. This is in contrast to much higher ^3He/H abundance ratios found in some planetary nebulae. This discrepancy is known as the ""^3He Problem"". Stellar evolution models that include thermohaline mixing can resolve the ^3He Problem by drastically reducing the net ^3He production in most stars. These models predict a small negative ^3He/H abundance gradient across the Galactic disk. Here we use the Green Bank Telescope to observe ^3He⁺ in five H ii regions with high accuracy to confirm the predictions of stellar and Galactic chemical evolution models that include thermohaline mixing. We detect ^3He⁺ in all the sources and derive the ^3He⁺ abundance ratio using model H ii regions and the numerical radiative transfer code NEBULA. The over 35 radio recombination lines (RRLs) that are simultaneously observed, together with the ^3He⁺ transition provide stringent constraints for these models. We apply an ionization correction using observations of ^4He RRLs. We determine a ^3He/H abundance gradient as a function of Galactocentric radius of −(0.116 ± 0.022) × 10⁻⁵ kpc⁻¹, consistent with stellar evolution models including thermohaline mixing that predict a small net contribution of ^3He from solar mass stars."
THOMAS M BANIA,The GBT diffuse ionized gas survey: tracing the diffuse ionized gas around the Giant H𝖨𝖨 region W43,"The Green Bank Telescope Diffuse Ionized Gas Survey (GDIGS) is a fully sampled radio recombination line (RRL) survey of the inner Galaxy at C-band (4–8 GHz). We average together ∼15 Hnα RRLs within the receiver bandpass to improve the spectral signal-to-noise ratio. The average beam size for the RRL observations at these frequencies is ∼2′. We grid these data to have spatial and velocity spacings of 30″ and 0.5 kms^-, respectively. Here we discuss the first RRL data from GDIGS: a 6 deg2 area surrounding the Galactic H II region complex W43. We attempt to create a map devoid of emission from discrete H II regions and detect RRL emission from the diffuse ionized gas (DIG) across nearly the entire mapped area. We estimate the intensity of the DIG emission by a simple empirical model, taking only the H II region locations, angular sizes, and RRL intensities into account. The DIG emission is predominantly found at two distinct velocities: ∼40 and ∼100 kms^-1. While the 100 kms^-1 component is associated with W43 at a distance of ∼6 kpc, the origin of the 40 km^-1 component is less clear. Since the distribution of the 40 km^-1emission cannot be adequately explained by ionizing sources at the same velocity, we hypothesize that the plasma at the two velocity components is interacting, placing the 40 kms^-1DIG at a similar distance as the 100 kms^-1 emission. We find a correlation between dust temperature and integrated RRL intensity, suggesting that the same radiation field that heats the dust also maintains the ionization of the DIG."
THOMAS M BANIA,A Green Bank Telescope survey of large Galactic H II regions,"As part of our ongoing HII Region Discovery Survey (HRDS), we report the Green Bank Telescope detection of 148 new angularly-large Galactic HII regions in radio recombination line (RRL) emission. Our targets are located at a declination greater than -45deg., which corresponds to 266deg. > l > -20deg. at b = 0deg. All sources were selected from the WISE Catalog of Galactic HII Regions, and have infrared angular diameters >260''. The Galactic distribution of these ""large"" HII regions is similar to that of the previously-known sample of Galactic HII regions. The large HII region RRL line width and peak line intensity distributions are skewed toward lower values compared with that of previous HRDS surveys. We discover 7 sources with extremely narrow RRLs <10 km/s. If half the line width is due to turbulence, these 7 sources have thermal plasma temperatures <1100 K. These temperatures are lower than any measured for Galactic HII regions, and the narrow line components may arise instead from partially ionized zones in the HII region photo-dissociation regions. We discover G039.515+00.511, one of the most luminous HII regions in the Galaxy. We also detect the RRL emission from three HII regions with diameters >100 pc, making them some of the physically largest known HII regions in the Galaxy. This survey completes the HRDS HII region census in the Northern sky, where we have discovered 887 HII regions and more than doubled the previously-known census of Galactic HII regions."
THOMAS M BANIA,The GBT diffuse ionized gas survey (GDIGS): survey overview and first data release,"The Green Bank Telescope (GBT) Diffuse Ionized Gas Survey (GDIGS) traces ionized gas in the Galactic midplane by measuring 4–8 GHz radio recombination line (RRL) emission. The nominal survey zone is 32.°3 > ℓ > − 5°, ∣b∣ < 0.°5, but coverage extends above and below the plane in select fields and additionally includes the areas around W47 (ℓ ≃ 37.°5) and W49 (ℓ ≃ 43°). GDIGS simultaneously observes 22 Hnα (15 usable), 25 Hnβ (18 usable), and 8 Hnγ RRLs (all usable), as well as multiple molecular line transitions (including those of H_2^13CO, H2CO, and CH3OH). Here, we describe the GDIGS survey parameters and characterize the RRL data, focusing primarily on the Hnα data. We produce sensitive data cubes by averaging the usable RRLs, after first smoothing to a common spectral resolution of 0.5 km s−1 and a spatial resolution of 2.′65 for Hnα, 2.′62 for Hnβ, and 2.′09 for Hnγ. The average spectral noise per spaxel in the Hnα data cubes is ∼10 mK (∼5 mJy beam−1). This sensitivity allows GDIGS to detect RRLs from plasma throughout the inner Galaxy. The GDIGS Hnα data are sensitive to emission measures EM ≳ 1100 cm^−6 pc, which corresponds to a mean electron density ⟨n_e⟩ ≳ 30 cm^−3 for a 1 pc path length or ⟨n_e〉 ≳ 1 cm^−3 for a 1 kpc path length."
THOMAS M BANIA,A Galactic plane defined by the Milky Way H II region distribution,"We develop a framework for a new definition of the Galactic midplane, allowing for tilt theta_tilt; rotation about Galactic azimuth 90°) and roll (theta_roll; rotation about Galactic azimuth 0°) of the midplane with respect to the current definition. Derivation of the tilt and roll angles also determines the solar height above the midplane. Here we use nebulae from the Wide-field Infrared Survey Explorer (WISE) Catalog of Galactic H ii Regions to define the Galactic high-mass star formation (HMSF) midplane. We analyze various subsamples of the WISE catalog and find that all have Galactic latitude scale heights near 0.°30 and z-distribution scale heights near 30 pc. The vertical distribution for small (presumably young) H ii regions is narrower than that of larger (presumably old) H ii regions (∼25 p versus ∼40 pc ), implying that the larger regions have migrated further from their birth sites. For all H ii region subsamples and for a variety of fitting methodologies, we find that the HMSF midplane is not significantly tilted or rolled with respect to the currently defined midplane, and, therefore, the Sun is near to the HMSF midplane. These results are consistent with other studies of HMSF, but are inconsistent with many stellar studies, perhaps because of asymmetries in the stellar distribution near the Sun. Our results are sensitive to latitude restrictions and also to the completeness of the sample, indicating that similar analyses cannot be done accurately with less complete samples. The midplane framework we develop can be used for any future sample of Galactic objects to redefine the midplane."
THOMAS M BANIA,Hydrogen radio recombination line emission from M51 and NGC 628,"We report the discovery of hydrogen radio recombination line (RRL) emission from two galaxies with star formation rates (SFRs) similar to that of the Milky Way: M51 and NGC628. We use the Green Bank Telescope (GBT) to measure ∼15 Hnɑ recombination transitions simultaneously and average these data to improve our spectral signal-to-noise ratio. We show that our data can be used to estimate the total ionizing photon flux of these two sources, and we derive their SFRs within the GBT beam: Ψ_OB = 3.46 M⊙ yr^-1 for M51 and Ψ_OB = 0.56 M⊙ yr^-1 for NGC628. Here, we demonstrate that it is possible to detect RRLs from normal galaxies that are not undergoing a starburst with current instrumentation and reasonable integration times (∼12 hr for each source). We also show that we can characterize the overall star-forming properties of M51 and NGC628, although the GBT beam cannot resolve individual HII region complexes. Our results suggest that future instruments, such as the Square Kilometre Array and the Next Generation Very Large Array, will be able to detect RRL emission from a multitude of Milky Way-like galaxies, making it possible to determine SFRs of normal galaxies unaffected by extinction and to measure global star formation properties in the local universe."
THOMAS M BANIA,Science with an ngVLA: radio recombination lines from HII regions,"The ngVLA will create a Galaxy-wide, volume-limited sample of HII regions; solve some long standing problems in the physics of HII regions; and provide an extinction-free star formation tracer in nearby galaxies."
THOMAS M BANIA,Metallicity structure in the Milky Way disk revealed by Galactic H II regions,"The metallicity structure of the Milky Way disk stems from the chemodynamical evolutionary history of the Galaxy. We use the National Radio Astronomy Observatory Karl G. Jansky Very Large Array to observe ∼8–10 GHz hydrogen radio recombination line and radio-continuum emission toward 82 Galactic H ii regions. We use these data to derive the electron temperatures and metallicities for these nebulae. Since collisionally excited lines from metals (e.g., oxygen, nitrogen) are the dominant cooling mechanism in H ii regions, the nebular metallicity can be inferred from the electron temperature. Including previous single-dish studies, there are now 167 nebulae with radio-determined electron temperature and either parallax or kinematic distance determinations. The interferometric electron temperatures are systematically 10% larger than those found in previous single-dish studies, likely due to incorrect data analysis strategies, optical depth effects, and/or the observation of different gas by the interferometer. By combining the interferometer and single-dish samples, we find an oxygen abundance gradient across the Milky Way disk with a slope of −0.052 ± 0.004 dex kpc−1. We also find significant azimuthal structure in the metallicity distribution. The slope of the oxygen gradient varies by a factor of ∼2 when Galactocentric azimuths near ∼30° are compared with those near ∼100°. This azimuthal structure is consistent with simulations of Galactic chemodynamical evolution influenced by spiral arms."
THOMAS M BANIA,Kinematic distances: a Monte Carlo method,"Distances to high-mass star-forming regions (HMSFRs) in the Milky Way are a crucial constraint on the structure of the Galaxy. Only kinematic distances are available for a majority of the HMSFRs in the Milky Way. Here, we compare the kinematic and parallax distances of 75 Galactic HMSFRs to assess the accuracy of kinematic distances. We derive the kinematic distances using three different methods: the traditional method using the Brand & Blitz rotation curve (Method A), the traditional method using the Reid et al. rotation curve and updated solar motion parameters (Method B), and a Monte Carlo technique (Method C). Methods B and C produce kinematic distances closest to the parallax distances, with median differences of 13% (0.43 kpc) and 17% (0.42 kpc), respectively. Except in the vicinity of the tangent point, the kinematic distance uncertainties derived by Method C are smaller than those of Methods A and B. In a large region of the Galaxy, the Method C kinematic distances constrain both the distances and the Galactocentric positions of HMSFRs more accurately than parallax distances. Beyond the tangent point along ℓ = 30°, for example, the Method C kinematic distance uncertainties reach a minimum of 10% of the parallax distance uncertainty at a distance of 14 kpc. We develop a prescription for deriving and applying the Method C kinematic distances and distance uncertainties. The code to generate the Method C kinematic distances is publicly available and may be utilized through an online tool."
THOMAS M BANIA,A VLA census of the Galactic H II region population,"The Milky Way contains thousands of H ii region candidates identified by their characteristic mid-infrared morphology, but lacking detections of ionized gas tracers such as radio continuum or radio recombination line emission. These targets thus remain unconfirmed as H ii regions. With only ∼2500 confirmed H ii regions in the Milky Way, Galactic surveys are deficient by several thousand nebulae when compared to external galaxies with similar star formation rates. Using sensitive 9 GHz radio continuum observations with the Karl G. Jansky Very Large Array, we explore a sample of H ii region candidates in order to set observational limits on the actual total population of Galactic H ii regions. We target all infrared–identified “radio-quiet” sources from the Wide-field Infrared Survey Explorer Catalog of Galactic H ii regions between 245° ≥ ℓ ≥ 90° with infrared diameters less than 80′′. We detect radio continuum emission from 50% of the targeted H ii region candidates, providing strong evidence that most of the radio-quiet candidates are bona fide H ii regions. We measure the peak and integrated radio flux densities and compare the inferred Lyman continuum fluxes using models of OB stars. We conclude that stars of approximately spectral type B2 and earlier are able to create H ii regions with similar infrared and radio continuum morphologies as the more luminous H ii regions created by O stars. From our 50% detection rate of “radio-quiet” sources, we set a lower limit of ∼7000 for the H ii region population of the Galaxy. Thus the vast majority of the Milky Way’s H ii regions remain to be discovered."
THOMAS M BANIA,The Galactic H II region luminosity function at radio and infrared wavelengths,"The Galactic H ii region luminosity function (LF) is an important metric for understanding global star formation properties of the Milky Way, but only a few studies have been done, and all use relatively small numbers of H ii regions. We use a sample of 797 first Galactic quadrant H ii regions compiled from the Wide-field Infrared Survey Explorer Catalog of Galactic H ii Regions to examine the form of the LF at multiple infrared and radio wavelengths. Our sample is statistically complete for all regions powered by single stars of type O9.5V and earlier. We fit the LF at each wavelength with single and double power laws. Averaging the results from all wavelengths, the mean of the best-fit single power-law index is 〈α〉 = −1.75 ± 0.01. The mean best-fit double power-law indices are 〈α 1〉 = −1.40 ± 0.03 and 〈α 2〉 = −2.33 ± 0.04. We conclude that neither a single nor a double power law is strongly favored over the other. The LFs show some variation when we separate the H ii region sample into subsets by heliocentric distance, physical size, Galactocentric radius, and location relative to the spiral arms, but blending individual H ii regions into larger complexes does not change the value of the power-law indices of the best-fit LF models. The consistency of the power-law indices across multiple wavelengths suggests that the LF is independent of wavelength. This implies that infrared and radio tracers can be employed in place of Hα."
UDAY PAL,Co-infiltration of nickel and mixed conducting Gd0.1Ce0.9O2−δ and La0.6Sr0.3Ni0.15Cr0.85O3−δ phases in Ni-YSZ anodes for improved stability and performance,
UDAY PAL,Effect of anodic current density on the spreading of infiltrated nickel nanoparticles in nickel-yttria stabilized zirconia cermet anodes,
UDAY PAL,"Chromium poisoning effects on performance of (La, Sr) MnO3-based cathode in anode-supported solid oxide fuel cells","Chromium (Cr) vapor species from chromia-forming alloy interconnects are known to cause cathode performance degradation in solid oxide fuel cells (SOFCs). To understand the impact of Cr-poisoning on cathode performance, it is important to determine its effects on different cathode polarization losses. In this study, anode-supported SOFCs, with a (La,Sr)MnO3 (LSM) + yttria-stabilized zirconia (YSZ) cathode active layer and a LSM cathode current collector layer were fabricated. At 800°C, cells were electrochemically tested in direct contact with Crofer22H meshes, under different cathode atmospheres (dry air or humidified air) and current conditions (open-circuit or galvanostatic). Significant performance degradation was observed when cell was tested under galvanostatic condition (0.5 A/cm2), which was not the case under open-circuit condition. Humidity was found to accelerate the performance degradation. By curve-fitting the experimentally measured current-voltage traces to a polarization model, the effects of Cr-poisoning on different cathodic polarization losses were estimated. It is found that, under normal operating conditions, increase of activation polarization dominates the cathode performance degradation. Microstructures of the cathodes were characterized and Cr-containing deposits were identified. Higher concentrations of Cr-containing deposits were found at the cathode/electrolyte interface and the amounts directly correlated with the cell performance degradations."
UDAY PAL,Quantifying percolated triple phase boundary density and its effects on anodic polarization in Ni-infiltrated Ni/YSZ SOFC snodes,"Increasing the density of percolated triple phase boundaries (TPBs) by infiltrating nanoscale electrocatalysts can improve the performance of solid oxide fuel cell (SOFC) anodes. However, the complex microstructure of these infiltrated nanocatalysts creates challenges in quantifying their role in anode performance improvements. In this research, scanning electron microscopy of fractured cross-sections of a Ni-nanocatalyst infiltrated anodic symmetric cell along with three-dimensional (3-D) reconstruction of the same anode have been used to quantify the changes in percolated TPB densities due to infiltration. This change in percolated TPB density has been compared to the improvement in anode activation polarization resistance measured by electrochemical impedance spectroscopy (EIS). It was found that increased TPB densities only partially accounted for the measured performance improvement. Distribution of relaxation times (DRT) analyses showed that a reduction in the time constants of the catalytic processes in the anode also play a role, suggesting that the added nanoscale percolated TPB boundaries are more electrochemically active as compared to the cermet TPB boundaries."
UDAY PAL,X-ray and molecular dynamics study of the temperature-dependent structure of FLiNaK,
TERRY EVERSON,Beyond rehearsal: before and after,"A discussion of pre-rehearsal preparation and post-rehearsal presentation of chamber music. To be published in quarterly ""Chamber Music Connection"" series edited by Marc Reese."
THERESA D ELLIS,Two-year trajectory of fall risk in people with Parkinson disease: a latent class analysis,"OBJECTIVE: To examine fall risk trajectories occurring naturally in a sample of individuals with early to middle stage Parkinson disease (PD). DESIGN: Latent class analysis, specifically growth mixture modeling (GMM), of longitudinal fall risk trajectories. SETTING: Assessments were conducted at 1 of 4 universities. PARTICIPANTS: Community-dwelling participants with PD of a longitudinal cohort study who attended at least 2 of 5 assessments over a 2-year follow-up period (N=230). INTERVENTIONS: Not applicable. MAIN OUTCOME MEASURES: Fall risk trajectory (low, medium, or high risk) and stability of fall risk trajectory (stable or fluctuating). Fall risk was determined at 6 monthly intervals using a simple clinical tool based on fall history, freezing of gait, and gait speed. RESULTS: The GMM optimally grouped participants into 3 fall risk trajectories that closely mirrored baseline fall risk status (P=.001). The high fall risk trajectory was most common (42.6%) and included participants with longer and more severe disease and with higher postural instability and gait disability (PIGD) scores than the low and medium fall risk trajectories (P<.001). Fluctuating fall risk (posterior probability <0.8 of belonging to any trajectory) was found in only 22.6% of the sample, most commonly among individuals who were transitioning to PIGD predominance. CONCLUSIONS: Regardless of their baseline characteristics, most participants had clear and stable fall risk trajectories over 2 years. Further investigation is required to determine whether interventions to improve gait and balance may improve fall risk trajectories in people with PD."
THERESA D ELLIS,"Design of the WHIP-PD study: a phase II, twelve-month, dual-site, randomized controlled trial evaluating the effects of a cognitive-behavioral approach for promoting enhanced walking activity using mobile health technology in people with Parkinson-disease","BACKGROUND: Parkinson disease (PD) is a debilitating and chronic neurodegenerative disease resulting in ambulation difficulties. Natural walking activity often declines early in disease progression despite the relative stability of motor impairments. In this study, we propose a paradigm shift with a ""connected behavioral approach"" that targets real-world walking using cognitive-behavioral training and mobile health (mHealth) technology. METHODS/DESIGN: The Walking and mHealth to Increase Participation in Parkinson Disease (WHIP-PD) study is a twelve-month, dual site, two-arm, randomized controlled trial recruiting 148 participants with early to mid-stage PD. Participants will be randomly assigned to connected behavioral or active control conditions. Both conditions will include a customized program of goal-oriented walking, walking-enhancing strengthening exercises, and eight in-person visits with a physical therapist. Participants in the connected behavioral condition also will (1) receive cognitive-behavioral training to promote self-efficacy for routine walking behavior and (2) use a mHealth software application to manage their program and communicate remotely with their physical therapist. Active control participants will receive no cognitive-behavioral training and manage their program on paper. Evaluations will occur at baseline, three-, six-, and twelve-months and include walking assessments, self-efficacy questionnaires, and seven days of activity monitoring. Primary outcomes will include the change between baseline and twelve months in overall amount of walking activity (mean number of steps per day) and amount of moderate intensity walking activity (mean number of minutes per day in which > 100 steps were accumulated). Secondary outcomes will include change in walking capacity as measured by the six-minute walk test and ten-meter walk test. We also will examine if self-efficacy mediates change in amount of walking activity and if change in amount of walking activity mediates change in walking capacity. DISCUSSION: We expect this study to show the connected behavioral approach will be more effective than the active control condition in increasing the amount and intensity of real-world walking activity and improving walking capacity. Determining effective physical activity interventions for persons with PD is important for preserving mobility and essential for maintaining quality of life. Clinical trials registration NCT03517371, May 7, 2018. TRIAL REGISTRATION: ClinicalTrials.gov: NCT03517371. Date of registration: May 7, 2018. Protocol version: Original."
SHYAMSUNDER ERRAMILLI,Wireless transfer of power by a 35-GHz metamaterial split-ring resonator rectenna,Wireless transfer of power via high frequency microwave radiation using a miniature split ring resonator rectenna is reported. RF power is converted into DC power by integrating a rectification circuit with the split ring resonator. The near-field behavior of the rectenna is investigated with microwave radiation in the frequency range between 20-40 GHz with a maximum power level of 17 dBm. The observed resonance peaks match those predicted by simulation. Polarization studies show the expected maximum in signal when the electric field is polarized along the edge of the split ring resonator with the gap and minimum for perpendicular orientation. The efficiency of the rectenna is on the order of 1% for a frequency of 37.2 GHz. By using a cascading array of 9 split ring resonators the output power was increased by a factor of 20.
SHYAMSUNDER ERRAMILLI,Wireless actuation of bulk acoustic modes in micromechanical resonators,We report wireless actuation of a Lamb wave micromechanical resonator from a distance of over 1 m with an efficiency of over 15%. Wireless actuation of conventional micromechanical resonators can have broad impact in a number of applications from wireless communication and implantable biomedical devices to distributed sensor networks.
SHYAMSUNDER ERRAMILLI,Perspective: Melanoma diagnosis and monitoring: Sunrise for melanoma therapy but early detection remains in the shade,"Melanoma is one of the most dangerous forms of cancer. The five-year survival rate is 98% if it is detected early. However, this rate plummets to 63% for regional disease and 17% when tumors have metastasized, that is, spread to distant sites. Furthermore, the incidence of melanoma has been rising by about 3% per year, whereas the incidence of cancers that are more common is decreasing. A handful of targeted therapies have recently become available that have finally shown real promise for treatment, but for reasons that remain unclear only a fraction of patients respond long term. These drugs often increase survival by only a few months in metastatic patient groups before relapse occurs. More effective treatment may be possible if a diagnosis can be made when the tumor burden is still low. Here, an overview of the current state-of-the-art is provided along with an argument for newer technologies towards early point-of-care diagnosis of melanoma."
SHYAMSUNDER ERRAMILLI,Detection of the melanoma biomarker TROY using silicon nanowire field-effect transistors,"Antibody-functionalized silicon nanowire field-effect transistors have been shown to exhibit excellent analyte detection sensitivity enabling sensing of analyte concentrations at levels not readily accessible by other methods. One example where accurate measurement of small concentrations is necessary is detection of serum biomarkers, such as the recently discovered tumor necrosis factor receptor superfamily member TROY (TNFRSF19), which may serve as a biomarker for melanoma. TROY is normally only present in brain but it is aberrantly expressed in primary and metastatic melanoma cells and shed into the surrounding environment. In this study, we show the detection of different concentrations of TROY in buffer solution using top-down fabricated silicon nanowires. We demonstrate the selectivity of our sensors by comparing the signal with that obtained from bovine serum albumin in buffer solution. Both the signal size and the reaction kinetics serve to distinguish the two signals. Using a fast-mixing two-compartment reaction model, we are able to extract the association and dissociation rate constants for the reaction of TROY with the antibody immobilized on the sensor surface."
SHYAMSUNDER ERRAMILLI,Micromechanical resonator with dielectric nonlinearity,"Nonlinear response of dielectric polarization to electric field in certain media is the foundation of nonlinear optics. Optically, such nonlinearities are observed at high light intensities, achievable by laser, where atomic-scale field strengths exceeding 106–108 V/m can be realized. Nonlinear optics includes a host of fascinating phenomena such as higher harmonic frequency generation, sum and difference frequency generation, four-wave mixing, self-focusing, optical phase conjugation, and optical rectification. Even though nonlinear optics has been studied for more than five decades, such studies in analogous acoustic or microwave frequency ranges are yet to be realized. Here, we demonstrate a nonlinear dielectric resonator composed of a silicon micromechanical resonator with an aluminum nitride piezoelectric layer, a material known to have a nonlinear optical susceptibility. Using a novel multiport approach, we demonstrate second and third-harmonic generation, sum and difference frequency generation, and four-wave mixing. Our demonstration of a nonlinear dielectric resonator opens up unprecedented possibilities for exploring nonlinear dielectric effects in engineered structures with an equally broad range of effects such as those observed in nonlinear optics. Furthermore, integration of a nonlinear dielectric layer on a chip-scale silicon micromechanical resonator offers tantalizing prospects for novel applications, such as ultra high harmonic generation, frequency multipliers, microwave frequency-comb generators, and nonlinear microwave signal processing."
SHYAMSUNDER ERRAMILLI,Wireless actuation of micromechanical resonators,"The wireless transfer of power is of fundamental and technical interest, with applications ranging from the remote operation of consumer electronics and implanted biomedical devices and sensors to the actuation of devices for which hard-wired power sources are neither desirable nor practical. In particular, biomedical devices that are implanted in the body or brain require small-footprint power receiving elements for wireless charging, which can be accomplished by micromechanical resonators. Moreover, for fundamental experiments, the ultralow-power wireless operation of micromechanical resonators in the microwave range can enable the performance of low-temperature studies of mechanical systems in the quantum regime, where the heat carried by the electrical wires in standard actuation techniques is detrimental to maintaining the resonator in a quantum state. Here we demonstrate the successful actuation of micron-sized silicon-based piezoelectric resonators with resonance frequencies ranging from 36 to 120 MHz at power levels of nanowatts and distances of ~3 feet, including comprehensive polarization, distance and power dependence measurements. Our unprecedented demonstration of the wireless actuation of micromechanical resonators via electric-field coupling down to nanowatt levels may enable a multitude of applications that require the wireless control of sensors and actuators based on micromechanical resonators, which was inaccessible until now."
SHYAMSUNDER ERRAMILLI,Channel-width dependent enhancement in nanoscale field effect transistor,"We report the observation of channel-width dependent enhancement in nanoscale field effect transistors containing lithographically-patterned silicon nanowires as the conduction channel. These devices behave as conventional metal-oxide-semiconductor field-effect transistors in reverse source drain bias. Reduction of nanowire width below 200 nm leads to dramatic change in the threshold voltage. Due to increased surface-to-volume ratio, these devices show higher transconductance per unit width at smaller width. Our devices with nanoscale channel width demonstrate extreme sensitivity to surface field profile, and therefore can be used as logic elements in computation and as ultrasensitive sensors of surface-charge in chemical and biological species."
SHYAMSUNDER ERRAMILLI,Sensing of the melanoma biomarker TROY using silicon nanowire field-effect transistors,"Antibody-functionalized silicon nanowire field-effect transistors have been shown to exhibit excellent analyte detection sensitivity enabling sensing of analyte concentrations at levels not readily accessible by other methods. One example where accurate measurement of small concentrations is necessary is detection of serum biomarkers, such as the recently discovered tumor necrosis factor receptor superfamily member TROY (TNFRSF19), which may serve as a biomarker for melanoma. TROY is normally only present in brain but it is aberrantly expressed in primary and metastatic melanoma cells and shed into the surrounding environment. In this study, we show the detection of different concentrations of TROY in buffer solution using top-down fabricated silicon nanowires. We demonstrate the selectivity of our sensors by comparing the signal with that obtained from bovine serum albumin in buffer solution. Both the signal size and the reaction kinetics serve to distinguish the two signals. Using a fast-mixing two-compartment reaction model we are able to extract the association and dissociation rate constants for the reaction of TROY with the antibody immobilized on the sensor surface."
SHYAMSUNDER ERRAMILLI,Femtosecond photonic viral inactivation probed using solid-state nanopores,"We report on detection of virus inactivation using femtosecond laser radiation by measuring the conductance of a solid state nanopore designed for detecting single particles. Conventional methods of assaying for viral inactivation based on plaque forming assays require 24–48 h for bacterial growth. Nanopore conductance measurements provide information on morphological changes at a single virion level.We show that analysis of a time series of nanopore conductance can quantify the detection of inactivation, requiring only a few minutes from collection to analysis. Morphological changes were verified by dynamic light scattering. Statistical analysis maximizing the information entropy provides a measure of the log reduction value. This work provides a rapid method for assaying viral inactivation with femtosecond lasers using solid-state nanopores."
SHYAMSUNDER ERRAMILLI,Nanoscale field effect transistor for biomolecular signal amplification,We report amplification of biomolecular recognition signal in lithographically defined silicon nanochannel devices. The devices are configured as field effect transistors (FET) in the reversed source-drain bias region. The measurement of the differential conductance of the nanowire channels in the FET allows sensitive detection of changes in the surface potential due to biomolecular binding. Narrower silicon channels demonstrate higher sensitivity to binding due to increased surface-to-volume ratio. The operation of the device in the negative source-drain region demonstrates signal amplification. The equivalence between protein binding and change in the surface potential is described.
SHYAMSUNDER ERRAMILLI,Field Effect Transistor Nanosensor for Breast Cancer Diagnostics,"Silicon nanochannel field effect transistor (FET) biosensors are one of the most promising technologies in the development of highly sensitive and label-free analyte detection for cancer diagnostics. With their exceptional electrical properties and small dimensions, silicon nanochannels are ideally suited for extraordinarily high sensitivity. In fact, the high surface-to-volume ratios of these systems make single molecule detection possible. Further, FET biosensors offer the benefits of high speed, low cost, and high yield manufacturing, without sacrificing the sensitivity typical for traditional optical methods in diagnostics. Top down manufacturing methods leverage advantages in Complementary Metal Oxide Semiconductor (CMOS) technologies, making richly multiplexed sensor arrays a reality. Here, we discuss the fabrication and use of silicon nanochannel FET devices as biosensors for breast cancer diagnosis and monitoring."
SHYAMSUNDER ERRAMILLI,Silicon-based nanochannel glucose sensor,"Silicon nanochannel biological field effect transistors have been developed for glucose detection. The device is nanofabricated from a silicon-on-insulator wafer with a top-down approach and surface functionalized with glucose oxidase. The differential conductance of silicon nanowires, tuned with source-drain bias voltage, is demonstrated to be sensitive to the biocatalyzed oxidation of glucose. The glucose biosensor response is linear in the 0.5–8mM concentration range with 3–5min response time. This silicon nanochannel-based glucose biosensor technology offers the possibility of high density, high quality glucose biosensor integration with silicon-based circuitry."
SHYAMSUNDER ERRAMILLI,Dynamical response of nanomechanical oscillators in immiscible viscous fluid for in vitro biomolecular recognition,"Dynamical response of nanomechanical cantilever structures immersed in a viscous fluid is important to in vitro single-molecule force spectroscopy, biomolecular recognition of disease-specific proteins, and the study of microscopic protein dynamics. Here we study the stochastic response of biofunctionalized nanomechanical cantilever beams in a viscous fluid. Using the fluctuation-dissipation theorem we derive an exact expression for the spectral density of displacement and a linear approximation for resonance frequency shift. We find that in a viscous solution the frequency shift of the nanoscale cantilever is determined by surface stress generated by biomolecular interaction with negligible contributions from mass loading due to the biomolecules."
SHYAMSUNDER ERRAMILLI,Measurement of nonlinear piezoelectric coefficients using a micromechanical resonator,"We describe and demonstrate a method by which the nonlinear piezoelectric properties of a piezoelectric material may be measured by detecting the force that it applies on a suspended micromechanical resonator at one of its mechanical resonance frequencies. Resonators are used in countless applications; this method could provide a means for better-characterizing material behaviors within real MEMS devices. Further, special devices can be designed to probe this nonlinear behavior at specific frequencies with enhanced signal sizes. The resonators used for this experiment are actuated using a 1-μm-thick layer of aluminum nitride. When driven at large amplitudes, the piezoelectric layer generates harmonics, which are measurable in the response of the resonator. In this experiment, we measured the second-order piezoelectric coefficient of aluminum nitride to be −(23.1±14.1)×10^−22m/V^2."
SHYAMSUNDER ERRAMILLI,Alanine aminotransferase assay biosensor platform using silicon nanowire field effect transistors,"Frequent monitoring of serum alanine aminotransferase (ALT) activity is essential to prevent drug-induced liver injury (DILI). Current ALT assays are restricted to centralized clinical laboratories, making frequent patient monitoring logistically difficult. To address this, we demonstrated the capability of commercial foundry manufactured silicon nanowire field effect transistor (SiNW-FET) biosensors in a form factor that enables frequent near-patient monitoring. Here, we designed an ALT assay, by coupling the ALT-catalyzed production of pyruvate to the reduction of ferricyanide, enabling both spectrophotometric and electrical measurement of ALT activity. The two methods yield comparable ALT activity detection across a dynamic range wide enough to monitor patients at risk for DILI. This study demonstrates kinetic activity measurement of an endogenous enzyme using uncoupled SiNW-FETs, and commercial manufacturing of SiNW-FET sensor arrays for use in a portable biosensor platform."
WILLIAM C KARL,Butterfly genome reveals promiscuous exchange of mimicry adaptations among species,"The evolutionary importance of hybridization and introgression has long been debated1. Hybrids are usually rare and unfit, but even infrequent hybridization can aid adaptation by transferring beneficial traits between species. Here we use genomic tools to investigate introgression in Heliconius, a rapidly radiating genus of neotropical butterflies widely used in studies of ecology, behaviour, mimicry and speciation2,3,4,5. We sequenced the genome of Heliconius melpomene and compared it with other taxa to investigate chromosomal evolution in Lepidoptera and gene flow among multiple Heliconius species and races. Among 12,669 predicted genes, biologically important expansions of families of chemosensory and Hox genes are particularly noteworthy. Chromosomal organization has remained broadly conserved since the Cretaceous period, when butterflies split from the Bombyx (silkmoth) lineage. Using genomic resequencing, we show hybrid exchange of genes between three co-mimics, Heliconius melpomene, Heliconius timareta and Heliconius elevatus, especially at two genomic regions that control mimicry pattern. We infer that closely related Heliconius species exchange protective colour-pattern genes promiscuously, implying that hybridization has an important role in adaptive radiation."
WILLIAM C KARL,First M87 Event Horizon Telescope results. IV. Imaging the central supermassive black hole,
WILLIAM C KARL,First Sagittarius A* Event Horizon Telescope results. V. Testing astrophysical models of the galactic center black hole,"In this paper we provide a first physical interpretation for the Event Horizon Telescope's (EHT) 2017 observations of Sgr A*. Our main approach is to compare resolved EHT data at 230 GHz and unresolved non-EHT observations from radio to X-ray wavelengths to predictions from a library of models based on time-dependent general relativistic magnetohydrodynamics simulations, including aligned, tilted, and stellar-wind-fed simulations; radiative transfer is performed assuming both thermal and nonthermal electron distribution functions. We test the models against 11 constraints drawn from EHT 230 GHz data and observations at 86 GHz, 2.2 μm, and in the X-ray. All models fail at least one constraint. Light-curve variability provides a particularly severe constraint, failing nearly all strongly magnetized (magnetically arrested disk (MAD)) models and a large fraction of weakly magnetized models. A number of models fail only the variability constraints. We identify a promising cluster of these models, which are MAD and have inclination i ≤ 30°. They have accretion rate (5.2–9.5) × 10−9 M ⊙ yr−1, bolometric luminosity (6.8–9.2) × 1035 erg s−1, and outflow power (1.3–4.8) × 1038 erg s−1. We also find that all models with i ≥ 70° fail at least two constraints, as do all models with equal ion and electron temperature; exploratory, nonthermal model sets tend to have higher 2.2 μm flux density; and the population of cold electrons is limited by X-ray constraints due to the risk of bremsstrahlung overproduction. Finally, we discuss physical and numerical limitations of the models, highlighting the possible importance of kinetic effects and duration of the simulations."
WILLIAM C KARL,First M87 Event Horizon Telescope results. VII. Polarization of the ring,"In 2017 April, the Event Horizon Telescope (EHT) observed the near-horizon region around the supermassive black hole at the core of the M87 galaxy. These 1.3 mm wavelength observations revealed a compact asymmetric ring-like source morphology. This structure originates from synchrotron emission produced by relativistic plasma located in the immediate vicinity of the black hole. Here we present the corresponding linear-polarimetric EHT images of the center of M87. We find that only a part of the ring is significantly polarized. The resolved fractional linear polarization has a maximum located in the southwest part of the ring, where it rises to the level of ∼15%. The polarization position angles are arranged in a nearly azimuthal pattern. We perform quantitative measurements of relevant polarimetric properties of the compact emission and find evidence for the temporal evolution of the polarized source structure over one week of EHT observations. The details of the polarimetric data reduction and calibration methodology are provided. We carry out the data analysis using multiple independent imaging and modeling techniques, each of which is validated against a suite of synthetic data sets. The gross polarimetric structure and its apparent evolution with time are insensitive to the method used to reconstruct the image. These polarimetric images carry information about the structure of the magnetic fields responsible for the synchrotron emission. Their physical interpretation is discussed in an accompanying publication."
WILLIAM C KARL,Resolving the inner parsec of the blazar J1924–2914 with the event horizon telescope,"The blazar J1924–2914 is a primary Event Horizon Telescope (EHT) calibrator for the Galactic center’s black hole Sagittarius A*. Here we present the first total and linearly polarized intensity images of this source obtained with the unprecedented 20 μas resolution of the EHT. J1924–2914 is a very compact flat-spectrum radio source with strong optical variability and polarization. In April 2017 the source was observed quasi-simultaneously with the EHT (April 5–11), the Global Millimeter VLBI Array (April 3), and the Very Long Baseline Array (April 28), giving a novel view of the source at four observing frequencies, 230, 86, 8.7, and 2.3 GHz. These observations probe jet properties from the subparsec to 100 pc scales. We combine the multifrequency images of J1924–2914 to study the source morphology. We find that the jet exhibits a characteristic bending, with a gradual clockwise rotation of the jet projected position angle of about 90° between 2.3 and 230 GHz. Linearly polarized intensity images of J1924–2914 with the extremely fine resolution of the EHT provide evidence for ordered toroidal magnetic fields in the blazar compact core."
WILLIAM C KARL,A universal power-law prescription for variability from synthetic images of black hole accretion flows,"We present a framework for characterizing the spatiotemporal power spectrum of the variability expected from the horizon-scale emission structure around supermassive black holes, and we apply this framework to a library of general relativistic magnetohydrodynamic (GRMHD) simulations and associated general relativistic ray-traced images relevant for Event Horizon Telescope (EHT) observations of Sgr A*. We find that the variability power spectrum is generically a red-noise process in both the temporal and spatial dimensions, with the peak in power occurring on the longest timescales and largest spatial scales. When both the time-averaged source structure and the spatially integrated light-curve variability are removed, the residual power spectrum exhibits a universal broken power-law behavior. On small spatial frequencies, the residual power spectrum rises as the square of the spatial frequency and is proportional to the variance in the centroid of emission. Beyond some peak in variability power, the residual power spectrum falls as that of the time-averaged source structure, which is similar across simulations; this behavior can be naturally explained if the variability arises from a multiplicative random field that has a steeper high-frequency power-law index than that of the time-averaged source structure. We briefly explore the ability of power spectral variability studies to constrain physical parameters relevant for the GRMHD simulations, which can be scaled to provide predictions for black holes in a range of systems in the optically thin regime. We present specific expectations for the behavior of the M87* and Sgr A* accretion flows as observed by the EHT."
WILLIAM C KARL,Millimeter light curves of Sagittarius A* observed during the 2017 Event Horizon Telescope campaign,"The Event Horizon Telescope (EHT) observed the compact radio source, Sagittarius A* (Sgr A*), in the Galactic Center on 2017 April 5–11 in the 1.3 mm wavelength band. At the same time, interferometric array data from the Atacama Large Millimeter/submillimeter Array and the Submillimeter Array were collected, providing Sgr A* light curves simultaneous with the EHT observations. These data sets, complementing the EHT very long baseline interferometry, are characterized by a cadence and signal-to-noise ratio previously unattainable for Sgr A* at millimeter wavelengths, and they allow for the investigation of source variability on timescales as short as a minute. While most of the light curves correspond to a low variability state of Sgr A*, the April 11 observations follow an X-ray flare and exhibit strongly enhanced variability. All of the light curves are consistent with a red-noise process, with a power spectral density (PSD) slope measured to be between −2 and −3 on timescales between 1 minute and several hours. Our results indicate a steepening of the PSD slope for timescales shorter than 0.3 hr. The spectral energy distribution is flat at 220 GHz, and there are no time lags between the 213 and 229 GHz frequency bands, suggesting low optical depth for the event horizon scale source. We characterize Sgr A*’s variability, highlighting the different behavior observed just after the X-ray flare, and use Gaussian process modeling to extract a decorrelation timescale and a PSD slope. We also investigate the systematic calibration uncertainties by analyzing data from independent data reduction pipelines."
WILLIAM C KARL,First Sagittarius A* Event Horizon Telescope results. VI. Testing the black hole metric,"Astrophysical black holes are expected to be described by the Kerr metric. This is the only stationary, vacuum, axisymmetric metric, without electromagnetic charge, that satisfies Einstein’s equations and does not have pathologies outside of the event horizon. We present new constraints on potential deviations from the Kerr prediction based on 2017 EHT observations of Sagittarius A* (Sgr A*). We calibrate the relationship between the geometrically defined black hole shadow and the observed size of the ring-like images using a library that includes both Kerr and non-Kerr simulations. We use the exquisite prior constraints on the mass-to-distance ratio for Sgr A* to show that the observed image size is within ∼10% of the Kerr predictions. We use these bounds to constrain metrics that are parametrically different from Kerr, as well as the charges of several known spacetimes. To consider alternatives to the presence of an event horizon, we explore the possibility that Sgr A* is a compact object with a surface that either absorbs and thermally reemits incident radiation or partially reflects it. Using the observed image size and the broadband spectrum of Sgr A*, we conclude that a thermal surface can be ruled out and a fully reflective one is unlikely. We compare our results to the broader landscape of gravitational tests. Together with the bounds found for stellar-mass black holes and the M87 black hole, our observations provide further support that the external spacetimes of all black holes are described by the Kerr metric, independent of their mass."
WILLIAM C KARL,Polarimetric properties of Event Horizon Telescope targets from ALMA,"We present the results from a full polarization study carried out with the Atacama Large Millimeter/submillimeter Array (ALMA) during the first Very Long Baseline Interferometry (VLBI) campaign, which was conducted in 2017 April in the λ3 mm and λ1.3 mm bands, in concert with the Global mm-VLBI Array (GMVA) and the Event Horizon Telescope (EHT), respectively. We determine the polarization and Faraday properties of all VLBI targets, including Sgr A*, M87, and a dozen radio-loud active galactic nuclei (AGNs), in the two bands at several epochs in a time window of 10 days. We detect high linear polarization fractions (2%–15%) and large rotation measures (RM &gt; 103.3–105.5 rad m−2), confirming the trends of previous AGN studies at millimeter wavelengths. We find that blazars are more strongly polarized than other AGNs in the sample, while exhibiting (on average) order-of-magnitude lower RM values, consistent with the AGN viewing angle unification scheme. For Sgr A* we report a mean RM of (−4.2 ± 0.3) × 105 rad m−2 at 1.3 mm, consistent with measurements over the past decade and, for the first time, an RM of (–2.1 ± 0.1) × 105 rad m−2 at 3 mm, suggesting that about half of the Faraday rotation at 1.3 mm may occur between the 3 mm photosphere and the 1.3 mm source. We also report the first unambiguous measurement of RM toward the M87 nucleus at millimeter wavelengths, which undergoes significant changes in magnitude and sign reversals on a one year timescale, spanning the range from −1.2 to 0.3 × 105 rad m−2 at 3 mm and −4.1 to 1.5 × 105 rad m−2 at 1.3 mm. Given this time variability, we argue that, unlike the case of Sgr A*, the RM in M87 does not provide an accurate estimate of the mass accretion rate onto the black hole. We put forward a two-component model, comprised of a variable compact region and a static extended region, that can simultaneously explain the polarimetric properties observed by both the EHT (on horizon scales) and ALMA (which observes the combined emission from both components). These measurements provide critical constraints for the calibration, analysis, and interpretation of simultaneously obtained VLBI data with the EHT and GMVA."
WILLIAM C KARL,"First Sagittarius A* Event Horizon Telescope results. IV. Variability, morphology, and black hole mass","In this paper we quantify the temporal variability and image morphology of the horizon-scale emission from Sgr A*, as observed by the EHT in 2017 April at a wavelength of 1.3 mm. We find that the Sgr A* data exhibit variability that exceeds what can be explained by the uncertainties in the data or by the effects of interstellar scattering. The magnitude of this variability can be a substantial fraction of the correlated flux density, reaching ∼100% on some baselines. Through an exploration of simple geometric source models, we demonstrate that ring-like morphologies provide better fits to the Sgr A* data than do other morphologies with comparable complexity. We develop two strategies for fitting static geometric ring models to the time-variable Sgr A* data; one strategy fits models to short segments of data over which the source is static and averages these independent fits, while the other fits models to the full data set using a parametric model for the structural variability power spectrum around the average source structure. Both geometric modeling and image-domain feature extraction techniques determine the ring diameter to be 51.8 ± 2.3 μas (68% credible intervals), with the ring thickness constrained to have an FWHM between ∼30% and 50% of the ring diameter. To bring the diameter measurements to a common physical scale, we calibrate them using synthetic data generated from GRMHD simulations. This calibration constrains the angular size of the gravitational radius to be 4.8_-0.7^+1.4 μas, which we combine with an independent distance measurement from maser parallaxes to determine the mass of Sgr A* to be 4.0_-0.6^+10^6 M⊙."
WILLIAM C KARL,"First Sagittarius A* Event Horizon Telescope results. II. EHT and multiwavelength observations, data processing, and calibration","We present Event Horizon Telescope (EHT) 1.3 mm measurements of the radio source located at the position of the supermassive black hole Sagittarius A* (Sgr A*), collected during the 2017 April 5–11 campaign. The observations were carried out with eight facilities at six locations across the globe. Novel calibration methods are employed to account for Sgr A*'s flux variability. The majority of the 1.3 mm emission arises from horizon scales, where intrinsic structural source variability is detected on timescales of minutes to hours. The effects of interstellar scattering on the image and its variability are found to be subdominant to intrinsic source structure. The calibrated visibility amplitudes, particularly the locations of the visibility minima, are broadly consistent with a blurred ring with a diameter of ∼50 μas, as determined in later works in this series. Contemporaneous multiwavelength monitoring of Sgr A* was performed at 22, 43, and 86 GHz and at near-infrared and X-ray wavelengths. Several X-ray flares from Sgr A* are detected by Chandra, one at low significance jointly with Swift on 2017 April 7 and the other at higher significance jointly with NuSTAR on 2017 April 11. The brighter April 11 flare is not observed simultaneously by the EHT but is followed by a significant increase in millimeter flux variability immediately after the X-ray outburst, indicating a likely connection in the emission physics near the event horizon. We compare Sgr A*’s broadband flux during the EHT campaign to its historical spectral energy distribution and find that both the quiescent emission and flare emission are consistent with its long-term behavior."
WILLIAM C KARL,First Sagittarius A* Event Horizon Telescope results. I. The shadow of the supermassive black hole in the center of the Milky Way,"We present the first Event Horizon Telescope (EHT) observations of Sagittarius A* (Sgr A*), the Galactic center source associated with a supermassive black hole. These observations were conducted in 2017 using a global interferometric array of eight telescopes operating at a wavelength of λ = 1.3 mm. The EHT data resolve a compact emission region with intrahour variability. A variety of imaging and modeling analyses all support an image that is dominated by a bright, thick ring with a diameter of 51.8 ± 2.3 μas (68% credible interval). The ring has modest azimuthal brightness asymmetry and a comparatively dim interior. Using a large suite of numerical simulations, we demonstrate that the EHT images of Sgr A* are consistent with the expected appearance of a Kerr black hole with mass ∼4 × 106 M ⊙, which is inferred to exist at this location based on previous infrared observations of individual stellar orbits, as well as maser proper-motion studies. Our model comparisons disfavor scenarios where the black hole is viewed at high inclination (i &gt; 50°), as well as nonspinning black holes and those with retrograde accretion disks. Our results provide direct evidence for the presence of a supermassive black hole at the center of the Milky Way, and for the first time we connect the predictions from dynamical measurements of stellar orbits on scales of 103–105 gravitational radii to event-horizon-scale images and variability. Furthermore, a comparison with the EHT results for the supermassive black hole M87* shows consistency with the predictions of general relativity spanning over three orders of magnitude in central mass."
WILLIAM C KARL,The long-baseline neutrino experiment: exploring fundamental symmetries of the universe,"The preponderance of matter over antimatter in the early Universe, the dynamics of the supernova bursts that produced the heavy elements necessary for life and whether protons eventually decay --- these mysteries at the forefront of particle physics and astrophysics are key to understanding the early evolution of our Universe, its current state and its eventual fate. The Long-Baseline Neutrino Experiment (LBNE) represents an extensively developed plan for a world-class experiment dedicated to addressing these questions. LBNE is conceived around three central components: (1) a new, high-intensity neutrino source generated from a megawatt-class proton accelerator at Fermi National Accelerator Laboratory, (2) a near neutrino detector just downstream of the source, and (3) a massive liquid argon time-projection chamber deployed as a far detector deep underground at the Sanford Underground Research Facility. This facility, located at the site of the former Homestake Mine in Lead, South Dakota, is approximately 1,300 km from the neutrino source at Fermilab -- a distance (baseline) that delivers optimal sensitivity to neutrino charge-parity symmetry violation and mass ordering effects. This ambitious yet cost-effective design incorporates scalability and flexibility and can accommodate a variety of upgrades and contributions. With its exceptional combination of experimental configuration, technical capabilities, and potential for transformative discoveries, LBNE promises to be a vital facility for the field of particle physics worldwide, providing physicists from around the globe with opportunities to collaborate in a twenty to thirty year program of exciting science. In this document we provide a comprehensive overview of LBNE's scientific objectives, its place in the landscape of neutrino physics worldwide, the technologies it will incorporate and the capabilities it will possess."
WILLIAM C KARL,Broadband multi-wavelength properties of M87 during the 2017 Event Horizon Telescope campaign,"In 2017, the Event Horizon Telescope (EHT) Collaboration succeeded in capturing the first direct image of the center of the M87 galaxy. The asymmetric ring morphology and size are consistent with theoretical expectations for a weakly accreting supermassive black hole of mass ∼6.5 × 109 M ⊙. The EHTC also partnered with several international facilities in space and on the ground, to arrange an extensive, quasi-simultaneous multi-wavelength campaign. This Letter presents the results and analysis of this campaign, as well as the multi-wavelength data as a legacy data repository. We captured M87 in a historically low state, and the core flux dominates over HST-1 at high energies, making it possible to combine core flux constraints with the more spatially precise very long baseline interferometry data. We present the most complete simultaneous multi-wavelength spectrum of the active nucleus to date, and discuss the complexity and caveats of combining data from different spatial scales into one broadband spectrum. We apply two heuristic, isotropic leptonic single-zone models to provide insight into the basic source properties, but conclude that a structured jet is necessary to explain M87’s spectrum. We can exclude that the simultaneous γ-ray emission is produced via inverse Compton emission in the same region producing the EHT mm-band emission, and further conclude that the γ-rays can only be produced in the inner jets (inward of HST-1) if there are strongly particle-dominated regions. Direct synchrotron emission from accelerated protons and secondaries cannot yet be excluded."
WILLIAM C KARL,Event Horizon Telescope imaging of the archetypal blazar 3C 279 at an extreme 20 microarcsecond resolution,"3C 279 is an archetypal blazar with a prominent radio jet that show broadband flux density variability across the entire electromagnetic spectrum. We use an ultra-high angular resolution technique – global Very Long Baseline Interferometry (VLBI) at 1.3 mm (230 GHz) – to resolve the innermost jet of 3C 279 in order to study its fine-scale morphology close to the jet base where highly variable γ-ray emission is thought to originate, according to various models. The source was observed during four days in April 2017 with the Event Horizon Telescope at 230 GHz, including the phased Atacama Large Millimeter/submillimeter Array (ALMA), at an angular resolution of ∼20 μas (at a redshift of z = 0.536 this corresponds to ∼0.13 pc  ∼ 1700 Schwarzschild radii with a black hole mass M<jats:sub>BH</jats:sub> = 8 × 10^8 M_⊙). Imaging and model-fitting techniques were applied to the data to parameterize the fine-scale source structure and its variation. We find a multicomponent inner jet morphology with the northernmost component elongated perpendicular to the direction of the jet, as imaged at longer wavelengths. The elongated nuclear structure is consistent on all four observing days and across different imaging methods and model-fitting techniques, and therefore appears robust. Owing to its compactness and brightness, we associate the northern nuclear structure as the VLBI “core”. This morphology can be interpreted as either a broad resolved jet base or a spatially bent jet. We also find significant day-to-day variations in the closure phases, which appear most pronounced on the triangles with the longest baselines. Our analysis shows that this variation is related to a systematic change of the source structure. Two inner jet components move non-radially at apparent speeds of ∼15 c and ∼20 c (∼1.3 and ∼1.7 μas day^−1, respectively), which more strongly supports the scenario of traveling shocks or instabilities in a bent, possibly rotating jet. The observed apparent speeds are also coincident with the 3C 279 large-scale jet kinematics observed at longer (cm) wavelengths, suggesting no significant jet acceleration between the 1.3 mm core and the outer jet. The intrinsic brightness temperature of the jet components are ≲10^10 K, a magnitude or more lower than typical values seen at ≥7 mm wavelengths. The low brightness temperature and morphological complexity suggest that the core region of 3C 279 becomes optically thin at short (mm) wavelengths."
WILLIAM C KARL,First M87 Event Horizon Telescope results. II. Array and instrumentation,"The Event Horizon Telescope (EHT) is a very long baseline interferometry (VLBI) array that comprises millimeter- and submillimeter-wavelength telescopes separated by distances comparable to the diameter of the Earth. At a nominal operating wavelength of ~1.3 mm, EHT angular resolution (λ/D) is ~25 μas, which is sufficient to resolve nearby supermassive black hole candidates on spatial and temporal scales that correspond to their event horizons. With this capability, the EHT scientific goals are to probe general relativistic effects in the strong-field regime and to study accretion and relativistic jet formation near the black hole boundary. In this Letter we describe the system design of the EHT, detail the technology and instrumentation that enable observations, and provide measures of its performance. Meeting the EHT science objectives has required several key developments that have facilitated the robust extension of the VLBI technique to EHT observing wavelengths and the production of instrumentation that can be deployed on a heterogeneous array of existing telescopes and facilities. To meet sensitivity requirements, high-bandwidth digital systems were developed that process data at rates of 64 gigabit s−1, exceeding those of currently operating cm-wavelength VLBI arrays by more than an order of magnitude. Associated improvements include the development of phasing systems at array facilities, new receiver installation at several sites, and the deployment of hydrogen maser frequency standards to ensure coherent data capture across the array. These efforts led to the coordination and execution of the first Global EHT observations in 2017 April, and to event-horizon-scale imaging of the supermassive black hole candidate in M87."
WILLIAM C KARL,First M87 Event Horizon Telescope results. I. The shadow of the supermassive black hole,"When surrounded by a transparent emission region, black holes are expected to reveal a dark shadow caused by gravitational light bending and photon capture at the event horizon. To image and study this phenomenon, we have assembled the Event Horizon Telescope, a global very long baseline interferometry array observing at a wavelength of 1.3 mm. This allows us to reconstruct event-horizon-scale images of the supermassive black hole candidate in the center of the giant elliptical galaxy M87. We have resolved the central compact radio source as an asymmetric bright emission ring with a diameter of 42 ± 3 μas, which is circular and encompasses a central depression in brightness with a flux ratio gsim10:1. The emission ring is recovered using different calibration and imaging schemes, with its diameter and width remaining stable over four different observations carried out in different days. Overall, the observed image is consistent with expectations for the shadow of a Kerr black hole as predicted by general relativity. The asymmetry in brightness in the ring can be explained in terms of relativistic beaming of the emission from a plasma rotating close to the speed of light around a black hole. We compare our images to an extensive library of ray-traced general-relativistic magnetohydrodynamic simulations of black holes and derive a central mass of M = (6.5 ± 0.7) × 10^9 M ⊙. Our radio-wave observations thus provide powerful evidence for the presence of supermassive black holes in centers of galaxies and as the central engines of active galactic nuclei. They also present a new tool to explore gravity in its most extreme limit and on a mass scale that was so far not accessible."
WILLIAM C KARL,First Sagittarius A* Event Horizon Telescope results. III. Imaging of the Galactic center supermassive black hole,"We present the first event-horizon-scale images and spatiotemporal analysis of Sgr A* taken with the Event Horizon Telescope in 2017 April at a wavelength of 1.3 mm. Imaging of Sgr A* has been conducted through surveys over a wide range of imaging assumptions using the classical CLEAN algorithm, regularized maximum likelihood methods, and a Bayesian posterior sampling method. Different prescriptions have been used to account for scattering effects by the interstellar medium toward the Galactic center. Mitigation of the rapid intraday variability that characterizes Sgr A* has been carried out through the addition of a “variability noise budget” in the observed visibilities, facilitating the reconstruction of static full-track images. Our static reconstructions of Sgr A* can be clustered into four representative morphologies that correspond to ring images with three different azimuthal brightness distributions and a small cluster that contains diverse nonring morphologies. Based on our extensive analysis of the effects of sparse (u, v)-coverage, source variability, and interstellar scattering, as well as studies of simulated visibility data, we conclude that the Event Horizon Telescope Sgr A* data show compelling evidence for an image that is dominated by a bright ring of emission with a ring diameter of ∼50 μas, consistent with the expected “shadow” of a 4 × 106 M⊙ black hole in the Galactic center located at a distance of 8 kpc."
WILLIAM C KARL,Characterizing and mitigating intraday variability: reconstructing source structure in accreting black holes with mm-VLBI,"The extraordinary physical resolution afforded by the Event Horizon Telescope has opened a window onto the astrophysical phenomena unfolding on horizon scales in two known black holes, M87* and Sgr A*. However, with this leap in resolution has come a new set of practical complications. Sgr A* exhibits intraday variability that violates the assumptions underlying Earth aperture synthesis, limiting traditional image reconstruction methods to short timescales and data sets with very sparse (u, v) coverage. We present a new set of tools to detect and mitigate this variability. We develop a data-driven, model-agnostic procedure to detect and characterize the spatial structure of intraday variability. This method is calibrated against a large set of mock data sets, producing an empirical estimator of the spatial power spectrum of the brightness fluctuations. We present a novel Bayesian noise modeling algorithm that simultaneously reconstructs an average image and statistical measure of the fluctuations about it using a parameterized form for the excess variance in the complex visibilities not otherwise explained by the statistical errors. These methods are validated using a variety of simulated data, including general relativistic magnetohydrodynamic simulations appropriate for Sgr A* and M87*. We find that the reconstructed source structure and variability are robust to changes in the underlying image model. We apply these methods to the 2017 EHT observations of M87*, finding evidence for variability across the EHT observing campaign. The variability mitigation strategies presented are widely applicable to very long baseline interferometry observations of variable sources generally, for which they provide a data-informed averaging procedure and natural characterization of inter-epoch image consistency."
ALAN L FELD,The long-baseline neutrino experiment: exploring fundamental symmetries of the universe,"The preponderance of matter over antimatter in the early Universe, the dynamics of the supernova bursts that produced the heavy elements necessary for life and whether protons eventually decay --- these mysteries at the forefront of particle physics and astrophysics are key to understanding the early evolution of our Universe, its current state and its eventual fate. The Long-Baseline Neutrino Experiment (LBNE) represents an extensively developed plan for a world-class experiment dedicated to addressing these questions. LBNE is conceived around three central components: (1) a new, high-intensity neutrino source generated from a megawatt-class proton accelerator at Fermi National Accelerator Laboratory, (2) a near neutrino detector just downstream of the source, and (3) a massive liquid argon time-projection chamber deployed as a far detector deep underground at the Sanford Underground Research Facility. This facility, located at the site of the former Homestake Mine in Lead, South Dakota, is approximately 1,300 km from the neutrino source at Fermilab -- a distance (baseline) that delivers optimal sensitivity to neutrino charge-parity symmetry violation and mass ordering effects. This ambitious yet cost-effective design incorporates scalability and flexibility and can accommodate a variety of upgrades and contributions. With its exceptional combination of experimental configuration, technical capabilities, and potential for transformative discoveries, LBNE promises to be a vital facility for the field of particle physics worldwide, providing physicists from around the globe with opportunities to collaborate in a twenty to thirty year program of exciting science. In this document we provide a comprehensive overview of LBNE's scientific objectives, its place in the landscape of neutrino physics worldwide, the technologies it will incorporate and the capabilities it will possess."
STEVEN SANDAGE,Relational spirituality and psychotherapy: healing suffering and promoting growth,Led half day continuing education workshop
STEVEN SANDAGE,Practicing forgiveness: spirituality and psychotherapy,Presentation as part of Science for Seminaries grant
STEVEN SANDAGE,Focus: Summer 2020,
STEVEN SANDAGE,The moderating influence of religiousness/spirituality on COVID-19 impact and change in Psychotherapy,"The COVID-19 pandemic has spurred a global surge in empirical research examining the influence of the pandemic on individuals’ mental health symptoms and well-being. Within this larger literature is a rapidly growing literature on the associations among religiousness/spirituality, COVID-19 impact, symptoms and well-being. Largely absent from this literature is a specific research focus on psychotherapy clients, and the influence of religiousness/spirituality and COVID-19 impact on change during treatment. One prominent theory in the existing literature centers on the notion that religiousness/spirituality is a coping resource for individuals during times of adversity. Yet, existing empirical findings present mixed evidence for the religious/spiritual coping hypothesis. We expanded upon these emerging research trends to examine the influence of religious/spiritual struggles, religious/spiritual commitment, religious/spiritual exploration, and COVID-19 impact ratings on psychotherapy change in a sample of adult clients (N = 185; Mage = 38.06; SD = 15.78; range = 19–81; 61.1% female; 69.7% White). The results of latent trajectory analysis identified three subgroups that differed on initial levels of symptoms and well-being and the nature of change over three time points. The COVID-19 impact ratings predicted change trajectories. As more positive ratings of COVID-19 impact increased, membership in the no change trajectory was more likely relative to the deterioration trajectory at high levels of both religious/spiritual commitment and exploration. The implications emphasize the need for judicious assessment of religiousness/spirituality and COVID-19 impact before integrating religiousness/spirituality into treatment."
STEVEN SANDAGE,Think therapy is navel-gazing? Think again.,
STEVEN SANDAGE,Humility and relational spirituality in clinical practice,
STEVEN SANDAGE,Humility among religious leaders: testing a relational spirituality model,
STEVEN SANDAGE,Humility in an age of narcissism,
STEVEN SANDAGE,Seeking well-being in the terrain of despair: Relational strengths for practitioner formation,Continuing education event
STEVEN SANDAGE,New horizons in group psychotherapy research and practice from third wave positive psychology: a practice-friendly review,"Group psychotherapy has been shown to be equivalent to individual therapy for many disorders, including anxiety, depression, grief, eating disorders, and schizophrenia (Burlingame & Strauss, 2021). In addition to effectiveness in reducing symptoms, group offers members a sense of belonging, purpose, hope, altruism, and meaning throughout treatment (Yalom & Leszcz, 2020). These additional outcomes are especially important considering the COVID-19 pandemic and national/international conflicts, given the trauma, disruptions, and losses people have experienced. Applying recent developments in positive psychology to group therapy can enhance treatment. A practice-friendly review examined recent advances in the positive psychology literature, demonstrating how group therapy offers members unique growth opportunities in addition to reducing symptoms. Key findings from studies applying positive psychological constructs to group therapy outcomes are synthesized. Our review sheds light on the relevance of third wave positive psychology to enrich group therapy (Lomas et al., 2021). Specifically, group therapy can facilitate the development of vitalizing psychological virtues, and these can be used to assess treatment outcome: humanity, wisdom, transcendence, courage, temperance, and justice. Interrelatedly, we present support for including attachment theory and mentalization within a positive psychological group framework. Implications are explored for group therapy research, clinical work, and training."
STEVEN SANDAGE,Spiritual diversity in psychotherapy (book event panel),
STEVEN SANDAGE,Spiritual formation and seminary training: Implications for leadership training,
STEVEN SANDAGE,Integrating positive psychology constructs in psychotherapy: therapist perspectives on flourishing and virtue,"The term ""flourishing"" means to grow or prosper and refers to a holistic, developmental sense of well being (VanderWeele et a l., 2019). There is growing interest in the principles of flourishing in psychotherapy. For instance, there are 20 studies of positive psychotherapy with results on par with other bona fide treatments and emphasize aspects of flourishing as factors in clie nt progress (e.g., AEDP, ACT; Fosha, 200; Rashid & Seligman, 2018). Still, the concept of flourishing remains largely segregated from mainstream mental healthcare practice. The disease model dominance within mental healthcare employs a view of the human pe rson as ""clusters of symptoms"" with the goal of treatment being the reduction of negative symptoms rather than increasing flourishing. Little is known about how mental health treatments improve clients' flourishing, as most studies have focused on symptom reduction. To address these limitations, the current study facilitated collaboration among four clinical research teams representing different clinical sites and training perspectives (e.g., CBT, Psychodynamic, Integrated). In doing so, we conducted eight focus groups utilizing a grounded theory qualitative approach to explore the processes through which flourishing is fostered in psychotherapy and the training of mental healthcare professionals."
STEVEN SANDAGE,"Relational spirituality, intercultural competence, and social justice in systemic therapies","The Relational Spirituality Model (RSM) builds on relational, psychodynamic, and systemic approaches and serves as an orienting framework for clinical services and training. In this article, we provide an overview of the RSM, a pluralistic contextual approach to spirituality in clinical practice that (a) considers developmental dialectics of spiritual dwelling and seeking and (b) explores diverse ways that religious and spiritual dynamics can range from salutary to harmful. In light of growing attention to racism in U.S. society, we review salient research on justice-seeking spirituality and consider the roles of humility, differentiation, and hope in developing intercultural competence. Throughout, we consider implications for clinical practice and training."
STEVEN SANDAGE,"Virtue, flourishing, and religious and cultural diversity in psychotherapy","[The key thesis across many virtue traditions that growth in character strengths and relational virtues should lead to growth in human flourishing remains mostly untested in the context of mental healthcare. We should empirically test this big idea with appropriate attention to the potentially complex relationships between mental distress, virtue, and flourishing among diverse individuals and contexts over time. ]"
STEVEN SANDAGE,"Analysis in mixed method designs, part 1",
STEVEN SANDAGE,Virtue and flourishing in mental healthcare,"[The key thesis across many virtue traditions that growth in character strengths and virtues should lead to growth in human flourishing remains untested in the context of mental healthcare. We need to empirically test this big idea with appropriate attention to the potentially complex relationships between mental distress, virtue, and flourishing among diverse individuals and contexts over time.]"
STEVEN SANDAGE,Humility as a predictor of eudaimonic flourishing among adult clients: mediator effects for self-regulation,"The scientific literature on humility has grown rapidly over the past decade with hundreds of studies now available, yet very few studies have investigated humility in clinical settings. Some clinicians might question the relevance of humility to key mental health and well being outcomes for clients, and there have been some discrepant findings on humi lity and well being in non clinical settings. The present cross sectional study tested an emotion regulation model of humility and well being drawing on attachment and family systems theories and research in positive psychology in a sample of Adult outpati ent clients (N=147) at a community mental health clinic in the United States. Dependent variables included: (a) Eudaimonic well being or flourishing (Mental Health Continuum Short Form; Lamers et al., 2011), (b) life satisfaction problems (Treatment Out come Package/TOP; Krauss et al., 2005), (c) work functioning problems (TOP), and (d) general health (TOP). Humility was operationalized using the General Humility Scale (Hill et al., 2015), a multi dimensional measure previously used in the same clinical c ontext (Paine et al., 2018). Results found humility was significantly related to each outcome in predicted directions with mediator effects for emotion regulation. Findings are discussed in terms of future research (particularly further validation of clinical measures of virtue and flourishing) and clinical practice."
STEVEN SANDAGE,"Positive psychology, virtue, and flourishing in psychotherapy","The field of mental healthcare in the United States largely promotes a view of the human person as ""clusters of negative symptoms"" and focuses predominately on reducing these negative symptoms in individuals rather than on additional pathways towards social and relational well being. This dominant approach, based on the medical disease model of mental illness, includes certain strengths and efficiencies. However, the large body of positive psychology research since the 1990s has advanced scientific understan dings of human strengths and virtues with related investigations of individual and communal flourishing. By ""virtue,"" we mean embodied traits of character that tend to promote resilience and the integration of ethics and health toward the ultimate goals of both personal and communal flourishing (e.g., humility, forgiveness, gratitude, compassion, and justice among others; Hill & Sandage, 2016). A goal of ""flourishing"" moves beyond hedonic or subjective forms of well being, and each of the research teams in this panel are informed by eudaimonic or developmental theories of well being characterized by relational maturity, meaningful purpose, integrity and the pursuit of virtue, and communal concern through prosocial behavior (Boettscher, Sandage, Latin, & Barl ow, 2019; Waterman, 2013). To date, efforts at integrating positive psychology into psychotherapy have tended to involve the development of new ""positive psychotherapies,"" however we will consider the possibilities (and challenges) of integrating positive psychology into mainstream psychotherapy approaches. Drawing on work from a multi site Templeton grant, this panel will describe ways these complex topics of virtue and flourishing can be engaged in psychotherapy research and practice."
STEVEN SANDAGE,"Mental health, well-being, and experiences of the COVID-19 pandemic: a mixed methods practice-based study",
STEVEN SANDAGE,"Exploring virtue ethics in psychodynamic psychotherapy: latent changes in humility, affect regulation, symptoms, and well-being","[Empirical exploration of a salutary role for virtues on both mental health symptoms and well-being has increased. Yet, tests for this role in psychotherapy have not matched research in other contexts. As such, we tested the virtue ethics premise that growth in humility could facilitate changes in symptoms and well-being in the context of contemporary relational psychotherapy (CRP; Sandage et al., 2020). CRP is grounded in three premises: (a) conceptualizing clients within their historical, familial and sociocultural contexts, (b) understanding distress as stemming from maladaptive intra- and interpersonal patterns, and (c) prioritizing a here-and-now focus within the therapeutic relationship. We proposed experiential avoidance as the mechanism whereby humility influences symptoms and well-being.]"
STEVEN SANDAGE,(Re)Framing resilience: a trajectory-based study involving emerging religious/spiritual leaders,"The COVID-19 pandemic has provided a unique circumstance for the study of resilience, and clergy resilience has garnered increased research attention due to greater recognition that religious/spiritual leaders are at risk for elevated levels of anxiety and burnout. We examined longitudinal patterns of change during the pandemic in a sample of emerging leaders (N = 751; Mage = 32.82; SD 11.37; 49.9% female; 59.8% White). In doing so, we offered a conceptual and methodological approach based on historical and critical evaluations of the study of resilience. Results revealed a subgroup that exhibited resilience over three waves of data. The labeling of this trajectory was based on established criteria for determining resilience: (a) significant adversity in the form of COVID-19 stress at time 1, which included the highest levels of the subjective appraisal of stress; (b) risk in the form of low religiousness/spirituality and greater likelihood of reporting marginalized identifications, relative to those who were flourishing; (c) a protective influence for transformative experiences to promote positive adaptation; and (d) interruption to the trajectory in the form of improvement in levels of symptoms and well-being. Practical implications center on the potential for transformative experiences to clarify emotional experience and construct new meaning."
STEVEN SANDAGE,Virtue formation begins in the face of the other,Presentation was a response to talk given by Dr. Liz Gulliford from Jubilee Centre For Character and Virtue
STEVEN SANDAGE,Promoting flourishing and preventing trauma among religious leaders: a relational spirituality model,
STEVEN SANDAGE,Intellectual humility among religious leaders,
ABIGAIL GILLMAN,Futurity: contemporary literature and the quest for the past by Amir Eshel,
ABIGAIL GILLMAN,The labor of secrecy: interpreting parables from the Bible to Kafka,
LEE M WETZLER,Bronchus-Associated Lymphoid Tissue (BALT) and Survival in a Vaccine Mouse Model of Tularemia,"BACKGROUND. Francisella tularensis causes severe pulmonary disease, and nasal vaccination could be the ideal measure to effectively prevent it. Nevertheless, the efficacy of this type of vaccine is influenced by the lack of an effective mucosal adjuvant. METHODOLOGY/PRINCIPAL FINDINGS. Mice were immunized via the nasal route with lipopolysaccharide isolated from F. tularensis and neisserial recombinant PorB as an adjuvant candidate. Then, mice were challenged via the same route with the F. tularensis attenuated live vaccine strain (LVS). Mouse survival and analysis of a number of immune parameters were conducted following intranasal challenge. Vaccination induced a systemic antibody response and 70% of mice were protected from challenge as showed by their improved survival and weight regain. Lungs from mice recovering from infection presented prominent lymphoid aggregates in peribronchial and perivascular areas, consistent with the location of bronchus-associated lymphoid tissue (BALT). BALT areas contained proliferating B and T cells, germinal centers, T cell infiltrates, dendritic cells (DCs). We also observed local production of antibody generating cells and homeostatic chemokines in BALT areas. CONCLUSIONS. These data indicate that PorB might be an optimal adjuvant candidate for improving the protective effect of F. tularensis antigens. The presence of BALT induced after intranasal challenge in vaccinated mice might play a role in regulation of local immunity and long-term protection, but more work is needed to elucidate mechanisms that lead to its formation."
HELEN B TAGER-FLUSBERG,Neural processing of facial identity and emotion in infants at high-risk for autism spectrum disorders,"Deficits in face processing and social impairment are core characteristics of autism spectrum disorder. The present work examined 7-month-old infants at high-risk for developing autism and typically developing controls at low-risk, using a face perception task designed to differentiate between the effects of face identity and facial emotions on neural response using functional Near-Infrared Spectroscopy. In addition, we employed independent component analysis, as well as a novel method of condition-related component selection and classification to identify group differences in hemodynamic waveforms and response distributions associated with face and emotion processing. The results indicate similarities of waveforms, but differences in the magnitude, spatial distribution, and timing of responses between groups. These early differences in local cortical regions and the hemodynamic response may, in turn, contribute to differences in patterns of functional connectivity."
HELEN B TAGER-FLUSBERG,Functional connectivity in the first year of life in infants at-risk for autism: a preliminary near-infrared spectroscopy study,"BACKGROUND: Autism spectrum disorder (ASD) has been called a “developmental disconnection syndrome,” however the majority of the research examining connectivity in ASD has been conducted exclusively with older children and adults. Yet, prior ASD research suggests that perturbations in neurodevelopmental trajectories begin as early as the first year of life. Prospective longitudinal studies of infants at risk for ASD may provide a window into the emergence of these aberrant patterns of connectivity. The current study employed functional connectivity near-infrared spectroscopy (NIRS) in order to examine the development of intra- and inter-hemispheric functional connectivity in high- and low-risk infants across the first year of life. METHODS: NIRS data were collected from 27 infants at high risk for autism (HRA) and 37 low-risk comparison (LRC) infants who contributed a total of 116 data sets at 3-, 6-, 9-, and 12-months. At each time point, HRA and LRC groups were matched on age, sex, head circumference, and Mullen Scales of Early Learning scores. Regions of interest (ROI) were selected from anterior and posterior locations of each hemisphere. The average time course for each ROI was calculated and correlations for each ROI pair were computed. Differences in functional connectivity were examined in a cross-sectional manner. RESULTS: At 3-months, HRA infants showed increased overall functional connectivity compared to LRC infants. This was the result of increased connectivity for intra- and inter-hemispheric ROI pairs. No significant differences were found between HRA and LRC infants at 6- and 9-months. However, by 12-months, HRA infants showed decreased connectivity relative to LRC infants. CONCLUSIONS: Our preliminary results suggest that atypical functional connectivity may exist within the first year of life in HRA infants, providing support to the growing body of evidence that aberrant patterns of connectivity may be a potential endophenotype for ASD."
HELEN B TAGER-FLUSBERG,A multimeasure approach to investigating affective appraisal of social information in Williams syndrome,"People with Williams syndrome (WS) have been consistently described as showing heightened sociability, gregariousness, and interest in people, in conjunction with an uneven cognitive profile and mild to moderate intellectual or learning disability. To explore the mechanisms underlying this unusual social–behavioral phenotype, we investigated whether individuals with WS show an atypical appraisal style and autonomic responsiveness to emotionally laden images with social or nonsocial content. Adolescents and adults with WS were compared to chronological age-matched and nonverbal mental age-matched groups in their responses to positive and negative images with or without social content, using measures of self-selected viewing time (SSVT), autonomic arousal reflected in pupil dilation measures, and likeability ratings. The participants with WS looked significantly longer at the social images compared to images without social content and had reduced arousal to the negative social images compared to the control groups. In contrast to the comparison groups, the explicit ratings of likeability in the WS group did not correlate with their SSVT; instead, they reflected an appraisal style of more extreme ratings. This distinctive pattern of viewing interest, likeability ratings, and autonomic arousal to images with social content in the WS group suggests that their heightened social drive may be related to atypical functioning of reward-related brain systems reflected in SSVT and autonomic reactivity measures, but not in explicit ratings."
HELEN B TAGER-FLUSBERG,Neural processing of repetition and non-repetition grammars in 7-and 9-month-old infants,"An essential aspect of infant language development involves the extraction of meaningful information from a continuous stream of auditory input. Studies have identified early abilities to differentiate auditory input along various dimensions, including the presence or absence of structural regularities. In newborn infants, frontal and temporal regions were found to respond differentially to these regularities (Gervain et al., 2008), and in order to examine the development of this abstract rule learning we presented 7- and 9-month-old infants with syllables containing an ABB pattern (e.g., “balolo”) or an ABC pattern (e.g., “baloti”) and measured activity in left and right lateral brain regions using near-infrared spectroscopy (NIRS). While prior newborn work found increases in oxyhemoglobin (oxyHb) activity in response to ABB blocks as compared to ABC blocks in anterior regions, 7- and 9-month-olds showed no differentiation between grammars in oxyHb. However, changes in deoxyhemoglobin (deoxyHb) pointed to a developmental shift, whereby 7-month-olds showed deoxyHb responding significantly different from zero for ABB blocks, but not ABC blocks, and 9-month-olds showed the opposite pattern, with deoxyHb responding significantly different from zero for the ABC blocks but not the ABB blocks. DeoxyHb responses were more pronounced over anterior regions. A grammar by time interaction also illustrated that during the early blocks, deoxyHb was significantly greater to ABC than in later blocks, but there was no change in ABB activation over time. The shift from stronger activation to ABB in newborns (Gervain et al., 2008) and 7-month-olds in the present study to stronger activation to ABC by 9-month-olds here is discussed in terms of changes in stimulus salience and novelty preference over the first year of life. The present discussion also highlights the importance of future work exploring the coupling between oxyHb and deoxyHb activation in infant NIRS studies."
ADAM D SMITH,Priorities for synthesis research in ecology and environmental science,
ADAM D SMITH,Turning HATE into LOVE: compact homomorphic ad hoc threshold encryption for scalable MPC,"In a public-key threshold encryption scheme, the sender produces a single ciphertext, and any 𝑡+1 out of 𝑛 intended recipients can combine their partial decryptions to obtain the plaintext. Ad hoc threshold encryption (ATE) schemes require no correlated setup, enabling each party to simply generate its own key pair. In this paper, we initiate a systematic study of the possibilities and limitations of ad-hoc threshold encryption, and introduce a key application to scalable multiparty computation (MPC). Assuming indistinguishability obfuscation (iO), we construct the first ATE that is sender-compact—that is, with ciphertext length independent of 𝑛. This allows for succinct communication once public keys have been shared. We also show a basic lower bound on the extent of key sharing: every sender-compact scheme requires that recipients of a message know the public keys of other recipients in order to decrypt. We then demonstrate that threshold encryption that is ad hoc and homomorphic can be used to build efficient large-scale fault-tolerant multiparty computation (MPC) on a minimal (star) communication graph. We explore several homomorphic schemes, in particular obtaining one iO-based ATE scheme that is both sender-compact and homomorphic: each recipient can derive what they need for evaluation from a single short ciphertext. In the resulting MPC protocol, once the public keys have been distributed, all parties in the graph except for the central server send and receive only short messages, whose size is independent of the number of participants. Taken together, our results chart new possibilities for threshold encryption and raise intriguing open questions."
ADAM D SMITH,Reproductive inequality in humans and other mammals,"To address claims of human exceptionalism, we determine where humans fit within the greater mammalian distribution of reproductive inequality. We show that humans exhibit lower reproductive skew (i.e., inequality in the number of surviving offspring) among males and smaller sex differences in reproductive skew than most other mammals, while nevertheless falling within the mammalian range. Additionally, female reproductive skew is higher in polygynous human populations than in polygynous nonhumans mammals on average. This patterning of skew can be attributed in part to the prevalence of monogamy in humans compared to the predominance of polygyny in nonhuman mammals, to the limited degree of polygyny in the human societies that practice it, and to the importance of unequally held rival resources to women's fitness. The muted reproductive inequality observed in humans appears to be linked to several unusual characteristics of our species-including high levels of cooperation among males, high dependence on unequally held rival resources, complementarities between maternal and paternal investment, as well as social and legal institutions that enforce monogamous norms."
ADAM D SMITH,Prospects for beyond the standard model physics searches at the deep underground neutrino experiment: DUNE collaboration,"The Deep Underground Neutrino Experiment (DUNE) will be a powerful tool for a variety of physics topics. The high-intensity proton beams provide a large neutrino flux, sampled by a near detector system consisting of a combination of capable precision detectors, and by the massive far detector system located deep underground. This configuration sets up DUNE as a machine for discovery, as it enables opportunities not only to perform precision neutrino measurements that may uncover deviations from the present three-flavor mixing paradigm, but also to discover new particles and unveil new interactions and symmetries beyond those predicted in the Standard Model (SM). Of the many potential beyond the Standard Model (BSM) topics DUNE will probe, this paper presents a selection of studies quantifying DUNE's sensitivities to sterile neutrino mixing, heavy neutral leptons, non-standard interactions, CPT symmetry violation, Lorentz invariance violation, neutrino trident production, dark matter from both beam induced and cosmogenic sources, baryon number violation, and other new physics topics that complement those at high-energy colliders and significantly extend the present reach."
ADAM D SMITH,Identification and reconstruction of low-energy electrons in the ProtoDUNE-SP detector,
ADAM D SMITH,The long-baseline neutrino experiment: exploring fundamental symmetries of the universe,"The preponderance of matter over antimatter in the early Universe, the dynamics of the supernova bursts that produced the heavy elements necessary for life and whether protons eventually decay --- these mysteries at the forefront of particle physics and astrophysics are key to understanding the early evolution of our Universe, its current state and its eventual fate. The Long-Baseline Neutrino Experiment (LBNE) represents an extensively developed plan for a world-class experiment dedicated to addressing these questions. LBNE is conceived around three central components: (1) a new, high-intensity neutrino source generated from a megawatt-class proton accelerator at Fermi National Accelerator Laboratory, (2) a near neutrino detector just downstream of the source, and (3) a massive liquid argon time-projection chamber deployed as a far detector deep underground at the Sanford Underground Research Facility. This facility, located at the site of the former Homestake Mine in Lead, South Dakota, is approximately 1,300 km from the neutrino source at Fermilab -- a distance (baseline) that delivers optimal sensitivity to neutrino charge-parity symmetry violation and mass ordering effects. This ambitious yet cost-effective design incorporates scalability and flexibility and can accommodate a variety of upgrades and contributions. With its exceptional combination of experimental configuration, technical capabilities, and potential for transformative discoveries, LBNE promises to be a vital facility for the field of particle physics worldwide, providing physicists from around the globe with opportunities to collaborate in a twenty to thirty year program of exciting science. In this document we provide a comprehensive overview of LBNE's scientific objectives, its place in the landscape of neutrino physics worldwide, the technologies it will incorporate and the capabilities it will possess."
ADAM D SMITH,Scintillation light detection in the 6-m drift-length ProtoDUNE Dual Phase liquid argon TPC,"DUNE is a dual-site experiment for long-baseline neutrino oscillation studies, neutrino astrophysics and nucleon decay searches. ProtoDUNE Dual Phase (DP) is a 6  ×  6  ×  6 m 3 liquid argon time-projection-chamber (LArTPC) that recorded cosmic-muon data at the CERN Neutrino Platform in 2019-2020 as a prototype of the DUNE Far Detector. Charged particles propagating through the LArTPC produce ionization and scintillation light. The scintillation light signal in these detectors can provide the trigger for non-beam events. In addition, it adds precise timing capabilities and improves the calorimetry measurements. In ProtoDUNE-DP, scintillation and electroluminescence light produced by cosmic muons in the LArTPC is collected by photomultiplier tubes placed up to 7 m away from the ionizing track. In this paper, the ProtoDUNE-DP photon detection system performance is evaluated with a particular focus on the different wavelength shifters, such as PEN and TPB, and the use of Xe-doped LAr, considering its future use in giant LArTPCs. The scintillation light production and propagation processes are analyzed and a comparison of simulation to data is performed, improving understanding of the liquid argon properties."
MARIE-HELENE SAINT-HILAIRE,Association of Cumulative Lead Exposure with Parkinson's Disease,"BACKGROUND. Research using reconstructed exposure histories has suggested an association between heavy metal exposures, including lead, and Parkinson's disease (PD), but the only study that used bone lead, a biomarker of cumulative lead exposure, found a nonsignificant increase in risk of PD with increasing bone lead. OBJECTIVES. We sought to assess the association between bone lead and PD. METHODS. Bone lead concentrations were measured using 109Cd excited K-shell X-ray fluorescence from 330 PD patients (216 men, 114 women) and 308 controls (172 men, 136 women) recruited from four clinics for movement disorders and general-community cohorts. Adjusted odds ratios (ORs) for PD were calculated using logistic regression. RESULTS. The average age of cases and controls at bone lead measurement was 67 (SD = 10) and 69 (SD = 9) years of age, respectively. In primary analyses of cases and controls recruited from the same groups, compared with the lowest quartile of tibia lead, the OR for PD in the highest quartile was 3.21 [95% confidence interval (CI), 1.17-8.83]. Results were similar but slightly weaker in analyses restricted to cases and controls recruited from the movement disorders clinics only (fourth-quartile OR = 2.57; 95% CI, 1.11-5.93) or when we included controls recruited from sites that did not also contribute cases (fourth-quartile OR = 1.91; 95% CI, 1.01-3.60). We found no association with patella bone lead. CONCLUSIONS. These findings, using an objective biological marker of cumulative lead exposure among typical PD patients seen in our movement disorders clinics, strengthen the evidence that cumulative exposure to lead increases the risk of PD."
MARIE-HELENE SAINT-HILAIRE,"Design of the WHIP-PD study: a phase II, twelve-month, dual-site, randomized controlled trial evaluating the effects of a cognitive-behavioral approach for promoting enhanced walking activity using mobile health technology in people with Parkinson-disease","BACKGROUND: Parkinson disease (PD) is a debilitating and chronic neurodegenerative disease resulting in ambulation difficulties. Natural walking activity often declines early in disease progression despite the relative stability of motor impairments. In this study, we propose a paradigm shift with a ""connected behavioral approach"" that targets real-world walking using cognitive-behavioral training and mobile health (mHealth) technology. METHODS/DESIGN: The Walking and mHealth to Increase Participation in Parkinson Disease (WHIP-PD) study is a twelve-month, dual site, two-arm, randomized controlled trial recruiting 148 participants with early to mid-stage PD. Participants will be randomly assigned to connected behavioral or active control conditions. Both conditions will include a customized program of goal-oriented walking, walking-enhancing strengthening exercises, and eight in-person visits with a physical therapist. Participants in the connected behavioral condition also will (1) receive cognitive-behavioral training to promote self-efficacy for routine walking behavior and (2) use a mHealth software application to manage their program and communicate remotely with their physical therapist. Active control participants will receive no cognitive-behavioral training and manage their program on paper. Evaluations will occur at baseline, three-, six-, and twelve-months and include walking assessments, self-efficacy questionnaires, and seven days of activity monitoring. Primary outcomes will include the change between baseline and twelve months in overall amount of walking activity (mean number of steps per day) and amount of moderate intensity walking activity (mean number of minutes per day in which > 100 steps were accumulated). Secondary outcomes will include change in walking capacity as measured by the six-minute walk test and ten-meter walk test. We also will examine if self-efficacy mediates change in amount of walking activity and if change in amount of walking activity mediates change in walking capacity. DISCUSSION: We expect this study to show the connected behavioral approach will be more effective than the active control condition in increasing the amount and intensity of real-world walking activity and improving walking capacity. Determining effective physical activity interventions for persons with PD is important for preserving mobility and essential for maintaining quality of life. Clinical trials registration NCT03517371, May 7, 2018. TRIAL REGISTRATION: ClinicalTrials.gov: NCT03517371. Date of registration: May 7, 2018. Protocol version: Original."
SIOBHAN O'MAHONY,Opening the aperture: explaining the complementary roles of advice and testing when forming entrepreneurial strategy,"Forming entrepreneurial strategy is difficult as the future value of strategy alternatives is uncertain. To create and capture value, firms are advised to consider and test multiple alternative strategy elements. Yet, how firms generate and test alternatives remains understudied. As entrepreneurial firms lack resources for broad search, they often draw upon advisory resources from outside the firm. However, advice can be difficult to extract, absorb and apply. While scholars have examined static attributes of the entrepreneur or advisor to explain if advice is used, a dynamic explanation of how advice is produced and informs strategy testing and formation is missing. In an 11-month field study, we observed 25 founders of 12 food and agriculture firms interacting with a common pool of 34 advisors in an entrepreneurship training program. Leveraging the program’s structured design, we observed 165 advice interactions over three phases. No firm took advice and applied it directly to firm strategy. When entrepreneurs engaged literally with advice, they later discounted it – distancing advice from strategy. In contrast, entrepreneurs that coproduced advice challenged advisors to craft novel advice relevant to their strategy, translated it to make it actionable, and tested it – integrating advice into strategy. Firms that distanced advice from strategy did not test strategy alternatives, while firms that integrated advice into strategy tested multiple alternatives, explored broader markets and adapted their strategies. We contribute a grounded process model that explains how coproducing advice opens firms’ apertures to consider strategy alternatives, while testing informs the strategy elements chosen."
BARBARA L PHILIPP,Experimenting with Open Innovation in Science (OIS) practices: a novel approach to co-developing research proposals,
STEPHEN P CHRISTIANSEN,"The L 98-59 system: three transiting, terrestrial-size planets orbiting a nearby M dwarf","We report the Transiting Exoplanet Survey Satellite (TESS) discovery of three terrestrial-size planets transiting L 98-59 (TOI-175, TIC 307210830)—a bright M dwarf at a distance of 10.6 pc. Using the Gaia-measured distance and broadband photometry, we find that the host star is an M3 dwarf. Combined with the TESS transits from three sectors, the corresponding stellar parameters yield planet radii ranging from 0.8 R ⊕ to 1.6 R ⊕. All three planets have short orbital periods, ranging from 2.25 to 7.45 days with the outer pair just wide of a 2:1 period resonance. Diagnostic tests produced by the TESS Data Validation Report and the vetting package DAVE rule out common false-positive sources. These analyses, along with dedicated follow-up and the multiplicity of the system, lend confidence that the observed signals are caused by planets transiting L 98-59 and are not associated with other sources in the field. The L 98-59 system is interesting for a number of reasons: the host star is bright (V = 11.7 mag, K = 7.1 mag) and the planets are prime targets for further follow-up observations including precision radial-velocity mass measurements and future transit spectroscopy with the James Webb Space Telescope; the near-resonant configuration makes the system a laboratory to study planetary system dynamical evolution; and three planets of relatively similar size in the same system present an opportunity to study terrestrial planets where other variables (age, metallicity, etc.) can be held constant. L 98-59 will be observed in four more TESS sectors, which will provide a wealth of information on the three currently known planets and have the potential to reveal additional planets in the system."
TAMAR F BARLAM,Revitalizing the infection prevention workforce with a fellowship program for underrepresented groups,"Infection preventionist (IP) positions are difficult to fill, and future workforce shortages are anticipated. The IP field has less racial and ethnic diversity than the general nursing workforce or patient population. A targeted fellowship program for underrepresented groups allowed the recruitment and training of IPs while avoiding staffing shortages."
GREGORY WELLENIUS,"Traffic-Related Air Pollution and QT Interval: Modification by Diabetes, Obesity, and Oxidative Stress Gene Polymorphisms in the Normative Aging Study","BACKGROUND. Acute exposure to ambient air pollution has been associated with acute changes in cardiac outcomes, often within hours of exposure. OBJECTIVES. We examined the effects of air pollutants on heart-rate-corrected QT interval (QTc), an electrocardiographic marker of ventricular repolarization, and whether these associations were modified by participant characteristics and genetic polymorphisms related to oxidative stress. METHODS. We studied repeated measurements of QTc on 580 men from the Veterans Affairs Normative Aging Study (NAS) using mixed-effects models with random intercepts. We fitted a quadratic constrained distributed lag model to estimate the cumulative effect on QTc of ambient air pollutants including fine particulate matter ≤ 2.5 μm in aerodynamic diameter (PM2.5), ozone (O3), black carbon (BC), nitrogen dioxide (NO2), carbon monoxide (CO), and sulfur dioxide (SO2) concentrations during the 10 hr before the visit. We genotyped polymorphisms related to oxidative stress and analyzed pollution-susceptibility score interactions using the genetic susceptibility score (GSS) method. RESULTS. Ambient traffic pollutant concentrations were related to longer QTc. An interquartile range (IQR) change in BC cumulative during the 10 hr before the visit was associated with increased QTc [1.89 msec change; 95% confidence interval (CI), -0.16 to 3.93]. We found a similar association with QTc for an IQR change in 1-hr BC that occurred 4 hr before the visit (2.54 msec change; 95% CI, 0.28-4.80). We found increased QTc for IQR changes in NO2 and CO, but the change was statistically insignificant. In contrast, we found no association between QTc and PM2.5, SO2, and O3. The association between QTc and BC was stronger among participants who were obese, who had diabetes, who were nonsmokers, or who had higher GSSs. CONCLUSIONS. Traffic-related pollutants may increase QTc among persons with diabetes, persons who are obese, and nonsmoking elderly individuals; the number of genetic variants related to oxidative stress increases this effect."
GREGORY WELLENIUS,Inequality in the availability of residential air conditioning across 115 US metropolitan areas,"Continued climate change is increasing the frequency, severity, and duration of populations' high temperature exposures. Indoor cooling is a key adaptation, especially in urban areas, where heat extremes are intensified-the urban heat island effect (UHI)-making residential air conditioning (AC) availability critical to protecting human health. In the United States, the differences in residential AC prevalence from one metropolitan area to another is well understood, but its intra-urban variation is poorly characterized, obscuring neighborhood-scale variability in populations' heat vulnerability and adaptive capacity. We address this gap by constructing empirically derived probabilities of residential AC for 45,995 census tracts across 115 metropolitan areas. Within cities, AC is unequally distributed, with census tracts in the urban ""core"" exhibiting systematically lower prevalence than their suburban counterparts. Moreover, this disparity correlates strongly with multiple indicators of social vulnerability and summer daytime surface UHI intensity, highlighting the challenges that vulnerable urban populations face in adapting to climate-change driven heat stress amplification."
MARY E COLLINS,The eighteenth data release of the Sloan Digital Sky Surveys: targeting and first spectra from SDSS-V,"The eighteenth data release (DR18) of the Sloan Digital Sky Survey (SDSS) is the first one for SDSS-V, the fifth generation of the survey. SDSS-V comprises three primary scientific programs or “Mappers”: the Milky Way Mapper (MWM), the Black Hole Mapper (BHM), and the Local Volume Mapper. This data release contains extensive targeting information for the two multiobject spectroscopy programs (MWM and BHM), including input catalogs and selection functions for their numerous scientific objectives. We describe the production of the targeting databases and their calibration and scientifically focused components. DR18 also includes ∼25,000 new SDSS spectra and supplemental information for X-ray sources identified by eROSITA in its eFEDS field. We present updates to some of the SDSS software pipelines and preview changes anticipated for DR19. We also describe three value-added catalogs (VACs) based on SDSS-IV data that have been published since DR17, and one VAC based on the SDSS-V data in the eFEDS field."
JOYCE Y WONG,Generation of a purified iPSC-derived smooth muscle-like population for cell sheet engineering,"Induced pluripotent stem cells (iPSCs) provide a potential source for the derivation of smooth muscle cells (SMCs); however, current approaches are limited by the production of heterogeneous cell types and a paucity of tools or markers for tracking and purifying candidate SMCs. Here, we develop murine and human iPSC lines carrying fluorochrome reporters (Acta2hrGFP and ACTA2eGFP, respectively) that identify Acta2+/ACTA2+ cells as they emerge in vitro in real time during iPSC-directed differentiation. We find that Acta2hrGFP+ and ACTA2eGFP+ cells can be sorted to purity and are enriched in markers characteristic of an immature or synthetic SMC. We characterize the resulting GFP+ populations through global transcriptomic profiling and functional studies, including the capacity to form engineered cell sheets. We conclude that these reporter lines allow for generation of sortable, live iPSC-derived Acta2+/ACTA2+ cells highly enriched in smooth muscle lineages for basic developmental studies, tissue engineering, or future clinical regenerative applications."
JOYCE Y WONG,"Silk-fibronectin protein alloy fibres support cell adhesion and viability as a high strength, matrix fibre analogue","Silk is a natural polymer with broad utility in biomedical applications because it exhibits general biocompatibility and high tensile material properties. While mechanical integrity is important for most biomaterial applications, proper function and integration also requires biomaterial incorporation into complex surrounding tissues for many physiologically relevant processes such as wound healing. In this study, we spin silk fibroin into a protein alloy fibre with whole fibronectin using wet spinning approaches in order to synergize their respective strength and cell interaction capabilities. Results demonstrate that silk fibroin alone is a poor adhesive surface for fibroblasts, endothelial cells, and vascular smooth muscle cells in the absence of serum. However, significantly improved cell attachment is observed to silk-fibronectin alloy fibres without serum present while not compromising the fibres' mechanical integrity. Additionally, cell viability is improved up to six fold on alloy fibres when serum is present while migration and spreading generally increase as well. These findings demonstrate the utility of composite protein alloys as inexpensive and effective means to create durable, biologically active biomaterials."
ROBERTO PAIELLA,Optical properties of tensilely strained Ge nanomembranes,"Group-IV semiconductors, which provide the leading materials platform of micro- electronics, are generally unsuitable for light emitting device applications because of their indirect- bandgap nature. This property currently limits the large-scale integration of electronic and photonic functionalities on Si chips. The introduction of tensile strain in Ge, which has the effect of lowering the direct conduction-band minimum relative to the indirect valleys, is a promising approach to address this challenge. Here we review recent work focused on the basic science and technology of mechanically stressed Ge nanomembranes, i.e., single-crystal sheets with thicknesses of a few tens of nanometers, which can sustain particularly large strain levels before the onset of plastic deformation. These nanomaterials have been employed to demonstrate large strain-enhanced photoluminescence, population inversion under optical pumping, and the formation of direct-bandgap Ge. Furthermore, Si-based photonic-crystal cavities have been developed that can be combined with these Ge nanomembranes without limiting their mechanical flexibility. These results highlight the potential of strained Ge as a CMOS-compatible laser material, and more in general the promise of nanomembrane strain engineering for novel device technologies."
ROBERTO PAIELLA,One-dimensional carbon nanostructures for terahertz electron-beam radiation,"One-dimensional carbon nanostructures such as nanotubes and nanoribbons can feature near-ballistic electronic transport over micron-scale distances even at room temperature. As a result, these materials provide a uniquely suited solid-state platform for radiation mechanisms that so far have been the exclusive domain of electron beams in vacuum. Here we consider the generation of terahertz light based on two such mechanisms, namely, the emission of cyclotronlike radiation in a sinusoidally corrugated nanowire (where periodic angular motion is produced by the mechanical corrugation rather than an externally applied magnetic field), and the Smith-Purcell effect in a rectilinear nanowire over a dielectric grating. In both cases, the radiation properties of the individual charge carriers are investigated via full-wave electrodynamic simulations, including dephasing effects caused by carrier collisions. The overall light output is then computed with a standard model of charge transport for two particularly suitable types of carbon nanostructures, i.e., zigzag graphene nanoribbons and armchair single-wall nanotubes. Relatively sharp emission peaks at geometrically tunable terahertz frequencies are obtained in each case. The corresponding output powers are experimentally accessible even with individual nanowires, and can be scaled to technologically significant levels using array configurations. These radiation mechanisms therefore represent a promising paradigm for light emission in condensed matter, which may find important applications in nanoelectronics and terahertz photonics."
ROBERTO PAIELLA,Plasmonic ommatidia for lensless compound-eye vision,"The vision system of arthropods such as insects and crustaceans is based on the compound-eye architecture, consisting of a dense array of individual imaging elements (ommatidia) pointing along different directions. This arrangement is particularly attractive for imaging applications requiring extreme size miniaturization, wide-angle fields of view, and high sensitivity to motion. However, the implementation of cameras directly mimicking the eyes of common arthropods is complicated by their curved geometry. Here, we describe a lensless planar architecture, where each pixel of a standard image-sensor array is coated with an ensemble of metallic plasmonic nanostructures that only transmits light incident along a small geometrically-tunable distribution of angles. A set of near-infrared devices providing directional photodetection peaked at different angles is designed, fabricated, and tested. Computational imaging techniques are then employed to demonstrate the ability of these devices to reconstruct high-quality images of relatively complex objects."
WILLIAM G ADAMS,Predicting chronic disease hospitalizations from electronic health records: an interpretable classification approach,"Urban living in modern large cities has significant adverse effects on health, increasing the risk of several chronic diseases. We focus on the two leading clusters of chronic diseases, heart disease and diabetes, and develop data-driven methods to predict hospitalizations due to these conditions. We base these predictions on the patients' medical history, recent and more distant, as described in their Electronic Health Records (EHRs). We formulate the prediction problem as a binary classification problem and consider a variety of machine learning methods, including kernelized and sparse Support Vector Machines (SVMs), sparse logistic regression, and random forests. To strike a balance between accuracy and interpretability of the prediction, which is important in a medical setting, we propose two novel methods: K -LRT, a likelihood ratio test-based method, and a Joint Clustering and Classification (JCC) method which identifies hidden patient clusters and adapts classifiers to each cluster. We develop theoretical out-of-sample guarantees for the latter method. We validate our algorithms on large data sets from the Boston Medical Center, the largest safety-net hospital system in New England."
WILLIAM G ADAMS,"Canvass: a crowd-sourced, natural-product screening library for exploring biological space",
WILLIAM G ADAMS,Priorities for synthesis research in ecology and environmental science,
WILLIAM G ADAMS,Polarimetric properties of Event Horizon Telescope targets from ALMA,"We present the results from a full polarization study carried out with the Atacama Large Millimeter/submillimeter Array (ALMA) during the first Very Long Baseline Interferometry (VLBI) campaign, which was conducted in 2017 April in the λ3 mm and λ1.3 mm bands, in concert with the Global mm-VLBI Array (GMVA) and the Event Horizon Telescope (EHT), respectively. We determine the polarization and Faraday properties of all VLBI targets, including Sgr A*, M87, and a dozen radio-loud active galactic nuclei (AGNs), in the two bands at several epochs in a time window of 10 days. We detect high linear polarization fractions (2%–15%) and large rotation measures (RM &gt; 103.3–105.5 rad m−2), confirming the trends of previous AGN studies at millimeter wavelengths. We find that blazars are more strongly polarized than other AGNs in the sample, while exhibiting (on average) order-of-magnitude lower RM values, consistent with the AGN viewing angle unification scheme. For Sgr A* we report a mean RM of (−4.2 ± 0.3) × 105 rad m−2 at 1.3 mm, consistent with measurements over the past decade and, for the first time, an RM of (–2.1 ± 0.1) × 105 rad m−2 at 3 mm, suggesting that about half of the Faraday rotation at 1.3 mm may occur between the 3 mm photosphere and the 1.3 mm source. We also report the first unambiguous measurement of RM toward the M87 nucleus at millimeter wavelengths, which undergoes significant changes in magnitude and sign reversals on a one year timescale, spanning the range from −1.2 to 0.3 × 105 rad m−2 at 3 mm and −4.1 to 1.5 × 105 rad m−2 at 1.3 mm. Given this time variability, we argue that, unlike the case of Sgr A*, the RM in M87 does not provide an accurate estimate of the mass accretion rate onto the black hole. We put forward a two-component model, comprised of a variable compact region and a static extended region, that can simultaneously explain the polarimetric properties observed by both the EHT (on horizon scales) and ALMA (which observes the combined emission from both components). These measurements provide critical constraints for the calibration, analysis, and interpretation of simultaneously obtained VLBI data with the EHT and GMVA."
WILLIAM G ADAMS,"Cosmology intertwined: a review of the particle physics, astrophysics, and cosmology associated with the cosmological tensions and anomalies",
WILLIAM G ADAMS,A super-earth and sub-neptune transiting the late-type M Dwarf LP 791-18,"Planets occur most frequently around cool dwarfs, but only a handful of specific examples are known to orbit the latest-type M stars. Using TESS photometry, we report the discovery of two planets transiting the low-mass star called LP 791-18 (identified by TESS as TOI 736). This star has spectral type M6V, effective temperature 2960 K, and radius 0.17 R ⊙, making it the third-coolest star known to host planets. The two planets straddle the radius gap seen for smaller exoplanets; they include a 1.1R ⊕ planet on a 0.95 day orbit and a 2.3R ⊕ planet on a 5 day orbit. Because the host star is small the decrease in light during these planets' transits is fairly large (0.4% and 1.7%). This has allowed us to detect both planets' transits from ground-based photometry, refining their radii and orbital ephemerides. In the future, radial velocity observations and transmission spectroscopy can both probe these planets' bulk interior and atmospheric compositions, and additional photometric monitoring would be sensitive to even smaller transiting planets."
WILLIAM G ADAMS,Very regular high-frequency pulsation modes in young intermediate-mass stars,"Asteroseismology probes the internal structures of stars by using their natural pulsation frequencies1. It relies on identifying sequences of pulsation modes that can be compared with theoretical models, which has been done successfully for many classes of pulsators, including low-mass solar-type stars2, red giants3, high-mass stars4 and white dwarfs5. However, a large group of pulsating stars of intermediate mass-the so-called δ Scuti stars-have rich pulsation spectra for which systematic mode identification has not hitherto been possible6,7. This arises because only a seemingly random subset of possible modes are excited and because rapid rotation tends to spoil regular patterns8-10. Here we report the detection of remarkably regular sequences of high-frequency pulsation modes in 60 intermediate-mass main-sequence stars, which enables definitive mode identification. The space motions of some of these stars indicate that they are members of known associations of young stars, as confirmed by modelling of their pulsation spectra."
WILLIAM G ADAMS,"BMQ : Boston medical quarterly: v. 5, no. 1-4",
WILLIAM G ADAMS,The eighteenth data release of the Sloan Digital Sky Surveys: targeting and first spectra from SDSS-V,"The eighteenth data release (DR18) of the Sloan Digital Sky Survey (SDSS) is the first one for SDSS-V, the fifth generation of the survey. SDSS-V comprises three primary scientific programs or “Mappers”: the Milky Way Mapper (MWM), the Black Hole Mapper (BHM), and the Local Volume Mapper. This data release contains extensive targeting information for the two multiobject spectroscopy programs (MWM and BHM), including input catalogs and selection functions for their numerous scientific objectives. We describe the production of the targeting databases and their calibration and scientifically focused components. DR18 also includes ∼25,000 new SDSS spectra and supplemental information for X-ray sources identified by eROSITA in its eFEDS field. We present updates to some of the SDSS software pipelines and preview changes anticipated for DR19. We also describe three value-added catalogs (VACs) based on SDSS-IV data that have been published since DR17, and one VAC based on the SDSS-V data in the eFEDS field."
WILLIAM G ADAMS,First radial velocity results from the MINiature Exoplanet Radial Velocity Array (MINERVA),"The MINiature Exoplanet Radial Velocity Array (MINERVA) is a dedicated observatory of four 0.7 m robotic telescopes fiber-fed to a KiwiSpec spectrograph. The MINERVA mission is to discover super-Earths in the habitable zones of nearby stars. This can be accomplished with MINERVA's unique combination of high precision and high cadence over long time periods. In this work, we detail changes to the MINERVA facility that have occurred since our previous paper. We then describe MINERVA's robotic control software, the process by which we perform 1D spectral extraction, and our forward modeling Doppler pipeline. In the process of improving our forward modeling procedure, we found that our spectrograph's intrinsic instrumental profile is stable for at least nine months. Because of that, we characterized our instrumental profile with a time-independent, cubic spline function based on the profile in the cross dispersion direction, with which we achieved a radial velocity precision similar to using a conventional ""sum-of-Gaussians"" instrumental profile: 1.8 m s−1 over 1.5 months on the RV standard star HD 122064. Therefore, we conclude that the instrumental profile need not be perfectly accurate as long as it is stable. In addition, we observed 51 Peg and our results are consistent with the literature, confirming our spectrograph and Doppler pipeline are producing accurate and precise radial velocities."
WILLIAM G ADAMS,Prospects for beyond the standard model physics searches at the deep underground neutrino experiment: DUNE collaboration,"The Deep Underground Neutrino Experiment (DUNE) will be a powerful tool for a variety of physics topics. The high-intensity proton beams provide a large neutrino flux, sampled by a near detector system consisting of a combination of capable precision detectors, and by the massive far detector system located deep underground. This configuration sets up DUNE as a machine for discovery, as it enables opportunities not only to perform precision neutrino measurements that may uncover deviations from the present three-flavor mixing paradigm, but also to discover new particles and unveil new interactions and symmetries beyond those predicted in the Standard Model (SM). Of the many potential beyond the Standard Model (BSM) topics DUNE will probe, this paper presents a selection of studies quantifying DUNE's sensitivities to sterile neutrino mixing, heavy neutral leptons, non-standard interactions, CPT symmetry violation, Lorentz invariance violation, neutrino trident production, dark matter from both beam induced and cosmogenic sources, baryon number violation, and other new physics topics that complement those at high-energy colliders and significantly extend the present reach."
WILLIAM G ADAMS,Identification and reconstruction of low-energy electrons in the ProtoDUNE-SP detector,
WILLIAM G ADAMS,The long-baseline neutrino experiment: exploring fundamental symmetries of the universe,"The preponderance of matter over antimatter in the early Universe, the dynamics of the supernova bursts that produced the heavy elements necessary for life and whether protons eventually decay --- these mysteries at the forefront of particle physics and astrophysics are key to understanding the early evolution of our Universe, its current state and its eventual fate. The Long-Baseline Neutrino Experiment (LBNE) represents an extensively developed plan for a world-class experiment dedicated to addressing these questions. LBNE is conceived around three central components: (1) a new, high-intensity neutrino source generated from a megawatt-class proton accelerator at Fermi National Accelerator Laboratory, (2) a near neutrino detector just downstream of the source, and (3) a massive liquid argon time-projection chamber deployed as a far detector deep underground at the Sanford Underground Research Facility. This facility, located at the site of the former Homestake Mine in Lead, South Dakota, is approximately 1,300 km from the neutrino source at Fermilab -- a distance (baseline) that delivers optimal sensitivity to neutrino charge-parity symmetry violation and mass ordering effects. This ambitious yet cost-effective design incorporates scalability and flexibility and can accommodate a variety of upgrades and contributions. With its exceptional combination of experimental configuration, technical capabilities, and potential for transformative discoveries, LBNE promises to be a vital facility for the field of particle physics worldwide, providing physicists from around the globe with opportunities to collaborate in a twenty to thirty year program of exciting science. In this document we provide a comprehensive overview of LBNE's scientific objectives, its place in the landscape of neutrino physics worldwide, the technologies it will incorporate and the capabilities it will possess."
WILLIAM G ADAMS,The health equity explorer: an open-source resource for distributed health equity visualization and research across common data models,"Introduction: There is an urgent need to address pervasive inequities in health and healthcare in the USA. Many areas of health inequity are well known, but there remain important unexplored areas, and for many populations in the USA, accessing data to visualize and monitor health equity is difficult. Methods: We describe the development and evaluation of an open-source, R-Shiny application, the “Health Equity Explorer (H2E),” designed to enable users to explore health equity data in a way that can be easily shared within and across common data models (CDMs). Results: We have developed a novel, scalable informatics tool to explore a wide variety of drivers of health, including patient-reported Social Determinants of Health (SDoH), using data in an OMOP CDM research data repository in a way that can be easily shared. We describe our development process, data schema, potential use cases, and pilot data for 705,686 people who attended our health system at least once since 2016. For this group, 996,382 unique observations for questions related to food and housing security were available for 324,630 patients (at least one answer for all 46% of patients) with 65,152 (20.1% of patients with at least one visit and answer) reporting food or housing insecurity at least once. Conclusions: H2E can be used to support dynamic and interactive explorations that include rich social and environmental data. The tool can support multiple CDMs and has the potential to support distributed health equity research and intervention on a national scale."
WILLIAM G ADAMS,Scintillation light detection in the 6-m drift-length ProtoDUNE Dual Phase liquid argon TPC,"DUNE is a dual-site experiment for long-baseline neutrino oscillation studies, neutrino astrophysics and nucleon decay searches. ProtoDUNE Dual Phase (DP) is a 6  ×  6  ×  6 m 3 liquid argon time-projection-chamber (LArTPC) that recorded cosmic-muon data at the CERN Neutrino Platform in 2019-2020 as a prototype of the DUNE Far Detector. Charged particles propagating through the LArTPC produce ionization and scintillation light. The scintillation light signal in these detectors can provide the trigger for non-beam events. In addition, it adds precise timing capabilities and improves the calorimetry measurements. In ProtoDUNE-DP, scintillation and electroluminescence light produced by cosmic muons in the LArTPC is collected by photomultiplier tubes placed up to 7 m away from the ionizing track. In this paper, the ProtoDUNE-DP photon detection system performance is evaluated with a particular focus on the different wavelength shifters, such as PEN and TPB, and the use of Xe-doped LAr, considering its future use in giant LArTPCs. The scintillation light production and propagation processes are analyzed and a comparison of simulation to data is performed, improving understanding of the liquid argon properties."
WILLIAM G ADAMS,Broadband multi-wavelength properties of M87 during the 2017 Event Horizon Telescope campaign,"In 2017, the Event Horizon Telescope (EHT) Collaboration succeeded in capturing the first direct image of the center of the M87 galaxy. The asymmetric ring morphology and size are consistent with theoretical expectations for a weakly accreting supermassive black hole of mass ∼6.5 × 109 M ⊙. The EHTC also partnered with several international facilities in space and on the ground, to arrange an extensive, quasi-simultaneous multi-wavelength campaign. This Letter presents the results and analysis of this campaign, as well as the multi-wavelength data as a legacy data repository. We captured M87 in a historically low state, and the core flux dominates over HST-1 at high energies, making it possible to combine core flux constraints with the more spatially precise very long baseline interferometry data. We present the most complete simultaneous multi-wavelength spectrum of the active nucleus to date, and discuss the complexity and caveats of combining data from different spatial scales into one broadband spectrum. We apply two heuristic, isotropic leptonic single-zone models to provide insight into the basic source properties, but conclude that a structured jet is necessary to explain M87’s spectrum. We can exclude that the simultaneous γ-ray emission is produced via inverse Compton emission in the same region producing the EHT mm-band emission, and further conclude that the γ-rays can only be produced in the inner jets (inward of HST-1) if there are strongly particle-dominated regions. Direct synchrotron emission from accelerated protons and secondaries cannot yet be excluded."
ALICE CRONIN-GOLOMB,Spatial judgment in Parkinson's disease: Contributions of attentional and executive dysfunction,"Spatial judgment is impaired in Parkinson's disease (PD), with previous research suggesting that disruptions in attention and executive function are likely contributors. If judgment of center places demands on frontal systems, performance on tests of attention/executive function may correlate with extent of bias in PD, and attentional disturbance may predict inconsistency in spatial judgment. The relation of spatial judgment to attention/executive function may differ for those with left-side versus right-side motor onset (LPD, RPD), reflecting effects of attentional lateralization. We assessed 42 RPD, 37 LPD, and 67 healthy control participants with a Landmark task (LM) in which a cursor moved horizontally from the right (right-LM) or left (left-LM). The task was to judge the center of the line. Participants also performed neuropsychological tests of attention and executive function. LM group differences were found on left-LM only, with both PD subgroups biased leftward of the control group (RPD p < .05; LPD p < .01; no RPD-LPD difference). For left-LM trials, extent of bias significantly correlated with performance on the cognitive tasks for PD but not for the control group. PD showed greater variability in perceived center than the control group; this variability correlated with performance on the cognitive tasks. The correlations between performance on the test of spatial judgment and the tests of attention/executive function suggest that frontal-based attentional dysfunction affects dynamic spatial judgment, both in extent of spatial bias and in consistency of response as indexed by intertrial variability. (PsycINFO Database Record (c) 2019 APA, all rights reserved)."
ALICE CRONIN-GOLOMB,The relation of anxiety and cognition in Parkinson's disease,"OBJECTIVE: Parkinson’s disease (PD) has long been conceptualized as a motor disorder, but nonmotor symptoms also manifest in the disease and significantly reduce quality of life. Anxiety and cognitive dysfunction are prevalent nonmotor symptoms, even in early disease stages, but the relation between these symptoms remains poorly understood. We examined self-reported anxiety and neurocognitive function, indexed by measures of executive function (set-shifting and phonemic fluency), categorical fluency, and attention/working memory. We hypothesized that anxiety would correlate with cognitive performance. METHOD: The Beck Anxiety Inventory and cognitive tests (Trail Making, Verbal Fluency, Digit Span) were administered to 77 nondemented adults with mild to moderate idiopathic PD (39 men, 38 women; Mage = 62.9 years). RESULTS: Higher anxiety was associated with more advanced disease stage and severity and with poorer set-shifting when using a derived metric to account for motoric slowing. Depression correlated with greater anxiety and disease severity, but not with cognitive performance. CONCLUSIONS: Our findings support the association of anxiety with a specific domain of executive function, set-shifting, in nondemented individuals with mild to moderate PD, raising the possibility that treatment of anxiety may alleviate aspects of executive dysfunction in this population."
ALICE CRONIN-GOLOMB,Altered intrinsic functional coupling between core neurocognitive networks in Parkinson's disease,"Parkinson's disease (PD) is largely attributed to disruptions in the nigrostriatal dopamine system. These neurodegenerative changes may also have a more global effect on intrinsic brain organization at the cortical level. Functional brain connectivity between neurocognitive systems related to cognitive processing is critical for effective neural communication, and is disrupted across neurological disorders. Three core neurocognitive networks have been established as playing a critical role in the pathophysiology of many neurological disorders: the default-mode network (DMN), the salience network (SN), and the central executive network (CEN). In healthy adults, DMN-CEN interactions are anti-correlated while SN-CEN interactions are strongly positively correlated even at rest, when individuals are not engaging in any task. These intrinsic between-network interactions at rest are necessary for efficient suppression of the DMN and activation of the CEN during a range of cognitive tasks. To identify whether these network interactions are disrupted in individuals with PD, we used resting state functional magnetic resonance imaging (rsfMRI) to compare between-network connectivity between 24 PD participants and 20 age-matched controls (MC). In comparison to the MC, individuals with PD showed significantly less SN-CEN coupling and greater DMN-CEN coupling during rest. Disease severity, an index of striatal dysfunction, was related to reduced functional coupling between the striatum and SN. These results demonstrate that individuals with PD have a dysfunctional pattern of interaction between core neurocognitive networks compared to what is found in healthy individuals, and that interaction between the SN and the striatum is even more profoundly disrupted in those with greater disease severity."
ALICE CRONIN-GOLOMB,Functional correlates of optic flow motion processing in Parkinson’s disease,"The visual input created by the relative motion between an individual and the environment, also called optic flow, influences the sense of self-motion, postural orientation, veering of gait, and visuospatial cognition. An optic flow network comprising visual motion areas V6, V3A, and MT+, as well as visuo-vestibular areas including posterior insula vestibular cortex (PIVC) and cingulate sulcus visual area (CSv), has been described as uniquely selective for parsing egomotion depth cues in humans. Individuals with Parkinson’s disease (PD) have known behavioral deficits in optic flow perception and visuospatial cognition compared to age- and education-matched control adults (MC). The present study used functional magnetic resonance imaging (fMRI) to investigate neural correlates related to impaired optic flow perception in PD. We conducted fMRI on 40 non-demented participants (23 PD and 17 MC) during passive viewing of simulated optic flow motion and random motion. We hypothesized that compared to the MC group, PD participants would show abnormal neural activity in regions comprising this optic flow network. MC participants showed robust activation across all regions in the optic flow network, consistent with studies in young adults, suggesting intact optic flow perception at the neural level in healthy aging. PD participants showed diminished activity compared to MC particularly within visual motion area MT+ and the visuo-vestibular region CSv. Further, activation in visuo-vestibular region CSv was associated with disease severity. These findings suggest that behavioral reports of impaired optic flow perception and visuospatial performance may be a result of impaired neural processing within visual motion and visuo-vestibular regions in PD."
ALICE CRONIN-GOLOMB,Visuospatial attention to single and multiple objects Is independently impaired in Parkinson's disease,"Parkinson’s disease (PD) is associated with deficits in visuospatial attention. It is as yet unknown whether these attentional deficits begin at a perceptual level or instead reflect disruptions in oculomotor or higher-order processes. In the present study, non-demented individuals with PD and matched normal control adults (NC) participated in two tasks requiring sustained visuospatial attention, both based on a multiple object tracking paradigm. Eye tracking was used to ensure central fixation. In Experiment 1 (26 PD, 21 NC), a pair of identical red dots (one target, one distractor) rotated randomly for three seconds at varied speeds. The task was to maintain the identity of the sole target, which was labeled prior to each trial. PD were less accurate than NC overall (p = .049). When considering only trials where fixation was maintained, however, there was no significant group difference, suggesting that the deficit’s origin is closely related to oculomotor processing. To determine whether PD had additional impairment in multifocal attention, in Experiment 2 (25 PD, 15 NC), two targets were presented along with distractors at a moderate speed, along with a control condition in which dots remained stationary. PD were less accurate than NC for moving (p = 0.02) but not stationary targets. This group difference remained significant when considering only trials where fixation was maintained, suggesting the source of the PD deficit was independent from oculomotor processing. Taken together, the results implicate separate mechanisms for single vs. multiple object tracking deficits in PD."
ALICE CRONIN-GOLOMB,"Bingo! Externally supported performance intervention for deficient visual search in normal aging, Parkinson's disease, and Alzheimer's disease","External support may improve task performance regardless of an individual’s ability to compensate for cognitive deficits through internally-generated mechanisms. We investigated if performance of a complex, familiar visual search task (the game of bingo) could be enhanced in groups with suboptimal vision by providing external support through manipulation of task stimuli. Participants were 19 younger adults, 14 individuals with probable Alzheimer’s disease (AD), 13 AD-matched healthy adults, 17 non-demented individuals with Parkinson’s disease (PD), and 20 PD-matched healthy adults. We varied stimulus contrast, size, and visual complexity during game play. The externally-supported performance interventions of increased stimulus size and decreased complexity resulted in improvements in performance by all groups. Performance improvement through increased stimulus size and decreased complexity was demonstrated by all groups. AD also obtained benefit from increasing contrast, presumably by compensating for their contrast sensitivity deficit. The general finding of improved performance across healthy and afflicted groups suggests the value of visual support as an easy-to-apply intervention to enhance cognitive performance."
ALICE CRONIN-GOLOMB,Circadian rest-activity rhythms predict cognitive function in early Parkinson's disease independently of sleep,"BACKGROUND: Cognitive impairment is a common and debilitating symptom of Parkinson's disease (PD), and its etiology is likely multifactorial. One candidate mechanism is circadian disruption. Although there is evidence of circadian abnormalities in PD, no studies have directly assessed their association with cognitive impairment. OBJECTIVES: Investigate whether circadian rest-activity rhythm is associated with cognitive function in PD independently of sleep. METHODS: Thirty-five participants with PD wore wrist actigraph monitors and completed sleep diaries for 7 to 10 days, then underwent neuropsychological testing. Rest-activity rhythm was characterized using nonparametric circadian rhythm analysis of actigraphy data. Objective sleep parameters were also estimated using actigraphy data. Hierarchical regression models assessed the independent contributions of sleep and rest-activity rhythm to cognitive performance. RESULTS: Less stable day-to-day rest-activity rhythm was associated with poorer executive, visuospatial, and psychomotor functioning, but not with memory. Hierarchical regressions showed that interdaily stability's contribution to cognitive performance was independent of sleep's contributions. Whereas sleep contributed to executive function, but not psychomotor or visuospatial performance, rest-activity rhythm stability significantly contributed to variance in all three of these domains, uniquely accounting for 14.4% to 17.6% of their performance variance. CONCLUSIONS: Our findings indicate that circadian rest-activity rhythm is associated with cognitive impairment independently of sleep. This suggests the possible utility of rest-activity rhythm as a biomarker for circadian function in PD. Future research should explore interventions to stabilize behavioral rhythms in order to strengthen circadian function, which, in turn, may reduce cognitive impairment in PD."
ALICE CRONIN-GOLOMB,Dual tasking in Parkinson's disease: cognitive consequences while walking,"OBJECTIVE: Cognitive deficits are common in Parkinson's disease (PD) and exacerbate the functional limitations imposed by PD's hallmark motor symptoms, including impairments in walking. Though much research has addressed the effect of dual cognitive-locomotor tasks on walking, less is known about their effect on cognition. The purpose of this study was to investigate the relation between gait and executive function, with the hypothesis that dual tasking would exacerbate cognitive vulnerabilities in PD as well as being associated with gait disturbances. METHOD: Nineteen individuals with mild-moderate PD without dementia and 13 age- and education-matched normal control adults (NC) participated. Executive function (set-shifting) and walking were assessed singly and during dual tasking. RESULTS: Dual tasking had a significant effect on cognition (reduced set-shifting) and on walking (speed, stride length) for both PD and NC, and also on stride frequency for PD only. The impact of dual tasking on walking speed and stride frequency was significantly greater for PD than NC. Though the group by condition interaction was not significant, PD had fewer set-shifts than NC on dual task. Further, relative to NC, PD showed significantly greater variability in cognitive performance under dual tasking, whereas variability in motor performance remained unaffected by dual tasking. CONCLUSIONS: Dual tasking had a significantly greater effect in PD than in NC on cognition as well as on walking. The results suggest that assessment and treatment of PD should consider the cognitive as well as the gait components of PD-related deficits under dual-task conditions. (PsycINFO Database Record)."
ALICE CRONIN-GOLOMB,Randomized controlled trial of a home-based action observation intervention to improve walking in Parkinson disease,"OBJECTIVE: To examine the feasibility and efficacy of a home-based gait observation intervention for improving walking in Parkinson disease (PD). DESIGN: Participants were randomly assigned to an intervention or control condition. A baseline walking assessment, a training period at home, and a posttraining assessment were conducted. SETTING: The laboratory and participants' home and community environments. PARTICIPANTS: Nondemented individuals with PD (N=23) experiencing walking difficulty. INTERVENTION: In the gait observation (intervention) condition, participants viewed videos of healthy and parkinsonian gait. In the landscape observation (control) condition, participants viewed videos of moving water. These tasks were completed daily for 8 days. MAIN OUTCOME MEASURES: Spatiotemporal walking variables were assessed using accelerometers in the laboratory (baseline and posttraining assessments) and continuously at home during the training period. Variables included daily activity, walking speed, stride length, stride frequency, leg swing time, and gait asymmetry. Questionnaires including the 39-item Parkinson Disease Questionnaire (PDQ-39) were administered to determine self-reported change in walking, as well as feasibility. RESULTS: At posttraining assessment, only the gait observation group reported significantly improved mobility (PDQ-39). No improvements were seen in accelerometer-derived walking data. Participants found the at-home training tasks and accelerometer feasible to use. CONCLUSIONS: Participants found procedures feasible and reported improved mobility, suggesting that observational training holds promise in the rehabilitation of walking in PD. Observational training alone, however, may not be sufficient to enhance walking in PD. A more challenging and adaptive task, and the use of explicit perceptual learning and practice of actions, may be required to effect change."
ALICE CRONIN-GOLOMB,"Vision-fair neuropsychological assessment in normal aging, Parkinson's disease and Alzheimer's disease","We examined performance of healthy older and younger adults and individuals with Alzheimer's disease (AD) and Parkinson's disease (PD) on digit cancellation, a task putatively sensitive to cognitive impairment, but possibly affected by visual impairment, particularly in contrast sensitivity. Critical contrast thresholds were established to create custom stimulus arrays that were proximally matched across individuals. Age- and PD-related differences in search were fully accounted for by the sensory deficit. Increased contrast benefited AD patients, but could not override cognitive impairment. We conclude that visually fair neuropsychological testing can effectively compensate for normal age- and PD-related visual changes that affect cognitive performance."
ALICE CRONIN-GOLOMB,Perception of communicative and non-communicative motion-defined gestures in Parkinson’s disease,"OBJECTIVES: Parkinson’s disease (PD) is associated with deficits in social cognition and visual perception, but little is known about how the disease affects perception of socially complex biological motion, specifically motion-defined communicative and non-communicative gestures. We predicted that individuals with PD would perform more poorly than normal control (NC) participants in discriminating between communicative and non-communicative gestures, and in describing communicative gestures. We related the results to the participants’ gender, as there are gender differences in social cognition in PD. METHODS: The study included 23 individuals with PD (10 men) and 24 NC participants (10 men) matched for age and education level. Participants viewed point-light human figures that conveyed communicative and non-communicative gestures and were asked to describe each gesture while discriminating between the two gesture types. RESULTS: PD as a group were less accurate than NC in describing non-communicative but not communicative gestures. Men with PD were impaired in describing and discriminating between communicative as well as non-communicative gestures. CONCLUSIONS: The present study demonstrated PD-related impairments in perceiving and inferring the meaning of biological motion gestures. Men with PD may have particular difficulty in understanding the communicative gestures of others in interpersonal exchanges."
ALICE CRONIN-GOLOMB,Dysregulation of visual motion inhibition in major depression,"Individuals with depression show depleted concentrations of the inhibitory neurotransmitter GABA in occipital (visual) cortex, predicting weakened inhibition within their visual systems. Yet, visual inhibition in depression remains largely unexplored. To fill this gap, we examined the inhibitory process of centersurround suppression (CSS) of visual motion in depressed individuals. Perceptual performance in discriminating the direction of motion was measured as a function of stimulus presentation time and contrast in depressed individuals (n¼27) and controls (n¼22). CSS was operationalized as the accuracy difference between conditions using large (7.5°) and small (1.5°) grating stimuli. Both depressed and control participants displayed the expected advantage in accuracy for small stimuli at high contrast. A significant interaction emerged between subject group, contrast level and presentation time, indicating that alterations of CSS in depression were modulated by stimulus conditions. At high contrast, depressed individuals showed significantly greater CSS than controls at the 66 ms presentation time (where the effect peaked in both groups). The results' specificity and dependence on stimulus features such as contrast, size and presentation time suggest that they arise from changes in early visual processing, and are not the results of a generalized deficit or cognitive bias."
ALICE CRONIN-GOLOMB,Effects of orthostatic hypotension on cognition in Parkinson disease,"OBJECTIVE: To investigate the relation between orthostatic hypotension (OH) and posturemediated cognitive impairment in Parkinson disease (PD) using a cross-sectional and withingroup design. METHODS: Individuals without dementia with idiopathic PD included 18 with OH (PDOH) and 19 without OH; 18 control participants were also included. Neuropsychological tests were conducted in supine and upright-tilted positions. Blood pressure was assessed in each posture. RESULTS: The PD groups performed similarly while supine, demonstrating executive dysfunction in sustained attention and response inhibition, and reduced semantic fluency and verbal memory (encoding and retention). Upright posture exacerbated and broadened these deficits in the PDOH group to include phonemic fluency, psychomotor speed, and auditory working memory. When group-specific supine scores were used as baseline anchors, both PD groups showed cognitive changes following tilt, with the PDOH group exhibiting a wider range of deficits in executive function and memory as well as significant changes in visuospatial function. CONCLUSIONS: Cognitive deficits in PD have been widely reported with assessments performed in the supine position, as seen in both our PD groups. Here we demonstrated that those with PDOH had transient, posture-mediated changes in excess of those found in PD without OH. These observed changes suggest an acute, reversible effect. Understanding the effects of OH due to autonomic failure on cognition is desirable, particularly as neuroimaging and clinical assessments collect data only in the supine or seated positions. Identification of a distinct neuropsychological profile in PD with OH has quality of life implications, and OH presents itself as a possible target for intervention in cognitive disturbance."
ALICE CRONIN-GOLOMB,American Geriatrics Society and National Institute on Aging Bench-to-Bedside conference: sensory impairment and cognitive decline in older adults,"This article summarizes the presentations and recommendations of the tenth annual American Geriatrics Society and National Institute on Aging Bench‐to‐Bedside research conference, “Sensory Impairment and Cognitive Decline,” on October 2–3, 2017, in Bethesda, Maryland. The risk of impairment in hearing, vision, and other senses increases with age, and almost 15% of individuals aged 70 and older have dementia. As the number of older adults increases, sensory and cognitive impairments will affect a growing proportion of the population. To limit its scope, this conference focused on sensory impairments affecting vision and hearing. Comorbid vision, hearing, and cognitive impairments in older adults are more common than would be expected by chance alone, suggesting that some common mechanisms might affect these neurological systems. This workshop explored the mechanisms and consequences of comorbid vision, hearing, and cognitive impairment in older adults; effects of sensory loss on the aging brain; and bench‐to‐bedside innovations and research opportunities. Presenters and participants identified many research gaps and questions; the top priorities fell into 3 themes: mechanisms, measurement, and interventions. The workshop delineated specific research questions that provide opportunities to improve outcomes in this growing population."
ALICE CRONIN-GOLOMB,Author response: Effects of orthostatic hypotension on cognition in Parkinson disease,"OBJECTIVE: To investigate the relation between orthostatic hypotension (OH) and posture-mediated cognitive impairment in persons with Parkinson's disease (PD) without dementia. METHODS: There were 55 participants: 37 non-demented individuals with idiopathic PD, including 18 with OH (PDOH), and 19 without (PDWOH), and18 control participants (C). All participants completed neuropsychological tests in the supine and in the upright tilted position. Blood pressure was assessed in each posture using a standardized oscillometric cuff at the right brachial artery. RESULTS: The two PD groups performed similarly while supine, with a profile notable for executive dysfunction consisting of deficits in sustained attention, response inhibition, and semantic verbal fluency, as well as reduced verbal memory encoding and retention. When upright, these deficits were exacerbated and broadened to include additional cognitive functions in the PDOH group: deficits in phonemic verbal fluency, psychomotor speed, and both basic and complex aspects of auditory working memory. When group-specific supine scores were used as baseline anchors, both PD groups showed cognitive changes following tilt, though the PDOH group had a wider range of deficits in the executive functioning and memory domains and was the only group to show significant changes in visuospatial skills. CONCLUSIONS: Cognitive deficits in idiopathic PD have been widely reported, though assessments are typically performed in the supine position. While both PD groups had supine deficits that aligned with prior studies and clinical findings, we demonstrated that those with PD and orthostatic hypotension had transient, posture-mediated changes in excess of those found in PD without autonomic failure. These observed changes suggest an acute, reversible effect, and as orthostatic hypotension is a significant comorbid factor in PD, an independent target for clinical intervention. Further understanding of the effects of autonomic failure on cognition in other disorders is desirable, particularly in the context of neuroimaging studies and clinical assessments where data are collected only in the supine or seated positions. Identification of a distinct neuropsychological profile in PD with autonomic failure also has implications for functional activities of daily living and overall quality of life."
ALICE CRONIN-GOLOMB,Increasing contrast improves object perception in Parkinson's disease with visual hallucinations,"BACKGROUND: Deficits in basic vision are associated with visual hallucinations in Parkinson's disease. Of particular interest is contrast sensitivity loss in this disorder and its effect on object identification. OBJECTIVES: Evaluate whether increased contrast improves object perception in persons with Parkinson's disease and visual hallucinations, without dementia. METHODS: We assessed 26 individuals with mild to moderate idiopathic Parkinson's disease, half of whom reported one or more episodes of hallucinations/unusual perceptual experiences in the past month, with a letter-identification task that determined the contrast level required to achieve 80% accuracy. Contrast sensitivity was further assessed with a chart that presented stimuli at multiple spatial frequencies. The groups were closely matched for demographic and clinical characteristics except for experience of hallucinations. RESULTS: Relative to participants without visual hallucinations, those with hallucinations had poorer spatial frequency contrast sensitivity and required significantly greater contrast to correctly identify the letters on the identification task. Specifically, participants with hallucinations required a mean contrast of 52.8%, whereas participants without hallucinations required 35.0%. When given sufficient contrast, the groups with and without hallucinations were equally accurate in letter identification. CONCLUSIONS: Compared to those without hallucinations, individuals with Parkinson's disease and hallucinations without dementia showed poorer contrast sensitivity. Once contrast was individually enhanced, the groups were equally accurate at object identification. These findings suggest the potential of visual perception tests to predict, and perception-based interventions to reduce, hallucinations in Parkinson's disease."
ALICE CRONIN-GOLOMB,Effect of visual cues on the resolution of perceptual ambiguity in Parkinson’s disease and normal aging,"Parkinson's disease (PD) and normal aging have been associated with changes in visual perception, including reliance on external cues to guide behavior. This raises the question of the extent to which these groups use visual cues when disambiguating information. Twenty-seven individuals with PD, 23 normal control adults (NC), and 20 younger adults (YA) were presented a Necker cube in which one face was highlighted by thickening the lines defining the face. The hypothesis was that the visual cues would help PD and NC to exert better control over bistable perception. There were three conditions, including passive viewing and two volitional-control conditions (hold one percept in front; and switch: speed up the alternation between the two). In the Hold condition, the cue was either consistent or inconsistent with task instructions. Mean dominance durations (time spent on each percept) under passive viewing were comparable in PD and NC, and shorter in YA. PD and YA increased dominance durations in the Hold cue-consistent condition relative to NC, meaning that appropriate cues helped PD but not NC hold one perceptual interpretation. By contrast, in the Switch condition, NC and YA decreased dominance durations relative to PD, meaning that the use of cues helped NC but not PD in expediting the switch between percepts. Provision of low-level cues has effects on volitional control in PD that are different from in normal aging, and only under task-specific conditions does the use of such cues facilitate the resolution of perceptual ambiguity."
ALICE CRONIN-GOLOMB,The effect of Parkinson’s disease subgroups on verbal and nonverbal fluency,"BACKGROUND: Parkinson’s disease (PD) leads to deficits in executive function, including verbal and nonverbal fluency, as a result of compromised frontostriatal circuits. It is unknown whether deficits in verbal and nonverbal fluency in PD are driven by certain subgroups of patients, or how strategy use may facilitate performance. PARTICIPANTS: Sixty-five nondemented individuals with PD, including 36 with right-body onset (RPD; 20 with tremor as their initial symptom, 16 nontremor) and 29 with left-body onset (LPD; 14 with tremor as their initial symptom, 15 nontremor), and 52 normal control participants (NC) took part in the study. MEASUREMENTS: Verbal fluency was assessed using the FAS and Animals tests. Nonverbal fluency was assessed using the Ruff Figural Fluency Test. RESULTS: Both RPD and LPD were impaired in generating words and in using clustering and switching strategies on phonemic verbal fluency, whereas different patterns of impairment were found on nonverbal fluency depending on the interaction of side of onset and initial motor symptom (tremor vs. nontremor). Strategy use correlated with number of correct responses on verbal fluency in LPD, RPD, and NC. By contrast, on nonverbal fluency, strategy use correlated with correct responses for RPD and LPD, but not for NC. CONCLUSION: Our findings demonstrate the importance of considering subgroups in PD and analyzing subcomponents of verbal and nonverbal fluency (correct responses, errors, and strategies), which may depend differently on the integrity of dorsolateral prefrontal cortex, inferior frontal cortex, and anterior cingulate cortex."
ALICE CRONIN-GOLOMB,Normal discrimination of spatial frequency and contrast across visual hemifields in left-onset Parkinson’s disease: evidence against perceptual hemifield biases,"Individuals with Parkinson's disease (PD) with symptom onset on the left side of the body (LPD) show a mild type of left-sided visuospatial neglect, whereas those with right-onset (RPD) generally do not. The functional mechanisms underlying these observations are unknown. Two hypotheses are that the representation of left-space in LPD is either compressed or reduced in salience. We tested these hypotheses psychophysically. Participants were 31 non-demented adults with PD (15 LPD, 16 RPD) and 17 normal control adults (NC). The spatial compression hypothesis was tested by showing two sinusoidal gratings, side by side. One grating's spatial frequency (SF) was varied across trials, following a staircase procedure, whereas the comparison grating was held at a constant SF. While fixating on a central target, participants estimated the point at which they perceived the two gratings to be equal in SF. The reduced salience hypothesis was tested in a similar way, but by manipulating the contrast of the test grating rather than its SF. There were no significant differences between groups in the degree of bias across hemifields for SF discrimination or for contrast discrimination. Results did not support either the spatial compression hypothesis or the reduced salience hypothesis. Instead, they suggest that at this perceptual level, LPD do not have a systematically biased way of representing space in the left hemifield that differs from healthy individuals, nor do they perceive stimuli on the left as less salient than stimuli on the right. Neglect-like syndrome in LPD instead presumably arises from dysfunction of higher-order attention."
ALICE CRONIN-GOLOMB,Luminance affects age-related deficits in object detection: Implications for computerized psychological assessments,"As psychological instruments are converted for administration on computers, differences in luminance and contrast of these displays may affect performance. Specifically, high-luminance assessments may mask age-group differences that are apparent under lower luminance conditions. We examined the effects of luminance and contrast on object detection using computerized and naturalistic assessments. Younger and older adults displayed more differences in performance across differing contrast levels in conditions that were matched for luminance, despite the conditions appearing perceptually different. These findings indicate that computerized assessments should be created with luminance levels that are similar to those of the tasks they purport to simulate in order to enhance their validity."
ALICE CRONIN-GOLOMB,Relation of Parkinson's disease subtypes to visual activities of daily living,"Visual perceptual problems are common in Parkinson's disease (PD) and often affect activities of daily living (ADLs). PD patients with non-tremor symptoms at disease onset (i.e., rigidity, bradykinesia, gait disturbance or postural instability) have more diffuse neurobiological abnormalities and report worse non-motor symptoms and functional changes than patients whose initial symptom is tremor, but the relation of motor symptom subtype to perceptual deficits remains unstudied. We assessed visual ADLs with the Visual Activities Questionnaire in 25 non-demented patients with PD, 13 with tremor as the initial symptom and 12 with an initial symptom other than tremor, as well as in 23 healthy control participants (NC). As expected, the non-tremor patients, but not the tremor patients, reported more impairment in visual ADLs than the NC group, including in light/dark adaptation, acuity/spatial vision, depth perception, peripheral vision and visual processing speed. Non-tremor patients were significantly worse than tremor patients overall and on light/dark adaptation and depth perception. Environmental enhancements especially targeted to patients with the non-tremor PD subtype may help to ameliorate their functional disability."
ALICE CRONIN-GOLOMB,"Smartphone-based neuropsychological assessment in Parkinson's disease: feasibility, validity, and contextually driven variability in cognition","OBJECTIVES: The prevalence of neurodegenerative disorders demands methods of accessible assessment that reliably captures cognition in daily life contexts. We investigated the feasibility of smartphone cognitive assessment in people with Parkinson's disease (PD), who may have cognitive impairment in addition to motor-related problems that limit attending in-person clinics. We examined how daily-life factors predicted smartphone cognitive performance and examined the convergent validity of smartphone assessment with traditional neuropsychological tests. METHODS: Twenty-seven nondemented individuals with mild-moderate PD attended one in-lab session and responded to smartphone notifications over 10 days. The smartphone app queried participants 5x/day about their location, mood, alertness, exercise, and medication state and administered mobile games of working memory and executive function. RESULTS: Response rate to prompts was high, demonstrating feasibility of the approach. Between-subject reliability was high on both cognitive games. Within-subject variability was higher for working memory than executive function. Strong convergent validity was seen between traditional tests and smartphone working memory but not executive function, reflecting the latter's ceiling effects. Participants performed better on mobile working memory tasks when at home and after recent exercise. Less self-reported daytime sleepiness and lower PD symptom burden predicted a stronger association between later time of day and higher smartphone test performance. CONCLUSIONS: These findings support feasibility and validity of repeat smartphone assessments of cognition and provide preliminary evidence of the effects of context on cognitive variability in PD. Further development of this accessible assessment method could increase sensitivity and specificity regarding daily cognitive dysfunction for PD and other clinical populations."
ALICE CRONIN-GOLOMB,Towards neuroscience of the everyday world (NEW) using functional near infrared spectroscopy,"Functional near-infrared spectroscopy (fNIRS) assesses human brain activity by noninvasively measuring changes of cerebral hemoglobin concentrations caused by modulation of neuronal activity. Recent progress in signal processing and advances in system design, such as miniaturization, wearability, and system sensitivity, have strengthened fNIRS as a viable and cost-effective complement to functional magnetic resonance imaging, expanding the repertoire of experimental studies that can be performed by the neuroscience community. The availability of fNIRS and electroencephalography for routine, increasingly unconstrained, and mobile brain imaging is leading toward a new domain that we term “Neuroscience of the Everyday World” (NEW). In this light, we review recent advances in hardware, study design, and signal processing, and discuss challenges and future directions."
ALICE CRONIN-GOLOMB,"Changes in apathy, depression, and anxiety in Parkinson's disease from before to during the COVID-19 era","Apathy, depression, and anxiety are common non-motor symptoms of Parkinson's disease (PD). Tracking the changes in such symptoms over time would be valuable not only to determine their natural course during the disease, but also to establish the effects of unusual historical events interacting with the natural course. Having collected data on apathy (Apathy Scale), depression (Beck Depression Inventory-II), and anxiety (Parkinson's Anxiety Scale) in a large sample of persons with PD (PwPD) before the beginning of the COVID-19 era, we followed up with these individuals to investigate the changes in their prevalence of apathy, depression, and anxiety across two timepoints (T1 and T2). Of the original 347 participants, 111 responded and provided complete data at T2. The data collection at T1, before COVID-19, occurred between 2017-2018. The data collection at T2 occurred in 2021 and included the same measures, with the addition of the Coronavirus Impact Scale to assess the effects of the pandemic on the individual participants. Over this period, there was a significant increase in apathy, but not in depression or anxiety. Anxiety and depression, but not apathy, were correlated with the impact of COVID-19."
ALICE CRONIN-GOLOMB,Impact of anxiety on quality of life in Parkinson's disease,"In Parkinson's disease (PD), both the patient and the health care provider look for ways to preserve the patient's quality of life. Many studies focus on the impact of depression and motor disability on poor life quality but neglect to examine the role of anxiety. We investigated the impact of anxiety and depression on health-related quality of life in PD, using the Parkinson's Disease Quality of Life measure (PDQ-39). Symptoms of anxiety, more than depression, cognitive status, or motor stage, significantly affected quality of life in 38 nondemented patients with mild-to-moderate motor disability. Stepwise regression analyses revealed that anxiety explained 29% of the variance in the PDQ-39 sum score, and depression explained 10% of the variance beyond that accounted for by anxiety. The findings suggest that primary management of anxiety as well as depression may be important to optimizing the quality of life of PD patients."
ALICE CRONIN-GOLOMB,Cognitive-behavioral therapy for anxiety in Parkinson's disease,"Parkinson's disease (PD) is characterized by motor symptoms, but nonmotor symptoms also significantly impair daily functioning and reduce quality of life. Anxiety is prevalent and debilitating in PD, but remains understudied and undertreated. Much affective research in PD focuses on depression rather than anxiety, and as such, there are no evidence-based treatments for anxiety in this population. Cognitive-behavioral therapy (CBT) has shown promise for treating depression in PD and may be efficacious for anxiety. This exploratory study implemented a multiple-baseline single-case experimental design to evaluate the utility and feasibility of CBT for individuals with PD who also met criteria for a DSM-5 anxiety disorder ( n = 9). Participants were randomized to a 2-, 4-, or 6-week baseline phase, followed by 12 CBT sessions, and two post treatment assessments (immediately post treatment and 6-week follow-up). Multiple outcome measures of anxiety and depression were administered weekly during baseline and intervention. Weekly CBT sessions were conducted in-person ( n = 5) or via secure videoconferencing ( n = 4). At post treatment, seven of the nine participants showed significant reductions in anxiety and/or depression, with changes functionally related to treatment and most improvements maintained at 6-week follow-up. Effects of CBT on secondary outcomes varied across participants, with preliminary evidence for reduction in fear of falling. Adherence and retention were high, as were treatment satisfaction and acceptability. The findings of this pilot study provide preliminary evidence for the utility of CBT as a feasible treatment for anxiety and comorbid depressive symptoms in PD and highlight the potential of telehealth interventions for mood in this population."
ALICE CRONIN-GOLOMB,Involuntary saccades and binocular coordination during visual pursuit in Parkinson's disease,"Prior studies of oculomotor function in Parkinson's disease (PD) have either focused on saccades while smooth pursuit eye movements were not involved, or tested smooth pursuit without considering the effect of any involuntary saccades. The present study investigated whether these involuntary saccades could serve as a useful biomarker for PD. Ten observers with PD participated in the study along with 10 age-matched normal control (NC) and 10 young control participants (YC). Observers fixated on a central cross while a disk (target) moved toward it from either side of the screen. Once the target reached the fixation cross, observers began to pursue the moving target until the target reached to the other side. To vary the difficulty of fixation and pursuit, the moving target was presented on a blank or a moving background. The moving background consisted of uniformly distributed dots moved in either the same or the opposite direction of the target once the target reached the central fixation cross. To investigate binocular coordination, each background condition was presented under a binocular condition, in which both eyes saw the same stimulus, and under a dichoptic condition, in which one eye saw only the target and the other eye only saw the background. The results showed that in both background conditions, observers with PD made more involuntary saccades than NC and YC during both fixation and pursuit periods while YC and NC showed no difference. Moreover, the difference between left and right eye positions increased over time during the pursuit period for PD group but not for the other two groups. This suggests that individuals with PD may be impaired not only in saccade inhibition, but also in binocular coordination during pursuit. [Meeting abstract presented at VSS 2016.]"
ALICE CRONIN-GOLOMB,The potential of cognitive-behavioral intervention for anxiety in Parkinson's disease,"OBJECTIVES: Anxiety is a prevalent but understudied non-motor symptom of Parkinson’s disease (PD), for which pharmacological treatments yield mixed results. Cognitive-behavioral therapy (CBT) has shown promise in improving depression in PD, and case studies suggest that it may also alleviate anxiety. Because of the deleterious effects of anxiety on cognition and quality of life, there is need for evidence-based psychological interventions. METHODS: This study explores the utility and feasibility of a transdiagnostic intervention to improve anxiety in PD using a multiple-baseline, single-case experimental design. Following a two-, four-, or six-week baseline phase, five individuals with PD who met DSM-5 criteria for an anxiety disorder received/are receiving 12 weekly sessions of CBT1, followed by a post-intervention assessment and a 6-week no-contact follow-up. Weekly levels of anxiety and depression are measured throughout the study, with measures of cognition, quality of life, sleep, and motor function completed at all pre- and post-intervention assessments. RESULTS: Results to date (2 completers, 3 currently enrolled) suggest that CBT may produce clinically meaningful changes in anxiety, depression, quality of life, and fear of falling (Figure 01). Results are mixed regarding effects on cognition, sleep, and motor function. Treatment satisfaction has been high. CONCLUSIONS: CBT may improve anxiety, depression, quality of life, and fear of falling among adults with PD and clinical anxiety. These findings warrant further exploration of CBT for anxiety across various stages of PD."
ALICE CRONIN-GOLOMB,Great nature’s second course: Introduction to the special issue on the behavioral neuroscience of sleep,"Sleep is necessary for normal psychological functioning, and psychological function in turn affects sleep integrity. Recent investigations delineate the relation of sleep to a broad array of processes ranging from learning and memory to emotional reactivity and mood, and use a variety of methodological approaches (imaging, electrophysiological, behavioral) to reveal the complex relations between sleep and the functioning of the awake brain. The articles in this issue advance our fundamental knowledge of the relation of sleep to psychological function. In addition, several of the articles discuss how sleep is affected by or affects human clinical conditions, including insomnia, epilepsy, mild cognitive impairment, bipolar disorder, and cancer. Together, the articles of this special issue highlight recent progress in understanding the behavioral neuroscience of sleep and identify promising areas for future research, including the possibility of sleep-based interventions to improve psychological health."
ALICE CRONIN-GOLOMB,Web-based assessment of visual and visuospatial symptoms in Parkinson’s disease,"Visual and visuospatial dysfunction is prevalent in Parkinson’s disease (PD). To promote assessment of these often overlooked symptoms, we adapted the PD Vision Questionnaire for Internet administration. The questionnaire evaluates visual and visuospatial symptoms, impairments in activities of daily living (ADLs), and motor symptoms. PD participants of mild to moderate motor severity (𝑛 = 24) and healthy control participants (HC, 𝑛 = 23) completed the questionnaire in paper and web-based formats. Reliability was assessed by comparing responses across formats. Construct validity was evaluated by reference to performance on measures of vision, visuospatial cognition, ADLs, and motor symptoms. The web-based format showed excellent reliability with respect to the paper format for both groups (all 𝘗'𝑠 < 0,001; HC completing the visual and visuospatial section only). Demonstrating the construct validity of the web-based questionnaire, self-rated ADL and visual and visuospatial functioning were significantly associated with performance on objective measures of these abilities (all 𝘗'𝑠 < 0.01). The findings indicate that web-based administration may be a reliable and valid method of assessing visual and visuospatial and ADL functioning in PD."
ALICE CRONIN-GOLOMB,Temporal associations between sleep and daytime functioning in Parkinson’s disease: A smartphone-based ecological momentary assessment,
ALICE CRONIN-GOLOMB,Telehealth cognitive behavioral therapy for depression in Parkinson’s disease: a case study,"Parkinson's disease (PD) is characterized as a motor disorder, but the majority of individuals with PD also suffer from nonmotor symptoms, including mental health difficulties, such as depression, anxiety, and apathy, as well as decreased cognitive function, daily function, sleep quality, and quality of life. Cognitive behavioral therapy (CBT) is an effective treatment for depression in PD, but motor disability, work schedule, transportation issues, and care partner burden may cause difficulty in attending weekly face-to-face therapy sessions. A promising avenue in the delivery of CBT is telehealth. CBT administered live via videoconference technology may circumvent many of the barriers that prevent those with PD from receiving treatment. This case study evaluates the preliminary efficacy, feasibility, and acceptability of 12-week telehealth CBT for depression in PD. CBT administered via telehealth was feasible, acceptable, and efficacious for a study participant with PD and major depressive disorder. In addition to effectively treating depression, the telehealth intervention improved quality of life and aspects of cognitive functioning, as well as symptoms of anxiety, apathy, and subjective cognitive impairment, all of which are prevalent nonmotor symptoms of PD. (PsycInfo Database Record (c) 2022 APA, all rights reserved)."
ALICE CRONIN-GOLOMB,Biological and cognitive markers of Presenilin1 E280A autosomal dominant Alzheimer's disease: A comprehensive review of the Colombian kindred,"The study of individuals with autosomal dominant Alzheimer's disease affords one of the best opportunities to characterize the biological and cognitive changes of Alzheimer's disease that occur over the course of the preclinical and symptomatic stages. Unifying the knowledge gained from the past three decades of research in the world's largest single-mutation autosomal dominant Alzheimer's disease kindred - a family in Antioquia, Colombia with the E280A mutation in the Presenilin1 gene - will provide new directions for Alzheimer's research and a framework for generalizing the findings from this cohort to the more common sporadic form of Alzheimer's disease. As this specific mutation is virtually 100% penetrant for the development of the disease by midlife, we use a previously defined median age of onset for mild cognitive impairment for this cohort to examine the trajectory of the biological and cognitive markers of the disease as a function of the carriers' estimated years to clinical onset. Studies from this cohort suggest that structural and functional brain abnormalities - such as cortical thinning and hyperactivation in memory networks - as well as differences in biofluid and in vivo measurements of Alzheimer's-related pathological proteins distinguish Presenilin1 E280A mutation carriers from non-carriers as early as childhood, or approximately three decades before the median age of onset of clinical symptoms. We conclude our review with discussion on future directions for Alzheimer's disease research, with specific emphasis on ways to design studies that compare the generalizability of research in autosomal dominant Alzheimer's disease to the larger sporadic Alzheimer's disease population."
ALICE CRONIN-GOLOMB,Emergence of nonmotor symptoms as the focus of research and treatment of Parkinson's disease: Introduction to the special section on nonmotor dysfunctions in Parkinson's disease,"Parkinson's disease (PD) is traditionally characterized by the cardinal motor symptoms of tremor, rigidity, slowness of movement, and impairments of posture, gait, and balance. A relatively new focus of research and treatment is the nonmotor symptoms of the disease, following from recent understanding of the neuropathological stages. Disruptions of arousal, mood, sleep, and autonomic function before the first motor signs of PD implicate the lower brainstem, which is affected before the substantia nigra and dopaminergic system. In later stages of the disease, the pathology extends to the cortex, accompanied by impairments in cognition and perception. The articles in this special section advance our knowledge of the brain bases of the nonmotor symptoms of PD, including disrupted visual perception, impaired cognition across a range of domains, and psychiatric and artistic manifestations. Subtypes under investigation include those described by side of disease onset (left or right body side), predominant cognitive profile, and gender. Taken together, the articles in this special section reflect the field's growing focus on the nonmotor symptoms of PD, their brain bases, and the corresponding potential for their treatment."
ALICE CRONIN-GOLOMB,Frontal and posterior subtypes of neuropsychological deficit in Parkinson's disease,"Mild cognitive impairment in Parkinson's disease (PD) is heterogeneous in regard to affected domains. Although patterns of cognitive performance that may predict later dementia are as yet undetermined, posterior-versus frontal-type assessments show promise for differential predictive value. The present study included 70 individuals: 42 with idiopathic PD without dementia and 28 age- and education-matched healthy control adults (HC). Participants completed assessments of cognition with emphasis on tests that are sensitive to frontal and posterior deficits. PD patients were classified into cognitive subgroups and the subgroups were compared on demographic and disease variables. Individual performance across neuropsychological tests was evaluated for the PD group. Patients with PD performed more poorly than HC on several measures of cognition, and they were classified into frontal (12), posterior (3), both (10) and neither subgroups (17), the latter two in reference to frontal- and posterior-type deficits. The neither subgroup was distinguished by less motor impairment than the both subgroup, but the four subgroups did not otherwise differ on demographic or disease variables. Across patients, the tests most sensitive to cognitive impairment included measures of attention and executive functioning (frontal-type tests). Examination of individual test performance for PD revealed substantial heterogeneity across tests with respect to number and severity of deficits. The current study provides insight into which commonly used neuropsychological tests are most sensitive to cognitive deficits (strictly defined) in a nondemented, well characterized PD sample, and into the relation of cognitive subgroups to demographic and disease-specific variables."
ALICE CRONIN-GOLOMB,"Line bisection in Parkinson's disease: investigation of contributions of visual field, retinal vision, and scanning patterns to visuospatial function","Parkinson's disease (PD) is characterized by disorders of visuospatial function that can impact everyday functioning. Visuospatial difficulties are more prominent in those whose motor symptoms begin on the left body side (LPD) than the right body side (RPD) and have mainly been attributed to parietal dysfunction. The source of visuospatial dysfunction is unclear, as in addition to subcortical–cortical changes, there are irregularities of visual scanning and potentially of retinal-level vision in PD. To assess these potential contributors, performance on a visuospatial task—line bisection—was examined together with retinal structure (nerve fiber layer thickness, measured by optical coherence tomography [OCT]), retinal function (contrast sensitivity, measured by frequency-doubling technology [FDT]), and visual scanning patterns. Participants included 20 nondemented patients (10 LPD, 10 RPD) and 11 normal control (NC) adults. Relative to the other groups, LPD were expected to show rightward bias on horizontal line bisection, especially within the left visual hemispace, and downward bias on vertical bisection. LPD relative rightward bias was confirmed, though not mainly within the left hemispace and not correlated with retinal structure or function. Retinal thinning was seen in LPD relative to RPD. Qualitative visualization of eye movements suggested greater LPD exploration of the right than left side of the line during horizontal bisection, and some overall compression of scanning range in RPD (both orientations) and LPD (primarily vertical). Results indicated that rightward visuospatial bias in our LPD sample arose not from abnormalities at the retinal level but potentially from attentional biases, reflected in eye movement patterns. (PsycINFO Database Record (c) 2016 APA, all rights reserved)"
ALICE CRONIN-GOLOMB,"Perceptual, cognitive, and personality rigidity in Parkinson's disease","Parkinson’s disease (PD) is associated with motor and non-motor rigidity symptoms (e.g., cognitive and personality). The question is raised as to whether rigidity in PD also extends to perception, and if so, whether perceptual, cognitive, and personality rigidities are correlated. Bistable stimuli were presented to 28 non-demented individuals with PD and 26 normal control adults (NC). Necker cube perception and binocular rivalry were examined during passive viewing, and the Necker cube was additionally used for two volitional-control conditions: Hold one percept in front, and Switch between the two percepts. Relative to passive viewing, PD were significantly less able than NC to reduce dominance durations in the Switch condition, indicating perceptual rigidity. Tests of cognitive flexibility and a personality questionnaire were administered to explore the association with perceptual rigidity. Cognitive flexibility was not correlated with perceptual rigidity for either group. Personality (novelty seeking) correlated with dominance durations on Necker passive viewing for PD but not NC. The results indicate the presence in mild-moderate PD of perceptual rigidity and suggest shared neural substrates with novelty seeking, but functional divergence from those supporting cognitive flexibility. The possibility is raised that perceptual rigidity may be a harbinger of cognitive inflexibility later in the disease course."
ALICE CRONIN-GOLOMB,Salience and default mode network coupling predicts cognition in aging and Parkinson’s disease,"OBJECTIVES: Cognitive impairment is common in Parkinson’s disease (PD). Three neurocognitive networks support efficient cognition: the salience network, the default mode network, and the central executive network. The salience network is thought to switch between activating and deactivating the default mode and central executive networks. Anti-correlated interactions between the salience and default mode networks in particular are necessary for efficient cognition. Our previous work demonstrated altered functional coupling between the neurocognitive networks in non-demented individuals with PD compared to age-matched control participants. Here, we aim to identify associations between cognition and functional coupling between these neurocognitive networks in the same group of participants. METHODS: We investigated the extent to which intrinsic functional coupling among these neurocognitive networks is related to cognitive performance across three neuropsychological domains: executive functioning, psychomotor speed, and verbal memory. Twenty-four non-demented individuals with mild to moderate PD and 20 control participants were scanned at rest and evaluated on three neuropsychological domains. RESULTS: PD participants were impaired on tests from all three domains compared to control participants. Our imaging results demonstrated that successful cognition across healthy aging and Parkinson’s disease participants was related to anti-correlated coupling between the salience and default mode networks. Individuals with poorer performance scores across groups demonstrated more positive salience network/default-mode network coupling. CONCLUSIONS: Successful cognition relies on healthy coupling between the salience and default mode networks, which may become dysfunctional in PD. These results can help inform non-pharmacological interventions (repetitive transcranial magnetic stimulation) targeting these specific networks before they become vulnerable in early stages of Parkinson’s disease."
ALICE CRONIN-GOLOMB,Veering in hemi-Parkinson’s disease: primacy of visual over motor contributions,"Veering while walking is often reported in individuals with Parkinson’s disease (PD), with potential mechanisms being vision-based (asymmetrical perception of the visual environment) or motoric (asymmetry in stride length between relatively affected and non-affected body side). We examined these competing hypotheses by assessing veering in 13 normal control participants (NC) and 20 non-demented individuals with PD: 9 with left-side onset of motor symptoms (LPD) and 11 with right-side onset (RPD). Participants walked in a corridor under three conditions: eyes-open, egocentric reference point (ECRP; walk toward a subjectively perceived center of a target at the end of the corridor), and vision-occluded. The visual hypothesis predicted that LPD participants would veer rightward, in line with their rightward visual-field bias, whereas those with RPD would veer leftward. The motor hypothesis predicted the opposite pattern of results, with veering toward the side with shorter stride length. Results supported the visual hypothesis. Under visual guidance, RPD participants significantly differed from NC, veering leftward despite a shorter right- than left-stride length, whereas LPD veered rightward (not significantly different from NC), despite shorter left- than right-stride length. LPD participants showed significantly reduced rightward veering and stride asymmetry when they walked in the presence of a visual landmark (ECRP) than in the eyes-open condition without a target. There were no group differences in veering in the vision-occluded condition. The findings suggest that interventions to correct walking abnormalities such as veering in PD should incorporate vision-based strategies rather than solely addressing motor asymmetries, and should be tailored to the distinctive navigational profiles of LPD and RPD."
ALICE CRONIN-GOLOMB,Impaired perception of biological motion in Parkinson’s disease,"OBJECTIVE: We examined biological motion perception in Parkinson’s disease (PD). Biological motion perception is related to one’s own motor function and depends on the integrity of brain areas affected in PD, including posterior superior temporal sulcus. If deficits in biological motion perception exist, they may be specific to perceiving natural/fast walking patterns that individuals with PD can no longer perform, and may correlate with disease-related motor dysfunction. METHOD: Twenty-six nondemented individuals with PD and 24 control participants viewed videos of point-light walkers and scrambled versions that served as foils, and indicated whether each video depicted a human walking. Point-light walkers varied by gait type (natural, parkinsonian) and speed (0.5, 1.0, 1.5 m/s). Participants also completed control tasks (object motion, coherent motion perception), a contrast sensitivity assessment, and a walking assessment. RESULTS: The PD group demonstrated significantly less sensitivity to biological motion than the control group (p < .001, Cohen’s d = 1.22), regardless of stimulus gait type or speed, with a less substantial deficit in object motion perception (p = .02, Cohen’s d = .68). There was no group difference in coherent motion perception. Although individuals with PD had slower walking speed and shorter stride length than control participants, gait parameters did not correlate with biological motion perception. Contrast sensitivity and coherent motion perception also did not correlate with biological motion perception. CONCLUSION: PD leads to a deficit in perceiving biological motion, which is independent of gait dysfunction and low-level vision changes, and may therefore arise from difficulty perceptually integrating form and motion cues in posterior superior temporal sulcus."
ALICE CRONIN-GOLOMB,Effects of Parkinson’s disease on optic flow perception for heading direction during navigation,"Visuoperceptual disorders have been identified in individuals with Parkinson’s disease (PD) and may affect the perception of optic flow for heading direction during navigation. Studies in healthy subjects have confirmed that heading direction can be determined by equalizing the optic flow speed (OS) between visual fields. The present study investigated the effects of PD on the use of optic flow for heading direction, walking parameters, and interlimb coordination during navigation, examining the contributions of OS and spatial frequency (dot density). Twelve individuals with PD without dementia, 18 age-matched normal control adults (NC), and 23 young control adults (YC) walked through a virtual hallway at about 0.8 m/s. The hallway was created by random dots on side walls. Three levels of OS (0.8, 1.2, and 1.8 m/s) and dot density (1, 2, and 3 dots/m2) were presented on one wall while on the other wall, OS and dot density were fixed at 0.8 m/s and 3 dots/m2, respectively. Three-dimensional kinematic data were collected, and lateral drift, walking speed, stride frequency and length, and frequency, and phase relations between arms and legs were calculated. A significant linear effect was observed on lateral drift to the wall with lower OS for YC and NC, but not for PD. Compared to YC and NC, PD veered more to the left under OS and dot density conditions. The results suggest that healthy adults perceive optic flow for heading direction. Heading direction in PD may be more affected by the asymmetry of dopamine levels between the hemispheres and by motor lateralization as indexed by handedness."
ALICE CRONIN-GOLOMB,"Patterns of Prefrontal Dysfunction in Alcoholics with and without Korsakoff's Syndrome, Patients with Parkinson's Disease, and Patients with Rupture and Repair of the Anterior Communicating Artery","This study compared patterns of frontal-lobe dysfunction in alcoholics with Korsakoff's syndrome (KS: n = 9), non-Korsakoff alcoholics (AL: n = 28), patients with Parkinson's disease (PD: n = 18), and patients with rupture and repair of the anterior communicating artery (ACoA: n = 4) relative to healthy non-neurological control (NC) participants (n = 70). The tests administered were sensitive to functions of dorsolateral prefrontal and orbito-frontal subsystems. Measures included perseverative errors on the Wisconsin Card Sorting Test (WCST-pe), errors on object alternation (OA), errors on Trails B, number of words generated on the Controlled Oral Word Association Test (COWAT), and number of categories completed on the WCST (WCST-cc). KS patients were as impaired as AL participants on orbitofrontal measures and, on dorsolateral prefrontal measures, were impaired relative to AL participants, whose performance did not differ from controls. Patients with PD also were impaired on tests of orbitofrontal and dorsolateral prefrontal functioning but to a lesser extent than the KS patients. Moreover, most of the PD deficits were driven by the impaired performance of patients whose initial symptoms were on the right side of the body. The ACoA patients were significantly impaired on tests of orbitofrontal but not dorsolateral prefrontal functioning relative to the control group. Together, the results confirm different patterns of frontal-system impairments in patient groups having compromised frontal lobe functioning consequent to varying etiologies."
ALICE CRONIN-GOLOMB,Visual scanning patterns and executive function in relation to facial emotion recognition in aging,"OBJECTIVE: The ability to perceive facial emotion varies with age. Relative to younger adults (YA), older adults (OA) are less accurate at identifying fear, anger, and sadness, and more accurate at identifying disgust. Because different emotions are conveyed by different parts of the face, changes in visual scanning patterns may account for age-related variability. We investigated the relation between scanning patterns and recognition of facial emotions. Additionally, as frontal-lobe changes with age may affect scanning patterns and emotion recognition, we examined correlations between scanning parameters and performance on executive function tests. METHODS: We recorded eye movements from 16 OA (mean age 68.9) and 16 YA (mean age 19.2) while they categorized facial expressions and non-face control images (landscapes), and administered standard tests of executive function. RESULTS: OA were less accurate than YA at identifying fear (p < .05, r = .44) and more accurate at identifying disgust (p < .05, r = .39). OA fixated less than YA on the top half of the face for disgust, fearful, happy, neutral, and sad faces (p values < .05, r values ≥ .38), whereas there was no group difference for landscapes. For OA, executive function was correlated with recognition of sad expressions and with scanning patterns for fearful, sad, and surprised expressions. CONCLUSION: We report significant age-related differences in visual scanning that are specific to faces. The observed relation between scanning patterns and executive function supports the hypothesis that frontal-lobe changes with age may underlie some changes in emotion recognition."
ALICE CRONIN-GOLOMB,Eye movement control during visual pursuit in Parkinson's disease,"BACKGROUND: Prior studies of oculomotor function in Parkinson’s disease (PD) have either focused on saccades without considering smooth pursuit, or tested smooth pursuit while excluding saccades. The present study investigated the control of saccadic eye movements during pursuit tasksand assessed the quality of binocular coordinationas potential sensitive markers of PD. METHODS: Observers fixated on a central cross while a target moved toward it. Once the target reached the fixation cross, observers began to pursue the moving target. To further investigate binocular coordination, the moving target was presented on both eyes (binocular condition), or on one eye only (dichoptic condition). RESULTS: The PD group made more saccades than age-matched normal control adults (NC) both during fixation and pursuit. The difference between left and right gaze positions increased over time during the pursuit period for PD but not for NC. The findings were not related to age, as NC and young-adult control group (YC) performed similarly on most of the eye movement measures, and were not correlated with classical measures of PD severity (e.g., Unified Parkinson’s Disease Rating Scale (UPDRS) score). DISCUSSION: Our results suggest that PD may be associated with impairment not only in saccade inhibition, but also in binocular coordination during pursuit, and these aspects of dysfunction may be useful in PD diagnosis or tracking of disease course."
ALICE CRONIN-GOLOMB,Perceived stigma and quality of life in Parkinson's disease with additional health conditions,"BACKGROUND: Parkinson's disease (PD) is associated with perceived stigma and affects quality of life (QoL). Additional health conditions may influence these consequences of PD. AIMS: This study assessed the impact of health conditions on perceived stigma and QoL in persons with PD. We hypothesised that individuals with more health conditions would report more stigma and poorer QoL. We also examined the contributions of demographic and clinical characteristics to the correlations between health conditions and perceived stigma/QoL. METHODS: We identified 196 eligible participants from the Boston University Online Survey Study of Parkinson's Disease and examined their health history, performance on multiple stigma measures, and scores on the 39-item Parkinson's Disease Questionnaire assessing QoL. RESULTS: At least one health condition was reported by 79% of the sample, with a median of 2 and a range of 0-7 health conditions. More perceived stigma and poorer QoL were associated with thyroid disease, depression, anxiety, and the total number of health conditions. These correlations were related to younger age, less education, and earlier disease onset. Other health conditions (high blood pressure, back/leg surgery, headache, cancer/tumours, and heart disease) were not significantly correlated with stigma or QoL. CONCLUSIONS: Having more health conditions, or thyroid disease, depression, or anxiety, was associated with more perceived stigma and poorer QoL, with younger age, less education, and earlier disease onset affecting the associations. It is important to consider the burden of health conditions and how they affect persons with PD with specific clinical characteristics."
ALICE CRONIN-GOLOMB,Sustained attention training reduces spatial bias in Parkinson’s disease: a pilot case series,"Individuals with Parkinson’s disease (PD) commonly demonstrate lateralized spatial biases, which affect daily functioning. Those with PD with initial motor symptoms on the left body side (LPD) have reduced leftward attention, whereas PD with initial motor symptoms on the right side (RPD) may display reduced rightward attention. We investigated whether a sustained attention training program could help reduce these spatial biases. Four non-demented individuals with PD (2 LPD, 2 RPD) performed a visual search task before and after 1 month of computer training. Before training, all participants showed a significant spatial bias and after training, all participants’ spatial bias was eliminated."
ALICE CRONIN-GOLOMB,Side and type of initial motor symptom influences visuospatial functioning in Parkinson's disease,"BACKGROUND/OBJECTIVES: Visuospatial problems are common in Parkinson's disease (PD) and likely stem from dysfunction in dopaminergic pathways and consequent disruption of cortical functioning. Characterizing the motor symptoms at disease onset provides a method of observing how dysfunction in these pathways influences visuospatial cognition. We examined two types of motor characteristics: Body side (left or right) and type of initial symptom (tremor or symptom other than tremor). METHODS: 31 non-demented patients with PD, 16 with left-side onset (LPD) and 15 with right-side onset (RPD), as well as 17 healthy control participants (HC). The PD group was also divided by type of initial motor symptom, 15 having tremor as the initial symptom and 16 having an initial symptom other than tremor. Visuospatial function was assessed with the Clock Drawing Test. RESULTS: Of the four Clock Drawing scoring methods used, the Rouleau method showed sensitivity to subgroup differences. As predicted, the LPD and non-tremor subgroups, but not the other subgroups, performed more poorly than the HC group. CONCLUSION: The findings provide further evidence for differences in cognition between these subtypes of PD and highlight the importance of considering disease subtypes when examining cognition."
ALICE CRONIN-GOLOMB,"Initial investigation of test-retest reliability of home-to-home teleneuropsychological assessment in healthy, English-speaking adults","Prior teleneuropsychological research has assessed the reliability between in-person and remote administration of cognitive assessments. Few, if any, studies have examined the test-retest reliability of cognitive assessments conducted in sequential clinic-to-home or home-to-home teleneuropsychological evaluations - a critical issue given the state of clinical practice during the COVID-19 pandemic. This study examined this key psychometric question for several cognitive tests administered over repeated videoconferencing visits 4-6 months apart in a sample of healthy English-speaking adults. A total of 44 participants (ages 18-75) completed baseline and follow-up cognitive testing 4-6 months apart. Testing was conducted in a home-to-home setting over HIPAA-compliant videoconferencing meetings on participants' audio-visual enabled laptop or desktop computers. The following measures were repeated at both virtual visits: the Controlled Oral Word Association Test (FAS), Category Fluency (Animals), and Digit Span Forward and Backward from the Wechsler Adult Intelligence Scale, Fourth Edition. Intraclass correlation coefficients (ICC), Pearson correlations, root mean square difference (RMSD), and concordance correlation coefficients (CCC) were calculated as test-retest reliability metrics, and practice effects were assessed using paired-samples t-tests. Some tests exhibited small practice effects, and test-retest reliability was marginal or worse for all measures except FAS, which had adequate reliability (based on ICC and r). Reliability estimates with RMSD suggested that change within +/- 1 SD on these measures may reflect typical test-retest variability. The included cognitive measures exhibited questionable reliability over repeated home-to-home videoconferencing evaluations. Future teleneuropsychology test-retest reliability research is needed with larger, more diverse samples and in clinical populations."
ALICE CRONIN-GOLOMB,Predictors of self-perceived stigma in Parkinson’s disease,"OBJECTIVE: The burden of PD extends beyond physical limitations and includes significant psychosocial adjustments as individuals undergo changes to their self-perception and how others perceive them. There is limited quantitative evidence of the factors that contribute to self-perceived stigma, which we addressed in the present study. METHODS: In 362 individuals with PD (157 women, 205 men), self-perceived stigma was measured by the four-item stigma subscale of the Parkinson's Disease Questionnaire (PDQ-39). Hierarchical linear modeling was used to assess predictors of stigma including demographics (age, gender) and disease characteristics: duration, stage (Hoehn & Yahr Scale), motor severity (Unified Parkinson's Disease Rating Scale, UPDRS, Part 3), activities of daily living (UPDRS Part 2), and depression (Geriatric Depression Scale). Predictor variables were chosen based on their significant correlations with the stigma subscale. Further analyses were conducted for men and women separately. RESULTS: For the total sample, the full model accounted for 14% of the variance in stigma perception (p < .001). Younger age and higher depression scores were the only significant predictors (both p < .001). This pattern was also seen for the men in the sample. For the women, only depression was a significant predictor. Depression mediated the relation between stigma and activities of daily living. CONCLUSIONS: Younger age (men) and depression (men and women) were the primary predictors of self-perceived stigma in PD. Disease characteristics (motor and ADL) did not contribute to stigma perception. Depression is a potential treatment target for self-perceived stigma in PD."
ALICE CRONIN-GOLOMB,Sleep quality influences subsequent motor skill acquisition,"While the influence of sleep on motor memory consolidation has been extensively investigated, its relation to initial skill acquisition is less well understood. The purpose of the present study was to investigate the influence of sleep quality and quantity on subsequent motor skill acquisition in young adults without sleep disorders. Fifty-five healthy adults (mean age = 23.8 years; 34 women) wore actigraph wristbands for 4 nights, which provided data on sleep patterns before the experiment, and then returned to the laboratory to engage in a motor sequence learning task (explicit 5-item finger sequence tapping task). Indicators of sleep quality and quantity were then regressed on a measure of motor skill acquisition (Gains Within Training, GWT). Wake After Sleep Onset (WASO; i.e., the total amount of time the participants spent awake after falling asleep) was significantly and negatively related to GWT. This effect was not because of general arousal level, which was measured immediately before the motor task. Conversely, there was no relationship between GWT and sleep duration or self-reported sleep quality. These results indicate that sleep quality, as assessed by WASO and objectively measured with actigraphy before the motor task, significantly impacts motor skill acquisition in young healthy adults without sleep disorders. (PsycINFO Database Record. (c) 2016 APA, all rights reserved)."
ALICE CRONIN-GOLOMB,Objective measurement of sleep by smartphone application: comparison with actigraphy and relation to self-reported sleep,"AIM: Smartphone technology is increasingly used by the public to assess sleep. Specific features of some sleep-tracking applications are comparable to actigraphy in objectively monitoring sleep. The clinical utility of smartphone apps should be investigated further to increase access to convenient means of monitoring sleep. METHODS: Smartphone and subjective sleep measures were administered to 29 community-dwelling healthy adults [aged 20–67, Mean (M) = 26.8; 18 women, 11 men], and actigraphy to 19 of them. Total sleep time (TST) and sleep efficiency were measured with actigraphy and the Sleep Time App (Azumio Inc.). Sleep diaries captured subjective TST and sleep efficiency, and the Epworth Sleepiness Scale and Pittsburgh Sleep Quality Index provided self-report data. An exit questionnaire was administered to examine App feasibility and likelihood of future use. RESULTS: The App significantly overestimated TST when compared to actigraphy. There was no significant difference in sleep efficiency between methodologies. There was also no significant difference between TST recorded through the App and through sleep diaries. Participants’ self-reported ease of use of the smartphone App positively correlated with likelihood of future use. CONCLUSIONS: Based on the current findings, future research is needed to investigate the utility and feasibility of multiple smartphone applications in monitoring sleep in healthy and clinical populations."
ALICE CRONIN-GOLOMB,Relation of subjective quality of life to motor symptom profile in Parkinson's disease,"Parkinson's disease (PD) presents with extensive heterogeneity in symptomatology, inviting examination of disease subtypes. One significant categorization is by whether patients present at onset with tremor as the dominant symptom (TD) or with nontremor symptoms (NTD). We examined differences in quality of life between TD and NTD patients using the Parkinson's Disease Questionnaire-39 (PDQ-39), correlating performance with aspects of motor function as indexed by the Unified Parkinson's Disease Rating Scale (UPDRS). Participants included 35 nondemented individuals (19 TD, 16 NTD) matched on clinical and demographic characteristics. NTD had significantly lower overall PDQ-39 scores, particularly for the mobility subscale. Several UPDRS subscale scores significantly correlated with quality of life, especially for NTD. Further, the correlations were driven by nontremor type symptoms, even in TD patients. Determining reliable subtypes of PD may aid in prognosis and treatment optimization, thereby enhancing quality of life in afflicted individuals."
ALICE CRONIN-GOLOMB,Fluency boost from walking in Parkinson's disease,"OBJECTIVE: Examine the impact of a motor task on verbal fluency in individuals with Parkinson’s disease (PD) Background: Dual-tasking, in which individuals engage simultaneously in motor and cognitive tasks, has long been known to impair motor performance in PD; recent evidence indicates that it also impairs cognition (set-shifting). In healthy adults without PD, motor activity can improve performance on tasks of ideational fluency. Performance on phonemic verbal fluency (VF), an executive-function task widely used in clinical evaluation and in research studies of PD, is correlated with ideational fluency in healthy young adults and in those with focal frontal-lobe lesions. VF may likewise be enhanced by motor activity, perhaps even in PD, in which both motor and executive functions are impaired. METHODS: Non-demented individuals with mild-moderate idiopathic PD (n=18, 10 men, mean age 64 [SD=10]) performed the Timed Up and Go (TUG), a brief motor task that taxes both motor and executive function. The order was single-task TUG; cognitive/motor dual-task VF/TUG; single-task VF. VF was measured in words per second (wps), calculated for single-task VF for the time the participant needed to complete the dual-task condition. Proportionate words per second (pwps) measured dual-task impact as a percentage of single-task VF. We also examined correlations between the dual-task effect and disease severity: Hoehn & Yahr stage and United Parkinson’s Disease Rating Scale (UPDRS) total score. RESULTS: Mean wps was higher for dual-task than single-task VF (t=3.5, p=.003). The size of this dual-task benefit inversely correlated with UPDRS total score (ρ=-.50, p= .03). Mean pwps also showed an advantage for dual-task VF (t=3.7, p=.002) and an inverse correlation with UPDRS total score (ρ=-.47, p=.05). CONCLUSIONS: Individuals with PD may perform better on verbal fluency when it is performed simultaneously with a motor task. A potential explanation is the reported premotor-parietal hyperconnectivity in PD that is associated with fewer motor difficulties during dual-tasking. In mild PD, this presumably compensatory connectivity may facilitate prefrontal-parietal arousal and central executive network processing, leading to improved function or regulation of attention or fluency. In later stages of PD, this compensation may decline or be insufficient to aid fluency when basal ganglia dysfunction worsens."
ALICE CRONIN-GOLOMB,Discordance between reports of internalized symptoms in persons with Parkinson’s disease and informants: results from an online survey,"BACKGROUND: Self-report of motor and non-motor symptoms is integral to understanding daily challenges of persons with Parkinson's disease (PwPD). Care partners are often asked to serve as informants regarding symptom severity, raising the question of concordance with PwPD self-reports, especially regarding internalized (not outwardly visible) symptoms. OBJECTIVES: Concordance between PwPD and informant ratings of motor and non-motor symptoms was evaluated across multiple domains. METHODS: In 60 PwPD-informant pairs, we compared ratings on 11 online self-report measures comprising 33 total scores, 2/3 of which represented purely internalized symptoms. For discordant scores, multiple regression analyses were used to examine demographic/clinical predictors. RESULTS: Though concordant on 85% of measures, PwPD endorsed more non-motor symptoms, bodily discomfort, stigma, and motor symptoms than informants. For PwPD, younger age, greater disease severity, and female gender predicted discordance. CONCLUSIONS: Discordance between PwPD and informants on measures assessing symptoms that cannot be outwardly observed may require targeted education."
ALICE CRONIN-GOLOMB,"The therapeutic potential of exercise to improve mood, cognition, and sleep in Parkinson's disease","In addition to the classic motor symptoms, Parkinson's disease (PD) is associated with a variety of nonmotor symptoms that significantly reduce quality of life, even in the early stages of the disease. There is an urgent need to develop evidence‐based treatments for these symptoms, which include mood disturbances, cognitive dysfunction, and sleep disruption. We focus here on exercise interventions, which have been used to improve mood, cognition, and sleep in healthy older adults and clinical populations, but to date have primarily targeted motor symptoms in PD. We synthesize the existing literature on the benefits of aerobic exercise and strength training on mood, sleep, and cognition as demonstrated in healthy older adults and adults with PD, and suggest that these types of exercise offer a feasible and promising adjunct treatment for mood, cognition, and sleep difficulties in PD. Across stages of the disease, exercise interventions represent a treatment strategy with the unique ability to improve a range of nonmotor symptoms while also alleviating the classic motor symptoms of the disease. Future research in PD should include nonmotor outcomes in exercise trials with the goal of developing evidence‐based exercise interventions as a safe, broad‐spectrum treatment approach to improve mood, cognition, and sleep for individuals with PD."
ALICE CRONIN-GOLOMB,Alexithymia and apathy in Parkinson's disease: neurocognitive correlates,"Non-motor symptoms such as neuropsychiatric and cognitive dysfunction have been found to be common in Parkinson’s disease (PD) but the relation between such symptoms is poorly understood. We focused on alexithymia, an impairment of affective and cognitive emotional processing, as there is evidence for its interaction with cognition in other disorders. Twenty-two non-demented PD patients and 22 matched normal control adults (NC) were administered rating scales assessing neuropsychiatric status, including alexithymia, apathy, and depression, and a series of neuropsychological tests. As expected, PD patients showed more alexithymia than NC, and there was a significant association between alexithymia and disease stage. Alexithymia was associated with performance on non-verbally mediated measures of executive and visuospatial function, but not on verbally mediated tasks. By contrast, there was no correlation between cognition and ratings of either depression or apathy. Our findings demonstrate a distinct association of alexithymia with non-verbal cognition in PD, implicating right hemisphere processes, and differentiate between alexithymia and other neuropsychiatric symptoms in regard to PD cognition."
ALICE CRONIN-GOLOMB,"Patterns of prefrontal dysfunction in alcoholics with and without Korsakoff’s syndrome, patients with Parkinson’s disease, and patients with rupture and repair of the anterior communicating artery","This study compared patterns of frontal-lobe dysfunction in alcoholics with Korsakoff’s syndrome (KS: n = 9), non-Korsakoff alcoholics (AL: n = 28), patients with Parkinson’s disease (PD: n = 18), and patients with rupture and repair of the anterior communicating artery (ACoA: n = 4) relative to healthy non-neurological control (NC) participants (n = 70). The tests administered were sensitive to functions of dorsolateral prefrontal and orbitofrontal subsystems. Measures included perseverative errors on the Wisconsin Card Sorting Test (WCST-pe), errors on object alternation (OA), errors on Trails B, number of words generated on the Controlled Oral Word Association Test (COWAT), and number of categories completed on the WCST (WCST-cc). KS patients were as impaired as AL participants on orbitofrontal measures and, on dorsolateral prefrontal measures, were impaired relative to AL participants, whose performance did not differ from controls. Patients with PD also were impaired on tests of orbitofrontal and dorsolateral prefrontal functioning but to a lesser extent than the KS patients. Moreover, most of the PD defi cits were driven by the impaired performance of patients whose initial symptoms were on the right side of the body. The ACoA patients were signifi cantly impaired on tests of orbitofrontal but not dorsolateral prefrontal functioning relative to the control group. Together, the results confi rm different patterns of frontal-system impairments in patient groups having compromised frontal lobe functioning consequent to varying etiologies."
ALICE CRONIN-GOLOMB,Genetic and environmental influences on sleep quality in middle‐aged men: a twin study,"Poor sleep quality is a risk factor for a number of cognitive and physiological age-related disorders. Identifying factors underlying sleep quality are important in understanding the etiology of these age-related health disorders. We investigated the extent to which genes and the environment contribute to subjective sleep quality in middle-aged male twins using the classical twin design. We used the Pittsburgh Sleep Quality Index to measure sleep quality in 1218 middle-aged twin men from the Vietnam Era Twin Study of Aging (mean age = 55.4 years; range 51-60; 339 monozygotic twin pairs, 257 dizygotic twin pairs, 26 unpaired twins). The mean PSQI global score was 5.6 [SD = 3.6; range 0-20]. Based on univariate twin models, 34% of variability in the global PSQI score was due to additive genetic effects (heritability) and 66% was attributed to individual-specific environmental factors. Common environment did not contribute to the variability. Similarly, the heritability of poor sleep-a dichotomous measure based on the cut-off of global PSQI>5-was 31%, with no contribution of the common environment. Heritability of six of the seven PSQI component scores (subjective sleep quality, sleep latency, sleep duration, habitual sleep efficiency, sleep disturbances, and daytime dysfunction) ranged from 0.15 to 0.31, whereas no genetic influences contributed to the use of sleeping medication. Additive genetic influences contribute to approximately one-third of the variability of global subjective sleep quality. Our results in middle-aged men constitute a first step towards examination of the genetic relationship between sleep and other facets of aging."
ALICE CRONIN-GOLOMB,The impact of motor symptoms on self-reported anxiety in Parkinson's disease,"OBJECTIVE: Anxiety is commonly endorsed in Parkinson's disease (PD) and significantly affects quality of life. The Beck Anxiety Inventory (BAI) is often used but contains items that overlap with common PD motor symptoms (e.g., “hands trembling”). Because of these overlapping items, we hypothesized that PD motor symptoms would significantly affect BAI scores. METHODS: One hundred non-demented individuals with PD and 74 healthy control participants completed the BAI. PD motor symptoms were assessed by the Unified Parkinson's Disease Rating Scale (UPDRS). Factor analysis of the BAI assessed for a PD motor factor, and further analyses assessed how this factor affected BAI scores. RESULTS: BAI scores were significantly higher for PD than NC. A five-item PD motor factor correlated with UPDRS observer-rated motor severity and mediated the PD-control difference on BAI total scores. An interaction occurred, whereby removal of the PD motor factor resulted in a significant reduction in BAI scores for PD relative to NC. The correlation between the BAI and UPDRS significantly declined when controlling for the PD motor factor. CONCLUSIONS: The results indicate that commonly endorsed BAI items may reflect motor symptoms such as tremor instead of, or in addition to, genuine mood symptoms. These findings highlight the importance of considering motor symptoms in the assessment of anxiety in PD and point to the need for selecting anxiety measures that are less subject to contamination by the motor effects of movement disorders."
ALICE CRONIN-GOLOMB,The impact of sleep quality on cognitive functioning in Parkinson's disease,"In healthy individuals and those with insomnia, poor sleep quality is associated with decrements in performance on tests of cognition, especially executive function. Sleep disturbances and cognitive deficits are both prevalent in Parkinson's disease (PD). Sleep problems occur in over 75% of patients, with sleep fragmentation and decreased sleep efficiency being the most common sleep complaints, but their relation to cognition is unknown. We examined the association between sleep quality and cognition in PD. In 35 non-demented individuals with PD and 18 normal control adults (NC), sleep was measured using 24-hr wrist actigraphy over 7 days. Cognitive domains tested included attention and executive function, memory and psychomotor function. In both groups, poor sleep was associated with worse performance on tests of attention/executive function but not memory or psychomotor function. In the PD group, attention/executive function was predicted by sleep efficiency, whereas memory and psychomotor function were not predicted by sleep quality. Psychomotor and memory function were predicted by motor symptom severity. This study is the first to demonstrate that sleep quality in PD is significantly correlated with cognition and that it differentially impacts attention and executive function, thereby furthering our understanding of the link between sleep and cognition."
ALICE CRONIN-GOLOMB,Cognitive and brain imaging markers of presenilin1 E280A autosomal dominant Alzheimer’s disease: findings from the Colombian kindred,"The study of individuals with autosomal dominant Alzheimer's disease affords one of the best opportunities to characterize the biological and cognitive changes of Alzheimer's disease that occur over the course of the preclinical and symptomatic stages. Unifying the knowledge gained from the past three decades of research in the world's largest single-mutation autosomal dominant Alzheimer's disease kindred - a family in Antioquia, Colombia with the E280A mutation in the Presenilin1 gene - will provide new directions for Alzheimer's research and a framework for generalizing the findings from this cohort to the more common sporadic form of Alzheimer's disease. As this specific mutation is virtually 100% penetrant for the development of the disease by midlife, we use a previously defined median age of onset for mild cognitive impairment for this cohort to examine the trajectory of the biological and cognitive markers of the disease as a function of the carriers' estimated years to clinical onset. Studies from this cohort suggest that structural and functional brain abnormalities - such as cortical thinning and hyperactivation in memory networks - as well as differences in biofluid and in vivo measurements of Alzheimer's-related pathological proteins distinguish Presenilin1 E280A mutation carriers from non-carriers as early as childhood, or approximately three decades before the median age of onset of clinical symptoms. We conclude our review with discussion on future directions for Alzheimer's disease research, with specific emphasis on ways to design studies that compare the generalizability of research in autosomal dominant Alzheimer's disease to the larger sporadic Alzheimer's disease population."
ALICE CRONIN-GOLOMB,Neurocognitive correlates of apathy and anxiety in Parkinson's disease,"Parkinson's disease (PD) is associated with various nonmotor symptoms including neuropsychiatric and cognitive dysfunction. We examined the relation between apathy, anxiety, side of onset of motor symptoms, and cognition in PD. We hypothesized that PD patients would show different neuropsychiatric and neurocognitive profiles depending on the side of onset. 22 nondemented PD patients (11 right-side onset (RPD) with predominant left-hemisphere pathology, and 11 LPD) and 22 matched healthy controls (NC) were administered rating scales assessing apathy and anxiety, and a series of neuropsychological tests. PD patients showed a higher anxiety level than NC. There was a significant association between apathy, anxiety, and disease duration. In LPD, apathy but not anxiety was associated with performance on nonverbally mediated executive function and visuospatial measures, whereas, in RPD, anxiety but not apathy correlated with performance on verbally mediated tasks. Our findings demonstrated a differential association of apathy and anxiety to cognition in PD."
ALICE CRONIN-GOLOMB,Effects of orthostatic hypotension on cognition in Parkinson's disease,"OBJECTIVE: To investigate the relation between orthostatic hypotension (OH) and posture-mediated cognitive impairment in Parkinson disease (PD) using a cross-sectional and within-group design. METHODS: Individuals without dementia with idiopathic PD included 18 with OH (PDOH) and 19 without OH; 18 control participants were also included. Neuropsychological tests were conducted in supine and upright-tilted positions. Blood pressure was assessed in each posture. RESULTS: The PD groups performed similarly while supine, demonstrating executive dysfunction in sustained attention and response inhibition, and reduced semantic fluency and verbal memory (encoding and retention). Upright posture exacerbated and broadened these deficits in the PDOH group to include phonemic fluency, psychomotor speed, and auditory working memory. When group-specific supine scores were used as baseline anchors, both PD groups showed cognitive changes following tilt, with the PDOH group exhibiting a wider range of deficits in executive function and memory as well as significant changes in visuospatial function. CONCLUSIONS: Cognitive deficits in PD have been widely reported with assessments performed in the supine position, as seen in both our PD groups. Here we demonstrated that those with PDOH had transient, posture-mediated changes in excess of those found in PD without OH. These observed changes suggest an acute, reversible effect. Understanding the effects of OH due to autonomic failure on cognition is desirable, particularly as neuroimaging and clinical assessments collect data only in the supine or seated positions. Identification of a distinct neuropsychological profile in PD with OH has quality of life implications, and OH presents itself as a possible target for intervention in cognitive disturbance."
ALICE CRONIN-GOLOMB,Bistable perception in normal aging: perceptual reversibility and its relation to cognition,"The effects of age on the ability to resolve perceptual ambiguity are unknown, though it depends on fronto-parietal attentional networks known to change with age. We presented the bistable Necker cube to 24 middle-aged and older adults (OA; 56–78 years) and 20 younger adults (YA; 18–24 years) under passive-viewing and volitional control conditions: Hold one cube percept and Switch between cube percepts. During passive viewing, OA had longer dominance durations (time spent on each percept) than YA. In the Hold condition, OA were less able than YA to increase dominance durations. In the Switch condition, OA and YA did not differ in performance. Dominance durations in either condition correlated with performance on tests of executive function mediated by the frontal lobes. Eye movements (fixation deviations) did not differ between groups. These results suggest that OA’s reduced ability to hold a percept may arise from reduced selective attention. The lack of correlation of performance between Hold and executive-function measures suggests at least a partial segregation of underlying mechanisms."
PANKAJ MEHTA,Emergent simplicity in microbial community assembly,"A major unresolved question in microbiome research is whether the complex taxonomic architectures observed in surveys of natural communities can be explained and predicted by fundamental, quantitative principles. Bridging theory and experiment is hampered by the multiplicity of ecological processes that simultaneously affect community assembly in natural ecosystems. We addressed this challenge by monitoring the assembly of hundreds of soil- and plant-derived microbiomes in well-controlled minimal synthetic media. Both the community-level function and the coarse-grained taxonomy of the resulting communities are highly predictable and governed by nutrient availability, despite substantial species variability. By generalizing classical ecological models to include widespread nonspecific cross-feeding, we show that these features are all emergent properties of the assembly of large microbial communities, explaining their ubiquity in natural microbiomes."
PANKAJ MEHTA,Exponential sensitivity of noise-driven switching in genetic networks,"Cells are known to utilize biochemical noise to probabilistically switch between distinct gene expression states. We demonstrate that such noise-driven switching is dominated by tails of probability distributions and is therefore exponentially sensitive to changes in physiological parameters such as transcription and translation rates. However, provided mRNA lifetimes are short, switching can still be accurately simulated using protein-only models of gene expression. Exponential sensitivity limits the robustness of noise-driven switching, suggesting cells may use other mechanisms in order to switch reliably."
PANKAJ MEHTA,Geometry and non-adiabatic response in quantum and classical systems,"In these lecture notes, partly based on a course taught at the Karpacz Winter School in March 2014, we explore the close connections between non-adiabatic response of a system with respect to macroscopic parameters and the geometry of quantum and classical states. We center our discussion around adiabatic gauge potentials, which are the generators of unitary basis transformations in quantum systems and generators of special canonical transformations in classical systems. In quantum systems, eigenstate expectation values of these potentials are the Berry connections and the covariance matrix of these gauge potentials is the geometric tensor, whose antisymmetric part defines the Berry curvature and whose symmetric part is the Fubini-Study metric tensor. In classical systems one simply replaces the eigenstate expectation value by an average over the micro-canonical shell. For complicated interacting systems, we show that a variational principle may be used to derive approximate gauge potentials. We then express the non-adiabatic response of the physical observables of the system through these gauge potentials, specifically demonstrating the close connection of the geometric tensor to the notions of Lorentz force and renormalized mass. We highlight applications of this formalism to deriving counter-diabatic (dissipationless) driving protocols in various systems, as well as to finding equations of motion for slow macroscopic parameters coupled to fast microscopic degrees of freedom that go beyond macroscopic Hamiltonian dynamics. Finally, we illustrate these ideas with a number of simple examples and highlight a few more complicated ones drawn from recent literature."
PANKAJ MEHTA,Flux imbalance analysis and the sensitivity of cellular growth to changes in metabolite pools,"Stoichiometric models of metabolism, such as flux balance analysis (FBA), are classically applied to predicting steady state rates - or fluxes - of metabolic reactions in genome-scale metabolic networks. Here we revisit the central assumption of FBA, i.e. that intracellular metabolites are at steady state, and show that deviations from flux balance (i.e. flux imbalances) are informative of some features of in vivo metabolite concentrations. Mathematically, the sensitivity of FBA to these flux imbalances is captured by a native feature of linear optimization, the dual problem, and its corresponding variables, known as shadow prices. First, using recently published data on chemostat growth of Saccharomyces cerevisae under different nutrient limitations, we show that shadow prices anticorrelate with experimentally measured degrees of growth limitation of intracellular metabolites. We next hypothesize that metabolites which are limiting for growth (and thus have very negative shadow price) cannot vary dramatically in an uncontrolled way, and must respond rapidly to perturbations. Using a collection of published datasets monitoring the time-dependent metabolomic response of Escherichia coli to carbon and nitrogen perturbations, we test this hypothesis and find that metabolites with negative shadow price indeed show lower temporal variation following a perturbation than metabolites with zero shadow price. Finally, we illustrate the broader applicability of flux imbalance analysis to other constraint-based methods. In particular, we explore the biological significance of shadow prices in a constraint-based method for integrating gene expression data with a stoichiometric model. In this case, shadow prices point to metabolites that should rise or drop in concentration in order to increase consistency between flux predictions and gene expression data. In general, these results suggest that the sensitivity of metabolic optima to violations of the steady state constraints carries biologically significant information on the processes that control intracellular metabolites in the cell."
PANKAJ MEHTA,"Available energy fluxes drive a phase transition in the diversity, stability, and functional structure of microbial communities","A fundamental goal of microbial ecology is to understand what determines the diversity, stability, and structure of microbial ecosystems. The microbial context poses special conceptual challenges because of the strong mutual influences between the microbes and their chemical environment through the consumption and production of metabolites. By analyzing a generalized consumer resource model that explicitly includes cross-feeding, stochastic colonization, and thermodynamics, we show that complex microbial communities generically exhibit a transition as a function of available energy fluxes from a “resource-limited” regime where community structure and stability is shaped by energetic and metabolic considerations to a diverse regime where the dominant force shaping microbial communities is the overlap between species’ consumption preferences. These two regimes have distinct species abundance patterns, different functional profiles, and respond differently to environmental perturbations. Our model reproduces large-scale ecological patterns observed across multiple experimental settings such as nestedness and differential beta diversity patterns along energy gradients. We discuss the experimental implications of our results and possible connections with disorder-induced phase transitions in statistical physics."
PANKAJ MEHTA,Reinforcement learning in different phases of quantum control,"The ability to prepare a physical system in a desired quantum state is central to many areas of physics such as nuclear magnetic resonance, cold atoms, and quantum computing. Yet, preparing states quickly and with high fidelity remains a formidable challenge. In this work, we implement cutting-edge reinforcement learning (RL) techniques and show that their performance is comparable to optimal control methods in the task of finding short, high-fidelity driving protocol from an initial to a target state in nonintegrable many-body quantum systems of interacting qubits. RL methods learn about the underlying physical system solely through a single scalar reward (the fidelity of the resulting state) calculated from numerical simulations of the physical system. We further show that quantum-state manipulation viewed as an optimization problem exhibits a spin-glass-like phase transition in the space of protocols as a function of the protocol duration. Our RL-aided approach helps identify variational protocols with nearly optimal fidelity, even in the glassy phase, where optimal state manipulation is exponentially hard. This study highlights the potential usefulness of RL for applications in out-of-equilibrium quantum physics."
PANKAJ MEHTA,Broken symmetry in a correlated quantum control landscape,"We analyze the physics of optimal protocols to prepare a target state with high fidelity in a symmetrically coupled two-qubit system. By varying the protocol duration, we find a discontinuous phase transition, which is characterized by a spontaneous breaking of a Z2 symmetry in the functional form of the optimal protocol, and occurs below the quantum speed limit. We study in detail this phase and demonstrate that even though high-fidelity protocols come degenerate with respect to their fidelity, they lead to final states of different entanglement entropy shared between the qubits. Consequently, while globally both optimal protocols are equally far away from the target state, one is locally closer than the other. An approximate variational mean-field theory which captures the physics of the different phases is developed."
HELEN BARBAS,The emotional gatekeeper: a computational model of attentional selection and suppression through the pathway from the amygdala to the inhibitory thalamic reticular nucleus,"In a complex environment that contains both opportunities and threats, it is important for an organism to flexibly direct attention based on current events and prior plans. The amygdala, the hub of the brain's emotional system, is involved in forming and signaling affective associations between stimuli and their consequences. The inhibitory thalamic reticular nucleus (TRN) is a hub of the attentional system that gates thalamo-cortical signaling. In the primate brain, a recently discovered pathway from the amygdala sends robust projections to TRN. Here we used computational modeling to demonstrate how the amygdala-TRN pathway, embedded in a wider neural circuit, can mediate selective attention guided by emotions. Our Emotional Gatekeeper model demonstrates how this circuit enables focused top-down, and flexible bottom-up, allocation of attention. The model suggests that the amygdala-TRN projection can serve as a unique mechanism for emotion-guided selection of signals sent to cortex for further processing. This inhibitory selection mechanism can mediate a powerful affective 'framing' effect that may lead to biased decision-making in highly charged emotional situations. The model also supports the idea that the amygdala can serve as a relevance detection system. Further, the model demonstrates how abnormal top-down drive and dysregulated local inhibition in the amygdala and in the cortex can contribute to the attentional symptoms that accompany several neuropsychiatric disorders."
HELEN BARBAS,Parallel driving and modulatory pathways link the prefrontal cortex and thalamus,"Pathways linking the thalamus and cortex mediate our daily shifts from states of attention to quiet rest, or sleep, yet little is known about their architecture in high-order neural systems associated with cognition, emotion and action. We provide novel evidence for neurochemical and synaptic specificity of two complementary circuits linking one such system, the prefrontal cortex with the ventral anterior thalamic nucleus in primates. One circuit originated from the neurochemical group of parvalbumin-positive thalamic neurons and projected focally through large terminals to the middle cortical layers, resembling ‘drivers’ in sensory pathways. Parvalbumin thalamic neurons, in turn, were innervated by small ‘modulatory’ type cortical terminals, forming asymmetric (presumed excitatory) synapses at thalamic sites enriched with the specialized metabotropic glutamate receptors. A second circuit had a complementary organization: it originated from the neurochemical group of calbindin-positive thalamic neurons and terminated through small ‘modulatory’ terminals over long distances in the superficial prefrontal layers. Calbindin thalamic neurons, in turn, were innervated by prefrontal axons through small and large terminals that formed asymmetric synapses preferentially at sites with ionotropic glutamate receptors, consistent with a driving pathway. The largely parallel thalamo-cortical pathways terminated among distinct and laminar-specific neurochemical classes of inhibitory neurons that differ markedly in inhibitory control. The balance of activation of these parallel circuits that link a high-order association cortex with the thalamus may allow shifts to different states of consciousness, in processes that are disrupted in psychiatric diseases."
HELEN BARBAS,Organization of primate amygdalar-thalamic pathways for emotions,"Studies on the thalamus have mostly focused on sensory relay nuclei, but the organization of pathways associated with emotions is not well understood. We addressed this issue by testing the hypothesis that the primate amygdala acts, in part, like a sensory structure for the affective import of stimuli and conveys this information to the mediodorsal thalamic nucleus, magnocellular part (MDmc). We found that primate sensory cortices innervate amygdalar sites that project to the MDmc, which projects to the orbitofrontal cortex. As in sensory thalamic systems, large amygdalar terminals innervated excitatory relay and inhibitory neurons in the MDmc that facilitate faithful transmission to the cortex. The amygdala, however, uniquely innervated a few MDmc neurons by surrounding and isolating large segments of their proximal dendrites, as revealed by three-dimensional high-resolution reconstruction. Physiologic studies have shown that large axon terminals are found in pathways issued from motor systems that innervate other brain centers to help distinguish self-initiated from other movements. By analogy, the amygdalar pathway to the MDmc may convey signals forwarded to the orbitofrontal cortex to monitor and update the status of the environment in processes deranged in schizophrenia, resulting in attribution of thoughts and actions to external sources."
HELEN BARBAS,Altered neural connectivity in excitatory and inhibitory cortical circuits in autism,"Converging evidence from diverse studies suggests that atypical brain connectivity in autism affects in distinct ways short- and long-range cortical pathways, disrupting neural communication and the balance of excitation and inhibition. This hypothesis is based mostly on functional non-invasive studies that show atypical synchronization and connectivity patterns between cortical areas in children and adults with autism. Indirect methods to study the course and integrity of major brain pathways at low resolution show changes in fractional anisotropy (FA) or diffusivity of the white matter in autism. Findings in post-mortem brains of adults with autism provide evidence of changes in the fine structure of axons below prefrontal cortices, which communicate over short- or long-range pathways with other cortices and subcortical structures. Here we focus on evidence of cellular and axon features that likely underlie the changes in short- and long-range communication in autism. We review recent findings of changes in the shape, thickness, and volume of brain areas, cytoarchitecture, neuronal morphology, cellular elements, and structural and neurochemical features of individual axons in the white matter, where pathology is evident even in gross images. We relate cellular and molecular features to imaging and genetic studies that highlight a variety of polymorphisms and epigenetic factors that primarily affect neurite growth and synapse formation and function in autism. We report preliminary findings of changes in autism in the ratio of distinct types of inhibitory neurons in prefrontal cortex, known to shape network dynamics and the balance of excitation and inhibition. Finally we present a model that synthesizes diverse findings by relating them to developmental events, with a goal to identify common processes that perturb development in autism and affect neural communication, reflected in altered patterns of attention, social interactions, and language."
HELEN BARBAS,"Distinction of neurons, glia and endothelial cells in the cerebral cortex: an algorithm based on cytological features","The estimation of the number or density of neurons and types of glial cells and their relative proportions in different brain areas are at the core of rigorous quantitative neuroanatomical studies. Unfortunately, the lack of detailed, updated, systematic and well-illustrated descriptions of the cytology of neurons and glial cell types, especially in the primate brain, makes such studies especially demanding, often limiting their scope and broad use. Here, following an extensive analysis of histological materials and the review of current and classical literature, we compile a list of precise morphological criteria that can facilitate and standardize identification of cells in stained sections examined under the microscope. We describe systematically and in detail the cytological features of neurons and glial cell types in the cerebral cortex of the macaque monkey and the human using semithin and thick sections stained for Nissl. We used this classical staining technique because it labels all cells in the brain in distinct ways. In addition, we corroborate key distinguishing characteristics of different cell types in sections immunolabeled for specific markers counterstained for Nissl and in ultrathin sections processed for electron microscopy. Finally, we summarize the core features that distinguish each cell type in easy-to-use tables and sketches, and structure these key features in an algorithm that can be used to systematically distinguish cellular types in the cerebral cortex. Moreover, we report high inter-observer algorithm reliability, which is a crucial test for obtaining consistent and reproducible cell counts in unbiased stereological studies. This protocol establishes a consistent framework that can be used to reliably identify and quantify cells in the cerebral cortex of primates as well as other mammalian species in health and disease."
HELEN BARBAS,"The Structural Model: a theory linking connections, plasticity, pathology, development and evolution of the cerebral cortex","The classical theory of cortical systematic variation has been independently described in reptiles, monotremes, marsupials and placental mammals, including primates, suggesting a common bauplan in the evolution of the cortex. The Structural Model is based on the systematic variation of the cortex and is a platform for advancing testable hypotheses about cortical organization and function across species, including humans. The Structural Model captures the overall laminar structure of areas by dividing the cortical architectonic continuum into discrete categories (cortical types), which can be used to test hypotheses about cortical organization. By type, the phylogenetically ancient limbic cortices-which form a ring at the base of the cerebral hemisphere-are agranular if they lack layer IV, or dysgranular if they have an incipient granular layer IV. Beyond the dysgranular areas, eulaminate type cortices have six layers. The number and laminar elaboration of eulaminate areas differ depending on species or cortical system within a species. The construct of cortical type retains the topology of the systematic variation of the cortex and forms the basis for a predictive Structural Model, which has successfully linked cortical variation to the laminar pattern and strength of cortical connections, the continuum of plasticity and stability of areas, the regularities in the distribution of classical and novel markers, and the preferential vulnerability of limbic areas to neurodegenerative and psychiatric diseases. The origin of cortical types has been recently traced to cortical development, and helps explain the variability of diseases with an onset in ontogeny."
HELEN BARBAS,Serial Pathways from Primate Prefrontal Cortex to Autonomic Areas May Influence Emotional Expression,"BACKGROUND: Experiencing emotions engages high-order orbitofrontal and medial prefrontal areas, and expressing emotions involves low-level autonomic structures and peripheral organs. How is information from the cortex transmitted to the periphery? We used two parallel approaches to map simultaneously multiple pathways to determine if hypothalamic autonomic centres are a key link for orbitofrontal areas and medial prefrontal areas, which have been associated with emotional processes, as well as low-level spinal and brainstem autonomic structures. The latter innervate peripheral autonomic organs, whose activity is markedly increased during emotional arousal. RESULTS: We first determined if pathways linking the orbitofrontal cortex with the hypothalamus overlapped with projection neurons directed to the intermediolateral column of the spinal cord, with the aid of neural tracers injected in these disparate structures. We found that axons from orbitofrontal and medial prefrontal cortices converged in the hypothalamus with neurons projecting to brainstem and spinal autonomic centers, linking the highest with the lowest levels of the neuraxis. Using a parallel approach, we injected bidirectional tracers in the lateral hypothalamic area, an autonomic center, to label simultaneously cortical pathways leading to the hypothalamus, as well as hypothalamic axons projecting to low-level brainstem and spinal autonomic centers. We found densely distributed projection neurons in medial prefrontal and orbitofrontal cortices leading to the hypothalamus, as well as hypothalamic axonal terminations in several brainstem structures and the intermediolateral column of the spinal cord, which innervate peripheral autonomic organs. We then provided direct evidence that axons from medial prefrontal cortex synapse with hypothalamic neurons, terminating as large boutons, comparable in size to the highly efficient thalamocortical system. The interlinked orbitofrontal, medial prefrontal areas and hypothalamic autonomic centers were also connected with the amygdala. CONCLUSIONS: Descending pathways from orbitofrontal and medial prefrontal cortices, which are also linked with the amygdala, provide the means for speedy influence of the prefrontal cortex on the autonomic system, in processes underlying appreciation and expression of emotions."
HELEN BARBAS,Parallel Organization of Contralateral and Ipsilateral Prefrontal Cortical Projections in the Rhesus Monkey,"BACKGROUND: The neocortical commissures have a fundamental role in functional integration across the cerebral hemispheres. We investigated whether commissural projections in prefrontal cortices are organized according to the same or different rules as those within the same hemisphere, by quantitatively comparing density, topography, and laminar origin of contralateral and ipsilateral projections, labeled after unilateral injection of retrograde tracers in prefrontal areas. RESULTS: Commissural projection neurons constituted less than one third of the ipsilateral. Nevertheless, projections from the two hemispheres were strongly correlated in topography and relative density. We investigated to what extent the distribution of contralateral projections depended on: (a) geographic proximity of projection areas to the area homotopic to the injection site; (b) the structural type of the linked areas, based on the number and neuronal density of their layers. Although both measures were good predictors, structural type was a comparatively stronger determinant of the relative distribution and density of projections. Ipsilateral projection neurons were distributed in the superficial (II-III) and deep (V-VI) layers, in proportions that varied across areas. In contrast, contralateral projection neurons were found mostly in the superficial layers, but still showed a gradient in their distribution within cortical layers that correlated significantly with cortical type, but not with geographic proximity to the homotopic area. CONCLUSION: The organization of ipsilateral and contralateral prefrontal projections is similar in topography and relative density, differing only by higher overall density and more widespread laminar origin of ipsilateral than contralateral projections. The projections on both sides are highly correlated with the structural architecture of the linked areas, and their remarkable organization is likely established by punctuated development of distinct cortical types. The preponderance of contralateral projections from layer III may be traced to the late development of the callosal system, whose function may be compromised in diseases that have their root late in ontogeny."
HELEN BARBAS,"Sleep spindles in primates: modelling the effects of distinct laminar thalamocortical connectivity in core, matrix, and reticular thalamic circuits","Sleep spindles are associated with the beginning of deep sleep and memory consolidation and are disrupted in schizophrenia and autism. In primates, distinct core and matrix thalamocortical (TC) circuits regulate sleep-spindle activity, through communications that are filtered by the inhibitory thalamic reticular nucleus (TRN) however, little is known about typical TC network interactions and the mechanisms that are disrupted in brain disorders. We developed a primate-specific, circuit-based TC computational model with distinct core and matrix loops that can simulate sleep spindles. We implemented novel multilevel cortical and thalamic mixing, and included local thalamic inhibitory interneurons, and direct layer 5 projections of variable density to TRN and thalamus to investigate the functional consequences of different ratios of core and matrix node connectivity contribution to spindle dynamics. Our simulations showed that spindle power in primates can be modulated based on the level of cortical feedback, thalamic inhibition, and engagement of model core vs. matrix, with the latter having a greater role in spindle dynamics. The study of the distinct spatial and temporal dynamics of core-, matrix-, and mix-generated sleep spindles establishes a framework to study disruption of TC circuit balance underlying deficits in sleep and attentional gating seen in autism and schizophrenia."
HELEN BARBAS,Parallel trends in cortical gray and white matter architecture and connections in primates allow fine study of pathways in humans and reveal network disruptions in autism,"Noninvasive imaging and tractography methods have yielded information on broad communication networks but lack resolution to delineate intralaminar cortical and subcortical pathways in humans. An important unanswered question is whether we can use the wealth of precise information on pathways from monkeys to understand connections in humans. We addressed this question within a theoretical framework of systematic cortical variation and used identical high-resolution methods to compare the architecture of cortical gray matter and the white matter beneath, which gives rise to short- and long-distance pathways in humans and rhesus monkeys. We used the prefrontal cortex as a model system because of its key role in attention, emotions, and executive function, which are processes often affected in brain diseases. We found striking parallels and consistent trends in the gray and white matter architecture in humans and monkeys and between the architecture and actual connections mapped with neural tracers in rhesus monkeys and, by extension, in humans. Using the novel architectonic portrait as a base, we found significant changes in pathways between nearby prefrontal and distant areas in autism. Our findings reveal that a theoretical framework allows study of normal neural communication in humans at high resolution and specific disruptions in diverse psychiatric and neurodegenerative diseases."
HELEN BARBAS,Anatomy and computational modeling of networks underlying cognitive-emotional interaction,"The classical dichotomy between cognition and emotion equated the first with rationality or logic and the second with irrational behaviors. The idea that cognition and emotion are separable, antagonistic forces competing for dominance of mind has been hard to displace despite abundant evidence to the contrary. For instance, it is now known that a pathological absence of emotion leads to profound impairment of decision making. Behavioral observations of this kind are corroborated at the mechanistic level: neuroanatomical studies reveal that brain areas typically described as underlying either cognitive or emotional processes are linked in ways that imply complex interactions that do not resemble a simple mutual antagonism. Instead, physiological studies and network simulations suggest that top-down signals from prefrontal cortex realize ""cognitive control"" in part by either suppressing or promoting emotional responses controlled by the amygdala, in a way that facilitates adaptation to changing task demands. Behavioral, anatomical, and physiological data suggest that emotion and cognition are equal partners in enabling a continuum or matrix of flexible behaviors that are subserved by multiple brain regions acting in concert. Here we focus on neuroanatomical data that highlight circuitry that structures cognitive-emotional interactions by directly or indirectly linking prefrontal areas with the amygdala. We also present an initial computational circuit model, based on anatomical, physiological, and behavioral data to explicitly frame the learning and performance mechanisms by which cognition and emotion interact to achieve flexible behavior."
HELEN BARBAS,The intercalated nuclear complex of the primate amygdala,"The organization of the inhibitory intercalated cell masses (IM) of the primate amygdala is largely unknown despite their key role in emotional processes. We studied the structural, topographic, neurochemical and intrinsic connectional features of IM neurons in the rhesus monkey brain. We found that the intercalated neurons are not confined to discrete cell clusters, but form a neuronal net that is interposed between the basal nuclei and extends to the dorsally located anterior, central, and medial nuclei of the amygdala. Unlike the IM in rodents, which are prominent in the anterior half of the amygdala, the primate inhibitory net stretched throughout the antero-posterior axis of the amygdala, and was most prominent in the central and posterior extent of the amygdala. There were two morphologic types of intercalated neurons: spiny and aspiny. Spiny neurons were the most abundant; their somata were small or medium size, round or elongated, and their dendritic trees were round or bipolar, depending on location. The aspiny neurons were on average slightly larger and had varicose dendrites with no spines. There were three non-overlapping neurochemical populations of IM neurons, in descending order of abundance: (1) Spiny neurons that were positive for the striatal associated dopamine- and cAMP-regulated phosphoprotein (DARPP-32+); (2) Aspiny neurons that expressed the calcium-binding protein calbindin (CB+); and (3) Aspiny neurons that expressed nitric oxide synthase (NOS+). The unique combinations of structural and neurochemical features of the three classes of IM neurons suggest different physiological properties and function. The three types of IM neurons were intermingled and likely interconnected in distinct ways, and were innervated by intrinsic neurons within the amygdala, or by external sources, in pathways that underlie fear conditioning and anxiety."
HELEN BARBAS,"Parallel development of chromatin patterns, neuron morphology, and connections: potential for disruption in autism","The phenotype of neurons and their connections depend on complex genetic and epigenetic processes that regulate the expression of genes in the nucleus during development and throughout life. Here we examined the distribution of nuclear chromatin patters in relation to the epigenetic landscape, phenotype and connections of neurons with a focus on the primate cerebral cortex. We show that nuclear patterns of chromatin in cortical neurons are related to neuron size and cortical connections. Moreover, we point to evidence that reveals an orderly sequence of events during development, linking chromatin and gene expression patterns, neuron morphology, function, and connections across cortical areas and layers. Based on this synthesis, we posit that systematic studies of changes in chromatin patterns and epigenetic marks across cortical areas will provide novel insights on the development and evolution of cortical networks, and their disruption in connectivity disorders of developmental origin, like autism. Achieving this requires embedding and interpreting genetic, transcriptional, and epigenetic studies within a framework that takes into consideration distinct types of neurons, local circuit interactions, and interareal pathways. These features vary systematically across cortical areas in parallel with laminar structure and are differentially affected in disorders. Finally, based on evidence that autism-associated genetic polymorphisms are especially prominent in excitatory neurons and connectivity disruption affects mostly limbic cortices, we employ this systematic approach to propose novel, targeted studies of projection neurons in limbic areas to elucidate the emergence and time-course of developmental disruptions in autism."
HELEN BARBAS,Visual attention deficits in schizophrenia can arise from inhibitory dysfunction in thalamus or cortex,"Schizophrenia is associated with diverse cognitive deficits, including disorders of attention-related oculomotor behavior. At the structural level, schizophrenia is associated with abnormal inhibitory control in the circuit linking cortex and thalamus. We developed a spiking neural network model that demonstrates how dysfunctional inhibition can degrade attentive gaze control. Our model revealed that perturbations of two functionally distinct classes of cortical inhibitory neurons, or of the inhibitory thalamic reticular nucleus, disrupted processing vital for sustained attention to a stimulus, leading to distractibility. Because perturbation at each circuit node led to comparable but qualitatively distinct disruptions in attentive tracking or fixation, our findings support the search for new eye movement metrics that may index distinct underlying neural defects. Moreover, because the cortico-thalamic circuit is a common motif across sensory, association, and motor systems, the model and extensions can be broadly applied to study normal function and the neural bases of other cognitive deficits in schizophrenia."
HELEN BARBAS,Posterior orbitofrontal and anterior cingulate pathways to the amygdala target inhibitory and excitatory systems with opposite functions,"The bidirectional dialogue of the primate posterior orbitofrontal cortex (pOFC) with the amygdala is essential in cognitive–emotional functions. The pOFC also sends a uniquely one-way excitatory pathway to the amygdalar inhibitory intercalated masses (IM), which inhibit the medial part of the central amygdalar nucleus (CeM). Inhibition of IM has the opposite effect, allowing amygdalar activation of autonomic structures and emotional arousal. Using multiple labeling approaches to identify pathways and their postsynaptic sites in the amygdala in rhesus monkeys, we found that the anterior cingulate cortex innervated mostly the basolateral and CeM amygdalar nuclei, poised to activate CeM for autonomic arousal. By contrast, a pathway from pOFC to IM exceeded all other pathways to the amygdala by density and size and proportion of large and efficient terminals. Moreover, whereas pOFC terminals in IM innervated each of the three distinct classes of inhibitory neurons, most targeted neurons expressing dopamine- and cAMP-regulated phosphoprotein (DARPP-32+), known to be modulated by dopamine. The predominant pOFC innervation of DARPP-32+ neurons suggests activation of IM and inhibition of CeM, resulting in modulated autonomic function. By contrast, inhibition of DARPP-32 neurons in IM by high dopamine levels disinhibits CeM and triggers autonomic arousal. The findings provide a mechanism to help explain how a strong pOFC pathway, which is poised to moderate activity of CeM, through IM, can be undermined by the high level of dopamine during stress, resulting in collapse of potent inhibitory mechanisms in the amygdala and heightened autonomic drive, as seen in chronic anxiety disorders."
HELEN BARBAS,Opposite development of short- and long-range anterior cingulate pathways in autism,"Autism has been linked with the changes in brain connectivity that disrupt neural communication, especially involving frontal networks. Pathological changes in white matter are evident in adults with autism, particularly affecting axons below the anterior cingulate cortices (ACC). It is still unknown whether axon pathology appears early or late in development and whether it changes or not from childhood through adulthood. To address these questions, we examined typical and pathological development of about 1 million axons in post-mortem brains of children, adolescents, and adults with and without autism (ages 3–67 years). We used high-resolution microscopy to systematically sample and study quantitatively the fine structure of myelinated axons in the white matter below ACC. We provide novel evidence of changes in the density, size and trajectories of ACC axons in typical postnatal development from childhood through adulthood. Against the normal profile of axon development, our data revealed lower density of myelinated axons that connect ACC with neighboring cortices in children with autism. In the course of development the proportion of thin axons, which form short-range pathways, increased significantly in individuals with autism, but remained flat in controls. In contrast, the relative proportion of thick axons, which form long-range pathways, increased from childhood to adulthood in the control group, but decreased in autism. Our findings provide a timeline for profound changes in axon density and thickness below ACC that affect axon physiology in a direction suggesting bias in short over distant neural communication in autism. Importantly, measures of axon density, myelination, and orientation provide white matter anisotropy/diffusivity estimates at the level of single axons. The structural template established can be used to compare with measures obtained from imaging in living subjects, and guide analysis of functional and structural imaging data from humans for comparison with pathological states."
HELEN BARBAS,Parallel Driving and Modulatory Pathways Link the Prefrontal Cortex and Thalamus,"Pathways linking the thalamus and cortex mediate our daily shifts from states of attention to quiet rest, or sleep, yet little is known about their architecture in high-order neural systems associated with cognition, emotion and action. We provide novel evidence for neurochemical and synaptic specificity of two complementary circuits linking one such system, the prefrontal cortex with the ventral anterior thalamic nucleus in primates. One circuit originated from the neurochemical group of parvalbumin-positive thalamic neurons and projected focally through large terminals to the middle cortical layers, resembling 'drivers' in sensory pathways. Parvalbumin thalamic neurons, in turn, were innervated by small 'modulatory' type cortical terminals, forming asymmetric (presumed excitatory) synapses at thalamic sites enriched with the specialized metabotropic glutamate receptors. A second circuit had a complementary organization: it originated from the neurochemical group of calbindin-positive thalamic neurons and terminated through small 'modulatory' terminals over long distances in the superficial prefrontal layers. Calbindin thalamic neurons, in turn, were innervated by prefrontal axons through small and large terminals that formed asymmetric synapses preferentially at sites with ionotropic glutamate receptors, consistent with a driving pathway. The largely parallel thalamo-cortical pathways terminated among distinct and laminar-specific neurochemical classes of inhibitory neurons that differ markedly in inhibitory control. The balance of activation of these parallel circuits that link a high-order association cortex with the thalamus may allow shifts to different states of consciousness, in processes that are disrupted in psychiatric diseases."
HELEN BARBAS,A Proposal for a Coordinated Effort for the Determination of Brainwide Neuroanatomical Connectivity in Model Organisms at a Mesoscopic Scale,"In this era of complete genomes, our knowledge of neuroanatomical circuitry remains surprisingly sparse. Such knowledge is critical, however, for both basic and clinical research into brain function. Here we advocate for a concerted effort to fill this gap, through systematic, experimental mapping of neural circuits at a mesoscopic scale of resolution suitable for comprehensive, brainwide coverage, using injections of tracers or viral vectors. We detail the scientific and medical rationale and briefly review existing knowledge and experimental techniques. We define a set of desiderata, including brainwide coverage; validated and extensible experimental techniques suitable for standardization and automation; centralized, open-access data repository; compatibility with existing resources; and tractability with current informatics technology. We discuss a hypothetical but tractable plan for mouse, additional efforts for the macaque, and technique development for human. We estimate that the mouse connectivity project could be completed within five years with a comparatively modest budget."
HELEN BARBAS,Mirror trends of plasticity and stability indicators in primate prefrontal cortex,"Research on plasticity markers in the cerebral cortex has largely focused on their timing of expression and role in shaping circuits during critical and normal periods. By contrast, little attention has been focused on the spatial dimension of plasticity–stability across cortical areas. The rationale for this analysis is based on the systematic variation in cortical structure that parallels functional specialization and raises the possibility of varying levels of plasticity. Here, we investigated in adult rhesus monkeys the expression of markers related to synaptic plasticity or stability in prefrontal limbic and eulaminate areas that vary in laminar structure. Our findings revealed that limbic areas are impoverished in three markers of stability: intracortical myelin, the lectin Wisteria floribunda agglutinin, which labels perineuronal nets, and parvalbumin, which is expressed in a class of strong inhibitory neurons. By contrast, prefrontal limbic areas were enriched in the enzyme calcium/calmodulin-dependent protein kinase II (CaMKII), known to enhance plasticity. Eulaminate areas have more elaborate laminar architecture than limbic areas and showed the opposite trend: they were enriched in markers of stability and had lower expression of the plasticity-related marker CaMKII. The expression of glial fibrillary acidic protein (GFAP), a marker of activated astrocytes, was also higher in limbic areas, suggesting that cellular stress correlates with the rate of circuit reshaping. Elevated markers of plasticity may endow limbic areas with flexibility necessary for learning and memory within an affective context, but may also render them vulnerable to abnormal structural changes, as seen in neurologic and psychiatric diseases."
HELEN BARBAS,Role of mechanical factors in the morphology of the primate cerebral cortex,"The convoluted cortex of primates is instantly recognizable in its principal morphologic features, yet puzzling in its complex finer structure. Various hypotheses have been proposed about the mechanisms of its formation. Based on the analysis of databases of quantitative architectonic and connection data for primate prefrontal cortices, we offer support for the hypothesis that tension exerted by corticocortical connections is a significant factor in shaping the cerebral cortical landscape. Moreover, forces generated by cortical folding influence laminar morphology, and appear to have a previously unsuspected impact on cellular migration during cortical development. The evidence for a significant role of mechanical factors in cortical morphology opens the possibility of constructing computational models of cortical develoment based on physical principles. Such models are particularly relevant for understanding the relationship of cortical morphology to the connectivity of normal brains, and structurally altered brains in diseases of developmental origin, such as schizophrenia and autism. Synopsis How are the characteristic folds of primate brains formed? New answers to this old question support the idea that folding occurs as nerve fibers connect the brain's different surface regions. The fibers pull together regions that are strongly connected, while unconnected regions drift apart. Furthermore, as the brain develops before birth and its surface expands, folding may affect the passage of new neurons into different regions, influencing the brain's architecture. These findings underscore the role of mechanical forces in shaping the normal brain. Moreover, the findings suggest that changes in brain shape in developmental diseases, such as schizophrenia and autism, may result from changes in the connections."
HELEN BARBAS,"Sleep spindles in primates: Modeling the effects of distinct laminar thalamocortical connectivity in core, matrix, and reticular thalamic circuits","Sleep spindles are associated with the beginning of deep sleep and memory consolidation and are disrupted in schizophrenia and autism. In primates, distinct core and matrix thalamocortical (TC) circuits regulate sleep spindle activity through communications that are filtered by the inhibitory thalamic reticular nucleus (TRN); however, little is known about typical TC network interactions and the mechanisms that are disrupted in brain disorders. We developed a primate-specific, circuit-based TC computational model with distinct core and matrix loops that can simulate sleep spindles. We implemented novel multilevel cortical and thalamic mixing, and included local thalamic inhibitory interneurons, and direct layer 5 projections of variable density to TRN and thalamus to investigate the functional consequences of different ratios of core and matrix node connectivity contribution to spindle dynamics. Our simulations showed that spindle power in primates can be modulated based on the level of cortical feedback, thalamic inhibition, and engagement of model core versus matrix, with the latter having a greater role in spindle dynamics. The study of the distinct spatial and temporal dynamics of core-, matrix-, and mix-generated sleep spindles establishes a framework to study disruption of TC circuit balance underlying deficits in sleep and attentional gating seen in autism and schizophrenia."
HELEN BARBAS,Subgenual and hippocampal pathways in amygdala are set to balance affect and context processing,"The amygdala, hippocampus, and subgenual cortex area 25 (A25) are engaged in complex cognitive-emotional processes. Yet pathway interactions from hippocampus and A25 with postsynaptic sites in amygdala remain largely unknown. In rhesus monkeys of both sexes, we studied with neural tracers how pathways from A25 and hippocampus interface with excitatory and inhibitory microcircuits in amygdala at multiple scales. We found that both hippocampus and A25 innervate distinct as well as overlapping sites of the basolateral (BL) amygdalar nucleus. Unique hippocampal pathways heavily innervated the intrinsic paralaminar basolateral nucleus, which is associated with plasticity. In contrast, orbital A25 preferentially innervated another intrinsic network, the intercalated masses, an inhibitory reticulum that gates amygdalar autonomic output and inhibits fear-related behaviors. Finally, using high-resolution confocal and electron microscopy (EM), we found that among inhibitory postsynaptic targets in BL, both hippocampal and A25 pathways preferentially formed synapses with calretinin (CR) neurons, which are known for disinhibition and may enhance excitatory drive in the amygdala. Among other inhibitory postsynaptic sites, A25 pathways innervated the powerful parvalbumin (PV) neurons which may flexibly regulate the gain of neuronal assemblies in the BL that affect the internal state. In contrast, hippocampal pathways innervated calbindin (CB) inhibitory neurons, which modulate specific excitatory inputs for processing context and learning correct associations. Common and unique patterns of innervation in amygdala by hippocampus and A25 have implications for how complex cognitive and emotional processes may be selectively disrupted in psychiatric disorders.SIGNIFICANCE STATEMENT The hippocampus, subgenual A25, and amygdala are associated with learning, memory, and emotions. We found that A25 is poised to affect diverse amygdalar processes, from emotional expression to fear learning by innervating the basal complex and the intrinsic intercalated masses. Hippocampal pathways uniquely interacted with another intrinsic amygdalar nucleus which is associated with plasticity, suggesting flexible processing of signals in context for learning. In the basolateral (BL) amygdala, which has a role in fear learning, both hippocampal and A25 interacted preferentially with disinhibitory neurons, suggesting a boost in excitation. The two pathways diverged in innervating other classes of inhibitory neurons, suggesting circuit specificities that could become perturbed in psychiatric diseases."
PAUL TORNETTA,Internet Versus Mailed Questionnaires: A Randomized Comparison (2),"BACKGROUND Low response rates among surgeons can threaten the validity of surveys. Internet technologies may reduce the time, effort, and financial resources needed to conduct surveys. OBJECTIVE We investigated whether using Web-based technology could increase the response rates to an international survey. METHODS We solicited opinions from the 442 surgeon–members of the Orthopaedic Trauma Association regarding the treatment of femoral neck fractures. We developed a self-administered questionnaire after conducting a literature review, focus groups, and key informant interviews, for which we used sampling to redundancy techniques. We administered an Internet version of the questionnaire on a Web site, as well as a paper version, which looked similar to the Internet version and which had identical content. Only those in our sample could access the Web site. We alternately assigned the participants to receive the survey by mail (n=221) or an email invitation to participate on the Internet (n=221). Non-respondents in the mail arm received up to three additional copies of the survey, while non-respondents in the Internet arm received up to three additional requests, including a final mailed copy. All participants in the Internet arm had an opportunity to request an emailed Portable Document Format (PDF) version. RESULTS The Internet arm demonstrated a lower response rate (99/221, 45%) than the mail questionnaire arm (129/221, 58%) (absolute difference 13%, 95% confidence interval 4%-22%, P<0.01). CONCLUSIONS. Our Internet-based survey to surgeons resulted in a significantly lower response rate than a traditional mailed survey. Researchers should not assume that the widespread availability and potential ease of Internet-based surveys will translate into higher response rates."
PAUL TORNETTA,The Orthopaedic Trauma Literature: An Evaluation of Statistically Significant Findings in Orthopaedic Trauma Randomized Trials,"BACKGROUND.: Evidence-based medicine posits that health care research is founded upon clinically important differences in patient centered outcomes. Statistically significant differences between two treatments may not necessarily reflect a clinically important difference. We aimed to quantify the sample sizes and magnitude of treatment effects in a review of orthopaedic randomized trials with statistically significant findings. METHODS: We conducted a comprehensive search (PubMed, Cochrane) for all randomized controlled trials between 1/1/95 to 12/31/04. Eligible studies include those that focused upon orthopaedic trauma. Baseline characteristics and treatment effects were abstracted by two reviewers. Briefly, for continuous outcome measures (ie functional scores), we calculated effect sizes (mean difference/standard deviation). Dichotomous variables (ie infection, nonunion) were summarized as absolute risk differences and relative risk reductions (RRR). Effect sizes >0.80 and RRRs>50% were defined as large effects. Using regression analysis we examined the association between the total number of outcome events and treatment effect (dichotomous outcomes). RESULTS: Our search yielded 433 randomized controlled trials (RCTs), of which 76 RCTs with statistically significant findings on 184 outcomes (122 continuous/62 dichotomous outcomes) met study eligibility criteria. The mean effect size across studies with continuous outcome variables was 1.7 (95% confidence interval: 1.43–1.97). For dichotomous outcomes, the mean risk difference was 30% (95%confidence interval:24%–36%) and the mean relative risk reduction was 61% (95% confidence interval: 55%–66%; range: 0%–97%). Fewer numbers of total outcome events in studies was strongly correlated with increasing magnitude of the treatment effect (Pearson's R = -0.70, p < 0.01). When adjusted for sample size, the number of outcome events revealed an independent association with the size of the treatment effect (Odds ratio = 50, 95% confidence interval: 3.0–1000, p = 0.006). CONCLUSION: Our review suggests that statistically significant results in orthopaedic trials have the following implications-1) On average large risk reductions are reported 2) Large treatment effects (>50% relative risk reduction) are correlated with few number of total outcome events. Readers should interpret the results of such small trials with these issues in mind."
PAUL TORNETTA,Communicating Study Results to Our Patients: Which Way Is Best?,"Before we are able to communicate evidence and evidence results to patients we must first be familiar with the common ways by which results may be presented to our patients. We describe five approaches (relative risk, risk reduction, odds ratio, absolute risk difference and number needed to treat) of transforming the results of an orthopaedic study for communication with patients."
PAUL TORNETTA,The Age of Evidence-Based Orthopaedics,
PAUL TORNETTA,Low Intensity Pulsed Ultrasonography for Fractures: Systematic Review of Randomised Controlled Trials,"Objective To determine the efficacy of low intensity pulsed ultrasonography for healing of fractures. Design Systematic review of randomised controlled trials. Data sources Electronic literature search without language restrictions of CINAHL, Embase, Medline, HealthSTAR, and the Cochrane Central Registry of Controlled Trials, from inception of the database to 10 September 2008. Review methods Eligible studies were randomised controlled trials that enrolled patients with any kind of fracture and randomly assigned them to low intensity pulsed ultrasonography or to a control group. Two reviewers independently agreed on eligibility; three reviewers independently assessed methodological quality and extracted outcome data. All outcomes were included and meta-analyses done when possible. Results 13 randomised trials, of which five assessed outcomes of importance to patients, were included. Moderate quality evidence from one trial found no effect of low intensity pulsed ultrasonography on functional recovery from conservatively managed fresh clavicle fractures; whereas low quality evidence from three trials suggests benefit in non-operatively managed fresh fractures (faster radiographic healing time mean 36.9%, 95% confidence interval 25.6% to 46.0%). A single trial provided moderate quality evidence suggesting no effect of low intensity pulsed ultrasonography on return to function among non-operatively treated stress fractures. Three trials provided very low quality evidence for accelerated functional improvement after distraction osteogenesis. One trial provided low quality evidence for a benefit of low intensity pulsed ultrasonography in accelerating healing of established non-unions managed with bone graft. Four trials provided low quality evidence for acceleration of healing of operatively managed fresh fractures. Conclusion Evidence for the effect of low intensity pulsed ultrasonography on healing of fractures is moderate to very low in quality and provides conflicting results. Although overall results are promising, establishing the role of low intensity pulsed ultrasonography in the management of fractures requires large, blinded trials, directly addressing patient important outcomes such as return to function."
ROBERT A BROWN,Rocaglates induce gain-of-function alterations to eIF4A and eIF4F,"Rocaglates are a diverse family of biologically active molecules that have gained tremendous interest in recent years due to their promising activities in pre-clinical cancer studies. As a result, this family of compounds has been significantly expanded through the development of efficient synthetic schemes. However, it is unknown whether all of the members of the rocaglate family act through similar mechanisms of action. Here, we present a comprehensive study comparing the biological activities of >200 rocaglates to better understand how the presence of different chemical entities influences their biological activities. Through this, we find that most rocaglates preferentially repress the translation of mRNAs containing purine-rich 5' leaders, but certain rocaglates lack this bias in translation repression. We also uncover an aspect of rocaglate mechanism of action in which the pool of translationally active eIF4F is diminished due to the sequestration of the complex onto RNA."
ROBERT A BROWN,"Bostonia: v. 17, no. 1-9",
ROBERT A BROWN,"BMQ : Boston medical quarterly: v. 16, no. 1-4",
ROBERT A BROWN,"Canvass: a crowd-sourced, natural-product screening library for exploring biological space",
ROBERT A BROWN,"Bostonia: v. 12, no. 1-6, 8-10",
ROBERT A BROWN,EIF4A supports an oncogenic translation program in pancreatic ductal adenocarcinoma.,"Pancreatic ductal adenocarcinoma (PDA) is a lethal malignancy with limited treatment options. Although metabolic reprogramming is a hallmark of many cancers, including PDA, previous attempts to target metabolic changes therapeutically have been stymied by drug toxicity and tumour cell plasticity. Here, we show that PDA cells engage an eIF4F-dependent translation program that supports redox and central carbon metabolism. Inhibition of the eIF4F subunit, eIF4A, using the synthetic rocaglate CR-1-31-B (CR-31) reduced the viability of PDA organoids relative to their normal counterparts. In vivo, CR-31 suppresses tumour growth and extends survival of genetically-engineered murine models of PDA. Surprisingly, inhibition of eIF4A also induces glutamine reductive carboxylation. As a consequence, combined targeting of eIF4A and glutaminase activity more effectively inhibits PDA cell growth both in vitro and in vivo. Overall, our work demonstrates the importance of eIF4A in translational control of pancreatic tumour metabolism and as a therapeutic target against PDA."
ROBERT A BROWN,"Bostonia: v. 15, no. 1-10",
ROBERT A BROWN,"First Sagittarius A* Event Horizon Telescope results. II. EHT and multiwavelength observations, data processing, and calibration","We present Event Horizon Telescope (EHT) 1.3 mm measurements of the radio source located at the position of the supermassive black hole Sagittarius A* (Sgr A*), collected during the 2017 April 5–11 campaign. The observations were carried out with eight facilities at six locations across the globe. Novel calibration methods are employed to account for Sgr A*'s flux variability. The majority of the 1.3 mm emission arises from horizon scales, where intrinsic structural source variability is detected on timescales of minutes to hours. The effects of interstellar scattering on the image and its variability are found to be subdominant to intrinsic source structure. The calibrated visibility amplitudes, particularly the locations of the visibility minima, are broadly consistent with a blurred ring with a diameter of ∼50 μas, as determined in later works in this series. Contemporaneous multiwavelength monitoring of Sgr A* was performed at 22, 43, and 86 GHz and at near-infrared and X-ray wavelengths. Several X-ray flares from Sgr A* are detected by Chandra, one at low significance jointly with Swift on 2017 April 7 and the other at higher significance jointly with NuSTAR on 2017 April 11. The brighter April 11 flare is not observed simultaneously by the EHT but is followed by a significant increase in millimeter flux variability immediately after the X-ray outburst, indicating a likely connection in the emission physics near the event horizon. We compare Sgr A*’s broadband flux during the EHT campaign to its historical spectral energy distribution and find that both the quiescent emission and flare emission are consistent with its long-term behavior."
ROBERT A BROWN,Buildout and integration of an automated high-throughput CLIA laboratory for SARS-CoV-2 testing on a large urban campus,"In 2019, the first cases of SARS-CoV-2 were detected in Wuhan, China, and by early 2020 the first cases were identified in the United States. SARS-CoV-2 infections increased in the US causing many states to implement stay-at-home orders and additional safety precautions to mitigate potential outbreaks. As policies changed throughout the pandemic and restrictions lifted, there was an increase in demand for COVID-19 testing which was costly, difficult to obtain, or had long turn-around times. Some academic institutions, including Boston University (BU), created an on-campus COVID-19 screening protocol as part of a plan for the safe return of students, faculty, and staff to campus with the option for in-person classes. At BU, we put together an automated high-throughput clinical testing laboratory with the capacity to run 45,000 individual tests weekly by Fall of 2020, with a purpose-built clinical testing laboratory, a multiplexed reverse transcription PCR (RT-qPCR) test, robotic instrumentation, and trained staff. There were many challenges including supply chain issues for personal protective equipment and testing materials in addition to equipment that were in high demand. The BU Clinical Testing Laboratory (CTL) was operational at the start of Fall 2020 and performed over 1 million SARS-CoV-2 PCR tests during the 2020-2021 academic year."
ROBERT A BROWN,First Sagittarius A* Event Horizon Telescope results. I. The shadow of the supermassive black hole in the center of the Milky Way,"We present the first Event Horizon Telescope (EHT) observations of Sagittarius A* (Sgr A*), the Galactic center source associated with a supermassive black hole. These observations were conducted in 2017 using a global interferometric array of eight telescopes operating at a wavelength of λ = 1.3 mm. The EHT data resolve a compact emission region with intrahour variability. A variety of imaging and modeling analyses all support an image that is dominated by a bright, thick ring with a diameter of 51.8 ± 2.3 μas (68% credible interval). The ring has modest azimuthal brightness asymmetry and a comparatively dim interior. Using a large suite of numerical simulations, we demonstrate that the EHT images of Sgr A* are consistent with the expected appearance of a Kerr black hole with mass ∼4 × 106 M ⊙, which is inferred to exist at this location based on previous infrared observations of individual stellar orbits, as well as maser proper-motion studies. Our model comparisons disfavor scenarios where the black hole is viewed at high inclination (i &gt; 50°), as well as nonspinning black holes and those with retrograde accretion disks. Our results provide direct evidence for the presence of a supermassive black hole at the center of the Milky Way, and for the first time we connect the predictions from dynamical measurements of stellar orbits on scales of 103–105 gravitational radii to event-horizon-scale images and variability. Furthermore, a comparison with the EHT results for the supermassive black hole M87* shows consistency with the predictions of general relativity spanning over three orders of magnitude in central mass."
ROBERT A BROWN,The long-baseline neutrino experiment: exploring fundamental symmetries of the universe,"The preponderance of matter over antimatter in the early Universe, the dynamics of the supernova bursts that produced the heavy elements necessary for life and whether protons eventually decay --- these mysteries at the forefront of particle physics and astrophysics are key to understanding the early evolution of our Universe, its current state and its eventual fate. The Long-Baseline Neutrino Experiment (LBNE) represents an extensively developed plan for a world-class experiment dedicated to addressing these questions. LBNE is conceived around three central components: (1) a new, high-intensity neutrino source generated from a megawatt-class proton accelerator at Fermi National Accelerator Laboratory, (2) a near neutrino detector just downstream of the source, and (3) a massive liquid argon time-projection chamber deployed as a far detector deep underground at the Sanford Underground Research Facility. This facility, located at the site of the former Homestake Mine in Lead, South Dakota, is approximately 1,300 km from the neutrino source at Fermilab -- a distance (baseline) that delivers optimal sensitivity to neutrino charge-parity symmetry violation and mass ordering effects. This ambitious yet cost-effective design incorporates scalability and flexibility and can accommodate a variety of upgrades and contributions. With its exceptional combination of experimental configuration, technical capabilities, and potential for transformative discoveries, LBNE promises to be a vital facility for the field of particle physics worldwide, providing physicists from around the globe with opportunities to collaborate in a twenty to thirty year program of exciting science. In this document we provide a comprehensive overview of LBNE's scientific objectives, its place in the landscape of neutrino physics worldwide, the technologies it will incorporate and the capabilities it will possess."
ROBERT A BROWN,"The medical student: v. 5, no. 1-8",
ELIZABETH A STIER,Phase IIA Trial of 1% Topical Cidofovir for Treatment of High-Grade Perianal Squamous Intraepithelial Neoplasia in HIV-Infected Men and Women (AMC046),
ROBERT KAUFMANN,The multiplier effect of endogenous technical change accelerates the transition to photovoltaic cells,"Abating emissions of carbon dioxide depends in part on how quickly the levelized cost of electricity (LCOE) from photovoltaic cells (PV) achieves grid parity without policy interventions. Reaching this threshold is accelerated by learning by doing, which reduces the LCOE generated by PV and increases installed capacity. Here, we expand previous estimates of unidirectional (Capacity → Price) learning curves to include the effect of prices on capacity by estimating a cointegrating vector autoregression (CVAR) model, which can capture a simultaneous relation between price and capacity. Results indicate that the simultaneous relation between price and capacity increases the estimate for the learning rate and creates a multiplier that amplifies static effects by nearly a factor of ten. This same multiplier effect enhances the ability of policies, such as a carbon tax, to lower the costs of PV, increase capacity, and lower carbon emissions. Together, these results suggest that grid parity is closer than indicated by unidirectional learning curves."
ROBERT KAUFMANN,Sensitivity of global pasturelands to climate variation,"Pasturelands are globally extensive, sensitive to climate, and support livestock production systems that provide an essential source of food in many parts of the world. In this paper, we integrate information from remote sensing, global climate, and land use databases to improve understanding of the resilience and resistance of this ecologically vulnerable and societally-critical land use. To characterize the effect of climate on pastureland productivity, we analyze the relationship between satellite-derived vegetation index and gridded precipitation datasets at 1 to 6-month time lags. To account for the effects of different production systems, we stratify our analysis by agroecological zone and by rangeland-based versus mixed crop-livestock system. Results show that 14.5% of global pasturelands experienced statistically significant greening or browning trends over the 15-year study period, with the majority of these locations showing greening. In arid ecosystems, precipitation and lagged vegetation index anomalies explain up to 69% of variation in vegetation productivity in both crop-livestock and rangeland-based production systems. Livestock production systems in Australia are least resistant to contemporaneous and short-term precipitation anomalies, while arid livestock production systems in Latin America are least resilient to short-term vegetation greenness anomalies. More generally, large swaths of semi-arid global pasturelands show substantial sensitivity to variation in precipitation, and hence, are vulnerable to climate change. Because many arid regions of the world are projected to experience decreased total precipitation and increased precipitation variability in the coming decades, improved understanding regarding the sensitivity of pasturelands to the joint effects of climate change and production system is required to support sustainable land management in global pasturelands."
ROBERT KAUFMANN,An investigation of the envelope of motion and its effect on final canal shape,"The purpose of the study was to investigate the action of the reamer through its envelope of motion and how this action affects the final results in shaped root canals. Root canal models, made from casting acrylic, were used to examine the effects of modification of reamer curvature, and rate of rotation and withdrawal on the post operative shapes of forty curved and forty straight simulated root canals. The canals were shaped according to standard B.U. technique, with modifications made In the area of reamer manipulation. Reamers were used with three types of curvatures, Straight (unmodified), Minor Curvature, and Major Curvature. These reamers were used with two types of motions: (1) The classic Envelope of Motion CE.M.) In which the reamer Is passively placed, and then given one ful I turn while belng rapidly withdrawn from the canal. (2) Rapid rotation and slow withdrawal In which the reamer Is ""spun"" while being slowly withdrawn from the canal. Each of the canals was photographed and measured, graphed and examined pre- and post-operatively using an optical comparator. The shaped canals were then graded and a composite grade was given for each combination of reamer curvature and motion type. [TRUNCATED]"
ROBERT KAUFMANN,The effect of climate change on electricity expenditures in Massachusetts,"Climate change affects consumer expenditures by altering the consumption of and price for electricity. Previous analyses focus solely on the former, which implicitly assumes that climate-induced changes in consumption do not affect price. But this assumption is untenable because a shift in demand alters quantity and price at equilibrium. Here we present the first empirical estimates for the effect of climate change on electricity prices. Translated through the merit order dispatch of existing capacity for generating electricity, climate-induced changes in daily and monthly patterns of electricity consumption cause non-linear changes in electricity prices. A 2°C increase in global mean temperature increases the prices for and consumption of electricity in Massachusetts USA, such that the average household’s annual expenditures on electricity increase by about 12 percent. Commercial customers incur a 9 percent increase. These increases are caused largely by higher prices for electricity, whose impacts on expenditures are 1.3 and 3.6 fold larger than changes in residential and commercial consumption, respectively. This suggests that previous empirical studies understate the effects of climate change on electricity expenditures and that policy may be needed to ensure that the market generates investments in peaking capacity to satisfy climate-driven changes in summer-time consumption."
ROBERT KAUFMANN,Effects of warming temperatures on winning times in the Boston Marathon,"It is not known whether global warming will affect winning times in endurance events, and counterbalance improvements in race performances that have occurred over the past century. We examined a time series (1933–2004) from the Boston Marathon to test for an effect of warming on winning times by men and women. We found that warmer temperatures and headwinds on the day of the race slow winning times. However, 1.6°C warming in annual temperatures in Boston between 1933 and 2004 did not consistently slow winning times because of high variability in temperatures on race day. Starting times for the race changed to earlier in the day beginning in 2006, making it difficult to anticipate effects of future warming on winning times. However, our models indicate that if race starting times had not changed and average race day temperatures had warmed by 0.058°C/yr, a high-end estimate, we would have had a 95% chance of detecting a consistent slowing of winning marathon times by 2100. If average race day temperatures had warmed by 0.028°C/yr, a mid-range estimate, we would have had a 64% chance of detecting a consistent slowing of winning times by 2100."
ROBERT KAUFMANN,Data for: Understanding glacial cycles: a multivariate disequilibrium approach,"We find a consistent relation between orbital geometry and components of the climate system by returning to Milankovitch’s original hypothesis and focusing on the well-established physical concepts of an equilibrium state, disequilibrium from that state, and adjustment towards equilibrium. These mechanisms imply that the state of the climate system at any time depends on; (1) the state of the climate system in the previous period, (2) the degree to which this previous state is out-of-equilibrium with orbital geometry, and (3) the rate at which the climate system adjusts towards equilibrium. We evaluate this explanation by running experiments with a statistical model of climate that explicitly represents equilibria among variables and their movements towards equilibrium. Results indicate that; (1) skipped obliquity/precession beats are an artifact of ignoring adjustments towards an equilibrium state, (2) accounting for equilibrium and adjustments to equilibrium can account for all phases of the glacial cycle, and (3) glacial cycles are generated by adjustments to equilibrium relations between orbital geometry and climate and among components of the climate system. Together, these results suggest a new approach to understanding glacial cycles that is based on models which include a rich set of equilibria and adjustments to equilibria for a full suite of climate variables simulated over long periods."
ROBERT KAUFMANN,"Data and code for: Feedbacks among electric vehicle adoption, charging, and the cost and installation of rooftop solar","Identifying feedback loops in consumer behaviors is important to develop policies to accentuate desired behavior. Here, we use Granger causality to provide empirical evidence for feedback loops among four important components of a low-carbon economy. One loop includes the cost of installing rooftop solar (Cost) and the installation of rooftop solar (PV); this loop is likely generated by learning by doing and reductions in the levelized cost of electricity. The second includes the purchase of electric vehicles (EV) and the installation of rooftop solar that is likely created by environmental complementarity. Finally, we address whether installing charging stations enhances the purchase of electric vehicles and vice versa; surprisingly, there is no evidence for a causal relation in either direction. Together, these results suggest ways to modify existing policy in ways that could trigger the Cost ↔PV ↔EV feedback loops and accelerate the transition to carbon free technologies."
ROBERT KAUFMANN,Energy price volatility affects decisions to purchase energy using capital: motor vehicles,"I investigate how energy price volatility affects consumer decisions to purchase durable, energy using capital, motor vehicles. A cointegrating vector autoregression (CVAR) model indicates that; (1) high price volatility reduces the negative effect of motor gasoline prices on monthly purchases of motor vehicles, (2) consumers consider passenger cars to be an inferior good and this drives the substitution of light trucks for passenger cars, (3) the own price elasticity of demand for vehicles is about -0.3, which is consistent with estimates used to justify The Safer Affordable Fuel-Efficient (SAFE) Vehicles Rule for Models Years 2021-2026 Passenger Cars and Light Trucks. Simulating the CVAR model indicates that implementing an energy tax rapidly or during a period of low price volatility has a greater effect on motor vehicle sales than implementing the same tax slowly or during a period of high price volatility. Together these results suggest that more research should focus on how energy price volatility affects consumer decisions to purchase energy using capital."
ROBERT KAUFMANN,Social and Environmental Events Disrupt the Relation Between Motor Gasoline Prices and Market Fundamentals,"After the Russian invasion of Ukraine, higher prices for motor gasoline re-ignited debate about price discovery; do market fundamentals largely determine motor gasoline prices. We evaluate the degree to which market fundamentals, as proxied by prices for crude oil, taxes on motor gasoline, inventories of crude oil and motor gasoline, and refinery utilization rates, can account for prices of motor gasoline. During most of the 2010 - 2022 sample, these proxies for market fundamentals account for weekly changes in the price for motor gasoline. But a saturation indicator procedure identifies weeks after the Russian invasion of Ukraine, Hurricane Harvey, and the start of the Covid-19 pandemic when prices for crude oil, taxes on motor gasoline, inventories of crude oil and motor gasoline, and refinery utilization rates fail to account for weekly changes in the price of motor gasoline in a statistically significant manner. During these weeks, the price of motor gasoline rises by up to $0.25 per gallon relative to that implied by market fundamentals, which increased expenditures on motor gasoline $10 to $72 per licensed driver."
ROBERT KAUFMANN,Data and code for: Testing Hypotheses About Glacial Dynamics and the Stage 11 Paradox Using a Statistical Model of Paleo-Climate,"To test hypotheses about glacial dynamics, the Mid-Brunhes event, and the stage 11 paradox, we evaluate the ability of a statistical model to simulate climate during the previous ~800,000 years. Throughout this period, the model simulates the timing and magnitude of glacial cycles, including the saw-tooth pattern in which ice accumulates gradually and ablates rapidly, without nonlinearities or threshold effects. This suggests that nonlinearities and/or threshold effects do not play a critical role in glacial cycles. Furthermore, model accuracy throughout the previous ~800,000 years suggest that changes in glacial cycles associated with the Mid-Brunhes event, which occurs near the division between the out-of-sample period and the in-sample period, are not caused by changes in the dynamics of the climate system. Conversely, poor model performance during MIS stage 11 and Termination V is consistent with arguments that the ‘stage 11 paradox’ represents a mismatch between orbital geometry and climate. Statistical orderings of simulation errors indicate that periods of reduced accuracy start with significant reductions in the model’s ability to simulate carbon dioxide, non-sea-salt sodium, and non-sea-salt calcium. Their importance suggests that the stage 11 paradox is generated by changes in atmospheric and/or oceanic circulation that affect ocean ventilation of carbon dioxide."
ROBERT KAUFMANN,Sub-continental-scale carbon stocks of individual trees in African drylands,"The distribution of dryland trees and their density, cover, size, mass and carbon content are not well known at sub-continental to continental scales1-14. This information is important for ecological protection, carbon accounting, climate mitigation and restoration efforts of dryland ecosystems15-18. We assessed more than 9.9 billion trees derived from more than 300,000 satellite images, covering semi-arid sub-Saharan Africa north of the Equator. We attributed wood, foliage and root carbon to every tree in the 0-1,000 mm year-1 rainfall zone by coupling field data19, machine learning20-22, satellite data and high-performance computing. Average carbon stocks of individual trees ranged from 0.54 Mg C ha-1 and 63 kg C tree-1 in the arid zone to 3.7 Mg C ha-1 and 98 kg tree-1 in the sub-humid zone. Overall, we estimated the total carbon for our study area to be 0.84 (±19.8%) Pg C. Comparisons with 14 previous TRENDY numerical simulation studies23 for our area found that the density and carbon stocks of scattered trees have been underestimated by three models and overestimated by 11 models, respectively. This benchmarking can help understand the carbon cycle and address concerns about land degradation24-29. We make available a linked database of wood mass, foliage mass, root mass and carbon stock of each tree for scientists, policymakers, dryland-restoration practitioners and farmers, who can use it to estimate farmland tree carbon stocks from tablets or laptops."
ALISA BOKULICH,Towards a taxanomy of the model-ladenness of data,"Model-data symbiosis is the view that there is an interdependent and mutually beneficial relationship between data and models, whereby models are not only data-laden, but data are also model-laden or model filtered. In this paper I elaborate and defend the second, more controversial, component of the symbiosis view. In particular, I construct a preliminary taxonomy of the different ways in which theoretical and simulation models are used in the production of data sets. These include data conversion, data correction, data interpolation, data scaling, data fusion, data assimilation, and synthetic data. Each is defined and briefly illustrated with an example from the geosciences. I argue that model-filtered data are typically more accurate and reliable than the so-called raw data, and hence beneficially serve the epistemic aims of science. By illuminating the methods by which raw data are turned into scientifically useful data sets, this taxonomy provides a foundation for developing a more adequate philosophy of data."
ALISA BOKULICH,"Calibration, coherence, and consilience in radiometric measures of geologic time","In 2012 the Geological Time Scale, which sets the temporal framework for studying the timing and tempo of all major geological, biological, and climatic events in Earth's history, had one-quarter of its boundaries moved in a wide-spread revision of radiometric dates. The philosophy of metrology helps us understand this episode, and it, in turn, elucidates the notions of calibration, coherence, and consilience. I argue that coherence testing is a distinct activity preceding calibration and consilience, and highlight the value of discordant evidence and tradeoffs scientists face in calibration. The iterative nature of calibration, moreover, raises the problem of legacy data."
ALISA BOKULICH,Using models to correct data: paleodiversity and the fossil record,"Despite an enormous philosophical literature on models in science, surprisingly little has been written about data models and how they are constructed. In this paper, I examine the case of how paleodiversity data models are constructed from the fossil data. In particular, I show how paleontologists are using various model-based techniques to correct the data. Drawing on this research, I argue for the following related theses: first, the ‘purity’ of a data model is not a measure of its epistemic reliability. Instead it is the fidelity of the data that matters. Second, the fidelity of a data model in capturing the signal of interest is a matter of degree. Third, the fidelity of a data model can be improved ‘vicariously’, such as through the use of post hoc model-based correction techniques. And, fourth, data models, like theoretical models, should be assessed as adequate (or inadequate) for particular purposes."
ALISA BOKULICH,"Data models, representation and adequacy-for-purpose","We critically engage two traditional views of scientific data and outline a novel philosophical view that we call the pragmatic-representational (PR) view of data. On the PR view, data are representations that are the product of a process of inquiry, and they should be evaluated in terms of their adequacy or fitness for particular purposes. Some important implications of the PR view for data assessment, related to misrepresentation, context-sensitivity, and complementary use, are highlighted. The PR view provides insight into the common but little-discussed practices of iteratively reusing and repurposing data, which result in many datasets' having a phylogeny-an origin and complex evolutionary history-that is relevant to their evaluation and future use. We relate these insights to the open-data and data-rescue movements, and highlight several future avenues of research that build on the PR view of data."
ALISA BOKULICH,Representing and explaining: the eikonic conception of explanation,"The ontic conception of explanation, according to which explanations are ""full-bodied things in the world,"" is fundamentally misguided. I argue instead for what I call the eikonic conception, according to which explanations are the product of an epistemic activity involving representations of the phenomena to be explained. What is explained in the first instance is a particular conceptualization of the explanandum phenomenon, contextualized within a given research program or explanatory project. I conclude that this eikonic conception has a number of benefits, including making better sense of scientific practice and allowing for the full range of normative constraints on explanation."
ALISA BOKULICH,Fiction as a vehicle for truth: moving beyond the ontic conception,"Despite widespread evidence that fictional models play an explanatory role in science, resistance remains to the idea that fictions can explain. A central source of this resistance is a particular view about what explanations are, namely, the ontic conception of explanation. According to the ontic conception, explanations just are the concrete entities in the world. I argue this conception is ultimately incoherent and that even a weaker version of the ontic conception fails. Fictional models can succeed in offering genuine explanations by correctly capturing relevant patterns of counterfactual dependence and licensing correct inferences. Using the example of Newtonian force explanations of the tides, I show how, even in science, fiction can be a vehicle for truth."
ALISA BOKULICH,Losing sight of the forest for the Ψ: beyond the wavefunction hegemony,"Traditionally Ψ is used to stand in for both the mathematical wavefunction (the representation) and the quantum state (the thing in the world). This elision has been elevated to a metaphysical thesis by advocates of the view known as wavefunction realism. My aim in this paper is to challenge the hegemony of the wavefunction by calling attention to a little-known formulation of quantum theory that does not make use of the wavefunction in representing the quantum state. This approach, called Lagrangian quantum hydrodynamics (LQH), is not an approximation scheme, but rather a full alternative formulation of quantum theory. I argue that a careful consideration of alternative formalisms is an essential part of any realist project that attempts to read the ontology of a theory off of the mathematical formalism. In particular, I show that LQH undercuts the central presumption of wavefunction realism and falsifies the claim that one must represent the many-body quantum state as living in a 3n-dimensional configuration space. I conclude by briefly sketching three different realist approaches one could take toward LQH, and argue that both models of the quantum state should be admitted. When exploring quantum realism, regaining sight of the proverbial forest of quantum representations beyond the Ψ is just the first step."
ALISA BOKULICH,Are we in a sixth mass extinction? The challenges of answering and value of asking,
ALISA BOKULICH,"Kuhn’s ‘5th law of thermodynamics’: measurement, data, and anomalies","We reconstruct Kuhn’s philosophy of measurement and data paying special attention to what he calls the “fifth law of thermodynamics”. According to this ""law,"" there will always be discrepancies between experimental results and scientists’ prior expectations. The history of experiments to determine the values of the fundamental constants offers a striking illustration of Kuhn’s fifth law of thermodynamics, with no experiment giving quite the expected result. We highlight the synergy between Kuhn’s view and the systematic project of iteratively determining the value of physical constants, initiated by spectroscopist Raymond Birge, that was ongoing when Kuhn joined Berkeley in 1956. Our analysis sheds light on various underappreciated aspects of Kuhn’s thought, especially his notion of progress as improvement in measurement accuracy."
KONSTANTINOS SPILIOPOULOS,Normalization effects on shallow neural networks and related asymptotic expansions,"We consider shallow (single hidden layer) neural networks and characterize their performance when trained with stochastic gradient descent as the number of hidden units N and gradient descent steps grow to infinity. In particular, we investigate the effect of different scaling schemes, which lead to different normalizations of the neural network, on the network's statistical output, closing the gap between 1/√N and the mean-field 1/N normalization. We develop an asymptotic expansion for the neural network's statistical output pointwise with respect to the scaling parameter as the number of hidden units grows to infinity. Based on this expansion we demonstrate mathematically that to leading order in N there is no bias-variance trade off, in that both bias and variance (both explicitly characterized) decrease as the number of hidden units increases and time grows. In addition, we show that to leading order in N, the variance of the neural network's statistical output decays as the implied normalization by the scaling parameter approaches the mean field normalization. Numerical studies on the MNIST and CIFAR10 datasets show that test and train accuracy monotonically improve as the neural network's normalization gets closer to the mean field normalization."
KONSTANTINOS SPILIOPOULOS,Large deviations for interacting multiscale particle systems,"We consider a collection of weakly interacting diffusion processes moving in a two-scale locally periodic environment. We study the large deviations principle of the empirical distribution of the particles’ positions in the combined limit as the number of particles grow to infinity and the time-scale separation parameter goes to zero simultaneously. We make use of weak convergence methods providing a convenient representation for the large deviations rate function, which allow us to characterize the effective controlled mean field dynamics. In addition, we obtain in certain special cases equivalent representations for the large deviations rate function."
KONSTANTINOS SPILIOPOULOS,Selection of quasi-stationary states in the Navier-Stokes equation on the torus,"The two dimensional incompressible Navier–Stokes equation on 𝘋𝛿 := [0,2𝜋𝛿] × [0,2𝜋] with 𝛿 ≈ 1, periodic boundary conditions, and viscosity 0 < 𝑣 ≪ 1 is considered. Bars and dipoles, two explicitly given quasi-stationary states of the system, evolve on the time scale 𝒪(e‾ᵛᵗ) and have been shown to play a key role in its long-time evolution. Of particular interest is the role that δ plays in selecting which of these two states is observed. Recent numerical studies suggest that, after a transient period of rapid decay of the high Fourier modes, the bar state will be selected if 𝛿 ≠ 1, while the dipole will be selected if 𝛿 = 1. Our results support this claim and seek to mathematically formalize it. We consider the system in Fourier space, project it onto a center manifold consisting of the lowest eight Fourier modes, and use this as a model to study the selection of bars and dipoles. It is shown for this ODE model that the value of δ controls the behavior of the asymptotic ratio of the low modes, thus determining the likelihood of observing a bar state or dipole after an initial transient period. Moreover, in our model, for all 𝛿 ≈ 1, there is an initial time period in which the high modes decay at the rapid rate 𝒪(e‾ᵗ⧸ᵛ), while the low modes evolve at the slower 𝒪(e‾ᵛᵗ) rate. The results for the ODE model are proven using energy estimates and invariant manifolds and further supported by formal asymptotic expansions and numerics."
KONSTANTINOS SPILIOPOULOS,Metastability and exit problems for systems of stochastic reaction-diffusion equations,"In this paper we develop a metastability theory for a class of stochastic reaction-diffusion equations exposed to small multiplicative noise. We consider the case where the unperturbed reaction-diffusion equation features multiple asymptotically stable equilibria. When the system is exposed to small stochastic perturbations, it is likely to stay near one equilibrium for a long period of time, but will eventually transition to the neighborhood of another equilibrium. We are interested in studying the exit time from the full domain of attraction (in a function space) surrounding an equilibrium and therefore do not assume that the domain of attraction features uniform attraction to the equilibrium. This means that the boundary of the domain of attraction is allowed to contain saddles and limit cycles. Our method of proof is purely infinite dimensional, i.e., we do not go through finite dimensional approximations. In addition, we address the multiplicative noise case and we do not impose gradient type of assumptions on the nonlinearity. We prove large deviations logarithmic asymptotics for the exit time and for the exit shape, also characterizing the most probable set of shapes of solutions at the time of exit from the domain of attraction."
KONSTANTINOS SPILIOPOULOS,Large deviations and averaging for systems of slow–fast reaction–diffusion equations,"We study a large deviation principle for a system of stochastic reaction--diffusion equations (SRDEs) with a separation of fast and slow components and small noise in the slow component. The derivation of the large deviation principle is based on the weak convergence method in infinite dimensions, which results in studying averaging for controlled SRDEs. By appropriate choice of the parameters, the fast process and the associated control that arises from the weak convergence method decouple from each other. We show that in this decoupling case one can use the weak convergence method to characterize the limiting process via a ""viable pair"" that captures the limiting controlled dynamics and the effective invariant measure simultaneously. The characterization of the limit of the controlled slow-fast processes in terms of viable pair enables us to obtain a variational representation of the large deviation action functional. Due to the infinite--dimensional nature of our set--up, the proof of tightness as well as the analysis of the limit process and in particular the proof of the large deviations lower bound is considerably more delicate here than in the finite--dimensional situation. Smoothness properties of optimal controls in infinite dimensions (a necessary step for the large deviations lower bound) need to be established. We emphasize that many issues that are present in the infinite dimensional case, are completely absent in finite dimensions."
KONSTANTINOS SPILIOPOULOS,Rare event simulation via importance sampling for linear SPDE's,"The goal of this paper is to develop provably efficient importance sampling Monte Carlo methods for the estimation of rare events within the class of linear stochastic partial differential equations (SPDEs). We find that if a spectral gap of appropriate size exists, then one can identify a lower dimensional manifold where the rare event takes place. This allows one to build importance sampling changes of measures that perform provably well even pre-asymptotically (i.e. for small but non-zero size of the noise) without degrading in performance due to infinite dimensionality or due to long simulation time horizons. Simulation studies supplement and illustrate the theoretical results."
KONSTANTINOS SPILIOPOULOS,"Markov processes with spatial delay: path space characterization, occupation time and properties","In this paper, we study one-dimensional Markov processes with spatial delay. Since the seminal work of Feller, we know that virtually any one-dimensional, strong, homogeneous, continuous Markov process can be uniquely characterized via its infinitesimal generator and the generator’s domain of definition. Unlike standard diffusions like Brownian motion, processes with spatial delay spend positive time at a single point of space. Interestingly, the set of times that a delay process spends at its delay point is nowhere dense and forms a positive measure Cantor set. The domain of definition of the generator has restrictions involving second derivatives. In this paper we provide a pathwise characterization for processes with delay in terms of an SDE and an occupation time formula involving the symmetric local time. This characterization provides an explicit Doob–Meyer decomposition, demonstrating that such processes are semi-martingales and that all of stochastic calculus including Itô formula and Girsanov formula applies. We also establish an occupation time formula linking the time that the process spends at a delay point with its symmetric local time there. A physical example of a stochastic dynamical system with delay is lastly presented and analyzed."
KONSTANTINOS SPILIOPOULOS,Moderate deviations for systems of slow-fast stochastic reaction-diffusion equations,"The goal of this paper is to study the Moderate Deviation Principle (MDP) for a system of stochastic reaction-diffusion equations with a time-scale separation in slow and fast components and small noise in the slow component. Based on weak convergence methods in infinite dimensions and related stochastic control arguments, we obtain an exact form for the moderate deviations rate function in different regimes as the small noise and time-scale separation parameters vanish. Many issues that come up due to the infinite dimensionality of the problem are completely absent in their finite-dimensional counterpart. In comparison to corresponding Large Deviation Principles, the moderate deviation scaling necessitates a more delicate approach to establishing tightness and properly identifying the limiting behavior of the underlying controlled problem. The latter involves regularity properties of a solution of an associated elliptic Kolmogorov equation on Hilbert space along with a finite-dimensional approximation argument."
KONSTANTINOS SPILIOPOULOS,Mean field limits of particle-based stochastic reaction-diffusion models,"Particle-based stochastic reaction-diffusion (PBSRD) models are a popular approach for studying biological systems involving both noise in the reaction process and diffusive transport. In this work we derive coarse-grained deterministic partial integro-differential equation (PIDE) models that provide a mean field approximation to the volume reactivity PBSRD model, a model commonly used for studying cellular processes. We formulate a weak measure-valued stochastic process (MVSP) representation for the volume reactivity PBSRD model, demonstrating for a simplified but representative system that it is consistent with the commonly used Doi Fock Space representation of the corresponding forward equation. We then prove the convergence of the general volume reactivity model MVSP to the mean field PIDEs in the large-population (i.e. thermodynamic) limit."
KONSTANTINOS SPILIOPOULOS,How reaction-diffusion PDEs approximate the large-population limit of stochastic particle models,"Reaction-diffusion PDEs and particle-based stochastic reaction-diffusion (PBSRD) models are commonly-used approaches for modeling the spatial dynamics of chemical and biological systems. Standard reaction-diffusion PDE models ignore the underlying stochasticity of spatial transport and reactions, and are often described as appropriate in regimes where there are large numbers of particles in a system. Recent studies have proven the rigorous large-population limit of PBSRD models, showing the resulting mean-field models (MFM) correspond to non-local systems of partial-integro differential equations. In this work we explore the rigorous relationship between standard reaction-diffusion PDE models and the derived MFM. We prove that the former can be interpreted as an asymptotic approximation to the later in the limit that bimolecular reaction kernels are short-range and averaging. As the reactive interaction length scale approaches zero, we prove the MFMs converge at second order to standard reaction-diffusion PDE models. In proving this result we also establish local well-posedness of the MFM model in time for general systems, and global well-posedness for specific reaction systems and kernels. Finally, we illustrate the agreement and disagreement between the MFM, SM and the underlying particle model for several numerical examples."
KONSTANTINOS SPILIOPOULOS,Normalization effects on deep neural networks,
KONSTANTINOS SPILIOPOULOS,Moderate deviations for fully-coupled multiscale weakly interacting particle systems,"We consider a collection of fully coupled weakly interacting diffusion processes moving in a two-scale environment. We study the moderate deviations principle of the empirical distribution of the particles’ positions in the combined limit as the number of particles grow to infinity and the time-scale separation parameter goes to zero simultaneously. We make use of weak convergence methods, which provide a convenient representation for the moderate deviations rate function in terms of an effective mean field control problem. We rigorously obtain equivalent representations for the moderate deviations rate function in an appropriate “negative Sobolev” form, which is reminiscent of the large deviations rate function for the empirical measure of weakly interacting diffusions obtained in the 1987 seminal paper by Dawson-G¨artner. In the course of the proof we obtain related ergodic theorems and we rigorously study the regularity of Poisson type of equations associated to McKean-Vlasov problems, both of which are topics of independent interest."
KONSTANTINOS SPILIOPOULOS,Online adjoint methods for optimization of PDEs,"We present and mathematically analyze an online adjoint algorithm for the optimization of partial differential equations (PDEs). Traditional adjoint algorithms would typically solve a new adjoint PDE at each optimization iteration, which can be computationally costly. In contrast, an online adjoint algorithm updates the design variables in continuous-time and thus constantly makes progress towards minimizing the objective function. The online adjoint algorithm we consider is similar in spirit to the the pseudo-time-stepping, one-shot method which has been previously proposed. Motivated by the application of such methods to engineering problems, we mathematically study the convergence of the online adjoint algorithm. The online adjoint algorithm relies upon a time-relaxed adjoint PDE which provides an estimate of the direction of steepest descent. The algorithm updates this estimate continuously in time, and it asymptotically converges to the exact direction of steepest descent as 𝑡→∞. We rigorously prove that the online adjoint algorithm converges to a critical point of the objective function for optimizing the PDE. Under appropriate technical conditions, we also prove a convergence rate for the algorithm. A crucial step in the convergence proof is a multi-scale analysis of the coupled system for the forward PDE, adjoint PDE, and the gradient descent ODE for the design variables."
KONSTANTINOS SPILIOPOULOS,Moderate deviations for systems of slow–fast stochastic reaction–diffusion equations,
KONSTANTINOS SPILIOPOULOS,How reaction-diffusion PDEs approximate the large-population limit of stochastic particle models,"Reaction-diffusion PDEs and particle-based stochastic reaction-diffusion (PBSRD) models are commonly used approaches for modeling the spatial dynamics of chemical and biological systems. Standard reaction-diffusion PDE models ignore the underlying stochasticity of spatial transport and reactions and are often described as appropriate in regimes where there are large numbers of particles in a system. Recent studies have proven the rigorous large-population limit of PBSRD models, showing the resulting mean-field models (MFMs) correspond to nonlocal systems of partial-integro differential equations. In this work we explore the rigorous relationship between standard reaction-diffusion PDE models and the derived MFM. We prove that the former can be interpreted as an asymptotic approximation to the later in the limit that bimolecular reaction kernels are short-range and averaging. As the reactive interaction length scale approaches zero, we prove the MFMs converge at second order to standard reaction-diffusion PDE models. In proving this result we also establish local well-posedness of the MFM model in time for general systems and global well-posedness for specific reaction systems and kernels. Finally, we illustrate the agreement and disagreement between the MFM, SM, and underlying particle model for several numerical examples."
KONSTANTINOS SPILIOPOULOS,Rate of homogenization for fully-coupled McKean–Vlasov SDEs,"In this paper, we consider a fully-coupled slow–fast system of McKean–Vlasov stochastic differential equations with full dependence on the slow and fast component and on the law of the slow component and derive convergence rates to its homogenized limit. We do not make periodicity assumptions, but we impose conditions on the fast motion to guarantee ergodicity. In the course of the proof we obtain related ergodic theorems and we gain results on the regularity of Poisson type of equations and of the associated Cauchy problem on the Wasserstein space that are of independent interest."
KONSTANTINOS SPILIOPOULOS,Fluctuation analysis for particle-based stochastic reaction-diffusion models,"Recent works have derived and proven the large-population mean-field limit for several classes of particle-based stochastic reaction-diffusion (PBSRD) models. These limits correspond to systems of partial integral-differential equations (PIDEs) that generalize standard mass-action reaction-diffusion PDE models. In this work we derive and prove the next order fluctuation corrections to such limits, which we show satisfy systems of stochastic PIDEs with Gaussian noise. Numerical examples are presented to illustrate how including the fluctuation corrections can enable the accurate estimation of higher order statistics of the underlying PBSRD model."
KONSTANTINOS SPILIOPOULOS,Mean field limits of particle-based stochastic reaction-drift-diffusion models,
KONSTANTINOS SPILIOPOULOS,Importance sampling for stochastic reaction–diffusion equations in the moderate deviation regime,"We develop a provably efficient importance sampling scheme that estimates exit probabilities of solutions to small-noise stochastic reaction–diffusion equations from scaled neighborhoods of a stable equilibrium. The moderate deviation scaling allows for a local approximation of the nonlinear dynamics by their linearized version. In addition, we identify a finite-dimensional subspace where exits take place with high probability. Using stochastic control and variational methods we show that our scheme performs well both in the zero noise limit and pre-asymptotically. Simulation studies for stochastically perturbed bistable dynamics illustrate the theoretical results."
KONSTANTINOS SPILIOPOULOS,Kernel limit of recurrent neural networks trained on ergodic data sequences,
KONSTANTINOS SPILIOPOULOS,Quantitative fluctuation analysis of multiscale diffusion systems via Malliavin calculus,
GEORGE KOLLIOS,BoostMap: A Method for Efficient Approximate Similarity Rankings,"This paper introduces BoostMap, a method that can significantly reduce retrieval time in image and video database systems that employ computationally expensive distance measures, metric or non-metric. Database and query objects are embedded into a Euclidean space, in which similarities can be rapidly measured using a weighted Manhattan distance. Embedding construction is formulated as a machine learning task, where AdaBoost is used to combine many simple, 1D embeddings into a multidimensional embedding that preserves a significant amount of the proximity structure in the original space. Performance is evaluated in a hand pose estimation system, and a dynamic gesture recognition system, where the proposed method is used to retrieve approximate nearest neighbors under expensive image and video similarity measures. In both systems, BoostMap significantly increases efficiency, with minimal losses in accuracy. Moreover, the experiments indicate that BoostMap compares favorably with existing embedding methods that have been employed in computer vision and database applications, i.e., FastMap and Bourgain embeddings."
GEORGE KOLLIOS,Extraction and Clustering of Motion Trajectories in Video,"A system is described that tracks moving objects in a video dataset so as to extract a representation of the objects' 3D trajectories. The system then finds hierarchical clusters of similar trajectories in the video dataset. Objects' motion trajectories are extracted via an EKF formulation that provides each object's 3D trajectory up to a constant factor. To increase accuracy when occlusions occur, multiple tracking hypotheses are followed. For trajectory-based clustering and retrieval, a modified version of edit distance, called longest common subsequence (LCSS) is employed. Similarities are computed between projections of trajectories on coordinate axes. Trajectories are grouped based, using an agglomerative clustering algorithm. To check the validity of the approach, experiments using real data were performed."
GEORGE KOLLIOS,Discovering Clusters in Motion Time-Series Data,"A new approach is proposed for clustering time-series data. The approach can be used to discover groupings of similar object motions that were observed in a video collection. A finite mixture of hidden Markov models (HMMs) is fitted to the motion data using the expectation-maximization (EM) framework. Previous approaches for HMM-based clustering employ a k-means formulation, where each sequence is assigned to only a single HMM. In contrast, the formulation presented in this paper allows each sequence to belong to more than a single HMM with some probability, and the hard decision about the sequence class membership can be deferred until a later time when such a decision is required. Experiments with simulated data demonstrate the benefit of using this EM-based approach when there is more ""overlap"" in the processes generating the data. Experiments with real data show the promising potential of HMM-based motion clustering in a number of applications."
GEORGE KOLLIOS,Authenticated Index Structures for Aggregation Queries in Outsourced Databases,"In an outsourced database system the data owner publishes information through a number of remote, untrusted servers with the goal of enabling clients to access and query the data more efficiently. As clients cannot trust servers, query authentication is an essential component in any outsourced database system. Clients should be given the capability to verify that the answers provided by the servers are correct with respect to the actual data published by the owner. While existing work provides authentication techniques for selection and projection queries, there is a lack of techniques for authenticating aggregation queries. This article introduces the first known authenticated index structures for aggregation queries. First, we design an index that features good performance characteristics for static environments, where few or no updates occur to the data. Then, we extend these ideas and propose more involved structures for the dynamic case, where the database owner is allowed to update the data arbitrarily. Our structures feature excellent average case performance for authenticating queries with multiple aggregate attributes and multiple selection predicates. We also implement working prototypes of the proposed techniques and experimentally validate the correctness of our ideas."
GEORGE KOLLIOS,Discovering Frequent Poly-Regions in DNA Sequences,"The problem of discovering frequent arrangements of regions of high occurrence of one or more items of a given alphabet in a sequence is studied, and two efficient approaches are proposed to solve it. The first approach is entropy-based and uses an existing recursive segmentation technique to split the input sequence into a set of homogeneous segments. The key idea of the second approach is to use a set of sliding windows over the sequence. Each sliding window keeps a set of statistics of a sequence segment that mainly includes the number of occurrences of each item in that segment. Combining these statistics efficiently yields the complete set of regions of high occurrence of the items of the given alphabet. After identifying these regions, the sequence is converted to a sequence of labeled intervals (each one corresponding to a region). An efficient algorithm for mining frequent arrangements of temporal intervals on a single sequence is applied on the converted sequence to discover frequently occurring arrangements of these regions. The proposed algorithms are tested on various DNA sequences producing results with significant biological meaning."
GEORGE KOLLIOS,Query-Sensitive Embeddings,"A common problem in many types of databases is retrieving the most similar matches to a query object. Finding those matches in a large database can be too slow to be practical, especially in domains where objects are compared using computationally expensive similarity (or distance) measures. This paper proposes a novel method for approximate nearest neighbor retrieval in such spaces. Our method is embedding-based, meaning that it constructs a function that maps objects into a real vector space. The mapping preserves a large amount of the proximity structure of the original space, and it can be used to rapidly obtain a short list of likely matches to the query. The main novelty of our method is that it constructs, together with the embedding, a query-sensitive distance measure that should be used when measuring distances in the vector space. The term ""query-sensitive"" means that the distance measure changes depending on the current query object. We report experiments with an image database of handwritten digits, and a time-series database. In both cases, the proposed method outperforms existing state-of-the-art embedding methods, meaning that it provides significantly better trade-offs between efficiency and retrieval accuracy."
GEORGE KOLLIOS,The Cache Inference Problem and its Application to Content and Request Routing,"In many networked applications, independent caching agents cooperate by servicing each other's miss streams, without revealing the operational details of the caching mechanisms they employ. Inference of such details could be instrumental for many other processes. For example, it could be used for optimized forwarding (or routing) of one's own miss stream (or content) to available proxy caches, or for making cache-aware resource management decisions. In this paper, we introduce the Cache Inference Problem (CIP) as that of inferring the characteristics of a caching agent, given the miss stream of that agent. While CIP is insolvable in its most general form, there are special cases of practical importance in which it is, including when the request stream follows an Independent Reference Model (IRM) with generalized power-law (GPL) demand distribution. To that end, we design two basic ""litmus"" tests that are able to detect LFU and LRU replacement policies, the effective size of the cache and of the object universe, and the skewness of the GPL demand for objects. Using extensive experiments under synthetic as well as real traces, we show that our methods infer such characteristics accurately and quite efficiently, and that they remain robust even when the IRM/GPL assumptions do not hold, and even when the underlying replacement policies are not ""pure"" LFU or LRU. We exemplify the value of our inference framework by considering example applications."
GEORGE KOLLIOS,Authenticated Index Structures for Outsourced Database Systems,"In outsourced database (ODB) systems the database owner publishes its data through a number of remote servers, with the goal of enabling clients at the edge of the network to access and query the data more efficiently. As servers might be untrusted or can be compromised, query authentication becomes an essential component of ODB systems. Existing solutions for this problem concentrate mostly on static scenarios and are based on idealistic properties for certain cryptographic primitives. In this work, first we define a variety of essential and practical cost metrics associated with ODB systems. Then, we analytically evaluate a number of different approaches, in search for a solution that best leverages all metrics. Most importantly, we look at solutions that can handle dynamic scenarios, where owners periodically update the data residing at the servers. Finally, we discuss query freshness, a new dimension in data authentication that has not been explored before. A comprehensive experimental evaluation of the proposed and existing approaches is used to validate the analytical models and verify our claims. Our findings exhibit that the proposed solutions improve performance substantially over existing approaches, both for static and dynamic environments."
GEORGE KOLLIOS,Nearest-neighbor Queries in Probabilistic Graphs,"Large probabilistic graphs arise in various domains spanning from social networks to biological and communication networks. An important query in these graphs is the k nearest-neighbor query, which involves finding and reporting the k closest nodes to a specific node. This query assumes the existence of a measure of the ""proximity"" or the ""distance"" between any two nodes in the graph. To that end, we propose various novel distance functions that extend well known notions of classical graph theory, such as shortest paths and random walks. We argue that many meaningful distance functions are computationally intractable to compute exactly. Thus, in order to process nearest-neighbor queries, we resort to Monte Carlo sampling and exploit novel graph-transformation ideas and pruning opportunities. In our extensive experimental analysis, we explore the trade-offs of our approximation algorithms and demonstrate that they scale well on real-world probabilistic graphs with tens of millions of edges."
GEORGE KOLLIOS,Generalized Methods for Discovering Frequent Poly-Regions in DNA,"The problem of discovering frequent poly-regions (i.e. regions of high occurrence of a set of items or patterns of a given alphabet) in a sequence is studied, and three efficient approaches are proposed to solve it. The first one is entropy-based and applies a recursive segmentation technique that produces a set of candidate segments which may potentially lead to a poly-region. The key idea of the second approach is the use of a set of sliding windows over the sequence. Each sliding window covers a sequence segment and keeps a set of statistics that mainly include the number of occurrences of each item or pattern in that segment. Combining these statistics efficiently yields the complete set of poly-regions in the given sequence. The third approach applies a technique based on the majority vote, achieving linear running time with a minimal number of false negatives. After identifying the poly-regions, the sequence is converted to a sequence of labeled intervals (each one corresponding to a poly-region). An efficient algorithm for mining frequent arrangements of intervals is applied to the converted sequence to discover frequently occurring arrangements of poly-regions in different parts of DNA, including coding regions. The proposed algorithms are tested on various DNA sequences producing results of significant biological meaning."
GEORGE KOLLIOS,Learning Euclidean Embeddings for Indexing and Classification,"BoostMap is a recently proposed method for efficient approximate nearest neighbor retrieval in arbitrary non-Euclidean spaces with computationally expensive and possibly non-metric distance measures. Database and query objects are embedded into a Euclidean space, in which similarities can be rapidly measured using a weighted Manhattan distance. The key idea is formulating embedding construction as a machine learning task, where AdaBoost is used to combine simple, 1D embeddings into a multidimensional embedding that preserves a large amount of the proximity structure of the original space. This paper demonstrates that, using the machine learning formulation of BoostMap, we can optimize embeddings for indexing and classification, in ways that are not possible with existing alternatives for constructive embeddings, and without additional costs in retrieval time. First, we show how to construct embeddings that are query-sensitive, in the sense that they yield a different distance measure for different queries, so as to improve nearest neighbor retrieval accuracy for each query. Second, we show how to optimize embeddings for nearest neighbor classification tasks, by tuning them to approximate a parameter space distance measure, instead of the original feature-based distance measure."
GEORGE KOLLIOS,The price of privacy: a performance study of confidential virtual machines for database systems,"Confidential virtual machines (CVM) use trusted hardware to encrypt data being processed in memory to prevent unauthorized access. Applications can be migrated to CVM without changes, i.e., lift and shift, to handle sensitive workloads securely in public clouds. AMD Secure Encrypted Virtualization (SEV) is one of the prominent technologies that provides hardware support for CVM. In this paper, we investigate various system operations, including CPU, memory, and disk and network I/O, to understand the performance overheads of SEV-supported CVMs. Our findings indicate that memory and I/O-intensive workloads can incur significant overhead. We then study the performance implications of running unmodified database applications, specifically Cock-roachDB, on CVMs by examining typical data access patterns of OLTP and OLAP workloads. A notable performance overhead of up to 18% is observed for TPC-C workload running on multinode database clusters, and an overhead of up to 13% is observed for TPC-H workload running on single-node database instances. The non-negligible overhead suggests the potential and necessity for database optimizations with respect to CVM, particularly for time-sensitive workloads. We offer a glimpse of the effect that CVM overhead can have in query planning using a simple join query: the optimal join algorithm becomes suboptimal on CVM, along with discussion of potential optimizations for reducing CVM overhead in the realm of database applications."
CATHERINE M KLAPPERICH,A paperfluidic platform to detect Neisseria gonorrhoeae in clinical samples,"Globally, the microbe Neisseria gonorrhoeae (NG) causes 106 million newly documented sexually transmitted infections each year. Once appropriately diagnosed, NG infections can be readily treated with antibiotics, but high-risk patients often do not return to the clinic for treatment if results are not provided at the point of care. A rapid, sensitive molecular diagnostic would help increase NG treatment and reduce the prevalence of this sexually transmitted disease. Here, we report on the design and development of a rapid, highly sensitive, paperfluidic device for point-of-care diagnosis of NG. The device integrates patient swab sample lysis, nucleic acid extraction, thermophilic helicase-dependent amplification (tHDA), an internal amplification control (NGIC), and visual lateral flow detection within an 80 min run time. Limits of NG detection for the NG/NGIC multiplex tHDA assay were determined within the device, and clinical performance was validated retroactively against qPCR-quantified patient samples in a proof-of-concept study. This paperfluidic diagnostic has a clinically relevant limit of detection of 500 NG cells per device with analytical sensitivity down to 10 NG cells per device. In triplicate testing of 40 total urethral and vaginal swab samples, the device had 95% overall sensitivity and 100% specificity, approaching current laboratory-based molecular NG diagnostics. This diagnostic platform could increase access to accurate NG diagnoses to those most in need."
CATHERINE M KLAPPERICH,Electrochemical strategy for low-cost viral detection,"Sexually transmitted infections, including the human immunodeficiency virus (HIV) and the human papillomavirus (HPV), disproportionally impact those in low-resource settings. Early diagnosis is essential for managing HIV. Similarly, HPV causes nearly all cases of cervical cancer, the majority (90%) of which occur in low-resource settings. Importantly, infection with HPV is six times more likely to progress to cervical cancer in women who are HIV-positive. An inexpensive, adaptable point-of-care test for viral infections would make screening for these viruses more accessible to a broader set of the population. Here, we report a novel, cost-effective electrochemical platform using gold leaf electrodes to detect clinically relevant viral loads. We have combined this platform with loop-mediated isothermal amplification and a CRISPR-based recognition assay to detect HPV. Lower limits of detection were demonstrated down to 104 total copies of input nucleic acids, which is a clinically relevant viral load for HPV DNA. Further, proof-of-concept experiments with cervical swab samples, extracted using standard extraction protocols, demonstrated that the strategy is extendable to complex human samples. This adaptable technology could be applied to detect any viral infection rapidly and cost-effectively."
CATHERINE M KLAPPERICH,Development and clinical validation of Iso-IMRS: a novel diagnostic assay for P. falciparum malaria,"In many countries targeting malaria elimination, persistent malaria infections can have parasite loads significantly below the lower limit of detection (LLOD) of standard diagnostic techniques, making them difficult to identify and treat. The most sensitive diagnostic methods involve amplification and detection of Plasmodium DNA by polymerase chain reaction (PCR), which requires expensive thermal cycling equipment and is difficult to deploy in resource-limited settings. Isothermal DNA amplification assays have been developed, but they require complex primer design, resulting in high nonspecific amplification, and show a decrease in sensitivity than PCR methods. Here, we have used a computational approach to design a novel isothermal amplification assay with a simple primer design to amplify P. falciparum DNA with analytical sensitivity comparable to PCR. We have identified short DNA sequences repeated throughout the parasite genome to be used as primers for DNA amplification and demonstrated that these primers can be used, without modification, to isothermally amplify P. falciparum parasite DNA via strand displacement amplification. Our novel assay shows a LLOD of ∼1 parasite/μL within a 30 min amplification time. The assay was demonstrated with clinical samples using patient blood and saliva. We further characterized the assay using direct amplicon next-generation sequencing and modified the assay to work with a visual readout. The technique developed here achieves similar analytical sensitivity to current gold standard PCR assays requiring a fraction of time and resources for PCR. This highly sensitive isothermal assay can be more easily adapted to field settings, making it a potentially useful tool for malaria elimination."
CATHERINE M KLAPPERICH,Amplicon residues in research laboratories masquerade as COVID-19 in surveillance tests,"Asymptomatic surveillance testing together with COVID-19-related research can lead to positive SARS-CoV-2 tests resulting not from true infections, but non-infectious, non-hazardous by-products of research (amplicons). Amplicons can be widespread and persistent in lab environments and can be difficult to distinguish for true infections. We discuss prevention and mitigation strategies."
CATHERINE M KLAPPERICH,Low concentration DNA extraction and recovery using a silica solid phase,"DNA extraction from clinical samples is commonly achieved with a silica solid phase extraction column in the presence of a chaotrope. Versions of these protocols have been adapted for point of care (POC) diagnostic devices in miniaturized platforms, but commercial kits require a high amount of input DNA. Thus, when the input clinical sample contains less than 1 μg of total DNA, the target-specific DNA recovery from most of these protocols is low without supplementing the sample with exogenous carrier DNA. In fact, many clinical samples used in the development of POC diagnostics often exhibit target DNA concentrations as low as 3 ng/mL. With the broader goal of improving the yield and efficiency of nucleic acid-based POC devices for dilute samples, we investigated both DNA adsorption and recovery from silica particles by using 1 pg– 1 μg of DNA with a set of adsorption and elution buffers ranging in pH and chaotropic presence. In terms of adsorption, we found that low pH and the presence of chaotropic guanidinium thiocyanate (GuSCN) enhanced DNA-silica adsorption. When eluting with a standard low-salt, high-pH buffer, > 70% of DNA was unrecoverable, except when DNA was initially adsorbed with 5 M GuSCN at pH 5.2. Unrecovered DNA was either not initially adsorbed or irreversibly bound on the silica surface. Recovery was improved when eluting with 95°C formamide and 1 M NaOH, which suggested that DNA-silica-chaotrope interactions are dominated by hydrophobic interactions and hydrogen bonding. While heated formamide and NaOH are non-ideal elution buffers for practical POC devices, the salient results are important for engineering a set of optimized reagents that could maximize nucleic acid recovery from a microfluidic DNA-silica-chaotrope system."
CATHERINE M KLAPPERICH,Enabling the development and deployment of next generation point-of-care diagnostics,
CATHERINE M KLAPPERICH,Low cost extraction and isothermal amplification of DNA for infectious diarrhea diagnosis,"In order to counter the common perception that molecular diagnostics are too complicated to work in low resource settings, we have performed a difficult sample preparation and DNA amplification protocol using instrumentation designed to be operated without wall or battery power. In this work we have combined a nearly electricity-free nucleic acid extraction process with an electricity-free isothermal amplification assay to detect the presence of Clostridium difficile (C. difficile) DNA in the stool of infected patients. We used helicase-dependent isothermal amplification (HDA) to amplify the DNA in a low-cost, thermoplastic reaction chip heated with a pair of commercially available toe warmers, while using a simple Styrofoam insulator. DNA was extracted from known positive and negative stool samples. The DNA extraction protocol utilized an air pressure driven solid phase extraction device run using a standard bicycle pump. The simple heater setup required no electricity or battery and was capable of maintaining the temperature at 65°C±2°C for 55 min, suitable for repeatable HDA amplification. Experiments were performed to explore the adaptability of the system for use in a range of ambient conditions. When compared to a traditional centrifuge extraction protocol and a laboratory thermocycler, this disposable, no power platform achieved approximately the same lower limit of detection (1.25×10−2 pg of C. difficile DNA) while requiring much less raw material and a fraction of the lab infrastructure and cost. This proof of concept study could greatly impact the accessibility of molecular assays for applications in global health."
CATHERINE M KLAPPERICH,Ultrasensitive CRISPR-based diagnostic for field-applicable detection of Plasmodium species in symptomatic and asymptomatic malaria,"Asymptomatic carriers of Plasmodium parasites hamper malaria control and eradication. Achieving malaria eradication requires ultrasensitive diagnostics for low parasite density infections (<100 parasites per microliter blood) that work in resource-limited settings (RLS). Sensitive point-of-care diagnostics are also lacking for nonfalciparum malaria, which is characterized by lower density infections and may require additional therapy for radical cure. Molecular methods, such as PCR, have high sensitivity and specificity, but remain high-complexity technologies impractical for RLS. Here we describe a CRISPR-based diagnostic for ultrasensitive detection and differentiation of Plasmodium falciparum, Plasmodium vivax, Plasmodium ovale, and Plasmodium malariae, using the nucleic acid detection platform SHERLOCK (specific high-sensitivity enzymatic reporter unlocking). We present a streamlined, field-applicable, diagnostic comprised of a 10-min SHERLOCK parasite rapid extraction protocol, followed by SHERLOCK for 60 min for Plasmodium species-specific detection via fluorescent or lateral flow strip readout. We optimized one-pot, lyophilized, isothermal assays with a simplified sample preparation method independent of nucleic acid extraction, and showed that these assays are capable of detection below two parasites per microliter blood, a limit of detection suggested by the World Health Organization. Our P. falciparum and P. vivax assays exhibited 100% sensitivity and specificity on clinical samples (5 P. falciparum and 10 P. vivax samples). This work establishes a field-applicable diagnostic for ultrasensitive detection of asymptomatic carriers as well as a rapid point-of-care clinical diagnostic for nonfalciparum malaria species and low parasite density P. falciparum infections."
CATHERINE M KLAPPERICH,A progesterone biosensor derived from microbial screening,"Bacteria are an enormous and largely untapped reservoir of biosensing proteins. We describe an approach to identify and isolate bacterial allosteric transcription factors (aTFs) that recognize a target analyte and to develop these TFs into biosensor devices. Our approach utilizes a combination of genomic screens and functional assays to identify and isolate biosensing TFs, and a quantum-dot Förster Resonance Energy Transfer (FRET) strategy for transducing analyte recognition into real-time quantitative measurements. We use this approach to identify a progesterone-sensing bacterial aTF and to develop this TF into an optical sensor for progesterone. The sensor detects progesterone in artificial urine with sufficient sensitivity and specificity for clinical use, while being compatible with an inexpensive and portable electronic reader for point-of-care applications. Our results provide proof-of-concept for a paradigm of microbially-derived biosensors adaptable to inexpensive, real-time sensor devices."
CATHERINE M KLAPPERICH,Buildout and integration of an automated high-throughput CLIA laboratory for SARS-CoV-2 testing on a large urban campus,"In 2019, the first cases of SARS-CoV-2 were detected in Wuhan, China, and by early 2020 the first cases were identified in the United States. SARS-CoV-2 infections increased in the US causing many states to implement stay-at-home orders and additional safety precautions to mitigate potential outbreaks. As policies changed throughout the pandemic and restrictions lifted, there was an increase in demand for COVID-19 testing which was costly, difficult to obtain, or had long turn-around times. Some academic institutions, including Boston University (BU), created an on-campus COVID-19 screening protocol as part of a plan for the safe return of students, faculty, and staff to campus with the option for in-person classes. At BU, we put together an automated high-throughput clinical testing laboratory with the capacity to run 45,000 individual tests weekly by Fall of 2020, with a purpose-built clinical testing laboratory, a multiplexed reverse transcription PCR (RT-qPCR) test, robotic instrumentation, and trained staff. There were many challenges including supply chain issues for personal protective equipment and testing materials in addition to equipment that were in high demand. The BU Clinical Testing Laboratory (CTL) was operational at the start of Fall 2020 and performed over 1 million SARS-CoV-2 PCR tests during the 2020-2021 academic year."
ANDREW F NEWMAN,Come together: firm boundaries and delegation,"We jointly study firm boundaries and the allocation of decision rights within them by confronting an incomplete-contracts model with data on vertical integration and delegation for thousands of firms around the world. Integration has an option value: it confers authority to delegate or centralize decision rights, depending on who can best solve problems that arise in the course of an uncertain production process. In line with the model’s predictions, we find that firms are more likely to integrate suppliers that produce more valuable inputs and operate in industries with more dispersed productivity, and that firms delegate more decisions to integrated suppliers that produce more valuable inputs and operate in more productive industries."
JOHN B BAILLIEUL,Optimal provision of distributed reserves under dynamic energy service preferences,"We propose and solve a stochastic dynamic programming (DP) problem addressing the optimal provision of regulation service reserves (RSR) by controlling dynamic demand preferences in smart buildings. A major contribution over past dynamic pricing work is that we pioneer the relaxation of static, uniformly distributed utility of demand. In this paper we model explicitly the dynamics of energy service preferences leading to a non-uniform and time varying probability distribution of demand utility. More explicitly, we model active and idle duty cycle appliances in a smart building as a closed queuing system with price-controlled arrival rates into the active appliance queue. Focusing on cooling appliances, we model the utility associated with the transition from idle to active as a non-uniform time varying function. We (i) derive an analytic characterization of the optimal policy and the differential cost function, and (ii) prove optimal policy monotonicity and value function convexity. These properties enable us to propose and implement a smart assisted value iteration (AVI) algorithm and an approximate DP (ADP) that exploits related functional approximations. Numerical results demonstrate the validity of the solution techniques and the computational advantage of the proposed ADP on realistic, large-state-space problems."
JOHN B BAILLIEUL,The first IEEE workshop on the Future of Research Curation and Research Reproducibility,"This report describes perspectives from the Workshop on the Future of Research Curation and Research Reproducibility that was collaboratively sponsored by the U.S. National Science Foundation (NSF) and IEEE (Institute of Electrical and Electronics Engineers) in November 2016. The workshop brought together stakeholders including researchers, funders, and notably, leading science, technology, engineering, and mathematics (STEM) publishers. The overarching objective was a deep dive into new kinds of research products and how the costs of creation and curation of these products can be sustainably borne by the agencies, publishers, and researcher communities that were represented by workshop participants."
JEFFREY FURMAN,Firm performance and state innovation funding: evidence from China’s Innofund program,"Can firms leverage public entrepreneurship investments to improve innovation and financial performance? Analysis of this question is frustrated by the difficulty of distinguishing treatment from selection effects. We take advantage of internal administrative data on applications to China’s Innofund program in order (a) to identify which application features are associated with higher chances of obtaining grants and (b) to evaluate the causal impact of receiving a grant on firm performance using a regression discontinuity (RD) design. With regards to grant receipt, we find that firms possessing observable merits and political connections are more likely to receive Innofund grants. We also find evidence of bureaucratic intervention, as applicants’ evaluation scores are non-randomly missing and that some firms whose scores did not meet funding standards nonetheless received grants. With regards to post-grant performance, we find that firms receiving high project evaluation scores and Innofund grants perform better than those that do not receive grants and have lower scores. These do not appear to be causal effects, however. Applying Fuzzy RD methods, we find no evidence that receiving an Innofund grant boosts survival, patenting, or venture funding. Our analysis demonstrates the value of administrative data for causal analysis and for uncovering evidence regarding the possibility that bureaucratic intervention affects firm and program outcomes."
JEFFREY FURMAN,"Automation, research technology, and researchers’ trajectories: evidence from computer science and electrical engineering","We examine how the introduction of a technology that automates research tasks influences the rate and type of researchers’ knowledge production. To do this, we leverage the unanticipated arrival of an automating motion-sensing research technology that occurred as a consequence of the introduction and subsequent hacking of the Microsoft Kinect system. To estimate whether this technology induces changes in the type of knowledge produced, we employ novel measures based on machine learning (topic modeling) techniques and traditional measures based on bibliometric indicators. Our analysis demonstrates that the shock associated with the introduction of Kinect increased the production of ideas and induced researchers to pursue ideas more diverse than and distant from their original trajectories. We find that this holds for both researchers who had published in motion-sensing research prior to the Kinect shock (within-area researchers) and those who did not (outside-area researchers), with the effects being stronger among outside-area researchers."
JEFFREY FURMAN,Machine learning could improve innovation policy,
JEFFREY FURMAN,Migration and global network formation,
JEFFREY FURMAN,Disclosure and subsequent innovation: evidence from the patent depository library program,"How important is access to patent documents for subsequent innovation? We examine the expansion of the USPTO Patent Library system after 1975. Patent libraries provided access to patents before the Internet. We find that after patent library opening, local patenting increases by 8–20 percent relative to similar regions. Additional analyses suggest that disclosure of technical information drives this effect: inventors increasingly take up ideas from outside their region, and the effect is strongest in technologies where patents are more informative. We thus provide evidence that disclosure plays an important role in cumulative innovation."
JEFFREY FURMAN,Social globalization and design innovation,"While designs play a critical role in corporate innovation and business operations, the determinants of design innovation (i.e., new aesthetic or stylistic forms) are largely underexplored in the literature. Accumulating evidence suggests that openness to the exchange of ideas, the adoption of progressive policies, and the circulation of human capital play significant roles in driving regional innovative activities. In this study, we ask a variation of this question: Is there evidence that social globalization, i.e., “the spread of ideas, information, images and people” (Dreher et al., 2008, p. 43) – can drive the extent of national design innovation? We leverage a survey instrument reporting on globalization levels, the KOF Globalization Index, to measure national levels of social globalization. We find that national levels of social globalization predict design innovation, as measured by the number of annual design awards granted by the Industrie Forum (iF) over the period 1973-2015. To address the potential endogeneity in our analysis, we instrument for social globalization using a differences-in-differences approach and an instrumental variable approach. Our findings remain robust when we use U.S. design patents as an alternative measure for design innovation. We further show that personal contact could be the main underlying mechanism for social globalization to encourage design innovation."
JEFFREY FURMAN,Measuring the direction of innovation: frontier tools in unassisted machine learning,
TIMOTHY A BROWN,"First Sagittarius A* Event Horizon Telescope results. II. EHT and multiwavelength observations, data processing, and calibration","We present Event Horizon Telescope (EHT) 1.3 mm measurements of the radio source located at the position of the supermassive black hole Sagittarius A* (Sgr A*), collected during the 2017 April 5–11 campaign. The observations were carried out with eight facilities at six locations across the globe. Novel calibration methods are employed to account for Sgr A*'s flux variability. The majority of the 1.3 mm emission arises from horizon scales, where intrinsic structural source variability is detected on timescales of minutes to hours. The effects of interstellar scattering on the image and its variability are found to be subdominant to intrinsic source structure. The calibrated visibility amplitudes, particularly the locations of the visibility minima, are broadly consistent with a blurred ring with a diameter of ∼50 μas, as determined in later works in this series. Contemporaneous multiwavelength monitoring of Sgr A* was performed at 22, 43, and 86 GHz and at near-infrared and X-ray wavelengths. Several X-ray flares from Sgr A* are detected by Chandra, one at low significance jointly with Swift on 2017 April 7 and the other at higher significance jointly with NuSTAR on 2017 April 11. The brighter April 11 flare is not observed simultaneously by the EHT but is followed by a significant increase in millimeter flux variability immediately after the X-ray outburst, indicating a likely connection in the emission physics near the event horizon. We compare Sgr A*’s broadband flux during the EHT campaign to its historical spectral energy distribution and find that both the quiescent emission and flare emission are consistent with its long-term behavior."
TIMOTHY A BROWN,"Data for ""designing lattices for impact protection using transfer learning""",
TIMOTHY A BROWN,First Sagittarius A* Event Horizon Telescope results. I. The shadow of the supermassive black hole in the center of the Milky Way,"We present the first Event Horizon Telescope (EHT) observations of Sagittarius A* (Sgr A*), the Galactic center source associated with a supermassive black hole. These observations were conducted in 2017 using a global interferometric array of eight telescopes operating at a wavelength of λ = 1.3 mm. The EHT data resolve a compact emission region with intrahour variability. A variety of imaging and modeling analyses all support an image that is dominated by a bright, thick ring with a diameter of 51.8 ± 2.3 μas (68% credible interval). The ring has modest azimuthal brightness asymmetry and a comparatively dim interior. Using a large suite of numerical simulations, we demonstrate that the EHT images of Sgr A* are consistent with the expected appearance of a Kerr black hole with mass ∼4 × 106 M ⊙, which is inferred to exist at this location based on previous infrared observations of individual stellar orbits, as well as maser proper-motion studies. Our model comparisons disfavor scenarios where the black hole is viewed at high inclination (i &gt; 50°), as well as nonspinning black holes and those with retrograde accretion disks. Our results provide direct evidence for the presence of a supermassive black hole at the center of the Milky Way, and for the first time we connect the predictions from dynamical measurements of stellar orbits on scales of 103–105 gravitational radii to event-horizon-scale images and variability. Furthermore, a comparison with the EHT results for the supermassive black hole M87* shows consistency with the predictions of general relativity spanning over three orders of magnitude in central mass."
TIMOTHY A BROWN,"Data for ""a physics-informed impact model refined by multi-fidelity transfer learning""","This folder contains the experimental data for the research paper ""A physics-informed impact model refined by multi-fidelity transfer learning"" which is scheduled to be published in Extreme Mechanics Letters. The designs are broken up into several categories: TPU 1-6: Figures 1-3 TPU 7-8: Figure 4 TPU Foaming 1-14: Figure 5 Both STL and Gcode files are available for each design. Note that spiral/vase mode is used when slicing, so the part will be hollow when sliced. The proper extrusion multiplier must be used to reach the mass target for each design. The slicer settings are included in the Gcode at the end of the file. The raw data for physical experiments performed in both constant velocity tests in a universal testing machine (UTM) and impact tests using a drop tower are also included. They use the following naming structure: UTM tests: <material>_V<velocity>_<trial>.csv Note that velocity is in mm/min and trial is either a, b, or c. Only 2 mm/min velocity tests contain b and c trials. Impact tests: <material>_V<velocity>.csv Note that velocity is in m/s and goes to two decimal places."
TIMOTHY A BROWN,The long-baseline neutrino experiment: exploring fundamental symmetries of the universe,"The preponderance of matter over antimatter in the early Universe, the dynamics of the supernova bursts that produced the heavy elements necessary for life and whether protons eventually decay --- these mysteries at the forefront of particle physics and astrophysics are key to understanding the early evolution of our Universe, its current state and its eventual fate. The Long-Baseline Neutrino Experiment (LBNE) represents an extensively developed plan for a world-class experiment dedicated to addressing these questions. LBNE is conceived around three central components: (1) a new, high-intensity neutrino source generated from a megawatt-class proton accelerator at Fermi National Accelerator Laboratory, (2) a near neutrino detector just downstream of the source, and (3) a massive liquid argon time-projection chamber deployed as a far detector deep underground at the Sanford Underground Research Facility. This facility, located at the site of the former Homestake Mine in Lead, South Dakota, is approximately 1,300 km from the neutrino source at Fermilab -- a distance (baseline) that delivers optimal sensitivity to neutrino charge-parity symmetry violation and mass ordering effects. This ambitious yet cost-effective design incorporates scalability and flexibility and can accommodate a variety of upgrades and contributions. With its exceptional combination of experimental configuration, technical capabilities, and potential for transformative discoveries, LBNE promises to be a vital facility for the field of particle physics worldwide, providing physicists from around the globe with opportunities to collaborate in a twenty to thirty year program of exciting science. In this document we provide a comprehensive overview of LBNE's scientific objectives, its place in the landscape of neutrino physics worldwide, the technologies it will incorporate and the capabilities it will possess."
TIMOTHY A BROWN,Data for autonomous discovery of tough structures,"This dataset contains the experimental data for the research paper ""Autonomous Discovery of Tough Structure"" which is currently in the peer review process. The document contains the record of physical experiments performed over the course of two years by a self-driving lab called the BEAR (Bayesian Autonomous Experimental Researcher). The BEAR consists of five FFF 3d printers, a scale, and an Instron. A UR5 robot arm moves samples for testing."
TIMOTHY A BROWN,"Targeted micro-fiber arrays for measuring and manipulating localized multi-scale neural dynamics over large, deep brain volumes during behavior","Neural population dynamics relevant for behavior vary over multiple spatial and temporal scales across 3-dimensional volumes. Current optical approaches lack the spatial coverage and resolution necessary to measure and manipulate naturally occurring patterns of large-scale, distributed dynamics within and across deep brain regions such as the striatum. We designed a new micro-fiber array and imaging approach capable of chronically measuring and optogenetically manipulating local dynamics across over 100 targeted locations simultaneously in head-fixed and freely moving mice. We developed a semi-automated micro-CT based strategy to precisely localize positions of each optical fiber. This highly-customizable approach enables investigation of multi-scale spatial and temporal patterns of cell-type and neurotransmitter specific signals over arbitrary 3-D volumes at a spatial resolution and coverage previously inaccessible. We applied this method to resolve rapid dopamine release dynamics across the striatum volume which revealed distinct, modality specific spatiotemporal patterns in response to salient sensory stimuli extending over millimeters of tissue. Targeted optogenetics through our fiber arrays enabled flexible control of neural signaling on multiple spatial scales, better matching endogenous signaling patterns, and spatial localization of behavioral function across large circuits."
XIN ZHANG,Wireless transfer of power by a 35-GHz metamaterial split-ring resonator rectenna,Wireless transfer of power via high frequency microwave radiation using a miniature split ring resonator rectenna is reported. RF power is converted into DC power by integrating a rectification circuit with the split ring resonator. The near-field behavior of the rectenna is investigated with microwave radiation in the frequency range between 20-40 GHz with a maximum power level of 17 dBm. The observed resonance peaks match those predicted by simulation. Polarization studies show the expected maximum in signal when the electric field is polarized along the edge of the split ring resonator with the gap and minimum for perpendicular orientation. The efficiency of the rectenna is on the order of 1% for a frequency of 37.2 GHz. By using a cascading array of 9 split ring resonators the output power was increased by a factor of 20.
XIN ZHANG,Horn-like space-coiling metamaterials toward simultaneous phase and amplitude modulation,"Acoustic metasurfaces represent a family of planar wavefront-shaping devices garnering increasing attention due to their capacity for novel acoustic wave manipulation. By precisely tailoring the geometry of these engineered surfaces, the effective refractive index may be modulated and, consequently, acoustic phase delays tuned. Despite the successful demonstration of phase engineering using metasurfaces, amplitude modulation remains overlooked. Herein, we present a class of metasurfaces featuring a horn-like space-coiling structure, enabling acoustic control with simultaneous phase and amplitude modulation. The functionality of this class of metasurfaces, featuring a gradient in channel spacing, has been investigated theoretically and numerically and an equivalent model simplifying the structural behavior is presented. A metasurface featuring this geometry has been designed and its functionality in modifying acoustic radiation patterns experimentally validated. This class of acoustic metasurface provides an efficient design methodology enabling complete acoustic wave manipulation, which may find utility in applications including biomedical imaging, acoustic communication, and non-destructive testing."
XIN ZHANG,A Virtual Deadline Scheduler for Window-Constrained Service Guarantees,"This paper presents a new approach to window-constrained scheduling, suitable for multimedia and weakly-hard real-time systems. We originally developed an algorithm, called Dynamic Window-Constrained Scheduling (DWCS), that attempts to guarantee no more than x out of y deadlines are missed for real-time jobs such as periodic CPU tasks, or delay-constrained packet streams. While DWCS is capable of generating a feasible window-constrained schedule that utilizes 100% of resources, it requires all jobs to have the same request periods (or intervals between successive service requests). We describe a new algorithm called Virtual Deadline Scheduling (VDS), that provides window-constrained service guarantees to jobs with potentially different request periods, while still maximizing resource utilization. VDS attempts to service m out of k job instances by their virtual deadlines, that may be some finite time after the corresponding real-time deadlines. Notwithstanding, VDS is capable of outperforming DWCS and similar algorithms, when servicing jobs with potentially different request periods. Additionally, VDS is able to limit the extent to which a fraction of all job instances are serviced late. Results from simulations show that VDS can provide better window-constrained service guarantees than other related algorithms, while still having as good or better delay bounds for all scheduled jobs. Finally, an implementation of VDS in the Linux kernel compares favorably against DWCS for a range of scheduling loads."
XIN ZHANG,Highly sensitive transient absorption imaging of graphene and graphene oxide in living cells and circulating blood,We report a transient absorption (TA) imaging method for fast visualization and quantitative layer analysis of graphene and GO. Forward and backward imaging of graphene on various substrates under ambient condition was imaged with a speed of 2 μs per pixel. The TA intensity linearly increased with the layer number of graphene. Real-time TA imaging of GO in vitro with capability of quantitative analysis of intracellular concentration and ex vivo in circulating blood were demonstrated. These results suggest that TA microscopy is a valid tool for the study of graphene based materials.
XIN ZHANG,Bringing power and progress to Africa in a financially and environmentally sustainable manner,"EXECUTIVE SUMMARY: The future of electricity supply and delivery on the continent of Africa represents one of the thorniest challenges facing professionals in the global energy, economics, finance, environmental, and philanthropic communities. Roughly 600 million people in Africa lack any access to electricity. If this deficiency is not solved, extreme poverty for many Africans is virtually assured for the foreseeable future, as it is widely recognized that economic advancement cannot be achieved in the 21st Century without good electricity supply. Yet, if Africa were to electrify in the same manner pursued in developed economies around the world during the 20th Century, the planet’s global carbon budget would be vastly exceeded, greatly exacerbating the worldwide damages from climate change. Moreover, due to low purchasing power in most African economies and fiscal insolvency of most African utilities, it is unclear exactly how the necessary infrastructure investments can be deployed to bring ample quantities of power – especially zero-carbon power – to all Africans, both those who currently are unconnected to any grid as well as those who are now served by expensive, high-emitting, limited and unreliable electricity supply. With the current population of 1.3 billion people expected to double by 2050, the above-noted challenges associated with the African electricity sector may well get substantially worse than they already are – unless new approaches to infrastructure planning, development, finance and operation can be mobilized and propagated across the continent. This paper presents a summary of the present state and possible futures for the African electricity sector. A synthesis of an ever-growing body of research on electricity in Africa, this paper aims to provide the reader a thorough and balanced context as well as general conclusions and recommendations to better inform and guide decision-making and action. [TRUNCATED]"
XIN ZHANG,Plasmon-enhanced stimulated Raman scattering microscopy with single-molecule detection sensitivity,"Stimulated Raman scattering (SRS) microscopy allows for high-speed label-free chemical imaging of biomedical systems. The imaging sensitivity of SRS microscopy is limited to ~10 mM for endogenous biomolecules. Electronic pre-resonant SRS allows detection of sub-micromolar chromophores. However, label-free SRS detection of single biomolecules having extremely small Raman cross-sections (~10-30 cm2 sr-1) remains unreachable. Here, we demonstrate plasmon-enhanced stimulated Raman scattering (PESRS) microscopy with single-molecule detection sensitivity. Incorporating pico-Joule laser excitation, background subtraction, and a denoising algorithm, we obtain robust single-pixel SRS spectra exhibiting single-molecule events, verified by using two isotopologues of adenine and further confirmed by digital blinking and bleaching in the temporal domain. To demonstrate the capability of PESRS for biological applications, we utilize PESRS to map adenine released from bacteria due to starvation stress. PESRS microscopy holds the promise for ultrasensitive detection and rapid mapping of molecular events in chemical and biomedical systems."
XIN ZHANG,Extreme suppression of antiferromagnetic order and critical scaling in a two-dimensional random quantum magnet,"Sr_2CuTeO_6 is a square-lattice Néel antiferromagnet with superexchange between first-neighbor S=1/2 Cu spins mediated by plaquette centered Te ions. Substituting Te by W, the affected impurity plaquettes have predominantly second-neighbor interactions, thus causing local magnetic frustration. Here we report a study of Sr_2CuTe_1-xW_xO_6 using neutron diffraction and μSR techniques, showing that the Néel order vanishes already at x=0.025±0.005. We explain this extreme order suppression using a two-dimensional Heisenberg spin model, demonstrating that a W-type impurity induces a deformation of the order parameter that decays with distance as 1/r^2 at temperature T=0. The associated logarithmic singularity leads to loss of order for any x>0. Order for small x>0 and T>0 is induced by weak interplane couplings. In the nonmagnetic phase of Sr_2CuTe_1-x W_x O_6, the μSR relaxation rate exhibits quantum critical scaling with a large dynamic exponent, z≈3, consistent with a random-singlet state."
XIN ZHANG,Background-suppressed high-throughput mid-infrared photothermal microscopy via pupil engineering,
XIN ZHANG,Nrg4 promotes fuel oxidation and a healthy adipokine profile to ameliorate diet-induced metabolic disorders,"OBJECTIVE: Brown and white adipose tissue exerts pleiotropic effects on systemic energy metabolism in part by releasing endocrine factors. Neuregulin 4 (Nrg4) was recently identified as a brown fat-enriched secreted factor that ameliorates diet-induced metabolic disorders, including insulin resistance and hepatic steatosis. However, the physiological mechanisms through which Nrg4 regulates energy balance and glucose and lipid metabolism remain incompletely understood. The aims of the current study were: i) to investigate the regulation of adipose Nrg4 expression during obesity and the physiological signals involved, ii) to elucidate the mechanisms underlying Nrg4 regulation of energy balance and glucose and lipid metabolism, and iii) to explore whether Nrg4 regulates adipose tissue secretome gene expression and adipokine secretion. METHODS: We examined the correlation of adipose Nrg4 expression with obesity in a cohort of diet-induced obese mice and investigated the upstream signals that regulate Nrg4 expression. We performed metabolic cage and hyperinsulinemic-euglycemic clamp studies in Nrg4 transgenic mice to dissect the metabolic pathways regulated by Nrg4. We investigated how Nrg4 regulates hepatic lipid metabolism in the fasting state and explored the effects of Nrg4 on adipose tissue gene expression, particularly those encoding secreted factors. RESULTS: Adipose Nrg4 expression is inversely correlated with adiposity and regulated by pro-inflammatory and anti-inflammatory signaling. Transgenic expression of Nrg4 increases energy expenditure and augments whole body glucose metabolism. Nrg4 protects mice from diet-induced hepatic steatosis in part through activation of hepatic fatty acid oxidation and ketogenesis. Finally, Nrg4 promotes a healthy adipokine profile during obesity. CONCLUSIONS: Nrg4 exerts pleiotropic beneficial effects on energy balance and glucose and lipid metabolism to ameliorate obesity-associated metabolic disorders. Biologic therapeutics based on Nrg4 may improve both type 2 diabetes and non-alcoholic fatty liver disease (NAFLD) in patients."
XIN ZHANG,Non-line-of-sight imaging over 1.43 km,"Non-line-of-sight (NLOS) imaging has the ability to reconstruct hidden objects from indirect light paths that scatter multiple times in the surrounding environment, which is of considerable interest in a wide range of applications. Whereas conventional imaging involves direct line-of-sight light transport to recover the visible objects, NLOS imaging aims to reconstruct the hidden objects from the indirect light paths that scatter multiple times, typically using the information encoded in the time-of-flight of scattered photons. Despite recent advances, NLOS imaging has remained at short-range realizations, limited by the heavy loss and the spatial mixing due to the multiple diffuse reflections. Here, both experimental and conceptual innovations yield hardware and software solutions to increase the standoff distance of NLOS imaging from meter to kilometer range, which is about three orders of magnitude longer than previous experiments. In hardware, we develop a high-efficiency, low-noise NLOS imaging system at near-infrared wavelength based on a dual-telescope confocal optical design. In software, we adopt a convex optimizer, equipped with a tailored spatial-temporal kernel expressed using three-dimensional matrix, to mitigate the effect of the spatial-temporal broadening over long standoffs. Together, these enable our demonstration of NLOS imaging and real-time tracking of hidden objects over a distance of 1.43 km. The results will open venues for the development of NLOS imaging techniques and relevant applications to real-world conditions."
XIN ZHANG,Nanosecond-resolution photothermal dynamic imaging via MHZ digitization and match filtering,"Photothermal microscopy has enabled highly sensitive label-free imaging of absorbers, from metallic nanoparticles to chemical bonds. Photothermal signals are conventionally detected via modulation of excitation beam and demodulation of probe beam using lock-in amplifier. While convenient, the wealth of thermal dynamics is not revealed. Here, we present a lock-in free, mid-infrared photothermal dynamic imaging (PDI) system by MHz digitization and match filtering at harmonics of modulation frequency. Thermal-dynamic information is acquired at nanosecond resolution within single pulse excitation. Our method not only increases the imaging speed by two orders of magnitude but also obtains four-fold enhancement of signal-to-noise ratio over lock-in counterpart, enabling high-throughput metabolism analysis at single-cell level. Moreover, by harnessing the thermal decay difference between water and biomolecules, water background is effectively separated in mid-infrared PDI of living cells. This ability to nondestructively probe chemically specific photothermal dynamics offers a valuable tool to characterize biological and material specimens."
XIN ZHANG,Multiwindow SRS imaging using a rapid widely tunable fiber laser,"Spectroscopic stimulated Raman scattering (SRS) imaging has become a useful tool finding a broad range of applications. Yet, wider adoption is hindered by the bulky and environmentally sensitive solid-state optical parametric oscillator (OPO) in a current SRS microscope. Moreover, chemically informative multiwindow SRS imaging across C-H, C-D, and fingerprint Raman regions is challenging due to the slow wavelength tuning speed of the solid-state OPO. In this work, we present a multiwindow SRS imaging system based on a compact and robust fiber laser with rapid and wide tuning capability. To address the relative intensity noise intrinsic to a fiber laser, we implemented autobalanced detection, which enhances the signal-to-noise ratio of stimulated Raman loss imaging by 23 times. We demonstrate high-quality SRS metabolic imaging of fungi, cancer cells, and Caenorhabditis elegans across the C-H, C-D, and fingerprint Raman windows. Our results showcase the potential of the compact multiwindow SRS system for a broad range of applications."
XIN ZHANG,The long-baseline neutrino experiment: exploring fundamental symmetries of the universe,"The preponderance of matter over antimatter in the early Universe, the dynamics of the supernova bursts that produced the heavy elements necessary for life and whether protons eventually decay --- these mysteries at the forefront of particle physics and astrophysics are key to understanding the early evolution of our Universe, its current state and its eventual fate. The Long-Baseline Neutrino Experiment (LBNE) represents an extensively developed plan for a world-class experiment dedicated to addressing these questions. LBNE is conceived around three central components: (1) a new, high-intensity neutrino source generated from a megawatt-class proton accelerator at Fermi National Accelerator Laboratory, (2) a near neutrino detector just downstream of the source, and (3) a massive liquid argon time-projection chamber deployed as a far detector deep underground at the Sanford Underground Research Facility. This facility, located at the site of the former Homestake Mine in Lead, South Dakota, is approximately 1,300 km from the neutrino source at Fermilab -- a distance (baseline) that delivers optimal sensitivity to neutrino charge-parity symmetry violation and mass ordering effects. This ambitious yet cost-effective design incorporates scalability and flexibility and can accommodate a variety of upgrades and contributions. With its exceptional combination of experimental configuration, technical capabilities, and potential for transformative discoveries, LBNE promises to be a vital facility for the field of particle physics worldwide, providing physicists from around the globe with opportunities to collaborate in a twenty to thirty year program of exciting science. In this document we provide a comprehensive overview of LBNE's scientific objectives, its place in the landscape of neutrino physics worldwide, the technologies it will incorporate and the capabilities it will possess."
XIN ZHANG,Bayesian reconstruction of magnetic resonance images using Gaussian processes,"A central goal of modern magnetic resonance imaging (MRI) is to reduce the time required to produce high-quality images. Efforts have included hardware and software innovations such as parallel imaging, compressed sensing, and deep learning-based reconstruction. Here, we propose and demonstrate a Bayesian method to build statistical libraries of magnetic resonance (MR) images in k-space and use these libraries to identify optimal subsampling paths and reconstruction processes. Specifically, we compute a multivariate normal distribution based upon Gaussian processes using a publicly available library of T1-weighted images of healthy brains. We combine this library with physics-informed envelope functions to only retain meaningful correlations in k-space. This covariance function is then used to select a series of ring-shaped subsampling paths using Bayesian optimization such that they optimally explore space while remaining practically realizable in commercial MRI systems. Combining optimized subsampling paths found for a range of images, we compute a generalized sampling path that, when used for novel images, produces superlative structural similarity and error in comparison to previously reported reconstruction processes (i.e. 96.3% structural similarity and < 0.003 normalized mean squared error from sampling only 12.5% of the k-space data). Finally, we use this reconstruction process on pathological data without retraining to show that reconstructed images are clinically useful for stroke identification. Since the model trained on images of healthy brains could be directly used for predictions in pathological brains without retraining, it shows the inherent transferability of this approach and opens doors to its widespread use."
MARIA C O'BRIEN,Broadband multi-wavelength properties of M87 during the 2017 Event Horizon Telescope campaign,"In 2017, the Event Horizon Telescope (EHT) Collaboration succeeded in capturing the first direct image of the center of the M87 galaxy. The asymmetric ring morphology and size are consistent with theoretical expectations for a weakly accreting supermassive black hole of mass ∼6.5 × 109 M ⊙. The EHTC also partnered with several international facilities in space and on the ground, to arrange an extensive, quasi-simultaneous multi-wavelength campaign. This Letter presents the results and analysis of this campaign, as well as the multi-wavelength data as a legacy data repository. We captured M87 in a historically low state, and the core flux dominates over HST-1 at high energies, making it possible to combine core flux constraints with the more spatially precise very long baseline interferometry data. We present the most complete simultaneous multi-wavelength spectrum of the active nucleus to date, and discuss the complexity and caveats of combining data from different spatial scales into one broadband spectrum. We apply two heuristic, isotropic leptonic single-zone models to provide insight into the basic source properties, but conclude that a structured jet is necessary to explain M87’s spectrum. We can exclude that the simultaneous γ-ray emission is produced via inverse Compton emission in the same region producing the EHT mm-band emission, and further conclude that the γ-rays can only be produced in the inner jets (inward of HST-1) if there are strongly particle-dominated regions. Direct synchrotron emission from accelerated protons and secondaries cannot yet be excluded."
JAMES A CHERRY,"Gonadal hormones, but not sex, affect the acquisition and maintenance of a Go/No-Go odor discrimination task in mice","In mice, olfaction is crucial for identifying social odors (pheromones) that signal the presence of suitable mates. We used a custom-built olfactometer and a thirst-motivated olfactory discrimination Go/No-Go (GNG) task to ask whether discrimination of volatile odors is sexually dimorphic and modulated in mice by adult sex hormones. Males and females gonadectomized prior to training failed to learn even the initial phase of the task, which involved nose poking at a port in one location obtaining water at an adjacent port. Gonadally intact males and females readily learned to seek water when male urine (S+) was present but not when female urine (S−) was present; they also learned the task when non-social odorants (amyl acetate, S+; peppermint, S−) were used. When mice were gonadectomized after training the ability of both sexes to discriminate urinary as well as non-social odors was reduced; however, after receiving testosterone propionate (castrated males) or estradiol benzoate (ovariectomized females), task performance was restored to pre-gonadectomy levels. There were no overall sex differences in performance across gonadal conditions in tests with either set of odors; however, ovariectomized females performed more poorly than castrated males in tests with non-social odors. Our results show that circulating sex hormones enable mice of both sexes to learn a GNG task and that gonadectomy reduces, while hormone replacement restores, their ability to discriminate between odors irrespective of the saliency of the odors used. Thus, gonadal hormones were essential for both learning and maintenance of task performance across sex and odor type."
JAMES A CHERRY,Optogenetic activation of accessory olfactory bulb input to the forebrain differentially modulates investigation of opposite versus same-sex urinary chemosignals and stimulates mating in male mice,"Surgical or genetic disruption of vomeronasal organ (VNO)-accessory olfactory bulb (AOB) function previously eliminated the ability of male mice to processes pheromones that elicit territorial behavior and aggression. By contrast, neither disruption significantly affected mating behaviors, although VNO lesions reduced males' investigation of nonvolatile female pheromones. We explored the contribution of VNO-AOB pheromonal processing to male courtship using optogenetic activation of AOB projections to the forebrain. Protocadherin-Cre male transgenic mice received bilateral AOB infections with channelrhodopsin2 (ChR2) viral vectors, and an optical fiber was implanted above the AOB. In olfactory choice tests, males preferred estrous female urine (EFU) over water; however, this preference was eliminated when diluted (5%) EFU was substituted for 100% EFU. Optogenetic AOB activation concurrent with nasal contact significantly augmented males' investigation compared to 5% EFU alone. Conversely, concurrent optogenetic AOB activation significantly reduced males' nasal investigation of diluted urine from gonadally intact males (5% IMU) compared to 5% IMU alone. These divergent effects of AOB optogenetic activation were lost when males were prevented from making direct nasal contact. Optogenetic AOB stimulation also failed to augment males' nasal investigation of deionized water or of food odors. Finally, during mating tests, optogenetic AOB stimulation delivered for 30 s when the male was in physical contact with an estrous female significantly facilitated the occurrence of penile intromission. Our results suggest that VNO-AOB signaling differentially modifies males' motivation to seek out female vs male urinary pheromones while augmenting males' sexual arousal leading to intromission and improved reproductive performance."
JAMES A CHERRY,"DREADD-induced silencing of the medial olfactory tubercle disrupts the preference of female mice for opposite-sex chemosignals(1,2,3)","Attraction to opposite-sex pheromones during rodent courtship involves a pathway that includes inputs to the medial amygdala (Me) from the main and accessory olfactory bulbs, and projections from the Me to nuclei in the medial hypothalamus that control reproduction. However, the consideration of circuitry that attributes hedonic properties to opposite-sex odors has been lacking. The medial olfactory tubercle (mOT) has been implicated in the reinforcing effects of natural stimuli and drugs of abuse. We performed a tract-tracing study wherein estrous female mice that had received injections of the retrograde tracer, cholera toxin B, into the mOT were exposed to volatile odors from soiled bedding. Both the anterior Me and ventral tegmental area sent direct projections to the mOT, of which a significant subset was selectively activated (expressed Fos protein) by testes-intact male (but not female) volatile odors from soiled bedding. Next, the inhibitory DREADD (designer receptors exclusively activated by designer drugs) receptor hM4Di was bilaterally expressed in the mOT of female mice. Urinary preferences were then assessed after intraperitoneal injection of either saline or clozapine-N-oxide (CNO), which binds to the hM4Di receptor to hyperpolarize infected neurons. After receiving CNO, estrous females lost their preference for male over female urinary odors, whereas the ability to discriminate these odors remained intact. Male odor preference returned after vehicle treatment in counterbalanced tests. There were no deficits in locomotor activity or preference for food odors when subject mice received CNO injections prior to testing. The mOT appears to be a critical segment in the pheromone-reward pathway of female mice."
SHELLEY J RUSSEK,Positional Clustering Improves Computational Binding Site Detection and Identifies Novel Cis-Regulatory Sites in Mammalian GABAA Receptor Subunit Genes,"Understanding transcription factor (TF) mediated control of gene expression remains a major challenge at the interface of computational and experimental biology. Computational techniques predicting TF-binding site specificity are frequently unreliable. On the other hand, comprehensive experimental validation is difficult and time consuming. We introduce a simple strategy that dramatically improves robustness and accuracy of computational binding site prediction. First, we evaluate the rate of recurrence of computational TFBS predictions by commonly used sampling procedures. We find that the vast majority of results are biologically meaningless. However clustering results based on nucleotide position improves predictive power. Additionally, we find that positional clustering increases robustness to long or imperfectly selected input sequences. Positional clustering can also be used as a mechanism to integrate results from multiple sampling approaches for improvements in accuracy over each one alone. Finally, we predict and validate regulatory sequences partially responsible for transcriptional control of the mammalian type A γ-aminobutyric acid receptor (GABAAR) subunit genes. Positional clustering is useful for improving computational binding site predictions, with potential application to improving our understanding of mammalian gene expression. In particular, predicted regulatory mechanisms in the mammalian GABAAR subunit gene family may open new avenues of research towards understanding this pharmacologically important neurotransmitter receptor system."
IGOR LUKES,Panel discussion: Europe confronts its past,
IGOR LUKES,Introduction: The Munich Diktat: fifty years after,
IGOR LUKES,Stalin's diplomatic maneuvers during the 1938 Czechoslovak crisis,
IGOR LUKES,The long-baseline neutrino experiment: exploring fundamental symmetries of the universe,"The preponderance of matter over antimatter in the early Universe, the dynamics of the supernova bursts that produced the heavy elements necessary for life and whether protons eventually decay --- these mysteries at the forefront of particle physics and astrophysics are key to understanding the early evolution of our Universe, its current state and its eventual fate. The Long-Baseline Neutrino Experiment (LBNE) represents an extensively developed plan for a world-class experiment dedicated to addressing these questions. LBNE is conceived around three central components: (1) a new, high-intensity neutrino source generated from a megawatt-class proton accelerator at Fermi National Accelerator Laboratory, (2) a near neutrino detector just downstream of the source, and (3) a massive liquid argon time-projection chamber deployed as a far detector deep underground at the Sanford Underground Research Facility. This facility, located at the site of the former Homestake Mine in Lead, South Dakota, is approximately 1,300 km from the neutrino source at Fermilab -- a distance (baseline) that delivers optimal sensitivity to neutrino charge-parity symmetry violation and mass ordering effects. This ambitious yet cost-effective design incorporates scalability and flexibility and can accommodate a variety of upgrades and contributions. With its exceptional combination of experimental configuration, technical capabilities, and potential for transformative discoveries, LBNE promises to be a vital facility for the field of particle physics worldwide, providing physicists from around the globe with opportunities to collaborate in a twenty to thirty year program of exciting science. In this document we provide a comprehensive overview of LBNE's scientific objectives, its place in the landscape of neutrino physics worldwide, the technologies it will incorporate and the capabilities it will possess."
SARAH A. CRABTREE,The moderating influence of religiousness/spirituality on COVID-19 impact and change in Psychotherapy,"The COVID-19 pandemic has spurred a global surge in empirical research examining the influence of the pandemic on individuals’ mental health symptoms and well-being. Within this larger literature is a rapidly growing literature on the associations among religiousness/spirituality, COVID-19 impact, symptoms and well-being. Largely absent from this literature is a specific research focus on psychotherapy clients, and the influence of religiousness/spirituality and COVID-19 impact on change during treatment. One prominent theory in the existing literature centers on the notion that religiousness/spirituality is a coping resource for individuals during times of adversity. Yet, existing empirical findings present mixed evidence for the religious/spiritual coping hypothesis. We expanded upon these emerging research trends to examine the influence of religious/spiritual struggles, religious/spiritual commitment, religious/spiritual exploration, and COVID-19 impact ratings on psychotherapy change in a sample of adult clients (N = 185; Mage = 38.06; SD = 15.78; range = 19–81; 61.1% female; 69.7% White). The results of latent trajectory analysis identified three subgroups that differed on initial levels of symptoms and well-being and the nature of change over three time points. The COVID-19 impact ratings predicted change trajectories. As more positive ratings of COVID-19 impact increased, membership in the no change trajectory was more likely relative to the deterioration trajectory at high levels of both religious/spiritual commitment and exploration. The implications emphasize the need for judicious assessment of religiousness/spirituality and COVID-19 impact before integrating religiousness/spirituality into treatment."
SARAH A. CRABTREE,"Relational spirituality, intercultural competence, and social justice in systemic therapies","The Relational Spirituality Model (RSM) builds on relational, psychodynamic, and systemic approaches and serves as an orienting framework for clinical services and training. In this article, we provide an overview of the RSM, a pluralistic contextual approach to spirituality in clinical practice that (a) considers developmental dialectics of spiritual dwelling and seeking and (b) explores diverse ways that religious and spiritual dynamics can range from salutary to harmful. In light of growing attention to racism in U.S. society, we review salient research on justice-seeking spirituality and consider the roles of humility, differentiation, and hope in developing intercultural competence. Throughout, we consider implications for clinical practice and training."
SARAH A. CRABTREE,"Mental health, well-being, and experiences of the COVID-19 pandemic: a mixed methods practice-based study",
LAURA CAPTARI,New horizons in group psychotherapy research and practice from third wave positive psychology: a practice-friendly review,"Group psychotherapy has been shown to be equivalent to individual therapy for many disorders, including anxiety, depression, grief, eating disorders, and schizophrenia (Burlingame & Strauss, 2021). In addition to effectiveness in reducing symptoms, group offers members a sense of belonging, purpose, hope, altruism, and meaning throughout treatment (Yalom & Leszcz, 2020). These additional outcomes are especially important considering the COVID-19 pandemic and national/international conflicts, given the trauma, disruptions, and losses people have experienced. Applying recent developments in positive psychology to group therapy can enhance treatment. A practice-friendly review examined recent advances in the positive psychology literature, demonstrating how group therapy offers members unique growth opportunities in addition to reducing symptoms. Key findings from studies applying positive psychological constructs to group therapy outcomes are synthesized. Our review sheds light on the relevance of third wave positive psychology to enrich group therapy (Lomas et al., 2021). Specifically, group therapy can facilitate the development of vitalizing psychological virtues, and these can be used to assess treatment outcome: humanity, wisdom, transcendence, courage, temperance, and justice. Interrelatedly, we present support for including attachment theory and mentalization within a positive psychological group framework. Implications are explored for group therapy research, clinical work, and training."
LAURA CAPTARI,"Relational spirituality, intercultural competence, and social justice in systemic therapies","The Relational Spirituality Model (RSM) builds on relational, psychodynamic, and systemic approaches and serves as an orienting framework for clinical services and training. In this article, we provide an overview of the RSM, a pluralistic contextual approach to spirituality in clinical practice that (a) considers developmental dialectics of spiritual dwelling and seeking and (b) explores diverse ways that religious and spiritual dynamics can range from salutary to harmful. In light of growing attention to racism in U.S. society, we review salient research on justice-seeking spirituality and consider the roles of humility, differentiation, and hope in developing intercultural competence. Throughout, we consider implications for clinical practice and training."
LAURA CAPTARI,"Mental health, well-being, and experiences of the COVID-19 pandemic: a mixed methods practice-based study",
LAURA CAPTARI,"Exploring virtue ethics in psychodynamic psychotherapy: latent changes in humility, affect regulation, symptoms, and well-being","[Empirical exploration of a salutary role for virtues on both mental health symptoms and well-being has increased. Yet, tests for this role in psychotherapy have not matched research in other contexts. As such, we tested the virtue ethics premise that growth in humility could facilitate changes in symptoms and well-being in the context of contemporary relational psychotherapy (CRP; Sandage et al., 2020). CRP is grounded in three premises: (a) conceptualizing clients within their historical, familial and sociocultural contexts, (b) understanding distress as stemming from maladaptive intra- and interpersonal patterns, and (c) prioritizing a here-and-now focus within the therapeutic relationship. We proposed experiential avoidance as the mechanism whereby humility influences symptoms and well-being.]"
KUANG LI,Can they always transfer to a four-year college later? Examining high school English learner (EL) graduates’ community college transfer pathway,"High school English learners (ELs) who aspire to earn a bachelor’s degree are often guided by their high school counselors and teachers to attend community colleges first, and are assured that they can always transfer to a four-year institution later. While community colleges are an important port of entry for ELs’ college education, little is known about what happens to high school EL graduates after they enter community colleges and whether they can actually make the four-year-college transfer a reality. The extant literature has focused primarily on ELs’ high school experiences while a few studies have followed high school ELs only up until they complete English as a second language (ESL) courses in community colleges. Thus, high school EL graduates’ community college studies beyond ESL courses and their four-year-college transfer process have not been well understood. To fill these gaps, this qualitative multiple-case study, adopting a hybrid of cross-sectional and longitudinal methods, followed nine baccalaureate-aspiring high school EL graduates, who were at different stages of community college studies (e.g., some were at the very beginning of taking ESL/remedial courses while others were close to transferring to a four-year college), over one year. Major data included semi-structured interviews with students and educators, classroom observations, and document collection (e.g., students’ high school and community college transcripts). Drawing upon the lenses of opportunity to learn (OTL) and college readiness, this study found that high school EL graduates’ transfer trajectories were far from straightforward and it was rare that an EL was able to persist through their original transfer plan. Findings also showed that the high school–community college pipeline was broken for ELs in many ways, and ELs’ OTL was systematically and repeatedly reduced across high school and community college. Thus, the community college transfer pathway does not guarantee a four-year-college transfer for high school EL graduates if they are not provided with adequate OTL and do not achieve higher levels of college readiness in the first place. The findings interrogate the current institutional practices that normalize ELs’ limited OTL, reveal many issues overlooked in the extant four-year-college transfer literature, and contribute to ELs’ educational equity."
SOUVIK BANDYOPADHYAY,Observing dynamical quantum phase transitions through quasilocal string operators,"We analyze signatures of the dynamical quantum phase transitions in physical observables. In particular, we show that both the expectation value and various out of time order correlation functions of the finite length product or string operators develop cusp singularities following quench protocols, which become sharper and sharper as the string length increases. We illustrated our ideas analyzing both integrable and nonintegrable one-dimensional Ising models showing that these transitions are robust both to the details of the model and to the choice of the initial state."
SOUVIK BANDYOPADHYAY,Late-time critical behavior of local stringlike observables under quantum quenches,
STEVEN AUSCAVITCH,"Oceanographic drivers of deep-sea coral species distribution and community assembly on seamounts, islands, atolls, and reefs within the Phoenix Islands Protected Area","The Phoenix Islands Protected Area, in the central Pacific waters of the Republic of Kiribati, is a model for large marine protected area (MPA) development and maintenance, but baseline records of the protected biodiversity in its largest environment, the deep sea (>200 m), have not yet been determined. In general, the equatorial central Pacific lacks biogeographic perspective on deep-sea benthic communities compared to more well-studied regions of the North and South Pacific Ocean. In 2017, explorations by the NOAA ship Okeanos Explorer and R/V Falkor were among the first to document the diversity and distribution of deep-water benthic megafauna on numerous seamounts, islands, shallow coral reef banks, and atolls in the region. Here, we present baseline deep-sea coral species distribution and community assembly patterns within the Scleractinia, Octocorallia, Antipatharia, and Zoantharia with respect to different seafloor features and abiotic environmental variables across bathyal depths (200–2500 m). Remotely operated vehicle (ROV) transects were performed on 17 features throughout the Phoenix Islands and Tokelau Ridge Seamounts resulting in the observation of 12,828 deep-water corals and 167 identifiable morphospecies. Anthozoan assemblages were largely octocoral-dominated consisting of 78% of all observations with seamounts having a greater number of observed morphospecies compared to other feature types. Overlying water masses were observed to have significant effects on community assembly across bathyal depths. Revised species inventories further suggest that the protected area it is an area of biogeographic overlap for Pacific deep-water corals, containing species observed across bathyal provinces in the North Pacific, Southwest Pacific, and Western Pacific. These results underscore significant geographic and environmental complexity associated with deep-sea coral communities that remain in under-characterized in the equatorial central Pacific, but also highlight the additional efforts that need to be brought forth to effectively establish baseline ecological metrics in data deficient bathyal provinces."
STEVEN AUSCAVITCH,The unknown and the unexplored: insights Into the Pacific deep-sea following NOAA CAPSTONE expeditions,"Over a 3-year period, the National Oceanic and Atmospheric Administration (NOAA) organized and implemented a Pacific-wide field campaign entitled CAPSTONE: Campaign to Address Pacific monument Science, Technology, and Ocean NEeds. Under the auspices of CAPSTONE, NOAA mapped 597,230 km2 of the Pacific seafloor (with ∼61% of mapped area located within US waters), including 323 seamounts, conducted 187 ROV dives totaling 891.5 h of ROV benthic imaging time, and documented >347,000 individual organisms. This comprehensive effort yielded dramatic insight into differences in biodiversity across depths, regions, and features, at multiple taxonomic scales. For all deep sea taxonomic groups large enough to be visualized with the ROV, we found that fewer than 20% of the species were able to be identified. The most abundant and highest diversity taxa across the dataset were from three phyla (Cnidaria, Porifera, and Echinodermata). We further examined these phyla for taxonomic assemblage patterns by depth, geographic region, and geologic feature. Within each taxa, there were multiple genera with specific distribution and abundance by depth, region, and feature. Additionally, we observed multiple genera with broad abundance and distribution, which may focus future ecological research efforts. Novel taxa, records, and behaviors were observed, suggestive of many new types of species interactions, drivers of community composition, and overall diversity patterns. To date, only 13.8% of the Pacific has been mapped using modern methods. Despite the incredible amount of new known and unknown information about the Pacific deep-sea, CAPSTONE is far from the culminating experience the name suggests. Rather, it marks the beginning of a new era for exploration that will offer extensive opportunities via mapping, technology, analysis, and insights."
STEVEN AUSCAVITCH,Corrigendum: the unknown and the unexplored: insights into the Pacific deep-sea following NOAA CAPSTONE expeditions,
JOSEPH KERN,Hippo pathway Inactivation drives basal-like mammary tumorigenesis,"Basal-like breast cancers make up an aggressive subtype of breast cancer with diverse intratumor cell heterogeneity and poor clinical outcomes. These cancers are predominantly triple-negative cancers such that they lack expression of estrogen receptor, progesterone receptor, and human epidermal growth factor receptor 2 (HER2). Evidence suggests that basal-like cancers arise from luminal mammary epithelial cells that acquire basal-like traits during tumorigenesis, a process known as cell plasticity. These cancers also present with dramatic remodeling of stromal cell populations such as fibroblasts and immune cells, which become rewired to form a pro-tumorigenic niche. An improved understanding of the cellular mechanisms that drive epithelial plasticity and stromal remodeling may therefore help advance treatment avenues for basal-like breast cancer. This dissertation describes important roles for the Hippo signaling pathway, a pathway involved in development and stem cell traits, in basal-like breast cancer pathogenesis. Using genetic mouse models, we demonstrate that deletion of the Hippo pathway kinases large tumor suppressor kinase (LATS)1 and LATS2 (LATS1/2) in mammary luminal epithelial cells leads to luminal-basal plasticity and the development of basal-like carcinomas through activation of the transcriptional regulators yes-associated protein (YAP) and transcriptional co-activator with PDZ-binding motif (TAZ). These carcinomas are accompanied by remodeling of the mammary stroma, including an accumulation of cancer-associated fibroblasts and a distinct deposition of extracellular matrix. We further present analyses suggesting that LATS1/2 inactivation promotes a tumor-associated niche through reciprocal epithelial-stromal signaling. Together, these results implicate the Hippo pathway as a critical mediator of homeostasis in the mammary gland and suggest that Hippo dysregulation drives basal-like breast cancer initiation by promoting mammary epithelial transformation and stromal remodeling."
MICHAEL H RAYMOND,First M87 Event Horizon Telescope results. III. Data processing and calibration,"We present the calibration and reduction of Event Horizon Telescope (EHT) 1.3 mm radio wavelength observations of the supermassive black hole candidate at the center of the radio galaxy M87 and the quasar 3C 279, taken during the 2017 April 5–11 observing campaign. These global very long baseline interferometric observations include for the first time the highly sensitive Atacama Large Millimeter/submillimeter Array (ALMA); reaching an angular resolution of 25 μas, with characteristic sensitivity limits of ~1 mJy on baselines to ALMA and ~10 mJy on other baselines. The observations present challenges for existing data processing tools, arising from the rapid atmospheric phase fluctuations, wide recording bandwidth, and highly heterogeneous array. In response, we developed three independent pipelines for phase calibration and fringe detection, each tailored to the specific needs of the EHT. The final data products include calibrated total intensity amplitude and phase information. They are validated through a series of quality assurance tests that show consistency across pipelines and set limits on baseline systematic errors of 2% in amplitude and 1° in phase. The M87 data reveal the presence of two nulls in correlated flux density at ~3.4 and ~8.3 Gλ and temporal evolution in closure quantities, indicating intrinsic variability of compact structure on a timescale of days, or several light-crossing times for a few billion solar-mass black hole. These measurements provide the first opportunity to image horizon-scale structure in M87."
MICHAEL H RAYMOND,First M87 Event Horizon Telescope results. V. Physical origin of the asymmetric ring,"The Event Horizon Telescope (EHT) has mapped the central compact radio source of the elliptical galaxy M87 at 1.3 mm with unprecedented angular resolution. Here we consider the physical implications of the asymmetric ring seen in the 2017 EHT data. To this end, we construct a large library of models based on general relativistic magnetohydrodynamic (GRMHD) simulations and synthetic images produced by general relativistic ray tracing. We compare the observed visibilities with this library and confirm that the asymmetric ring is consistent with earlier predictions of strong gravitational lensing of synchrotron emission from a hot plasma orbiting near the black hole event horizon. The ring radius and ring asymmetry depend on black hole mass and spin, respectively, and both are therefore expected to be stable when observed in future EHT campaigns. Overall, the observed image is consistent with expectations for the shadow of a spinning Kerr black hole as predicted by general relativity. If the black hole spin and M87's large scale jet are aligned, then the black hole spin vector is pointed away from Earth. Models in our library of non-spinning black holes are inconsistent with the observations as they do not produce sufficiently powerful jets. At the same time, in those models that produce a sufficiently powerful jet, the latter is powered by extraction of black hole spin energy through mechanisms akin to the Blandford-Znajek process. We briefly consider alternatives to a black hole for the central compact object. Analysis of existing EHT polarization data and data taken simultaneously at other wavelengths will soon enable new tests of the GRMHD models, as will future EHT campaigns at 230 and 345 GHz."
MICHAEL H RAYMOND,First M87 Event Horizon Telescope results. VI. The shadow and mass of the central black hole,"We present measurements of the properties of the central radio source in M87 using Event Horizon Telescope data obtained during the 2017 campaign. We develop and fit geometric crescent models (asymmetric rings with interior brightness depressions) using two independent sampling algorithms that consider distinct representations of the visibility data. We show that the crescent family of models is statistically preferred over other comparably complex geometric models that we explore. We calibrate the geometric model parameters using general relativistic magnetohydrodynamic (GRMHD) models of the emission region and estimate physical properties of the source. We further fit images generated from GRMHD models directly to the data. We compare the derived emission region and black hole parameters from these analyses with those recovered from reconstructed images. There is a remarkable consistency among all methods and data sets. We find that >50% of the total flux at arcsecond scales comes from near the horizon, and that the emission is dramatically suppressed interior to this region by a factor >10, providing direct evidence of the predicted shadow of a black hole. Across all methods, we measure a crescent diameter of 42 ± 3 μas and constrain its fractional width to be <0.5. Associating the crescent feature with the emission surrounding the black hole shadow, we infer an angular gravitational radius of GM/Dc^2 = 3.8 ± 0.4 μas. Folding in a distance measurement of {16.8}_{-0.7}^{+0.8}{Mpc} gives a black hole mass of M = 6.5 ± 0.2{| }_{stat} ± 0.7{| }_{sys} × {10}^{9} {M}_{odot }. This measurement from lensed emission near the event horizon is consistent with the presence of a central Kerr black hole, as predicted by the general theory of relativity."
MICHAEL H RAYMOND,The Event Horizon general relativistic magnetohydrodynamic code comparison project,"Recent developments in compact object astrophysics, especially the discovery of merging neutron stars by LIGO, the imaging of the black hole in M87 by the Event Horizon Telescope, and high- precision astrometry of the Galactic Center at close to the event horizon scale by the GRAVITY experiment motivate the development of numerical source models that solve the equations of general relativistic magnetohydrodynamics (GRMHD). Here we compare GRMHD solutions for the evolution of a magnetized accretion flow where turbulence is promoted by the magnetorotational instability from a set of nine GRMHD codes: Athena++, BHAC, Cosmos++, ECHO, H-AMR, iharm3D, HARM-Noble, IllinoisGRMHD, and KORAL. Agreement among the codes improves as resolution increases, as measured by a consistently applied, specially developed set of code performance metrics. We conclude that the community of GRMHD codes is mature, capable, and consistent on these test problems."
MICHAEL H RAYMOND,Gravitational test beyond the first post-Newtonian order with the shadow of the M87 black hole,"The 2017 Event Horizon Telescope (EHT) observations of the central source in M87 have led to the first measurement of the size of a black-hole shadow. This observation offers a new and clean gravitational test of the black-hole metric in the strong-field regime. We show analytically that spacetimes that deviate from the Kerr metric but satisfy weak-field tests can lead to large deviations in the predicted black-hole shadows that are inconsistent with even the current EHT measurements. We use numerical calculations of regular, parametric, non-Kerr metrics to identify the common characteristic among these different parametrizations that control the predicted shadow size. We show that the shadow-size measurements place significant constraints on deviation parameters that control the second post-Newtonian and higher orders of each metric and are, therefore, inaccessible to weak-field tests. The new constraints are complementary to those imposed by observations of gravitational waves from stellar-mass sources."
MICHAEL H RAYMOND,First M87 Event Horizon Telescope results. IV. Imaging the central supermassive black hole,
MICHAEL H RAYMOND,Verification of radiative transfer schemes for the EHT,"The Event Horizon Telescope (EHT) Collaboration has recently produced the first resolved images of the central supermassive black hole in the giant elliptical galaxy M87. Here we report on tests of the consistency and accuracy of the general relativistic radiative transfer codes used within the collaboration to model M87* and Sgr A*. We compare and evaluate (1) deflection angles for equatorial null geodesics in a Kerr spacetime; (2) images calculated from a series of simple, parameterized matter distributions in the Kerr metric using simplified emissivities and absorptivities; (3) for a subset of codes, images calculated from general relativistic magnetohydrodynamics simulations using different realistic synchrotron emissivities and absorptivities; (4) observables for the 2017 configuration of EHT, including visibility amplitudes and closure phases. The error in total flux is of order 1% when the codes are run with production numerical parameters. The dominant source of discrepancies for small camera distances is the location and detailed setup of the software ""camera"" that each code uses to produce synthetic images. We find that when numerical parameters are suitably chosen and the camera is sufficiently far away the images converge and that for given transfer coefficients, numerical uncertainties are unlikely to limit parameter estimation for the current generation of EHT observations. The purpose of this paper is to describe a verification and comparison of EHT radiative transfer codes. It is not to verify EHT models more generally."
MICHAEL H RAYMOND,Monitoring the mmorphology of M87* in 2009–2017 with the Event Horizon Telescope,"The Event Horizon Telescope (EHT) has recently delivered the first resolved images of M87*, the supermassive black hole in the center of the M87 galaxy. These images were produced using 230 GHz observations performed in April 2017. Additional observations are required to investigate the persistence of the primary image feature – a ring with azimuthal brightness asymmetry – and to quantify the image variability on event horizon scales. To address this need, we analyze M87* data collected with prototype EHT arrays in 2009, 2011, 2012, and 2013. While these observations do not contain enough information to produce images, they are sufficient to constrain simple geometric models. We develop a modeling approach based on the framework utilized for the 2017 EHT data analysis and validate our procedures using synthetic data. Applying the same approach to the observational data sets, we find the M87* morphology in 2009–2017 to be consistent with a persistent asymmetric ring of 40 as diameter. The position angle of peak intensity varies in time. In particular, we find a significant difference between the position angle measured in 2013 and 2017. These variations are in broad agreement with predictions of a subset of general relativistic magnetohydrodynamic simulations. We show that quantifying the variability across multiple observational epochs has the potential to constrain physical properties of the source, such as the accretion state or the black hole spin."
MICHAEL H RAYMOND,THEMIS: a parameter estimation framework for the Event Horizon Telescope,"The Event Horizon Telescope (EHT) provides the unprecedented ability to directly resolve the structure and dynamics of black hole emission regions on scales smaller than their horizons. This has the potential to critically probe the mechanisms by which black holes accrete and launch outflows, and the structure of supermassive black hole spacetimes. However, accessing this information is a formidable analysis challenge for two reasons. First, the EHT natively produces a variety of data types that encode information about the image structure in nontrivial ways; these are subject to a variety of systematic effects associated with very long baseline interferometry and are supplemented by a wide variety of auxiliary data on the primary EHT targets from decades of other observations. Second, models of the emission regions and their interaction with the black hole are complex, highly uncertain, and computationally expensive to construct. As a result, the scientific utilization of EHT observations requires a flexible, extensible, and powerful analysis framework. We present such a framework, Themis, which defines a set of interfaces between models, data, and sampling algorithms that facilitates future development. We describe the design and currently existing components of Themis, how Themis has been validated thus far, and present additional analyses made possible by Themis that illustrate its capabilities. Importantly, we demonstrate that Themis is able to reproduce prior EHT analyses, extend these, and do so in a computationally efficient manner that can efficiently exploit modern high-performance computing facilities. Themis has already been used extensively in the scientific analysis and interpretation of the first EHT observations of M87."
MICHAEL H RAYMOND,First Sagittarius A* Event Horizon Telescope results. V. Testing astrophysical models of the galactic center black hole,"In this paper we provide a first physical interpretation for the Event Horizon Telescope's (EHT) 2017 observations of Sgr A*. Our main approach is to compare resolved EHT data at 230 GHz and unresolved non-EHT observations from radio to X-ray wavelengths to predictions from a library of models based on time-dependent general relativistic magnetohydrodynamics simulations, including aligned, tilted, and stellar-wind-fed simulations; radiative transfer is performed assuming both thermal and nonthermal electron distribution functions. We test the models against 11 constraints drawn from EHT 230 GHz data and observations at 86 GHz, 2.2 μm, and in the X-ray. All models fail at least one constraint. Light-curve variability provides a particularly severe constraint, failing nearly all strongly magnetized (magnetically arrested disk (MAD)) models and a large fraction of weakly magnetized models. A number of models fail only the variability constraints. We identify a promising cluster of these models, which are MAD and have inclination i ≤ 30°. They have accretion rate (5.2–9.5) × 10−9 M ⊙ yr−1, bolometric luminosity (6.8–9.2) × 1035 erg s−1, and outflow power (1.3–4.8) × 1038 erg s−1. We also find that all models with i ≥ 70° fail at least two constraints, as do all models with equal ion and electron temperature; exploratory, nonthermal model sets tend to have higher 2.2 μm flux density; and the population of cold electrons is limited by X-ray constraints due to the risk of bremsstrahlung overproduction. Finally, we discuss physical and numerical limitations of the models, highlighting the possible importance of kinetic effects and duration of the simulations."
MICHAEL H RAYMOND,First M87 Event Horizon Telescope results. VII. Polarization of the ring,"In 2017 April, the Event Horizon Telescope (EHT) observed the near-horizon region around the supermassive black hole at the core of the M87 galaxy. These 1.3 mm wavelength observations revealed a compact asymmetric ring-like source morphology. This structure originates from synchrotron emission produced by relativistic plasma located in the immediate vicinity of the black hole. Here we present the corresponding linear-polarimetric EHT images of the center of M87. We find that only a part of the ring is significantly polarized. The resolved fractional linear polarization has a maximum located in the southwest part of the ring, where it rises to the level of ∼15%. The polarization position angles are arranged in a nearly azimuthal pattern. We perform quantitative measurements of relevant polarimetric properties of the compact emission and find evidence for the temporal evolution of the polarized source structure over one week of EHT observations. The details of the polarimetric data reduction and calibration methodology are provided. We carry out the data analysis using multiple independent imaging and modeling techniques, each of which is validated against a suite of synthetic data sets. The gross polarimetric structure and its apparent evolution with time are insensitive to the method used to reconstruct the image. These polarimetric images carry information about the structure of the magnetic fields responsible for the synchrotron emission. Their physical interpretation is discussed in an accompanying publication."
MICHAEL H RAYMOND,First M87 Event Horizon Telescope results. VIII. Magnetic field structure near The Event Horizon,"Event Horizon Telescope (EHT) observations at 230 GHz have now imaged polarized emission around the supermassive black hole in M87 on event-horizon scales. This polarized synchrotron radiation probes the structure of magnetic fields and the plasma properties near the black hole. Here we compare the resolved polarization structure observed by the EHT, along with simultaneous unresolved observations with the Atacama Large Millimeter/submillimeter Array, to expectations from theoretical models. The low fractional linear polarization in the resolved image suggests that the polarization is scrambled on scales smaller than the EHT beam, which we attribute to Faraday rotation internal to the emission region. We estimate the average density n_e ∼ 10^4–7 cm^−3, magnetic field strength B ∼ 1–30 G, and electron temperature T_e ∼ (1–12) × 10^10 K of the radiating plasma in a simple one-zone emission model. We show that the net azimuthal linear polarization pattern may result from organized, poloidal magnetic fields in the emission region. In a quantitative comparison with a large library of simulated polarimetric images from general relativistic magnetohydrodynamic (GRMHD) simulations, we identify a subset of physical models that can explain critical features of the polarimetric EHT observations while producing a relativistic jet of sufficient power. The consistent GRMHD models are all of magnetically arrested accretion disks, where near-horizon magnetic fields are dynamically important. We use the models to infer a mass accretion rate onto the black hole in M87 of (3–20) × 10^−4 M ⊙ yr^−1."
MICHAEL H RAYMOND,Resolving the inner parsec of the blazar J1924–2914 with the event horizon telescope,"The blazar J1924–2914 is a primary Event Horizon Telescope (EHT) calibrator for the Galactic center’s black hole Sagittarius A*. Here we present the first total and linearly polarized intensity images of this source obtained with the unprecedented 20 μas resolution of the EHT. J1924–2914 is a very compact flat-spectrum radio source with strong optical variability and polarization. In April 2017 the source was observed quasi-simultaneously with the EHT (April 5–11), the Global Millimeter VLBI Array (April 3), and the Very Long Baseline Array (April 28), giving a novel view of the source at four observing frequencies, 230, 86, 8.7, and 2.3 GHz. These observations probe jet properties from the subparsec to 100 pc scales. We combine the multifrequency images of J1924–2914 to study the source morphology. We find that the jet exhibits a characteristic bending, with a gradual clockwise rotation of the jet projected position angle of about 90° between 2.3 and 230 GHz. Linearly polarized intensity images of J1924–2914 with the extremely fine resolution of the EHT provide evidence for ordered toroidal magnetic fields in the blazar compact core."
MICHAEL H RAYMOND,A universal power-law prescription for variability from synthetic images of black hole accretion flows,"We present a framework for characterizing the spatiotemporal power spectrum of the variability expected from the horizon-scale emission structure around supermassive black holes, and we apply this framework to a library of general relativistic magnetohydrodynamic (GRMHD) simulations and associated general relativistic ray-traced images relevant for Event Horizon Telescope (EHT) observations of Sgr A*. We find that the variability power spectrum is generically a red-noise process in both the temporal and spatial dimensions, with the peak in power occurring on the longest timescales and largest spatial scales. When both the time-averaged source structure and the spatially integrated light-curve variability are removed, the residual power spectrum exhibits a universal broken power-law behavior. On small spatial frequencies, the residual power spectrum rises as the square of the spatial frequency and is proportional to the variance in the centroid of emission. Beyond some peak in variability power, the residual power spectrum falls as that of the time-averaged source structure, which is similar across simulations; this behavior can be naturally explained if the variability arises from a multiplicative random field that has a steeper high-frequency power-law index than that of the time-averaged source structure. We briefly explore the ability of power spectral variability studies to constrain physical parameters relevant for the GRMHD simulations, which can be scaled to provide predictions for black holes in a range of systems in the optically thin regime. We present specific expectations for the behavior of the M87* and Sgr A* accretion flows as observed by the EHT."
MICHAEL H RAYMOND,Millimeter light curves of Sagittarius A* observed during the 2017 Event Horizon Telescope campaign,"The Event Horizon Telescope (EHT) observed the compact radio source, Sagittarius A* (Sgr A*), in the Galactic Center on 2017 April 5–11 in the 1.3 mm wavelength band. At the same time, interferometric array data from the Atacama Large Millimeter/submillimeter Array and the Submillimeter Array were collected, providing Sgr A* light curves simultaneous with the EHT observations. These data sets, complementing the EHT very long baseline interferometry, are characterized by a cadence and signal-to-noise ratio previously unattainable for Sgr A* at millimeter wavelengths, and they allow for the investigation of source variability on timescales as short as a minute. While most of the light curves correspond to a low variability state of Sgr A*, the April 11 observations follow an X-ray flare and exhibit strongly enhanced variability. All of the light curves are consistent with a red-noise process, with a power spectral density (PSD) slope measured to be between −2 and −3 on timescales between 1 minute and several hours. Our results indicate a steepening of the PSD slope for timescales shorter than 0.3 hr. The spectral energy distribution is flat at 220 GHz, and there are no time lags between the 213 and 229 GHz frequency bands, suggesting low optical depth for the event horizon scale source. We characterize Sgr A*’s variability, highlighting the different behavior observed just after the X-ray flare, and use Gaussian process modeling to extract a decorrelation timescale and a PSD slope. We also investigate the systematic calibration uncertainties by analyzing data from independent data reduction pipelines."
MICHAEL H RAYMOND,Selective dynamical imaging of interferometric data,"Recent developments in very long baseline interferometry (VLBI) have made it possible for the Event Horizon Telescope (EHT) to resolve the innermost accretion flows of the largest supermassive black holes on the sky. The sparse nature of the EHT’s (u, v)-coverage presents a challenge when attempting to resolve highly time-variable sources. We demonstrate that the changing (u, v)-coverage of the EHT can contain regions of time over the course of a single observation that facilitate dynamical imaging. These optimal time regions typically have projected baseline distributions that are approximately angularly isotropic and radially homogeneous. We derive a metric of coverage quality based on baseline isotropy and density that is capable of ranking array configurations by their ability to produce accurate dynamical reconstructions. We compare this metric to existing metrics in the literature and investigate their utility by performing dynamical reconstructions on synthetic data from simulated EHT observations of sources with simple orbital variability. We then use these results to make recommendations for imaging the 2017 EHT Sgr A* data set."
MICHAEL H RAYMOND,First Sagittarius A* Event Horizon Telescope results. VI. Testing the black hole metric,"Astrophysical black holes are expected to be described by the Kerr metric. This is the only stationary, vacuum, axisymmetric metric, without electromagnetic charge, that satisfies Einstein’s equations and does not have pathologies outside of the event horizon. We present new constraints on potential deviations from the Kerr prediction based on 2017 EHT observations of Sagittarius A* (Sgr A*). We calibrate the relationship between the geometrically defined black hole shadow and the observed size of the ring-like images using a library that includes both Kerr and non-Kerr simulations. We use the exquisite prior constraints on the mass-to-distance ratio for Sgr A* to show that the observed image size is within ∼10% of the Kerr predictions. We use these bounds to constrain metrics that are parametrically different from Kerr, as well as the charges of several known spacetimes. To consider alternatives to the presence of an event horizon, we explore the possibility that Sgr A* is a compact object with a surface that either absorbs and thermally reemits incident radiation or partially reflects it. Using the observed image size and the broadband spectrum of Sgr A*, we conclude that a thermal surface can be ruled out and a fully reflective one is unlikely. We compare our results to the broader landscape of gravitational tests. Together with the bounds found for stellar-mass black holes and the M87 black hole, our observations provide further support that the external spacetimes of all black holes are described by the Kerr metric, independent of their mass."
MICHAEL H RAYMOND,Polarimetric properties of Event Horizon Telescope targets from ALMA,"We present the results from a full polarization study carried out with the Atacama Large Millimeter/submillimeter Array (ALMA) during the first Very Long Baseline Interferometry (VLBI) campaign, which was conducted in 2017 April in the λ3 mm and λ1.3 mm bands, in concert with the Global mm-VLBI Array (GMVA) and the Event Horizon Telescope (EHT), respectively. We determine the polarization and Faraday properties of all VLBI targets, including Sgr A*, M87, and a dozen radio-loud active galactic nuclei (AGNs), in the two bands at several epochs in a time window of 10 days. We detect high linear polarization fractions (2%–15%) and large rotation measures (RM &gt; 103.3–105.5 rad m−2), confirming the trends of previous AGN studies at millimeter wavelengths. We find that blazars are more strongly polarized than other AGNs in the sample, while exhibiting (on average) order-of-magnitude lower RM values, consistent with the AGN viewing angle unification scheme. For Sgr A* we report a mean RM of (−4.2 ± 0.3) × 105 rad m−2 at 1.3 mm, consistent with measurements over the past decade and, for the first time, an RM of (–2.1 ± 0.1) × 105 rad m−2 at 3 mm, suggesting that about half of the Faraday rotation at 1.3 mm may occur between the 3 mm photosphere and the 1.3 mm source. We also report the first unambiguous measurement of RM toward the M87 nucleus at millimeter wavelengths, which undergoes significant changes in magnitude and sign reversals on a one year timescale, spanning the range from −1.2 to 0.3 × 105 rad m−2 at 3 mm and −4.1 to 1.5 × 105 rad m−2 at 1.3 mm. Given this time variability, we argue that, unlike the case of Sgr A*, the RM in M87 does not provide an accurate estimate of the mass accretion rate onto the black hole. We put forward a two-component model, comprised of a variable compact region and a static extended region, that can simultaneously explain the polarimetric properties observed by both the EHT (on horizon scales) and ALMA (which observes the combined emission from both components). These measurements provide critical constraints for the calibration, analysis, and interpretation of simultaneously obtained VLBI data with the EHT and GMVA."
MICHAEL H RAYMOND,"First Sagittarius A* Event Horizon Telescope results. IV. Variability, morphology, and black hole mass","In this paper we quantify the temporal variability and image morphology of the horizon-scale emission from Sgr A*, as observed by the EHT in 2017 April at a wavelength of 1.3 mm. We find that the Sgr A* data exhibit variability that exceeds what can be explained by the uncertainties in the data or by the effects of interstellar scattering. The magnitude of this variability can be a substantial fraction of the correlated flux density, reaching ∼100% on some baselines. Through an exploration of simple geometric source models, we demonstrate that ring-like morphologies provide better fits to the Sgr A* data than do other morphologies with comparable complexity. We develop two strategies for fitting static geometric ring models to the time-variable Sgr A* data; one strategy fits models to short segments of data over which the source is static and averages these independent fits, while the other fits models to the full data set using a parametric model for the structural variability power spectrum around the average source structure. Both geometric modeling and image-domain feature extraction techniques determine the ring diameter to be 51.8 ± 2.3 μas (68% credible intervals), with the ring thickness constrained to have an FWHM between ∼30% and 50% of the ring diameter. To bring the diameter measurements to a common physical scale, we calibrate them using synthetic data generated from GRMHD simulations. This calibration constrains the angular size of the gravitational radius to be 4.8_-0.7^+1.4 μas, which we combine with an independent distance measurement from maser parallaxes to determine the mass of Sgr A* to be 4.0_-0.6^+10^6 M⊙."
MICHAEL H RAYMOND,"First Sagittarius A* Event Horizon Telescope results. II. EHT and multiwavelength observations, data processing, and calibration","We present Event Horizon Telescope (EHT) 1.3 mm measurements of the radio source located at the position of the supermassive black hole Sagittarius A* (Sgr A*), collected during the 2017 April 5–11 campaign. The observations were carried out with eight facilities at six locations across the globe. Novel calibration methods are employed to account for Sgr A*'s flux variability. The majority of the 1.3 mm emission arises from horizon scales, where intrinsic structural source variability is detected on timescales of minutes to hours. The effects of interstellar scattering on the image and its variability are found to be subdominant to intrinsic source structure. The calibrated visibility amplitudes, particularly the locations of the visibility minima, are broadly consistent with a blurred ring with a diameter of ∼50 μas, as determined in later works in this series. Contemporaneous multiwavelength monitoring of Sgr A* was performed at 22, 43, and 86 GHz and at near-infrared and X-ray wavelengths. Several X-ray flares from Sgr A* are detected by Chandra, one at low significance jointly with Swift on 2017 April 7 and the other at higher significance jointly with NuSTAR on 2017 April 11. The brighter April 11 flare is not observed simultaneously by the EHT but is followed by a significant increase in millimeter flux variability immediately after the X-ray outburst, indicating a likely connection in the emission physics near the event horizon. We compare Sgr A*’s broadband flux during the EHT campaign to its historical spectral energy distribution and find that both the quiescent emission and flare emission are consistent with its long-term behavior."
MICHAEL H RAYMOND,The polarized image of a synchrotron-emitting ring of gas orbiting a black hole,"Synchrotron radiation from hot gas near a black hole results in a polarized image. The image polarization is determined by effects including the orientation of the magnetic field in the emitting region, relativistic motion of the gas, strong gravitational lensing by the black hole, and parallel transport in the curved spacetime. We explore these effects using a simple model of an axisymmetric, equatorial accretion disk around a Schwarzschild black hole. By using an approximate expression for the null geodesics derived by Beloborodov and conservation of the Walker–Penrose constant, we provide analytic estimates for the image polarization. We test this model using currently favored general relativistic magnetohydrodynamic simulations of M87*, using ring parameters given by the simulations. For a subset of these with modest Faraday effects, we show that the ring model broadly reproduces the polarimetric image morphology. Our model also predicts the polarization evolution for compact flaring regions, such as those observed from Sgr A* with GRAVITY. With suitably chosen parameters, our simple model can reproduce the EVPA pattern and relative polarized intensity in Event Horizon Telescope images of M87*. Under the physically motivated assumption that the magnetic field trails the fluid velocity, this comparison is consistent with the clockwise rotation inferred from total intensity images."
MICHAEL H RAYMOND,First Sagittarius A* Event Horizon Telescope results. I. The shadow of the supermassive black hole in the center of the Milky Way,"We present the first Event Horizon Telescope (EHT) observations of Sagittarius A* (Sgr A*), the Galactic center source associated with a supermassive black hole. These observations were conducted in 2017 using a global interferometric array of eight telescopes operating at a wavelength of λ = 1.3 mm. The EHT data resolve a compact emission region with intrahour variability. A variety of imaging and modeling analyses all support an image that is dominated by a bright, thick ring with a diameter of 51.8 ± 2.3 μas (68% credible interval). The ring has modest azimuthal brightness asymmetry and a comparatively dim interior. Using a large suite of numerical simulations, we demonstrate that the EHT images of Sgr A* are consistent with the expected appearance of a Kerr black hole with mass ∼4 × 106 M ⊙, which is inferred to exist at this location based on previous infrared observations of individual stellar orbits, as well as maser proper-motion studies. Our model comparisons disfavor scenarios where the black hole is viewed at high inclination (i &gt; 50°), as well as nonspinning black holes and those with retrograde accretion disks. Our results provide direct evidence for the presence of a supermassive black hole at the center of the Milky Way, and for the first time we connect the predictions from dynamical measurements of stellar orbits on scales of 103–105 gravitational radii to event-horizon-scale images and variability. Furthermore, a comparison with the EHT results for the supermassive black hole M87* shows consistency with the predictions of general relativity spanning over three orders of magnitude in central mass."
MICHAEL H RAYMOND,Broadband multi-wavelength properties of M87 during the 2017 Event Horizon Telescope campaign,"In 2017, the Event Horizon Telescope (EHT) Collaboration succeeded in capturing the first direct image of the center of the M87 galaxy. The asymmetric ring morphology and size are consistent with theoretical expectations for a weakly accreting supermassive black hole of mass ∼6.5 × 109 M ⊙. The EHTC also partnered with several international facilities in space and on the ground, to arrange an extensive, quasi-simultaneous multi-wavelength campaign. This Letter presents the results and analysis of this campaign, as well as the multi-wavelength data as a legacy data repository. We captured M87 in a historically low state, and the core flux dominates over HST-1 at high energies, making it possible to combine core flux constraints with the more spatially precise very long baseline interferometry data. We present the most complete simultaneous multi-wavelength spectrum of the active nucleus to date, and discuss the complexity and caveats of combining data from different spatial scales into one broadband spectrum. We apply two heuristic, isotropic leptonic single-zone models to provide insight into the basic source properties, but conclude that a structured jet is necessary to explain M87’s spectrum. We can exclude that the simultaneous γ-ray emission is produced via inverse Compton emission in the same region producing the EHT mm-band emission, and further conclude that the γ-rays can only be produced in the inner jets (inward of HST-1) if there are strongly particle-dominated regions. Direct synchrotron emission from accelerated protons and secondaries cannot yet be excluded."
MICHAEL H RAYMOND,Event Horizon Telescope imaging of the archetypal blazar 3C 279 at an extreme 20 microarcsecond resolution,"3C 279 is an archetypal blazar with a prominent radio jet that show broadband flux density variability across the entire electromagnetic spectrum. We use an ultra-high angular resolution technique – global Very Long Baseline Interferometry (VLBI) at 1.3 mm (230 GHz) – to resolve the innermost jet of 3C 279 in order to study its fine-scale morphology close to the jet base where highly variable γ-ray emission is thought to originate, according to various models. The source was observed during four days in April 2017 with the Event Horizon Telescope at 230 GHz, including the phased Atacama Large Millimeter/submillimeter Array (ALMA), at an angular resolution of ∼20 μas (at a redshift of z = 0.536 this corresponds to ∼0.13 pc  ∼ 1700 Schwarzschild radii with a black hole mass M<jats:sub>BH</jats:sub> = 8 × 10^8 M_⊙). Imaging and model-fitting techniques were applied to the data to parameterize the fine-scale source structure and its variation. We find a multicomponent inner jet morphology with the northernmost component elongated perpendicular to the direction of the jet, as imaged at longer wavelengths. The elongated nuclear structure is consistent on all four observing days and across different imaging methods and model-fitting techniques, and therefore appears robust. Owing to its compactness and brightness, we associate the northern nuclear structure as the VLBI “core”. This morphology can be interpreted as either a broad resolved jet base or a spatially bent jet. We also find significant day-to-day variations in the closure phases, which appear most pronounced on the triangles with the longest baselines. Our analysis shows that this variation is related to a systematic change of the source structure. Two inner jet components move non-radially at apparent speeds of ∼15 c and ∼20 c (∼1.3 and ∼1.7 μas day^−1, respectively), which more strongly supports the scenario of traveling shocks or instabilities in a bent, possibly rotating jet. The observed apparent speeds are also coincident with the 3C 279 large-scale jet kinematics observed at longer (cm) wavelengths, suggesting no significant jet acceleration between the 1.3 mm core and the outer jet. The intrinsic brightness temperature of the jet components are ≲10^10 K, a magnitude or more lower than typical values seen at ≥7 mm wavelengths. The low brightness temperature and morphological complexity suggest that the core region of 3C 279 becomes optically thin at short (mm) wavelengths."
MICHAEL H RAYMOND,First M87 Event Horizon Telescope results. II. Array and instrumentation,"The Event Horizon Telescope (EHT) is a very long baseline interferometry (VLBI) array that comprises millimeter- and submillimeter-wavelength telescopes separated by distances comparable to the diameter of the Earth. At a nominal operating wavelength of ~1.3 mm, EHT angular resolution (λ/D) is ~25 μas, which is sufficient to resolve nearby supermassive black hole candidates on spatial and temporal scales that correspond to their event horizons. With this capability, the EHT scientific goals are to probe general relativistic effects in the strong-field regime and to study accretion and relativistic jet formation near the black hole boundary. In this Letter we describe the system design of the EHT, detail the technology and instrumentation that enable observations, and provide measures of its performance. Meeting the EHT science objectives has required several key developments that have facilitated the robust extension of the VLBI technique to EHT observing wavelengths and the production of instrumentation that can be deployed on a heterogeneous array of existing telescopes and facilities. To meet sensitivity requirements, high-bandwidth digital systems were developed that process data at rates of 64 gigabit s−1, exceeding those of currently operating cm-wavelength VLBI arrays by more than an order of magnitude. Associated improvements include the development of phasing systems at array facilities, new receiver installation at several sites, and the deployment of hydrogen maser frequency standards to ensure coherent data capture across the array. These efforts led to the coordination and execution of the first Global EHT observations in 2017 April, and to event-horizon-scale imaging of the supermassive black hole candidate in M87."
MICHAEL H RAYMOND,First M87 Event Horizon Telescope results. I. The shadow of the supermassive black hole,"When surrounded by a transparent emission region, black holes are expected to reveal a dark shadow caused by gravitational light bending and photon capture at the event horizon. To image and study this phenomenon, we have assembled the Event Horizon Telescope, a global very long baseline interferometry array observing at a wavelength of 1.3 mm. This allows us to reconstruct event-horizon-scale images of the supermassive black hole candidate in the center of the giant elliptical galaxy M87. We have resolved the central compact radio source as an asymmetric bright emission ring with a diameter of 42 ± 3 μas, which is circular and encompasses a central depression in brightness with a flux ratio gsim10:1. The emission ring is recovered using different calibration and imaging schemes, with its diameter and width remaining stable over four different observations carried out in different days. Overall, the observed image is consistent with expectations for the shadow of a Kerr black hole as predicted by general relativity. The asymmetry in brightness in the ring can be explained in terms of relativistic beaming of the emission from a plasma rotating close to the speed of light around a black hole. We compare our images to an extensive library of ray-traced general-relativistic magnetohydrodynamic simulations of black holes and derive a central mass of M = (6.5 ± 0.7) × 10^9 M ⊙. Our radio-wave observations thus provide powerful evidence for the presence of supermassive black holes in centers of galaxies and as the central engines of active galactic nuclei. They also present a new tool to explore gravity in its most extreme limit and on a mass scale that was so far not accessible."
MICHAEL H RAYMOND,First Sagittarius A* Event Horizon Telescope results. III. Imaging of the Galactic center supermassive black hole,"We present the first event-horizon-scale images and spatiotemporal analysis of Sgr A* taken with the Event Horizon Telescope in 2017 April at a wavelength of 1.3 mm. Imaging of Sgr A* has been conducted through surveys over a wide range of imaging assumptions using the classical CLEAN algorithm, regularized maximum likelihood methods, and a Bayesian posterior sampling method. Different prescriptions have been used to account for scattering effects by the interstellar medium toward the Galactic center. Mitigation of the rapid intraday variability that characterizes Sgr A* has been carried out through the addition of a “variability noise budget” in the observed visibilities, facilitating the reconstruction of static full-track images. Our static reconstructions of Sgr A* can be clustered into four representative morphologies that correspond to ring images with three different azimuthal brightness distributions and a small cluster that contains diverse nonring morphologies. Based on our extensive analysis of the effects of sparse (u, v)-coverage, source variability, and interstellar scattering, as well as studies of simulated visibility data, we conclude that the Event Horizon Telescope Sgr A* data show compelling evidence for an image that is dominated by a bright ring of emission with a ring diameter of ∼50 μas, consistent with the expected “shadow” of a 4 × 106 M⊙ black hole in the Galactic center located at a distance of 8 kpc."
MICHAEL H RAYMOND,Characterizing and mitigating intraday variability: reconstructing source structure in accreting black holes with mm-VLBI,"The extraordinary physical resolution afforded by the Event Horizon Telescope has opened a window onto the astrophysical phenomena unfolding on horizon scales in two known black holes, M87* and Sgr A*. However, with this leap in resolution has come a new set of practical complications. Sgr A* exhibits intraday variability that violates the assumptions underlying Earth aperture synthesis, limiting traditional image reconstruction methods to short timescales and data sets with very sparse (u, v) coverage. We present a new set of tools to detect and mitigate this variability. We develop a data-driven, model-agnostic procedure to detect and characterize the spatial structure of intraday variability. This method is calibrated against a large set of mock data sets, producing an empirical estimator of the spatial power spectrum of the brightness fluctuations. We present a novel Bayesian noise modeling algorithm that simultaneously reconstructs an average image and statistical measure of the fluctuations about it using a parameterized form for the excess variance in the complex visibilities not otherwise explained by the statistical errors. These methods are validated using a variety of simulated data, including general relativistic magnetohydrodynamic simulations appropriate for Sgr A* and M87*. We find that the reconstructed source structure and variability are robust to changes in the underlying image model. We apply these methods to the 2017 EHT observations of M87*, finding evidence for variability across the EHT observing campaign. The variability mitigation strategies presented are widely applicable to very long baseline interferometry observations of variable sources generally, for which they provide a data-informed averaging procedure and natural characterization of inter-epoch image consistency."
QING XIA,Single virus fingerprinting by widefield interferometric defocus-enhanced mid-infrared photothermal microscopy,"Clinical identification and fundamental study of viruses rely on the detection of viral proteins or viral nucleic acids. Yet, amplification-based and antigen-based methods are not able to provide precise compositional information of individual virions due to small particle size and low-abundance chemical contents (e.g., ~ 5000 proteins in a vesicular stomatitis virus). Here, we report a widefield interferometric defocus-enhanced mid-infrared photothermal (WIDE-MIP) microscope for high-throughput fingerprinting of single viruses. With the identification of feature absorption peaks, WIDE-MIP reveals the contents of viral proteins and nucleic acids in single DNA vaccinia viruses and RNA vesicular stomatitis viruses. Different nucleic acid signatures of thymine and uracil residue vibrations are obtained to differentiate DNA and RNA viruses. WIDE-MIP imaging further reveals an enriched β sheet components in DNA varicella-zoster virus proteins. Together, these advances open a new avenue for compositional analysis of viral vectors and elucidating protein function in an assembled virion."
YUXIN ZHOU,Synaptic remodeling after cortical injury: effects of neuroinflammatory modulation,"The brain is capable of plasticity, so that the structural and functional loss that are caused by cortical injury may recover. Neuroinflammatory response can greatly influence post-injury recovery by modulating synaptic plasticity. In our previous work, mesenchymal derived exosomes were found to promote functional recovery by converting microglia from a pro-inflammatory state to an anti-inflammatory state in aged rhesus monkeys after cortical injury in the primary motor cortex. In the present project, we demonstrated the effects of exosomes on synaptic changes and synapse-microglia interactions after lesion in the same monkeys. To further investigate the effects of modulating neuroinflammation on synaptic changes after injury, we also investigated dietary curcumin, an anti-inflammatory substance, in a separate group of monkeys. Both treatments showed an effect as neuroinflammatory modulators that reduced the density of microglial markers, Iba- 1/P2RY12. However, the cortical injury induced synaptic loss was reversed by the exosome treatment, whereas the other anti-inflammatory treatment, curcumin, did not show the same effect. Our results are consistent with previous study that cortical injury induced synaptic loss and microglia activation. Exosomes can both reduce inflammation and synapse loss after injury, but curcumin only showed anti-inflammatory effects. Overall, these data suggested that exosomes and curcumin had different mechanisms of how to modulate inflammation and synaptic properties to promote recovery after cortical injury."
YUEMING LI,Optically-generated ultrasound for non-invasive brain stimulation,"Neuromodulation plays a crucial role in facilitating research into brain function and enabling treatments for neurological and psychiatric disorders. In brain research, current non-invasive tools face challenges when studying brain sub-regions due to their limited spatial resolution, which can barely reach a scale of 100 μm. Moreover, precise control over the volume of tissue activated (VTA) is needed to effectively target diverse-shaped brain regions, such as ocular dominance columns. Similarly, in disease treatment, the lack of sufficient spatial resolution poses obstacles in restoring normal vision using existing FDA-approved retina prostheses for retinitis pigmentosa. To address these challenges, my thesis work focuses on the development of optically-generated ultrasound devices for non-invasive brain stimulation and implantable retina prostheses. Firstly, to meet the need for non-invasive neuromodulation with ultrahigh precision, we have developed an optically-generated focused ultrasound device. By embedding candle soot nanoparticles in a curved polydimethylsiloxane pad, this device generates a transcranial ultrasound focus at 15 MHz with an ultrahigh lateral resolution of 83 μm. This resolution is two orders of magnitude smaller than conventional transcranial-focused ultrasound, enabling successful submillimeter transcranial stimulation in vivo targeting the mouse motor cortex. Addressing the requirement for a customized VTA in specific brain sub-regions, we have developed an optically-generated Bessel beam ultrasound device. This device was specifically designed to target brain columns with an elongated acoustic focus, and it successfully achieved a VTA with a lateral resolution of 152 μm and an axial resolution of 1.93 mm. The stimulation capability of the device has been confirmed through immunofluorescence imaging, which showed that the stimulation depth in mouse brains reached up to 2.2 mm. Furthermore, in order to address the need for an ultrahigh spatial resolution in retina prosthesis, we have developed an optically-generated ultrasound film as a subretinal prosthesis. In proof-of-concept experiments using blind rat retina, this film has successfully achieved retina stimulation ex vivo. In conclusion, optically-generated ultrasound devices offer promising opportunities for brain science research and disease treatments. They revolutionize non-invasive brain stimulation with ultrahigh precision and customized VTA for studying brain sub-regions. Additionally, they hold the potential for enhancing spatial resolution in retina prostheses, bringing hope to individuals with retinal disorders."
CHI ZHANG,First M87 Event Horizon Telescope results. V. Physical origin of the asymmetric ring,"The Event Horizon Telescope (EHT) has mapped the central compact radio source of the elliptical galaxy M87 at 1.3 mm with unprecedented angular resolution. Here we consider the physical implications of the asymmetric ring seen in the 2017 EHT data. To this end, we construct a large library of models based on general relativistic magnetohydrodynamic (GRMHD) simulations and synthetic images produced by general relativistic ray tracing. We compare the observed visibilities with this library and confirm that the asymmetric ring is consistent with earlier predictions of strong gravitational lensing of synchrotron emission from a hot plasma orbiting near the black hole event horizon. The ring radius and ring asymmetry depend on black hole mass and spin, respectively, and both are therefore expected to be stable when observed in future EHT campaigns. Overall, the observed image is consistent with expectations for the shadow of a spinning Kerr black hole as predicted by general relativity. If the black hole spin and M87's large scale jet are aligned, then the black hole spin vector is pointed away from Earth. Models in our library of non-spinning black holes are inconsistent with the observations as they do not produce sufficiently powerful jets. At the same time, in those models that produce a sufficiently powerful jet, the latter is powered by extraction of black hole spin energy through mechanisms akin to the Blandford-Znajek process. We briefly consider alternatives to a black hole for the central compact object. Analysis of existing EHT polarization data and data taken simultaneously at other wavelengths will soon enable new tests of the GRMHD models, as will future EHT campaigns at 230 and 345 GHz."
CHI ZHANG,Essays on social dynamics and application of machine learning,"This dissertation consists of two essays on social interactions and one exploring the application of machine learning. The first chapter develops a game-theoretic model of favor exchange where one can request indirect favors through a chain of contacts in a network. I study the cooperative behavior fostered by potential collective sanctions, provide a full characterization of “renegotiation-proof” networks and propose a robustness refinement. When the maximum length of contact chains is larger than 3, only star-shaped (i.e. highly centralized) networks achieve highest robustness. I provide empirical evidence of higher centrality in social networks from exploring network data from Indian rural villages. The second chapter studies whether there could be an evolutionary basis for discrimination against traits that are irrelevant to payoff. This study shows that discriminatory behaviors can be derived from non-discriminatory preferences. Each period the population is updated by a deterministic payoff-based update rule and incurs stochastic mutations. Without traits, in the long-run equilibrium agents coordinate on the risk dominant action. When traits are present and subject to change by population update and mutation, the long-run equilibria become the set of Pareto-efficient equilibria, among which a trait can be eliminated from the population and agents choose different actions based on opponent's trait. In an alternative setting where traits cannot mutate or be adjusted and another inferior location is introduced, there exists an update rule that will lead to the Pareto-inefficient outcome where both locations are populated. The third chapter focuses on techniques to predict health outcomes at the census tract level and utilizes Machine Learning methods to reduce overfitting that traditional methods suffer from due to the large number of variables used. This study demonstrates how extensive data on social-demographic characteristics can be used to improve health outcome predictions relative to the previous literature that mostly focuses on using health-related variables. Using survey data from 2010 to 2015, I compare various regularization methods tuned by cross-validation by out-of-sample metrics and obtain high quality estimators of regional prevalence of 12 chronic diseases."
CHI ZHANG,Identification of a novel polyprenylated acylphloroglucinol‑derived SIRT1 inhibitor with cancer‑specific anti-proliferative and invasion-suppressing activities,"SIRT1, a class III histone deacetylase, plays a critical role in regulating cancer cell growth, migration and invasion, which makes it a potential target for cancer therapeutics. In this study, we screened derivatives of several groups of natural products and identified a novel SIRT1 inhibitor JQ-101, a synthetic derivative of the polyprenylated acylphloroglucinol (PPAP) natural products, with an IC(50) for SIRT1 of 30 µM in vitro, with 5-fold higher activity for SIRT1 vs. SIRT2. Exposure of tumor cells to JQ-101 significantly enhanced acetylation of p53 and histone H4K16 at known sites of SIRT1 deacetylation, validating SIRT1 as its cellular target. JQ-101 suppressed cancer cell growth and survival by targeting SIRT1, and also exhibited selective cytotoxicity towards a panel of human tumor cell lines, while producing no toxicity in two normal human cell types at comparable concentrations. JQ-101 induced both apoptosis and cell senescence, and suppressed cancer cell invasion in vitro. In summary, we have identified JQ-101 as a new SIRT1 inhibitor which may have potential application in cancer treatment through its ability to induce tumor cell apoptosis and senescence and suppress cancer cell invasion."
CHI ZHANG,Plasmon-enhanced stimulated Raman scattering microscopy with single-molecule detection sensitivity,"Stimulated Raman scattering (SRS) microscopy allows for high-speed label-free chemical imaging of biomedical systems. The imaging sensitivity of SRS microscopy is limited to ~10 mM for endogenous biomolecules. Electronic pre-resonant SRS allows detection of sub-micromolar chromophores. However, label-free SRS detection of single biomolecules having extremely small Raman cross-sections (~10-30 cm2 sr-1) remains unreachable. Here, we demonstrate plasmon-enhanced stimulated Raman scattering (PESRS) microscopy with single-molecule detection sensitivity. Incorporating pico-Joule laser excitation, background subtraction, and a denoising algorithm, we obtain robust single-pixel SRS spectra exhibiting single-molecule events, verified by using two isotopologues of adenine and further confirmed by digital blinking and bleaching in the temporal domain. To demonstrate the capability of PESRS for biological applications, we utilize PESRS to map adenine released from bacteria due to starvation stress. PESRS microscopy holds the promise for ultrasensitive detection and rapid mapping of molecular events in chemical and biomedical systems."
CHI ZHANG,First Sagittarius A* Event Horizon Telescope results. V. Testing astrophysical models of the galactic center black hole,"In this paper we provide a first physical interpretation for the Event Horizon Telescope's (EHT) 2017 observations of Sgr A*. Our main approach is to compare resolved EHT data at 230 GHz and unresolved non-EHT observations from radio to X-ray wavelengths to predictions from a library of models based on time-dependent general relativistic magnetohydrodynamics simulations, including aligned, tilted, and stellar-wind-fed simulations; radiative transfer is performed assuming both thermal and nonthermal electron distribution functions. We test the models against 11 constraints drawn from EHT 230 GHz data and observations at 86 GHz, 2.2 μm, and in the X-ray. All models fail at least one constraint. Light-curve variability provides a particularly severe constraint, failing nearly all strongly magnetized (magnetically arrested disk (MAD)) models and a large fraction of weakly magnetized models. A number of models fail only the variability constraints. We identify a promising cluster of these models, which are MAD and have inclination i ≤ 30°. They have accretion rate (5.2–9.5) × 10−9 M ⊙ yr−1, bolometric luminosity (6.8–9.2) × 1035 erg s−1, and outflow power (1.3–4.8) × 1038 erg s−1. We also find that all models with i ≥ 70° fail at least two constraints, as do all models with equal ion and electron temperature; exploratory, nonthermal model sets tend to have higher 2.2 μm flux density; and the population of cold electrons is limited by X-ray constraints due to the risk of bremsstrahlung overproduction. Finally, we discuss physical and numerical limitations of the models, highlighting the possible importance of kinetic effects and duration of the simulations."
CHI ZHANG,Resolving the inner parsec of the blazar J1924–2914 with the event horizon telescope,"The blazar J1924–2914 is a primary Event Horizon Telescope (EHT) calibrator for the Galactic center’s black hole Sagittarius A*. Here we present the first total and linearly polarized intensity images of this source obtained with the unprecedented 20 μas resolution of the EHT. J1924–2914 is a very compact flat-spectrum radio source with strong optical variability and polarization. In April 2017 the source was observed quasi-simultaneously with the EHT (April 5–11), the Global Millimeter VLBI Array (April 3), and the Very Long Baseline Array (April 28), giving a novel view of the source at four observing frequencies, 230, 86, 8.7, and 2.3 GHz. These observations probe jet properties from the subparsec to 100 pc scales. We combine the multifrequency images of J1924–2914 to study the source morphology. We find that the jet exhibits a characteristic bending, with a gradual clockwise rotation of the jet projected position angle of about 90° between 2.3 and 230 GHz. Linearly polarized intensity images of J1924–2914 with the extremely fine resolution of the EHT provide evidence for ordered toroidal magnetic fields in the blazar compact core."
CHI ZHANG,A universal power-law prescription for variability from synthetic images of black hole accretion flows,"We present a framework for characterizing the spatiotemporal power spectrum of the variability expected from the horizon-scale emission structure around supermassive black holes, and we apply this framework to a library of general relativistic magnetohydrodynamic (GRMHD) simulations and associated general relativistic ray-traced images relevant for Event Horizon Telescope (EHT) observations of Sgr A*. We find that the variability power spectrum is generically a red-noise process in both the temporal and spatial dimensions, with the peak in power occurring on the longest timescales and largest spatial scales. When both the time-averaged source structure and the spatially integrated light-curve variability are removed, the residual power spectrum exhibits a universal broken power-law behavior. On small spatial frequencies, the residual power spectrum rises as the square of the spatial frequency and is proportional to the variance in the centroid of emission. Beyond some peak in variability power, the residual power spectrum falls as that of the time-averaged source structure, which is similar across simulations; this behavior can be naturally explained if the variability arises from a multiplicative random field that has a steeper high-frequency power-law index than that of the time-averaged source structure. We briefly explore the ability of power spectral variability studies to constrain physical parameters relevant for the GRMHD simulations, which can be scaled to provide predictions for black holes in a range of systems in the optically thin regime. We present specific expectations for the behavior of the M87* and Sgr A* accretion flows as observed by the EHT."
CHI ZHANG,Millimeter light curves of Sagittarius A* observed during the 2017 Event Horizon Telescope campaign,"The Event Horizon Telescope (EHT) observed the compact radio source, Sagittarius A* (Sgr A*), in the Galactic Center on 2017 April 5–11 in the 1.3 mm wavelength band. At the same time, interferometric array data from the Atacama Large Millimeter/submillimeter Array and the Submillimeter Array were collected, providing Sgr A* light curves simultaneous with the EHT observations. These data sets, complementing the EHT very long baseline interferometry, are characterized by a cadence and signal-to-noise ratio previously unattainable for Sgr A* at millimeter wavelengths, and they allow for the investigation of source variability on timescales as short as a minute. While most of the light curves correspond to a low variability state of Sgr A*, the April 11 observations follow an X-ray flare and exhibit strongly enhanced variability. All of the light curves are consistent with a red-noise process, with a power spectral density (PSD) slope measured to be between −2 and −3 on timescales between 1 minute and several hours. Our results indicate a steepening of the PSD slope for timescales shorter than 0.3 hr. The spectral energy distribution is flat at 220 GHz, and there are no time lags between the 213 and 229 GHz frequency bands, suggesting low optical depth for the event horizon scale source. We characterize Sgr A*’s variability, highlighting the different behavior observed just after the X-ray flare, and use Gaussian process modeling to extract a decorrelation timescale and a PSD slope. We also investigate the systematic calibration uncertainties by analyzing data from independent data reduction pipelines."
CHI ZHANG,First Sagittarius A* Event Horizon Telescope results. VI. Testing the black hole metric,"Astrophysical black holes are expected to be described by the Kerr metric. This is the only stationary, vacuum, axisymmetric metric, without electromagnetic charge, that satisfies Einstein’s equations and does not have pathologies outside of the event horizon. We present new constraints on potential deviations from the Kerr prediction based on 2017 EHT observations of Sagittarius A* (Sgr A*). We calibrate the relationship between the geometrically defined black hole shadow and the observed size of the ring-like images using a library that includes both Kerr and non-Kerr simulations. We use the exquisite prior constraints on the mass-to-distance ratio for Sgr A* to show that the observed image size is within ∼10% of the Kerr predictions. We use these bounds to constrain metrics that are parametrically different from Kerr, as well as the charges of several known spacetimes. To consider alternatives to the presence of an event horizon, we explore the possibility that Sgr A* is a compact object with a surface that either absorbs and thermally reemits incident radiation or partially reflects it. Using the observed image size and the broadband spectrum of Sgr A*, we conclude that a thermal surface can be ruled out and a fully reflective one is unlikely. We compare our results to the broader landscape of gravitational tests. Together with the bounds found for stellar-mass black holes and the M87 black hole, our observations provide further support that the external spacetimes of all black holes are described by the Kerr metric, independent of their mass."
CHI ZHANG,"First Sagittarius A* Event Horizon Telescope results. IV. Variability, morphology, and black hole mass","In this paper we quantify the temporal variability and image morphology of the horizon-scale emission from Sgr A*, as observed by the EHT in 2017 April at a wavelength of 1.3 mm. We find that the Sgr A* data exhibit variability that exceeds what can be explained by the uncertainties in the data or by the effects of interstellar scattering. The magnitude of this variability can be a substantial fraction of the correlated flux density, reaching ∼100% on some baselines. Through an exploration of simple geometric source models, we demonstrate that ring-like morphologies provide better fits to the Sgr A* data than do other morphologies with comparable complexity. We develop two strategies for fitting static geometric ring models to the time-variable Sgr A* data; one strategy fits models to short segments of data over which the source is static and averages these independent fits, while the other fits models to the full data set using a parametric model for the structural variability power spectrum around the average source structure. Both geometric modeling and image-domain feature extraction techniques determine the ring diameter to be 51.8 ± 2.3 μas (68% credible intervals), with the ring thickness constrained to have an FWHM between ∼30% and 50% of the ring diameter. To bring the diameter measurements to a common physical scale, we calibrate them using synthetic data generated from GRMHD simulations. This calibration constrains the angular size of the gravitational radius to be 4.8_-0.7^+1.4 μas, which we combine with an independent distance measurement from maser parallaxes to determine the mass of Sgr A* to be 4.0_-0.6^+10^6 M⊙."
CHI ZHANG,"First Sagittarius A* Event Horizon Telescope results. II. EHT and multiwavelength observations, data processing, and calibration","We present Event Horizon Telescope (EHT) 1.3 mm measurements of the radio source located at the position of the supermassive black hole Sagittarius A* (Sgr A*), collected during the 2017 April 5–11 campaign. The observations were carried out with eight facilities at six locations across the globe. Novel calibration methods are employed to account for Sgr A*'s flux variability. The majority of the 1.3 mm emission arises from horizon scales, where intrinsic structural source variability is detected on timescales of minutes to hours. The effects of interstellar scattering on the image and its variability are found to be subdominant to intrinsic source structure. The calibrated visibility amplitudes, particularly the locations of the visibility minima, are broadly consistent with a blurred ring with a diameter of ∼50 μas, as determined in later works in this series. Contemporaneous multiwavelength monitoring of Sgr A* was performed at 22, 43, and 86 GHz and at near-infrared and X-ray wavelengths. Several X-ray flares from Sgr A* are detected by Chandra, one at low significance jointly with Swift on 2017 April 7 and the other at higher significance jointly with NuSTAR on 2017 April 11. The brighter April 11 flare is not observed simultaneously by the EHT but is followed by a significant increase in millimeter flux variability immediately after the X-ray outburst, indicating a likely connection in the emission physics near the event horizon. We compare Sgr A*’s broadband flux during the EHT campaign to its historical spectral energy distribution and find that both the quiescent emission and flare emission are consistent with its long-term behavior."
CHI ZHANG,First Sagittarius A* Event Horizon Telescope results. I. The shadow of the supermassive black hole in the center of the Milky Way,"We present the first Event Horizon Telescope (EHT) observations of Sagittarius A* (Sgr A*), the Galactic center source associated with a supermassive black hole. These observations were conducted in 2017 using a global interferometric array of eight telescopes operating at a wavelength of λ = 1.3 mm. The EHT data resolve a compact emission region with intrahour variability. A variety of imaging and modeling analyses all support an image that is dominated by a bright, thick ring with a diameter of 51.8 ± 2.3 μas (68% credible interval). The ring has modest azimuthal brightness asymmetry and a comparatively dim interior. Using a large suite of numerical simulations, we demonstrate that the EHT images of Sgr A* are consistent with the expected appearance of a Kerr black hole with mass ∼4 × 106 M ⊙, which is inferred to exist at this location based on previous infrared observations of individual stellar orbits, as well as maser proper-motion studies. Our model comparisons disfavor scenarios where the black hole is viewed at high inclination (i &gt; 50°), as well as nonspinning black holes and those with retrograde accretion disks. Our results provide direct evidence for the presence of a supermassive black hole at the center of the Milky Way, and for the first time we connect the predictions from dynamical measurements of stellar orbits on scales of 103–105 gravitational radii to event-horizon-scale images and variability. Furthermore, a comparison with the EHT results for the supermassive black hole M87* shows consistency with the predictions of general relativity spanning over three orders of magnitude in central mass."
CHI ZHANG,Prospects for beyond the standard model physics searches at the deep underground neutrino experiment: DUNE collaboration,"The Deep Underground Neutrino Experiment (DUNE) will be a powerful tool for a variety of physics topics. The high-intensity proton beams provide a large neutrino flux, sampled by a near detector system consisting of a combination of capable precision detectors, and by the massive far detector system located deep underground. This configuration sets up DUNE as a machine for discovery, as it enables opportunities not only to perform precision neutrino measurements that may uncover deviations from the present three-flavor mixing paradigm, but also to discover new particles and unveil new interactions and symmetries beyond those predicted in the Standard Model (SM). Of the many potential beyond the Standard Model (BSM) topics DUNE will probe, this paper presents a selection of studies quantifying DUNE's sensitivities to sterile neutrino mixing, heavy neutral leptons, non-standard interactions, CPT symmetry violation, Lorentz invariance violation, neutrino trident production, dark matter from both beam induced and cosmogenic sources, baryon number violation, and other new physics topics that complement those at high-energy colliders and significantly extend the present reach."
CHI ZHANG,Identification and reconstruction of low-energy electrons in the ProtoDUNE-SP detector,
CHI ZHANG,The long-baseline neutrino experiment: exploring fundamental symmetries of the universe,"The preponderance of matter over antimatter in the early Universe, the dynamics of the supernova bursts that produced the heavy elements necessary for life and whether protons eventually decay --- these mysteries at the forefront of particle physics and astrophysics are key to understanding the early evolution of our Universe, its current state and its eventual fate. The Long-Baseline Neutrino Experiment (LBNE) represents an extensively developed plan for a world-class experiment dedicated to addressing these questions. LBNE is conceived around three central components: (1) a new, high-intensity neutrino source generated from a megawatt-class proton accelerator at Fermi National Accelerator Laboratory, (2) a near neutrino detector just downstream of the source, and (3) a massive liquid argon time-projection chamber deployed as a far detector deep underground at the Sanford Underground Research Facility. This facility, located at the site of the former Homestake Mine in Lead, South Dakota, is approximately 1,300 km from the neutrino source at Fermilab -- a distance (baseline) that delivers optimal sensitivity to neutrino charge-parity symmetry violation and mass ordering effects. This ambitious yet cost-effective design incorporates scalability and flexibility and can accommodate a variety of upgrades and contributions. With its exceptional combination of experimental configuration, technical capabilities, and potential for transformative discoveries, LBNE promises to be a vital facility for the field of particle physics worldwide, providing physicists from around the globe with opportunities to collaborate in a twenty to thirty year program of exciting science. In this document we provide a comprehensive overview of LBNE's scientific objectives, its place in the landscape of neutrino physics worldwide, the technologies it will incorporate and the capabilities it will possess."
CHI ZHANG,Scintillation light detection in the 6-m drift-length ProtoDUNE Dual Phase liquid argon TPC,"DUNE is a dual-site experiment for long-baseline neutrino oscillation studies, neutrino astrophysics and nucleon decay searches. ProtoDUNE Dual Phase (DP) is a 6  ×  6  ×  6 m 3 liquid argon time-projection-chamber (LArTPC) that recorded cosmic-muon data at the CERN Neutrino Platform in 2019-2020 as a prototype of the DUNE Far Detector. Charged particles propagating through the LArTPC produce ionization and scintillation light. The scintillation light signal in these detectors can provide the trigger for non-beam events. In addition, it adds precise timing capabilities and improves the calorimetry measurements. In ProtoDUNE-DP, scintillation and electroluminescence light produced by cosmic muons in the LArTPC is collected by photomultiplier tubes placed up to 7 m away from the ionizing track. In this paper, the ProtoDUNE-DP photon detection system performance is evaluated with a particular focus on the different wavelength shifters, such as PEN and TPB, and the use of Xe-doped LAr, considering its future use in giant LArTPCs. The scintillation light production and propagation processes are analyzed and a comparison of simulation to data is performed, improving understanding of the liquid argon properties."
CHI ZHANG,Event Horizon Telescope imaging of the archetypal blazar 3C 279 at an extreme 20 microarcsecond resolution,"3C 279 is an archetypal blazar with a prominent radio jet that show broadband flux density variability across the entire electromagnetic spectrum. We use an ultra-high angular resolution technique – global Very Long Baseline Interferometry (VLBI) at 1.3 mm (230 GHz) – to resolve the innermost jet of 3C 279 in order to study its fine-scale morphology close to the jet base where highly variable γ-ray emission is thought to originate, according to various models. The source was observed during four days in April 2017 with the Event Horizon Telescope at 230 GHz, including the phased Atacama Large Millimeter/submillimeter Array (ALMA), at an angular resolution of ∼20 μas (at a redshift of z = 0.536 this corresponds to ∼0.13 pc  ∼ 1700 Schwarzschild radii with a black hole mass M<jats:sub>BH</jats:sub> = 8 × 10^8 M_⊙). Imaging and model-fitting techniques were applied to the data to parameterize the fine-scale source structure and its variation. We find a multicomponent inner jet morphology with the northernmost component elongated perpendicular to the direction of the jet, as imaged at longer wavelengths. The elongated nuclear structure is consistent on all four observing days and across different imaging methods and model-fitting techniques, and therefore appears robust. Owing to its compactness and brightness, we associate the northern nuclear structure as the VLBI “core”. This morphology can be interpreted as either a broad resolved jet base or a spatially bent jet. We also find significant day-to-day variations in the closure phases, which appear most pronounced on the triangles with the longest baselines. Our analysis shows that this variation is related to a systematic change of the source structure. Two inner jet components move non-radially at apparent speeds of ∼15 c and ∼20 c (∼1.3 and ∼1.7 μas day^−1, respectively), which more strongly supports the scenario of traveling shocks or instabilities in a bent, possibly rotating jet. The observed apparent speeds are also coincident with the 3C 279 large-scale jet kinematics observed at longer (cm) wavelengths, suggesting no significant jet acceleration between the 1.3 mm core and the outer jet. The intrinsic brightness temperature of the jet components are ≲10^10 K, a magnitude or more lower than typical values seen at ≥7 mm wavelengths. The low brightness temperature and morphological complexity suggest that the core region of 3C 279 becomes optically thin at short (mm) wavelengths."
CHI ZHANG,First M87 Event Horizon Telescope results. I. The shadow of the supermassive black hole,"When surrounded by a transparent emission region, black holes are expected to reveal a dark shadow caused by gravitational light bending and photon capture at the event horizon. To image and study this phenomenon, we have assembled the Event Horizon Telescope, a global very long baseline interferometry array observing at a wavelength of 1.3 mm. This allows us to reconstruct event-horizon-scale images of the supermassive black hole candidate in the center of the giant elliptical galaxy M87. We have resolved the central compact radio source as an asymmetric bright emission ring with a diameter of 42 ± 3 μas, which is circular and encompasses a central depression in brightness with a flux ratio gsim10:1. The emission ring is recovered using different calibration and imaging schemes, with its diameter and width remaining stable over four different observations carried out in different days. Overall, the observed image is consistent with expectations for the shadow of a Kerr black hole as predicted by general relativity. The asymmetry in brightness in the ring can be explained in terms of relativistic beaming of the emission from a plasma rotating close to the speed of light around a black hole. We compare our images to an extensive library of ray-traced general-relativistic magnetohydrodynamic simulations of black holes and derive a central mass of M = (6.5 ± 0.7) × 10^9 M ⊙. Our radio-wave observations thus provide powerful evidence for the presence of supermassive black holes in centers of galaxies and as the central engines of active galactic nuclei. They also present a new tool to explore gravity in its most extreme limit and on a mass scale that was so far not accessible."
CHI ZHANG,Polarized x-rays constrain the disk-jet geometry in the black hole x-ray binary Cygnus X-1,"A black hole x-ray binary (XRB) system forms when gas is stripped from a normal star and accretes onto a black hole, which heats the gas sufficiently to emit x-rays. We report a polarimetric observation of the XRB Cygnus X-1 using the Imaging X-ray Polarimetry Explorer. The electric field position angle aligns with the outflowing jet, indicating that the jet is launched from the inner x-ray-emitting region. The polarization degree is 4.01 ± 0.20% at 2 to 8 kiloelectronvolts, implying that the accretion disk is viewed closer to edge-on than the binary orbit. These observations reveal that hot x-ray-emitting plasma is spatially extended in a plane perpendicular to, not parallel to, the jet axis."
CHI ZHANG,First Sagittarius A* Event Horizon Telescope results. III. Imaging of the Galactic center supermassive black hole,"We present the first event-horizon-scale images and spatiotemporal analysis of Sgr A* taken with the Event Horizon Telescope in 2017 April at a wavelength of 1.3 mm. Imaging of Sgr A* has been conducted through surveys over a wide range of imaging assumptions using the classical CLEAN algorithm, regularized maximum likelihood methods, and a Bayesian posterior sampling method. Different prescriptions have been used to account for scattering effects by the interstellar medium toward the Galactic center. Mitigation of the rapid intraday variability that characterizes Sgr A* has been carried out through the addition of a “variability noise budget” in the observed visibilities, facilitating the reconstruction of static full-track images. Our static reconstructions of Sgr A* can be clustered into four representative morphologies that correspond to ring images with three different azimuthal brightness distributions and a small cluster that contains diverse nonring morphologies. Based on our extensive analysis of the effects of sparse (u, v)-coverage, source variability, and interstellar scattering, as well as studies of simulated visibility data, we conclude that the Event Horizon Telescope Sgr A* data show compelling evidence for an image that is dominated by a bright ring of emission with a ring diameter of ∼50 μas, consistent with the expected “shadow” of a 4 × 106 M⊙ black hole in the Galactic center located at a distance of 8 kpc."
CHI ZHANG,Characterizing and mitigating intraday variability: reconstructing source structure in accreting black holes with mm-VLBI,"The extraordinary physical resolution afforded by the Event Horizon Telescope has opened a window onto the astrophysical phenomena unfolding on horizon scales in two known black holes, M87* and Sgr A*. However, with this leap in resolution has come a new set of practical complications. Sgr A* exhibits intraday variability that violates the assumptions underlying Earth aperture synthesis, limiting traditional image reconstruction methods to short timescales and data sets with very sparse (u, v) coverage. We present a new set of tools to detect and mitigate this variability. We develop a data-driven, model-agnostic procedure to detect and characterize the spatial structure of intraday variability. This method is calibrated against a large set of mock data sets, producing an empirical estimator of the spatial power spectrum of the brightness fluctuations. We present a novel Bayesian noise modeling algorithm that simultaneously reconstructs an average image and statistical measure of the fluctuations about it using a parameterized form for the excess variance in the complex visibilities not otherwise explained by the statistical errors. These methods are validated using a variety of simulated data, including general relativistic magnetohydrodynamic simulations appropriate for Sgr A* and M87*. We find that the reconstructed source structure and variability are robust to changes in the underlying image model. We apply these methods to the 2017 EHT observations of M87*, finding evidence for variability across the EHT observing campaign. The variability mitigation strategies presented are widely applicable to very long baseline interferometry observations of variable sources generally, for which they provide a data-informed averaging procedure and natural characterization of inter-epoch image consistency."
SAMANTHA SCHILDROTH,Metal mixtures and the role of iron status in early adolescent cognition,"Children are commonly exposed to metals in the environment, particularly those living in proximity to steel-producing ferroalloy industry. Exposure to metals, including lead (Pb), manganese (Mn), chromium (Cr) and copper (Cu), impacts neurodevelopment, but less attention has been given to examining associations of metal mixtures with cognitive function in children. Further, recent epidemiological studies have identified iron (Fe) status as a modifier of metals-induced neurotoxicity, such that adverse associations of metals (e.g., Mn or Pb) tend to be stronger among children with Fe deficiency. However, no study to date has quantified the modifying or mediating role of Fe status on a complex metal mixture in relation to any neurodevelopmental outcome. We used data from the Public Health Impact of Metals Exposure Study (PHIME), a cohort of 720 Italian adolescents (10–14 years) to quantify the association of an industry-relevant metal mixture (Pb, Mn, Cr, Cu) with neurodevelopment, and examine the role of Fe status as a modifier or mediator of these associations. Metals were measured in blood (Pb) or hair (Mn, Cr, Cu) using inductively-coupled plasma mass spectrometry, and Fe status was assessed using three clinically relevant biomarkers (ferritin, hemoglobin, transferrin) measured in whole blood or serum using luminescent or immunoassays. Of note, there was no indication of Fe deficiency in the study population. In Chapter 2, we identified associations of the metal mixture with verbal learning and memory, measured using the California Verbal Learning Test for Children (CVLT-C), using Bayesian Kernel Machine Regression (BKMR). In adjusted models, we found that the mixture was jointly associated with higher scores for the recall trials: compared to the 50th percentile, the 90th percentile of the mixture was associated with a 0.12 standard deviation increase (95% credible interval [CI]= -0.27, 0.50) in the number of words recalled on trial 5 recall, indicating better cognitive performance. This association was driven primarily by Cu, which was further modified by Fe status: the beneficial association of Cu with recall was stronger at increasing percentiles of ferritin. In Chapter 3, we found that the metal mixture was jointly associated with self-reported attention-related behaviors measured on the Conners Rating Scales. For example, the 90th percentile of the mixture, compared to the 50th percentile, was associated with a 4.1% increase (β= 0.04; 95% CI= 0.00, 0.08) in self-reported inattention T-scores in BKMR models, reflecting worse cognitive performance. These associations were driven primarily by Mn, though there was no indication of modification by Fe status. Lastly, in Chapter 4, we quantified the mediating role of Fe status on the association between the metal mixture and CVLT-C scores using the newly developed BKMR Causal Mediation Analysis (BKMR-CMA). Though the metal mixture was associated with aspects of Fe status (e.g., ferritin), there was no evidence that Fe status mediated the association between the metal mixture and CVLT-C scores. Overall, the findings from this dissertation suggest that an industry-relevant metal mixture can impact aspects of neurodevelopment, including learning, memory and attention-related behaviors, and that Fe status may be a modifier of these associations. These findings have significant implications for potential public health interventions aimed at improving cognitive development in adolescents. However, the generalizability of our findings in a Fe-replete population of healthy adolescents may be limited, and further research in Fe-deficient populations is warranted."
PING HU,Pixel-level video understanding with efficient deep models,"The ability to understand videos at the level of pixels plays a key role in a wide range of computer vision applications. For example, a robot or autonomous vehicle relies on classifying each pixel in the video stream into semantic categories to holistically understand the surrounding environment, and video editing software needs to exploit the spatiotemporal context of video pixels to generate various visual effects. Despite the great progress of Deep Learning (DL) techniques, applying DL-based vision models to process video pixels remains practically challenging, due to the high volume of video data and the compute-intensive design of DL approaches. In this thesis, we aim to design efficient and robust deep models for pixel-level video understanding of high-level semantics, mid-level grouping, and low-level interpolation. Toward this goal, in Part I, we address the semantic analysis of video pixels with the task of Video Semantic Segmentation (VSS), which aims to assign pixel-level semantic labels to video frames. We introduce methods that utilize temporal redundancy and context to efficiently recognize video pixels without sacrificing performance. Extensive experiments on various datasets demonstrate our methods' effectiveness and efficiency on both common GPUs and edge devices. Then, in Part II, we show that pixel-level motion patterns help to differentiate video objects from their background. In particular, we propose a fast and efficient contour-based algorithm to group and separate motion patterns for video objects. Furthermore, we present learning-based models to solve the tracking of objects across frames. We show that by explicitly separating the object segmentation and object tracking problems, our framework achieves efficiency during both training and inference. Finally, in Part III, we study the temporal interpolation of pixels given their spatial-temporal context. We show that intermediate video frames can be inferred via interpolation in a very efficient way, by introducing the many-to-many splatting framework that can quickly warp and fuse pixels at any number of arbitrary intermediate time steps. We also propose a dynamic refinement mechanism to further improve the interpolation quality by reducing redundant computation. Evaluation on various types of datasets shows that our method can interpolate videos with state-of-the-art quality and efficiency. To summarize, we discuss and propose efficient pipelines for pixel-level video understanding tasks across high-level semantics, mid-level grouping, and low-level interpolation. The proposed models can contribute to tackling a wide range of real-world video perception and understanding problems in future research."
FEIYANG DENG,CRISPR-Cas-mediated multianalyte synthetic urine biomarker test for portable diagnostics,
TAO ZHANG,Using Electronic Drug Monitor Feedback to Improve Adherence to Antiretroviral Therapy Among HIV-Positive Patients in China,"Effective antiretroviral therapy (ART) requires excellent adherence. Little is known about how to improve ART adherence in many HIV/AIDS-affected countries, including China. We therefore assessed an adherence intervention among HIV-positive patients in southwestern China. Eighty subjects were enrolled and monitored for 6 months. Sixty-eight remaining subjects were randomized to intervention/control arms. In months 7–12, intervention subjects were counseled using EDM feedback; controls continued with standard of care. Among randomized subjects, mean adherence and CD4 count were 86.8 vs. 83.8% and 297 vs. 357 cells/μl in intervention vs. control subjects, respectively. At month 12, among 64 subjects who completed the trial, mean adherence had risen significantly among intervention subjects to 96.5% but remained unchanged in controls. Mean CD4 count rose by 90 cells/μl and declined by 9 cells/μl among intervention and control subjects, respectively. EDM feedback as a counseling tool appears promising for management of HIV and other chronic diseases."
TAO ZHANG,Development and validation of a prognostic risk score system for COVID-19 inpatients: a multi-center retrospective study in China,"Coronavirus disease 2019 (COVID-19) has become a worldwide pandemic. Hospitalized patients of COVID-19 suffer from a high mortality rate, motivating the development of convenient and practical methods that allow clinicians to promptly identify high-risk patients. Here, we have developed a risk score using clinical data from 1479 inpatients admitted to Tongji Hospital, Wuhan, China (development cohort) and externally validated with data from two other centers: 141 inpatients from Jinyintan Hospital, Wuhan, China (validation cohort 1) and 432 inpatients from The Third People's Hospital of Shenzhen, Shenzhen, China (validation cohort 2). The risk score is based on three biomarkers that are readily available in routine blood samples and can easily be translated into a probability of death. The risk score can predict the mortality of individual patients more than 12 d in advance with more than 90% accuracy across all cohorts. Moreover, the Kaplan-Meier score shows that patients can be clearly differentiated upon admission as low, intermediate, or high risk, with an area under the curve (AUC) score of 0.9551. In summary, a simple risk score has been validated to predict death in patients infected with severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2); it has also been validated in independent cohorts."
TAO ZHANG,Targeting oncoprotein translation with rocaglates in MYC-driven lymphoma,"MYC-driven lymphomas, especially those with concurrent MYC and BCL2 dysregulation, are currently a challenge in clinical practice due to rapid disease progression, resistance to standard chemotherapy and high risk of refractory disease. MYC plays a central role by coordinating hyperactive protein synthesis with upregulated transcription in order to support rapid proliferation of tumor cells. Translation initiation inhibitor rocaglates have been identified as the most potent drugs in MYC-driven lymphomas as they efficiently inhibit MYC expression and tumor cell viability. We found that this class of compounds can overcome eIF4A abundance by stabilizing target mRNA-eIF4A interaction that directly prevents translation. Proteome-wide quantification demonstrated selective repression of multiple critical oncoproteins in addition to MYC in B cell lymphoma including NEK2, MCL1, AURKA, PLK1, and several transcription factors that are generally considered undruggable. Finally, (−)-SDS-1–021, the most promising synthetic rocaglate, was confirmed to be highly potent as a single agent, and displayed significant synergy with the BCL2 inhibitor ABT199 in inhibiting tumor growth and survival in primary lymphoma cells in vitro and in patient-derived xenograft mouse models. Overall, our findings support the strategy of using rocaglates to target oncoprotein synthesis in MYC-driven lymphomas."
TAO ZHANG,Predicting substance use disorder using long-term ADHD medication records in Truven,"About 20% of individuals with attention deficit hyperactivity disorder are first diagnosed during adolescence. While preclinical experiments suggest that adolescent-onset exposure to attention deficit hyperactivity disorder medication is an important factor in the development of substance use disorder phenotypes in adulthood, the long-term impact of attention deficit hyperactivity disorder medication initiated during adolescence has been largely unexplored in humans. Our analysis of 11,624 adolescent enrollees with attention deficit hyperactivity disorder in the Truven database indicates that temporal medication features, rather than stationary features, are the most important factors on the health consequences related to substance use disorder and attention deficit hyperactivity disorder medication initiation during adolescence."
TAO ZHANG,The H-index of a network node and its relation to degree and coreness,"Identifying influential nodes in dynamical processes is crucial in understanding network structure and function. Degree, H-index and coreness are widely used metrics, but previously treated as unrelated. Here we show their relation by constructing an operator , in terms of which degree, H-index and coreness are the initial, intermediate and steady states of the sequences, respectively. We obtain a family of H-indices that can be used to measure a node’s importance. We also prove that the convergence to coreness can be guaranteed even under an asynchronous updating process, allowing a decentralized local method of calculating a node’s coreness in large-scale evolving networks. Numerical analyses of the susceptible-infected-removed spreading dynamics on disparate real networks suggest that the H-index is a good tradeoff that in many cases can better quantify node influence than either degree or coreness."
TAO ZHANG,Omicron infection following vaccination enhances a broad spectrum of immune responses dependent on infection history,"Pronounced immune escape by the SARS-CoV-2 Omicron variant has resulted in many individuals possessing hybrid immunity, generated through a combination of vaccination and infection. Concerns have been raised that omicron breakthrough infections in triple-vaccinated individuals result in poor induction of omicron-specific immunity, and that prior SARS-CoV-2 infection is associated with immune dampening. Taking a broad and comprehensive approach, we characterize mucosal and blood immunity to spike and non-spike antigens following BA.1/BA.2 infections in triple mRNA-vaccinated individuals, with and without prior SARS-CoV-2 infection. We find that most individuals increase BA.1/BA.2/BA.5-specific neutralizing antibodies following infection, but confirm that the magnitude of increase and post-omicron titres are higher in the infection-naive. In contrast, significant increases in nasal responses, including neutralizing activity against BA.5 spike, are seen regardless of infection history. Spike-specific T cells increase only in infection-naive vaccinees; however, post-omicron T cell responses are significantly higher in the previously-infected, who display a maximally induced response with a highly cytotoxic CD8+ phenotype following their 3rd mRNA vaccine dose. Responses to non-spike antigens increase significantly regardless of prior infection status. These findings suggest that hybrid immunity induced by omicron breakthrough infections is characterized by significant immune enhancement that can help protect against future omicron variants."
TAO ZHANG,Genomic insights of body plan transitions from bilateral to pentameral symmetry in echinoderms,"Echinoderms are an exceptional group of bilaterians that develop pentameral adult symmetry from a bilaterally symmetric larva. However, the genetic basis in evolution and development of this unique transformation remains to be clarified. Here we report newly sequenced genomes, developmental transcriptomes, and proteomes of diverse echinoderms including the green sea urchin (L. variegatus), a sea cucumber (A. japonicus), and with particular emphasis on a sister group of the earliest-diverged echinoderms, the feather star (A. japonica). We learned that the last common ancestor of echinoderms retained a well-organized Hox cluster reminiscent of the hemichordate, and had gene sets involved in endoskeleton development. Further, unlike in other animal groups, the most conserved developmental stages were not at the body plan establishing phase, and genes normally involved in bilaterality appear to function in pentameric axis development. These results enhance our understanding of the divergence of protostomes and deuterostomes almost 500 Mya."
TAO ZHANG,Derivedness index for estimating degree of phenotypic evolution of embryos: a study of comparative transcriptomic analyses of chordates and echinoderms,"Species retaining ancestral features, such as species called living fossils, are often regarded as less derived than their sister groups, but such discussions are usually based on qualitative enumeration of conserved traits. This approach creates a major barrier, especially when quantifying the degree of phenotypic evolution or degree of derivedness, since it focuses only on commonly shared traits, and newly acquired or lost traits are often overlooked. To provide a potential solution to this problem, especially for inter-species comparison of gene expression profiles, we propose a new method named ""derivedness index"" to quantify the degree of derivedness. In contrast to the conservation-based approach, which deals with expressions of commonly shared genes among species being compared, the derivedness index also considers those that were potentially lost or duplicated during evolution. By applying our method, we found that the gene expression profiles of penta-radial phases in echinoderm tended to be more highly derived than those of the bilateral phase. However, our results suggest that echinoderms may not have experienced much larger modifications to their developmental systems than chordates, at least at the transcriptomic level. In vertebrates, we found that the mid-embryonic and organogenesis stages were generally less derived than the earlier or later stages, indicating that the conserved phylotypic period is also less derived. We also found genes that potentially explain less derivedness, such as Hox genes. Finally, we highlight technical concerns that may influence the measured transcriptomic derivedness, such as read depth and library preparation protocols, for further improvement of our method through future studies. We anticipate that this index will serve as a quantitative guide in the search for constrained developmental phases or processes."
DA-YUAN CHEN,Identification and reconstruction of low-energy electrons in the ProtoDUNE-SP detector,
DA-YUAN CHEN,The long-baseline neutrino experiment: exploring fundamental symmetries of the universe,"The preponderance of matter over antimatter in the early Universe, the dynamics of the supernova bursts that produced the heavy elements necessary for life and whether protons eventually decay --- these mysteries at the forefront of particle physics and astrophysics are key to understanding the early evolution of our Universe, its current state and its eventual fate. The Long-Baseline Neutrino Experiment (LBNE) represents an extensively developed plan for a world-class experiment dedicated to addressing these questions. LBNE is conceived around three central components: (1) a new, high-intensity neutrino source generated from a megawatt-class proton accelerator at Fermi National Accelerator Laboratory, (2) a near neutrino detector just downstream of the source, and (3) a massive liquid argon time-projection chamber deployed as a far detector deep underground at the Sanford Underground Research Facility. This facility, located at the site of the former Homestake Mine in Lead, South Dakota, is approximately 1,300 km from the neutrino source at Fermilab -- a distance (baseline) that delivers optimal sensitivity to neutrino charge-parity symmetry violation and mass ordering effects. This ambitious yet cost-effective design incorporates scalability and flexibility and can accommodate a variety of upgrades and contributions. With its exceptional combination of experimental configuration, technical capabilities, and potential for transformative discoveries, LBNE promises to be a vital facility for the field of particle physics worldwide, providing physicists from around the globe with opportunities to collaborate in a twenty to thirty year program of exciting science. In this document we provide a comprehensive overview of LBNE's scientific objectives, its place in the landscape of neutrino physics worldwide, the technologies it will incorporate and the capabilities it will possess."
DA-YUAN CHEN,Scintillation light detection in the 6-m drift-length ProtoDUNE Dual Phase liquid argon TPC,"DUNE is a dual-site experiment for long-baseline neutrino oscillation studies, neutrino astrophysics and nucleon decay searches. ProtoDUNE Dual Phase (DP) is a 6  ×  6  ×  6 m 3 liquid argon time-projection-chamber (LArTPC) that recorded cosmic-muon data at the CERN Neutrino Platform in 2019-2020 as a prototype of the DUNE Far Detector. Charged particles propagating through the LArTPC produce ionization and scintillation light. The scintillation light signal in these detectors can provide the trigger for non-beam events. In addition, it adds precise timing capabilities and improves the calorimetry measurements. In ProtoDUNE-DP, scintillation and electroluminescence light produced by cosmic muons in the LArTPC is collected by photomultiplier tubes placed up to 7 m away from the ionizing track. In this paper, the ProtoDUNE-DP photon detection system performance is evaluated with a particular focus on the different wavelength shifters, such as PEN and TPB, and the use of Xe-doped LAr, considering its future use in giant LArTPCs. The scintillation light production and propagation processes are analyzed and a comparison of simulation to data is performed, improving understanding of the liquid argon properties."
DA-YUAN CHEN,Broadband multi-wavelength properties of M87 during the 2017 Event Horizon Telescope campaign,"In 2017, the Event Horizon Telescope (EHT) Collaboration succeeded in capturing the first direct image of the center of the M87 galaxy. The asymmetric ring morphology and size are consistent with theoretical expectations for a weakly accreting supermassive black hole of mass ∼6.5 × 109 M ⊙. The EHTC also partnered with several international facilities in space and on the ground, to arrange an extensive, quasi-simultaneous multi-wavelength campaign. This Letter presents the results and analysis of this campaign, as well as the multi-wavelength data as a legacy data repository. We captured M87 in a historically low state, and the core flux dominates over HST-1 at high energies, making it possible to combine core flux constraints with the more spatially precise very long baseline interferometry data. We present the most complete simultaneous multi-wavelength spectrum of the active nucleus to date, and discuss the complexity and caveats of combining data from different spatial scales into one broadband spectrum. We apply two heuristic, isotropic leptonic single-zone models to provide insight into the basic source properties, but conclude that a structured jet is necessary to explain M87’s spectrum. We can exclude that the simultaneous γ-ray emission is produced via inverse Compton emission in the same region producing the EHT mm-band emission, and further conclude that the γ-rays can only be produced in the inner jets (inward of HST-1) if there are strongly particle-dominated regions. Direct synchrotron emission from accelerated protons and secondaries cannot yet be excluded."
ANAHITA PILVAR,Shortwave infrared spatial frequency domain imaging to quantify blood lipids,"Blood lipids are one of the main biomarkers for cardiovascular diseases (CVD). Current method for blood lipid assessment requires invasive blood draws, usually after an overnight fast, followed by lab-based testing, makes the technique not suitable for regular monitoring. The limitations of invasive blood testing techniques, alongside the growing evidence supporting the benefits of regular lipid monitoring, particularly after a meal, for better CVD prediction, indicate that there is an unmet need for non-invasive blood lipid assessment. Our lab recently demonstrated the possibility of non-invasive blood lipids monitoring using a diffuse optical technique called shortwave infrared spatial frequency domain imaging (SWIR SFDI). Using SWIR SFDI, we have shown that an increase of blood lipids after a high fat meal that was measured with gold standard blood draw, correlates strongly with an SFDI derived parameter called SWIR-MPI index. While the results were promising, advancements in SFDI instrumentation, modeling, and experiments are needed prior to clinical testing and translation. The work presented in this dissertation was focused on identifying the source of optical contrast induced by blood lipids, and advancing SFDI instrumentation and processing methodology for the goal of developing a clinic-ready optical system for non-invasive blood lipid assessment. To improve SFDI instrumentation, the SFDI parameters were fine-tuned, and a new compact and portable system was developed and assessed for its performance using optical phantoms. A new two-layer model that accounts for the effect of skin was developed to improve SFDI processing methodology when measuring human subjects. The integration of the new instrument and model make SFDI technique more suitable to use for human studies. Next, the effect of lipids on optical properties of blood were investigated through literature reviews, theoretical simulations and ex-vivo experiments on bovine blood. Finally, we conducted a healthy volunteer study to monitor blood lipid alteration after low fat and high fat meal with SFDI. Together, the works in this project advances our comprehension of the optical effects of blood lipids and their association with meal consumption in both healthy individuals and those with dyslipidemia. The innovative technology and the preliminary feasibility study push the boundaries of non-invasive blood lipid assessment, offering promising prospects for translating this technology to clinical settings, thereby enhancing CVD risk assessment."
JU HYOUNG MUN,Relational memory: native in-memory accesses on rows and columns,"Analytical database systems are typically designed to use a column-first data layout to access only the desired fields. On the other hand, storing data row-first works great for accessing, inserting, or updating entire rows. Transforming rows to columns at runtime is expensive, hence, many analytical systems ingest data in row-first form and transform it in the background to columns to facilitate future analytical queries. How will this design change if we can always efficiently access only the desired set of columns? To address this question, we present a radically new approach to data transformation from rows to columns. We build upon recent advancements in embedded platforms with re-programmable logic to design native in-memory access on rows and columns. Our approach, termed Relational Memory, relies on an FPGA-based accelerator that sits between the CPU and main memory and transparently transforms base data to any group of columns with minimal overhead at runtime. This design allows accessing any group of columns as if it already exists in memory. We implement and deploy Relational Memory in real hardware, and we show that we can access the desired columns up to 1.63× faster than accessing them from their row-wise counterpart, while matching the performance of a pure columnar access for low projectivity, and outperforming it by up to 1.87× as projectivity (and tuple reconstruction cost) increases. Moreover, our approach can be easily extended to support offloading of a number of operations to hardware, e.g., selection, group by, aggregation, and joins, having the potential to vastly simplify the software logic and accelerate the query execution."
AUSTIN LEE,Legacy of the developmental state and political preferences: development of the South Korean new and renewable energy sector,"This research aims to study the mechanisms through which the South Korean state influenced the new and renewable energy sector during the Lee Myung-bak and Moon Jae-in administrations. Furthermore, this study aims to analyze the different approaches that each administration favored towards new and renewable energy policy based on their ideological leanings, as well as identify the relevant actors and the mechanisms of compensation for each actor. This study finds that both administrations actively shaped the new and renewable energy sector through legislative developments and major energy initiatives, despite the Moon administration showing efforts for democratization of the new and renewable energy sector."
AUSTIN LEE,Thalamocortical control of propofol phase-amplitude coupling,"The anesthetic propofol elicits many different spectral properties on the EEG, including alpha oscillations (8-12 Hz), Slow Wave Oscillations (SWO, 0.1-1.5 Hz), and dose-dependent phase-amplitude coupling (PAC) between alpha and SWO. Propofol is known to increase GABAA inhibition and decrease H-current strength, but how it generates these rhythms and their interactions is still unknown. To investigate both generation of the alpha rhythm and its PAC to SWO, we simulate a Hodgkin-Huxley network model of a hyperpolarized thalamus and corticothalamic inputs. We find, for the first time, that the model thalamic network is capable of independently generating the sustained alpha seen in propofol, which may then be relayed to cortex and expressed on the EEG. This dose-dependent sustained alpha critically relies on propofol GABAA potentiation to alter the intrinsic spindling mechanisms of the thalamus. Furthermore, the H-current conductance and background excitation of these thalamic cells must be within specific ranges to exhibit any intrinsic oscillations, including sustained alpha. We also find that, under corticothalamic SWO UP and DOWN states, thalamocortical output can exhibit maximum alpha power at either the peak or trough of this SWO; this implies the thalamus may be the source of propofol-induced PAC. Hyperpolarization level is the main determinant of whether the thalamus exhibits trough-max PAC, which is associated with lower propofol dose, or peak-max PAC, associated with higher dose. These findings suggest: the thalamus generates a novel rhythm under GABAA potentiation such as under propofol, its hyperpolarization may determine whether a patient experiences trough-max or peak-max PAC, and the thalamus is a critical component of propofol-induced cortical spectral phenomena. Changes to the thalamus may be a critical part of how propofol accomplishes its effects, including unconsciousness."
AUSTIN LEE,Simvastatin is associated with a reduced incidence of dementia and Parkinson's disease,"BACKGROUND: Statins are a class of medications that reduce cholesterol by inhibiting 3-hydroxy-3-methylglutaryl-coenzyme A reductase. Whether statins can benefit patients with dementia remains unclear because of conflicting results. We hypothesized that some of the confusion in the literature might arise from differences in efficacy of different statins. We used a large database to compare the action of several different statins to investigate whether some statins might be differentially associated with a reduction in the incidence of dementia and Parkinson's disease. METHODS: We analyzed data from the decision support system of the US Veterans Affairs database, which contains diagnostic, medication and demographic information on 4.5 million subjects. The association of lovastatin, simvastatin and atorvastatin with dementia was examined with Cox proportional hazard models for subjects taking statins compared with subjects taking cardiovascular medications other than statins, after adjusting for covariates associated with dementia or Parkinson's disease. RESULTS: We observed that simvastatin is associated with a significant reduction in the incidence of dementia in subjects ≥65 years, using any of three models. The first model incorporated adjustment for age, the second model included adjusted for three known risk factors for dementia, hypertension, cardiovascular disease or diabetes, and the third model incorporated adjustment for the Charlson index, which is an index that provides a broad assessment of chronic disease. Data were obtained for over 700000 subjects taking simvastatin and over 50000 subjects taking atorvastatin who were aged >64 years. Using model 3, the hazard ratio for incident dementia for simvastatin and atorvastatin are 0.46 (CI 0.44–0.48, p < 0.0001) and 0.91 (CI 0.80–1.02, p = 0.11), respectively. Lovastatin was not associated with a reduction in the incidence of dementia. Simvastatin also exhibited a reduced hazard ratio for newly acquired Parkinson's disease (HR 0.51, CI 0.4–0.55, p < 0.0001). CONCLUSION: Simvastatin is associated with a strong reduction in the incidence of dementia and Parkinson's disease, whereas atorvastatin is associated with a modest reduction in incident dementia and Parkinson's disease, which shows only a trend towards significance."
AUSTIN LEE,Use of angiotensin receptor blockers and risk of dementia in a predominantly male population: prospective cohort analysis,"Objective: To investigate whether angiotensin receptor blockers protect against Alzheimer's disease and dementia or reduce the progression of both diseases. Design Prospective cohort analysis. Setting Administrative database of the US Veteran Affairs, 2002-6. Population 819491 predominantly male participants (98%) aged 65 or more with cardiovascular disease. Main outcome measures Time to incident Alzheimer's disease or dementia in three cohorts (angiotensin receptor blockers, lisinopril, and other cardiovascular drugs, the ""cardiovascular comparator"") over a four year period (fiscal years 2003-6) using Cox proportional hazard models with adjustments for age, diabetes, stroke, and cardiovascular disease. Disease progression was the time to admission to a nursing home or death among participants with pre-existing Alzheimer's disease or dementia. Results Hazard rates for incident dementia in the angiotensin receptor blocker group were 0.76 (95% confidence interval 0.69 to 0.84) compared with the cardiovascular comparator and 0.81 (0.73 to 0.90) compared with the lisinopril group. Compared with the cardiovascular comparator, angiotensin receptor blockers in patients with pre-existing Alzheimer's disease were associated with a significantly lower risk of admission to a nursing home (0.51, 0.36 to 0.72) and death (0.83, 0.71 to 0.97). Angiotensin receptor blockers exhibited a dose-response as well as additive effects in combination with angiotensin converting enzyme inhibitors. This combination compared with angiotensin converting enzyme inhibitors alone was associated with a reduced risk of incident dementia (0.54, 0.51 to 0.57) and admission to a nursing home (0.33, 0.22 to 0.49). Minor differences were shown in mean systolic and diastolic blood pressures between the groups. Similar results were observed for Alzheimer's disease. Conclusions Angiotensin receptor blockers are associated with a significant reduction in the incidence and progression of Alzheimer's disease and dementia compared with angiotensin converting enzyme inhibitors or other cardiovascular drugs in a predominantly male population."
AUSTIN LEE,The Challenges of Multimorbidity from the Patient Perspective,"BACKGROUND Although multiple co-occurring chronic illnesses within the same individual are increasingly common, few studies have examined the challenges of multimorbidity from the patient perspective. OBJECTIVE The aim of this study is to examine the self-management learning needs and willingness to see non-physician providers of patients with multimorbidity compared to patients with single chronic illnesses. DESIGN. This research is designed as a cross-sectional survey. PARTICIPANTS Based upon ICD-9 codes, patients from a single VHA healthcare system were stratified into multimorbidity clusters or groups with a single chronic illness from the corresponding cluster. Nonproportional sampling was used to randomly select 720 patients. MEASUREMENTS Demographic characteristics, functional status, number of contacts with healthcare providers, components of primary care, self-management learning needs, and willingness to see nonphysician providers. RESULTS Four hundred twenty-two patients returned surveys. A higher percentage of multimorbidity patients compared to single morbidity patients were ""definitely"" willing to learn all 22 self-management skills, of these only 2 were not significant. Compared to patients with single morbidity, a significantly higher percentage of patients with multimorbidity also reported that they were ""definitely"" willing to see 6 of 11 non-physician healthcare providers. CONCLUSIONS Self-management learning needs of multimorbidity patients are extensive, and their preferences are consistent with team-based primary care. Alternative methods of providing support and chronic illness care may be needed to meet the needs of these complex patients."
DOU LI,Non-line-of-sight imaging over 1.43 km,"Non-line-of-sight (NLOS) imaging has the ability to reconstruct hidden objects from indirect light paths that scatter multiple times in the surrounding environment, which is of considerable interest in a wide range of applications. Whereas conventional imaging involves direct line-of-sight light transport to recover the visible objects, NLOS imaging aims to reconstruct the hidden objects from the indirect light paths that scatter multiple times, typically using the information encoded in the time-of-flight of scattered photons. Despite recent advances, NLOS imaging has remained at short-range realizations, limited by the heavy loss and the spatial mixing due to the multiple diffuse reflections. Here, both experimental and conceptual innovations yield hardware and software solutions to increase the standoff distance of NLOS imaging from meter to kilometer range, which is about three orders of magnitude longer than previous experiments. In hardware, we develop a high-efficiency, low-noise NLOS imaging system at near-infrared wavelength based on a dual-telescope confocal optical design. In software, we adopt a convex optimizer, equipped with a tailored spatial-temporal kernel expressed using three-dimensional matrix, to mitigate the effect of the spatial-temporal broadening over long standoffs. Together, these enable our demonstration of NLOS imaging and real-time tracking of hidden objects over a distance of 1.43 km. The results will open venues for the development of NLOS imaging techniques and relevant applications to real-world conditions."
JESSIE HUANG,The first habitable-zone Earth-sized planet from TESS. II. Spitzer confirms TOI-700 d,"We present Spitzer 4.5 μm observations of the transit of TOI-700 d, a habitable-zone Earth-sized planet in a multiplanet system transiting a nearby M-dwarf star (TIC 150428135, 2MASS J06282325–6534456). TOI-700 d has a radius of 1.144_-0.061^+0.062R_⨁ and orbits within its host star's conservative habitable zone with a period of 37.42 days (T eq ~ 269 K). TOI-700 also hosts two small inner planets (R b = 1.037_-0.064^+0.065R_⨁ and R c = 2.65_-0.15^+0.16R_⨁) with periods of 9.98 and 16.05 days, respectively. Our Spitzer observations confirm the Transiting Exoplanet Survey Satellite (TESS) detection of TOI-700 d and remove any remaining doubt that it is a genuine planet. We analyze the Spitzer light curve combined with the 11 sectors of TESS observations and a transit of TOI-700 c from the LCOGT network to determine the full system parameters. Although studying the atmosphere of TOI-700 d is not likely feasible with upcoming facilities, it may be possible to measure the mass of TOI-700 d using state-of-the-art radial velocity (RV) instruments (expected RV semiamplitude of ~70 cm s^−1)."
JULIE MARGIT PETERSEN,The application of novel analytic methods to gain new insights in historically well-studied areas of perinatal epidemiology,"Due to rapid growth in computing power, the collection of high dimensional and complex datasets is increasingly feasible. To reap their full benefit, novel analytic strategies may be required. Application of such methods remains limited in certain epidemiologic research areas. The overarching aim of this dissertation was to apply novel analytic strategies with close ties to causal inference and statistical learning theory to gain new insights into well-studied areas of perinatal epidemiology. In Study 1, we explored whether the association between short interpregnancy intervals (i.e., the end of one pregnancy to the start of the next) and increased risk of preterm birth may be due to residual confounding in three populations (n=693 American Indian and n=728 white women from the Northern Plains, U.S., and n=783 mixed ancestry women from the Western Cape, South Africa). Using data from the prospective Safe Passage cohort (2007-2015), we applied propensity score methods to control for a variety of sociodemographic and reproductive factors. A third-to-half of women with <6 months intervals had propensity scores that largely did not overlap with those of women with 18-23 months intervals. Since the propensity score models included factors related to both interpregnancy interval and preterm birth, these findings suggest the possibility of strong confounding in all three populations. The pooled associational estimate with preterm birth was attenuated in the propensity score trimmed and weighted data (risk ratio 1.4, 95% CI 0.75-2.6) compared with the crude results (risk ratio 1.7, 95% CI 1.1-2.7). However, the sample size and precision were reduced after propensity score trimming, and several covariates remained imbalanced. The data demonstrated the complexity of the processes leading to interpregnancy interval length. These issues may have been difficult to identify without comprehensive confounder data and with other methods, such as traditional regression adjustment. In Study 2, we examined the relative importance of timing (first trimester versus second/third trimesters) and degree of gestational weight gain in relation to infant size at birth (small-and-large-for-gestational age) among women with obesity using data from a medical records-based case-cohort study (Pittsburgh, PA, 1998-2010). We operationalized serial antenatal weight measurements as above, below, or within the current recommended ranges for U.S. pregnancies, i.e., 0.2-2.0 kg total gain in the first trimester and 0.17-0.27 kg per week in the second and third trimesters (based on group based trajectory modeling). Data were analyzed by obesity class (n=1290 in the class I subcohort, n=1247 class II, n=1198 class III). Our findings supported the current clinical guidelines, except for women with class III obesity. Among women with class III obesity, lower than recommended gain in the second and third trimesters was associated with decreased risk of having a large-for-gestational age infant (adjusted risk ratio 0.76, 95% CI 0.51-1.1), while not increasing small-for-gestational age (SGA) risk (adjusted risk ratio 1.0, 95% CI 0.63-1.7). Our results were in agreement with findings from several other studies of women with obesity using other methodologies to operationalize gestational weight gain. In Study 3, we used hierarchical clustering to explore latent groups of placental pathology features. We also investigated whether the placental clusters, in addition to birthweight percentiles, were beneficial to explain the variability of select adverse pregnancy outcomes. Data were from the Safe Passage Study (same as Study 1, n=2005). We identified one cluster with low prevalence of abnormalities (60.9%) and three clusters that mapped well to the expert consensus-based Amsterdam criteria: severe maternal vascular malperfusion (5.8%), fetal vascular malperfusion (11.1%), and inflammation (22.1%). The clusters were weakly-to-moderately associated with certain antenatal risk factors, pregnancy complications, and neonatal outcomes. Birthweight percentiles plus the placental clusters was better able to explain the variance of select adverse outcomes, compared with using small-for-gestational age only. This study serves as proof-of-concept that machine learning methods, and placental data, may aid in the identification and etiologic study of certain adverse pregnancy outcomes. In sum, all three studies support that the application of novel analytic methods to high-dimensional datasets may expand our understanding of certain causal questions, even ones that have been broached before, although, as seen in Study 2, such research may not always yield novel insights."
LONGZHI GAN,Nonlinear interactions between radiation belt electrons and chorus waves: dependence on wave amplitude modulation,"We use test particle simulations to model the interaction between radiation belt electrons and whistler mode chorus waves by focusing on wave amplitude modulations. We quantify the pitch angle and energy changes due to phase trapping and phase bunching (including both advection and scattering) for electrons with various initial energies and pitch angles. Three nonlinear regimes are identified in a broad range of pitch angle-energy space systematically, each indicating different nonlinear effects. Our simulation results show that wave amplitude modulations can extend the nonlinear regimes, while significantly reducing electron acceleration by phase trapping. By including amplitude modulations, the “advective” changes in pitch angle and energy caused by phase bunching are reduced, while the “diffusive” scattering due to phase bunching is enhanced. Our study demonstrates the importance of wave amplitude modulations in nonlinear effects and suggests that they need to be properly incorporated into future theoretical and numerical studies."
LONGZHI GAN,Dependence of nonlinear effects on whistler-mode chorus wave bandwidth and amplitude: a perspective from diffusion coefficients,"The electron resonant interaction with whistler-mode waves is characterized by transport in pitch angle–energy space. We calculate electron diffusion and advection coefficients (a simplified characterization of transport) for a large range of electron pitch angle and energy using test particle simulations. Nonlinear effects are analyzed by comparing the diffusion coefficients using test particle simulations and quasilinear theory, and by evaluating the advection rates. Dependence of nonlinear effects on the wave amplitude and bandwidth of whistler-mode waves is evaluated by running test particle simulations with a broad range of wave amplitude and bandwidth. The maximum amplitudes where the quasilinear approach is valid are found to increase with increasing bandwidth, from 50 pT for narrowband waves to 300 pT for broadband waves at L-shell of 6. Moreover, interactions between intense whistler-mode waves and small pitch angle electrons lead to large positive advection, which limits the applicability of diffusion-based models. This study demonstrates the parameter range of the applicability of quasilinear theory and diffusion model for different wave amplitudes and frequency bandwidths of whistler-mode waves, which is critical for evaluating the effects of whistler-mode waves on energetic electrons in the Earth’s magnetosphere."
LONGZHI GAN,Equations of motion near cyclotron resonance,"This work compares several versions of the equations of motion for a test particle encountering cyclotron resonance with a single, field-aligned whistler mode wave. The gyro-averaged Lorentz equation produces both widespread phase trapping (PT) and “positive phase bunching” of low pitch angle electrons by large amplitude waves. Approximations allow a Hamiltonian description to be reduced to a single pair of conjugate variables, which can account for PT as well as phase bunching at moderate pitch angle, and has recently been used to investigate this unexpected bahavior at low pitch angle. Here, numerical simulations using the Lorentz equation and several versions of Hamiltonian-based equations of motion are compared. Similar behavior at low pitch angle is found in each case."
LONGZHI GAN,Nonlinear interactions between whistler mode chorus waves and energetic electrons in the Earth’s radiation belts,"Plasma waves are key drivers of the highly variable electron dynamics in Earth’s outer radiation belts. In particular, whistler mode chorus waves, which are commonly observed with intense wave amplitudes, are known to be a key driver of rapid electron acceleration and precipitation observed by many recent satellite (e.g., Arase, ELFIN, THEMIS, and Van Allen Probes) and balloon missions (BARREL). However, quantitative understanding of how electron acceleration and precipitation is modified due to the nonlinear interactions with chorus waves is limited. This dissertation systematically evaluates the nonlinear effects of chorus waves in the full electron pitch angle-energy space using test particle simulations, quasilinear models, and satellite observations. More specifically, the dependences of these nonlinear effects on the chorus wave amplitude modulation (waveform structures), as well as wave amplitude and frequency bandwidth (spectrum structures), are quantified over a wide range of wave parameters. The results show that realistic chorus wave structures tend to limit the nonlinear effects on energetic electrons. The system can be described by a diffusion model similar to quasilinear theory, but nonlinear effects alter the diffusion coefficients from quasilinear ones. Using an intriguing event observed by the Van Allen Probes, I further demonstrate that nonlinear phase trapping by the upper-band chorus waves can efficiently accelerate electrons to form the distinct butterfly pitch angle distribution within 30 seconds. The effects of nonlinear interaction (Landau trapping) on electron precipitation are also evaluated during a bursty electron precipitation event observed by the ELFIN CubeSats, in association with very oblique chorus waves observed by THEMIS near the equatorial plane. The test particle simulation results provide the first direct evidence of rapid (~5 s) electron precipitation driven by high-order resonances due to chorus waves. Overall, this dissertation provides a full quantification of nonlinear effects and their dependences on various electron and chorus wave parameters. The findings in this dissertation are crucial to our fundamental understanding of wave-particle interactions, particularly on short timescales in the Earth’s radiation belts and in other space plasma environments, such as solar wind and other planets, as well as astrophysical and laboratory plasmas."
BRANDON GEI-CHIN WONG,"Design, development and application of an automated framework for cell growth and laboratory evolution","Precise control over microbial cell growth conditions could enable detection of minute phenotypic changes, which would improve our understanding of how genotypes are shaped by adaptive selection. Although automated cell- culture systems such as bioreactors offer strict control over liquid culture conditions, they often do not scale to high-throughput or require cumbersome redesign to alter growth conditions. I report the design and validation of eVOLVER, a scalable DIY framework that can be configured to carry out high- throughput growth experiments in molecular evolution, systems biology, and microbiology. I perform high-throughput evolution of yeast across systematically varied population density niches to show how eVOLVER can precisely characterize adaptive niches. I describe growth selection using time-varying temperature programs on a genome-wide yeast knockout library to identify strains with altered sensitivity to changes in temperature magnitude or frequency. Inspired by large-scale integration of electronics and microfluidics, I also demonstrate millifluidic multiplexing modules that enable multiplexed media routing, cleaning, vial-to-vial transfers and automated yeast mating."
LAURA LONG,Examining hope as a transdiagnostic mechanism of change across anxiety disorders and CBT treatment protocols.,"Hope is a trait that represents the capacity to identify strategies or pathways to achieve goals and the motivation or agency to effectively pursue those pathways. Hope has been demonstrated to be a robust source of resilience to anxiety and stress and there is limited evidence that, as has been suggested for decades, hope may function as a core process or transdiagnostic mechanism of change in psychotherapy. The current study examined the role of hope in predicting recovery in a clinical trial in which 223 individuals with 1 of 4 anxiety disorders were randomized to transdiagnostic cognitive behavior therapy (CBT), disorder-specific CBT, or a waitlist controlled condition. Effect size results indicated moderate to large intraindividual increases in hope, that changes in hope were consistent across the five CBT treatment protocols, that changes in hope were significantly greater in CBT relative to waitlist, and that changes in hope began early in treatment. Results of growth curve analyses indicated that CBT was a robust predictor of trajectories of change in hope compared to waitlist, and that changes in hope predicted changes in both self-reported and clinician-rated anxiety. Finally, a statistically significant indirect effect was found indicating that the effects of treatment on changes in anxiety were mediated by treatment effects on hope. Together, these results suggest that hope may be a promising transdiagnostic mechanism of change that is relevant across anxiety disorders and treatment protocols."
BIBEK RAJ THAPA,Cell state changes during compensatory lung regrowth following pneumonectomy,"Understanding the cellular and molecular dynamics of lung progenitors is necessary to achieve the ultimate therapeutic goal of tissue regeneration in the context of debilitating lung diseases. Alveolar type 2 cells (AT2s) of the distal lung epithelium are considered the predominant facultative progenitors of adult murine lung alveoli, the tissues responsible for gas exchange in mammals. While normally quiescent at homeostasis, AT2s can enter cell cycle and transdifferentiate into alveolar type 1 cells (AT1s) during lung repair. The kinetics of AT2 transdifferentiation into AT1s following mild insults, where alveolar architectural integrity is preserved and lung function is restored, is not completely understood. Additionally, the transcriptomic programs across all cell lineages that might regulate this process largely remain unknown. To identify putative molecular pathways activated in the epithelium and its overall niche, cell-type specific responses were characterized in a mouse model during adult lung compensatory growth following the mild injury of unilateral pneumonectomy (PNX). Histologic analyses showed proliferation of AT2s as a primary response, followed by differentiation into AT1s that peaked around day 10-12 post-PNX. Time-series single-cell RNA sequencing profiles revealed that AT2s express transitional markers such as Krt8 and Areg, consistent with previous studies. Trajectory analysis using a Continuous State Hidden Markov Model (CSHMM) predicted Hippo pathway effector Tead1 and Notch signaling target Hes1 as active transcription factors associated with the transdifferentiation of AT2s to AT1s. To verify involvement of Notch post PNX, AT2s were assessed for Notch activation using a genetic reporter, which showed an increase in the percentage of Notch-responsive AT2s post-PNX. To test potential function of Notch in AT2 transdifferentiation into AT1s, Notch signaling was inhibited in vitro in a mouse cell co-culture model which showed loss of AT2 transdifferentiation into AT1s. Single-cell RNA sequencing profiles post-PNX revealed an activated lung mesenchymal subset unique to injury that was characterized by high expression of extracellular matrix-related (ECM) genes along with low expression of Acta2 and Pdgfra. Interestingly, this mesenchymal subset included Cthrc1+ fibroblasts that expressed the highest levels of ECM genes and transcripts associated with BMP and TGFβ signaling pathways, including Fstl1, Grem1, and Tgfβ3, factors previously linked to epithelial differentiation. Immunostaining combined with in situ hybridization assays identified Cthrc1+ fibroblasts adjacent to Krt8+ transitional epithelial cells. Genetic lineage tracing of Cthrc1+ cells post-PNX revealed the persistence of their descendants after completion of compensatory lung regrowth. In summary, analysis of AT2 global transcriptomes and differentiation kinetics post-PNX revealed activated/transitional states, nascent AT1 cells, and the emergence of Cthrc1+ mesenchymal subpopulations. These results suggest that AT2s are one of the early responders to PNX stimuli and identified mesenchymal cells states that may assist in overall compensatory lung regrowth post-PNX."
KE WU,Magnetic field enhancement in metamaterials,"Metamaterials, defined as artificially constructed materials composed of subwavelength meta-atoms, have emerged as a promising tool to manipulate electromagnetic (EM) waves due to their extraordinary responses to incident EM waves. Efforts in developing metamaterials have progressed from initial demonstrations of breaking the generalized limitations of refraction and reflection of natural materials, to their current use in facilitating a range of practical applications. One flourishing research direction is the applications of metamaterial in electromagnetic field enhancement. The enhanced magnetic fields are attractive for a variety of applications such as magnetic resonance imaging (MRI), wireless power transfer, and magnetic induction tomography, among others. The common thread across this dissertation is incorporating conventional and novel methods to create some functional magnetic metamaterials for field enhancement, considering the practical application scenarios. First, this dissertation investigates mechanically tunable metamaterials by integrating EM resonators with deployable auxetics. Aided by digital parametric design tools, the computationally designed metamaterials could conformably cover a person’s kneecap, ankle, head, or any part of the body in need of imaging, and meanwhile they are readily to be tuned for ensuring frequency match, making magnetic metamaterials more feasible in clinical scenarios. Second, this dissertation presents a flexible, smart metamaterial composed of an assembly of meta-atoms featuring a controlling circuit loaded spiral resonator (CCLSR) inductively coupled with a varactor-loaded ring resonator (VLRR). The reported metamaterial may not only be readily tuned to achieve precise frequency match with MRI by a controlling circuit, but is also capable of selectively amplifying the magnetic field by sensing excitation signal strength passively, thereby remaining ‘off’ during RF transmission and ensuring its optimal performance when applied to MRI. Third, this dissertation introduces a metamaterial-enhanced near-field readout platform for interrogating passive microsensor tags. With the unique evanescent wave amplification properties, the insertion of a metamaterial between the transmitter and receiver antennas could amplify the magnetic flux density and, thus, increase the coupling coefficient, ultimately improving the wireless power transfer efficiency. These functional metamaterials demonstrated in this thesis can be employed to construct application-oriented electromagnetic devices and facilitate metamaterial-based technologies."
KE WU,Broadband multi-wavelength properties of M87 during the 2017 Event Horizon Telescope campaign,"In 2017, the Event Horizon Telescope (EHT) Collaboration succeeded in capturing the first direct image of the center of the M87 galaxy. The asymmetric ring morphology and size are consistent with theoretical expectations for a weakly accreting supermassive black hole of mass ∼6.5 × 109 M ⊙. The EHTC also partnered with several international facilities in space and on the ground, to arrange an extensive, quasi-simultaneous multi-wavelength campaign. This Letter presents the results and analysis of this campaign, as well as the multi-wavelength data as a legacy data repository. We captured M87 in a historically low state, and the core flux dominates over HST-1 at high energies, making it possible to combine core flux constraints with the more spatially precise very long baseline interferometry data. We present the most complete simultaneous multi-wavelength spectrum of the active nucleus to date, and discuss the complexity and caveats of combining data from different spatial scales into one broadband spectrum. We apply two heuristic, isotropic leptonic single-zone models to provide insight into the basic source properties, but conclude that a structured jet is necessary to explain M87’s spectrum. We can exclude that the simultaneous γ-ray emission is produced via inverse Compton emission in the same region producing the EHT mm-band emission, and further conclude that the γ-rays can only be produced in the inner jets (inward of HST-1) if there are strongly particle-dominated regions. Direct synchrotron emission from accelerated protons and secondaries cannot yet be excluded."
MATTHEW FISH,First M87 Event Horizon Telescope results. III. Data processing and calibration,"We present the calibration and reduction of Event Horizon Telescope (EHT) 1.3 mm radio wavelength observations of the supermassive black hole candidate at the center of the radio galaxy M87 and the quasar 3C 279, taken during the 2017 April 5–11 observing campaign. These global very long baseline interferometric observations include for the first time the highly sensitive Atacama Large Millimeter/submillimeter Array (ALMA); reaching an angular resolution of 25 μas, with characteristic sensitivity limits of ~1 mJy on baselines to ALMA and ~10 mJy on other baselines. The observations present challenges for existing data processing tools, arising from the rapid atmospheric phase fluctuations, wide recording bandwidth, and highly heterogeneous array. In response, we developed three independent pipelines for phase calibration and fringe detection, each tailored to the specific needs of the EHT. The final data products include calibrated total intensity amplitude and phase information. They are validated through a series of quality assurance tests that show consistency across pipelines and set limits on baseline systematic errors of 2% in amplitude and 1° in phase. The M87 data reveal the presence of two nulls in correlated flux density at ~3.4 and ~8.3 Gλ and temporal evolution in closure quantities, indicating intrinsic variability of compact structure on a timescale of days, or several light-crossing times for a few billion solar-mass black hole. These measurements provide the first opportunity to image horizon-scale structure in M87."
MATTHEW FISH,First M87 Event Horizon Telescope results. V. Physical origin of the asymmetric ring,"The Event Horizon Telescope (EHT) has mapped the central compact radio source of the elliptical galaxy M87 at 1.3 mm with unprecedented angular resolution. Here we consider the physical implications of the asymmetric ring seen in the 2017 EHT data. To this end, we construct a large library of models based on general relativistic magnetohydrodynamic (GRMHD) simulations and synthetic images produced by general relativistic ray tracing. We compare the observed visibilities with this library and confirm that the asymmetric ring is consistent with earlier predictions of strong gravitational lensing of synchrotron emission from a hot plasma orbiting near the black hole event horizon. The ring radius and ring asymmetry depend on black hole mass and spin, respectively, and both are therefore expected to be stable when observed in future EHT campaigns. Overall, the observed image is consistent with expectations for the shadow of a spinning Kerr black hole as predicted by general relativity. If the black hole spin and M87's large scale jet are aligned, then the black hole spin vector is pointed away from Earth. Models in our library of non-spinning black holes are inconsistent with the observations as they do not produce sufficiently powerful jets. At the same time, in those models that produce a sufficiently powerful jet, the latter is powered by extraction of black hole spin energy through mechanisms akin to the Blandford-Znajek process. We briefly consider alternatives to a black hole for the central compact object. Analysis of existing EHT polarization data and data taken simultaneously at other wavelengths will soon enable new tests of the GRMHD models, as will future EHT campaigns at 230 and 345 GHz."
MATTHEW FISH,First M87 Event Horizon Telescope results. VI. The shadow and mass of the central black hole,"We present measurements of the properties of the central radio source in M87 using Event Horizon Telescope data obtained during the 2017 campaign. We develop and fit geometric crescent models (asymmetric rings with interior brightness depressions) using two independent sampling algorithms that consider distinct representations of the visibility data. We show that the crescent family of models is statistically preferred over other comparably complex geometric models that we explore. We calibrate the geometric model parameters using general relativistic magnetohydrodynamic (GRMHD) models of the emission region and estimate physical properties of the source. We further fit images generated from GRMHD models directly to the data. We compare the derived emission region and black hole parameters from these analyses with those recovered from reconstructed images. There is a remarkable consistency among all methods and data sets. We find that >50% of the total flux at arcsecond scales comes from near the horizon, and that the emission is dramatically suppressed interior to this region by a factor >10, providing direct evidence of the predicted shadow of a black hole. Across all methods, we measure a crescent diameter of 42 ± 3 μas and constrain its fractional width to be <0.5. Associating the crescent feature with the emission surrounding the black hole shadow, we infer an angular gravitational radius of GM/Dc^2 = 3.8 ± 0.4 μas. Folding in a distance measurement of {16.8}_{-0.7}^{+0.8}{Mpc} gives a black hole mass of M = 6.5 ± 0.2{| }_{stat} ± 0.7{| }_{sys} × {10}^{9} {M}_{odot }. This measurement from lensed emission near the event horizon is consistent with the presence of a central Kerr black hole, as predicted by the general theory of relativity."
MATTHEW FISH,The Event Horizon general relativistic magnetohydrodynamic code comparison project,"Recent developments in compact object astrophysics, especially the discovery of merging neutron stars by LIGO, the imaging of the black hole in M87 by the Event Horizon Telescope, and high- precision astrometry of the Galactic Center at close to the event horizon scale by the GRAVITY experiment motivate the development of numerical source models that solve the equations of general relativistic magnetohydrodynamics (GRMHD). Here we compare GRMHD solutions for the evolution of a magnetized accretion flow where turbulence is promoted by the magnetorotational instability from a set of nine GRMHD codes: Athena++, BHAC, Cosmos++, ECHO, H-AMR, iharm3D, HARM-Noble, IllinoisGRMHD, and KORAL. Agreement among the codes improves as resolution increases, as measured by a consistently applied, specially developed set of code performance metrics. We conclude that the community of GRMHD codes is mature, capable, and consistent on these test problems."
MATTHEW FISH,First M87 Event Horizon Telescope results. IV. Imaging the central supermassive black hole,
MATTHEW FISH,Verification of radiative transfer schemes for the EHT,"The Event Horizon Telescope (EHT) Collaboration has recently produced the first resolved images of the central supermassive black hole in the giant elliptical galaxy M87. Here we report on tests of the consistency and accuracy of the general relativistic radiative transfer codes used within the collaboration to model M87* and Sgr A*. We compare and evaluate (1) deflection angles for equatorial null geodesics in a Kerr spacetime; (2) images calculated from a series of simple, parameterized matter distributions in the Kerr metric using simplified emissivities and absorptivities; (3) for a subset of codes, images calculated from general relativistic magnetohydrodynamics simulations using different realistic synchrotron emissivities and absorptivities; (4) observables for the 2017 configuration of EHT, including visibility amplitudes and closure phases. The error in total flux is of order 1% when the codes are run with production numerical parameters. The dominant source of discrepancies for small camera distances is the location and detailed setup of the software ""camera"" that each code uses to produce synthetic images. We find that when numerical parameters are suitably chosen and the camera is sufficiently far away the images converge and that for given transfer coefficients, numerical uncertainties are unlikely to limit parameter estimation for the current generation of EHT observations. The purpose of this paper is to describe a verification and comparison of EHT radiative transfer codes. It is not to verify EHT models more generally."
MATTHEW FISH,Monitoring the mmorphology of M87* in 2009–2017 with the Event Horizon Telescope,"The Event Horizon Telescope (EHT) has recently delivered the first resolved images of M87*, the supermassive black hole in the center of the M87 galaxy. These images were produced using 230 GHz observations performed in April 2017. Additional observations are required to investigate the persistence of the primary image feature – a ring with azimuthal brightness asymmetry – and to quantify the image variability on event horizon scales. To address this need, we analyze M87* data collected with prototype EHT arrays in 2009, 2011, 2012, and 2013. While these observations do not contain enough information to produce images, they are sufficient to constrain simple geometric models. We develop a modeling approach based on the framework utilized for the 2017 EHT data analysis and validate our procedures using synthetic data. Applying the same approach to the observational data sets, we find the M87* morphology in 2009–2017 to be consistent with a persistent asymmetric ring of 40 as diameter. The position angle of peak intensity varies in time. In particular, we find a significant difference between the position angle measured in 2013 and 2017. These variations are in broad agreement with predictions of a subset of general relativistic magnetohydrodynamic simulations. We show that quantifying the variability across multiple observational epochs has the potential to constrain physical properties of the source, such as the accretion state or the black hole spin."
MATTHEW FISH,THEMIS: a parameter estimation framework for the Event Horizon Telescope,"The Event Horizon Telescope (EHT) provides the unprecedented ability to directly resolve the structure and dynamics of black hole emission regions on scales smaller than their horizons. This has the potential to critically probe the mechanisms by which black holes accrete and launch outflows, and the structure of supermassive black hole spacetimes. However, accessing this information is a formidable analysis challenge for two reasons. First, the EHT natively produces a variety of data types that encode information about the image structure in nontrivial ways; these are subject to a variety of systematic effects associated with very long baseline interferometry and are supplemented by a wide variety of auxiliary data on the primary EHT targets from decades of other observations. Second, models of the emission regions and their interaction with the black hole are complex, highly uncertain, and computationally expensive to construct. As a result, the scientific utilization of EHT observations requires a flexible, extensible, and powerful analysis framework. We present such a framework, Themis, which defines a set of interfaces between models, data, and sampling algorithms that facilitates future development. We describe the design and currently existing components of Themis, how Themis has been validated thus far, and present additional analyses made possible by Themis that illustrate its capabilities. Importantly, we demonstrate that Themis is able to reproduce prior EHT analyses, extend these, and do so in a computationally efficient manner that can efficiently exploit modern high-performance computing facilities. Themis has already been used extensively in the scientific analysis and interpretation of the first EHT observations of M87."
MATTHEW FISH,First Sagittarius A* Event Horizon Telescope results. V. Testing astrophysical models of the galactic center black hole,"In this paper we provide a first physical interpretation for the Event Horizon Telescope's (EHT) 2017 observations of Sgr A*. Our main approach is to compare resolved EHT data at 230 GHz and unresolved non-EHT observations from radio to X-ray wavelengths to predictions from a library of models based on time-dependent general relativistic magnetohydrodynamics simulations, including aligned, tilted, and stellar-wind-fed simulations; radiative transfer is performed assuming both thermal and nonthermal electron distribution functions. We test the models against 11 constraints drawn from EHT 230 GHz data and observations at 86 GHz, 2.2 μm, and in the X-ray. All models fail at least one constraint. Light-curve variability provides a particularly severe constraint, failing nearly all strongly magnetized (magnetically arrested disk (MAD)) models and a large fraction of weakly magnetized models. A number of models fail only the variability constraints. We identify a promising cluster of these models, which are MAD and have inclination i ≤ 30°. They have accretion rate (5.2–9.5) × 10−9 M ⊙ yr−1, bolometric luminosity (6.8–9.2) × 1035 erg s−1, and outflow power (1.3–4.8) × 1038 erg s−1. We also find that all models with i ≥ 70° fail at least two constraints, as do all models with equal ion and electron temperature; exploratory, nonthermal model sets tend to have higher 2.2 μm flux density; and the population of cold electrons is limited by X-ray constraints due to the risk of bremsstrahlung overproduction. Finally, we discuss physical and numerical limitations of the models, highlighting the possible importance of kinetic effects and duration of the simulations."
MATTHEW FISH,First M87 Event Horizon Telescope results. VII. Polarization of the ring,"In 2017 April, the Event Horizon Telescope (EHT) observed the near-horizon region around the supermassive black hole at the core of the M87 galaxy. These 1.3 mm wavelength observations revealed a compact asymmetric ring-like source morphology. This structure originates from synchrotron emission produced by relativistic plasma located in the immediate vicinity of the black hole. Here we present the corresponding linear-polarimetric EHT images of the center of M87. We find that only a part of the ring is significantly polarized. The resolved fractional linear polarization has a maximum located in the southwest part of the ring, where it rises to the level of ∼15%. The polarization position angles are arranged in a nearly azimuthal pattern. We perform quantitative measurements of relevant polarimetric properties of the compact emission and find evidence for the temporal evolution of the polarized source structure over one week of EHT observations. The details of the polarimetric data reduction and calibration methodology are provided. We carry out the data analysis using multiple independent imaging and modeling techniques, each of which is validated against a suite of synthetic data sets. The gross polarimetric structure and its apparent evolution with time are insensitive to the method used to reconstruct the image. These polarimetric images carry information about the structure of the magnetic fields responsible for the synchrotron emission. Their physical interpretation is discussed in an accompanying publication."
MATTHEW FISH,First M87 Event Horizon Telescope results. VIII. Magnetic field structure near The Event Horizon,"Event Horizon Telescope (EHT) observations at 230 GHz have now imaged polarized emission around the supermassive black hole in M87 on event-horizon scales. This polarized synchrotron radiation probes the structure of magnetic fields and the plasma properties near the black hole. Here we compare the resolved polarization structure observed by the EHT, along with simultaneous unresolved observations with the Atacama Large Millimeter/submillimeter Array, to expectations from theoretical models. The low fractional linear polarization in the resolved image suggests that the polarization is scrambled on scales smaller than the EHT beam, which we attribute to Faraday rotation internal to the emission region. We estimate the average density n_e ∼ 10^4–7 cm^−3, magnetic field strength B ∼ 1–30 G, and electron temperature T_e ∼ (1–12) × 10^10 K of the radiating plasma in a simple one-zone emission model. We show that the net azimuthal linear polarization pattern may result from organized, poloidal magnetic fields in the emission region. In a quantitative comparison with a large library of simulated polarimetric images from general relativistic magnetohydrodynamic (GRMHD) simulations, we identify a subset of physical models that can explain critical features of the polarimetric EHT observations while producing a relativistic jet of sufficient power. The consistent GRMHD models are all of magnetically arrested accretion disks, where near-horizon magnetic fields are dynamically important. We use the models to infer a mass accretion rate onto the black hole in M87 of (3–20) × 10^−4 M ⊙ yr^−1."
MATTHEW FISH,Resolving the inner parsec of the blazar J1924–2914 with the event horizon telescope,"The blazar J1924–2914 is a primary Event Horizon Telescope (EHT) calibrator for the Galactic center’s black hole Sagittarius A*. Here we present the first total and linearly polarized intensity images of this source obtained with the unprecedented 20 μas resolution of the EHT. J1924–2914 is a very compact flat-spectrum radio source with strong optical variability and polarization. In April 2017 the source was observed quasi-simultaneously with the EHT (April 5–11), the Global Millimeter VLBI Array (April 3), and the Very Long Baseline Array (April 28), giving a novel view of the source at four observing frequencies, 230, 86, 8.7, and 2.3 GHz. These observations probe jet properties from the subparsec to 100 pc scales. We combine the multifrequency images of J1924–2914 to study the source morphology. We find that the jet exhibits a characteristic bending, with a gradual clockwise rotation of the jet projected position angle of about 90° between 2.3 and 230 GHz. Linearly polarized intensity images of J1924–2914 with the extremely fine resolution of the EHT provide evidence for ordered toroidal magnetic fields in the blazar compact core."
MATTHEW FISH,A universal power-law prescription for variability from synthetic images of black hole accretion flows,"We present a framework for characterizing the spatiotemporal power spectrum of the variability expected from the horizon-scale emission structure around supermassive black holes, and we apply this framework to a library of general relativistic magnetohydrodynamic (GRMHD) simulations and associated general relativistic ray-traced images relevant for Event Horizon Telescope (EHT) observations of Sgr A*. We find that the variability power spectrum is generically a red-noise process in both the temporal and spatial dimensions, with the peak in power occurring on the longest timescales and largest spatial scales. When both the time-averaged source structure and the spatially integrated light-curve variability are removed, the residual power spectrum exhibits a universal broken power-law behavior. On small spatial frequencies, the residual power spectrum rises as the square of the spatial frequency and is proportional to the variance in the centroid of emission. Beyond some peak in variability power, the residual power spectrum falls as that of the time-averaged source structure, which is similar across simulations; this behavior can be naturally explained if the variability arises from a multiplicative random field that has a steeper high-frequency power-law index than that of the time-averaged source structure. We briefly explore the ability of power spectral variability studies to constrain physical parameters relevant for the GRMHD simulations, which can be scaled to provide predictions for black holes in a range of systems in the optically thin regime. We present specific expectations for the behavior of the M87* and Sgr A* accretion flows as observed by the EHT."
MATTHEW FISH,Millimeter light curves of Sagittarius A* observed during the 2017 Event Horizon Telescope campaign,"The Event Horizon Telescope (EHT) observed the compact radio source, Sagittarius A* (Sgr A*), in the Galactic Center on 2017 April 5–11 in the 1.3 mm wavelength band. At the same time, interferometric array data from the Atacama Large Millimeter/submillimeter Array and the Submillimeter Array were collected, providing Sgr A* light curves simultaneous with the EHT observations. These data sets, complementing the EHT very long baseline interferometry, are characterized by a cadence and signal-to-noise ratio previously unattainable for Sgr A* at millimeter wavelengths, and they allow for the investigation of source variability on timescales as short as a minute. While most of the light curves correspond to a low variability state of Sgr A*, the April 11 observations follow an X-ray flare and exhibit strongly enhanced variability. All of the light curves are consistent with a red-noise process, with a power spectral density (PSD) slope measured to be between −2 and −3 on timescales between 1 minute and several hours. Our results indicate a steepening of the PSD slope for timescales shorter than 0.3 hr. The spectral energy distribution is flat at 220 GHz, and there are no time lags between the 213 and 229 GHz frequency bands, suggesting low optical depth for the event horizon scale source. We characterize Sgr A*’s variability, highlighting the different behavior observed just after the X-ray flare, and use Gaussian process modeling to extract a decorrelation timescale and a PSD slope. We also investigate the systematic calibration uncertainties by analyzing data from independent data reduction pipelines."
MATTHEW FISH,Selective dynamical imaging of interferometric data,"Recent developments in very long baseline interferometry (VLBI) have made it possible for the Event Horizon Telescope (EHT) to resolve the innermost accretion flows of the largest supermassive black holes on the sky. The sparse nature of the EHT’s (u, v)-coverage presents a challenge when attempting to resolve highly time-variable sources. We demonstrate that the changing (u, v)-coverage of the EHT can contain regions of time over the course of a single observation that facilitate dynamical imaging. These optimal time regions typically have projected baseline distributions that are approximately angularly isotropic and radially homogeneous. We derive a metric of coverage quality based on baseline isotropy and density that is capable of ranking array configurations by their ability to produce accurate dynamical reconstructions. We compare this metric to existing metrics in the literature and investigate their utility by performing dynamical reconstructions on synthetic data from simulated EHT observations of sources with simple orbital variability. We then use these results to make recommendations for imaging the 2017 EHT Sgr A* data set."
MATTHEW FISH,First Sagittarius A* Event Horizon Telescope results. VI. Testing the black hole metric,"Astrophysical black holes are expected to be described by the Kerr metric. This is the only stationary, vacuum, axisymmetric metric, without electromagnetic charge, that satisfies Einstein’s equations and does not have pathologies outside of the event horizon. We present new constraints on potential deviations from the Kerr prediction based on 2017 EHT observations of Sagittarius A* (Sgr A*). We calibrate the relationship between the geometrically defined black hole shadow and the observed size of the ring-like images using a library that includes both Kerr and non-Kerr simulations. We use the exquisite prior constraints on the mass-to-distance ratio for Sgr A* to show that the observed image size is within ∼10% of the Kerr predictions. We use these bounds to constrain metrics that are parametrically different from Kerr, as well as the charges of several known spacetimes. To consider alternatives to the presence of an event horizon, we explore the possibility that Sgr A* is a compact object with a surface that either absorbs and thermally reemits incident radiation or partially reflects it. Using the observed image size and the broadband spectrum of Sgr A*, we conclude that a thermal surface can be ruled out and a fully reflective one is unlikely. We compare our results to the broader landscape of gravitational tests. Together with the bounds found for stellar-mass black holes and the M87 black hole, our observations provide further support that the external spacetimes of all black holes are described by the Kerr metric, independent of their mass."
MATTHEW FISH,Polarimetric properties of Event Horizon Telescope targets from ALMA,"We present the results from a full polarization study carried out with the Atacama Large Millimeter/submillimeter Array (ALMA) during the first Very Long Baseline Interferometry (VLBI) campaign, which was conducted in 2017 April in the λ3 mm and λ1.3 mm bands, in concert with the Global mm-VLBI Array (GMVA) and the Event Horizon Telescope (EHT), respectively. We determine the polarization and Faraday properties of all VLBI targets, including Sgr A*, M87, and a dozen radio-loud active galactic nuclei (AGNs), in the two bands at several epochs in a time window of 10 days. We detect high linear polarization fractions (2%–15%) and large rotation measures (RM &gt; 103.3–105.5 rad m−2), confirming the trends of previous AGN studies at millimeter wavelengths. We find that blazars are more strongly polarized than other AGNs in the sample, while exhibiting (on average) order-of-magnitude lower RM values, consistent with the AGN viewing angle unification scheme. For Sgr A* we report a mean RM of (−4.2 ± 0.3) × 105 rad m−2 at 1.3 mm, consistent with measurements over the past decade and, for the first time, an RM of (–2.1 ± 0.1) × 105 rad m−2 at 3 mm, suggesting that about half of the Faraday rotation at 1.3 mm may occur between the 3 mm photosphere and the 1.3 mm source. We also report the first unambiguous measurement of RM toward the M87 nucleus at millimeter wavelengths, which undergoes significant changes in magnitude and sign reversals on a one year timescale, spanning the range from −1.2 to 0.3 × 105 rad m−2 at 3 mm and −4.1 to 1.5 × 105 rad m−2 at 1.3 mm. Given this time variability, we argue that, unlike the case of Sgr A*, the RM in M87 does not provide an accurate estimate of the mass accretion rate onto the black hole. We put forward a two-component model, comprised of a variable compact region and a static extended region, that can simultaneously explain the polarimetric properties observed by both the EHT (on horizon scales) and ALMA (which observes the combined emission from both components). These measurements provide critical constraints for the calibration, analysis, and interpretation of simultaneously obtained VLBI data with the EHT and GMVA."
MATTHEW FISH,"First Sagittarius A* Event Horizon Telescope results. IV. Variability, morphology, and black hole mass","In this paper we quantify the temporal variability and image morphology of the horizon-scale emission from Sgr A*, as observed by the EHT in 2017 April at a wavelength of 1.3 mm. We find that the Sgr A* data exhibit variability that exceeds what can be explained by the uncertainties in the data or by the effects of interstellar scattering. The magnitude of this variability can be a substantial fraction of the correlated flux density, reaching ∼100% on some baselines. Through an exploration of simple geometric source models, we demonstrate that ring-like morphologies provide better fits to the Sgr A* data than do other morphologies with comparable complexity. We develop two strategies for fitting static geometric ring models to the time-variable Sgr A* data; one strategy fits models to short segments of data over which the source is static and averages these independent fits, while the other fits models to the full data set using a parametric model for the structural variability power spectrum around the average source structure. Both geometric modeling and image-domain feature extraction techniques determine the ring diameter to be 51.8 ± 2.3 μas (68% credible intervals), with the ring thickness constrained to have an FWHM between ∼30% and 50% of the ring diameter. To bring the diameter measurements to a common physical scale, we calibrate them using synthetic data generated from GRMHD simulations. This calibration constrains the angular size of the gravitational radius to be 4.8_-0.7^+1.4 μas, which we combine with an independent distance measurement from maser parallaxes to determine the mass of Sgr A* to be 4.0_-0.6^+10^6 M⊙."
MATTHEW FISH,"First Sagittarius A* Event Horizon Telescope results. II. EHT and multiwavelength observations, data processing, and calibration","We present Event Horizon Telescope (EHT) 1.3 mm measurements of the radio source located at the position of the supermassive black hole Sagittarius A* (Sgr A*), collected during the 2017 April 5–11 campaign. The observations were carried out with eight facilities at six locations across the globe. Novel calibration methods are employed to account for Sgr A*'s flux variability. The majority of the 1.3 mm emission arises from horizon scales, where intrinsic structural source variability is detected on timescales of minutes to hours. The effects of interstellar scattering on the image and its variability are found to be subdominant to intrinsic source structure. The calibrated visibility amplitudes, particularly the locations of the visibility minima, are broadly consistent with a blurred ring with a diameter of ∼50 μas, as determined in later works in this series. Contemporaneous multiwavelength monitoring of Sgr A* was performed at 22, 43, and 86 GHz and at near-infrared and X-ray wavelengths. Several X-ray flares from Sgr A* are detected by Chandra, one at low significance jointly with Swift on 2017 April 7 and the other at higher significance jointly with NuSTAR on 2017 April 11. The brighter April 11 flare is not observed simultaneously by the EHT but is followed by a significant increase in millimeter flux variability immediately after the X-ray outburst, indicating a likely connection in the emission physics near the event horizon. We compare Sgr A*’s broadband flux during the EHT campaign to its historical spectral energy distribution and find that both the quiescent emission and flare emission are consistent with its long-term behavior."
MATTHEW FISH,The polarized image of a synchrotron-emitting ring of gas orbiting a black hole,"Synchrotron radiation from hot gas near a black hole results in a polarized image. The image polarization is determined by effects including the orientation of the magnetic field in the emitting region, relativistic motion of the gas, strong gravitational lensing by the black hole, and parallel transport in the curved spacetime. We explore these effects using a simple model of an axisymmetric, equatorial accretion disk around a Schwarzschild black hole. By using an approximate expression for the null geodesics derived by Beloborodov and conservation of the Walker–Penrose constant, we provide analytic estimates for the image polarization. We test this model using currently favored general relativistic magnetohydrodynamic simulations of M87*, using ring parameters given by the simulations. For a subset of these with modest Faraday effects, we show that the ring model broadly reproduces the polarimetric image morphology. Our model also predicts the polarization evolution for compact flaring regions, such as those observed from Sgr A* with GRAVITY. With suitably chosen parameters, our simple model can reproduce the EVPA pattern and relative polarized intensity in Event Horizon Telescope images of M87*. Under the physically motivated assumption that the magnetic field trails the fluid velocity, this comparison is consistent with the clockwise rotation inferred from total intensity images."
MATTHEW FISH,First Sagittarius A* Event Horizon Telescope results. I. The shadow of the supermassive black hole in the center of the Milky Way,"We present the first Event Horizon Telescope (EHT) observations of Sagittarius A* (Sgr A*), the Galactic center source associated with a supermassive black hole. These observations were conducted in 2017 using a global interferometric array of eight telescopes operating at a wavelength of λ = 1.3 mm. The EHT data resolve a compact emission region with intrahour variability. A variety of imaging and modeling analyses all support an image that is dominated by a bright, thick ring with a diameter of 51.8 ± 2.3 μas (68% credible interval). The ring has modest azimuthal brightness asymmetry and a comparatively dim interior. Using a large suite of numerical simulations, we demonstrate that the EHT images of Sgr A* are consistent with the expected appearance of a Kerr black hole with mass ∼4 × 106 M ⊙, which is inferred to exist at this location based on previous infrared observations of individual stellar orbits, as well as maser proper-motion studies. Our model comparisons disfavor scenarios where the black hole is viewed at high inclination (i &gt; 50°), as well as nonspinning black holes and those with retrograde accretion disks. Our results provide direct evidence for the presence of a supermassive black hole at the center of the Milky Way, and for the first time we connect the predictions from dynamical measurements of stellar orbits on scales of 103–105 gravitational radii to event-horizon-scale images and variability. Furthermore, a comparison with the EHT results for the supermassive black hole M87* shows consistency with the predictions of general relativity spanning over three orders of magnitude in central mass."
MATTHEW FISH,The variability of the black hole image in M87 at the dynamical timescale,"The black hole images obtained with the Event Horizon Telescope (EHT) are expected to be variable at the dynamical timescale near their horizons. For the black hole at the center of the M87 galaxy, this timescale (5–61 days) is comparable to the 6 day extent of the 2017 EHT observations. Closure phases along baseline triangles are robust interferometric observables that are sensitive to the expected structural changes of the images but are free of station-based atmospheric and instrumental errors. We explored the day-to-day variability in closure-phase measurements on all six linearly independent nontrivial baseline triangles that can be formed from the 2017 observations. We showed that three triangles exhibit very low day-to-day variability, with a dispersion of ∼3°–5°. The only triangles that exhibit substantially higher variability (∼90°–180°) are the ones with baselines that cross the visibility amplitude minima on the u–v plane, as expected from theoretical modeling. We used two sets of general relativistic magnetohydrodynamic simulations to explore the dependence of the predicted variability on various black hole and accretion-flow parameters. We found that changing the magnetic field configuration, electron temperature model, or black hole spin has a marginal effect on the model consistency with the observed level of variability. On the other hand, the most discriminating image characteristic of models is the fractional width of the bright ring of emission. Models that best reproduce the observed small level of variability are characterized by thin ring-like images with structures dominated by gravitational lensing effects and thus least affected by turbulence in the accreting plasmas."
MATTHEW FISH,Constraints on black-hole charges with the 2017 EHT observations of M87*,
MATTHEW FISH,SYMBA: an end-to-end VLBI synthetic data generation pipeline,"CONTEXT: Realistic synthetic observations of theoretical source models are essential for our understanding of real observational data. In using synthetic data, one can verify the extent to which source parameters can be recovered and evaluate how various data corruption effects can be calibrated. These studies are the most important when proposing observations of new sources, in the characterization of the capabilities of new or upgraded instruments, and when verifying model-based theoretical predictions in a direct comparison with observational data. AIMS: We present the SYnthetic Measurement creator for long Baseline Arrays (SYMBA), a novel synthetic data generation pipeline for Very Long Baseline Interferometry (VLBI) observations. SYMBA takes into account several realistic atmospheric, instrumental, and calibration effects. METHODS: We used SYMBA to create synthetic observations for the Event Horizon Telescope (EHT), a millimetre VLBI array, which has recently captured the first image of a black hole shadow. After testing SYMBA with simple source and corruption models, we study the importance of including all corruption and calibration effects, compared to the addition of thermal noise only. Using synthetic data based on two example general relativistic magnetohydrodynamics (GRMHD) model images of M 87, we performed case studies to assess the image quality that can be obtained with the current and future EHT array for different weather conditions. RESULTS: Our synthetic observations show that the effects of atmospheric and instrumental corruptions on the measured visibilities are significant. Despite these effects, we demonstrate how the overall structure of our GRMHD source models can be recovered robustly with the EHT2017 array after performing calibration steps, which include fringe fitting, a priori amplitude and network calibration, and self-calibration. With the planned addition of new stations to the EHT array in the coming years, images could be reconstructed with higher angular resolution and dynamic range. In our case study, these improvements allowed for a distinction between a thermal and a non-thermal GRMHD model based on salient features in reconstructed images."
MATTHEW FISH,Broadband multi-wavelength properties of M87 during the 2017 Event Horizon Telescope campaign,"In 2017, the Event Horizon Telescope (EHT) Collaboration succeeded in capturing the first direct image of the center of the M87 galaxy. The asymmetric ring morphology and size are consistent with theoretical expectations for a weakly accreting supermassive black hole of mass ∼6.5 × 109 M ⊙. The EHTC also partnered with several international facilities in space and on the ground, to arrange an extensive, quasi-simultaneous multi-wavelength campaign. This Letter presents the results and analysis of this campaign, as well as the multi-wavelength data as a legacy data repository. We captured M87 in a historically low state, and the core flux dominates over HST-1 at high energies, making it possible to combine core flux constraints with the more spatially precise very long baseline interferometry data. We present the most complete simultaneous multi-wavelength spectrum of the active nucleus to date, and discuss the complexity and caveats of combining data from different spatial scales into one broadband spectrum. We apply two heuristic, isotropic leptonic single-zone models to provide insight into the basic source properties, but conclude that a structured jet is necessary to explain M87’s spectrum. We can exclude that the simultaneous γ-ray emission is produced via inverse Compton emission in the same region producing the EHT mm-band emission, and further conclude that the γ-rays can only be produced in the inner jets (inward of HST-1) if there are strongly particle-dominated regions. Direct synchrotron emission from accelerated protons and secondaries cannot yet be excluded."
MATTHEW FISH,Event Horizon Telescope imaging of the archetypal blazar 3C 279 at an extreme 20 microarcsecond resolution,"3C 279 is an archetypal blazar with a prominent radio jet that show broadband flux density variability across the entire electromagnetic spectrum. We use an ultra-high angular resolution technique – global Very Long Baseline Interferometry (VLBI) at 1.3 mm (230 GHz) – to resolve the innermost jet of 3C 279 in order to study its fine-scale morphology close to the jet base where highly variable γ-ray emission is thought to originate, according to various models. The source was observed during four days in April 2017 with the Event Horizon Telescope at 230 GHz, including the phased Atacama Large Millimeter/submillimeter Array (ALMA), at an angular resolution of ∼20 μas (at a redshift of z = 0.536 this corresponds to ∼0.13 pc  ∼ 1700 Schwarzschild radii with a black hole mass M<jats:sub>BH</jats:sub> = 8 × 10^8 M_⊙). Imaging and model-fitting techniques were applied to the data to parameterize the fine-scale source structure and its variation. We find a multicomponent inner jet morphology with the northernmost component elongated perpendicular to the direction of the jet, as imaged at longer wavelengths. The elongated nuclear structure is consistent on all four observing days and across different imaging methods and model-fitting techniques, and therefore appears robust. Owing to its compactness and brightness, we associate the northern nuclear structure as the VLBI “core”. This morphology can be interpreted as either a broad resolved jet base or a spatially bent jet. We also find significant day-to-day variations in the closure phases, which appear most pronounced on the triangles with the longest baselines. Our analysis shows that this variation is related to a systematic change of the source structure. Two inner jet components move non-radially at apparent speeds of ∼15 c and ∼20 c (∼1.3 and ∼1.7 μas day^−1, respectively), which more strongly supports the scenario of traveling shocks or instabilities in a bent, possibly rotating jet. The observed apparent speeds are also coincident with the 3C 279 large-scale jet kinematics observed at longer (cm) wavelengths, suggesting no significant jet acceleration between the 1.3 mm core and the outer jet. The intrinsic brightness temperature of the jet components are ≲10^10 K, a magnitude or more lower than typical values seen at ≥7 mm wavelengths. The low brightness temperature and morphological complexity suggest that the core region of 3C 279 becomes optically thin at short (mm) wavelengths."
MATTHEW FISH,First M87 Event Horizon Telescope results. II. Array and instrumentation,"The Event Horizon Telescope (EHT) is a very long baseline interferometry (VLBI) array that comprises millimeter- and submillimeter-wavelength telescopes separated by distances comparable to the diameter of the Earth. At a nominal operating wavelength of ~1.3 mm, EHT angular resolution (λ/D) is ~25 μas, which is sufficient to resolve nearby supermassive black hole candidates on spatial and temporal scales that correspond to their event horizons. With this capability, the EHT scientific goals are to probe general relativistic effects in the strong-field regime and to study accretion and relativistic jet formation near the black hole boundary. In this Letter we describe the system design of the EHT, detail the technology and instrumentation that enable observations, and provide measures of its performance. Meeting the EHT science objectives has required several key developments that have facilitated the robust extension of the VLBI technique to EHT observing wavelengths and the production of instrumentation that can be deployed on a heterogeneous array of existing telescopes and facilities. To meet sensitivity requirements, high-bandwidth digital systems were developed that process data at rates of 64 gigabit s−1, exceeding those of currently operating cm-wavelength VLBI arrays by more than an order of magnitude. Associated improvements include the development of phasing systems at array facilities, new receiver installation at several sites, and the deployment of hydrogen maser frequency standards to ensure coherent data capture across the array. These efforts led to the coordination and execution of the first Global EHT observations in 2017 April, and to event-horizon-scale imaging of the supermassive black hole candidate in M87."
MATTHEW FISH,First M87 Event Horizon Telescope results. I. The shadow of the supermassive black hole,"When surrounded by a transparent emission region, black holes are expected to reveal a dark shadow caused by gravitational light bending and photon capture at the event horizon. To image and study this phenomenon, we have assembled the Event Horizon Telescope, a global very long baseline interferometry array observing at a wavelength of 1.3 mm. This allows us to reconstruct event-horizon-scale images of the supermassive black hole candidate in the center of the giant elliptical galaxy M87. We have resolved the central compact radio source as an asymmetric bright emission ring with a diameter of 42 ± 3 μas, which is circular and encompasses a central depression in brightness with a flux ratio gsim10:1. The emission ring is recovered using different calibration and imaging schemes, with its diameter and width remaining stable over four different observations carried out in different days. Overall, the observed image is consistent with expectations for the shadow of a Kerr black hole as predicted by general relativity. The asymmetry in brightness in the ring can be explained in terms of relativistic beaming of the emission from a plasma rotating close to the speed of light around a black hole. We compare our images to an extensive library of ray-traced general-relativistic magnetohydrodynamic simulations of black holes and derive a central mass of M = (6.5 ± 0.7) × 10^9 M ⊙. Our radio-wave observations thus provide powerful evidence for the presence of supermassive black holes in centers of galaxies and as the central engines of active galactic nuclei. They also present a new tool to explore gravity in its most extreme limit and on a mass scale that was so far not accessible."
MATTHEW FISH,First Sagittarius A* Event Horizon Telescope results. III. Imaging of the Galactic center supermassive black hole,"We present the first event-horizon-scale images and spatiotemporal analysis of Sgr A* taken with the Event Horizon Telescope in 2017 April at a wavelength of 1.3 mm. Imaging of Sgr A* has been conducted through surveys over a wide range of imaging assumptions using the classical CLEAN algorithm, regularized maximum likelihood methods, and a Bayesian posterior sampling method. Different prescriptions have been used to account for scattering effects by the interstellar medium toward the Galactic center. Mitigation of the rapid intraday variability that characterizes Sgr A* has been carried out through the addition of a “variability noise budget” in the observed visibilities, facilitating the reconstruction of static full-track images. Our static reconstructions of Sgr A* can be clustered into four representative morphologies that correspond to ring images with three different azimuthal brightness distributions and a small cluster that contains diverse nonring morphologies. Based on our extensive analysis of the effects of sparse (u, v)-coverage, source variability, and interstellar scattering, as well as studies of simulated visibility data, we conclude that the Event Horizon Telescope Sgr A* data show compelling evidence for an image that is dominated by a bright ring of emission with a ring diameter of ∼50 μas, consistent with the expected “shadow” of a 4 × 106 M⊙ black hole in the Galactic center located at a distance of 8 kpc."
MATTHEW FISH,Characterizing and mitigating intraday variability: reconstructing source structure in accreting black holes with mm-VLBI,"The extraordinary physical resolution afforded by the Event Horizon Telescope has opened a window onto the astrophysical phenomena unfolding on horizon scales in two known black holes, M87* and Sgr A*. However, with this leap in resolution has come a new set of practical complications. Sgr A* exhibits intraday variability that violates the assumptions underlying Earth aperture synthesis, limiting traditional image reconstruction methods to short timescales and data sets with very sparse (u, v) coverage. We present a new set of tools to detect and mitigate this variability. We develop a data-driven, model-agnostic procedure to detect and characterize the spatial structure of intraday variability. This method is calibrated against a large set of mock data sets, producing an empirical estimator of the spatial power spectrum of the brightness fluctuations. We present a novel Bayesian noise modeling algorithm that simultaneously reconstructs an average image and statistical measure of the fluctuations about it using a parameterized form for the excess variance in the complex visibilities not otherwise explained by the statistical errors. These methods are validated using a variety of simulated data, including general relativistic magnetohydrodynamic simulations appropriate for Sgr A* and M87*. We find that the reconstructed source structure and variability are robust to changes in the underlying image model. We apply these methods to the 2017 EHT observations of M87*, finding evidence for variability across the EHT observing campaign. The variability mitigation strategies presented are widely applicable to very long baseline interferometry observations of variable sources generally, for which they provide a data-informed averaging procedure and natural characterization of inter-epoch image consistency."
WEIWEI LIN,Intracellular density of wolbachia is mediated by host autophagy and the bacterial cytoplasmic incompatibility gene cifB in a cell type-dependent manner in drosophila melanogaster,"Autophagy is an intracellular degradation pathway involved in innate immunity. Pathogenic bacteria have evolved several mechanisms to escape degradation or exploit autophagy to acquire host nutrients. In the case of endosymbionts, which often have commensal or mutualistic interactions with the host, autophagy is not well characterized. We utilized tissue-specific autophagy mutants to determine if Wolbachia, a vertically transmitted obligate endosymbiont of Drosophila melanogaster, is regulated by autophagy in somatic and germ line cell types. Our analysis revealed core autophagy proteins Atg1 and Atg8 and a selective autophagy-specific protein Ref(2)p negatively regulate Wolbachia in the hub, a male gonad somatic cell type. Furthermore, we determined that the Wolbachia effector protein, CifB, modulates autophagy-Wolbachia interactions, identifying a new host-related pathway which these bacterial proteins interact with. In the female germ line, the cell type necessary for inheritance of Wolbachia through vertical transmission, we discovered that bulk autophagy mediated by Atg1 and Atg8 positively regulates Wolbachia density, whereas Ref(2)p had no effect. Global metabolomics of fly ovaries deficient in germ line autophagy revealed reduced lipid and carbon metabolism, implicating metabolites from these pathways as positive regulators of Wolbachia Our work provides further understanding of how autophagy affects bacteria in a cell type-dependent manner.IMPORTANCE Autophagy is a eukaryotic intracellular degradation pathway which can act as an innate immune response to eliminate pathogens. Conversely, pathogens can evolve proteins which modulate the autophagy pathway to subvert degradation and establish an infection. Wolbachia, a vertically transmitted obligate endosymbiont which infects up to 40% of insect species, is negatively regulated by autophagy in whole animals, but the specific molecular mechanism and tissue which govern this interaction remain unknown. Our studies use cell type-specific autophagy mutants to reveal that Wolbachia is negatively regulated by selective autophagy in the soma, while nonselective autophagy positively regulates Wolbachia in the female germ line. These data provide evidence that cell type can drive different basal autophagy programs which modulate intracellular microbes differently. Additionally, we identified that the Wolbachia effector CifB acts in the selective autophagy pathway to aid in intracellular bacterial survival, providing a new function for CifB beyond its previously identified role in reproductive manipulation."
DOUGLAS PARSONS,"BMQ : Boston medical quarterly: v. 3, no. 1-4",
DOUGLAS PARSONS,"BMQ : Boston medical quarterly: v. 2, no. 1-4",
KARINA SCAVO,"The importance of mangroves as coral habitat in a deteriorating ocean – an ecological, demographic, and genomic research program on model mangrove corals in the genus Porites","Coral reefs and many of the coral species that construct them are in a state of global decline, and this rapid decline of reef corals has incited worldwide conservation, restoration, and research efforts. One growing area of research involves exploring the value of reef-associated habitats, like mangroves, for the survival of particular coral species and the resilience of nearby reefs. Mangroves have long been regarded as inhospitable habitat for corals due to their combination of low light levels, low water clarity, and fluctuating water temperatures. However, roughly half of the coral species living on Caribbean reefs have been documented to inhabit mangrove habitats. If mangroves constitute a critical component of the ecological niche for some corals, then they should support viable, self-sustaining populations. If mangroves contribute directly to the resilience of reef coral populations, then corals must be able to exploit both habitat types coping with varying environmental conditions. Despite the growing number of documented mangrove-coral communities, these questions have not been studied. Here I use two Caribbean coral species from the genus Porites that are known to inhabit both reef and mangrove habitats (P. astreoides and P. divaricata) to explore the contribution of mangrove habitats to coral survival via the following objectives: 1) Characterize the survival, growth, and spatial-distribution of a mangrove-dwelling population of P. divaricata through a multi-year field study; 2) Determine if the population characterized in objective 1 is capable of self-recruitment and map dispersal through a population genomic study using 2bRAD markers; 3) Determine how key phenotypic traits vary between reef and mangrove habitats in P. divaricata and P. astreiodes, using a cross-habitat comparative field-study. Results suggest that mangroves may serve as important habitat for some reef corals due to their ability to support a viable, stable, and healthy coral population that is self-sustaining largely through asexual reproduction. Subsequently, mangrove and reef corals display predictable differences in phenotype, that could help reveal how a “reef coral” can exploit darker mangroves. This work can inform the design of marine protected areas, whereby both mangroves as well as the connection between mangroves and reefs is preserved to facilitate coral survival."
KARINA SCAVO,"Distinct phenotypes associated with mangrove and lagoon habitats in two widespread caribbean corals, porites astreoides and porites divaricata","AbstractAs coral reefs experience dramatic declines in coral cover throughout the tropics, there is an urgent need to understand the role that non-reef habitats, such as mangroves, play in the ecological niche of corals. Mangrove habitats present a challenge to reef-dwelling corals because they can differ dramatically from adjacent reef habitats with respect to key environmental parameters, such as light. Because variation in light within reef habitats is known to drive intraspecific differences in coral phenotype, we hypothesized that coral species that can exploit both reef and mangrove habitats will exhibit predictable differences in phenotypes between habitats. To investigate how intraspecific variation, driven by either local adaptation or phenotypic plasticity, might enable particular coral species to exploit these two qualitatively different habitat types, we compared the phenotypes of two widespread Caribbean corals, Porites divaricata and Porites astreoides, in mangrove versus lagoon habitats on Turneffe Atoll, Belize. We document significant differences in colony size, color, structural complexity, and corallite morphology between habitats. In every instance, the phenotypic differences between mangrove prop root and lagoon corals exhibited consistent trends in both P. divaricata and P. astreoides. We believe this study is the first to document intraspecific phenotypic diversity in corals occupying mangrove prop root versus lagoonal patch reef habitats. A difference in the capacity to adopt an alternative phenotype that is well suited to the mangrove habitat may explain why some reef coral species can exploit mangroves, while others cannot."
KARINA SCAVO,"Using bioindicator species to characterize distinct mangrove habitats on Turneffe Atoll, Belize","Mangroves are critical to the biodiversity and productivity of tropical ecosystems, and for this reason, they are part of the foundation for Belize’s Blue Economy. Despite their ecological and economic significance, mangroves are among the most threatened marine habitats in the world. Turneffe Atoll, home to the largest and newest marine reserve in Belize, is unique among the four atolls located along the Mesoamerican barrier reef in that it is dominated by mangroves. In an effort to investigate the role of Turneffe’s mangroves in supporting marine biodiversity, we conducted longitudinal surveys of the epibionts (marine species inhabiting the submerged mangrove roots). From 2016-2017, in two low-flow ponds and two high-flow channels, we scored the presence or absence of 28 indicator species on 182 roots (a total of 5096 observations). We observed significant differences between sites in the proportion of roots occupied by several indicator species, and we documented substantial stability from year-to-year in the presence of specific indicators. We also conducted exhaustive surveys of mangrove corals, revealing that particular mangroves on Turneffe appear to be important habitat for corals. Over a 4-year period, one population of the thin finger coral (Porites divaricata) occupying a few hundred meters of shoreline at Calabash Caye exhibited gradual population growth over multiple years. While all mangrove-lined shores may appear similar to observers on passing boats, beneath the water, mangrove communities are diverse, and this diversity is critical to their wider role in the health of coastal ecosystems. The study described here revealed that mangrove epibiont diversity on Turneffe Atoll varies over fine spatial scale in a manner that may be predictable, e.g., some mangrove species are generalists, while others are specialists found in selective mangrove habitats. Of course, “natural” patterns of species distribution are a reflection of habitat quality, which is directly impacted by human activities. Continuing longitudinal studies of biodiversity in these understudied habitats will be essential to monitoring the health of Turneffe Atoll and the health of Belize’s coastal ecosystems generally."
KARINA SCAVO,"Corals of the genus Porites are a locally abundant component of the epibiont community on mangrove prop roots at Calabash Caye, Turneffe Atoll, Belize","Mangroves are generally regarded as inhospitable for corals, but recent reports suggest they provide ecological refuge for some species. We surveyed diverse mangrove habitats on Turneffe Atoll, Belize, documenting 127 colonies of Porites divaricata (Thin Finger Coral) along 1858 m of mangrove prop roots at Calabash Caye and a much more diverse coral assemblage at Crooked Creek. At Calabash, corals were highly clumped, and varied widely in size and morphology, including large well-arborized colonies, encrusting forms with few branches, and new recruits with no branches, suggesting an age-structuredpopulation exhibiting extensive morphological plasticity. The data described here contributeto an emerging picture of mangroves as potentially critical habitat for many Caribbeancoral species."
PABLO D PEREZ,"Cosmology intertwined: a review of the particle physics, astrophysics, and cosmology associated with the cosmological tensions and anomalies",
PABLO D PEREZ,"First Sagittarius A* Event Horizon Telescope results. II. EHT and multiwavelength observations, data processing, and calibration","We present Event Horizon Telescope (EHT) 1.3 mm measurements of the radio source located at the position of the supermassive black hole Sagittarius A* (Sgr A*), collected during the 2017 April 5–11 campaign. The observations were carried out with eight facilities at six locations across the globe. Novel calibration methods are employed to account for Sgr A*'s flux variability. The majority of the 1.3 mm emission arises from horizon scales, where intrinsic structural source variability is detected on timescales of minutes to hours. The effects of interstellar scattering on the image and its variability are found to be subdominant to intrinsic source structure. The calibrated visibility amplitudes, particularly the locations of the visibility minima, are broadly consistent with a blurred ring with a diameter of ∼50 μas, as determined in later works in this series. Contemporaneous multiwavelength monitoring of Sgr A* was performed at 22, 43, and 86 GHz and at near-infrared and X-ray wavelengths. Several X-ray flares from Sgr A* are detected by Chandra, one at low significance jointly with Swift on 2017 April 7 and the other at higher significance jointly with NuSTAR on 2017 April 11. The brighter April 11 flare is not observed simultaneously by the EHT but is followed by a significant increase in millimeter flux variability immediately after the X-ray outburst, indicating a likely connection in the emission physics near the event horizon. We compare Sgr A*’s broadband flux during the EHT campaign to its historical spectral energy distribution and find that both the quiescent emission and flare emission are consistent with its long-term behavior."
PABLO D PEREZ,First Sagittarius A* Event Horizon Telescope results. I. The shadow of the supermassive black hole in the center of the Milky Way,"We present the first Event Horizon Telescope (EHT) observations of Sagittarius A* (Sgr A*), the Galactic center source associated with a supermassive black hole. These observations were conducted in 2017 using a global interferometric array of eight telescopes operating at a wavelength of λ = 1.3 mm. The EHT data resolve a compact emission region with intrahour variability. A variety of imaging and modeling analyses all support an image that is dominated by a bright, thick ring with a diameter of 51.8 ± 2.3 μas (68% credible interval). The ring has modest azimuthal brightness asymmetry and a comparatively dim interior. Using a large suite of numerical simulations, we demonstrate that the EHT images of Sgr A* are consistent with the expected appearance of a Kerr black hole with mass ∼4 × 106 M ⊙, which is inferred to exist at this location based on previous infrared observations of individual stellar orbits, as well as maser proper-motion studies. Our model comparisons disfavor scenarios where the black hole is viewed at high inclination (i &gt; 50°), as well as nonspinning black holes and those with retrograde accretion disks. Our results provide direct evidence for the presence of a supermassive black hole at the center of the Milky Way, and for the first time we connect the predictions from dynamical measurements of stellar orbits on scales of 103–105 gravitational radii to event-horizon-scale images and variability. Furthermore, a comparison with the EHT results for the supermassive black hole M87* shows consistency with the predictions of general relativity spanning over three orders of magnitude in central mass."
YOONJOO LEE,Sustained CA2+ mobilizations: a quantitative approach to predict their importance in cell-cell communication,"Epithelial wound healing requires the coordination of cells to migrate as a unit over the basement membrane after injury. An excellent model tissue is the corneal epithelium, which is an avascular stratified squamous tissue that responds to growth factors and nucleotides when the epithelial barrier is damaged. One signal that has a ubiquitous response in epithelial wound healing is the cellular release of the nucleotide ATP, which may occur because of mechanics forces and/or change in cell shape. Within milliseconds to seconds after injury, extracellular ATP binds to purinoreceptors and triggers a transient Ca2+ wave, which is used by cells to transduce mechanical signals into chemical signals and alter signaling pathways. To understand the process of this coordinated movement, it is critical to study the dynamics of cell-cell communication. In this study we developed a novel method to identify and characterize the degree of cell-cell communication that occurs through sustained Ca2+ mobilizations after injury, which are concentrated along the epithelial wound edge and reduced in cells distal to the injury. Using MATLAB analyses, we generated profiles of the sustained Ca2+ mobilizations, and demonstrated that the Ca2+ response was replicated in ex vivo organ culture models. The sustained Ca2+ mobilizations were present also after stimulation with either BzATP or UTP, which are agonists of P2X7 and P2Y2 respectively. The probability that cells would communicate was greater in response to BzATP compared to UTP. The specificity of these ligands was demonstrated using competitive inhibitors of P2Y2 and P2X7 receptors, AR-C 118925XX and A438079, respectively. An inhibitor of pannexin-1, 10Panx, attenuated both wound closure and BzATP agonist-initiated response. These sustained mobilizations are correlated with changes in cellular morphology and motility, which were prominent in cells at the leading edge of the wound during cell migration. Together, our results demonstrate that the sustained Ca2+ mobilizations mediated by purinoreceptors and pannexins are a vital component in regulating the long-term response to injury, as studied in organ culture."
INDORICA SUTRADHAR,Using one health approaches to study effects of antibiotic stewardship on AMR development,"Antimicrobial resistance (AMR) is a growing global threat to public health expected to impact 10 million people by 2050, with a disproportionate effect on low- and middle- income countries, that is further exacerbated in communities living in urban informal settlements and refugee camps. As a result, there is a heightened urgency to understand how current antibiotic use is driving the spread of drug resistance in communities with high population density and those that are in proximity to wastewater settings and environmentally contaminated surroundings. Currently, there is a limited quantitative and mechanistic understanding of the evolution and spread of multidrug resistant (MDR) pathogens in these complex settings where there are a multitude of antibiotic residues and bacterial species present. Computational and experimental work in this area can lead to predictive outcomes and more effective strategies to prevent outbreaks of resistant pathogens. The goal of this thesis was to develop and test an integrated mathematical modeling and high-throughput experimental approach to quantitatively analyze AMR evolution in complex environments. The mathematical model captures predicted behavior for systems with multiple antibiotic residues and metal ions, incorporating the effects of both antibiotic-antibiotic interactions and metal-antibiotic interactions. This model is rooted in fundamental principles of biological systems modeling and was continuously integrated with a novel experimental workflow utilizing the eVOLVER for rapid iterative model development and validation. This work has resulted in the development of a robust method of understanding and predicting the development and spread of MDR bacteria in complex environments and has the potential to provide robust strategies to protect the health of vulnerable populations in these environments."
JOHN KENGERE OKECHI,"An ecosystem-based approach to balancing cage aquaculture, capture fisheries, and biodiversity conservation in Lake Victoria, Kenya","Lake Victoria is known for its cichlid fish species flock of 500 or more, which have been drastically decreased due to mass extinction. The lake's fisheries transformed from artisanal to industrial, with exotic species displacing the indigenous flock, changes linked to local and global anthropogenic consequences. Cage aquaculture has been established in the lake as a result of dwindling catch fisheries, a growing human population, and increased demand for fish. This dissertation investigates: 1) the distributional ecology of fishes along a limnological gradient in Lake Victoria, Kenya; and 2) the effects of cage aquaculture in the lake on limnology and fish communities, as well as the scientific and social correlates of proper implementation and growth. In April/May and July/August 2017, fish distribution patterns in the lake were surveyed using gillnets at eleven littoral sites and trawls at thirty lake-wide locations. From November 2018 to July 2019, four sites arrayed on an inshore-offshore gradient were sampled using paired cages and control stations. Using established protocols, water quality variables were sampled and analyzed. The status of variables and their associations were investigated using descriptive and exploratory statistics in the R statistical programming language. There was a limnological gradient, with nutrient concentrations, chlorophyll-a, and turbidity decreasing dramatically from the inner gulf to the outer waters. In the gulf's eutrophic waters, indigenous catfishes and cyprinids were abundant, while Nile perch and haplochromines were abundant in the open less eutrophic waters. Along the inner gulf-open lake gradient, Nile perch population structure, size at 50% maturity, and feeding patterns differed. There were no significant variations in environmental metrics between the paired cage farms and the controls, implying that inputs like sewage and agricultural runoff contribute more to eutrophication and the state of the gulf. Near cages, the average monthly total fish biomass was higher than in control areas. In the inner gulf, non-haplochromine fishes were many and diversified, with some species being particularly prevalent near cages. Based on biophysical constraints and overlap between cage aquaculture, fisheries, and biodiversity conservation in the lake, it was projected that cages could yield 250,000 metric tons of Nile tilapia per year. The findings indicate that, when correctly managed, cage aquaculture in Lake Victoria has a positive impact on both biodiversity and economic prosperity in the region."
YAGE DING,Orthogonal inducible control of Cas13 circuits enables programmable RNA regulation in mammalian cells,"RNA plays an indispensable role in mammalian cell functions. Cas13, a class of RNA-guided ribonuclease, is a flexible tool for modifying and regulating coding and non-coding RNAs, with enormous potential for creating new cell functions. However, the lack of control over Cas13 activity has limited its cell engineering capability. Here, I present the CRISTAL (Control of RNA with Inducible SpliT CAs13 Orthologs and Exogenous Ligands) platform. CRISTAL is powered by a collection (10 total) of orthogonal split inducible Cas13s that can be turned ON or OFF via small molecules in multiple cell types, providing precise temporal control. Also, I engineered Cas13 logic circuits that can respond to endogenous signaling and exogenous small molecule inputs. Furthermore, the orthogonality, low leakiness, and high dynamic range of the inducible Cas13d and Cas13b enable the design and construction of a robust incoherent feedforward loop, leading to near-perfect and tunable adaptation response. Finally, using the inducible Cas13s, I achieve simultaneous multiplexed control of multiple genes in vitro and in mice. Together, the CRISTAL design represents a powerful platform for precisely regulating RNA dynamics to advance cell engineering and elucidate RNA biology."
CHERNOH SALLIEU JALLOH,Mechanisms of viral RNA-induced inflammation: molecular perspectives on inflammasome activation in myeloid cells,"Enveloped RNA viruses like human immunodeficiency virus type-1 (HIV-1) and SARS-CoV-2 enter host cells through fusion with the plasma membrane, a process facilitated by specific viral envelope proteins that recognize and bind to receptors expressed on the host cell surface. These receptors can diverge based on the type of cell and virus. For HIV-1, the primary receptors on myeloid cells are CD4 and CCR5 or CXCR4. For SARS-CoV-2, although the primary receptor is ACE2, other myeloid-cell specific sialic acid binding lectins can also facilitate entry. Following cellular invasion, different viral RNA species can be detected by distinct host nucleic acid sensors, resulting in type I interferons and pro-inflammatory cytokine induction. While these innate immune responses are essential for controlling viral infections, overactivation can lead to chronic inflammation, tissue damage, and disease pathogenesis. Herein, I examine the contribution of HIV-1 and SARS-CoV-2 de-novo RNA expression and the molecular mechanisms that contribute to innate immune activation in myeloid cells. Despite advancements in combination antiretroviral therapy (ART) in suppressing systemic viral replication in individuals infected with HIV, residual viral RNA expression in tissue reservoirs remains a significant hindrance to curative efforts. I hypothesized that persistent expression of viral RNAs in myeloid cells triggers dysregulated innate immune activation, and inflammasomes activation. This study centers on the long-lived tissue-resident innate immune cells - macrophages and microglia, which, owing to their self-renewing nature, operate as reservoirs of viral RNA production, and are thought to lead to chronic immune activation even in the absence of productive replication. Our previous studies suggest that de novo expression of unspliced intron-containing HIV-1 RNA (herein referred to as icRNA) triggers activation of pro-inflammatory cytokines in myeloid cells. Here, I demonstrate that cytosolic expression of HIV-1 icRNA, but not multiply-spliced viral RNAs induces inflammasome activation, LDH release and IL-1β secretion in productively infected monocyte-derived macrophages (MDM) and induced pluripotent stem cell (iPSC)-derived microglia. Interestingly, knockdown of RLRs, RIG-I and MDA5 or endosomal TLRs failed to abrogate HIV-1 icRNA-induced IL-1β secretion. Rather, knockdown of NLRP1, but not NLRP3, inflammasome resulted in a significant reduction in IL-1β secretion, underscoring NLRP1's pivotal role in the HIV-1 icRNA-induced IL-1β secretion. Furthermore, Rev-Crm1-dependent nucleocytoplasmic export of HIV-1 icRNA was required for NLRP1-mediated Caspase-1 activation, IL-1β secretion, LDH release and cell death. Similarly, SARS-CoV-2, while not establishing productive infection in macrophages, can activate these cells, contributing to a hyper-inflammatory response marked by the heightened expression of pro-inflammatory cytokines, which is understood to be a principal driver of COVID-19 pathology. SARS-CoV-2 established an abortive infection in macrophages. CD169, a macrophage-specific sialic-acid binding lectin, mediated ACE2-independent SARS-CoV-2 entry in human macrophages and establishment of restricted infection. Interestingly, CD169-mediated SARS-CoV-2 entry in macrophages led to the expression of viral genomic and subgenomic RNAs, with negligible viral protein expression and no release of infectious virus particles, implying a post-entry restriction to SARS-CoV-2 replication in macrophages that was curbed by exogenous ACE2 expression. Despite restricted viral RNA expression, cytoplasmic RLRs, RIG-I and MDA5, sensed abortive viral transcripts, and induced pro-inflammatory responses in a MAVS dependent manner. This dissertation reveals striking parallels between the role of viral RNAs in driving pro-inflammatory responses in HIV-1 and SARS-CoV-2 infections. These findings collectively underscore the central role of cytoplasmic sensing of viral RNAs and their contribution to chronic inflammation in virus-infected myeloid cells. Elucidating these molecular mechanisms further may pave the way for novel therapeutic interventions to mitigate the persistent innate immune activation and immunopathology detected in HIV-1 and SARS-CoV-2 infected individuals."
ROBBERT LANGWERDEN,Trial-based cognitive therapy: efficacy of a new CBT approach for treating social anxiety disorder with comorbid depression,"The present study aims to evaluate the efficacy of Trial-Based Cognitive Therapy (TBCT), a new cognitive-behavioral therapy approach, for generalized social anxiety disorder (GSAD) in a population with high rates of comorbid disorders, especially depression. This two-arm randomized clinical trial included 39 adults (TBCT = 18; waitlist group = 21) diagnosed with GSAD. The TBCT group received 16 weekly sessions of individual TBCT. Symptom severity was assessed at pre- and post-treatment. Participants in the TBCT group showed reduction in social anxiety, social avoidance, and depression, all associated with a large effect size. No differences between pre- and post-treatment scores were observed in the waitlist condition. Results also showed that comorbidity significantly moderated treatment efficacy. Patients with comorbid conditions showed greater reductions in social anxiety symptoms across treatment than those with SAD only. In summary, TBCT was effective in reducing social anxiety and depressive symptoms, particularly for patients with comorbidity."
WILLIAM SHAW,"Canvass: a crowd-sourced, natural-product screening library for exploring biological space",
WILLIAM SHAW,First Sagittarius A* Event Horizon Telescope results. I. The shadow of the supermassive black hole in the center of the Milky Way,"We present the first Event Horizon Telescope (EHT) observations of Sagittarius A* (Sgr A*), the Galactic center source associated with a supermassive black hole. These observations were conducted in 2017 using a global interferometric array of eight telescopes operating at a wavelength of λ = 1.3 mm. The EHT data resolve a compact emission region with intrahour variability. A variety of imaging and modeling analyses all support an image that is dominated by a bright, thick ring with a diameter of 51.8 ± 2.3 μas (68% credible interval). The ring has modest azimuthal brightness asymmetry and a comparatively dim interior. Using a large suite of numerical simulations, we demonstrate that the EHT images of Sgr A* are consistent with the expected appearance of a Kerr black hole with mass ∼4 × 106 M ⊙, which is inferred to exist at this location based on previous infrared observations of individual stellar orbits, as well as maser proper-motion studies. Our model comparisons disfavor scenarios where the black hole is viewed at high inclination (i &gt; 50°), as well as nonspinning black holes and those with retrograde accretion disks. Our results provide direct evidence for the presence of a supermassive black hole at the center of the Milky Way, and for the first time we connect the predictions from dynamical measurements of stellar orbits on scales of 103–105 gravitational radii to event-horizon-scale images and variability. Furthermore, a comparison with the EHT results for the supermassive black hole M87* shows consistency with the predictions of general relativity spanning over three orders of magnitude in central mass."
WILLIAM SHAW,Prospects for beyond the standard model physics searches at the deep underground neutrino experiment: DUNE collaboration,"The Deep Underground Neutrino Experiment (DUNE) will be a powerful tool for a variety of physics topics. The high-intensity proton beams provide a large neutrino flux, sampled by a near detector system consisting of a combination of capable precision detectors, and by the massive far detector system located deep underground. This configuration sets up DUNE as a machine for discovery, as it enables opportunities not only to perform precision neutrino measurements that may uncover deviations from the present three-flavor mixing paradigm, but also to discover new particles and unveil new interactions and symmetries beyond those predicted in the Standard Model (SM). Of the many potential beyond the Standard Model (BSM) topics DUNE will probe, this paper presents a selection of studies quantifying DUNE's sensitivities to sterile neutrino mixing, heavy neutral leptons, non-standard interactions, CPT symmetry violation, Lorentz invariance violation, neutrino trident production, dark matter from both beam induced and cosmogenic sources, baryon number violation, and other new physics topics that complement those at high-energy colliders and significantly extend the present reach."
WILLIAM SHAW,Identification and reconstruction of low-energy electrons in the ProtoDUNE-SP detector,
WILLIAM SHAW,The long-baseline neutrino experiment: exploring fundamental symmetries of the universe,"The preponderance of matter over antimatter in the early Universe, the dynamics of the supernova bursts that produced the heavy elements necessary for life and whether protons eventually decay --- these mysteries at the forefront of particle physics and astrophysics are key to understanding the early evolution of our Universe, its current state and its eventual fate. The Long-Baseline Neutrino Experiment (LBNE) represents an extensively developed plan for a world-class experiment dedicated to addressing these questions. LBNE is conceived around three central components: (1) a new, high-intensity neutrino source generated from a megawatt-class proton accelerator at Fermi National Accelerator Laboratory, (2) a near neutrino detector just downstream of the source, and (3) a massive liquid argon time-projection chamber deployed as a far detector deep underground at the Sanford Underground Research Facility. This facility, located at the site of the former Homestake Mine in Lead, South Dakota, is approximately 1,300 km from the neutrino source at Fermilab -- a distance (baseline) that delivers optimal sensitivity to neutrino charge-parity symmetry violation and mass ordering effects. This ambitious yet cost-effective design incorporates scalability and flexibility and can accommodate a variety of upgrades and contributions. With its exceptional combination of experimental configuration, technical capabilities, and potential for transformative discoveries, LBNE promises to be a vital facility for the field of particle physics worldwide, providing physicists from around the globe with opportunities to collaborate in a twenty to thirty year program of exciting science. In this document we provide a comprehensive overview of LBNE's scientific objectives, its place in the landscape of neutrino physics worldwide, the technologies it will incorporate and the capabilities it will possess."
WILLIAM SHAW,Scintillation light detection in the 6-m drift-length ProtoDUNE Dual Phase liquid argon TPC,"DUNE is a dual-site experiment for long-baseline neutrino oscillation studies, neutrino astrophysics and nucleon decay searches. ProtoDUNE Dual Phase (DP) is a 6  ×  6  ×  6 m 3 liquid argon time-projection-chamber (LArTPC) that recorded cosmic-muon data at the CERN Neutrino Platform in 2019-2020 as a prototype of the DUNE Far Detector. Charged particles propagating through the LArTPC produce ionization and scintillation light. The scintillation light signal in these detectors can provide the trigger for non-beam events. In addition, it adds precise timing capabilities and improves the calorimetry measurements. In ProtoDUNE-DP, scintillation and electroluminescence light produced by cosmic muons in the LArTPC is collected by photomultiplier tubes placed up to 7 m away from the ionizing track. In this paper, the ProtoDUNE-DP photon detection system performance is evaluated with a particular focus on the different wavelength shifters, such as PEN and TPB, and the use of Xe-doped LAr, considering its future use in giant LArTPCs. The scintillation light production and propagation processes are analyzed and a comparison of simulation to data is performed, improving understanding of the liquid argon properties."
WILLIAM SHAW,"The medical student: v. 5, no. 1-8",
WILLIAM SHAW,Event Horizon Telescope imaging of the archetypal blazar 3C 279 at an extreme 20 microarcsecond resolution,"3C 279 is an archetypal blazar with a prominent radio jet that show broadband flux density variability across the entire electromagnetic spectrum. We use an ultra-high angular resolution technique – global Very Long Baseline Interferometry (VLBI) at 1.3 mm (230 GHz) – to resolve the innermost jet of 3C 279 in order to study its fine-scale morphology close to the jet base where highly variable γ-ray emission is thought to originate, according to various models. The source was observed during four days in April 2017 with the Event Horizon Telescope at 230 GHz, including the phased Atacama Large Millimeter/submillimeter Array (ALMA), at an angular resolution of ∼20 μas (at a redshift of z = 0.536 this corresponds to ∼0.13 pc  ∼ 1700 Schwarzschild radii with a black hole mass M<jats:sub>BH</jats:sub> = 8 × 10^8 M_⊙). Imaging and model-fitting techniques were applied to the data to parameterize the fine-scale source structure and its variation. We find a multicomponent inner jet morphology with the northernmost component elongated perpendicular to the direction of the jet, as imaged at longer wavelengths. The elongated nuclear structure is consistent on all four observing days and across different imaging methods and model-fitting techniques, and therefore appears robust. Owing to its compactness and brightness, we associate the northern nuclear structure as the VLBI “core”. This morphology can be interpreted as either a broad resolved jet base or a spatially bent jet. We also find significant day-to-day variations in the closure phases, which appear most pronounced on the triangles with the longest baselines. Our analysis shows that this variation is related to a systematic change of the source structure. Two inner jet components move non-radially at apparent speeds of ∼15 c and ∼20 c (∼1.3 and ∼1.7 μas day^−1, respectively), which more strongly supports the scenario of traveling shocks or instabilities in a bent, possibly rotating jet. The observed apparent speeds are also coincident with the 3C 279 large-scale jet kinematics observed at longer (cm) wavelengths, suggesting no significant jet acceleration between the 1.3 mm core and the outer jet. The intrinsic brightness temperature of the jet components are ≲10^10 K, a magnitude or more lower than typical values seen at ≥7 mm wavelengths. The low brightness temperature and morphological complexity suggest that the core region of 3C 279 becomes optically thin at short (mm) wavelengths."
WILLIAM SHAW,First M87 Event Horizon Telescope results. II. Array and instrumentation,"The Event Horizon Telescope (EHT) is a very long baseline interferometry (VLBI) array that comprises millimeter- and submillimeter-wavelength telescopes separated by distances comparable to the diameter of the Earth. At a nominal operating wavelength of ~1.3 mm, EHT angular resolution (λ/D) is ~25 μas, which is sufficient to resolve nearby supermassive black hole candidates on spatial and temporal scales that correspond to their event horizons. With this capability, the EHT scientific goals are to probe general relativistic effects in the strong-field regime and to study accretion and relativistic jet formation near the black hole boundary. In this Letter we describe the system design of the EHT, detail the technology and instrumentation that enable observations, and provide measures of its performance. Meeting the EHT science objectives has required several key developments that have facilitated the robust extension of the VLBI technique to EHT observing wavelengths and the production of instrumentation that can be deployed on a heterogeneous array of existing telescopes and facilities. To meet sensitivity requirements, high-bandwidth digital systems were developed that process data at rates of 64 gigabit s−1, exceeding those of currently operating cm-wavelength VLBI arrays by more than an order of magnitude. Associated improvements include the development of phasing systems at array facilities, new receiver installation at several sites, and the deployment of hydrogen maser frequency standards to ensure coherent data capture across the array. These efforts led to the coordination and execution of the first Global EHT observations in 2017 April, and to event-horizon-scale imaging of the supermassive black hole candidate in M87."
WILLIAM SHAW,First M87 Event Horizon Telescope results. I. The shadow of the supermassive black hole,"When surrounded by a transparent emission region, black holes are expected to reveal a dark shadow caused by gravitational light bending and photon capture at the event horizon. To image and study this phenomenon, we have assembled the Event Horizon Telescope, a global very long baseline interferometry array observing at a wavelength of 1.3 mm. This allows us to reconstruct event-horizon-scale images of the supermassive black hole candidate in the center of the giant elliptical galaxy M87. We have resolved the central compact radio source as an asymmetric bright emission ring with a diameter of 42 ± 3 μas, which is circular and encompasses a central depression in brightness with a flux ratio gsim10:1. The emission ring is recovered using different calibration and imaging schemes, with its diameter and width remaining stable over four different observations carried out in different days. Overall, the observed image is consistent with expectations for the shadow of a Kerr black hole as predicted by general relativity. The asymmetry in brightness in the ring can be explained in terms of relativistic beaming of the emission from a plasma rotating close to the speed of light around a black hole. We compare our images to an extensive library of ray-traced general-relativistic magnetohydrodynamic simulations of black holes and derive a central mass of M = (6.5 ± 0.7) × 10^9 M ⊙. Our radio-wave observations thus provide powerful evidence for the presence of supermassive black holes in centers of galaxies and as the central engines of active galactic nuclei. They also present a new tool to explore gravity in its most extreme limit and on a mass scale that was so far not accessible."
CLAUDIA ANDERSON,"D-cycloserine augmentation of exposure-based cognitive behavior therapy for anxiety, obsessive-compulsive, and posttraumatic stress disorders: a systematic review and meta-analysis of individual participant data","Importance: Whether and under which conditions D-cycloserine (DCS) augments the effects of exposure-based cognitive behavior therapy for anxiety, obsessive-compulsive, and posttraumatic stress disorders is unclear. Objective: To clarify whether DCS is superior to placebo in augmenting the effects of cognitive behavior therapy for anxiety, obsessive-compulsive, and posttraumatic stress disorders and to evaluate whether antidepressants interact with DCS and the effect of potential moderating variables. Data Sources: PubMed, EMBASE, and PsycINFO were searched from inception to February 10, 2016. Reference lists of previous reviews and meta-analyses and reports of randomized clinical trials were also checked. Study Selection: Studies were eligible for inclusion if they were (1) double-blind randomized clinical trials of DCS as an augmentation strategy for exposure-based cognitive behavior therapy and (2) conducted in humans diagnosed as having specific phobia, social anxiety disorder, panic disorder with or without agoraphobia, obsessive-compulsive disorder, or posttraumatic stress disorder. Data Extraction and Synthesis: Raw data were obtained from the authors and quality controlled. Data were ranked to ensure a consistent metric across studies (score range, 0-100). We used a 3-level multilevel model nesting repeated measures of outcomes within participants, who were nested within studies. Results: Individual participant data were obtained for 21 of 22 eligible trials, representing 1047 of 1073 eligible participants. When controlling for antidepressant use, participants receiving DCS showed greater improvement from pretreatment to posttreatment (mean difference, -3.62; 95% CI, -0.81 to -6.43; P = .01; d = -0.25) but not from pretreatment to midtreatment (mean difference, -1.66; 95% CI, -4.92 to 1.60; P = .32; d = -0.14) or from pretreatment to follow-up (mean difference, -2.98, 95% CI, -5.99 to 0.03; P = .05; d = -0.19). Additional analyses showed that participants assigned to DCS were associated with lower symptom severity than those assigned to placebo at posttreatment and at follow-up. Antidepressants did not moderate the effects of DCS. None of the prespecified patient-level or study-level moderators was associated with outcomes. Conclusions and Relevance: D-cycloserine is associated with a small augmentation effect on exposure-based therapy. This effect is not moderated by the concurrent use of antidepressants. Further research is needed to identify patient and/or therapy characteristics associated with DCS response."
MATTEO BELLITTI,Aspects of damping in correlated quantum systems,"In this work we discuss three problems connected to the damping of oscillations in quantum systems: dynamics and decay in a random matrix model with conserved quantities, energy redistribution and decay of the amplitude mode in a superconductor, and the decay of the gapped mode in a molecular condensate, a bosonic analog to the superconductor. These problems highlight different aspects of damping and energy redistribution in quantum dynamics, while being simple enough that analytic control is possible. The first system captures the idea of a ""partially conserved"" quantity: in ergodic quantum systems, physical observables have a non-relaxing component if they overlap with a conserved quantity, but how to isolate the non-relaxing component is in general unclear. We compute exact dynamical correlators governed by a Hamiltonian composed of two large interacting random matrices, H=A+B, and we analytically obtain the late-time value of ⟨A(t) A(0)⟩, which quantifies the non-relaxing part of the observable A. We show that the relaxation to this value is governed by a power-law determined by the spectrum of the Hamiltonian H, independent of the observable A, while the long--time value and the amplitude of the oscillations depend on the trace--overlap between the operator and the Hamiltonian. For Gaussian matrices, we further compute out-of-time-ordered-correlators (OTOCs) and find that the existence of a non-relaxing part of A leads to modifications of the late time values and exponents. Our results follow from exact resummation of a diagrammatic expansion and hyperoperator techniques. The above problem deals with energy redistribution in a system with a complex internal structure, but without any spatial dependence nor many--body effects. In the second part of this work we discuss energy relaxation in a system with both: a BCS superconductor. In particular, we study the excitation of the collective Higgs oscillations of the order parameter by incoherent short pulses of light with frequency much larger than the superconducting gap. We find that the excitation amplitude of the Higgs mode is controlled by a single parameter, determined by the total number of quasiparticles excited by the pulse, which we trace back to the universality of the shape of the light-induced quasiparticle cascade at energy below the Debye frequency and above the gap. Our analysis is primarily based on the Keldysh technique for non--equilibrium field theory and the Boltzmann kinetic equation. Finally, we study the damping of the gapped mode in a molecular Bose--Einstein condensate, the Bosonic analogue of a BCS superconductor. This system has the advantage of giving the experimentalist fine control over the interatomic interactions using Feshbach resonances, and is the object of renewed interest as the molecular superfluid phase has only very recently been realized in the lab. We discuss damping in the nontrivial thermodynamic phases: in the molecular superfluid phase the gapped excitation is protected by parity, and is damped only above a threshold momentum --as in the Cherenkov effect--, while in the atomic superfluid phase the gapped mode is damped at all momentum scales. We propose a class of experiments where our results are measurable: transmission (and reflection) of an atom beam through a molecular condensate cloud."
JESSE SMITH,A systematic review on the biochemical threshold of mitochondrial genetic variants,"Mitochondrial DNA (mtDNA) variants cause a range of diseases from severe pediatric syndromes to aging-related conditions. The percentage of mtDNA copies carrying a pathogenic variant, variant allele frequency (VAF), must reach a threshold before a biochemical defect occurs, termed the biochemical threshold. Whether the often-cited biochemical threshold of >60% VAF is similar across mtDNA variants and cell types is unclear. In our systematic review, we sought to identify the biochemical threshold of mtDNA variants in relation to VAF by human tissue/cell type. We used controlled vocabulary terms to identify articles measuring oxidative phosphorylation (OXPHOS) complex activities in relation to VAF. We identified 76 eligible publications, describing 69, 12, 16, and 49 cases for complexes I, III, IV, and V, respectively. Few studies evaluated OXPHOS activities in diverse tissue types, likely reflective of clinical access. A number of cases with similar VAFs for the same pathogenic variant had varying degrees of residual activity of the affected complex, alluding to the presence of modifying variants. Tissues and cells with VAFs <60% associated with low complex activities were described, suggesting the possibility of a biochemical threshold of <60%. Using Kendall rank correlation tests, the VAF of the m.8993T > G variant correlated with complex V activity in skeletal muscle (τ = -0.58, P = 0.01, n = 13); however, no correlation was observed in fibroblasts (P = 0.7, n = 9). Our systematic review highlights the need to investigate the biochemical threshold over a wider range of VAFs in disease-relevant cell types to better define the biochemical threshold for specific mtDNA variants."
JOSHUA JONES,Toward bacterial bioelectric signal transduction,"Bacteria are electrically powered organisms; cells maintain an electrical potential across their plasma membrane as a source of free energy to drive essential processes. In recent years, however, bacterial membrane potential has been increasingly recognized as dynamic. Those dynamics have been implicated in diverse physiological functions and behaviors, including cell division and cell-to-cell signaling. In eukaryotic cells, such dynamics play major roles in coupling bioelectrical stimuli to changes in internal cell states. Neuroscientists and physiologists have established detailed molecular pathways that transduce eukaryotic membrane potential dynamics to physiological and gene expression responses. We are only just beginning to explore these intracellular responses to bioelectrical activity in bacteria. In this review, we summarize progress in this area, including evidence of gene expression responses to stimuli from electrodes and mechanically induced membrane potential spikes. We argue that the combination of provocative results, missing molecular detail, and emerging tools makes the investigation of bioelectrically induced long-term intracellular responses an important and rewarding effort in the future of microbiology."
JOSHUA JONES,Extending the near infrared emission range of indium phosphide quantum dots for multiplexed 'In Vivo' imaging,"This report of the reddest emitting indium phosphide quantum dots (InP QDs) to date demonstrates tunable, near infrared (NIR) photoluminescence and fluorescence multiplexing in the first optical tissue window with a material that avoids toxic constituents. This synthesis overcomes the InP synthesis “growth bottleneck” and extends the emission peak of InP QDs deeper into the first optical tissue window using an inverted QD heterostructure. The ZnSe/InP/ZnS core/shell/shell structure is designed to produce emission from excitons with heavy holes confined in InP shells wrapped around larger-bandgap ZnSe cores and protected by a second shell of ZnS. The InP QDs exhibit InP shell thickness-dependent tunable emission with peaks ranging from 515 – 845 nm. The high absorptivity of InP leads to effective absorbance and photoexcitation of the QDs with UV, visible, and NIR wavelengths in particles with diameters of eight nanometers or less. These nanoparticles extend the range of tunable direct-bandgap emission from InP-based nanostructures, effectively overcoming a synthetic barrier that has prevented InP-based QDs from reaching their full potential as NIR imaging agents. Multiplexed lymph node imaging in a mouse model shows the potential of the NIR-emitting InP particles for in vivo imaging."
JOSHUA JONES,The long-baseline neutrino experiment: exploring fundamental symmetries of the universe,"The preponderance of matter over antimatter in the early Universe, the dynamics of the supernova bursts that produced the heavy elements necessary for life and whether protons eventually decay --- these mysteries at the forefront of particle physics and astrophysics are key to understanding the early evolution of our Universe, its current state and its eventual fate. The Long-Baseline Neutrino Experiment (LBNE) represents an extensively developed plan for a world-class experiment dedicated to addressing these questions. LBNE is conceived around three central components: (1) a new, high-intensity neutrino source generated from a megawatt-class proton accelerator at Fermi National Accelerator Laboratory, (2) a near neutrino detector just downstream of the source, and (3) a massive liquid argon time-projection chamber deployed as a far detector deep underground at the Sanford Underground Research Facility. This facility, located at the site of the former Homestake Mine in Lead, South Dakota, is approximately 1,300 km from the neutrino source at Fermilab -- a distance (baseline) that delivers optimal sensitivity to neutrino charge-parity symmetry violation and mass ordering effects. This ambitious yet cost-effective design incorporates scalability and flexibility and can accommodate a variety of upgrades and contributions. With its exceptional combination of experimental configuration, technical capabilities, and potential for transformative discoveries, LBNE promises to be a vital facility for the field of particle physics worldwide, providing physicists from around the globe with opportunities to collaborate in a twenty to thirty year program of exciting science. In this document we provide a comprehensive overview of LBNE's scientific objectives, its place in the landscape of neutrino physics worldwide, the technologies it will incorporate and the capabilities it will possess."
ALLISON WILLIAMS,Identification and reconstruction of low-energy electrons in the ProtoDUNE-SP detector,
ALLISON WILLIAMS,Scintillation light detection in the 6-m drift-length ProtoDUNE Dual Phase liquid argon TPC,"DUNE is a dual-site experiment for long-baseline neutrino oscillation studies, neutrino astrophysics and nucleon decay searches. ProtoDUNE Dual Phase (DP) is a 6  ×  6  ×  6 m 3 liquid argon time-projection-chamber (LArTPC) that recorded cosmic-muon data at the CERN Neutrino Platform in 2019-2020 as a prototype of the DUNE Far Detector. Charged particles propagating through the LArTPC produce ionization and scintillation light. The scintillation light signal in these detectors can provide the trigger for non-beam events. In addition, it adds precise timing capabilities and improves the calorimetry measurements. In ProtoDUNE-DP, scintillation and electroluminescence light produced by cosmic muons in the LArTPC is collected by photomultiplier tubes placed up to 7 m away from the ionizing track. In this paper, the ProtoDUNE-DP photon detection system performance is evaluated with a particular focus on the different wavelength shifters, such as PEN and TPB, and the use of Xe-doped LAr, considering its future use in giant LArTPCs. The scintillation light production and propagation processes are analyzed and a comparison of simulation to data is performed, improving understanding of the liquid argon properties."
SELMA LINNEA HEDLUND,Costs and rewards of physician migration: comparing US and Swedish models,"The fact that many OECD countries are reliant on international medical graduates (IMGs) to serve their most vulnerable has become even more apparent in the wake of Covid-19. This dissertation examines the role that nation brands play on the international physician labor market and how visa regimes and migration industries shape IMG pathways to Sweden versus US; two widely different societies where around a third of all doctors are IMGs. The US and Sweden represent two different approaches to addressing the same problem — solving a shortfall of healthcare providers, especially in rural areas populated by ethnic minorities and low-income families. While many Swedish regions actively attempts to facilitate the incorporation of IMGs through an intra-European physician recruitment industry, the US seem to rely on the attraction of its political economy and has done little to modify the substantial financial and visa-related obstacles that IMGs face. As a high-skilled immigrant group, immigrant physicians occupy a complex position of advantage and disadvantage; they are privileged in comparison to low-income migrant workers and unauthorized immigrants, yet face more barriers in comparison to domestic physicians, and are often informally sorted into less prestigious positions. This study centers the two largest immigrant physician groups in each country: Indians in the US and Poles in Sweden. The experiences of these labor migrants are triangulated against a third IMG group that have undergone the asylum process in order to reach their host societies — Iraqis."
ROBIN FRANCIS,"Patterns, causes, and consequences of marine larval dispersal","Quantifying the probability of larval exchange among marine populations is key to predicting local population dynamics and optimizing networks of marine protected areas. The pattern of connectivity among populations can be described by the measurement of a dispersal kernel. However, a statistically robust, empirical dispersal kernel has been lacking for any marine species. Here, we use genetic parentage analysis to quantify a dispersal kernel for the reef fish Elacatinus lori, demonstrating that dispersal declines exponentially with distance. The spatial scale of dispersal is an order of magnitude less than previous estimates—the median dispersal distance is just 1.7 km and no dispersal events exceed 16.4 km despite intensive sampling out to 30 km from source. Overlaid on this strong pattern is subtle spatial variation, but neither pelagic larval duration nor direction is associated with the probability of successful dispersal. Given the strong relationship between distance and dispersal, we show that distance-driven logistic models have strong power to predict dispersal probabilities. Moreover, connectivity matrices generated from these models are congruent with empirical estimates of spatial genetic structure, suggesting that the pattern of dispersal we uncovered reflects long-term patterns of gene flow. These results challenge assumptions regarding the spatial scale and presumed predictors of marine population connectivity. We conclude that if marine reserve networks aim to connect whole communities of fishes and conserve biodiversity broadly, then reserves that are close in space (<10 km) will accommodate those members of the community that are short-distance dispersers."
ROBIN FRANCIS,The effects of variation in habitat quality on reproduction and dispersal of two model systems for coral reef fish metapopulation ecology,"1. A primary objective of marine ecology is to enhance our understanding of the drivers of marine fish population dynamics. These dynamics are readily understood using the framework of metapopulation ecology: for a population to persist, it must be able to self-replicate or be connected to other populations. The drivers of these criteria include rates of reproduction and dispersal, which together determine the rates of recruitment. Recruitment is, however, highly variable, and we do not know widespread patterns and causes of this variation in marine fishes. 2. Reproductive success is notoriously difficult to measure in the wild, and even more so for coral reef fishes that reside underwater. As a proxy, mating success can be predicted from more easily measured determinants, such as variation in individual traits and habitat characteristics. 3. Dispersal has, until recently, been the ‘black box’ of marine ecology, but it can be helpfully described using dispersal kernels. Currently, only five marine fishes have had their dispersal kernel empirically estimated using parentage analyses. From these studies, we see that there are at least four orders of magnitude variation in individual dispersal distances within a species. 4. Many coral reef fishes live in close association with macroinvertebrate hosts, which provide nesting sites and protection from predators. The fish themselves exhibit parental care over clutches of eggs, which hatch into dispersive larvae and recruit back into a suitable host. This life-history suggests habitat quality variation at the scale of the individual host may play a large role in determining relative reproductive and dispersal rates. To explore the role of variation in habitat quality on marine fish reproductive ecology and dispersal ecology, I perform the following three studies: 5. First, I investigate the determinants of variation in mating success in the sponge-dwelling goby, Elacatinus lori. While this fish was the first to have its larval dispersal kernel empirically measured, there were outstanding questions about its reproductive ecology. I describe the characteristics of its breeding habitat, its genetic mating system, and the correlates of male mating success. I show that sponges occupied by breeding males tend to be larger than other sponges, males exhibit a polygynous mating system, and male size, but not sponge size, is positively related to multiple metrics of male mating success. 6. Second, I test for the possibility of plasticity in larval dispersal phenotypes in response to parental habitat quality variation in the orange anemonefish, Amphiprion percula. To pursue this hypothesis, we must first establish that the conditions for parental effects to evolve are met in this system. An initial condition is whether parents can predict the habitat quality that their offspring will encounter. I test for spatial predictability of habitat quality by describing the strength and scale of spatial autocorrelation of three habitat quality indicators: anemone size, female size, and clutch size. I show that all three habitat quality indicators are positively correlated at the scale of an individual patch reef, suggesting that selection might favor parents that increase allocation to offspring that stay within the natal reef if they are in good habitat and increase allocation to offspring that travel farther and leave if they are in poor habitat. Results from this study motivate further investigation of dispersal plasticity in A. percula. 7. Third, I experimentally test the hypothesis that parents can adjust the dispersal-related traits of their offspring in response to changes in habitat quality using a lab population of Amphiprion percula. I describe changes in body size and swimming ability of offspring in response to parental habitat quality by manipulating the parents’ food rations. I find plasticity of larval body size, with parents producing larger offspring when on a low food ration than when on a high food ration. Results from this study motivate future studies to investigate plasticity of larval dispersal distances in the field. 8. In sum, I investigate how variation in habitat quality influences reproductive success, how variation in habitat quality is spatially predictable, and how variation in habitat quality influences dispersal-related traits in two important model systems in marine metapopulation ecology. The work advances our understanding of the role of habitat in determining these vital rates and population dynamics with implications for conservation and fisheries management."
SAUL FRANKFORD,Reliability of fMRI data during speech production tasks across scanning sessions,
SAUL FRANKFORD,Reliability of single-subject neural activation patterns in speech production tasks,"Speech neuroimaging research targeting individual speakers could help elucidate differences that may be crucial to understanding speech disorders. However, this research necessitates reliable brain activation across multiple speech production sessions. In the present study, we evaluated the reliability of speech-related brain activity measured by functional magnetic resonance imaging data from twenty neuro-typical subjects who participated in two experiments involving reading aloud simple speech stimuli. Using traditional methods like the Dice and intraclass correlation coefficients, we found that most individuals displayed moderate to high reliability. We also found that a novel machine-learning subject classifier could identify these individuals by their speech activation patterns with 97% accuracy from among a dataset of seventy-five subjects. These results suggest that single-subject speech research would yield valid results and that investigations into the reliability of speech activation in people with speech disorders are warranted."
SAUL FRANKFORD,Internal and external speech timing mechanisms in persistent developmental stuttering,"Stuttering is a developmental speech disorder characterized by interruptions of fluency. A large body of research suggests that stuttering occurs due to a reduced ability to generate timing signals in order to sequence speech sounds. One piece of supporting evidence for this is that when speaking along with an external timing source like a metronome, disfluencies suddenly and significantly decrease. The aim of this dissertation was to characterize the effects of using auditory cues to time speech on neural activation and auditory feedback processing, and how these effects may contribute to fluency in adults who stutter (AWS). Two studies were carried out to examine these effects. In the first study, functional magnetic resonance imaging was used to measure brain activity while AWS and adults who do not stutter (ANS) read sentences aloud either using natural speech timing or aligning each syllable to the beat of a metronome. Consistent with previous literature, AWS produced fewer disfluent trials in the externally paced condition than in the normal condition. Collapsing across the AWS and ANS groups, participants had greater activation in the metronome-timed condition in regions associated with speech sequencing, sensory feedback control, and timing perception. AWS also demonstrated increased functional connectivity among cerebellar regions during externally paced speech. In the second study, responses to online spectral and timing perturbations of auditory feedback were measured while AWS and ANS read sentences with and without metronome pacing. Results indicated that AWS showed no responses to spectral perturbations during the non-paced condition and significant compensatory responses during the paced condition along with fewer disfluencies, while responses in ANS showed the opposite effect. For the timing perturbation, no significant differences were found between groups in either condition. Together, these studies indicate that the deficit in stuttering is related to spectral processing rather than purely temporal processing, and that externally paced speech recruits compensatory neural regions that may help resolve this deficit."
SAUL FRANKFORD,Impaired responses to time-shifting perturbations in adults who stutter during rhythmic and non-rhythmic speech,
SAUL FRANKFORD,Reliability of single-subject neural activation patterns in speech production tasks,"Traditional group fMRI (functional magnetic resonance imaging) analyses are not designed to detect individual differences that may be crucial to better understanding speech disorders. Single-subject research could therefore provide a richer characterization of the neural substrates of speech production in development and disease. Before this line of research can be tackled, however, it is necessary to evaluate whether healthy individuals exhibit reproducible brain activation across multiple sessions during speech production tasks. In the present study, we evaluated the reliability and discriminability of cortical functional magnetic resonance imaging data from twenty neurotypical subjects who participated in two experiments involving reading aloud mono- or bisyllabic speech stimuli. Using traditional methods like the Dice and intraclass correlation coefficients, we found that most individuals displayed moderate to high reliability, with exceptions likely due to increased head motion in the scanner. Further, this level of reliability for speech production was not directly correlated with reliable patterns in the underlying average blood oxygenation level dependent signal across the brain. Finally, we found that a novel machine-learning subject classifier could identify these individuals by their speech activation patterns with 97% accuracy from among a dataset of seventy-five subjects. These results suggest that single-subject speech research would yield valid results and that investigations into the reliability of speech activation in people with speech disorders are warranted."
HANNAH ELISE AICHELMAN,Exploring coral symbiosis under climate change stress across spatial and temporal scales,"Human activity since the Industrial Revolution has increased global greenhouse gas concentrations resulting in rapid climate change, which now threatens terrestrial and marine ecosystems. Tropical coral reefs, along with the biodiversity and communities they support, are particularly threatened by these changes in climate. Corals are a consortium of organisms, with the coral host along with its photosynthetic endosymbiont (Family Symbiodiniaceae) and diverse community of microorganisms (bacteria, archaea, fungi, and viruses) together forming the ‘coral holobiont’. However, the symbiosis between tropical corals and Symbiodiniaceae algae is sensitive to even small changes in temperature and ‘coral bleaching’ events – the loss of symbiosis – are now occurring with increased frequency and severity. These bleaching events can result in coral mortality and loss of entire reefs if stressful conditions do not subside. While research efforts have increased our ability to understand and predict coral bleaching events, fundamental questions remain surrounding how genetic diversity of the coral holobiont and interactions with its environment can drive coral resilience or resistance under climate change. The overarching goal of my dissertation is to understand how various abiotic (i.e., stress duration, spatiotemporal variation on the reef) and biotic (i.e., holobiont diversity, symbiosis) factors determine a coral’s response to environmental change at the level of phenotype and genotype. To achieve this goal, I first tested how environmental history and stress duration modulated the physiological responses of two reef-building corals under combined ocean warming and ocean acidification conditions. I found that one species was more stress-resistant (Siderastrea siderea), but that both duration of stress exposure and environmental history (inshore vs. offshore reef origin) modulated coral physiology. Next, I investigated the importance of holobiont genetic identity and abiotic environment in driving phenotypic responses of S. siderea exposed to a diel temperature variability (DTV) and subsequent heat challenge experiment. I found that while DTV increased coral growth, cryptic host diversity and their unique pairings with algal symbiont strains were the strongest predictors of holobiont physiology and response to heat challenge. Lastly, I leveraged genome-wide gene expression profiling and the facultative symbiosis between the subtropical coral Oculina arbuscula and its symbiont Breviolum psygmophilum to disentangle the independent responses of both partners to heat and cold challenges in and out of symbiosis. I found that O. arbuscula host gene expression was more plastic under temperature challenges relative to B. psygmophilum when in symbiosis, and that symbionts exhibited more gene expression plasticity in culture compared to in symbiosis. Taken together, this dissertation provides valuable insights into the phenotypic and genotypic mechanisms that contribute to coral success in a changing climate."
HANNAH ELISE AICHELMAN,Exposure duration modulates the response of Caribbean corals to global change stressors,"Global change, including rising temperatures and acidification, threatens corals globally. Although bleaching events reveal fine-scale patterns of resilience, traits enabling persistence under global change remain elusive. We conducted a 95-d controlled-laboratory experiment investigating how duration of exposure to warming (~28, 31°C), acidification (pCO2 ~ 343 [present day], ~663 [end of century], ~3109 [extreme] μatm), and their combination influences physiology of reef-building corals (Siderastrea siderea, Pseudodiploria strigosa) from two reef zones on the Belize Mesoamerican Barrier Reef System. Every 30 d, net calcification rate, host protein and carbohydrate, chlorophyll a, and symbiont density were quantified for the same coral individual to characterize acclimation potential under global change. Coral physiologies of the two species were differentially affected by stressors and exposure duration was found to modulate these responses. Siderastrea siderea exhibited resistance to end of century pCO2 and temperature stress, but calcification was negatively affected by extreme pCO2. However, S. siderea calcification rates remained positive after 95 d of extreme pCO2 conditions, suggesting acclimation. In contrast, P. strigosa was more negatively influenced by elevated temperatures, which reduced most physiological parameters. An exception was nearshore P. strigosa, which maintained calcification rates under elevated temperature, suggesting local adaptation to the warmer environment of their natal reef zone. This work highlights how tracking coral physiology across various exposure durations can capture acclimatory responses to global change stressors."
HANNAH ELISE AICHELMAN,"Distinct phenotypes associated with mangrove and lagoon habitats in two widespread caribbean corals, porites astreoides and porites divaricata","AbstractAs coral reefs experience dramatic declines in coral cover throughout the tropics, there is an urgent need to understand the role that non-reef habitats, such as mangroves, play in the ecological niche of corals. Mangrove habitats present a challenge to reef-dwelling corals because they can differ dramatically from adjacent reef habitats with respect to key environmental parameters, such as light. Because variation in light within reef habitats is known to drive intraspecific differences in coral phenotype, we hypothesized that coral species that can exploit both reef and mangrove habitats will exhibit predictable differences in phenotypes between habitats. To investigate how intraspecific variation, driven by either local adaptation or phenotypic plasticity, might enable particular coral species to exploit these two qualitatively different habitat types, we compared the phenotypes of two widespread Caribbean corals, Porites divaricata and Porites astreoides, in mangrove versus lagoon habitats on Turneffe Atoll, Belize. We document significant differences in colony size, color, structural complexity, and corallite morphology between habitats. In every instance, the phenotypic differences between mangrove prop root and lagoon corals exhibited consistent trends in both P. divaricata and P. astreoides. We believe this study is the first to document intraspecific phenotypic diversity in corals occupying mangrove prop root versus lagoonal patch reef habitats. A difference in the capacity to adopt an alternative phenotype that is well suited to the mangrove habitat may explain why some reef coral species can exploit mangroves, while others cannot."
HANNAH ELISE AICHELMAN,Coral symbiodinium community composition across the Belize Mesoamerican barrier reef system is influenced by host species and thermal variability,
RUIZHI WANG,Mechanical and failure mechanisms of descending thoracic aorta: implications for health and disease,"Structural organization of the extracellular matrix components of the aorta is critical to its loading-bearing capacity and homeostasis. Aortic elastic fibers form concentric lamellar layers with a closely interwoven three-dimensional network of collagen and elastic fibers in the narrow interlamellar space. Aging and cardiovascular diseases are closely associated with disrupted microstructural organization, integrity, as well as altered mechanical and failure properties of the aortic wall. The overall goal of this research is to advance the current understanding of the mechanical and failure mechanisms of human descending thoracic aorta and provide insights for aortic remodeling during aging and disease progression using integrated biomechanical testing, imaging, and computational modeling approaches. Biaxial tensile tests revealed anisotropic stiffening of the aortic wall with aging with a more drastic stiffening behavior in the longitudinal direction. A newly developed constitutive model considering collagen crosslinking suggested that collagen crosslinking has an increasing contribution to the stress-stretch behavior and elastic energy storage in aortic senescence. The aorta relies on interlamellar structural components, mainly elastic and collagen fibers, for maintaining its structural and mechanical integrity. Our study using peeling and direct tension tests demonstrated that elastic and collagen fibers both play an important role in bonding of the arterial wall, while collagen fibers dominate the interlamellar stiffness, strength and toughness. Our study further reveals that the interlamellar strength and toughness both increase due to nonenzymatic glycation, which is in accordance with the reported inverse relation between diabetes and a reduced risk of aortic dissection. On the other hand, however, our study showed decreasing interlamellar bonding toughness of the medial layer of human descending thoracic aorta with aging. Avalanches and power-law behavior in dissection propagation was found for all age groups investigated. Finite element simulations incorporating discrete interlamellar collagen fibers successfully recapitulates the power-law behavior and points to prominent structural alterations in interlamellar collagen fibers with aging including reduced fiber density and higher degree of dispersion. In aging and diseases, changes to the extracellular matrix microstructure can trigger a cascade of effects on tissue and cellular function. The knowledge gained from this research provide insights into the microstructural mechanisms in determining the physiological and failure properties of aorta and will potentially generate clinical impact on the developments of new diagnostics and interventions."
YIFAN WANG,Multiwindow SRS imaging using a rapid widely tunable fiber laser,"Spectroscopic stimulated Raman scattering (SRS) imaging has become a useful tool finding a broad range of applications. Yet, wider adoption is hindered by the bulky and environmentally sensitive solid-state optical parametric oscillator (OPO) in a current SRS microscope. Moreover, chemically informative multiwindow SRS imaging across C-H, C-D, and fingerprint Raman regions is challenging due to the slow wavelength tuning speed of the solid-state OPO. In this work, we present a multiwindow SRS imaging system based on a compact and robust fiber laser with rapid and wide tuning capability. To address the relative intensity noise intrinsic to a fiber laser, we implemented autobalanced detection, which enhances the signal-to-noise ratio of stimulated Raman loss imaging by 23 times. We demonstrate high-quality SRS metabolic imaging of fungi, cancer cells, and Caenorhabditis elegans across the C-H, C-D, and fingerprint Raman windows. Our results showcase the potential of the compact multiwindow SRS system for a broad range of applications."
JULIANN HELEN NICHOLSON,Examining child care and child care subsidies for intimate partner violence survivors and their children: a mixed methods study,"There are well-established links between early exposure to intimate partner violence (IPV) and negative developmental outcomes for young children. Emerging evidence suggests that early care and education (ECE), an existing and widely used resource within children’s communities, may be a promising means to support and address the needs of young children experiencing adversity. However, little is known about ECE or ECE policies in the IPV context. This three-paper dissertation employs a triangulation mixed methods design to address these gaps in our current knowledge. Chapter 2 investigates the influences of ECE on the behavioral outcomes of children exposed to IPV. Four waves of national, longitudinal data from the Fragile Families and Child Well Being Survey (N=3,108) were used to examine the moderating roles of informal, formal home-based, and center-based child care on respective associations between children’s IPV exposure and internalizing and externalizing behavioral problems (IBP, EBP). Results from ordinary least squares regression models with interaction terms and subgroup analyses using inverse probability of treatment weighting (IPTW) suggest that non-parental child care, particularly center-based care, can attenuate the negative influences of some forms of IPV exposure on young children’s behavioral outcomes. Chapter 3 draws on primary data from in-depth, semi-structured interviews with 17 IPV survivor mothers of young children aged 0–5 to understand their ECE and child care subsidy decisions and arrangements. A data-driven thematic analysis revealed pervasive influences of IPV on mothers’ ECE access and utilization. Despite IPV perpetrators’ interference with and restraint of ECE, mothers sought child care arrangements they believed would enhance children’s well-being, particularly with respect to children’s IPV-related needs and safety. Social and structural factors (e.g., social isolation and ECE affordability and availability) also importantly influenced mothers’ ECE arrangements, and child care subsidies were a critical resource for some. Chapter 4 explores the mechanisms by which ECE can promote children’s resilience during and following IPV exposure, using data from the same 17 interviews with IPV survivor mothers as well as interviews with 6 ECE professionals with experience working with children exposed to IPV. Results from data-driven thematic analyses indicate that reduced exposure to IPV and associated risks, prevention of abusers’ unsafe contact with children, nurturance, enriching activities, stability, a balance of consistency and flexibility, access to therapeutic services, and support of children’s emotion regulation and social development may serve as key protective mechanisms for children exposed to IPV within ECE environments."
