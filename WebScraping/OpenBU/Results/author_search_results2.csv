Author,Title,Abstract
RODRIGO CANALES,When salespeople manage customer relationships: multidimensional incentives and private information,"At many firms, incentivized salespeople with private information about customers are responsible for customer relationship management. Although incentives motivate sales performance, private information can induce moral hazard by salespeople to gain compensation at the expense of the firm. The authors investigate the sales performance–moral hazard trade-off in response to multidimensional performance (acquisition and maintenance) incentives in the presence of private information. Using unique panel data on customer loan acquisition and repayments linked to salespeople from a microfinance bank, the authors detect evidence of salesperson private information. Acquisition incentives induce salesperson moral hazard, leading to adverse customer selection, but maintenance incentives moderate it as salespeople recognize the negative effects of acquiring low-quality customers on future payoffs. Critically, without the moderating effect of maintenance incentives, the adverse selection effect of acquisition incentives overwhelms the sales-enhancing effects, clarifying the importance of multidimensional incentives for customer relationship management. Reducing private information (through job transfers) hurts customer maintenance but has greater impact on productivity by moderating adverse selection at acquisition. This article also contributes to the recent literature on detecting and disentangling customer adverse selection and customer moral hazard (defaults) with a new identification strategy that exploits the time-varying effects of salesperson incentives."
RODRIGO CANALES,The stranger as friend: loan officers and positive deviance in microfinance,"This chapter explores positive deviance in the context of microfinance. Some loan officers frequently bend or choose not to enforce written rules in an effort to better address client needs, while others enforce the rules strictly. These differences in enforcement styles are analyzed to explore the structural characteristics that generate and sustain rule-bending behavior. In microfinance, the pressures to standardize and automate lending decisions challenge loan officers’ ability to manage clients because context uncertainty cannot be fully captured by centralized policies. The chapter explores the structural conditions that lead to positive deviance with productive outcomes by the organization’s own criteria. The paper unveils two inherent tensions in microfinance. First, increased efforts to centralize and enforce policies in fact only increase the pressures for loan officers to work outside the organizations’ regulations. Second, this type of positive deviance keeps the organizations connected to their core, social missions."
RODRIGO CANALES,Do startup employees earn more in the long run?,"Evaluating the attractiveness of startup employment requires an understanding of both what startups pay and the implications of these jobs for earnings trajectories. Analyzing Danish registry data, we find that employees hired by startups earn roughly 17% less over the next 10 years than those hired by large, established firms. About half of this earnings differential stems from sorting—from the fact that startup employees have less human capital. Long-term earnings also vary depending on when individuals are hired. Although the earliest employees of startups suffer an earnings penalty, those hired by already-successful startups earn a small premium. Two factors appear to account for the earnings penalties for the early employees: Startups fail at high rates, creating costly spells of unemployment for their (former) employees. Job-mobility patterns also diverge: After being employed by a small startup, individuals rarely return to the large employers that pay more."
RODRIGO CANALES,A darker side to decentralized banks: market power and credit rationing in SME lending,"We use loan-level data to study how the organizational structure of banks impacts small business lending. We find that decentralized banks — where branch managers have greater autonomy over lending decisions — give larger loans to small firms and those with ""soft information"". However, decentralized banks are also more responsive to their own competitive environment. They are more likely to expand credit when faced with competition but also cherry pick customers and restrict credit when they have market power. This ""darker side"" to decentralized banks in concentrated markets highlights that the level of local banking competition is key to determining which organizational structure provides better lending terms for small businesses."
RODRIGO CANALES,Evidence in practice: how structural and programmatic scaffolds enable knowledge transfer in international development,"This inductive study of eight international development interventions across four countries analyzes the mechanisms that enable effective integration of evidence in practice, as an enduring challenge of learning and coordination across occupational and organizational boundaries. We identify how a set of critical, complementary structural and programmatic scaffolding practices, jointly provided a shared architecture for actors across organizations and communities of practice to collaborate and learn within the uncertainty and complexity inherent in the international development context. Scaffolding practices offered modular, temporary counter-balancing stabilizing and extending support that enabled the unusual and counter normative behaviors and mindsets required for continuous learning and adaptive coordination. Through 226 in-depth interviews with a range of international development experts, including practitioners engaged in eight matched interventions in India, Mexico, South Africa, and Ghana,we identified and analyzed the mechanisms that explain the varying degrees of effectiveness with which rigorous evidence was integrated in each case. Our findings have implications for interorganizational innovation and collaboration under conditions of complexity and uncertainty, as well as the dynamic interactions between individuals, their organizations, and their communities of practice when attempting to bring about systemic change."
ASSEN G MARINTCHEV,Position of eukaryotic translation initiation factor eIF1A on the 40S ribosomal subunit mapped by directed hydroxyl radical probing,"The universally conserved eukaryotic initiation factor (eIF), eIF1A, plays multiple roles throughout initiation: it stimulates eIF2/GTP/Met-tRNAiMet attachment to 40S ribosomal subunits, scanning, start codon selection and subunit joining. Its bacterial ortholog IF1 consists of an oligonucleotide/oligosaccharide-binding (OB) domain, whereas eIF1A additionally contains a helical subdomain, N-terminal tail (NTT) and C-terminal tail (CTT). The NTT and CTT both enhance ribosomal recruitment of eIF2/GTP/Met-tRNAiMet, but have opposite effects on the stringency of start codon selection: the CTT increases, whereas the NTT decreases it. Here, we determined the position of eIF1A on the 40S subunit by directed hydroxyl radical cleavage. eIF1A's OB domain binds in the A site, similar to IF1, whereas the helical subdomain contacts the head, forming a bridge over the mRNA channel. The NTT and CTT both thread under Met-tRNAiMet reaching into the P-site. The NTT threads closer to the mRNA channel. In the proposed model, the NTT does not clash with either mRNA or Met-tRNAiMet, consistent with its suggested role in promoting the 'closed' conformation of ribosomal complexes upon start codon recognition. In contrast, eIF1A-CTT appears to interfere with the P-site tRNA-head interaction in the 'closed' complex and is likely ejected from the P-site upon start codon recognition."
ANGELA H JACKSON,Internal Medicine Residency Training for Unhealthy Alcohol and Other Drug Use: Recommendations for Curriculum Design,"BACKGROUND: Unhealthy substance use is the spectrum from use that risks harm, to use associated with problems, to the diagnosable conditions of substance abuse and dependence, often referred to as substance abuse disorders. Despite the prevalence and impact of unhealthy substance use, medical education in this area remains lacking, not providing physicians with the necessary expertise to effectively address one of the most common and costly health conditions. Medical educators have begun to address the need for physician training in unhealthy substance use, and formal curricula have been developed and evaluated, though broad integration into busy residency curricula remains a challenge. DISCUSSION: We review the development of unhealthy substance use related competencies, and describe a curriculum in unhealthy substance use that integrates these competencies into internal medicine resident physician training. We outline strategies to facilitate adoption of such curricula by the residency programs. This paper provides an outline for the actual implementation of the curriculum within the structure of a training program, with examples using common teaching venues. We describe and link the content to the core competencies mandated by the Accreditation Council for Graduate Medical Education, the formal accrediting body for residency training programs in the United States. Specific topics are recommended, with suggestions on how to integrate such teaching into existing internal medicine residency training program curricula. SUMMARY: Given the burden of disease and effective interventions available that can be delivered by internal medicine physicians, teaching about unhealthy substance use must be incorporated into internal medicine residency training, and can be done within existing teaching venues."
ANGELA H JACKSON,"Cosmology intertwined: a review of the particle physics, astrophysics, and cosmology associated with the cosmological tensions and anomalies",
ANGELA H JACKSON,The long-baseline neutrino experiment: exploring fundamental symmetries of the universe,"The preponderance of matter over antimatter in the early Universe, the dynamics of the supernova bursts that produced the heavy elements necessary for life and whether protons eventually decay --- these mysteries at the forefront of particle physics and astrophysics are key to understanding the early evolution of our Universe, its current state and its eventual fate. The Long-Baseline Neutrino Experiment (LBNE) represents an extensively developed plan for a world-class experiment dedicated to addressing these questions. LBNE is conceived around three central components: (1) a new, high-intensity neutrino source generated from a megawatt-class proton accelerator at Fermi National Accelerator Laboratory, (2) a near neutrino detector just downstream of the source, and (3) a massive liquid argon time-projection chamber deployed as a far detector deep underground at the Sanford Underground Research Facility. This facility, located at the site of the former Homestake Mine in Lead, South Dakota, is approximately 1,300 km from the neutrino source at Fermilab -- a distance (baseline) that delivers optimal sensitivity to neutrino charge-parity symmetry violation and mass ordering effects. This ambitious yet cost-effective design incorporates scalability and flexibility and can accommodate a variety of upgrades and contributions. With its exceptional combination of experimental configuration, technical capabilities, and potential for transformative discoveries, LBNE promises to be a vital facility for the field of particle physics worldwide, providing physicists from around the globe with opportunities to collaborate in a twenty to thirty year program of exciting science. In this document we provide a comprehensive overview of LBNE's scientific objectives, its place in the landscape of neutrino physics worldwide, the technologies it will incorporate and the capabilities it will possess."
EMILY WHITING,Data for autonomous discovery of tough structures,"This dataset contains the experimental data for the research paper ""Autonomous Discovery of Tough Structure"" which is currently in the peer review process. The document contains the record of physical experiments performed over the course of two years by a self-driving lab called the BEAR (Bayesian Autonomous Experimental Researcher). The BEAR consists of five FFF 3d printers, a scale, and an Instron. A UR5 robot arm moves samples for testing."
EMILY WHITING,"Data for ""designing lattices for impact protection using transfer learning""",
EMILY WHITING,"Data for ""using simulation to accelerate autonomous experimentation: a case study using mechanics""",
EMILY WHITING,"Data for ""a physics-informed impact model refined by multi-fidelity transfer learning""","This folder contains the experimental data for the research paper ""A physics-informed impact model refined by multi-fidelity transfer learning"" which is scheduled to be published in Extreme Mechanics Letters. The designs are broken up into several categories: TPU 1-6: Figures 1-3 TPU 7-8: Figure 4 TPU Foaming 1-14: Figure 5 Both STL and Gcode files are available for each design. Note that spiral/vase mode is used when slicing, so the part will be hollow when sliced. The proper extrusion multiplier must be used to reach the mass target for each design. The slicer settings are included in the Gcode at the end of the file. The raw data for physical experiments performed in both constant velocity tests in a universal testing machine (UTM) and impact tests using a drop tower are also included. They use the following naming structure: UTM tests: <material>_V<velocity>_<trial>.csv Note that velocity is in mm/min and trial is either a, b, or c. Only 2 mm/min velocity tests contain b and c trials. Impact tests: <material>_V<velocity>.csv Note that velocity is in m/s and goes to two decimal places."
EMILY WHITING,KnitScape: computational design and yarn-level simulation of slip and tuck colorwork knitting patterns,"Slipped and tucked stitches introduce small areas of deformation that compound and result in emergent textures on knitted fabrics. When used together with color changes and ladders, these can also produce dramatic colorwork and openwork effects. However, designing slip and tuck colorwork patterns is challenging due to the complex interactions between operations, yarns, and deformations. We present KnitScape, a browser-based tool for design and simulation of stitch patterns for knitting. KnitScape provides a design interface to specify 1) operation repeats, 2) color changes, and 3) needle positions. These inputs are used to build a graph of yarn topology and run a yarn-level spring simulation. This enables visualization of the deformation that arises from slip and tuck operations. Through its design tool and simulation, KnitScape enables rapid exploration of a complex colorwork design space. We demonstrate KnitScape with a series of example swatches."
MICHELLE SANDER,Genome-Wide Association with Select Biomarker Traits in the Framingham Heart Study,"BACKGROUND: Systemic biomarkers provide insights into disease pathogenesis, diagnosis, and risk stratification. Many systemic biomarker concentrations are heritable phenotypes. Genome-wide association studies (GWAS) provide mechanisms to investigate the genetic contributions to biomarker variability unconstrained by current knowledge of physiological relations. METHODS: We examined the association of Affymetrix 100K GeneChip single nucleotide polymorphisms (SNPs) to 22 systemic biomarker concentrations in 4 biological domains: inflammation/oxidative stress; natriuretic peptides; liver function; and vitamins. Related members of the Framingham Offspring cohort (n = 1012; mean age 59 ± 10 years, 51% women) had both phenotype and genotype data (minimum-maximum per phenotype n = 507–1008). We used Generalized Estimating Equations (GEE), Family Based Association Tests (FBAT) and variance components linkage to relate SNPs to multivariable-adjusted biomarker residuals. Autosomal SNPs (n = 70,987) meeting the following criteria were studied: minor allele frequency ≥ 10%, call rate ≥ 80% and Hardy-Weinberg equilibrium p ≥ 0.001. RESULTS: With GEE, 58 SNPs had p < 10-6: the top SNPs were rs2494250 (p = 1.00*10-14) and rs4128725 (p = 3.68*10-12) for monocyte chemoattractant protein-1 (MCP1), and rs2794520 (p = 2.83*10-8) and rs2808629 (p = 3.19*10-8) for C-reactive protein (CRP) averaged from 3 examinations (over about 20 years). With FBAT, 11 SNPs had p < 10-6: the top SNPs were the same for MCP1 (rs4128725, p = 3.28*10-8, and rs2494250, p = 3.55*10-8), and also included B-type natriuretic peptide (rs437021, p = 1.01*10-6) and Vitamin K percent undercarboxylated osteocalcin (rs2052028, p = 1.07*10-6). The peak LOD (logarithm of the odds) scores were for MCP1 (4.38, chromosome 1) and CRP (3.28, chromosome 1; previously described) concentrations; of note the 1.5 support interval included the MCP1 and CRP SNPs reported above (GEE model). Previous candidate SNP associations with circulating CRP concentrations were replicated at p < 0.05; the SNPs rs2794520 and rs2808629 are in linkage disequilibrium with previously reported SNPs. GEE, FBAT and linkage results are posted at . CONCLUSION: The Framingham GWAS represents a resource to describe potentially novel genetic influences on systemic biomarker variability. The newly described associations will need to be replicated in other studies."
MICHELLE SANDER,"Adiposity, Cardiometabolic Risk, and Vitamin D Status: The Framingham Heart Study","OBJECTIVE: Because vitamin D deficiency is associated with a variety of chronic diseases, understanding the characteristics that promote vitamin D deficiency in otherwise healthy adults could have important clinical implications. Few studies relating vitamin D deficiency to obesity have included direct measures of adiposity. Furthermore, the degree to which vitamin D is associated with metabolic traits after adjusting for adiposity measures is unclear. RESEARCH DESIGN AND METHODS: We investigated the relations of serum 25-hydroxyvitamin D (25[OH]D) concentrations with indexes of cardiometabolic risk in 3,890 nondiabetic individuals; 1,882 had subcutaneous adipose tissue (SAT) and visceral adipose tissue (VAT) volumes measured by multidetector computed tomography (CT). RESULTS: In multivariable-adjusted regression models, 25(OH)D was inversely associated with winter season, waist circumference, and serum insulin (P < 0.005 for all). In models further adjusted for CT measures, 25(OH)D was inversely related to SAT (−1.1 ng/ml per SD increment in SAT, P = 0.016) and VAT (−2.3 ng/ml per SD, P < 0.0001). The association of 25(OH)D with insulin resistance measures became nonsignificant after adjustment for VAT. Higher adiposity volumes were correlated with lower 25(OH)D across different categories of BMI, including in lean individuals (BMI <25 kg/m2). The prevalence of vitamin D deficiency (25[OH]D <20 ng/ml) was threefold higher in those with high SAT and high VAT than in those with low SAT and low VAT (P < 0.0001). CONCLUSIONS: Vitamin D status is strongly associated with variation in subcutaneous and especially visceral adiposity. The mechanisms by which adiposity promotes vitamin D deficiency warrant further study."
MICHELLE SANDER,Infrared inhibition impacts on locally initiated and propagating action potentials and the downstream synaptic transmission,"SIGNIFICANCE: Systematic studies of the physiological outputs induced by infrared (IR)-mediated inhibition of motor nerves can provide guidance for therapeutic applications and offer critical insights into IR light modulation of complex neural networks. AIM: We explore the IR-mediated inhibition of action potentials (APs) that either propagate along single axons or are initiated locally and their downstream synaptic transmission responses. APPROACH: APs were evoked locally by two-electrode current clamp or at a distance for propagating APs. The neuromuscular transmission was recorded with intracellular electrodes in muscle cells or macro-patch pipettes on terminal bouton clusters. RESULTS: IR light pulses completely and reversibly terminate the locally initiated APs firing at low frequencies, which leads to blocking of the synaptic transmission. However, IR light pulses only suppress but do not block the amplitude and duration of propagating APs nor locally initiated APs firing at high frequencies. Such suppressed APs do not influence the postsynaptic responses at a distance. While the suppression of AP amplitude and duration is similar for propagating and locally evoked APs, only the former exhibits a 7% to 21% increase in the maximum time derivative of the AP rising phase. CONCLUSIONS: The suppressed APs of motor axons can resume their waveforms after passing the localized IR light illumination site, leaving the muscular and synaptic responses unchanged. IR-mediated modulation on propagating and locally evoked APs should be considered as two separate models for axonal and somatic modulations."
ROSEMARIE ZIEDONIS,State governments as financiers of technology startups: evidence from Michigan's R&D loan program,"State governments in the United States often fund and support technology startups within their borders. Yet little is known about the magnitude with which these place-based policy interventions shift the performance trajectories of entrepreneurial firms. We provide new evidence based on 241 startups that compete for advanced research and technology commercialization loans between 2002 and 2008 through a Michigan-based program. Among applicants with project scores near the threshold required for funding, we find that award recipients are 20 to 30 percent more likely to remain in business four years after the competition relative to similar companies that seek but fail to receive funding. We also find that award receipt stimulates follow-on venture capital (VC) investments in surviving companies. The VC stimulus effect is, however, disproportionately driven by subsets of firms that are very young, relatively inexperienced at external fundraising, or located outside the dominant hub of entrepreneurial activity within the state. This distinctive pattern of heterogeneous effects remains visible for follow-on R&D financing from federal government sources, and for supplemental outcome measures that use news articles to track shifts in financing and business development activities. These findings are consistent with the view that public R&D programs are particularly beneficial when frictions in private resource markets are more severe."
ROSEMARIE ZIEDONIS,"Patent collateral, investor commitment, and the market for venture lending","This paper investigates the market for lending to technology startups (i.e., venture lending) and examines two mechanisms that may facilitate trade within it: (1) the ‘salability’ of patent collateral; and (2) the credible commitment of existing equity investors. We find that intensified trading in the secondary patent market is strongly related to the annual rate of startup lending, particularly for startups with more redeployable patent assets. Moreover, we show that the credibility of venture capitalist commitments to reinvest in their startups’ next round of financing can be critical for startup debt provision. Utilizing the crash of 2000 as a severe and unexpected capital supply shock for VCs, we show that lenders continue to finance startups with recently funded investors better able to credibly commit to refinance their portfolio companies, but withdraw from otherwise-promising projects that may have needed their funds the most. The findings are consistent with predictions of incomplete contracting and financial intermediation theory."
ROSEMARIE ZIEDONIS,How redeployable are patent assets? Evidence from failed startups,"Entrepreneurial firms are important sources of patented inventions. Yet little is known about what happens to patents ""released"" to the market when startups fail. This study provides a first look at the frequency and speed with which patents originating from failed startups are redeployed to new owners, and whether the value of patents is tied to the original venture and team. The evidence is based on 1,766 U.S. patents issued to 285 venture capital-backed startups that disband between 1988 and 2008 in three innovation-intensive sectors: medical devices, semiconductors, and software. At odds with the view that the resale market for patented inventions is illiquid, we find that most patents from these startups are sold, are sold quickly, and remain ""alive"" through renewal fee payment long after the startups are shuttered. The patents tend to be purchased by other operating companies in the same sector, and retain value beyond the original venture and team. We do find, however, that the patents and people sometimes move jointly to a new organization following the dissolution of the original venture, and explore the conditions under which such co-movement is more likely. The study provides new evidence on a phenomenon-of active markets for buying and selling patents-underexplored in the strategy literature and consequential for both entrepreneurial and established firms."
MICHELLE A AMAZEEN,Revisiting the epistemology of fact-checking,"Joseph E. Uscinski and Ryden W. Butler (2013) argue that fact-checking should be condemned to the dustbin of history because the methods fact-checkers use to select statements, consider evidence, and render judgment fail to stand up to the rigors of scientific inquiry and threaten to stifle political debate. However, the premises upon which they build their arguments are flawed. By sampling from multiple “fact-checking agencies” that do not practice fact-checking on a regular basis in a consistent manner, they perpetuate the selection effects they criticize and thus undermine their own position. Furthermore, not only do their arguments suffer from overgeneralization, they fail to offer empirical quantification to support some of their anecdotal criticisms. This rejoinder offers a study demonstrating a high level of consistency in fact-checking and argues that as long as unambiguous practices of deception continue, fact-checking has an important role to play in the United States and around the world."
MICHELLE A AMAZEEN,The agenda-setting power of fake news: A big data analysis of the online media landscape from 2014 to 2016,"This study examines the agenda-setting power of fake news and fact-checkers who fight them through a computational look at the online mediascape from 2014 to 2016. Although our study confirms that content from fake news websites is increasing, these sites do not exert excessive power. Instead, fake news has an intricately entwined relationship with online partisan media, both responding and setting its issue agenda. In 2016, partisan media appeared to be especially susceptible to the agendas of fake news, perhaps due to the election. Emerging news media are also responsive to the agendas of fake news, but to a lesser degree. Fake news coverage itself is diverging and becoming more autonomous topically. While fact-checkers are autonomous in their selection of issues to cover, they were not influential in determining the agenda of news media overall, and their influence appears to be declining, illustrating the difficulties fact-checkers face in disseminating their corrections."
MICHELLE A AMAZEEN,The effects of native advertising disclosure format on audience perceptions of legacy and online news publishers,"This experimental study examines elements of native advertising disclosures that influence consumers’ ability to recognize content as paid advertising and contrasts subsequent evaluations of legacy and digital-first publishers with those exposed to online display advertising. Although fewer than 1 in 10 participants were able to recognize native advertising, our study shows that effectively designed disclosure labels facilitate recognition. However, participants who did recognize native advertising had lessened opinions of the publisher and the institution of advertising, overall.
 This experiment with a representative sample of US adults (N = 800) examines the effects of disclosure design characteristics in sponsored news on readers’ ability to recognize such content as paid advertising, and examines whether such recognition differently affects perceptions of legacy and digital-first publishers. Although fewer than 1 in 10 participants were able to recognize native advertising, our study shows that effectively designed disclosure labels facilitate recognition. However, participants who did recognize native advertising had lessened opinions of the publisher and the institution of advertising, overall.
 "
MICHELLE A AMAZEEN,Checking the fact-checkers in 2008: predicting political ad scrutiny and assessing consistency,"Which types of political ads are most likely to draw criticism from fact-checkers? Are fact-checkers consistent in their evaluations of political ads? Examining general election television ads from the 2008 U.S. presidential race, and based upon the evaluations of FactCheck.org, PolitiFact.com, and the Washington Post's Fact Checker, this study demonstrates it was the attack ads from candidates that were most likely to draw scrutiny from the fact-checkers. Most importantly, a high level of agreement between the fact-checkers indicates their success at selecting political claims that can be consistently evaluated. While political advertisers are increasingly using evidence to support their claims, what may be more critical in drawing evaluations from fact-checkers is the verifiability of a claim. The implications of consistent fact-checking on the public, political actors, journalism and democracy are discussed. With the revelation that fact-checking can be consistently practiced, localized efforts at fact-checking need encouragement, particularly as political TV ads increasingly drown out other potential sources of information for the public and increasingly are used in downballot races, local initiatives, referendums and judicial races."
MICHELLE A AMAZEEN,The effects of disclosure format on native advertising recognition and audience perceptions of legacy and online news publishers,"This experiment with a representative sample of US adults (N=800) examines the effects of disclosure design characteristics in sponsored news on readers’ ability to recognize such content as paid advertising, and examines whether such recognition differently affects perceptions of legacy and digital-first publishers. Although fewer than 1 in 10 participants were able to recognize native advertising, our study shows that effectively designed disclosure labels facilitate recognition. However, participants who did recognize native advertising had lessened opinions of the publisher and the institution of advertising, overall."
MICHELLE A AMAZEEN,Saving media or trading on trust?,"Extending research from Wojdynski and Evans, this experimental study replicates the challenges of effectively disclosing native advertising to readers and demonstrates a promising inoculation method that increases likelihood of recognition. Moreover, this quantitative research indicates that both legacy and online news publishers were evaluated less favorably for displaying native advertising. Attitudes toward the publisher and perceptions of its credibility declined for both, although online publishers suffered greater attitudinal damage than did legacy publishers who may benefit from their established reputation."
MICHELLE A AMAZEEN,Journalistic interventions: The structural factors affecting the global emergence of fact-checking,"Since the emergence of FactCheck.org in the United States in 2003, fact-checking interventions have expanded both domestically and globally. The Duke Reporter’s Lab identified nearly 100 active initiatives around the world in 2016. Building off of previous exploratory work by Amazeen, this research utilizes the framework of critical juncture theory to examine why fact-checking interventions are spreading globally at this point in time. Seen as a professional reform movement in the journalistic community, historical research on reform movements suggests several possible factors influencing the emergence of fact-checking such as a decline in journalism, easy access to technology for the masses, and socio-political strife. This study offers empirical support that fact-checking may be understood as a democracy-building tool that emerges where democratic institutions are perceived to be weak or are under threat and examines similarities between the growth of fact-checking interventions and previous consumer reform movements. As politics increasingly adopts strategies orchestrated by marketing and advertising consultants and agencies – exemplified in the Brexit referendum – political fact-checking may benefit from examining the path of consumer reform movements. For, before fact-checking can be effective at informing individuals, it must first establish itself within a structural environment."
MICHELLE A AMAZEEN,Reinforcing attitudes in a gatewatching news era: individual-level antecedents to sharing fact-checks on social media,"Despite the prevalence of fact-checking, little is known about who posts fact-checks online. Based upon a content analysis of Facebook and Twitter digital trace data and a linked online survey (N = 783), this study reveals that sharing fact-checks in political conversations on social media is linked to age, ideology, and political behaviors. Moreover, an individual’s need for orientation (NFO) is an even stronger predictor of sharing a fact-check than ideological intensity or relevance, alone, and also influences the type of fact-check format (with or without a rating scale) that is shared. Finally, participants generally shared fact-checks to reinforce their existing attitudes. Consequently, concerns over the effects of fact-checking should move beyond a limited-effects approach (e.g., changing attitudes) to also include reinforcing accurate beliefs."
MICHELLE A AMAZEEN,Resisting covert persuasion in digital news: comparing inoculation and reactance in the processing of native advertising disclosures and in article engagement intentions,"An online experiment ( N = 931) assessing recognition of and responses to native advertising sought to explore how disclosures affect behavioral intent in digital news contexts. Findings suggest that resistance to persuasive attempts conferred by native advertising disclosures is explained by both inoculation and reactance processes and demonstrates how a simple, or generic, disclosure can inoculate people against a type of message (covert advertising mimicking authentic journalism) rather than the content of the message. Furthermore, the attenuating effect of a simple disclosure on behavioral intent is fully and serially mediated through advertising recognition, increased perception of threat to freedom, and increased reactance."
MICHELLE A AMAZEEN,Reducing Native advertising deception: revisiting the antecedents and consequences of persuasion knowledge in digital news contexts,"Building on the persuasion knowledge model, this study examines how audience characteristics and native advertising recognition influence the covert persuasion process. Among a nationally representative sample of U.S. adults (N = 738), we examined digital news readers’ recognition of a sponsored news article as advertising. Although fewer than 1 in 10 readers recognized the article as advertising, recognition was most likely among younger, more educated consumers who engaged with news media for informational purposes. Recognition led to greater counterarguing, and higher levels of informational motivation also led to less favorable evaluations of the content among recognizers. News consumers were most receptive to native advertising in a digital news context when publishers were more transparent about its commercial nature. Beyond theoretical insights into the covert persuasion process, this study offers practical utility to the advertisers, publishers, and policymakers who wish to better understand who is more likely to be confused by this type of advertising so that they can take steps to minimize deception."
MICHELLE A AMAZEEN,"The politics of memory: contesting the ""Convention Night"" version of this historic day","To the degree that the American press corps serves as the creator of the first draft of history, it is in a privileged position of shaping not only what we remember but also how we remember it. This article presents a case study of a political advertisement that aired during the 2008 U.S. presidential election. The mainstream media’s uncritical consideration of the ad invoking Martin Luther King Jr.’s memory in representing Barack Obama’s achievements suggests not only an uncontested version of racial achievements in America but also the power granted to political ads in narrating a naturalized version of memory. As political advertising increasingly drives news coverage in the U.S., the journalistic failure to scrutinize a political message beyond its face value illustrated by this case becomes all the more alarming and is indicative of a concerning disservice to the public interest."
MICHELLE A AMAZEEN,Developing an ad-reporting typology: a network analysis approach to newspaper and fact-checker coverage of the 2008 presidential election,"In a media environment where political ads influence U.S. news coverage, does the press hold politicians accountable for their ad claims? Using semantic network analysis, four types of reporting styles distinguished the ad coverage of 18 sampled newspapers and online fact-checkers from the 2008 presidential election. Horserace reporting characterized the largest group, whereas other models included partisan, he-said/she-said, and fact-checking. If educating the public about the accuracy of political ad claims is a goal of journalism, this study suggests that high-quality political adwatching following the fact-checking journalistic model did not predominate in the 2008 political ad coverage of newspapers."
MICHELLE A AMAZEEN,Correcting political and consumer misperceptions,"While fact-checking has grown dramatically in the last decade, little is known about the relative effectiveness of different formats in correcting false beliefs or overcoming partisan resistance to new information. This article addresses that gap by using theories from communication and psychology to compare two prevailing approaches: An online experiment examined how the use of visual “truth scales” interacts with partisanship to shape the effectiveness of corrections. We find that truth scales make fact-checks more effective in some conditions. Contrary to theoretical predictions and the fears of some journalists, their use does not increase partisan backlash against the correction or the organization that produced it."
MICHELLE A AMAZEEN,Sharing native advertising on Twitter: content analyses examining disclosure practices and their inoculating influence,"Based upon a large data set of tweets linking to native advertising in leading U.S. news publications, this study provides a content analysis of the practice of native advertising disclosure in the field and explores whether disclosures serve the inoculating function of resistance to persuasion. Leveraging the Persuasion Knowledge Model (Friestad & Wright, 1994) and inoculation theory (McGuire, 1964), results show a) regular use of disclosures on publisher landing pages, b) the absence of disclosures in over half of publisher thumbnail images, and c) the presence of disclosures corresponded to an increased likelihood of negatively-valenced posts."
MICHELLE A AMAZEEN,Practitioner perceptions: critical junctures and the global emergence and challenges of fact-checking,"Since 2003 and the emergence of FactCheck.org in the United States, fact-checking has expanded both domestically and internationally. As of February, 2016, the Duke Reporter’s Lab identified nearly 100 active initiatives around the world. This research explores why fact-checking is spreading globally at this point in time. Seen as a professional reform movement in the journalistic community (Graves, 2016), historical research on reform movements suggest several possible factors influencing the emergence of fact-checking including a decline in journalism, easy access to technology for the masses, and socio-political strife (McChesney, 2007; Pickard, 2015; Stole, 2006). Using a phenomenological approach, two focus groups were conducted among fact-checkers during the 2015 Global Fact-checking Summit in London, England. Participants shared rich experiences about conditions and contexts surrounding the emergence and challenges facing their organizations. Ultimately, as the purpose of this research is to help future fact-checkers around the world become aware of the circumstances under which fact-checking is most likely to emerge and thrive (or fail), recommendations from current global practitioners are offered."
MICHELLE A AMAZEEN,"Missing voices: examining how misinformation-susceptible individuals from underrepresented communities engage, perceive, and combat science misinformation","This study examines how misinformation-susceptible individuals from historically excluded and marginalized communities engage with science topics (e.g., climate change, vaccines, and health/wellness) and interpret misinformation and corrective intervention strategies. Two focus groups reveal that most participants are highly distrustful of authority figures, celebrity endorsements, and fact-checking strategies to combat misinformation. As one of the first studies to explore underrepresented community members’ experiences with science misinformation, findings reveal structural and institutional power dynamics that impede access to accurate information and indicate how missing voices must be included in the efforts at media and information literacy initiatives."
MICHELLE A AMAZEEN,News in an era of content confusion: effects of news use motivations and context on native advertising and digital news perceptions,"This study examined the effects of differing native advertising framing contexts (hard versus soft news) and individuals’ news use motivations on ability to perceive commercialized content, evaluations of native advertising, and ensuing digital news perceptions. Based upon the framework of the persuasion knowledge model, an online experiment was conducted among a sample of U.S. adults (N = 684). When revealed as advertising, people were more likely to perceive the hard news rather than the soft news framing as commercial in nature. Furthermore, hard news approaches to native advertising were perceived unfavorably by audiences and tarnished the subsequent reporting of actual journalists."
MICHELLE A AMAZEEN,Native advertising in a mobile era: effects of ability and motivation on recognition in digital news contexts,"Digital news is progressively blurring with commercial content at the same time that mobile technology is increasingly being used to access news. To understand if these trends affect ability to distinguish news from covert advertising, two experiments were conducted examining whether viewing content on a mobile device versus computer interacts with motivation levels in affecting recognition of native advertising. Consistent with tenets of the Covert Advertising Recognition and Effects model, results show that although people with greater motivation to process media content were more likely to recognize native advertising, it was not enough to compensate for the reduced ability to process information inherent with mobile consumption."
MICHELLE A AMAZEEN,Conferring resistance to digital disinformation: the innoculating influence of procedural news knowledge,"Despite the pervasiveness of digital disinformation in society, little is known about the individual characteristics that make some users more susceptible to erroneous information uptake than others, effectively dividing the media audience into prone and resistant groups. This study identifies and tests procedural news knowledge as a consequential civic resource with the capacity to inoculate audiences from disinformation and close this “resistance gap.” Engaging the persuasion knowledge model, the study utilizes data from two national surveys to demonstrate that possessing working knowledge of how the news media operate aids in the identification and effects of fabricated news and native advertising."
MICHELLE A AMAZEEN,Agenda-cutting versus agenda-building: does sponsored content influence corporate news coverage in U.S. media?,"Sponsored content articles (N = 2,711) from 27 major U.S. corporations were analyzed across five years in The New York Times, The Washington Post, and The Wall Street Journal. The degree to which sponsored content predicted significant changes in corporate news coverage was investigated for elite media and U.S. online media. Corporate-sponsored content appeared to mildly suppress coverage of that corporation in online news. This effect, known as agenda cutting, happened both inside elite media and across the media landscape. Conversely, agenda building, or instances where sponsored content resulted in more media coverage, was very rare. We suggest that “content studios,” the departments of news media organizations that create sponsored content, may be exhibiting an agenda-setting effect more akin to traditional advertising departments, which have been known to suppress critical coverage of corporations that pay for ads."
MICHELLE A AMAZEEN,The Debunking Handbook 2020,
MICHELLE A AMAZEEN,The COVID-19 vaccine communication handbook. A practical guide for improving vaccine communication and fighting misinformation,"This handbook is for journalists, doctors, nurses, policy makers, researchers, teachers, students, parents – in short, it’s for everyone who wants to know more: About the COVID-19 vaccines; How to talk to others about them; How to challenge misinformation about the vaccines."
CANAN GUNES CORLU,The role of learning on industrial simulation design and analysis,"The capability of modeling real-world system operations has turned simulation into an indispensable problemsolving methodology for business system design and analysis. Today, simulation supports decisions ranging from sourcing to operations to finance, starting at the strategic level and proceeding towards tactical and operational levels of decision-making. In such a dynamic setting, the practice of simulation goes beyond being a static problem-solving exercise and requires integration with learning. This article discusses the role of learning in simulation design and analysis motivated by the needs of industrial problems and describes how selected tools of statistical learning can be utilized for this purpose."
CANAN GUNES CORLU,Semiconductor manufacturing simulation design and analysis with limited data,"This paper discusses simulation design and analysis for Silicon Carbide (SiC) manufacturing operations management at New York Power Electronics Manufacturing Consortium (PEMC) facility. Prior work has addressed the development of manufacturing system simulation as the decision support to solve the strategic equipment portfolio selection problem for the SiC fab design [1]. As we move into the phase of collecting data from the equipment purchased for the PEMC facility, we discuss how to redesign our manufacturing simulations and analyze their outputs to overcome the challenges that naturally arise in the presence of limited fab data. We conclude with insights on how an approach aimed to reflect learning from data can enable our discrete-event stochastic simulation to accurately estimate the performance measures for SiC manufacturing at the PEMC facility."
JOSEPH MCGUIRE,Test-retest reliability of task-based measures of voluntary persistence,
JOSEPH MCGUIRE,Arousal‐based pupil modulation is dictated by luminance,"Pupillometry has become a standard measure for assessing arousal state. However, environmental factors such as luminance, a primary dictator of pupillary responses, often vary across studies. To what degree does luminance interact with arousal-driven pupillary changes? Here, we parametrically assessed luminance-driven pupillary responses across a wide-range of luminances, while concurrently manipulating cognitive arousal using auditory math problems of varying difficulty. At the group-level, our results revealed that the modulatory effect of cognitive arousal on pupil size interacts multiplicatively with luminance, with the largest effects occurring at low and mid-luminances. However, at the level of individuals, there were qualitatively distinct individual differences in the modulatory effect of cognitive arousal on luminance-driven pupillary responses. Our findings suggest that pupillometry as a measure for assessing arousal requires more careful consideration: there are ranges of luminance levels that are more ideal in observing pupillary differences between arousal conditions than others."
JOSHUA GOODMAN,Racial diversity and measuring merit: evidence from Boston's exam school admissions,"The impact of admissions process design on the racial diversity of schools and colleges has sparked heated debates. We study the pipeline into Boston's three public exam schools to understand racial gaps in enrollment. Admission to these schools has historically been based on a combination of grade point average (GPA) and a score on an optional test from a private developer. We document racial gaps in test-taking rates, test scores, GPAs, preferences for the most selective school, and ultimate admission rates to all three schools. These gaps persist even among students with similarly high baseline achievement as measured by the state's mandatory standardized test. Substantial numbers of high-achieving black and Hispanic students do not apply to the exam schools and to the most selective school in particular. The choice of standardized test used to measure academic merit strongly affects who is admitted."
JOSHUA GOODMAN,"The pandemic's effect on demand for public schools, homeschooling, and private schools","The Covid-19 pandemic drastically disrupted the functioning of U.S. public schools, potentially changing the relative appeal of alternatives such as homeschooling and private schools. Using longitudinal student-level administrative data from Michigan and nationally representative data from the Census Household Pulse Survey, we show how the pandemic affected families’ choices of school sector. We document four central facts. First, public school enrollment declined noticeably in fall 2020, with about 3 percent of Michigan students and 10 percent of kindergartners using other options. Second, most of this was driven by homeschooling rates jumping substantially, driven largely by families with children in elementary school. Third, homeschooling increased more where schools provided in-person instruction while private schooling increased more where instruction was remote, suggesting heterogeneity in parental concerns about children’s physical health and instructional quality. Fourth, kindergarten declines were highest among low income and Black families while declines in other grades were highest among higher income and White families, highlighting important heterogeneity by students’ existing attachment to public schools. Our results shed light on how families make schooling decisions and imply potential longer-run disruptions to public schools in the form of decreased enrollment and funding, changed composition of the student body, and increased size of the next kindergarten cohort."
JOSHUA GOODMAN,The COVID-19 pandemic disrupted both school bullying and cyberbullying,
JOSHUA GOODMAN,Objective course placement and college readiness: evidence from targeted middle school math acceleration,
JOSHUA GOODMAN,Take two! SAT retaking and college enrollment gaps,"Only half of SAT-takers retake the exam, with even lower retake rates among low-income students and underrepresented minority (URM) students. We exploit discontinuous jumps in retake probabilities at multiples of 100, driven by left-digit bias, to estimate retaking’s causal effects. Retaking substantially improves SAT scores and increases four-year college enrollment rates, particularly for low-income and URM students. Eliminating disparities in retake rates could close up to 10 percent of the income-based gap and up to 7 percent of the race-based gap in four-year college enrollment rates of high school graduates. (JEL I21, I23, I24, J15)"
JOSHUA GOODMAN,Heat and learning,
JOSHUA GOODMAN,Can online delivery increase access to education?,
JOSHUA GOODMAN,The challenges of leveraging online education for economically vulnerable mid-career Americans,
JOSHUA GOODMAN,"Can states take over and turn around school districts? Evidence from Lawrence, Massachusetts","The Every Student Succeeds Act (ESSA) requires states to identify and turn around struggling schools, with federal school improvement money required to fund evidence-based policies. Most research on turnarounds has focused on individual schools, whereas studies of district-wide turnarounds have come from relatively exceptional settings and interventions. We study a district-wide turnaround of a type that may become more common under ESSA, an accountability-driven state takeover of Massachusetts’s Lawrence Public Schools (LPS). A differences-in-differences framework comparing LPS to demographically similar districts not subject to state takeover shows that the turnaround’s first 2 years produced sizable achievement gains in math and modest gains in reading. We also find no evidence that the turnaround resulted in slippage on nontest score outcomes and suggestive evidence of positive effects on grade progression among high school students. Intensive small-group instruction over vacation breaks may have led to particularly large achievement gains for participating students."
JOSHUA GOODMAN,"O Brother, where start thou? Sibling spillovers on college and major choice in four countries","Family and social networks are widely believed to influence important life decisions, but causal identification of those effects is notoriously challenging. Using data from Chile, Croatia, Sweden, and the United States, we study within-family spillovers in college and major choice across a variety of national contexts. Exploiting college-specific admissions thresholds that directly affect older but not younger siblings’ college options, we show that in all four countries a meaningful portion of younger siblings follow their older sibling to the same college or college-major combination. Older siblings are followed regardless of whether their target and counterfactual options have large, small, or even negative differences in quality. Spillover effects disappear, however, if the older sibling drops out of college, suggesting that older siblings’ college experiences matter. That siblings influence important human capital investment decisions across such varied contexts suggests that our findings are not an artifact of particular institutional detail but a more generalizable description of human behavior. Causal links between the postsecondary paths of close peers may partly explain persistent college enrollment inequalities between social groups, and this suggests that interventions to improve college access may have multiplier effects."
JOSHUA GOODMAN,"The pandemic’s effect on demand for public schools, homeschooling, and private schools",
JOSHUA GOODMAN,The COVID-19 pandemic is a lousy natural experiment for studying the effects of online learning,
JOSHUA GOODMAN,The long-baseline neutrino experiment: exploring fundamental symmetries of the universe,"The preponderance of matter over antimatter in the early Universe, the dynamics of the supernova bursts that produced the heavy elements necessary for life and whether protons eventually decay --- these mysteries at the forefront of particle physics and astrophysics are key to understanding the early evolution of our Universe, its current state and its eventual fate. The Long-Baseline Neutrino Experiment (LBNE) represents an extensively developed plan for a world-class experiment dedicated to addressing these questions. LBNE is conceived around three central components: (1) a new, high-intensity neutrino source generated from a megawatt-class proton accelerator at Fermi National Accelerator Laboratory, (2) a near neutrino detector just downstream of the source, and (3) a massive liquid argon time-projection chamber deployed as a far detector deep underground at the Sanford Underground Research Facility. This facility, located at the site of the former Homestake Mine in Lead, South Dakota, is approximately 1,300 km from the neutrino source at Fermilab -- a distance (baseline) that delivers optimal sensitivity to neutrino charge-parity symmetry violation and mass ordering effects. This ambitious yet cost-effective design incorporates scalability and flexibility and can accommodate a variety of upgrades and contributions. With its exceptional combination of experimental configuration, technical capabilities, and potential for transformative discoveries, LBNE promises to be a vital facility for the field of particle physics worldwide, providing physicists from around the globe with opportunities to collaborate in a twenty to thirty year program of exciting science. In this document we provide a comprehensive overview of LBNE's scientific objectives, its place in the landscape of neutrino physics worldwide, the technologies it will incorporate and the capabilities it will possess."
SHEILAH A BERNARD,Listen Before You Auscultate Bedside Cardiac Assessment Trailer,"Introduction: Bedside cardiac assessment (BCA) is deficient across a spectrum of non-cardiology trainees. Learners not taught BCA well may become instructors who do not teach well, creating a self-perpetuating problem. We aimed to improve BCA teaching and learning by developing a high-quality, patient-centered curriculum for medicine clerkship students that could be flexibly implemented and accessible to other health professions learners. Methods: With a constructivist perspective, we aligned learning goals, activities, and assessments. The curriculum used a “listen before you auscultate” framework, capturing patient history as context for a six-step, systematic approach. In the flipped classroom, short videos and practice questions preceded two, 1-hour class activities that integrated diagnostic reasoning, pathophysiology, physical diagnosis, and reflection. Activities included case discussions, JVP evaluation, heart sound competitions, and simulated conversations with patients. 268 students at four U.S. and international medical schools participated. We incorporated feedback, performed thematic analysis, and assessed learners’ confidence and knowledge. Results: Low post-test data capture limited quantitative results. Students reported increased confidence in BCA ability. Knowledge increased in both BCA and control groups. Thematic analysis suggested instructional design strategies were effective and peer encounters, skills practice, and encounters with educators were meaningful. Discussion: The curriculum supported active learning of day-to-day clinical competencies. Explicitly incorporating notions of trust, it promoted professional identity formation alongside BCA ability. Feedback and increased confidence on the late-clerkship post-test suggested durable learning. We recommended approaches to confirm this and other elements of knowledge, skill acquisition, or behaviors, and are surveying impacts on professional identity formation-related constructs."
SHEILAH A BERNARD,Listen before you auscultate curriculum overview brochure,
CHANDRAMOULI CHANDRASEKARAN,Non-linear dimensionality reduction on extracellular waveforms reveals cell type diversity in premotor cortex,"Cortical circuits are thought to contain a large number of cell types that coordinate to produce behavior. Current in vivo methods rely on clustering of specified features of extracellular waveforms to identify putative cell types, but these capture only a small amount of variation. Here, we develop a new method (WaveMAP) that combines non-linear dimensionality reduction with graph clustering to identify putative cell types. We apply WaveMAP to extracellular waveforms recorded from dorsal premotor cortex of macaque monkeys performing a decision-making task. Using WaveMAP, we robustly establish eight waveform clusters and show that these clusters recapitulate previously identified narrow- and broad-spiking types while revealing previously unknown diversity within these subtypes. The eight clusters exhibited distinct laminar distributions, characteristic firing rate patterns, and decision-related dynamics. Such insights were weaker when using feature-based approaches. WaveMAP therefore provides a more nuanced understanding of the dynamics of cell types in cortical circuits."
CHANDRAMOULI CHANDRASEKARAN,A mechanistic multi-area recurrent network model of decision-making,"Recurrent neural networks (RNNs) trained on neuroscience-based tasks have been widely used as models for cortical areas performing analogous tasks. However, very few tasks involve a single cortical area, and instead require the coordination of multiple brain areas. Despite the importance of multi-area computation, there is a limited understanding of the principles underlying such computation. We propose to use multi-area RNNs with neuroscience-inspired architecture constraints to derive key features of multi-area computation. In particular, we show that incorpo- rating multiple areas and Dale’s Law is critical for biasing the networks to learn biologically plausible solutions. Additionally, we leverage the full observability of the RNNs to show that output-relevant information is preferentially propagated between areas. These results suggest that cortex uses modular computation to generate minimal sufficient representations of task information. More broadly, our results suggest that constrained multi-area RNNs can produce experimentally testable hypotheses for computations that occur within and across multiple brain areas, enabling new insights into distributed computation in neural systems."
CHANDRAMOULI CHANDRASEKARAN,A multi-stage recurrent neural network better describes decision-related activity in dorsal premotor cortex,"We studied how a network of recurrently connected artificial units solve a visual perceptual decision-making task. The goal of this task is to discriminate the dominant color of a central static checkerboard and report the decision with an arm movement. This task has been used to study neural activity in the dorsal premotor (PMd) cortex. When a single recurrent neural network (RNN) was trained to perform the task, the activity of artificial units in the RNN differed from neural recordings in PMd, suggesting that inputs to PMd differed from inputs to the RNN. We expanded our architecture and examined how a multi-stage RNN performed the task. In the multi-stage RNN, the last stage exhibited similarities with PMd by representing direction information but not color information. We then investigated how the representation of color and direction information evolve across RNN stages. Together, our results are a demonstration of the importance of incorporating architectural constraints into RNN models. These constraints can improve the ability of RNNs to model neural activity in association areas."
CHANDRAMOULI CHANDRASEKARAN,openEyeTrack - a high speed multi-threaded eye tracker for head-fixed applications,
CHANDRAMOULI CHANDRASEKARAN,Frequency shifts and depth dependence of premotor beta band activity during perceptual decision-making,"Neural activity in the premotor and motor cortices shows prominent structure in the beta frequency range (13–30 Hz). Currently, the behavioral relevance of this beta band activity (BBA) is debated. The underlying source of motor BBA and how it changes as a function of cortical depth are also not completely understood. Here, we addressed these unresolved questions by investigating BBA recorded using laminar electrodes in the dorsal premotor cortex of 2 male rhesus macaques performing a visual reaction time (RT) reach discrimination task. We observed robust BBA before and after the onset of the visual stimulus but not during the arm movement. While poststimulus BBA was positively correlated with RT throughout the beta frequency range, prestimulus correlation varied by frequency. Low beta frequencies (∼12–20 Hz) were positively correlated with RT, and high beta frequencies (∼22–30 Hz) were negatively correlated with RT. Analysis and simulations suggested that these frequency-dependent correlations could emerge due to a shift in the component frequencies of the prestimulus BBA as a function of RT, such that faster RTs are accompanied by greater power in high beta frequencies. We also observed a laminar dependence of BBA, with deeper electrodes demonstrating stronger power in low beta frequencies both prestimulus and poststimulus. The heterogeneous nature of BBA and the changing relationship between BBA and RT in different task epochs may be a sign of the differential network dynamics involved in cue expectation, decision-making, motor preparation, and movement execution."
CHANDRAMOULI CHANDRASEKARAN,Macaque dorsal premotor cortex exhibits decision-related activity only when specific stimulus-response associations are known,"How deliberation on sensory cues and action selection interact in decision-related brain areas is still not well understood. Here, monkeys reached to one of two targets, whose colors alternated randomly between trials, by discriminating the dominant color of a checkerboard cue composed of different numbers of squares of the two target colors in different trials. In a Targets First task the colored targets appeared first, followed by the checkerboard; in a Checkerboard First task, this order was reversed. After both cues appeared in both tasks, responses of dorsal premotor cortex (PMd) units covaried with action choices, strength of evidence for action choices, and RTs— hallmarks of decision-related activity. However, very few units were modulated by checkerboard color composition or the color of the chosen target, even during the checkerboard deliberation epoch of the Checkerboard First task. These findings implicate PMd in the action-selection but not the perceptual components of the decision-making process in these tasks."
CHANDRAMOULI CHANDRASEKARAN,ChaRTr: An R toolbox for modeling choices and response times in decision-making tasks,"BACKGROUND: Decision-making is the process of choosing and performing actions in response to sensory cues to achieve behavioral goals. Many mathematical models have been developed to describe the choice behavior and response time (RT) distributions of observers performing decision-making tasks. However, relatively few researchers use these models because it demands expertise in various numerical, statistical, and software techniques. NEW METHOD: We present a toolbox — Choices and Response Times in R, or ChaRTr — that provides the user the ability to implement and test a wide variety of decision-making models ranging from classic through to modern versions of the diffusion decision model, to models with urgency signals, or collapsing boundaries. RESULTS: In three different case studies, we demonstrate how ChaRTr can be used to effortlessly discriminate between multiple models of decision-making behavior. We also provide guidance on how to extend the toolbox to incorporate future developments in decision-making models. Comparison with existing method(s) Existing software packages surmounted some of the numerical issues but have often focused on the classical decision-making model, the diffusion decision model. Recent models that posit roles for urgency, time-varying decision thresholds, noise in various aspects of the decision-formation process or low pass filtering of sensory evidence have proven to be challenging to incorporate in a coherent software framework that permits quantitative evaluation among these competing classes of decision-making models. CONCLUSION: ChaRTr can be used to make insightful statements about the cognitive processes underlying observed decision-making behavior and ultimately for deeper insights into decision mechanisms."
CHANDRAMOULI CHANDRASEKARAN,Separating cognitive and motor processes in the behaving mouse,"The cognitive processes supporting complex animal behavior are closely associated with ubiquitous movements responsible for our posture, facial expressions, ability to actively sample our sensory environments, and other critical processes. These movements are strongly related to neural activity across much of the brain and are often highly correlated with ongoing cognitive processes, making it challenging to dissociate the neural dynamics that support cognitive processes from those supporting related movements. In such cases, a critical issue is whether cognitive processes are separable from related movements, or if they are driven by common neural mechanisms. Here, we demonstrate how the separability of cognitive and motor processes can be assessed, and, when separable, how the neural dynamics associated with each component can be isolated. We establish a novel two-context behavioral task in mice that involves multiple cognitive processes and show that commonly observed dynamics taken to support cognitive processes are strongly contaminated by movements. When cognitive and motor components are isolated using a novel approach for subspace decomposition, we find that they exhibit distinct dynamical trajectories. Further, properly accounting for movement revealed that largely separate populations of cells encode cognitive and motor variables, in contrast to the 'mixed selectivity' often reported. Accurately isolating the dynamics associated with particular cognitive and motor processes will be essential for developing conceptual and computational models of neural circuit function and evaluating the function of the cell types of which neural circuits are composed."
CHANDRAMOULI CHANDRASEKARAN,Initial conditions combine with sensory evidence to induce decision-related dynamics in premotor cortex,"We used a dynamical systems perspective to understand decision-related neural activity, a fundamentally unresolved problem. This perspective posits that time-varying neural activity is described by a state equation with an initial condition and evolves in time by combining at each time step, recurrent activity and inputs. We hypothesized various dynamical mechanisms of decisions, simulated them in models to derive predictions, and evaluated these predictions by examining firing rates of neurons in the dorsal premotor cortex (PMd) of monkeys performing a perceptual decision-making task. Prestimulus neural activity (i.e., the initial condition) predicted poststimulus neural trajectories, covaried with RT and the outcome of the previous trial, but not with choice. Poststimulus dynamics depended on both the sensory evidence and initial condition, with easier stimuli and fast initial conditions leading to the fastest choice-related dynamics. Together, these results suggest that initial conditions combine with sensory evidence to induce decision-related dynamics in PMd."
CHANDRAMOULI CHANDRASEKARAN,A cortical information bottleneck during decision-making,"Decision-making emerges from distributed computations across multiple brain areas, but it is unclear why the brain distributes the computation. In deep learning, artificial neural networks use multiple areas (or layers) to form optimal representations of task inputs. These optimal representations are sufficient to perform the task well, but minimal so they are invariant to other irrelevant variables. We recorded single neurons and multiunits in dorsolateral prefrontal cortex (DLPFC) and dorsal premotor cortex (PMd) in monkeys during a perceptual decision-making task. We found that while DLPFC represents task-related inputs required to compute the choice, the downstream PMd contains a minimal sufficient, or optimal, representation of the choice. To identify a mechanism for how cortex may form these optimal representations, we trained a multi-area recurrent neural network (RNN) to perform the task. Remarkably, DLPFC and PMd resembling representations emerged in the early and late areas of the multi-area RNN, respectively. The DLPFC-resembling area partially orthogonalized choice information and task inputs and this choice information was preferentially propagated to downstream areas through selective alignment with inter-area connections, while remaining task information was not. Our results suggest that cortex uses multi-area computation to form minimal sufficient representations by preferential propagation of relevant information between areas.The brain uses multiple areas for cognition, decision-making, and action, but it is unclear why the brain distributes the computation and why cortical activity differs by brain area. Machine learning and information theory suggests that one benefit of multiple areas is that it provides an “information bottleneck” that compresses inputs into an optimal representation that is minimal and sufficient to solve the task. Combining experimental recordings from behaving animals and computational simulations, we show that later brain areas have a tendency to form such minimal sufficient representations of task inputs through preferential propagation of task-relevant information present in earlier areas. Our results thus provide insight into why the brain uses multiple brain areas for supporting decision-making and action."
CHANDRAMOULI CHANDRASEKARAN,WaveMAP for identifying putative cell types from in vivo electrophysiology,
CLAUDIO FERRE,Using diffusion tensor imaging to identify corticospinal tract projection patterns in children with unilateral spastic cerebral palsy.,"AIM: To determine whether diffusion tensor imaging (DTI) can be an independent assessment for identifying the corticospinal tract (CST) projecting from the more-affected motor cortex in children with unilateral spastic cerebral palsy (CP). METHOD: Twenty children with unilateral spastic CP participated in this study (16 males, four females; mean age 9y 2mo [standard deviation (SD) 3y 2mo], Manual Ability Classification System [MACS] level I-III). We used DTI tractography to reconstruct the CST projecting from the more-affected motor cortex. We mapped the motor representation of the more-affected hand by stimulating the more- and the less-affected motor cortex measured with single-pulse transcranial magnetic stimulation (TMS). We then verified the presence or absence of the contralateral CST by comparing the TMS map and DTI tractography. Fisher's exact test was used to determine the association between findings of TMS and DTI. RESULTS: DTI tractography successfully identified the CST controlling the more-affected hand (sensitivity=82%, specificity=78%). INTERPRETATION: Contralateral CST projecting from the lesioned motor cortex assessed by DTI is consistent with findings of TMS mapping. Since CST connectivity may be predictive of response to certain upper extremity treatments, DTI-identified CST connectivity may potentially be valuable for determining such connectivity where TMS is unavailable or inadvisable for children with seizures."
CLAUDIO FERRE,Improvements in upper extremity function following intensive training are independent of corticospinal tract organization in children with unilateral spastic cerebral palsy: a clinical randomized trial,"BACKGROUND/OBJECTIVES: Intensive training of the more affected upper extremity (UE) has been shown to be effective for children with unilateral spastic cerebral palsy (USCP). Two types of UE training have been particularly successful: Constraint-Induced Movement Therapy (CIMT) and Bimanual training. Reorganization of the corticospinal tract (CST) early during development often occurs in USCP. Prior studies have suggested that children with an ipsilateral CST controlling the affected UE may improve less following CIMT than children with a contralateral CST. We tested the hypothesis that improvements in UE function after intensive training depend on CST laterality. Study Participants and Setting: Eighty-two children with USCP, age 5 years 10 months to 17 years, University laboratory setting. MATERIALS/METHODS: Single-pulse transcranial magnetic stimulation (TMS) was used to determine each child's CST connectivity pattern. Children were stratified by age, sex, baseline hand function and CST connectivity pattern, and randomized to receive either CIMT or Bimanual training, each of which were provided in a day-camp setting (90 h). Hand function was tested before, immediately and 6 months after the intervention with the Jebsen-Taylor Test of Hand Function, the Assisting Hand Assessment, the Box and Block Test, and ABILHAND-Kids. The Canadian Occupational Performance Measure was used to track goal achievement and the Pediatric Evaluation of Disability Inventory was used to assess functioning in daily living activities at home. RESULTS: In contrast to our hypothesis, participants had statistically similar improvements for both CIMT and Bimanual training for all measures independent of their CST connectivity pattern (contralateral, ipsilateral, or bilateral) (p < 0.05 in all cases). CONCLUSIONS/SIGNIFICANCE: The efficacy of CIMT and Bimanual training is independent of CST connectivity pattern. Children with an ipsilateral CST, previously thought to be maladaptive, have the capacity to improve as well as children with a contralateral or bilateral CST following intensive CIMT or Bimanual training. Clinical Trial Registration: www.ClinicalTrials.gov, identifier NCT02918890."
RENATO MANCUSO,Deterministic memory hierarchy and virtualization for modern multi-core embedded systems,"One of the main predictability bottlenecks of modern multi-core embedded systems is contention for access to shared memory resources. Partitioning and software-driven allocation of memory resources is an effective strategy to mitigate contention in the memory hierarchy. Unfortunately, however, many of the strategies adopted so far can have unforeseen side-effects when practically implemented latest-generation, high-performance embedded platforms. Predictability is further jeopardized by cache eviction policies based on random replacement, targeting average performance instead of timing determinism. In this paper, we present a framework of software-based techniques to restore memory access determinism in high-performance embedded systems. Our approach leverages OS-transparent and DMA-friendly cache coloring, in combination with an invalidation-driven allocation (IDA) technique. The proposed method allows protecting important cache blocks from (i) external eviction by tasks concurrently executing on different cores, and (ii) internal eviction by tasks running on the same core. A working implementation obtained by extending the Jailhouse partitioning hypervisor is presented and evaluated with a combination of synthetic and real benchmarks."
RENATO MANCUSO,Restart-based fault-tolerance: system design and schedulability analysis,"Embedded systems in safety-critical environments are continuously required to deliver more performance and functionality, while expected to provide verified safety guarantees. Nonetheless, platform-wide software verification (required for safety) is often expensive. Therefore, design methods that enable utilization of components such as real-time operating systems (RTOS), without requiring their correctness to guarantee safety, is necessary. In this paper, we propose a design approach to deploy safe-by-design embedded systems. To attain this goal, we rely on a small core of verified software to handle faults in applications and RTOS and recover from them while ensuring that timing constraints of safety-critical tasks are always satisfied. Faults are detected by monitoring the application timing and fault-recovery is achieved via full platform restart and software reload, enabled by the short restart time of embedded systems. Schedulability analysis is used to ensure that the timing constraints of critical plant control tasks are always satisfied in spite of faults and consequent restarts. We derive schedulability results for four restart-tolerant task models. We use a simulator to evaluate and compare the performance of the considered scheduling models."
RENATO MANCUSO,Verification of OS-level cache management,"Recently, the complexity of safety-critical cyber-physical systems has spiked due to an increasing demand for performance, impacting both software and hardware layers. The timing behavior of complex systems, however, is harder to analyze. Real-time hardware resource management aims at mitigating this problem, but the proposed solutions often involve OS-level modifications. In this sense, software verification is key to build trust and allow such techniques to be broadly adopted. This paper specifically focuses on CPU cache management, demonstrating that OS-level hardware management logic can be verified at the source code level in a modular way, ie, without verifying the entire OS."
RENATO MANCUSO,Evaluating memory subsystem of configurable heterogeneous MPSoC,"This paper presents the evaluation of the memory subsystem of the Xilinx Ultrascale+ MPSoC. The characteristics of various memories in the system are evaluated using carefully instrumented micro-benchmarks. The impact of micro-architectural features like caches, prefetchers and cache coherency are measured and discussed. The impact of multi-core contention on shared memory resources is evaluated. Finally, proposals are made for the design of mixed-criticality real-time applications on this platform."
RENATO MANCUSO,Deterministic memory abstraction and supporting multicore system architecture,"Poor time predictability of multicore processors has been a long-standing challenge in the real-time systems community. In this paper, we make a case that a fundamental problem that prevents efficient and predictable real-time computing on multicore is the lack of a proper memory abstraction to express memory criticality, which cuts across various layers of the system: the application, OS, and hardware. We, therefore, propose a new holistic resource management approach driven by a new memory abstraction, which we call Deterministic Memory. The key characteristic of deterministic memory is that the platform-the OS and hardware-guarantees small and tightly bounded worst-case memory access timing. In contrast, we call the conventional memory abstraction as best-effort memory in which only highly pessimistic worst-case bounds can be achieved. We propose to utilize both abstractions to achieve high time predictability but without significantly sacrificing performance. We present deterministic memory-aware OS and architecture designs, including OS-level page allocator, hardware-level cache, and DRAM controller designs. We implement the proposed OS and architecture extensions on Linux and gem5 simulator. Our evaluation results, using a set of synthetic and real-world benchmarks, demonstrate the feasibility and effectiveness of our approach."
RENATO MANCUSO,Good actors can come in smaller sizes: a case study on the value of actor-critic asymmetry,
RENATO MANCUSO,Work in progress: identifying unexpected inter-core interference induced by shared cache,"In modern real-time multicore systems, understanding and adequately managing shared caches is essential to ensure the temporal isolation of critical tasks. Recent research has identified and extensively studied the sources of unpredictability imputable to shared caches, heavily promoting techniques such as cache partitioning and internal resources management. In this article, we highlight the existence of an enigmatic source of inter-core interference: the CPU-brainfreeze. Experiments realized on a development board show that benchmarks (selected from the San-Diego Vision Benchmark Suite) can exhibit up to a 10-fold increase in their execution time. The same experiment shows that for extreme cases, the core cluster can be stalled indefinitely."
RENATO MANCUSO,E-WarP: a system-wide framework for memory bandwidth profiling and management,"The proliferation of multi-core, accelerator-enabled embedded systems has introduced new opportunities to consolidate real-time systems of increasing complexity. But the road to build confidence on the temporal behavior of co-running applications has presented formidable challenges. Most prominently, the main memory subsystem represents a performance bottleneck for both CPUs and accelerators. And industry-viable frameworks for full-system main memory management and performance analysis are past due. In this paper, we propose our Envelope-aWare Predictive model, or E-WarP for short. E-WarP is a methodology and technological framework to: (1) analyze the memory demand of applications following a profile-driven approach; (2) make realistic predictions on the temporal behavior of workload deployed on CPUs and accelerators; and (3) perform saturation-aware system consolidation. This work aims at providing the technological foundations as well as the theoretical grassroots for truly workload-aware analysis of real-time systems. We provide a full implementation of our techniques on a commercial platform (NXP S32V234) and make two key observations. First, we achieve, on average, a 6% overprediction on the runtime of bandwidth-regulated applications. Second, we experimentally validate that the calculated bounds hold if the main memory subsystem operates below saturation."
RENATO MANCUSO,Reconciling predictability and coherent caching,"Real-time systems are required to respond to their physical environment within predictable time. While multi-core platforms provide incredible computational power and throughput, they also introduce new sources of unpredictability. For parallel applications with data shared across multiple cores, overhead to maintain data coherence is a major cause of execution time variability. This source of variability can be eliminated by application level control for limiting data caching at different levels of the cache hierarchy. This removes the requirement of explicit coherence machinery for selected data. We show that such control can reduce the worst case write request latency on shared data by 52%. Benchmark evaluations show that proposed technique has a minimal impact on average performance."
RENATO MANCUSO,On the interplay of computation and memory regulation in multicore real-time systems,"The ever-increasing demand for high performance in the time-critical embedded domain has pushed the adoption of powerful yet unpredictable heterogeneous Systems-on-a-Chip. The shared memory subsystem, which is known to be a major source of unpredictability, has been extensively studied, and many mitigation techniques have been proposed. Among them, performance-counter-based regulation techniques have seen widespread adoption. However, the problem of combining performance-based regulation with time-domain isolation has not received enough attention. In this article, we discuss our current work-in-progress on SHCReg (Software Hardware Co-design Regulator). First, we assess the limitations and benefits of combined CPU and memory budgeting. Next, we outline a full-stack hardware/software codesign architecture that aims at improving the interplay between CPU and memory isolation for mixed-criticality tasks running on the same core."
RENATO MANCUSO,Know your enemy: benchmarking and experimenting with insight as a goal,"Available benchmark suites are used to provide realistic workloads and to understand their run-time characteristics. However, they do not necessarily target the same platforms and often offer a diverse set of metrics, leading to the lack of a knowledge base that could be used for both systems and theoretical research. RT-Bench, a new benchmark framework environment, tries to address these issues by providing a uniform interface and metrics while maintaining portability. This demo illustrates how to leverage this framework and its recently added features to improve the understanding of the benchmarks’ interaction with its system."
RENATO MANCUSO,Relational memory: native in-memory accesses on rows and columns,"Analytical database systems are typically designed to use a column-first data layout to access only the desired fields. On the other hand, storing data row-first works great for accessing, inserting, or updating entire rows. Transforming rows to columns at runtime is expensive, hence, many analytical systems ingest data in row-first form and transform it in the background to columns to facilitate future analytical queries. How will this design change if we can always efficiently access only the desired set of columns? To address this question, we present a radically new approach to data transformation from rows to columns. We build upon recent advancements in embedded platforms with re-programmable logic to design native in-memory access on rows and columns. Our approach, termed Relational Memory, relies on an FPGA-based accelerator that sits between the CPU and main memory and transparently transforms base data to any group of columns with minimal overhead at runtime. This design allows accessing any group of columns as if it already exists in memory. We implement and deploy Relational Memory in real hardware, and we show that we can access the desired columns up to 1.63× faster than accessing them from their row-wise counterpart, while matching the performance of a pure columnar access for low projectivity, and outperforming it by up to 1.87× as projectivity (and tuple reconstruction cost) increases. Moreover, our approach can be easily extended to support offloading of a number of operations to hardware, e.g., selection, group by, aggregation, and joins, having the potential to vastly simplify the software logic and accelerate the query execution."
RENATO MANCUSO,WCET derivation under single core equivalence with explicit memory budget assignment,"In the last decade there has been a steady uptrend in the popularity of embedded multi-core platforms. This represents a turning point in the theory and implementation of real-time systems. From a real-time standpoint, however, the extensive sharing of hardware resources (e.g. caches, DRAM subsystem, I/O channels) represents a major source of unpredictability. Budget-based memory regulation (throttling) has been extensively studied to enforce a strict partitioning of the DRAM subsystem’s bandwidth. The common approach to analyze a task under memory bandwidth regulation is to consider the budget of the core where the task is executing, and assume the worst-case about the remaining cores' budgets. In this work, we propose a novel analysis strategy to derive the WCET of a task under memory bandwidth regulation that takes into account the exact distribution of memory budgets to cores. In this sense, the proposed analysis represents a generalization of approaches that consider (i) even budget distribution across cores; and (ii) uneven but unknown (except for the core under analysis) budget assignment. By exploiting the additional piece of information, we show that it is possible to derive a more accurate WCET estimation. Our evaluations highlight that the proposed technique can reduce overestimation by 30% in average, and up to 60%, compared to the state of the art."
RENATO MANCUSO,How to train your quadrotor: a framework for consistently smooth and responsive flight control via reinforcement learning,"We focus on the problem of reliably training Reinforcement Learning (RL) models (agents) for stable low-level control in embedded systems and test our methods on a high-performance, custom-built quadrotor platform. A common but often under-studied problem in developing RL agents for continuous control is that the control policies developed are not always smooth. This lack of smoothness can be a major problem when learning controllers as it can result in control instability and hardware failure. Issues of noisy control are further accentuated when training RL agents in simulation due to simulators ultimately being imperfect representations of reality — what is known as the reality gap. To combat issues of instability in RL agents, we propose a systematic framework, ‘REinforcement-based transferable Agents through Learning’ (RE+AL), for designing simulated training environments which preserve the quality of trained agents when transferred to real platforms. RE+AL is an evolution of the Neuroflight infrastructure detailed in technical reports prepared by members of our research group. Neuroflight is a state-of-the-art framework for training RL agents for low-level attitude control. RE+AL improves and completes Neuroflight by solving a number of important limitations that hindered the deployment of Neuroflight to real hardware. We benchmark RE+AL on the NF1 racing quadrotor developed as part of Neuroflight. We demonstrate that RE+AL significantly mitigates the previously observed issues of smoothness in RL agents. Additionally, RE+AL is shown to consistently train agents that are flight-capable and with minimal degradation in controller quality upon transfer. RE+AL agents also learn to perform better than a tuned PID controller, with better tracking errors, smoother control and reduced power consumption. To the best of our knowledge, RE+AL agents are the first RL-based controllers trained in simulation to outperform a well-tuned PID controller on a real-world controls problem that is solvable with classical control."
RENATO MANCUSO,Deterministic memory abstraction and supporting multicore system architecture,"Poor timing predictability of multicore processors has been a long-standing challenge in the real-time systems community. In this paper, we make a case that a fundamental problem that prevents efficient and predictable real-time computing on multicore is the lack of a proper memory abstraction to express memory criticality, which cuts across various layers of the system: the application, OS, and hardware. We, therefore, propose a new holistic resource management approach driven by a new memory abstraction, which we call Deterministic Memory. The key characteristic of deterministic memory is that the platform - the OS and hardware - guarantees small and tightly bounded worst-case memory access timing. In contrast, we call the conventional memory abstraction as best-effort memory in which only highly pessimistic worst-case bounds can be achieved. We propose to utilize both abstractions to achieve high time predictability but without significantly sacrificing performance. We present deterministic memory-aware OS and architecture designs, including OS-level page allocator, hardware-level cache and DRAM controller designs. We implement the proposed OS and architecture extensions on Linux and gem5 simulator. Our evaluation results, using a set of synthetic and real-world benchmarks, demonstrate the feasibility and effectiveness of our approach."
RENATO MANCUSO,Regularizing action policies for smooth control with reinforcement learning,"A critical problem with the practical utility of controllers trained with deep Reinforcement Learning (RL) is the notable lack of smoothness in the actions learned by the RL policies. This trend often presents itself in the form of control signal oscillation and can result in poor control, high power consumption, and undue system wear. We introduce Conditioning for Action Policy Smoothness (CAPS), an effective yet intuitive regularization on action policies, which offers consistent improvement in the smoothness of the learned state-toaction mappings of neural network controllers, reflected in the elimination of high-frequency components in the control signal. Tested on a real system, improvements in controller smoothness on a quadrotor drone resulted in an almost 80% reduction in power consumption while consistently training flight-worthy controllers."
RENATO MANCUSO,Unikernels: the next stage of Linux’s dominance,"Unikernels have demonstrated enormous advantages over Linux in many important domains, causing some to propose that the days of Linux's dominance may be coming to an end. On the contrary, we believe that unikernels' advantages represent the next natural evolution for Linux, as it can adopt the best ideas from the unikernel approach and, along with its battle-tested codebase and large open source community, continue to dominate. In this paper, we posit that an upstreamable unikernel target is achievable from the Linux kernel, and, through an early Linux unikernel prototype, demonstrate that some simple changes can bring dramatic performance advantages."
RENATO MANCUSO,Analysis of dynamic memory bandwidth regulation in multi-core real-time systems,"One of the primary sources of unpredictability in modern multi-core embedded systems is contention over shared memory resources, such as caches, interconnects, and DRAM. Despite significant achievements in the design and analysis of multi-core systems, there is a need for a theoretical framework that can be used to reason on the worst-case behavior of real-time workload when both processors and memory resources are subject to scheduling decisions. In this paper, we focus our attention on dynamic allocation of main memory bandwidth. In particular, we study how to determine the worst-case response time of tasks spanning through a sequence of time intervals, each with a different bandwidth-to-core assignment. We show that the response time computation can be reduced to a maximization problem over assignment of memory requests to different time intervals, and we provide an efficient way to solve such problem. As a case study, we then demonstrate how our proposed analysis can be used to improve the schedulability of Integrated Modular Avionics systems in the presence of memory-intensive workload."
RENATO MANCUSO,Governing with insights: towards profile-driven cache management of Black-Box applications,"There exists a divide between the ever-increasing demand for high-performance embedded systems and the availability of practical methodologies to understand the interplay of complex data-intensive applications with hardware memory resources. On the one hand, traditional static analysis approaches are seldomly applicable to latest-generation multi-core platforms due to a lack of accurate micro-architectural models. On the other hand, measurement-based methods only provide coarse-grained information about the end-to-end execution of a given real-time application. In this paper, we describe a novel methodology, namely Black-Box Profiling (BBProf), to gather fine-grained insights on the usage of cache resources in applications of realistic complexity. The goal of our technique is to extract the relative importance of individual memory pages towards the overall temporal behavior of a target application. Importantly, BBProf does not require the semantics of the target application to be known - i.e., applications are treated as black-boxes - and it does not rely on any platform-specific hardware support. We provide an open-source full-system implementation and showcase how BBProf can be used to perform profile-driven cache management."
RENATO MANCUSO,A Memory Scheduling Infrastructure for Multi-Core Systems with Re-Programmable Logic,"The sharp increase in demand for performance has prompted an explosion in the complexity of modern multi-core embedded systems. This has led to unprecedented temporal unpredictability concerns in Cyber-Physical Systems (CPS). On-chip integration of programmable logic (PL) alongside a conventional Processing System (PS) in modern Systems-on-Chip (SoC) establishes a genuine compromise between specialization, performance, and reconfigurability. In addition to typical use-cases, it has been shown that the PL can be used to observe, manipulate, and ultimately manage memory traffic generated by a traditional multi-core processor. This paper explores the possibility of PL-aided memory scheduling by proposing a Scheduler In-the-Middle (SchIM). We demonstrate that the SchIM enables transaction-level control over the main memory traffic generated by a set of embedded cores. Focusing on extensibility and reconfigurability, we put forward a SchIM design covering two main objectives. First, to provide a safe playground to test innovative memory scheduling mechanisms; and second, to establish a transition path from software-based memory regulation to provably correct hardware-enforced memory scheduling. We evaluate our design through a full-system implementation on a commercial PS-PL platform using synthetic and real-world benchmarks."
RENATO MANCUSO,A real-time virtio-based framework for predictable inter-VM communication,"Ensuring real-time properties on current heterogeneous multiprocessor systems on a chip is a challenging task. Furthermore, online artificial intelligent applications –which are routinely deployed on such chips– pose increasing pressure on the memory subsystem that becomes a source of unpredictability. Although techniques have been proposed to restore independent access to memory for concurrently executing virtual machines (VM), providing predictable inter-VM communication remains challenging. In this work, we tackle the problem of predictably transferring data between virtual machines and virtualized hardware resources on multiprocessor systems on chips under consideration of memory interference. We design a ""broker-based"" real-time communication framework for otherwise isolated virtual machines, provide a virtio-based reference implementation on top of the Jailhouse hypervisor, assess its overheads for FreeRTOS virtual machines, and formally analyze its communication flow schedulability under consideration of the implementation overheads. Furthermore, we define a methodology to assess the maximum DRAM memory saturation empirically, evaluate the framework's performance and compare it with the theoretical schedulability."
RENATO MANCUSO,Observing the invisible: live cache inspection for high-performance embedded systems,"The vast majority of high-performance embedded systems implement multi-level CPU cache hierarchies. But the exact behavior of these CPU caches has historically been opaque to system designers. Absent expensive hardware debuggers, an understanding of cache makeup remains tenuous at best. This enduring opacity further obscures the complex interplay among applications and OS-level components, particularly as they compete for the allocation of cache resources. Notwithstanding the relegation of cache comprehension to proxies such as static cache analysis, performance counter-based profiling, and cache hierarchy simulations, the underpinnings of cache structure and evolution continue to elude software-centric solutions. In this article, we explore a novel method of studying cache contents and their evolution via snapshotting. Our method complements extant approaches for cache profiling to better formulate, validate, and refine hypotheses on the behavior of modern caches. We leverage cache introspection interfaces provided by vendors to perform live cache inspections without the need for external hardware. We present CacheFlow, a proof-of-concept Linux kernel module which snapshots cache contents on an NVIDIA Tegra TX1 system on chip and a Hardkernel Odroid XU4."
RENATO MANCUSO,CAESAR: coherence-aided elective and seamless alternative routing via on-chip FPGA,"Prompted by the ever-growing demand for high-performance System-on-Chip (SoC) and the plateauing of CPU frequencies, the SoC design landscape is shifting. In a quest to offer programmable specialization, the adoption of tightly-coupled FPGAs co-located with traditional compute clusters has been embraced by major vendors. This CPU+FPGA architectural paradigm opens the door to novel hardware/software co-design opportunities. The key principle is that CPU-originated memory traffic can be re-routed through the FPGA for analysis and management purposes. Albeit promising, the side-effect of this approach is that time-critical operations—such as cache-line refills—are fulfilled by moving data over slower interconnects meant for I/O traffic. In this article, we introduce a novel principle named Cache Coherence Backstabbing to precisely tackle these shortcomings. The technique leverages the ability to include the FGPA in the same coherence domain as the core processing elements. Importantly, this enables Coherence-Aided Elective and Seamless Alternative Routing (CAESAR), i.e., seamless inspection and routing of memory transactions, especially cache-line refills, through the FPGA. CAESAR allows the definition of new memory programming paradigms. We discuss the intrinsic potentials of the approach and evaluate it with a full-stack prototype implementation on a commercial platform. Our experiments show an improvement of up to 29% in read bandwidth, 23% in latency, and 13% in pragmatic workloads over the state of the art. Furthermore, we showcase the first in-coherence-domain run-time profiler design as a use-case of the CAESAR approach."
RENATO MANCUSO,Hardware data re-organization engine for real-time systems,"Access patterns and cache utilization play a key role in the analyzability of data-intensive applications. In this demo, we re-examine our previous research on software-hardware codesign to push data transformation closer to memory from a real-time perspective. Deployed in modern CPU+FPGA systems, our design enables efficient and cache-friendly access to large data by only moving relevant bytes from the target memory. This (1) compresses the cache footprint and (2) reorganizes complex memory access patterns into sequential and predictable patterns."
RENATO MANCUSO,Designing mixed criticality applications on modern heterogeneous MPSoC platforms,"Multiprocessor Systems-on-Chip (MPSoC) integrating hard processing cores with programmable logic (PL) are becoming increasingly common. While these platforms have been originally designed for high performance computing applications, their rich feature set can be exploited to efficiently implement mixed criticality domains serving both critical hard real-time tasks, as well as soft real-time tasks. In this paper, we take a deep look at commercially available heterogeneous MPSoCs that incorporate PL and a multicore processor. We show how one can tailor these processors to support a mixed criticality system, where cores are strictly isolated to avoid contention on shared resources such as Last-Level Cache (LLC) and main memory. In order to avoid conflicts in last-level cache, we propose the use of cache coloring, implemented in the Jailhouse hypervisor. In addition, we employ ScratchPad Memory (SPM) inside the PL to support a multi-phase execution model for real-time tasks that avoids conflicts in shared memory. We provide a full-stack, working implementation on a latest-generation MPSoC platform, and show results based on both a set of data intensive tasks, as well as a case study based on an image processing benchmark application."
RENATO MANCUSO,Impact of DM-LRU on WCET: a static analysis approach,"Cache memories in modern embedded processors are known to improve average memory access performance. Unfortunately, they are also known to represent a major source of unpredictability for hard real-time workload. One of the main limitations of typical caches is that content selection and replacement is entirely performed in hardware. As such, it is hard to control the cache behavior in software to favor caching of blocks that are known to have an impact on an application's worst-case execution time (WCET). In this paper, we consider a cache replacement policy, namely DM-LRU, that allows system designers to prioritize caching of memory blocks that are known to have an important impact on an application's WCET. Considering a single-core, single-level cache hierarchy, we describe an abstract interpretation-based timing analysis for DM-LRU. We implement the proposed analysis in a self-contained toolkit and study its qualitative properties on a set of representative benchmarks. Apart from being useful to compute the WCET when DM-LRU or similar policies are used, the proposed analysis can allow designers to perform WCET impact-aware selection of content to be retained in cache."
RENATO MANCUSO,Neuroflight: next generation flight control firmware,"Little innovation has been made to low-level attitude flight control used by unmanned aerial vehicles, which still predominantly uses the classical PID controller. In this work we introduce Neuroflight, the first open source neuro-flight controller firmware. We present our toolchain for training a neural network in simulation and compiling it to run on embedded hardware. Challenges faced jumping from simulation to reality are discussed along with our solutions. Our evaluation shows the neural network can execute at over 2.67kHz on an Arm Cortex-M7 processor and flight tests demonstrate a quadcopter running Neuroflight can achieve stable flight and execute aerobatic maneuvers."
RENATO MANCUSO,The potential of programmable logic in the middle: cache bleaching,"Consolidating hard real-time systems onto modern multi-core Systems-on-Chip (SoC) is an open challenge. The extensive sharing of hardware resources at the memory hierarchy raises important unpredictability concerns. The problem is exacerbated as more computationally demanding workload is expected to be handled with real-time guarantees in next-generation Cyber-Physical Systems (CPS). A large body of works has approached the problem by proposing novel hardware re-designs, and by proposing software-only solutions to mitigate performance interference. Strong from the observation that unpredictability arises from a lack of fine-grained control over the behavior of shared hardware components, we outline a promising new resource management approach. We demonstrate that it is possible to introduce Programmable Logic In-the-Middle (PLIM) between a traditional multi-core processor and main memory. This provides the unique capability of manipulating individual memory transactions. We propose a proof-of-concept system implementation of PLIM modules on a commercial multi-core SoC. The PLIM approach is then leveraged to solve long-standing issues with cache coloring. Thanks to PLIM, colored sparse addresses can be re-compacted in main memory. This is the base principle behind the technique we call Cache Bleaching. We evaluate our design on real applications and propose hypervisor-level adaptations to showcase the potential of the PLIM approach."
RENATO MANCUSO,Reinforcement learning for UAV attitude control,"Autopilot systems are typically composed of an “inner loop” providing stability and control, whereas an “outer loop” is responsible for mission-level objectives, such as way-point navigation. Autopilot systems for unmanned aerial vehicles are predominately implemented using Proportional-Integral-Derivative (PID) control systems, which have demonstrated exceptional performance in stable environments. However, more sophisticated control is required to operate in unpredictable and harsh environments. Intelligent flight control systems is an active area of research addressing limitations of PID control most recently through the use of reinforcement learning (RL), which has had success in other applications, such as robotics. Yet previous work has focused primarily on using RL at the mission-level controller. In this work, we investigate the performance and accuracy of the inner control loop providing attitude control when using intelligent flight control systems trained with state-of-the-art RL algorithms—Deep Deterministic Policy Gradient, Trust Region Policy Optimization, and Proximal Policy Optimization. To investigate these unknowns, we first developed an open source high-fidelity simulation environment to train a flight controller attitude control of a quadrotor through RL. We then used our environment to compare their performance to that of a PID controller to identify if using RL is appropriate in high-precision, time-critical flight control."
RENATO MANCUSO,Low-overhead online assessment of timely progress as a system commodity,"The correctness of safety-critical systems depends on both their logical and temporal behavior. Control-flow integrity (CFI) is a well-established and understood technique to safeguard the logical flow of safety-critical applications. But unfortunately, no established methodologies exist for the complementary problem of detecting violations of control flow timeliness. Worse yet, the latter dimension, which we term Timely Progress Integrity (TPI), is increasingly more jeopardized as the complexity of our embedded systems continues to soar. As key resources of the memory hierarchy become shared by several CPUs and accelerators, they become hard-to-analyze performance bottlenecks. And the precise interplay between software and hardware components becomes hard to predict and reason about. How to restore control over timely progress integrity? We postulate that the first stepping stone toward TPI is to develop methodologies for Timely Progress Assessment (TPA). TPA refers to the ability of a system to live-monitor the positive/negative slack - with respect to a known reference - at key milestones throughout an application’s lifespan. In this paper, we propose one such methodology that goes under the name of Milestone-Based Timely Progress Assessment or MB-TPA, for short. Among the key design principles of MB-TPA is the ability to operate on black-box binary executables with near-zero time overhead and implementable on commercial platforms. To prove its feasibility and effectiveness, we propose and evaluate a full-stack implementation called Timely Progress Assessment with 0 Overhead (TPAw0v). We demonstrate its capability in providing live TPA for complex vision applications while introducing less than 0.6% time overhead for applications under test. Finally, we demonstrate one use case where TPA information is used to restore TPI in the presence of temporal interference over shared memory resources."
RENATO MANCUSO,Memory latency distribution-driven regulation for temporal isolation in MPSoCs,"Temporal isolation is one of the most significant challenges that must be addressed before Multi-Processor Systems-on-Chip (MPSoCs) can be widely adopted in mixed-criticality systems with both time-sensitive real-time (RT) applications and performance-oriented non-real-time (NRT) applications. Specifically, the main memory subsystem is one of the most prevalent causes of interference, performance degradation and loss of isolation. Existing memory bandwidth regulation mechanisms use static, dynamic, or predictive DRAM bandwidth management techniques to restore the execution time of an application under contention as close as possible to the execution time in isolation. In this paper, we propose a novel distribution-driven regulation whose goal is to achieve a timeliness objective formulated as a constraint on the probability of meeting a certain target execution time for the RT applications. Using existing interconnect-level Performance Monitoring Units (PMU), we can observe the Cumulative Distribution Function (CDF) of the per-request memory latency. Regulation is then triggered to enforce first-order stochastical dominance with respect to a desired reference. Consequently, it is possible to enforce that the overall observed execution time random variable is dominated by the reference execution time. The mechanism requires no prior information of the contending application and treats the DRAM subsystem as a black box. We provide a full-stack implementation of our mechanism on a Commercial Off-The-Shelf (COTS) platform (Xilinx Ultrascale+ MPSoC), evaluate it using real and synthetic benchmarks, experimentally validate that the timeliness objectives are met for the RT applications, and demonstrate that it is able to provide 2.2x more overall throughput for NRT applications compared to DRAM bandwidth management-based regulation approaches."
RENATO MANCUSO,MemPol: policing core memory bandwidth from outside of the cores,
LISA SHEN,Small molecule inhibitors of Late SV40 Factor (LSF) abrogate hepatocellular carcinoma (HCC): evaluation using an endogenous HCC model,"Hepatocellular carcinoma (HCC) is a lethal malignancy with high mortality and poor prognosis. Oncogenic transcription factor Late SV40 Factor (LSF) plays an important role in promoting HCC. A small molecule inhibitor of LSF, Factor Quinolinone Inhibitor 1 (FQI1), significantly inhibited human HCC xenografts in nude mice without harming normal cells. Here we evaluated the efficacy of FQI1 and another inhibitor, FQI2, in inhibiting endogenous hepatocarcinogenesis. HCC was induced in a transgenic mouse with hepatocyte-specific overexpression of c-myc (Alb/c-myc) by injecting N-nitrosodiethylamine (DEN) followed by FQI1 or FQI2 treatment after tumor development. LSF inhibitors markedly decreased tumor burden in Alb/c-myc mice with a corresponding decrease in proliferation and angiogenesis. Interestingly, in vitro treatment of human HCC cells with LSF inhibitors resulted in mitotic arrest with an accompanying increase in CyclinB1. Inhibition of CyclinB1 induction by Cycloheximide or CDK1 activity by Roscovitine significantly prevented FQI-induced mitotic arrest. A significant induction of apoptosis was also observed upon treatment with FQI. These effects of LSF inhibition, mitotic arrest and induction of apoptosis by FQI1s provide multiple avenues by which these inhibitors eliminate HCC cells. LSF inhibitors might be highly potent and effective therapeutics for HCC either alone or in combination with currently existing therapies."
LISA SHEN,Proceedings of the Sixth International Workshop on Web Caching and Content Distribution,"OVERVIEW: The International Web Content Caching and Distribution Workshop (WCW) is a premiere technical meeting for researchers and practitioners interested in all aspects of content caching, distribution and delivery on the Internet. The 2001 WCW meeting was held on the Boston University Campus. Building on the successes of the five previous WCW meetings, WCW01 featured a strong technical program and record participation from leading researchers and practitioners in the field. This report includes all the technical papers presented at WCW'01. NOTE: Proceedings of WCW'01 are published by Elsevier. Hard copies of these proceedings can be purchased through the workshop organizers. As a service to the community, electronic copies of all WCW'01 papers are accessible through Technical Report BUCS‐TR‐2001‐017, available from the Boston University Computer Science Technical Report Archives at http://www.cs.bu.edu/techreps. [Ed.note: URL outdated. Use http://www.bu.edu/cs/research/technical-reports or http://hdl.handle.net/2144/1455 in this repository to access the reports.]"
NAOMI CASELLI,American sign language interpreters in public schools: an illusion of inclusion that perpetuates language deprivation,"PURPOSE: Many deaf children have limited access to language, spoken or signed, during early childhood – which has damaging effects on many aspects of development. There has been a recent shift to consider deafness and language deprivation as separate but related conditions. As such, educational plans should differentiate between services related to deafness and services related to language deprivation. DESCRIPTION: Many deaf children attend mainstream public schools, and the primary service offered to students who use American Sign Language (ASL) is generally a sign language interpreter. ASSESSMENT: We argue that while sign language interpreters can be an effective accommodation for deafness (i.e., students who are deaf and not language-deprived), there is no reason to believe they are an effective accommodation for language deprivation (i.e., students who are deaf and language-deprived). CONCLUSION: Using interpreters instead of appropriate educational supports may exacerbate symptoms of language deprivation by prolonging the period of time a child goes with limited access to language."
NAOMI CASELLI,"The road to language learning is not entirely iconic: Iconicity, neighborhood density, and frequency facilitate sign language acquisition","Iconic mappings between words and their meanings are far more prevalent than once estimated, and seem to support children’s acquisition of new words, spoken or signed. We asked whether iconicity’s prevalence in sign language overshadows other factors known to support spoken vocabulary development, including neighborhood density (the number of lexical items phonologically similar to the target), and lexical frequency. Using mixed-effects logistic regressions, we reanalyzed 58 parental reports of native-signing deaf children’s American Sign Language (ASL) productive acquisition of 332 signs (Anderson & Reilly, 2002), and found that iconicity, neighborhood density, and lexical frequency independently facilitated vocabulary acquisition. Despite differences in iconicity and phonological structure, signing children, like children learning a spoken language, track statistical information about lexical items and their phonological properties and leverage them to expand their vocabulary."
NAOMI CASELLI,"The ASL-CDI 2.0: an updated, normed adaptation of the MacArthur Bates Communicative Development Inventory for American Sign Language","Vocabulary is a critical early marker of language development. The MacArthur Bates Communicative Development Inventory has been adapted to dozens of languages, and provides a bird’s-eye view of children’s early vocabularies which can be informative for both research and clinical purposes. We present an update to the American Sign Language Communicative Development Inventory (the ASL-CDI 2.0, https://www.aslcdi.org), a normed assessment of early ASL vocabulary that can be widely administered online by individuals with no formal training in sign language linguistics. The ASL-CDI 2.0 includes receptive and expressive vocabulary, and a Gestures and Phrases section; it also introduces an online interface that presents ASL signs as videos. We validated the ASL-CDI 2.0 with expressive and receptive in-person tasks administered to a subset of participants. The norming sample presented here consists of 120 deaf children (ages 9 to 73 months) with deaf parents. We present an analysis of the measurement properties of the ASL-CDI 2.0. Vocabulary increases with age, as expected. We see an early noun bias that shifts with age, and a lag between receptive and expressive vocabulary. We present these findings with indications for how the ASL-CDI 2.0 may be used in a range of clinical and research settings"
NAOMI CASELLI,The development and evaluation of a new ASL text comprehension task,"Being able to comprehend a language entails not only mastery of its syntax, lexicon, or phonology, but also the ability to use language to construct meaning, draw inferences, and make connections to world knowledge. However, most available assessments of American Sign Language (ASL) focus on mastery of lower level skills, and as a result little is known about development of higher-order ASL comprehension skills. In this paper, we introduce the American Sign Language Text Comprehension Task (ASL-CMP), a new assessment tool to measure ASL text comprehension ability in deaf children. We first administered the task to a group of deaf children with deaf parents (n = 105, ages 8–18 years) in order to evaluate the reliability and validity of the task, and to develop norms. We found that the ASL-CMP has acceptable levels of internal consistency, difficulty, and discriminability. Next, we administered the task to an additional group of deaf children with hearing parents (n = 251, ages 8–18 years), and found that the ASL-CMP is sensitive to expected patterns: older children have better ASL text comprehension skills, literal questions are generally easier to answer than inferential questions, and children with early exposure to ASL generally outperform those with delayed exposure. We conclude that the ASL-CMP task is reliable and valid and can be used to characterize ASL text comprehension skills in deaf children."
NAOMI CASELLI,Operationalization and measurement of sign language,
NAOMI CASELLI,From “communication options” to global language proficiency,
NAOMI CASELLI,Degree and not type of iconicity affects sign language vocabulary acquisition,"Lexical iconicity—signs or words that resemble their meaning—is over-represented in children’s early vocabularies. Embodied theories of language acquisition predict that symbols are more learnable when they are grounded in a child’s first-hand experiences. As such, pantomimic iconic signs, which use the signer’s body to represent a body, might be more readily learned than other types of iconic signs. Alternatively, the Structure Mapping Theory of iconicity predicts that learners are sensitive to the amount of overlap between form and meaning. In this exploratory study of early vocabulary development in ASL, we asked whether type of iconicity predicts sign acquisition above and beyond degree of iconicity. We also controlled for concreteness and relevance to babies, two possible confounding factors. Highly concrete referents and concepts that are germane to babies may be amenable to iconic mappings. We re-analyzed a previously published set of ASL CDI reports from 58 deaf children learning ASL from their deaf parents (Anderson & Reilly, 2002). Pantomimic signs were more iconic than other types of iconic signs (perceptual, both pantomimic and perceptual, or arbitrary), but type of iconicity had no effect on acquisition. Children may not make use of the special status of pantomimic elements of signs. Their vocabularies are, however, shaped by degree of iconicity, which aligns with a Structure Mapping Theory of iconicity, though other explanations are also compatible (e.g., iconicity in child-directed signing). Previously demonstrated effects of type of iconicity may be an artifact of the increased degree of iconicity among pantomimic signs."
NAOMI CASELLI,"Deaf children need language, not (just) speech","Deaf and Hard of Hearing (DHH) children need to master at least one language (spoken or signed) to reach their full potential. Providing access to a natural sign language supports this goal. Despite evidence that natural sign languages are beneficial to DHH children, many researchers and practitioners advise families to focus exclusively on spoken language. We critique the Pediatrics article ‘Early Sign Language Exposure and Cochlear Implants’ (Geers et al., 2017) as an example of research that makes unsupported claims against the inclusion of natural sign languages. We refute claims that (1) there are harmful effects of sign language and (2) that listening and spoken language are necessary for optimal development of deaf children. While practical challenges remain (and are discussed) for providing a sign language-rich environment, research evidence suggests that such challenges are worth tackling in light of natural sign languages providing a host of benefits for DHH children – especially in the prevention and reduction of language deprivation."
NAOMI CASELLI,The Sem-Lex Benchmark: modeling signs and their phonemes,
NAOMI CASELLI,Learning a sign language does not hinder acquisition of a spoken language,"PURPOSE: The purpose of this study is to determine whether and how learning American Sign Language (ASL) is associated with spoken English skills in a sample of ASL-English bilingual deaf and hard of hearing (DHH) children. METHOD: This cross-sectional study of vocabulary size included 56 DHH children between 8 and 60 months of age who were learning both ASL and spoken English and had hearing parents. English and ASL vocabulary were independently assessed via parent report checklists. RESULTS: ASL vocabulary size positively correlated with spoken English vocabulary size. Spoken English vocabulary sizes in the ASL-English bilingual DHH children in the present sample were comparable to those in previous reports of monolingual DHH children who were learning only English. ASL-English bilingual DHH children had total vocabularies (combining ASL and English) that were equivalent to same-age hearing monolingual children. Children with large ASL vocabularies were more likely to have spoken English vocabularies in the average range based on norms for hearing monolingual children. CONCLUSIONS: Contrary to predictions often cited in the literature, acquisition of sign language does not harm spoken vocabulary acquisition. This retrospective, correlational study cannot determine whether there is a causal relationship between sign language and spoken language vocabulary acquisition, but if a causal relationship exists, the evidence here suggests that the effect would be positive. Bilingual DHH children have age-expected vocabularies when considering the entirety of their language skills. We found no evidence to support recommendations that families with DHH children avoid learning sign language. Rather, our findings show that children with early ASL exposure can develop age-appropriate vocabulary skills in both ASL and spoken English."
ANITA SAVO,History and story in Amazon's El Cid (2020),
ANITA SAVO,"Misogyny, magic, and more in Juan Manuel’s Book of Count Lucanor",
J. WILLIAM BOLEY,3D printing of liquid crystal elastomeric actuators with spatially programed nematic order,"Liquid crystal elastomers (LCEs) are soft materials capable of large, reversible shape changes, which may find potential application as artificial muscles, soft robots, and dynamic functional architectures. Here, the design and additive manufacturing of LCE actuators (LCEAs) with spatially programed nematic order that exhibit large, reversible, and repeatable contraction with high specific work capacity are reported. First, a photopolymerizable, solvent-free, main-chain LCE ink is created via aza-Michael addition with the appropriate viscoelastic properties for 3D printing. Next, high operating temperature direct ink writing of LCE inks is used to align their mesogen domains along the direction of the print path. To demonstrate the power of this additive manufacturing approach, shape-morphing LCEA architectures are fabricated, which undergo reversible planar-to-3D and 3D-to-3D′ transformations on demand, that can lift significantly more weight than other LCEAs reported to date."
J. WILLIAM BOLEY,High-operating-temperature direct ink writing of mesoscale eutectic architectures,"High-operating-temperature direct ink writing (HOT-DIW) of mesoscale architectures that are composed of eutectic silver chloride–potassium chloride. The molten ink undergoes directional solidification upon printing on a cold substrate. The lamellar spacing of the printed features can be varied between approximately 100 nm and 2 µm, enabling the manipulation of light in the visible and infrared range."
HUI A. CHEN,Improving the characterization of ex vivo human brain optical properties using high numerical aperture optical coherence tomography by spatially constraining the confocal parameters,"SIGNIFICANCE: The optical properties of biological samples provide information about the structural characteristics of the tissue and any changes arising from pathological conditions. Optical coherence tomography (OCT) has proven to be capable of extracting tissue's optical properties using a model that combines the exponential decay due to tissue scattering and the axial point spread function that arises from the confocal nature of the detection system, particularly for higher numerical aperture (NA) measurements. A weakness in estimating the optical properties is the inter-parameter cross-talk between tissue scattering and the confocal parameters defined by the Rayleigh range and the focus depth. AIM: In this study, we develop a systematic method to improve the characterization of optical properties with high-NA OCT. APPROACH: We developed a method that spatially parameterizes the confocal parameters in a previously established model for estimating the optical properties from the depth profiles of high-NA OCT. RESULTS: The proposed parametrization model was first evaluated on a set of intralipid phantoms and then validated using a low-NA objective in which cross-talk from the confocal parameters is negligible. We then utilize our spatially parameterized model to characterize optical property changes introduced by a tissue index matching process using a simple immersion agent, 2,2'-thiodiethonal. CONCLUSIONS: Our approach improves the confidence of parameter estimation by reducing the degrees of freedom in the non-linear fitting model."
DILLON BROUT,The impact of dust on Cepheid and Type Ia supernova distances,"Milky-Way and intergalactic dust extinction and reddening must be accounted for in measurements of distances throughout the universe. This work provides a comprehensive review of the various impacts of cosmic dust focusing specifically on its effects on two key distance indicators used in the distance ladder: Cepheid variable stars and Type Ia supernovae. We review the formalism used for computing and accounting for dust extinction and reddening as a function of wavelength. We also detail the current state of the art knowledge of dust properties in the Milky Way and in host galaxies. We discuss how dust has been accounted for in both the Cepheid and SN distance measurements. Finally, we show how current uncertainties on dust modeling impact the inferred luminosities and distances, but that measurements of the Hubble constant remain robust to these uncertainties."
JACOB BROWN,"Canvass: a crowd-sourced, natural-product screening library for exploring biological space",
JACOB BROWN,Childhood cross-ethnic exposure predicts political behavior seven decades later: evidence from linked administrative data,"Does contact across social groups influence sociopolitical behavior? This question is among the most studied in the social sciences with deep implications for the harmony of diverse societies. Yet, despite a voluminous body of scholarship, evidence around this question is limited to cross-sectional surveys that only measure short-term consequences of contact or to panel surveys with small samples covering short time periods. Using advances in machine learning that enable large-scale linkages across datasets, we examine the long-term determinants of sociopolitical behavior through an unprecedented individual-level analysis linking contemporary political records to the 1940 U.S. Census. These linked data allow us to measure the exact residential context of nearly every person in the United States in 1940 and, for men, connect this with the political behavior of those still alive over 70 years later. We find that, among white Americans, early-life exposure to black neighbors predicts Democratic partisanship over 70 years later."
JACOB BROWN,Priorities for synthesis research in ecology and environmental science,
JACOB BROWN,What is missing in autonomous discovery: open challenges for the community,"Self-driving labs (SDLs) leverage combinations of artificial intelligence, automation, and advanced computing to accelerate scientific discovery."
NICHOLAS ROCK,"Design, food and human connection: a research project with the intent of understanding the parallels between chefs and designers",
STEPHANIE CHARLES,"Chiasma: June 1970 v. 1, no. 4",
DEEPA GOPAL,A systematic review on the biochemical threshold of mitochondrial genetic variants,"Mitochondrial DNA (mtDNA) variants cause a range of diseases from severe pediatric syndromes to aging-related conditions. The percentage of mtDNA copies carrying a pathogenic variant, variant allele frequency (VAF), must reach a threshold before a biochemical defect occurs, termed the biochemical threshold. Whether the often-cited biochemical threshold of >60% VAF is similar across mtDNA variants and cell types is unclear. In our systematic review, we sought to identify the biochemical threshold of mtDNA variants in relation to VAF by human tissue/cell type. We used controlled vocabulary terms to identify articles measuring oxidative phosphorylation (OXPHOS) complex activities in relation to VAF. We identified 76 eligible publications, describing 69, 12, 16, and 49 cases for complexes I, III, IV, and V, respectively. Few studies evaluated OXPHOS activities in diverse tissue types, likely reflective of clinical access. A number of cases with similar VAFs for the same pathogenic variant had varying degrees of residual activity of the affected complex, alluding to the presence of modifying variants. Tissues and cells with VAFs <60% associated with low complex activities were described, suggesting the possibility of a biochemical threshold of <60%. Using Kendall rank correlation tests, the VAF of the m.8993T > G variant correlated with complex V activity in skeletal muscle (τ = -0.58, P = 0.01, n = 13); however, no correlation was observed in fibroblasts (P = 0.7, n = 9). Our systematic review highlights the need to investigate the biochemical threshold over a wider range of VAFs in disease-relevant cell types to better define the biochemical threshold for specific mtDNA variants."
DAVID C HARRISON,"Cosmology intertwined: a review of the particle physics, astrophysics, and cosmology associated with the cosmological tensions and anomalies",
AUDREY LI,Autism screening and diagnosis in low resource settings: Challenges and opportunities to enhance research and services worldwide.,"Most research into the epidemiology, etiology, clinical manifestations, diagnosis and treatment of autism is based on studies in high income countries. Moreover, within high income countries, individuals of high socioeconomic status are disproportionately represented among participants in autism research. Corresponding disparities in access to autism screening, diagnosis, and treatment exist globally. One of the barriers perpetuating this imbalance is the high cost of proprietary tools for diagnosing autism and for delivering evidence-based therapies. Another barrier is the high cost of training of professionals and para-professionals to use the tools. Open-source and open access models provide a way to facilitate global collaboration and training. Using these models and technologies, the autism scientific community and clinicians worldwide should be able to work more effectively and efficiently than they have to date to address the global imbalance in autism knowledge and at the same time advance our understanding of autism and our ability to deliver cost-effective services to everyone in need."
THAMARAH CREVECOEUR,"Developing culturally specific, patient-centered maternity care models for high-risk immigrant populations: recommendations from a study of the Haitian population at Boston Medical Center","BACKGROUND: Haitian women in Massachusetts experience higher than average rates of low birth weight, C-section and inadequate prenatal care. Given the disparities in Black maternal health in the U.S., creative and innovative care models are necessary to improve outcomes. Culturally specific care models have been found to improve satisfaction and uptake of prenatal care among immigrants. A mixed methods case study was conducted at Boston Medical Center with the aims of describing maternal health outcomes of Haitian women, understanding Haitian women’s experiences and barriers to care and the feasibility of culturally specific maternity care models. METHODS: Electronic medical records were used to obtain and analyze retrospective data about patients’ socio-economic factors, baseline health maternity characteristics, maternity care utilization characteristics, perinatal complications and obstetrical outcomes stratified by ethnicity from 2015-2019. Chi square analysis was performed to measure statistical significance. Four focus groups and three in-depth interviews were performed with Haitian pregnant and postpartum women (n=25). Key informant interviews (n=14) were conducted with hospital clinical providers and clinical program directors. The 3 delays model, respectful maternal care and cultural competence frameworks were applied, using Nvivo for coding and organizing emerging themes. RESULTS: Haitian women demonstrate significantly higher than average proportions of advanced maternal age {40.9 %; (<.001)}, obesity {43.8.8%; (<.001)}; delayed entry to prenatal care {49.5; (.007)}; and pre-eclampsia {7.0%; (.001)}. Inadequate access to insurance and transportation contributed to delays in accessing care. Disrespectful and poor coordination of care negatively affected Haitian women’s experience and perceived quality of care. Haitian women desired culturally competent, personalized heath care services. Barriers to cultural competence and the development of culturally specific care models include lack of staff diversity, finances, health care structural design and lack of training. CONCLUSION: Haitian women may benefit from additional social resources and culturally specific, tailored health programs such as group prenatal care. Recommendations to improve cultural competence include diversifying the workforce, collecting data on racial and ethnic disparities, and trainings to integrate culturally competent principles into clinical practice. Additional research needs include design, implementation and evaluation of culturally specific tailored maternal health interventions with the Haitian population."
JONATHAN MIJS,Sounds like meritocracy to my ears: exploring the link between inequality in popular music and personal culture,"Extant research documents the impact of meritocratic narratives in news media that justify economic inequality. This paper inductively explores whether popular music is a source of cultural frames about inequality. We construct an original dataset combining user data from Spotify with lyrics from Genius and employ unsupervised computational text analysis to classify the content of the 3,660 most popular songs across 23 European countries. Drawing on Lizardo’s enculturation framework, we analyze lyrics through the lens of public culture and explore their link with individual beliefs as a reflection of personal culture. We find that, in more unequal societies, songs that frame inequalities as a structural issue (lyrics about ‘Struggle’ or omnipresent ‘Risks’) are more popular than those adopting a meritocratic frame (songs we describe as ‘Bragging Rights’ or those telling a ‘Rags to Riches’ tale). Moreover, we find that the presence in public culture of a certain frame is associated with the expression of frame-consistent individual beliefs about inequality. We conclude by reflecting on the promise of automatic text classification for the study of lyrics, the theorized role of popular music in the study of culture, and by proposing venues for future research."
JONATHAN MIJS,Belief change in times of crisis: Providing facts about COVID-19-induced inequalities closes the partisan divide but fuels intra-partisan polarization about inequality,"Population-based survey research demonstrates that growing economic divides in Western countries have not gone together with increased popular concern about inequality. Extant explanations focus on ‘misperception’: people generally underestimate the extent of inequality and overestimate society's meritocratic nature. However, scholarly attempts to correct people's misperceptions have produced mixed results. We ask whether COVID-19, by upending everyday life, has made people responsive to information about inequality, even if that entails crossing ideological divides. We field an original survey experiment in the United States, a least-likely case of belief change, given high levels of inequality and partisan polarization. Our informational treatment increases (1) concerns over economic inequality, (2) support for redistribution, and (3) acknowledgement that COVID-19 has especially hurt the most vulnerable. Information provision renders non-significant the partisan gap between moderate Democrats and Republicans but increases that between moderate and strong Republicans. We discuss our findings' implications and suggestions for further research."
JONATHAN MIJS,Inequality Is a problem of inference: how people solve the social puzzle of unequal outcomes,"A new wave of scholarship recognizes the importance of people’s understanding of inequality that underlies their political convictions, civic values, and policy views. Much less is known, however, about the sources of people’s different beliefs. I argue that scholarship is hampered by a lack of consensus regarding the conceptualization and measurement of inequality beliefs, in the absence of an organizing theory. To fill this gap, in this paper, I develop a framework for studying the social basis of people’s explanations for inequality. I propose that people observe unequal outcomes and must infer the invisible forces that brought these about, be they meritocratic or structural in nature. In making inferences about the causes of inequality, people draw on lessons from past experience and information about the world, both of which are biased and limited by their background, social networks, and the environments they have been exposed to. Looking at inequality beliefs through this lens allows for an investigation into the kinds of experiences and environments that are particularly salient in shaping people’s inferential accounts of inequality. Specifically, I make a case for investigating how socializing institutions such as schools and neighborhoods are “inferential spaces” that shape how children and young adults come to learn about their unequal society and their own place in it. I conclude by proposing testable hypotheses and implication for research."
JONATHAN MIJS,Neoliberalism and symbolic boundaries in Europe,"Studies suggest that the rise of neoliberalism accompanies a foregrounding of individual responsibility and a weakening of community. The authors provide a theoretical agenda for studying the interactions between the global diffusion of neoliberal policies and ideologies, on the one side, and cultural repertoires and boundary configurations, on the other, in the context of local, national, and regional variation. Exploiting variation in the rate of adoption of neoliberal policies across European societies, the authors show how levels of neoliberal penetration covary with the way citizens draw symbolic boundaries along the lines of ethnoreligious otherness and moral deservingness."
JONATHAN MIJS,Correction to: deliberating inequality: a blueprint for studying the social formation of beliefs about economic inequality,
JONATHAN MIJS,Income inequality and residential segregation in ‘egalitarian’ Sweden: lessons from a least likely case,
JONATHAN MIJS,Learning about inequality in unequal America: How heterogeneity in college shapes students’ belief in meritocracy and racial discrimination,
JONATHAN MIJS,Deliberating inequality: a blueprint for studying the social formation of beliefs about economic inequality,"In most contemporary societies, people underestimate the extent of economic inequality, resulting in lower support for taxation and redistribution than might be expressed by better informed citizens. We still know little, however, about how understandings of inequality arise, and therefore about where perceptions and misperceptions of it might come from. This methodological article takes one step toward filling this gap by developing a research design-a blueprint-to study how people's understandings of wealth and income inequality develop through social interaction. Our approach combines insights from recent scholarship highlighting the socially situated character of inequality beliefs with those of survey experimental work testing how information about inequality changes people's understandings of it. Specifically, we propose to use deliberative focus groups to approximate the interactional contexts in which individuals process information and form beliefs in social life. Leveraging an experimental methodology, our design then varies the social makeup of deliberative groups, as well as the information about inequality we share with participants, to explore how different types of social environments and information shape people's understandings of economic inequality. This should let us test, in particular, whether the low socioeconomic diversity of people's discussion and interaction networks relates to their tendency to underestimate inequality, and whether beliefs about opportunity explain people's lack of appetite for redistributive policies. In this exploratory article we motivate our methodological apparatus and describe its key features, before reflecting on the findings from a proof-of-concept study conducted in London in the fall of 2019."
JONATHAN MIJS,Belief in meritocracy reexamined: scrutinizing the role of subjective social mobility,"Despite decreasing intergenerational mobility, strengthening the ties between family background and children’s economic outcomes, Western citizens continue to believe in meritocracy. We study how meritocratic beliefs about success relate to individuals’ social mobility experiences: Is subjective upward mobility associated with meritocratic attributions of success and downward mobility with structuralist views? Whereas previous studies addressed the relevance of individuals’ current position or objective mobility, we leverage diagonal reference models to disentangle the role of subjective mobility, origin, and destination. Surveying a representative Dutch sample (n = 1,507), we find, echoing the Thomas theorem, that if people experience social mobility as real, it is real in its consequences: subjective upward mobility is associated with stronger meritocratic beliefs, and downward mobility is associated with stronger structuralist beliefs—but has no bearing on people’s meritocracy beliefs. This helps understand the muted political response to growing inequality: a small share of upwardly mobile individuals may suffice to uphold public faith in meritocracy."
JONATHAN MIJS,"How information about inequality impacts belief in meritocracy: evidence from a randomized survey experiment in Australia, Indonesia and Mexico","Most people misperceive economic inequality. Learning about actual levels of inequality and social mobility, research suggests, heightens concerns but may push people’s policy preferences in any number of directions. This mixed empirical record, we argue, reflects the omission of a more fundamental question: under what conditions do people change their understanding of the meritocratic or non-meritocratic causes of inequality? To explore mechanisms of belief change we field a unique randomized survey experiment with representative populations in Australia, Indonesia, and Mexico—societies with varying levels of popular beliefs about economic inequality. Our results highlight the importance of information, perceived social position, and self-interest. In Indonesia, information describing (high) income inequality and (low) social mobility rocked our participants’ belief in meritocracy. The same information made less of a splash in Mexico, where unequal outcomes are commonly understood as the result of corruption and other non-meritocratic processes. In Australia, the impact of our informational treatment was strongest when it provided justification for people’s income position or when it corrected their perception of relative affluence. Our findings reveal asymmetric beliefs about poverty and wealth and heterogeneous responses to information. They are a call to rethink effective informational and policy interventions."
JONATHAN MIJS,"Meritocracy, elitism and inequality","The appeal of meritocracy is plain to see, because it appears to promote equality of opportunity. However, in this paper we argue that meritocracy is also a deeply elitist project. Firstly, we place Michael Young in context to show how his critique of meritocracy should be understood as a socialist vision to ameliorate class divides. Secondly, we show how economic inequality in the UK has not generated systematic resistance: in fact, inequality and belief in meritocracy have gone hand in hand. Thirdly, we argue that people see their own lives as meritocratic rather than ascribed, and that such values are deeply embedded in popular life. We offer two explanations for how such views have come about, and show how they have helped construct a more unequal society."
JONATHAN MIJS,"Visualizing belief in meritocracy, 1930–2010","In this figure I describe the long trend in popular belief in meritocracy across the Western world between 1930 and 2010. Studying trends in attitudes is limited by the paucity of survey data that can be compared across countries and over time. Here, I show how to complement survey waves with cohort-level data. Repeated surveys draw on a representative sample of the population to describe the typical beliefs held by citizens in a given country and period. Leveraging the fact that citizens surveyed in a given year were born in different time-periods allows for a comparison of beliefs across birth cohorts. The latter overlaps with the former, but considerably extends the time period covered by the data. Taken together, the two measures give a “triangulated” longitudinal record of popular belief in meritocracy. I find that in most countries, popular belief in meritocracy is (much) stronger for more recent periods and cohorts."
JONATHAN MIJS,"Is America coming apart? Socioeconomic segregation in neighborhoods, schools, workplaces, and social networks, 1970–2020","As income inequality in the United States has reached an all-time high, commentators from across the political spectrum warn about the social implications of these economic changes. America, they fear, is “coming apart” as the gap between the rich and poor grows into a fault line. This paper provides a comprehensive review of empirical scholarship in sociology, education, demography, and economics in order to address the question: How have five decades of growing economic inequality shaped America's social landscape? We find that growing levels of income inequality have been accompanied by increasing socioeconomic segregation across (1) friendship networks and romantic partners, (2) residential neighborhoods, (3) K-12 and university education, and (4) workplaces and the labor market. The trends documented in this review give substance to commentators' concerns: compared to the 1970s, rich and poor Americans today are less likely to know one another and to share the same social spaces. The United States is a nation divided."
JONATHAN MIJS,Observing many researchers using the same data and hypothesis reveals a hidden universe of uncertainty,
JONATHAN MIJS,Merit and ressentiment: how to tackle the tyranny of merit,"My contribution to this special issue engages with Michael Sandel’s The Tyranny of Meritocracy and its significance to the academic conversation about meritocracy and its discontents. Specifically, I highlight Sandel’s diagnosis of the rise of populism and his proposed remedy for the ‘tyranny of merit’. First, building on Menno ter Braak’s writings on the rise of fascism, I explore the sources of ressentiment in contemporary societies as stemming not from disillusionment with meritocracy but from the broken promise of liberalism and democracy more generally. Second, I consider Sandel’s proposals to reform elite university admissions and to ‘recognize work’, explore their wider applicability, and reflect on their limitations to meaningfully change how success and failure is socially experienced and morally understood."
JONATHAN MIJS,The missing organizational dimension of prisoner reentry: an ethnography of the road to reentry at a nonprofit service provider,"Prisoner reentry has received great interest in political sociology, criminology, and beyond. Research documents the struggles of individuals trying to find their way back into society. Less attention has been given to the organizational aspects of reentry. This is unfortunate given the rapid growth of nonprofit reentry organizations in the United States, which introduces a new set of questions about the context and challenges to prisoner reentry. Drawing on an ethnography of Safe, a nonprofit reentry organization in the Northeast, I describe the organization's pivotal role in institutionalizing the pathway to prisoner reentry: a road to reentry, which takes former prisoners through a process that reconfigures their morality, identity, and social relationships. The road-to-reentry concept helps bring together scholars of the welfare state and criminology by highlighting how the challenges of prisoner reentry rely on how this process is organized. The way in which prison reentry is organized, in turn, affects former prisoners’ agency and shapes the relationship between these men and women and their respective families and communities."
JONATHAN MIJS,"Do changes in material circumstances drive support for populist radical parties? Panel data evidence from the Netherlands during the Great Recession, 2007–2015","Political developments since the 2008 financial crisis have sparked renewed interest in the electoral implications of economic downturns. Research describes a correlation between adverse economic conditions and support for radical parties campaigning on the populist promise to retake the country from a corrupt elite. But does the success of radical parties following economic crises rely on people who are directly affected? To answer this question, we examine whether individual-level changes in economic circumstances drive support for radical parties across the ideological divide. Analysing eight waves of panel data collected in the Netherlands, before, during, and after the Great Recession (2007–2015), we demonstrate that people who experienced an income loss became more supportive of the radical left but not of the radical right. Looking at these parties’ core concerns, we find that income loss increased support for income redistribution championed by the radical left, but less so for the anti-immigration policies championed by the radical right. Our study establishes more directly than extant research the micro-foundations of support for radical parties across the ideological divide."
JONATHAN MIJS,The burden of acting wise: sanctioned success and ambivalence about hard work at an elite school in the Netherlands,"Sam and his classmates despise ‘nerds’: they say working hard in school makes a student unpopular, and that they purposefully do only the minimum to pass. Research suggests that such ‘oppositional’ attitudes are prevalent among working class students and/or ethnoracial minorities. Like most of his classmates, however, Sam is white, hails from a privileged background, and attends a selective school in the Netherlands. Deeply ambivalent about working hard and ‘acting wise’, Sam and the others constituting his adolescent society are thoroughly caught up in peer dynamics which sanction success and promote mediocrity. We link these anti-school peer dynamics to the institutional configuration of education in the Netherlands, characterized by rigid tracking at the end of primary school and non-selective universities: state structures and policies contribute to these privileged students’ rationale for ‘taking it easy’ and doing poorly in school."
JONATHAN MIJS,"Earning rent with your talent: modern-day inequality rests on the power to define, transfer and institutionalize talent","In this article, I develop the point that whereas talent is the basis for desert, talent itself is not meritocratically deserved. It is produced by three processes, none of which are meritocratic: (1) talent is unequally distributed by the rigged lottery of birth, (2) talent is defined in ways that favor some traits over others, and (3) the market for talent is manipulated to maximally extract advantages by those who have more of it. To see how, we require a sociological perspective on economic rent. I argue that talent is a major means through which people seek rent in modern-day capitalism. Talent today is what inherited land was to feudal societies; an unchallenged source of symbolic and economic rewards. Whereas God sanctified the aristocracy’s wealth, contemporary privilege is legitimated by meritocracy. Drawing on the work of Gary Becker, Pierre Bourdieu, and Jerome Karabel, I show how rent-seeking in modern societies has come to rely principally on rent-definition and creation. Inequality is produced by the ways in which talent is defined, institutionalized, and sustained by the moral deservingness we attribute to the accomplishments of talents. Consequently, today’s inequalities are as striking as ever, yet harder to challenge than ever before."
JONATHAN MIJS,The paradox of inequality: income inequality and belief in meritocracy go hand in hand,"Inequality is on the rise: gains have been concentrated with a small elite, while most have seen their fortunes stagnate or fall. Despite what scholars and journalists consider a worrying trend, there is no evidence of growing popular concern about inequality. In fact, research suggests that citizens in unequal societies are less concerned than those in more egalitarian societies. How to make sense of this paradox? I argue that citizens’ consent to inequality is explained by their growing conviction that societal success is reflective of a meritocratic process. Drawing on 25 years of International Social Survey Program data, I show that rising inequality is legitimated by the popular belief that the income gap is meritocratically deserved: the more unequal a society, the more likely its citizens are to explain success in meritocratic terms, and the less important they deem nonmeritocratic factors such as a person’s family wealth and connections."
JONATHAN MIJS,Does informing citizens about the non-meritocratic nature of inequality bolster support for a Universal Basic Income? evidence from a population-based survey experiment,
JONATHAN MIJS,Confronting racism of omission,"The COVID-19 pandemic and Black Lives Matter movement have brought ethnic and racial inequalities to the forefront of public conversation on both sides of the Atlantic. However, research shows that people routinely overestimate the progress made towards equality and underestimate disparities between racial and ethnic majority and minority groups. Common among the American public is a naive belief in equal opportunity that stands in sharp contrast to the reality of structural racial inequity. Across the Atlantic, Dutch people’s self-perception of a tolerant, progressive, and egalitarian society means that racism and discrimination are topics often avoided, rendering invisible the stigmatization of ethnic and racial minorities. The result is racism of omission: ethnic and racial disparities are minimized and attributed to factors other than discrimination, which leads to legitimize inequities and justify non-intervention. Against this background, we field an internationally comparative randomized survey experiment to study whether (willful) ignorance about racial and ethnic inequality can be addressed through the provision of information. We find that facts about ethnic and racial inequality, on the whole, (1) have the greatest impact on people’s perceptions of inequality as compared to their explanations of inequality and policy attitudes, (2) register most strongly with majority-group White participants as compared to participants from minority groups, (3) cut across partisan lines, and (4) effect belief change most consistently in the Netherlands, as compared to the United States. We make sense of these findings through the lens of how ‘shocking’ the information provided was to different groups of participants."
JONATHAN MIJS,Confronting racism of omission: experimental evidence of the impact of information about ethnic and racial inequality in the United States and the Netherlands,"The COVID-19 pandemic and Black Lives Matter movement have brought ethnic and racial inequalities to the forefront of public conversation on both sides of the Atlantic. However, research shows that people routinely overestimate the progress made towards equality and underestimate disparities between racial and ethnic majority and minority groups. Common among the American public is a naive belief in equal opportunity that stands in sharp contrast to the reality of structural racial inequity. Across the Atlantic, Dutch people’s self-perception of a tolerant, progressive, and egalitarian society means that racism and discrimination are topics often avoided, rendering invisible the stigmatization of ethnic and racial minorities. The result is racism of omission: ethnic and racial disparities are minimized and attributed to factors other than discrimination, which leads to legitimize inequities and justify non-intervention. Against this background, we field an internationally comparative randomized survey experiment to study whether (willful) ignorance about racial and ethnic inequality can be addressed through the provision of information. We find that facts about ethnic and racial inequality, on the whole, (1) have the greatest impact on people’s perceptions of inequality as compared to their explanations of inequality and policy attitudes, (2) register most strongly with majority-group White participants as compared to participants from minority groups, (3) cut across partisan lines, and (4) effect belief change most consistently in the Netherlands, as compared to the United States. We make sense of these findings through the lens of how ‘shocking’ the information provided was to different groups of participants."
ERIC L HWANG,Genome-Wide Association Study for Renal Traits in the Framingham Heart and Atherosclerosis Risk in Communities Studies,"BACKGROUND: The Framingham Heart Study (FHS) recently obtained initial results from the first genome-wide association scan for renal traits. The study of 70,987 single nucleotide polymorphisms (SNPs) in 1,010 FHS participants provides a list of SNPs showing the strongest associations with renal traits which need to be verified in independent study samples. METHODS: Sixteen SNPs were selected for replication based on the most promising associations with chronic kidney disease (CKD), estimated glomerular filtration rate (eGFR), and serum cystatin C in FHS. These SNPs were genotyped in 15,747 participants of the Atherosclerosis in Communities (ARIC) Study and evaluated for association using multivariable adjusted regression analyses. Primary outcomes in ARIC were CKD and eGFR. Secondary prospective analyses were conducted for association with kidney disease progression using multivariable adjusted Cox proportional hazards regression. The definition of the outcomes, all covariates, and the use of an additive genetic model was consistent with the original analyses in FHS. RESULTS: The intronic SNP rs6495446 in the gene MTHFS was significantly associated with CKD among white ARIC participants at visit 4: the odds ratio per each C allele was 1.24 (95% CI 1.09–1.41, p = 0.001). Borderline significant associations of rs6495446 were observed with CKD at study visit 1 (p = 0.024), eGFR at study visits 1 (p = 0.073) and 4 (lower mean eGFR per C allele by 0.6 ml/min/1.73 m2, p = 0.043) and kidney disease progression (hazard ratio 1.13 per each C allele, 95% CI 1.00–1.26, p = 0.041). Another SNP, rs3779748 in EYA1, was significantly associated with CKD at ARIC visit 1 (odds ratio per each T allele 1.22, p = 0.01), but only with eGFR and cystatin C in FHS. CONCLUSION: This genome-wide association study provides unbiased information implicating MTHFS as a candidate gene for kidney disease. Our findings highlight the importance of replication to identify common SNPs associated with renal traits."
ERIC L HWANG,"Genome-wide association studies of serum magnesium, potassium, and sodium concentrations identify six loci influencing serum magnesium levels","Magnesium, potassium, and sodium, cations commonly measured in serum, are involved in many physiological processes including energy metabolism, nerve and muscle function, signal transduction, and fluid and blood pressure regulation. To evaluate the contribution of common genetic variation to normal physiologic variation in serum concentrations of these cations, we conducted genome-wide association studies of serum magnesium, potassium, and sodium concentrations using ∼2.5 million genotyped and imputed common single nucleotide polymorphisms (SNPs) in 15,366 participants of European descent from the international CHARGE Consortium. Study-specific results were combined using fixed-effects inverse-variance weighted meta-analysis. SNPs demonstrating genome-wide significant (p<5×10−8) or suggestive associations (p<4×10−7) were evaluated for replication in an additional 8,463 subjects of European descent. The association of common variants at six genomic regions (in or near MUC1, ATP2B1, DCDC5, TRPM6, SHROOM3, and MDS1) with serum magnesium levels was genome-wide significant when meta-analyzed with the replication dataset. All initially significant SNPs from the CHARGE Consortium showed nominal association with clinically defined hypomagnesemia, two showed association with kidney function, two with bone mineral density, and one of these also associated with fasting glucose levels. Common variants in CNNM2, a magnesium transporter studied only in model systems to date, as well as in CNNM3 and CNNM4, were also associated with magnesium concentrations in this study. We observed no associations with serum sodium or potassium levels exceeding p<4×10−7. Follow-up studies of newly implicated genomic loci may provide additional insights into the regulation and homeostasis of human serum magnesium levels. Author Summary Magnesium, potassium, and sodium are involved in important physiological processes. To better understand how common genetic variation may contribute to inter-individual differences in serum concentrations of these electrolytes, we evaluated single nucleotide polymorphisms (SNPs) across the genome in association with serum magnesium, potassium, and sodium levels in 15,366 participants of European descent from the CHARGE Consortium. We then verified the associations in an additional 8,463 study participants. Six different genomic regions contain variants that are reproducibly associated with serum magnesium levels, and only one of the regions had been previously known to influence serum magnesium concentrations in humans. The identified SNPs also show association with clinically defined hypomagnesemia, and some of them with traits that have been linked to serum magnesium levels, including kidney function, fasting glucose, and bone mineral density. We further provide evidence for a physiological role of magnesium transporters in humans which have previously only been studied in model systems. None of the SNPs evaluated in our study are significantly associated with serum levels of sodium or potassium. Additional studies are needed to investigate the underlying molecular mechanisms in order to help us understand the contribution of these newly identified regions to magnesium homeostasis."
HANNAH BROWN,A study of the rate of occurrence of cleft lip and palates in different ethnicities,"Cleft lip and/or palate (CLP) is one of the most common birth defects and affects many individuals worldwide. There is still much unknown about the underlying causes. It is known that genetics and the environment play a role. This paper outlines various genetic mutations and inheritance patterns. It then takes a deeper look into the known genetics of CLP. More specifically, looking at the differences in prevalence of CLP based on ethnicity. The highest rate of CLP is seen in Native American populations, while the lowest rate is seen in African Americans. There are many reasons behind why this is the case, including a difference in the genes involved and environmental factors. Some genes are found to be more prevalent in certain ethnicities and certain environmental factors affect some groups more than others."
HANNAH BROWN,"Distinct phenotypes associated with mangrove and lagoon habitats in two widespread caribbean corals, porites astreoides and porites divaricata","AbstractAs coral reefs experience dramatic declines in coral cover throughout the tropics, there is an urgent need to understand the role that non-reef habitats, such as mangroves, play in the ecological niche of corals. Mangrove habitats present a challenge to reef-dwelling corals because they can differ dramatically from adjacent reef habitats with respect to key environmental parameters, such as light. Because variation in light within reef habitats is known to drive intraspecific differences in coral phenotype, we hypothesized that coral species that can exploit both reef and mangrove habitats will exhibit predictable differences in phenotypes between habitats. To investigate how intraspecific variation, driven by either local adaptation or phenotypic plasticity, might enable particular coral species to exploit these two qualitatively different habitat types, we compared the phenotypes of two widespread Caribbean corals, Porites divaricata and Porites astreoides, in mangrove versus lagoon habitats on Turneffe Atoll, Belize. We document significant differences in colony size, color, structural complexity, and corallite morphology between habitats. In every instance, the phenotypic differences between mangrove prop root and lagoon corals exhibited consistent trends in both P. divaricata and P. astreoides. We believe this study is the first to document intraspecific phenotypic diversity in corals occupying mangrove prop root versus lagoonal patch reef habitats. A difference in the capacity to adopt an alternative phenotype that is well suited to the mangrove habitat may explain why some reef coral species can exploit mangroves, while others cannot."
HANNAH BROWN,Single cell profiling distinguishes leukemia-selective chemotypes,"A central challenge in chemical biology is to distinguish molecular families in which small structural changes trigger large changes in cell biology. Such families might be ideal scaffolds for developing cell-selective chemical effectors - for example, molecules that activate DNA damage responses in malignant cells while sparing healthy cells. Across closely related structural variants, subtle structural changes have the potential to result in contrasting bioactivity patterns across different cell types. Here, we tested a 600-compound Diversity Set of screening molecules from the Boston University Center for Molecular Discovery (BU-CMD) in a novel phospho-flow assay that tracked fundamental cell biological processes, including DNA damage response, apoptosis, M-phase cell cycle, and protein synthesis in MV411 leukemia cells. Among the chemotypes screened, synthetic congeners of the rocaglate family were especially bioactive. In follow-up studies, 37 rocaglates were selected and deeply characterized using 12 million additional cellular measurements across MV411 leukemia cells and healthy peripheral blood mononuclear cells. Of the selected rocaglates, 92% displayed significant bioactivity in human cells, and 65% selectively induced DNA damage responses in leukemia and not healthy human blood cells. Furthermore, the signaling and cell-type selectivity were connected to structural features of rocaglate subfamilies. In particular, three rocaglates from the rocaglate pyrimidinone (RP) structural subclass were the only molecules that activated exceptional DNA damage responses in leukemia cells without activating a detectable DNA damage response in healthy cells. These results indicate that the RP subset should be extensively characterized for anticancer therapeutic potential as it relates to the DNA damage response. This single cell profiling approach advances a chemical biology platform to dissect how systematic variations in chemical structure can profoundly and differentially impact basic functions of healthy and diseased cells."
WON M LEE,First M87 Event Horizon Telescope results. III. Data processing and calibration,"We present the calibration and reduction of Event Horizon Telescope (EHT) 1.3 mm radio wavelength observations of the supermassive black hole candidate at the center of the radio galaxy M87 and the quasar 3C 279, taken during the 2017 April 5–11 observing campaign. These global very long baseline interferometric observations include for the first time the highly sensitive Atacama Large Millimeter/submillimeter Array (ALMA); reaching an angular resolution of 25 μas, with characteristic sensitivity limits of ~1 mJy on baselines to ALMA and ~10 mJy on other baselines. The observations present challenges for existing data processing tools, arising from the rapid atmospheric phase fluctuations, wide recording bandwidth, and highly heterogeneous array. In response, we developed three independent pipelines for phase calibration and fringe detection, each tailored to the specific needs of the EHT. The final data products include calibrated total intensity amplitude and phase information. They are validated through a series of quality assurance tests that show consistency across pipelines and set limits on baseline systematic errors of 2% in amplitude and 1° in phase. The M87 data reveal the presence of two nulls in correlated flux density at ~3.4 and ~8.3 Gλ and temporal evolution in closure quantities, indicating intrinsic variability of compact structure on a timescale of days, or several light-crossing times for a few billion solar-mass black hole. These measurements provide the first opportunity to image horizon-scale structure in M87."
WON M LEE,First M87 Event Horizon Telescope results. V. Physical origin of the asymmetric ring,"The Event Horizon Telescope (EHT) has mapped the central compact radio source of the elliptical galaxy M87 at 1.3 mm with unprecedented angular resolution. Here we consider the physical implications of the asymmetric ring seen in the 2017 EHT data. To this end, we construct a large library of models based on general relativistic magnetohydrodynamic (GRMHD) simulations and synthetic images produced by general relativistic ray tracing. We compare the observed visibilities with this library and confirm that the asymmetric ring is consistent with earlier predictions of strong gravitational lensing of synchrotron emission from a hot plasma orbiting near the black hole event horizon. The ring radius and ring asymmetry depend on black hole mass and spin, respectively, and both are therefore expected to be stable when observed in future EHT campaigns. Overall, the observed image is consistent with expectations for the shadow of a spinning Kerr black hole as predicted by general relativity. If the black hole spin and M87's large scale jet are aligned, then the black hole spin vector is pointed away from Earth. Models in our library of non-spinning black holes are inconsistent with the observations as they do not produce sufficiently powerful jets. At the same time, in those models that produce a sufficiently powerful jet, the latter is powered by extraction of black hole spin energy through mechanisms akin to the Blandford-Znajek process. We briefly consider alternatives to a black hole for the central compact object. Analysis of existing EHT polarization data and data taken simultaneously at other wavelengths will soon enable new tests of the GRMHD models, as will future EHT campaigns at 230 and 345 GHz."
WON M LEE,First M87 Event Horizon Telescope results. VI. The shadow and mass of the central black hole,"We present measurements of the properties of the central radio source in M87 using Event Horizon Telescope data obtained during the 2017 campaign. We develop and fit geometric crescent models (asymmetric rings with interior brightness depressions) using two independent sampling algorithms that consider distinct representations of the visibility data. We show that the crescent family of models is statistically preferred over other comparably complex geometric models that we explore. We calibrate the geometric model parameters using general relativistic magnetohydrodynamic (GRMHD) models of the emission region and estimate physical properties of the source. We further fit images generated from GRMHD models directly to the data. We compare the derived emission region and black hole parameters from these analyses with those recovered from reconstructed images. There is a remarkable consistency among all methods and data sets. We find that >50% of the total flux at arcsecond scales comes from near the horizon, and that the emission is dramatically suppressed interior to this region by a factor >10, providing direct evidence of the predicted shadow of a black hole. Across all methods, we measure a crescent diameter of 42 ± 3 μas and constrain its fractional width to be <0.5. Associating the crescent feature with the emission surrounding the black hole shadow, we infer an angular gravitational radius of GM/Dc^2 = 3.8 ± 0.4 μas. Folding in a distance measurement of {16.8}_{-0.7}^{+0.8}{Mpc} gives a black hole mass of M = 6.5 ± 0.2{| }_{stat} ± 0.7{| }_{sys} × {10}^{9} {M}_{odot }. This measurement from lensed emission near the event horizon is consistent with the presence of a central Kerr black hole, as predicted by the general theory of relativity."
WON M LEE,The Event Horizon general relativistic magnetohydrodynamic code comparison project,"Recent developments in compact object astrophysics, especially the discovery of merging neutron stars by LIGO, the imaging of the black hole in M87 by the Event Horizon Telescope, and high- precision astrometry of the Galactic Center at close to the event horizon scale by the GRAVITY experiment motivate the development of numerical source models that solve the equations of general relativistic magnetohydrodynamics (GRMHD). Here we compare GRMHD solutions for the evolution of a magnetized accretion flow where turbulence is promoted by the magnetorotational instability from a set of nine GRMHD codes: Athena++, BHAC, Cosmos++, ECHO, H-AMR, iharm3D, HARM-Noble, IllinoisGRMHD, and KORAL. Agreement among the codes improves as resolution increases, as measured by a consistently applied, specially developed set of code performance metrics. We conclude that the community of GRMHD codes is mature, capable, and consistent on these test problems."
WON M LEE,Gravitational test beyond the first post-Newtonian order with the shadow of the M87 black hole,"The 2017 Event Horizon Telescope (EHT) observations of the central source in M87 have led to the first measurement of the size of a black-hole shadow. This observation offers a new and clean gravitational test of the black-hole metric in the strong-field regime. We show analytically that spacetimes that deviate from the Kerr metric but satisfy weak-field tests can lead to large deviations in the predicted black-hole shadows that are inconsistent with even the current EHT measurements. We use numerical calculations of regular, parametric, non-Kerr metrics to identify the common characteristic among these different parametrizations that control the predicted shadow size. We show that the shadow-size measurements place significant constraints on deviation parameters that control the second post-Newtonian and higher orders of each metric and are, therefore, inaccessible to weak-field tests. The new constraints are complementary to those imposed by observations of gravitational waves from stellar-mass sources."
WON M LEE,Verification of radiative transfer schemes for the EHT,"The Event Horizon Telescope (EHT) Collaboration has recently produced the first resolved images of the central supermassive black hole in the giant elliptical galaxy M87. Here we report on tests of the consistency and accuracy of the general relativistic radiative transfer codes used within the collaboration to model M87* and Sgr A*. We compare and evaluate (1) deflection angles for equatorial null geodesics in a Kerr spacetime; (2) images calculated from a series of simple, parameterized matter distributions in the Kerr metric using simplified emissivities and absorptivities; (3) for a subset of codes, images calculated from general relativistic magnetohydrodynamics simulations using different realistic synchrotron emissivities and absorptivities; (4) observables for the 2017 configuration of EHT, including visibility amplitudes and closure phases. The error in total flux is of order 1% when the codes are run with production numerical parameters. The dominant source of discrepancies for small camera distances is the location and detailed setup of the software ""camera"" that each code uses to produce synthetic images. We find that when numerical parameters are suitably chosen and the camera is sufficiently far away the images converge and that for given transfer coefficients, numerical uncertainties are unlikely to limit parameter estimation for the current generation of EHT observations. The purpose of this paper is to describe a verification and comparison of EHT radiative transfer codes. It is not to verify EHT models more generally."
WON M LEE,Monitoring the mmorphology of M87* in 2009–2017 with the Event Horizon Telescope,"The Event Horizon Telescope (EHT) has recently delivered the first resolved images of M87*, the supermassive black hole in the center of the M87 galaxy. These images were produced using 230 GHz observations performed in April 2017. Additional observations are required to investigate the persistence of the primary image feature – a ring with azimuthal brightness asymmetry – and to quantify the image variability on event horizon scales. To address this need, we analyze M87* data collected with prototype EHT arrays in 2009, 2011, 2012, and 2013. While these observations do not contain enough information to produce images, they are sufficient to constrain simple geometric models. We develop a modeling approach based on the framework utilized for the 2017 EHT data analysis and validate our procedures using synthetic data. Applying the same approach to the observational data sets, we find the M87* morphology in 2009–2017 to be consistent with a persistent asymmetric ring of 40 as diameter. The position angle of peak intensity varies in time. In particular, we find a significant difference between the position angle measured in 2013 and 2017. These variations are in broad agreement with predictions of a subset of general relativistic magnetohydrodynamic simulations. We show that quantifying the variability across multiple observational epochs has the potential to constrain physical properties of the source, such as the accretion state or the black hole spin."
WON M LEE,THEMIS: a parameter estimation framework for the Event Horizon Telescope,"The Event Horizon Telescope (EHT) provides the unprecedented ability to directly resolve the structure and dynamics of black hole emission regions on scales smaller than their horizons. This has the potential to critically probe the mechanisms by which black holes accrete and launch outflows, and the structure of supermassive black hole spacetimes. However, accessing this information is a formidable analysis challenge for two reasons. First, the EHT natively produces a variety of data types that encode information about the image structure in nontrivial ways; these are subject to a variety of systematic effects associated with very long baseline interferometry and are supplemented by a wide variety of auxiliary data on the primary EHT targets from decades of other observations. Second, models of the emission regions and their interaction with the black hole are complex, highly uncertain, and computationally expensive to construct. As a result, the scientific utilization of EHT observations requires a flexible, extensible, and powerful analysis framework. We present such a framework, Themis, which defines a set of interfaces between models, data, and sampling algorithms that facilitates future development. We describe the design and currently existing components of Themis, how Themis has been validated thus far, and present additional analyses made possible by Themis that illustrate its capabilities. Importantly, we demonstrate that Themis is able to reproduce prior EHT analyses, extend these, and do so in a computationally efficient manner that can efficiently exploit modern high-performance computing facilities. Themis has already been used extensively in the scientific analysis and interpretation of the first EHT observations of M87."
WON M LEE,First Sagittarius A* Event Horizon Telescope results. V. Testing astrophysical models of the galactic center black hole,"In this paper we provide a first physical interpretation for the Event Horizon Telescope's (EHT) 2017 observations of Sgr A*. Our main approach is to compare resolved EHT data at 230 GHz and unresolved non-EHT observations from radio to X-ray wavelengths to predictions from a library of models based on time-dependent general relativistic magnetohydrodynamics simulations, including aligned, tilted, and stellar-wind-fed simulations; radiative transfer is performed assuming both thermal and nonthermal electron distribution functions. We test the models against 11 constraints drawn from EHT 230 GHz data and observations at 86 GHz, 2.2 μm, and in the X-ray. All models fail at least one constraint. Light-curve variability provides a particularly severe constraint, failing nearly all strongly magnetized (magnetically arrested disk (MAD)) models and a large fraction of weakly magnetized models. A number of models fail only the variability constraints. We identify a promising cluster of these models, which are MAD and have inclination i ≤ 30°. They have accretion rate (5.2–9.5) × 10−9 M ⊙ yr−1, bolometric luminosity (6.8–9.2) × 1035 erg s−1, and outflow power (1.3–4.8) × 1038 erg s−1. We also find that all models with i ≥ 70° fail at least two constraints, as do all models with equal ion and electron temperature; exploratory, nonthermal model sets tend to have higher 2.2 μm flux density; and the population of cold electrons is limited by X-ray constraints due to the risk of bremsstrahlung overproduction. Finally, we discuss physical and numerical limitations of the models, highlighting the possible importance of kinetic effects and duration of the simulations."
WON M LEE,First M87 Event Horizon Telescope results. VII. Polarization of the ring,"In 2017 April, the Event Horizon Telescope (EHT) observed the near-horizon region around the supermassive black hole at the core of the M87 galaxy. These 1.3 mm wavelength observations revealed a compact asymmetric ring-like source morphology. This structure originates from synchrotron emission produced by relativistic plasma located in the immediate vicinity of the black hole. Here we present the corresponding linear-polarimetric EHT images of the center of M87. We find that only a part of the ring is significantly polarized. The resolved fractional linear polarization has a maximum located in the southwest part of the ring, where it rises to the level of ∼15%. The polarization position angles are arranged in a nearly azimuthal pattern. We perform quantitative measurements of relevant polarimetric properties of the compact emission and find evidence for the temporal evolution of the polarized source structure over one week of EHT observations. The details of the polarimetric data reduction and calibration methodology are provided. We carry out the data analysis using multiple independent imaging and modeling techniques, each of which is validated against a suite of synthetic data sets. The gross polarimetric structure and its apparent evolution with time are insensitive to the method used to reconstruct the image. These polarimetric images carry information about the structure of the magnetic fields responsible for the synchrotron emission. Their physical interpretation is discussed in an accompanying publication."
WON M LEE,First M87 Event Horizon Telescope results. VIII. Magnetic field structure near The Event Horizon,"Event Horizon Telescope (EHT) observations at 230 GHz have now imaged polarized emission around the supermassive black hole in M87 on event-horizon scales. This polarized synchrotron radiation probes the structure of magnetic fields and the plasma properties near the black hole. Here we compare the resolved polarization structure observed by the EHT, along with simultaneous unresolved observations with the Atacama Large Millimeter/submillimeter Array, to expectations from theoretical models. The low fractional linear polarization in the resolved image suggests that the polarization is scrambled on scales smaller than the EHT beam, which we attribute to Faraday rotation internal to the emission region. We estimate the average density n_e ∼ 10^4–7 cm^−3, magnetic field strength B ∼ 1–30 G, and electron temperature T_e ∼ (1–12) × 10^10 K of the radiating plasma in a simple one-zone emission model. We show that the net azimuthal linear polarization pattern may result from organized, poloidal magnetic fields in the emission region. In a quantitative comparison with a large library of simulated polarimetric images from general relativistic magnetohydrodynamic (GRMHD) simulations, we identify a subset of physical models that can explain critical features of the polarimetric EHT observations while producing a relativistic jet of sufficient power. The consistent GRMHD models are all of magnetically arrested accretion disks, where near-horizon magnetic fields are dynamically important. We use the models to infer a mass accretion rate onto the black hole in M87 of (3–20) × 10^−4 M ⊙ yr^−1."
WON M LEE,Resolving the inner parsec of the blazar J1924–2914 with the event horizon telescope,"The blazar J1924–2914 is a primary Event Horizon Telescope (EHT) calibrator for the Galactic center’s black hole Sagittarius A*. Here we present the first total and linearly polarized intensity images of this source obtained with the unprecedented 20 μas resolution of the EHT. J1924–2914 is a very compact flat-spectrum radio source with strong optical variability and polarization. In April 2017 the source was observed quasi-simultaneously with the EHT (April 5–11), the Global Millimeter VLBI Array (April 3), and the Very Long Baseline Array (April 28), giving a novel view of the source at four observing frequencies, 230, 86, 8.7, and 2.3 GHz. These observations probe jet properties from the subparsec to 100 pc scales. We combine the multifrequency images of J1924–2914 to study the source morphology. We find that the jet exhibits a characteristic bending, with a gradual clockwise rotation of the jet projected position angle of about 90° between 2.3 and 230 GHz. Linearly polarized intensity images of J1924–2914 with the extremely fine resolution of the EHT provide evidence for ordered toroidal magnetic fields in the blazar compact core."
WON M LEE,A universal power-law prescription for variability from synthetic images of black hole accretion flows,"We present a framework for characterizing the spatiotemporal power spectrum of the variability expected from the horizon-scale emission structure around supermassive black holes, and we apply this framework to a library of general relativistic magnetohydrodynamic (GRMHD) simulations and associated general relativistic ray-traced images relevant for Event Horizon Telescope (EHT) observations of Sgr A*. We find that the variability power spectrum is generically a red-noise process in both the temporal and spatial dimensions, with the peak in power occurring on the longest timescales and largest spatial scales. When both the time-averaged source structure and the spatially integrated light-curve variability are removed, the residual power spectrum exhibits a universal broken power-law behavior. On small spatial frequencies, the residual power spectrum rises as the square of the spatial frequency and is proportional to the variance in the centroid of emission. Beyond some peak in variability power, the residual power spectrum falls as that of the time-averaged source structure, which is similar across simulations; this behavior can be naturally explained if the variability arises from a multiplicative random field that has a steeper high-frequency power-law index than that of the time-averaged source structure. We briefly explore the ability of power spectral variability studies to constrain physical parameters relevant for the GRMHD simulations, which can be scaled to provide predictions for black holes in a range of systems in the optically thin regime. We present specific expectations for the behavior of the M87* and Sgr A* accretion flows as observed by the EHT."
WON M LEE,Millimeter light curves of Sagittarius A* observed during the 2017 Event Horizon Telescope campaign,"The Event Horizon Telescope (EHT) observed the compact radio source, Sagittarius A* (Sgr A*), in the Galactic Center on 2017 April 5–11 in the 1.3 mm wavelength band. At the same time, interferometric array data from the Atacama Large Millimeter/submillimeter Array and the Submillimeter Array were collected, providing Sgr A* light curves simultaneous with the EHT observations. These data sets, complementing the EHT very long baseline interferometry, are characterized by a cadence and signal-to-noise ratio previously unattainable for Sgr A* at millimeter wavelengths, and they allow for the investigation of source variability on timescales as short as a minute. While most of the light curves correspond to a low variability state of Sgr A*, the April 11 observations follow an X-ray flare and exhibit strongly enhanced variability. All of the light curves are consistent with a red-noise process, with a power spectral density (PSD) slope measured to be between −2 and −3 on timescales between 1 minute and several hours. Our results indicate a steepening of the PSD slope for timescales shorter than 0.3 hr. The spectral energy distribution is flat at 220 GHz, and there are no time lags between the 213 and 229 GHz frequency bands, suggesting low optical depth for the event horizon scale source. We characterize Sgr A*’s variability, highlighting the different behavior observed just after the X-ray flare, and use Gaussian process modeling to extract a decorrelation timescale and a PSD slope. We also investigate the systematic calibration uncertainties by analyzing data from independent data reduction pipelines."
WON M LEE,Selective dynamical imaging of interferometric data,"Recent developments in very long baseline interferometry (VLBI) have made it possible for the Event Horizon Telescope (EHT) to resolve the innermost accretion flows of the largest supermassive black holes on the sky. The sparse nature of the EHT’s (u, v)-coverage presents a challenge when attempting to resolve highly time-variable sources. We demonstrate that the changing (u, v)-coverage of the EHT can contain regions of time over the course of a single observation that facilitate dynamical imaging. These optimal time regions typically have projected baseline distributions that are approximately angularly isotropic and radially homogeneous. We derive a metric of coverage quality based on baseline isotropy and density that is capable of ranking array configurations by their ability to produce accurate dynamical reconstructions. We compare this metric to existing metrics in the literature and investigate their utility by performing dynamical reconstructions on synthetic data from simulated EHT observations of sources with simple orbital variability. We then use these results to make recommendations for imaging the 2017 EHT Sgr A* data set."
WON M LEE,First Sagittarius A* Event Horizon Telescope results. VI. Testing the black hole metric,"Astrophysical black holes are expected to be described by the Kerr metric. This is the only stationary, vacuum, axisymmetric metric, without electromagnetic charge, that satisfies Einstein’s equations and does not have pathologies outside of the event horizon. We present new constraints on potential deviations from the Kerr prediction based on 2017 EHT observations of Sagittarius A* (Sgr A*). We calibrate the relationship between the geometrically defined black hole shadow and the observed size of the ring-like images using a library that includes both Kerr and non-Kerr simulations. We use the exquisite prior constraints on the mass-to-distance ratio for Sgr A* to show that the observed image size is within ∼10% of the Kerr predictions. We use these bounds to constrain metrics that are parametrically different from Kerr, as well as the charges of several known spacetimes. To consider alternatives to the presence of an event horizon, we explore the possibility that Sgr A* is a compact object with a surface that either absorbs and thermally reemits incident radiation or partially reflects it. Using the observed image size and the broadband spectrum of Sgr A*, we conclude that a thermal surface can be ruled out and a fully reflective one is unlikely. We compare our results to the broader landscape of gravitational tests. Together with the bounds found for stellar-mass black holes and the M87 black hole, our observations provide further support that the external spacetimes of all black holes are described by the Kerr metric, independent of their mass."
WON M LEE,Polarimetric properties of Event Horizon Telescope targets from ALMA,"We present the results from a full polarization study carried out with the Atacama Large Millimeter/submillimeter Array (ALMA) during the first Very Long Baseline Interferometry (VLBI) campaign, which was conducted in 2017 April in the λ3 mm and λ1.3 mm bands, in concert with the Global mm-VLBI Array (GMVA) and the Event Horizon Telescope (EHT), respectively. We determine the polarization and Faraday properties of all VLBI targets, including Sgr A*, M87, and a dozen radio-loud active galactic nuclei (AGNs), in the two bands at several epochs in a time window of 10 days. We detect high linear polarization fractions (2%–15%) and large rotation measures (RM &gt; 103.3–105.5 rad m−2), confirming the trends of previous AGN studies at millimeter wavelengths. We find that blazars are more strongly polarized than other AGNs in the sample, while exhibiting (on average) order-of-magnitude lower RM values, consistent with the AGN viewing angle unification scheme. For Sgr A* we report a mean RM of (−4.2 ± 0.3) × 105 rad m−2 at 1.3 mm, consistent with measurements over the past decade and, for the first time, an RM of (–2.1 ± 0.1) × 105 rad m−2 at 3 mm, suggesting that about half of the Faraday rotation at 1.3 mm may occur between the 3 mm photosphere and the 1.3 mm source. We also report the first unambiguous measurement of RM toward the M87 nucleus at millimeter wavelengths, which undergoes significant changes in magnitude and sign reversals on a one year timescale, spanning the range from −1.2 to 0.3 × 105 rad m−2 at 3 mm and −4.1 to 1.5 × 105 rad m−2 at 1.3 mm. Given this time variability, we argue that, unlike the case of Sgr A*, the RM in M87 does not provide an accurate estimate of the mass accretion rate onto the black hole. We put forward a two-component model, comprised of a variable compact region and a static extended region, that can simultaneously explain the polarimetric properties observed by both the EHT (on horizon scales) and ALMA (which observes the combined emission from both components). These measurements provide critical constraints for the calibration, analysis, and interpretation of simultaneously obtained VLBI data with the EHT and GMVA."
WON M LEE,"First Sagittarius A* Event Horizon Telescope results. IV. Variability, morphology, and black hole mass","In this paper we quantify the temporal variability and image morphology of the horizon-scale emission from Sgr A*, as observed by the EHT in 2017 April at a wavelength of 1.3 mm. We find that the Sgr A* data exhibit variability that exceeds what can be explained by the uncertainties in the data or by the effects of interstellar scattering. The magnitude of this variability can be a substantial fraction of the correlated flux density, reaching ∼100% on some baselines. Through an exploration of simple geometric source models, we demonstrate that ring-like morphologies provide better fits to the Sgr A* data than do other morphologies with comparable complexity. We develop two strategies for fitting static geometric ring models to the time-variable Sgr A* data; one strategy fits models to short segments of data over which the source is static and averages these independent fits, while the other fits models to the full data set using a parametric model for the structural variability power spectrum around the average source structure. Both geometric modeling and image-domain feature extraction techniques determine the ring diameter to be 51.8 ± 2.3 μas (68% credible intervals), with the ring thickness constrained to have an FWHM between ∼30% and 50% of the ring diameter. To bring the diameter measurements to a common physical scale, we calibrate them using synthetic data generated from GRMHD simulations. This calibration constrains the angular size of the gravitational radius to be 4.8_-0.7^+1.4 μas, which we combine with an independent distance measurement from maser parallaxes to determine the mass of Sgr A* to be 4.0_-0.6^+10^6 M⊙."
WON M LEE,"First Sagittarius A* Event Horizon Telescope results. II. EHT and multiwavelength observations, data processing, and calibration","We present Event Horizon Telescope (EHT) 1.3 mm measurements of the radio source located at the position of the supermassive black hole Sagittarius A* (Sgr A*), collected during the 2017 April 5–11 campaign. The observations were carried out with eight facilities at six locations across the globe. Novel calibration methods are employed to account for Sgr A*'s flux variability. The majority of the 1.3 mm emission arises from horizon scales, where intrinsic structural source variability is detected on timescales of minutes to hours. The effects of interstellar scattering on the image and its variability are found to be subdominant to intrinsic source structure. The calibrated visibility amplitudes, particularly the locations of the visibility minima, are broadly consistent with a blurred ring with a diameter of ∼50 μas, as determined in later works in this series. Contemporaneous multiwavelength monitoring of Sgr A* was performed at 22, 43, and 86 GHz and at near-infrared and X-ray wavelengths. Several X-ray flares from Sgr A* are detected by Chandra, one at low significance jointly with Swift on 2017 April 7 and the other at higher significance jointly with NuSTAR on 2017 April 11. The brighter April 11 flare is not observed simultaneously by the EHT but is followed by a significant increase in millimeter flux variability immediately after the X-ray outburst, indicating a likely connection in the emission physics near the event horizon. We compare Sgr A*’s broadband flux during the EHT campaign to its historical spectral energy distribution and find that both the quiescent emission and flare emission are consistent with its long-term behavior."
WON M LEE,Broadband multi-wavelength properties of M87 during the 2017 Event Horizon Telescope campaign,"In 2017, the Event Horizon Telescope (EHT) Collaboration succeeded in capturing the first direct image of the center of the M87 galaxy. The asymmetric ring morphology and size are consistent with theoretical expectations for a weakly accreting supermassive black hole of mass ∼6.5 × 109 M ⊙. The EHTC also partnered with several international facilities in space and on the ground, to arrange an extensive, quasi-simultaneous multi-wavelength campaign. This Letter presents the results and analysis of this campaign, as well as the multi-wavelength data as a legacy data repository. We captured M87 in a historically low state, and the core flux dominates over HST-1 at high energies, making it possible to combine core flux constraints with the more spatially precise very long baseline interferometry data. We present the most complete simultaneous multi-wavelength spectrum of the active nucleus to date, and discuss the complexity and caveats of combining data from different spatial scales into one broadband spectrum. We apply two heuristic, isotropic leptonic single-zone models to provide insight into the basic source properties, but conclude that a structured jet is necessary to explain M87’s spectrum. We can exclude that the simultaneous γ-ray emission is produced via inverse Compton emission in the same region producing the EHT mm-band emission, and further conclude that the γ-rays can only be produced in the inner jets (inward of HST-1) if there are strongly particle-dominated regions. Direct synchrotron emission from accelerated protons and secondaries cannot yet be excluded."
WON M LEE,Event Horizon Telescope imaging of the archetypal blazar 3C 279 at an extreme 20 microarcsecond resolution,"3C 279 is an archetypal blazar with a prominent radio jet that show broadband flux density variability across the entire electromagnetic spectrum. We use an ultra-high angular resolution technique – global Very Long Baseline Interferometry (VLBI) at 1.3 mm (230 GHz) – to resolve the innermost jet of 3C 279 in order to study its fine-scale morphology close to the jet base where highly variable γ-ray emission is thought to originate, according to various models. The source was observed during four days in April 2017 with the Event Horizon Telescope at 230 GHz, including the phased Atacama Large Millimeter/submillimeter Array (ALMA), at an angular resolution of ∼20 μas (at a redshift of z = 0.536 this corresponds to ∼0.13 pc  ∼ 1700 Schwarzschild radii with a black hole mass M<jats:sub>BH</jats:sub> = 8 × 10^8 M_⊙). Imaging and model-fitting techniques were applied to the data to parameterize the fine-scale source structure and its variation. We find a multicomponent inner jet morphology with the northernmost component elongated perpendicular to the direction of the jet, as imaged at longer wavelengths. The elongated nuclear structure is consistent on all four observing days and across different imaging methods and model-fitting techniques, and therefore appears robust. Owing to its compactness and brightness, we associate the northern nuclear structure as the VLBI “core”. This morphology can be interpreted as either a broad resolved jet base or a spatially bent jet. We also find significant day-to-day variations in the closure phases, which appear most pronounced on the triangles with the longest baselines. Our analysis shows that this variation is related to a systematic change of the source structure. Two inner jet components move non-radially at apparent speeds of ∼15 c and ∼20 c (∼1.3 and ∼1.7 μas day^−1, respectively), which more strongly supports the scenario of traveling shocks or instabilities in a bent, possibly rotating jet. The observed apparent speeds are also coincident with the 3C 279 large-scale jet kinematics observed at longer (cm) wavelengths, suggesting no significant jet acceleration between the 1.3 mm core and the outer jet. The intrinsic brightness temperature of the jet components are ≲10^10 K, a magnitude or more lower than typical values seen at ≥7 mm wavelengths. The low brightness temperature and morphological complexity suggest that the core region of 3C 279 becomes optically thin at short (mm) wavelengths."
WON M LEE,First M87 Event Horizon Telescope results. II. Array and instrumentation,"The Event Horizon Telescope (EHT) is a very long baseline interferometry (VLBI) array that comprises millimeter- and submillimeter-wavelength telescopes separated by distances comparable to the diameter of the Earth. At a nominal operating wavelength of ~1.3 mm, EHT angular resolution (λ/D) is ~25 μas, which is sufficient to resolve nearby supermassive black hole candidates on spatial and temporal scales that correspond to their event horizons. With this capability, the EHT scientific goals are to probe general relativistic effects in the strong-field regime and to study accretion and relativistic jet formation near the black hole boundary. In this Letter we describe the system design of the EHT, detail the technology and instrumentation that enable observations, and provide measures of its performance. Meeting the EHT science objectives has required several key developments that have facilitated the robust extension of the VLBI technique to EHT observing wavelengths and the production of instrumentation that can be deployed on a heterogeneous array of existing telescopes and facilities. To meet sensitivity requirements, high-bandwidth digital systems were developed that process data at rates of 64 gigabit s−1, exceeding those of currently operating cm-wavelength VLBI arrays by more than an order of magnitude. Associated improvements include the development of phasing systems at array facilities, new receiver installation at several sites, and the deployment of hydrogen maser frequency standards to ensure coherent data capture across the array. These efforts led to the coordination and execution of the first Global EHT observations in 2017 April, and to event-horizon-scale imaging of the supermassive black hole candidate in M87."
WON M LEE,First M87 Event Horizon Telescope results. I. The shadow of the supermassive black hole,"When surrounded by a transparent emission region, black holes are expected to reveal a dark shadow caused by gravitational light bending and photon capture at the event horizon. To image and study this phenomenon, we have assembled the Event Horizon Telescope, a global very long baseline interferometry array observing at a wavelength of 1.3 mm. This allows us to reconstruct event-horizon-scale images of the supermassive black hole candidate in the center of the giant elliptical galaxy M87. We have resolved the central compact radio source as an asymmetric bright emission ring with a diameter of 42 ± 3 μas, which is circular and encompasses a central depression in brightness with a flux ratio gsim10:1. The emission ring is recovered using different calibration and imaging schemes, with its diameter and width remaining stable over four different observations carried out in different days. Overall, the observed image is consistent with expectations for the shadow of a Kerr black hole as predicted by general relativity. The asymmetry in brightness in the ring can be explained in terms of relativistic beaming of the emission from a plasma rotating close to the speed of light around a black hole. We compare our images to an extensive library of ray-traced general-relativistic magnetohydrodynamic simulations of black holes and derive a central mass of M = (6.5 ± 0.7) × 10^9 M ⊙. Our radio-wave observations thus provide powerful evidence for the presence of supermassive black holes in centers of galaxies and as the central engines of active galactic nuclei. They also present a new tool to explore gravity in its most extreme limit and on a mass scale that was so far not accessible."
WON M LEE,First Sagittarius A* Event Horizon Telescope results. III. Imaging of the Galactic center supermassive black hole,"We present the first event-horizon-scale images and spatiotemporal analysis of Sgr A* taken with the Event Horizon Telescope in 2017 April at a wavelength of 1.3 mm. Imaging of Sgr A* has been conducted through surveys over a wide range of imaging assumptions using the classical CLEAN algorithm, regularized maximum likelihood methods, and a Bayesian posterior sampling method. Different prescriptions have been used to account for scattering effects by the interstellar medium toward the Galactic center. Mitigation of the rapid intraday variability that characterizes Sgr A* has been carried out through the addition of a “variability noise budget” in the observed visibilities, facilitating the reconstruction of static full-track images. Our static reconstructions of Sgr A* can be clustered into four representative morphologies that correspond to ring images with three different azimuthal brightness distributions and a small cluster that contains diverse nonring morphologies. Based on our extensive analysis of the effects of sparse (u, v)-coverage, source variability, and interstellar scattering, as well as studies of simulated visibility data, we conclude that the Event Horizon Telescope Sgr A* data show compelling evidence for an image that is dominated by a bright ring of emission with a ring diameter of ∼50 μas, consistent with the expected “shadow” of a 4 × 106 M⊙ black hole in the Galactic center located at a distance of 8 kpc."
WON M LEE,Characterizing and mitigating intraday variability: reconstructing source structure in accreting black holes with mm-VLBI,"The extraordinary physical resolution afforded by the Event Horizon Telescope has opened a window onto the astrophysical phenomena unfolding on horizon scales in two known black holes, M87* and Sgr A*. However, with this leap in resolution has come a new set of practical complications. Sgr A* exhibits intraday variability that violates the assumptions underlying Earth aperture synthesis, limiting traditional image reconstruction methods to short timescales and data sets with very sparse (u, v) coverage. We present a new set of tools to detect and mitigate this variability. We develop a data-driven, model-agnostic procedure to detect and characterize the spatial structure of intraday variability. This method is calibrated against a large set of mock data sets, producing an empirical estimator of the spatial power spectrum of the brightness fluctuations. We present a novel Bayesian noise modeling algorithm that simultaneously reconstructs an average image and statistical measure of the fluctuations about it using a parameterized form for the excess variance in the complex visibilities not otherwise explained by the statistical errors. These methods are validated using a variety of simulated data, including general relativistic magnetohydrodynamic simulations appropriate for Sgr A* and M87*. We find that the reconstructed source structure and variability are robust to changes in the underlying image model. We apply these methods to the 2017 EHT observations of M87*, finding evidence for variability across the EHT observing campaign. The variability mitigation strategies presented are widely applicable to very long baseline interferometry observations of variable sources generally, for which they provide a data-informed averaging procedure and natural characterization of inter-epoch image consistency."
WON M LEE,The polarized image of a synchrotron-emitting ring of gas orbiting a black hole,"Synchrotron radiation from hot gas near a black hole results in a polarized image. The image polarization is determined by effects including the orientation of the magnetic field in the emitting region, relativistic motion of the gas, strong gravitational lensing by the black hole, and parallel transport in the curved spacetime. We explore these effects using a simple model of an axisymmetric, equatorial accretion disk around a Schwarzschild black hole. By using an approximate expression for the null geodesics derived by Beloborodov and conservation of the Walker–Penrose constant, we provide analytic estimates for the image polarization. We test this model using currently favored general relativistic magnetohydrodynamic simulations of M87*, using ring parameters given by the simulations. For a subset of these with modest Faraday effects, we show that the ring model broadly reproduces the polarimetric image morphology. Our model also predicts the polarization evolution for compact flaring regions, such as those observed from Sgr A* with GRAVITY. With suitably chosen parameters, our simple model can reproduce the EVPA pattern and relative polarized intensity in Event Horizon Telescope images of M87*. Under the physically motivated assumption that the magnetic field trails the fluid velocity, this comparison is consistent with the clockwise rotation inferred from total intensity images."
WON M LEE,First Sagittarius A* Event Horizon Telescope results. I. The shadow of the supermassive black hole in the center of the Milky Way,"We present the first Event Horizon Telescope (EHT) observations of Sagittarius A* (Sgr A*), the Galactic center source associated with a supermassive black hole. These observations were conducted in 2017 using a global interferometric array of eight telescopes operating at a wavelength of λ = 1.3 mm. The EHT data resolve a compact emission region with intrahour variability. A variety of imaging and modeling analyses all support an image that is dominated by a bright, thick ring with a diameter of 51.8 ± 2.3 μas (68% credible interval). The ring has modest azimuthal brightness asymmetry and a comparatively dim interior. Using a large suite of numerical simulations, we demonstrate that the EHT images of Sgr A* are consistent with the expected appearance of a Kerr black hole with mass ∼4 × 106 M ⊙, which is inferred to exist at this location based on previous infrared observations of individual stellar orbits, as well as maser proper-motion studies. Our model comparisons disfavor scenarios where the black hole is viewed at high inclination (i &gt; 50°), as well as nonspinning black holes and those with retrograde accretion disks. Our results provide direct evidence for the presence of a supermassive black hole at the center of the Milky Way, and for the first time we connect the predictions from dynamical measurements of stellar orbits on scales of 103–105 gravitational radii to event-horizon-scale images and variability. Furthermore, a comparison with the EHT results for the supermassive black hole M87* shows consistency with the predictions of general relativity spanning over three orders of magnitude in central mass."
WON M LEE,SYMBA: an end-to-end VLBI synthetic data generation pipeline,"CONTEXT: Realistic synthetic observations of theoretical source models are essential for our understanding of real observational data. In using synthetic data, one can verify the extent to which source parameters can be recovered and evaluate how various data corruption effects can be calibrated. These studies are the most important when proposing observations of new sources, in the characterization of the capabilities of new or upgraded instruments, and when verifying model-based theoretical predictions in a direct comparison with observational data. AIMS: We present the SYnthetic Measurement creator for long Baseline Arrays (SYMBA), a novel synthetic data generation pipeline for Very Long Baseline Interferometry (VLBI) observations. SYMBA takes into account several realistic atmospheric, instrumental, and calibration effects. METHODS: We used SYMBA to create synthetic observations for the Event Horizon Telescope (EHT), a millimetre VLBI array, which has recently captured the first image of a black hole shadow. After testing SYMBA with simple source and corruption models, we study the importance of including all corruption and calibration effects, compared to the addition of thermal noise only. Using synthetic data based on two example general relativistic magnetohydrodynamics (GRMHD) model images of M 87, we performed case studies to assess the image quality that can be obtained with the current and future EHT array for different weather conditions. RESULTS: Our synthetic observations show that the effects of atmospheric and instrumental corruptions on the measured visibilities are significant. Despite these effects, we demonstrate how the overall structure of our GRMHD source models can be recovered robustly with the EHT2017 array after performing calibration steps, which include fringe fitting, a priori amplitude and network calibration, and self-calibration. With the planned addition of new stations to the EHT array in the coming years, images could be reconstructed with higher angular resolution and dynamic range. In our case study, these improvements allowed for a distinction between a thermal and a non-thermal GRMHD model based on salient features in reconstructed images."
THOMAS C MOORE,CD4+ and CD8+ T cells and antibodies are associated with protection against Delta vaccine breakthrough infection: a nested case-control study within the PITCH study,"Defining correlates of protection against severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) vaccine breakthrough infection informs vaccine policy for booster doses and future vaccine designs. Existing studies demonstrate humoral correlates of protection, but the role of T cells in protection is still unclear. In this study, we explore antibody and T cell immune responses associated with protection against Delta variant vaccine breakthrough infection in a well-characterized cohort of UK Healthcare Workers (HCWs). We demonstrate evidence to support a role for CD4+ and CD8+ T cells as well as antibodies against Delta vaccine breakthrough infection. In addition, our results suggest a potential role for cross-reactive T cells in vaccine breakthrough."
THOMAS C MOORE,Broadband multi-wavelength properties of M87 during the 2017 Event Horizon Telescope campaign,"In 2017, the Event Horizon Telescope (EHT) Collaboration succeeded in capturing the first direct image of the center of the M87 galaxy. The asymmetric ring morphology and size are consistent with theoretical expectations for a weakly accreting supermassive black hole of mass ∼6.5 × 109 M ⊙. The EHTC also partnered with several international facilities in space and on the ground, to arrange an extensive, quasi-simultaneous multi-wavelength campaign. This Letter presents the results and analysis of this campaign, as well as the multi-wavelength data as a legacy data repository. We captured M87 in a historically low state, and the core flux dominates over HST-1 at high energies, making it possible to combine core flux constraints with the more spatially precise very long baseline interferometry data. We present the most complete simultaneous multi-wavelength spectrum of the active nucleus to date, and discuss the complexity and caveats of combining data from different spatial scales into one broadband spectrum. We apply two heuristic, isotropic leptonic single-zone models to provide insight into the basic source properties, but conclude that a structured jet is necessary to explain M87’s spectrum. We can exclude that the simultaneous γ-ray emission is produced via inverse Compton emission in the same region producing the EHT mm-band emission, and further conclude that the γ-rays can only be produced in the inner jets (inward of HST-1) if there are strongly particle-dominated regions. Direct synchrotron emission from accelerated protons and secondaries cannot yet be excluded."
THOMAS C MOORE,"Weight, Blood Pressure, and Dietary Benefits After 12 Months of a Web-based Nutrition Education Program (DASH for Health): Longitudinal Observational Study","BACKGROUND The dietary habits of Americans are creating serious health concerns, including obesity, hypertension, diabetes, cardiovascular disease, and even some types of cancer. While considerable attention has been focused on calorie reduction and weight loss, approaches are needed that will not only help the population reduce calorie intake but also consume the type of healthy, well-balanced diet that would prevent this array of medical complications. OBJECTIVE To design an Internet-based nutrition education program and to explore its effect on weight, blood pressure, and eating habits after 12 months of participation. METHODS. We designed the DASH for Health program to provide weekly articles about healthy nutrition via the Internet. Dietary advice was based on the DASH diet (Dietary Approaches to Stop Hypertension). The program was offered as a free benefit to the employees of EMC Corporation, and 2834 employees and spouses enrolled. Enrollees voluntarily entered information about themselves on the website (food intake), and we used these self-entered data to determine if the program had any effect. Analyses were based upon the change in weight, blood pressure, and food intake between the baseline period (before the DASH program began) and the 12th month. To be included in an outcome, a subject had to have provided both a baseline and 12th-month entry. RESULTS After 12 months, 735 of 2834 original enrollees (26%) were still actively using the program. For subjects who were overweight/obese (body mass index >25; n = 151), weight change at 12 months was -4.2 lbs (95% CI: -2.2, -6.2; P< .001). For subjects with hypertension or prehypertension at baseline (n = 62), systolic blood pressure fell 6.8 mmHg at 12 months (CI: -2.6, -11.0; P<.001; n = 62). Diastolic pressure fell 2.1 mmHg (P = .16). Based upon self-entered food surveys, enrollees (n = 181) at 12 months were eating significantly more fruits, more vegetables, and fewer grain products. They also reduced consumption of carbonated beverages. Enrollees who had visited the website more often tended to have greater blood pressure and weight loss effect, suggesting that use of the DASH for Health program was at least partially responsible for the benefits we observed. CONCLUSIONS We have found that continued use of a nutrition education program delivered totally via the Internet, with no person-to-person contact with health professionals, is associated with significant weight loss, blood pressure lowering, and dietary improvements after 12 months. Effective programs like DASH for Health, delivered via the Internet, can provide benefit to large numbers of subjects at low cost and may help address the nutritional public health crisis."
THOMAS C MOORE,Taking the pulse of Earth's tropical forests using networks of highly distributed plots,"Tropical forests are the most diverse and productive ecosystems on Earth. While better understanding of these forests is critical for our collective future, until quite recently efforts to measure and monitor them have been largely disconnected. Networking is essential to discover the answers to questions that transcend borders and the horizons of funding agencies. Here we show how a global community is responding to the challenges of tropical ecosystem research with diverse teams measuring forests tree-by-tree in thousands of long-term plots. We review the major scientific discoveries of this work and show how this process is changing tropical forest science. Our core approach involves linking long-term grassroots initiatives with standardized protocols and data management to generate robust scaled-up results. By connecting tropical researchers and elevating their status, our Social Research Network model recognises the key role of the data originator in scientific discovery. Conceived in 1999 with RAINFOR (South America), our permanent plot networks have been adapted to Africa (AfriTRON) and Southeast Asia (T-FORCES) and widely emulated worldwide. Now these multiple initiatives are integrated via ForestPlots.net cyber-infrastructure, linking colleagues from 54 countries across 24 plot networks. Collectively these are transforming understanding of tropical forests and their biospheric role. Together we have discovered how, where and why forest carbon and biodiversity are responding to climate change, and how they feedback on it. This long-term pan-tropical collaboration has revealed a large long-term carbon sink and its trends, as well as making clear which drivers are most important, which forest processes are affected, where they are changing, what the lags are, and the likely future responses of tropical forests as the climate continues to change. By leveraging a remarkably old technology, plot networks are sparking a very modern revolution in tropical forest science. In the future, humanity can benefit greatly by nurturing the grassroots communities now collectively capable of generating unique, long-term understanding of Earth's most precious forests."
THOMAS C MOORE,A reporting format for leaf-level gas exchange data and metadata,"Leaf-level gas exchange data support the mechanistic understanding of plant fluxes of carbon and water. These fluxes inform our understanding of ecosystem function, are an important constraint on parameterization of terrestrial biosphere models, are necessary to understand the response of plants to global environmental change, and are integral to efforts to improve crop production. Collection of these data using gas analyzers can be both technically challenging and time consuming, and individual studies generally focus on a small range of species, restricted time periods, or limited geographic regions. The high value of these data is exemplified by the many publications that reuse and synthesize gas exchange data, however the lack of metadata and data reporting conventions make full and efficient use of these data difficult. Here we propose a reporting format for leaf-level gas exchange data and metadata to provide guidance to data contributors on how to store data in repositories to maximize their discoverability, facilitate their efficient reuse, and add value to individual datasets. For data users, the reporting format will better allow data repositories to optimize data search and extraction, and more readily integrate similar data into harmonized synthesis products. The reporting format specifies data table variable naming and unit conventions, as well as metadata characterizing experimental conditions and protocols. For common data types that were the focus of this initial version of the reporting format, i.e., survey measurements, dark respiration, carbon dioxide and light response curves, and parameters derived from those measurements, we took a further step of defining required additional data and metadata that would maximize the potential reuse of those data types. To aid data contributors and the development of data ingest tools by data repositories we provided a translation table comparing the outputs of common gas exchange instruments. Extensive consultation with data collectors, data users, instrument manufacturers, and data scientists was undertaken in order to ensure that the reporting format met community needs. The reporting format presented here is intended to form a foundation for future development that will incorporate additional data types and variables as gas exchange systems and measurement approaches advance in the future. The reporting format is published in the U.S. Department of Energy's ESS-DIVE data repository, with documentation and future development efforts being maintained in a version control system."
THOMAS C MOORE,The long-baseline neutrino experiment: exploring fundamental symmetries of the universe,"The preponderance of matter over antimatter in the early Universe, the dynamics of the supernova bursts that produced the heavy elements necessary for life and whether protons eventually decay --- these mysteries at the forefront of particle physics and astrophysics are key to understanding the early evolution of our Universe, its current state and its eventual fate. The Long-Baseline Neutrino Experiment (LBNE) represents an extensively developed plan for a world-class experiment dedicated to addressing these questions. LBNE is conceived around three central components: (1) a new, high-intensity neutrino source generated from a megawatt-class proton accelerator at Fermi National Accelerator Laboratory, (2) a near neutrino detector just downstream of the source, and (3) a massive liquid argon time-projection chamber deployed as a far detector deep underground at the Sanford Underground Research Facility. This facility, located at the site of the former Homestake Mine in Lead, South Dakota, is approximately 1,300 km from the neutrino source at Fermilab -- a distance (baseline) that delivers optimal sensitivity to neutrino charge-parity symmetry violation and mass ordering effects. This ambitious yet cost-effective design incorporates scalability and flexibility and can accommodate a variety of upgrades and contributions. With its exceptional combination of experimental configuration, technical capabilities, and potential for transformative discoveries, LBNE promises to be a vital facility for the field of particle physics worldwide, providing physicists from around the globe with opportunities to collaborate in a twenty to thirty year program of exciting science. In this document we provide a comprehensive overview of LBNE's scientific objectives, its place in the landscape of neutrino physics worldwide, the technologies it will incorporate and the capabilities it will possess."
JOHN R WEINSTEIN,"BMQ : Boston medical quarterly: v. 3, no. 1-4",
JOHN R WEINSTEIN,"Centerscope: v. 6, no. 1-6",
JOHN R WEINSTEIN,Broadband multi-wavelength properties of M87 during the 2017 Event Horizon Telescope campaign,"In 2017, the Event Horizon Telescope (EHT) Collaboration succeeded in capturing the first direct image of the center of the M87 galaxy. The asymmetric ring morphology and size are consistent with theoretical expectations for a weakly accreting supermassive black hole of mass ∼6.5 × 109 M ⊙. The EHTC also partnered with several international facilities in space and on the ground, to arrange an extensive, quasi-simultaneous multi-wavelength campaign. This Letter presents the results and analysis of this campaign, as well as the multi-wavelength data as a legacy data repository. We captured M87 in a historically low state, and the core flux dominates over HST-1 at high energies, making it possible to combine core flux constraints with the more spatially precise very long baseline interferometry data. We present the most complete simultaneous multi-wavelength spectrum of the active nucleus to date, and discuss the complexity and caveats of combining data from different spatial scales into one broadband spectrum. We apply two heuristic, isotropic leptonic single-zone models to provide insight into the basic source properties, but conclude that a structured jet is necessary to explain M87’s spectrum. We can exclude that the simultaneous γ-ray emission is produced via inverse Compton emission in the same region producing the EHT mm-band emission, and further conclude that the γ-rays can only be produced in the inner jets (inward of HST-1) if there are strongly particle-dominated regions. Direct synchrotron emission from accelerated protons and secondaries cannot yet be excluded."
SANGWON JUNG,Judging food by its description: a text mining approach examining the most influential words on restaurant menus,
SANGWON JUNG,Have restaurant firms been using right recession turnaround strategies?: Evaluating with propensity score measure,"Among the diverse strategies that restaurants use in recessions, some studies have shown that strategies that increase advertising, profit margins, or asset turnover have yielded promising results in terms of firm performance. However, the success of these turnaround strategies might be due to the health or size of a firm rather than the implementation of these strategies. Therefore, this study empirically tested this question utilizing the propensity score measure (PSM) due to concerns with selection bias across restaurant segments. The results showed significant improvements in revenue for limited-service and franchise restaurants when aggressive advertising was used but no improvements in profitability. The profit margin strategy had no impact on revenue but affected profitability and stock returns positively for all segments. Finally, the asset turnover strategy had adverse effects on revenue the year after a recession for all segments. These mixed results suggest that managers need to be cautious when implementing recession turnaround strategies."
ANNA BELKINA,viSNE fine-tuning enables better resolution of cell populations,"t-Distributed Stochastic Neighbor Embedding (t-SNE or viSNE) is a dimensionality reduction algorithm that allows visualization of complex high-dimensional cytometry data as a two-dimensional distribution or "" map "". These maps can be interrogated by human-guided or automated techniques to categorize single cell data into relevant biological populations and otherwise visualize important differences between samples. The method has been extensively adopted and reported in the literature to be superior to traditional biaxial gating. The analyst must carefully choose the parameters of a t-SNE computation, as incorrectly chosen parameters might create artifacts that make the resulting map difficult or impossible to interpret. The correct choice of algorithm parameters is complicated by a lack of agreed-upon quantitative framework for assessing the quality of algorithm results. Gauging result quality currently relies on subjective visual evaluation by an experienced t-SNE user. To overcome these limitations, we used Cytobank viSNE engine for all t-SNE analyses and employed 18-parameter flow cytometry data as well as 32-parameter mass cytometry data of varying numbers of events to optimize t-SNE parameters such as total number of iterations and perplexity. We also investigated the utility of Kullback-Liebler (KL) divergence as a metric for map quality as well as SPADE clustering as an indirect measure of multidimensional data integrity when flattened into t-SNE coordinates. We have established the imperative requirement for the number of t-SNE analysis optimization steps ('iteration number') to be scaled with the total number of data points (events) in the set, suggesting that a number of existing software solutions produce unclear t-SNE maps of flow and mass cytometry data due to built-in user control restrictions. We also evaluated lower-level parameters within the t-SNE code that control the 'early exaggeration' stage initially introduced into t-SNE algorithm for better map optimization. These parameters are not available as part of the standard algorithm interface, but we found that they can be tuned to produce high quality results in shorter periods of time, avoiding unnecessary increases of both analysis duration and computation cost. Therefore, our approach allows to fine-tune the t-SNE analysis to ensure both optimal resolution of t-SNE low-dimensional maps and better faithfulness of their presentation of high-parameter cytometry data."
ANNA BELKINA,Double bromodomain proteins as regulators of lymphopoiesis and inflammation,"Histone post-translational modifications are essential for the regulation of gene expression. These modifications are recognized by chromatin ""readers"" that control gene co-activation and co-repression through intrinsic or recruited enzymatic activity. Deregulated function of chromatin-binding proteins is associated with immune diseases and cancer. Bromodomains are highly conserved protein structures that bind acetylated lysines of nucleosomal histones. Among the diverse group of all bromodomain-containing proteins, the members of BET (bromodomain extra terminal) family- Brd2, Brd3, Brd4 and Brdt - are mutually homologous and feature tandem bromodomains that bind acetylated histones H3 and H4. BET proteins recruit diverse transcriptional machinery to gene promoters, including chromatin ""writers"", ""erasers"" and transcription elongation complexes. Lack of model systems hampers molecular understanding of BET protein function; because eukaryotic BET proteins and their orthologs are essential, knockouts are lethal. However, we previously reported a Brd2 whole-body knockdown. These 'brd2 lo' mice show reduced inflammation, while Brd2 transgenic mice develop an aggressive B-cell lymphoma. Based on these data, we hypothesized that BET proteins regulate proliferation of immune cells and inflammatory responses. We transduced hematopoietic stem cells with Brd2-expressing lentivirus, engrafted the cells in irradiated mice and found that Brd2 expression expands donor-derived B cells and promotes B-cell mitogenic responsiveness in the periphery through cyclin A upregulation, whereas Brd2 knockdown in hematopoietic cells ablates hematopoiesis. In macrophages, Brd2 knockdown diminishes inflammatory responses; the same effect was observed with Brd3 and Brd4. We found that BET proteins physically associate with promoter chromatin of cytokine genes. This association is disrupted by JQ1, a novel small molecule inhibitor of BET protein function, which competes with acetylated lysine for binding the bromodomain pocket of BET proteins. As expected, JQ1 displaces BET proteins from cytokine genes, thereby ablating cytokine production in vitro. Likewise, in an in vivo model of endotoxemia, JQ1 dramatically reduces pro-inflammatory cytokines in serum and protects mice from death. Considered together, these findings reveal how Brd2 expression skews hematopoiesis and couples inflammatory signal transduction to chromatin status. Targeting BET proteins with small molecule therapeutics may benefit patients with hematologic malignancies and inflammatory diseases."
ANNA BELKINA,Automated analysis of 16-color polychromatic flow cytometry data maps immune cell populations and reveals a distinct inhibitory receptor signature in systemic sclerosis,"Background. The phenotypic profiles of both peripheral blood and tissue-resident immune cells have been linked to the health status of individuals with infectious and autoimmune diseases, as well as cancer. In light of the promising clinical trial results of agents that block the Inhibitory Receptor (IR) Programmed Death 1 (PD-1) axis, novel flow cytometric panels that simultaneously measure multiple IRs on several immune cell subsets could provide the distinct IR signatures to target in combinational therapies for many disease states. Also, due to the paucity of human samples, larger (14+ color) ‘1-tube’ panels for immune cell characterization ex vivo are of a high value in translational studies. Development of fluorescent-based panels offer several advantages as compared with analogous mass cytometric methods, including the ability to sort multiple populations of interest from the sample for further study. However, automated platforms of multi-dimensional single cell analysis that allow objective and comprehensive population characterization are severely underutilized on data generated from large polychromatic panels. Methods. A 16-color flow cytometry (FCM) panel was developed and optimized for the simultaneous characterization and purification of multiple human immune cell populations on a 4- laser BD FACSARIA II cell sorter. FCM data of samples obtained from healthy subjects and individuals with systemic sclerosis (SSc) were loaded into Cytobank cloud, then compensated and analyzed with SPADE clustering algorithm. The viSNE algorithm was also employed to compress the data into a 2D map of phenotypic space that was subsequently clustered using SPADE. For comparison, the FCM data were also analyzed manually using FlowJo software. Results. Our novel 16-color panel recognizes CD3, CD4, CD8, CD45RO, CD25, CD127, CD16, CD56, γδTCR, vα24, PD-1, LAG-3, CTLA-4, and TIM-3; it also contains a CD1d-tetramer and a live-dead dye (with CD19 and CD14 included as a combined dump channel). This panel allows combinational IR signatures to be determined from CD4+ T, CD8+ T, Natural Killer (NK), invariant Natural Killer (iNKT), and gamma delta (γδ) immune cell subsets within one sample. We have successfully identified all subsets of interest using automatic SPADE and viSNE algorithms integrated into Cytobank services, and demonstrated a distinctive phenotype of IR distribution on healthy versus systemic sclerosis subject groups. Conclusions. Methods of automatic analysis that were originally developed for processing multi-dimensional mass cytometry can be applied to polychromatic FCM datasets and provide robust results, including subset identification and distinct IR signatures in healthy compared to diseased subject groups."
ANNA BELKINA,Reagent-driven reconfiguration/optimization of a 16-parameter BD FACSARIA II SORP to allow accurate detection of violet- and UV-excited Sirigen dyes,"Background. Modern flow cytometers can detect emission from a variety of commercially available fluorescent reagents. However, accurate detection of novel dyes is often difficult due to the lack of readily available quality assurance tools, for instrument manufacturer QC and optimization protocols often become available secondary to new fluorescent reagents. Aims. For standard QC and instrument calibration of the FACSARIA, BD provides a standard 'CS&T' method that includes three-peak beads and a dedicated software module. While this method allows tracking instrument state over time, it does not accurately access PMT performance on a significant part of the spectrum for violet-or UV-excited dyes. In order to ensure accurate detection of reagents within all 16 channels of the Boston University Flow Core 16-color, 4-laser BD FACSARIA SORP, we created and performed an novel optimization process that allows simultaneous accommodation of as many as nine polymeric Sirigen dyes, including those emitting in both long-wavelength violet-and UV-laser excited channels. Methods and Results. Firstly, the electronic noise of all PMTs was assessed and information was collected on rSD of non-stained cells within 100-800V range. The derived basal PMT values then provided a starting point for voltage optimization of instrument-specific panels. These values differed greatly from CS&T-deduced PMT voltages for abovementioned channels, for some the CS&T calculation of optimal PMT voltage was not possible due to a poor resolution of CS&T peaks at certain wavelengths. Several PMTs were identified with sub-par performance and were consequently replaced. Our testing of multiple commercial compensation beads found that the majority demonstrated prohibitively high backgrounds; the eBioscience UltraComp beads performed best and were therefore our reagent of choice. For instrument performance tracking, we also compared multi-peak beads from several manufacturers and found Spherotech Ultra Rainbow beads to be the sole bead type with satisfactory resolution of all peaks on long-wavelength UV channels. Finally, we developed an ergonomic protocol for facility users that includes an experiment template and electronic tables for data processing. With that protocol, a user can: (1) finely tune PMT voltages to accommodate a specific panel, (2) determine antibody concentrations for compensation control preparations, and (3) associate these optimized settings with multi-peak bead target values. Such preliminary setup allows quick panel-specific instrument calibration for each experimental run. This approach was successfully applied to several 16-color panels used in our Core facility and resulted in vastly improved reproducibility of acquired data over months of use. Conclusions. Synchronizing cutting-edge reagent technologies with existing instrument QC and maintenance methodology requires development of mix-and-match solutions not necessarily provided by the instrument manufacturer. Creating a user-friendly, accurate QC and calibration protocol that accommodates novel reagents allows dramatic expansion of our userbase's experimental capabilities."
ANNA BELKINA,Evaluation of ‘Super Bright’ polymer dyes in 13-16-color human immunophenotyping panels,"Sirigen Group Limited developed unique polymer 'Brilliant' dyes that have become a staple of modern multicolor panel design. Polymer-based conjugates are often 4-10 times brighter than conventional fluorochromes with similar excitation/emission parameters. A new group of polymer fluorochromes, the 'Super Bright' dyes, was recently launched by eBioscience. The performance of these new dyes in large polychromatic panels is unclear to date. Therefore, we tested several preparations of the Super Bright dyes (such as Super Bright 436 and Super Bright 600) in two polychromatic fluorescent panels (one 13-and one 16-color). Specifically, we evaluated the spillover spread matrices of both panels to evaluate the compatibility of Super Bright dyes with other fluorochromes in a setup with tight placement of fluorochrome emissions over the spectrum. We have also matched Super Bright conjugates with comparable Brilliant Violet-labeled antibodies of same specificity in an existing 13-color panel where those conjugates are staining relatively dim targets, such as CCR6 and CD25, on resting human PBMC cells. Our results show that Super Bright dyes inflict a modest spillover spread in neighboring channels. In a 16x16 spillover spread matrix (3-UV, 5-VIOLET, 5-BLUE, 3-RED) Super Bright dyes demonstrate low to moderate spillover that is very close quantitatively to the Brilliant Violet dyes. In a 13-color human immunophenotyping panel that we previously developed to quantify T cell subsets, the "" brightness "" (i.e. the staining index of the Super Bright-conjugated antibodies) appears to be lower than comparable Brilliant Violet dyes when titrated, although stained populations in a full panel are still well separated. As the use of up to nine Brilliant polymer dyes simultaneously in large panels is not uncommon, we also tested the performance of Super Bright dyes in staining protocols that include Brilliant Buffer (BD Biosciences) to prevent polymer dye interactions and found them compatible. Overall, we found Super Bright dyes to perform well in large polychromatic panels. This expansion of commercially available conjugated antibody repertoire with the addition of Super Brights is timely and will greatly facilitate the success of larger (13+ color) fluorescent panel design."
ANNA BELKINA,Targeting tumor multicellular aggregation through IGPR-1 inhibits colon cancer growth and improves chemotherapy,"Adhesion to extracellular matrix (ECM) is crucially important for survival of normal epithelial cells as detachment from ECM triggers specific apoptosis known as anoikis. As tumor cells lose the requirement for anchorage to ECM, they rely on cell-cell adhesion 'multicellular aggregation' for survival. Multicellular aggregation of tumor cells also significantly determines the sensitivity of tumor cells to the cytotoxic effects of chemotherapeutics. In this report, we demonstrate that expression of immunoglobulin containing and proline-rich receptor-1 (IGPR-1) is upregulated in human primary colon cancer. Our study demonstrates that IGPR-1 promotes tumor multicellular aggregation, and interfering with its adhesive function inhibits multicellular aggregation and, increases cell death. IGPR-1 supports colon carcinoma tumor xenograft growth in mouse, and inhibiting its activity by shRNA or blocking antibody inhibits tumor growth. More importantly, IGPR-1 regulates sensitivity of tumor cells to the chemotherapeutic agent, doxorubicin/adriamycin by a mechanism that involves doxorubicin-induced AKT activation and phosphorylation of IGPR-1 at Ser220. Our findings offer novel insight into IGPR-1's role in colorectal tumor growth, tumor chemosensitivity, and as a possible novel anti-cancer target."
ANNA BELKINA,14-color flow cytometry to determine the contribution of mitochondrial mass to differences in glycolytic capacity in human immune cell subsets,"Mitochondrial metabolism controls immune cell function, but comprehensive tools to assess human primary immune cell metabolic capacity remain rudimentary. We previously demonstrated that CD19+ B cells rely more heavily on anaerobic glycolysis (i.e. are more glycolytic) than CD4+ T cells. Furthermore, both PBMCs and CD4+ T cells from subjects with type 2 diabetes (T2D) are more glycolytic than their counterparts from BMI-matched non-T2D controls. The contribution of mitochondrial mass, an indicator of non-glycolytic metabolism, to the various metabolic phenotypes is untested. To assess the contribution of immune cell subset identity and mitochondrial mass to the enhanced glycolytic capacity of resting B cells and PBMCs from T2D subjects, we designed a 13-color panel based on standard immune cell subset markers and chemokine receptors, and included MitoTracker Green FM (MTG), which quantitatively indicates mitochondrial mass. We used this novel panel to phenotype 63 total samples from BMI-matched subjects in three groups: non-T2D, pre-T2D, and fulminant T2D, as defined by American Diabetes Association guidelines. The panel was built in several iterations to accommodate spillover of MTG fluorescence into neighboring channels and includes, besides MTG and live-dead discriminator, the following surface markers: CD4, CD8, CD19, CD45RA, CD25, CD127, CD14, CCR4, CCR5, CCR6, CXCR3, and CD161. The PBMC samples were run on a 4-laser BD FACSARIA II SORP with pre-established panel-specific PMT voltages tracked using 6-peak Ultrarainbow beads. To normalize MTG fluorescence intensity and thus minimize batch effects, each of 5 total batches included a reference donor PBMC sample that was frozen in multiple aliquots from one blood draw. Using this approach, we quantified the percentages of immune cell populations (CD19+ B cells, CD8+ naïve and memory/effector T cells, and CD4+ cells including Tregs and populations enriched in Th1, Th2 and Th17) along with the relative mitochondrial mass in each subset. We found that CD19+ B cells in PBMCs from both ND and T2D subjects had significantly less mitochondrial mass than CD4+ cells, supporting the demonstration that B cells are more glycolytic than CD4+ T cells. Of all the CD4+ T cell subsets, Th17 cells consistently had the lowest mitochondrial mass, consistent with the interpretation that Th17s are more dependent on glycolysis than previously appreciated. Our results validate the utility of our 13-color panel to simultaneously quantify relative mitochondrial mass in numerous immune cell subsets and thereby provide a new tool to explore metabolism in human primary cells."
ANNA BELKINA,Role of Fas and Treg cells in fracture healing as characterized in the Fas‐deficient (lpr) mouse model of lupus,"Previous studies showed that loss of tumor necrosis factora (TNFa) signaling delayed fracture healing by delaying chondrocyte apoptosis and cartilage resorption. Mechanistic studies showed that TNFa induced Fas expression within chondrocytes; however, the degree to which chondrocyte apoptosis ismediated by TNFa alone or dependent on the induction of Fas is unclear. This questionwas addressed by assessing fracture healing in Fas‐deficient B6.MRL/Faslpr/Jmice. Loss of Fas delayed cartilage resorption but also lowered bone fraction in the calluses. The reduced bone fraction was related to elevated rates of coupled bone turnover in the B6.MRL/Faslpr/J calluses, as evidenced by higher osteoclast numbers and increased osteogenesis. Analysis of the apoptoticmarker caspase 3 showed fewer positive chondrocytes and osteoclasts in calluses of B6.MRL/Faslpr/J mice. To determine if an active autoimmune state contributed to increased bone turnover, the levels of activated T cells and Treg cellswere assessed. B6.MRL/Faslpr/J mice had elevated Treg cells in both spleens and bones of B6.MRL/Faslpr/J but decreased percentage of activated T cells in bone tissues. Fracture led to 30% to 60% systemic increase in Treg cells in bothwild‐type and B6.MRL/Faslpr/J bone tissues during the period of cartilage formation and resorption but either decreased (wild type) or left unchanged (B6.MRL/Faslpr/J) the numbers of activated T cells in bone. These results show that an active autoimmune state is inhibited during the period of cartilage resorption and suggest that iTreg cells play a functional role in this process. These data show that loss of Fas activity specifically in chondrocytes prolonged the life span of chondrocytes and that Fas synergized with TNFa signaling tomediate chondrocyte apoptosis. Conversely, loss of Fas systemically led to increased osteoclast numbers during later periods of fracture healing and increased osteogenesis. These findings suggest that retention of viable chondrocytes locally inhibits osteoclast activity or matrix proteolysis during cartilage resorption."
ANNA BELKINA,Epithelial cell–derived secreted and transmembrane 1a signals to activated neutrophils during pneumococcal pneumonia,"Airway epithelial cell responses are critical to the outcome of lung infection. In this study, we aimed to identify unique contributions of epithelial cells during lung infection. To differentiate genes induced selectively in epithelial cells during pneumonia, we compared genome-wide expression profiles from three sorted cell populations: epithelial cells from uninfected mouse lungs, epithelial cells from mouse lungs with pneumococcal pneumonia, and nonepithelial cells from those same infected lungs. Of 1,166 transcripts that were more abundant in epithelial cells from infected lungs compared with nonepithelial cells from the same lungs or from epithelial cells of uninfected lungs, 32 genes were identified as highly expressed secreted products. Especially strong signals included two related secreted and transmembrane (Sectm) 1 genes, Sectm1a and Sectm1b. Refinement of sorting strategies suggested that both Sectm1 products were induced predominantly in conducting airway epithelial cells. Sectm1 was induced during the early stages of pneumococcal pneumonia, and mutation of NF-kB RelA in epithelial cells did not diminish its expression. Instead, type I IFN signaling was necessary and sufficient for Sectm1 induction in lung epithelial cells, mediated by signal transducer and activator of transcription 1. For target cells, Sectm1a bound to myeloid cells preferentially, in particular Ly6GbrightCD11bbright neutrophils in the infected lung. In contrast, Sectm1a did not bind to neutrophils from uninfected lungs. Sectm1a increased expression of the neutrophil-attracting chemokine CXCL2 by neutrophils from the infected lung. We propose that Sectm1a is an epithelial product that sustains a positive feedback loop amplifying neutrophilic inflammation during pneumococcal pneumonia."
ANNA BELKINA,Cloud-based highly parallel execution of t-SNE and SPADE with metaclustering for analysis and visualization of large single-cell datasets,"The use of machine learning techniques, in particular unsupervised clustering and dimensionality reduction algorithms, is quickly becoming a standard workflow for identifying and visualizing biological populations from within high-dimensional data. These methods allow researchers to approach data analysis without the bias and subjectivity that has traditionally been standard in the field. Algorithms have context-dependent strengths and weaknesses. Across algorithms, an inability to scale computation to large datasets is a common theme. Most algorithms are designed and distributed to run on individual computers where memory and CPU are quickly exhausted by large datasets. Even when high-performance compute resources are available, algorithms often don't scale to large datasets as a fundamental property of their design. If they do, it might result in an untenable increase in runtime or diminished quality of results. t-SNE and SPADE are two well-published algorithms that suffer problems as discussed above after datasets exceed a number of observations on the order of 1 million. This study introduces an alternative approach to the use of SPADE and t- SNE whereby a dataset is divided and distributed across numerous compute nodes in the cloud to process independently in parallel. The results of each computation are then combined in a metaclustering step for final visualization and analysis. The improvement in execution speed as a function of degree of parallelization is established. The method is validated against a non-parallel analysis of the same dataset to establish concordance of identified populations. The workflow is executed on Cytobank for portability to other researchers."
ANNA BELKINA,Abstract 803: Targeting β-catenin/CBP signaling in OSCC,"OBJECTIVES: Oral squamous cell carcinoma (OSCC) is an aggressive malignancy characterized by molecular heterogeneity and locoregional spread associated with high morbidity. Aggressive cancers are thought to arise from populations of cancer initiating cells (CICs) that exhibit the properties of stem cells and drive tumor development, recurrence and resistance to therapy. The transcriptional regulator, β-catenin, has been implicated in OSCC CICs. Nuclear β-catenin has been shown to recruit the chromatin remodeling CREB binding protein (CBP) to drive expression of proliferation and survival genes, as well as genes that maintain stem-like phenotypes. We hypothesized that targeting β-catenin-CBP interaction will inhibit CICs in oral tumors and restore an epithelial phenotype. METHODS: To test tumor aggressive potential of OSCC CICs, we used zebrafish as a model system. We isolated CD44+CD24hiCD29hi cells fom aggressive HSC-3 OSCC cells by FACS and assayed their ability to drive tumor growth and metastases in zebrafish compared to unsorted and CD44+CD24lowCD29low cells. In addition, we examined the role of the β-catenin/CBP axis in the aggressive phenotype of these cells. We also assessed whether the β-catenin/CBP axis affected CICs in tumors from immune competent HPV+ mice. RESULTS: Zebrafish injected with subpopulation of cells co-expressing CD44+CD24hiCD2hi primitive cell surface markers drove rapid tumor growth and metastases, followed by unsorted and sorted CD44+CD24lowCD29low. Treatment of CD44+CD24hiCD29hi cells with a small molecule inhibitor of the β-catenin-CBP interaction, ICG-001, interfered with tumor growth and metastases in zebrafish. Further, ICG-001 inhibited tumor growth in immunocompetent HPV+ murine model. On a cellular level, ICG-001 promoted membrane localization of β-catenin, enhanced E-cadherin adhesion and restored epithelial phenotype. Significantly, ICG-001 gene signatures tracked with reduced overall patient survival in the cancer genome atlas, TCGA. Conclusion: Our studies indicate that the β-catenin/CBP axis promotes OSCC CICs and that ICG-001 may be an effective therapeutic agent for this malignancy."
ANNA BELKINA,Inhibition of Ubc13-mediated ubiquitination by GPS2 regulates multiple stages of B cell development,"Non-proteolytic ubiquitin signaling mediated by Lys63 ubiquitin chains plays a critical role in multiple pathways that are key to the development and activation of immune cells. Our previous work indicates that GPS2 (G-protein Pathway Suppressor 2) is a multifunctional protein regulating TNF signaling and lipid metabolism in the adipose tissue through modulation of Lys63 ubiquitination events. However, the full extent of GPS2-mediated regulation of ubiquitination and the underlying molecular mechanisms are unknown. Here, we report that GPS2 is required for restricting the activation of TLR and BCR signaling pathways and the AKT/FOXO1 pathway in immune cells based on direct inhibition of Ubc13 enzymatic activity. Relevance of this regulatory strategy is confirmed in vivo by B cell-targeted deletion of GPS2, resulting in developmental defects at multiple stages of B cell differentiation. Together, these findings reveal that GPS2 genomic and non-genomic functions are critical for the development and cellular homeostasis of B cells."
SARAH E. VALENTINE,"Complexity of childhood sexual abuse: predictors of current post-traumatic stress disorder, mood disorders, substance use, and sexual risk behavior among adult men who have sex with men","Men who have sex with men (MSM) are the group most at risk for HIV and represent the majority of new infections in the United States. Rates of childhood sexual abuse (CSA) among MSM have been estimated as high as 46 %. CSA is associated with increased risk of HIV and greater likelihood of HIV sexual risk behavior. The purpose of this study was to identify the relationships between CSA complexity indicators and mental health, substance use, sexually transmitted infections, and HIV sexual risk among MSM. MSM with CSA histories (n = 162) who were screened for an HIV prevention efficacy trial completed comprehensive psychosocial assessments. Five indicators of complex CSA experiences were created: CSA by family member, CSA with penetration, CSA with physical injury, CSA with intense fear, and first CSA in adolescence. Adjusted regression models were used to identify relationships between CSA complexity and outcomes. Participants reporting CSA by family member were at 2.6 odds of current alcohol use disorder (OR 2.64: CI 1.24–5.63), two times higher odds of substance use disorder (OR 2.1: CI 1.02–2.36), and 2.7 times higher odds of reporting an STI in the past year (OR 2.7: CI 1.04–7.1). CSA with penetration was associated with increased likelihood of current PTSD (OR 3.17: CI 1.56–6.43), recent HIV sexual risk behavior (OR 2.7: CI 1.16–6.36), and a greater number of casual sexual partners (p = 0.02). Both CSA with Physical Injury (OR 4.05: CI 1.9–8.7) and CSA with Intense Fear (OR 5.16: CI 2.5–10.7) were related to increased odds for current PTSD. First CSA in adolescence was related to increased odds of major depressive disorder. These findings suggest that CSA, with one or more complexities, creates patterns of vulnerabilities for MSM, including post-traumatic stress disorder, substance use, and sexual risk taking, and suggests the need for detailed assessment of CSA and the development of integrated HIV prevention programs that address mental health and substance use comorbidities."
EMMA LEJEUNE,Exploring the potential of transfer learning for metamodels of heterogeneous material deformation,"From the nano-scale to the macro-scale, biological tissue is spatially heterogeneous. Even when tissue behavior is well understood, the exact subject specific spatial distribution of material properties is often unknown. And, when developing computational models of biological tissue, it is usually prohibitively computationally expensive to simulate every plausible spatial distribution of material properties for each problem of interest. Therefore, one of the major challenges in developing accurate computational models of biological tissue is capturing the potential effects of this spatial heterogeneity. Recently, machine learning based metamodels have gained popularity as a computationally tractable way to overcome this problem because they can make predictions based on a limited number of direct simulation runs. These metamodels are promising, but they often still require a high number of direct simulations to achieve an acceptable performance. Here we show that transfer learning, a strategy where knowledge gained while solving one problem is transferred to solving a different but related problem, can help overcome this limitation. Critically, transfer learning can be used to leverage both low-fidelity simulation data and simulation data that is the outcome of solving a different but related mechanical problem. In this paper, we extend Mechanical MNIST, our open source benchmark dataset of heterogeneous material undergoing large deformation, to include a selection of low-fidelity simulation results that require ≈ 2 - 4 orders of magnitude less CPU time to run. Then, we show that transferring the knowledge stored in metamodels trained on these low-fidelity simulation results can vastly improve the performance of metamodels used to predict the results of high-fidelity simulations. In the most dramatic examples, metamodels trained on 100 high fidelity simulations but pre-trained on 60,000 low-fidelity simulations achieves nearly the same test error as metamodels trained on 60,000 high-fidelity simulations (1 - 1.5% mean absolute percent error). In addition, we show that transfer learning is an effective method for leveraging data from different load cases, and for leveraging low-fidelity two-dimensional simulations to predict the outcomes of high-fidelity three-dimensional simulations. Looking forward, we anticipate that transfer learning will enable us to better capture the influence of tissue spatial heterogeneity on the mechanical behavior of biological materials across multiple different domains."
EMMA LEJEUNE,Mechanical response of cardiac microtissues to acute localized injury,"After a myocardial infarction (MI), the heart undergoes changes including local remodeling that can lead to regional abnormalities in mechanical and electrical properties, ultimately increasing the risk of arrhythmias and heart failure. Although these responses have been successfully recapitulated in animal models of MI, local changes in tissue and cell-level mechanics caused by MI remain difficult to study in vivo. Here, we developed an in vitro cardiac microtissue (CMT) injury system that through acute focal injury recapitulates aspects of the regional responses seen following an MI. With a pulsed laser, cell death was induced in the center of the microtissue causing a loss of calcium signaling and a complete loss of contractile function in the injured region and resulting in a 39% reduction in the CMT's overall force production. After 7 days, the injured area remained void of cardiomyocytes (CMs) and showed increased expression of vimentin and fibronectin, two markers for fibrotic remodeling. Interestingly, although the injured region showed minimal recovery, calcium amplitudes in uninjured regions returned to levels comparable with control. Furthermore, overall force production returned to preinjury levels despite the lack of contractile function in the injured region. Instead, uninjured regions exhibited elevated contractile function, compensating for the loss of function in the injured region, drawing parallels to changes in tissue-level mechanics seen in vivo. Overall, this work presents a new in vitro model to study cardiac tissue remodeling and electromechanical changes after injury.NEW & NOTEWORTHY We report an in vitro cardiac injury model that uses a high-powered laser to induce regional cell death and a focal fibrotic response within a human-engineered cardiac microtissue. The model captures the effects of acute injury on tissue response, remodeling, and electromechanical recovery in both the damaged region and surrounding healthy tissue, modeling similar changes to contractile function observed in vivo following myocardial infarction."
EMMA LEJEUNE,Asymmetric Buckling Columns (ABC),"The Asymmetric Buckling Columns (ABC) dataset contains spatially heterogeneous columns with fixed-fixed boundary conditions that are classified to be buckling left (label of 0) or right (label of 1). The dataset is split into 3 subdatasets: sub-dataset 1, sub-dataset 2, and sub-dataset 3, each with increasing levels of geometric complexity. For each sub-dataset, we provide information to reconstruct the domain geometry as txt files (subdataset*_geometry.zip), graphs from Simple Linear Iterative Clustering (SLIC) segmentation with varying degrees of node density as json files (subdatset*_sparse_graphs.zip, subdatset*_medium_graphs.zip, subdatset*_dense_graphs.zip) , and output labels as txt files (subdataset*_labels.zip). In brief, sub-dataset 1 is generated by stacking blocks with varying widths, sub-dataset 2 consists of overlapping rings of identical size, and sub-dataset 3 consists of overlapping and trimmed rings of varying sizes. For sub-dataset 1 geometry files, ""x.txt'' indicates the centers of each block and ""l.txt'' gives the length of each block. Each block is stacked top to bottom. For sub-dataset2, the files in folder ""x'' and folder ``y'' give the ""x'' an ""y'' coordinates for each ring with inner radius of 0.15w and outer radius of 0.25w. Note that there is a different number of rings in each structure. For sub-dataset 3, ""x.txt'' and ""y.txt'' contain the ""x'' and ""y'' coordinate of each ring, and ""outer.txt'' and ""inner.txt'' give the ring outer thickness and the ratio of inner thickness to outer thickness respectively. Note that the coordinates for all domains are defined such that the origin is in the top left. Details on how to generate ground truth domain geometry with the provided geometric information and the corresponding graphs and finite element meshes are provided on GitHub (https://github.com/pprachas/ABC_dataset). The json graph files are the graphs used in the corresponding manuscript, and the code to load the json files with Pytorch and Pytorch Geometric is also provided. All subdatasets contain 25,000 simulation results. For the manuscript, 20,000 data points are used to train the ML model with 2,500 datapoints used for validation, and 2,500 datapoints held out as test data. The graphs provided are first shuffled and then split into train, validation and test data. Details of our protocol for shuffling and splitting the data are also provided on GitHub."
EMMA LEJEUNE,Enhancing mechanical metamodels with a generative model-based augmented training dataset,"Modeling biological soft tissue is complex in part due to material heterogeneity. Microstructural patterns, which play a major role in defining the mechanical behavior of these tissues, are both challenging to characterize and difficult to simulate. Recently, machine learning (ML)-based methods to predict the mechanical behavior of heterogeneous materials have made it possible to more thoroughly explore the massive input parameter space associated with heterogeneous blocks of material. Specifically, we can train ML models to closely approximate computationally expensive heterogeneous material simulations where the ML model is trained on datasets of simulations with relevant spatial heterogeneity. However, when it comes to applying these techniques to tissue, there is a major limitation: the number of useful examples available to characterize the input domain under study is often limited. In this work, we investigate the efficacy of both ML-based generative models and procedural methods as tools for augmenting limited input pattern datasets. We find that a style-based generative adversarial network with an adaptive discriminator augmentation mechanism is able to successfully leverage just 1000 example patterns to create authentic generated patterns. In addition, we find that diverse generated patterns with adequate resemblance to real patterns can be used as inputs to finite element simulations to meaningfully augment the training dataset. To enable this methodological contribution, we have created an open access finite element analysis simulation dataset based on Cahn-Hilliard patterns. We anticipate that future researchers will be able to leverage this dataset and build on the work presented here."
EMMA LEJEUNE,"An introduction to the Ogden model in biomechanics: benefits, implementation tools and limitations","Constitutive models are important to biomechanics for two key reasons. First, constitutive modelling is an essential component of characterizing tissues' mechanical properties for informing theoretical and computational models of biomechanical systems. Second, constitutive models can be used as a theoretical framework for extracting and comparing key quantities of interest from material characterization experiments. Over the past five decades, the Ogden model has emerged as a popular constitutive model in soft tissue biomechanics with relevance to both informing theoretical and computational models and to comparing material characterization experiments. The goal of this short review is threefold. First, we will discuss the broad relevance of the Ogden model to soft tissue biomechanics and the general characteristics of soft tissues that are suitable for approximating with the Ogden model. Second, we will highlight exemplary uses of the Ogden model in brain tissue, blood clot and other tissues. Finally, we offer a tutorial on fitting the one-term Ogden model to pure shear experimental data via both an analytical approximation of homogeneous deformation and a finite-element model of the tissue domain. Overall, we anticipate that this short review will serve as a practical introduction to the use of the Ogden model in biomechanics. This article is part of the theme issue 'The Ogden model of rubber mechanics: Fifty years of impact on nonlinear elasticity'."
EMMA LEJEUNE,Towards out of distribution generalization for problems in mechanics,
EMMA LEJEUNE,Learning mechanically driven emergent behavior with message passing neural networks,
EMMA LEJEUNE,On the three-dimensional correlation between myofibroblast shape and contraction,"Myofibroblasts are responsible for wound healing and tissue repair across all organ systems. In periods of growth and disease, myofibroblasts can undergo a phenotypic transition characterized by an increase in extracellular matrix (ECM) deposition rate, changes in various protein expression (e.g., alpha-smooth muscle actin (αSMA)), and elevated contractility. Cell shape is known to correlate closely with stress-fiber geometry and function and is thus a critical feature of cell biophysical state. However, the relationship between myofibroblast shape and contraction is complex, even as well in regards to steady-state contractile level (basal tonus). At present, the relationship between myofibroblast shape and basal tonus in three-dimensional (3D) environments is poorly understood. Herein, we utilize the aortic valve interstitial cell (AVIC) as a representative myofibroblast to investigate the relationship between basal tonus and overall cell shape. AVICs were embedded within 3D poly(ethylene glycol) (PEG) hydrogels containing degradable peptide crosslinkers, adhesive peptide sequences, and submicron fluorescent microspheres to track the local displacement field. We then developed a methodology to evaluate the correlation between overall AVIC shape and basal tonus induced contraction. We computed a volume averaged stretch tensor ⟨U⟩ for the volume occupied by the AVIC, which had three distinct eigenvalues (λ1,2,3=1.08,0.99, and 0.89), suggesting that AVIC shape is a result of anisotropic contraction. Furthermore, the direction of maximum contraction correlated closely with the longest axis of a bounding ellipsoid enclosing the AVIC. As gel-imbedded AVICs are known to be in a stable state by 3 days of incubation used herein, this finding suggests that the overall quiescent AVIC shape is driven by the underlying stress-fiber directional structure and potentially contraction level."
EMMA LEJEUNE,FM-track: a fiducial marker tracking software for studying cell mechanics in a three-dimensional environment,"Tracking the deformation of fiducial markers in the vicinity of living cells embedded in compliant synthetic or biological gels is a powerful means to study cell mechanics and mechanobiology in three-dimensional environments. However, current approaches to track and quantify three-dimensional (3D) fiducial marker displacements remain ad-hoc, can be difficult to implement, and may not produce reliable results. Herein, we present a compact software package entitled “FM-Track,” written in the popular Python language, to facilitate feature-based particle tracking tailored for 3D cell micromechanical environment studies. FM-Track contains functions for pre-processing images, running fiducial marker tracking, and post-processing and visualization. FM-Track can thus aid the study of cellular mechanics and mechanobiology by providing an extensible software platform to more reliably extract complex local 3D cell contractile information in transparent compliant gel systems."
EMMA LEJEUNE,Mechanical MNIST Crack Path,"The Mechanical MNIST Crack Path dataset contains Finite Element simulation results from phase-field models of quasi-static brittle fracture in heterogeneous material domains subjected to prescribed loading and boundary conditions. For all samples, the material domain is a square with a side length of 1. There is an initial crack of fixed length (0.25) on the left edge of each domain. The bottom edge of the domain is fixed in x (horizontal) and y (vertical), the right edge of the domain is fixed in x and free in y, and the left edge is free in both x and y. The top edge is free in x, and in y it is displaced such that, at each step, the displacement increases linearly from zero at the top right corner to the maximum displacement on the top left corner. Maximum displacement starts at 0.0 and increases to 0.02 by increments of 0.0001 (200 simulation steps in total). The heterogeneous material distribution is obtained by adding rigid circular inclusions to the domain using the Fashion MNIST bitmaps as the reference location for the center of the inclusions. Specifically, each center point location is generated randomly inside a square region defined by the corresponding Fashion MNIST pixel when the pixel has an intensity value higher than 10. In addition, a minimum center-to-center distance limit of 0.0525 is applied while generating these center points for each sample. The values of Young’s Modulus (E), Fracture Toughness (G_f), and Failure Strength (f_t) near each inclusion are increased with respect to the background domain by a variable rigidity ratio r. The background value for E is 210000, the background value for G_f is 2.7, and the background value for f_t is 2445.42. The rigidity ratio throughout the domain depends on position with respect to all inclusion centers such that the closer a point is to the inclusion center the higher the rigidity ratio will be. We note that the full algorithm for constructing the heterogeneous material property distribution is included in the simulations scripts shared on GitHub. The following information is included in our dataset: (1) A rigidity ratio array to capture heterogeneous material distribution reported over a uniform 64x64 grid, (2) the damage field at the final level of applied displacement reported over a uniform 256x256 grid, (3) the damage field at the final level of applied displacement reported over a uniform 64x64 grid, and (4) the force-displacement curves for each simulation. All simulations are conducted with the FEniCS computing platform (https://fenicsproject.org). The code to reproduce these simulations is hosted on GitHub (https://github.com/saeedmhz/phase-field)."
EMMA LEJEUNE,Quantifying heart valve interstitial cell contractile state using highly tunable poly(ethylene glycol) hydrogels,"Valve interstitial cells (VIC) are the primary cell type residing within heart valve tissues. In many valve pathologies, VICs become activated and will subsequently profoundly remodel the valve tissue extracellular matrix (ECM). A primary indicator of VIC activation is the upregulation of a-smooth muscle actin (aSMA) stress fibers, which in turn increase VIC contractility. Thus, contractile state reflects VIC activation and ECM biosynthesis levels. In general, cell contraction studies have largely utilized two-dimensional substrates, which are a vastly different micro mechanical environment than 3D native leaflet tissue. To address this limitation, hydrogels have been a popular choice for studying cells in a three-dimensional environment due to their tunable properties and optical transparency, which allows for direct cell visualization. In the present study, we extended the use of hydrogels to study the active contractile behavior of VICs. Aortic VICs (AVIC) were encapsulated within poly(ethylene glycol) (PEG) hydrogels and were subjected to flexural-deformation tests to assess the state of AVIC contraction. Using a finite element model of the experimental setup, we determined the effective shear modulus l of the constructs. An increase in l resulting from AVIC active contraction was observed. Results further indicated that AVIC contraction had a more pronounced effect on l in softer gels (72 ± 21% increase in l within 2.5 kPa gels) and was dependent upon the availability of adhesion sites (0.5–1 mM CRGDS). The transparency of the gel allowed us to image AVICs directly within the hydrogel, where we observed a time-dependent decrease in volume (time constant s ¼ 3:04 min) when the AVICs were induced into a hypertensive state. Our results indicated that AVIC contraction was regulated by both the intrinsic (unseeded) gel stiffness and the CRGDS peptide concentrations. This finding suggests that AVIC contractile state can be profoundly modulated through their local micro environment using modifiable PEG gels in a 3D micromechanical-emulating environment. Moving forward, this approach has the potential to be used towards delineating normal and diseased VIC biomechanical properties using highly tunable PEG biomaterials."
EMMA LEJEUNE,Mechanical MNIST: A benchmark dataset for mechanical metamodels,"Metamodels, or models of models, map defined model inputs to defined model outputs. Typically, metamodels are constructed by generating a dataset through sampling a direct model and training a machine learning algorithm to predict a limited number of model outputs from varying model inputs. When metamodels are constructed to be computationally cheap, they are an invaluable tool for applications ranging from topology optimization, to uncertainty quantification, to multi-scale simulation. By nature, a given metamodel will be tailored to a specific dataset. However, the most pragmatic metamodel type and structure will often be general to larger classes of problems. At present, the most pragmatic metamodel selection for dealing with mechanical data has not been thoroughly explored. Drawing inspiration from the benchmark datasets available to the computer vision research community, we introduce a benchmark data set (Mechanical MNIST) for constructing metamodels of heterogeneous material undergoing large deformation. We then show examples of how our benchmark dataset can be used, and establish baseline metamodel performance. Because our dataset is readily available, it will enable the direct quantitative comparison between different metamodeling approaches in a pragmatic manner. We anticipate that it will enable the broader community of researchers to develop improved metamodeling techniques for mechanical data that will surpass the baseline performance that we show here."
EMMA LEJEUNE,Mechanical MNIST - Equibiaxial Extension,"Each dataset in the Mechanical MNIST collection contains the results of 70,000 (60,000 training examples + 10,000 test examples) finite element simulation of a heterogeneous material subject to large deformation. Mechanical MNIST is generated by first converting the MNIST bitmap images (http://www.pymvpa.org/datadb/mnist.html) to 2D heterogeneous blocks of material. Consistent with the MNIST bitmap ($28 \times 28$ pixels), the material domain is a $28 \times 28$ unit square. In ""Mechanical MNIST - Equibiaxial Extension,"" the material is Neo-Hookean with a varying modulus. The top of the domain is free horizontally and moved vertically to a set of given fixed displacements (d = [0.0, 0.0005, 0.005, 0.05, 0.25, 0.5, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0 ]), the right of the domain is free vertically and moved horizontally to a set of given fixed displacements (d = [0.0, 0.0005, 0.005, 0.05, 0.25, 0.5, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0 ]), the bottom of the domain is free horizontally and moved vertically to a set of given fixed displacements (d = [-0.0, -0.0005, -0.005, -0.05, -0.25, -0.5, -1.0, -2.0, -3.0, -4.0, -5.0, -6.0, -7.0 ]), and the left of the domain is free vertically and moved horizontally to a set of given fixed displacements (d = [-0.0, -0.0005, -0.005, -0.05, -0.25, -0.5, -1.0, -2.0, -3.0, -4.0, -5.0, -6.0, -7.0 ]). The results of the simulations include: (1) change in strain energy at each step, (2) total reaction force at the top and right boundaries at each step, and (3) full field displacement at each step. All simulations are conducted with the FEniCS computing platform (https://fenicsproject.org). The code to reproduce these simulations is hosted on GitHub (https://github.com/elejeune11/Mechanical-MNIST/tree/master/generate_dataset)."
EMMA LEJEUNE,Mechanical MNIST - Confined Compression,"Each dataset in the Mechanical MNIST collection contains the results of 70,000 (60,000 training examples + 10,000 test examples) finite element simulation of a heterogeneous material subject to large deformation. Mechanical MNIST is generated by first converting the MNIST bitmap images (http://www.pymvpa.org/datadb/mnist.html) to 2D heterogeneous blocks of material. Consistent with the MNIST bitmap ($28 \times 28$ pixels), the material domain is a $28 \times 28$ unit square. In ""Mechanical MNIST - Equibiaxial Extension,"" the material is Neo-Hookean with a varying modulus. The top of the domain is free horizontally and moved vertically to a set of given fixed displacements (d = [-0.0, -0.001, -0.01, -0.1, -0.5, -1.0, -1.5, -2.0, -2.5, -3.0, -3.5] ), the right and left sides of the domain are free vertically and fixed horizontally, and the bottom of the domain is free horizontally and fixed vertically. The results of the simulations include: (1) change in strain energy at each step, (2) total reaction force at the top and right boundaries at each step, and (3) full field displacement at each step. All simulations are conducted with the FEniCS computing platform (https://fenicsproject.org). The code to reproduce these simulations is hosted on GitHub (https://github.com/elejeune11/Mechanical-MNIST/tree/master/generate_dataset)."
EMMA LEJEUNE,Mechanical MNIST - Shear,"Each dataset in the Mechanical MNIST collection contains the results of 70,000 (60,000 training examples + 10,000 test examples) finite element simulation of a heterogeneous material subject to large deformation. Mechanical MNIST is generated by first converting the MNIST bitmap images (http://www.pymvpa.org/datadb/mnist.html) to 2D heterogeneous blocks of material. Consistent with the MNIST bitmap ($28 \times 28$ pixels), the material domain is a $28 \times 28$ unit square. In ""Mechanical MNIST - Shear,"" the material is Neo-Hookean with a varying modulus. The bottom of the domain is fixed (Dirichlet boundary condition), the left and right edges of the domain are free, and the top of the domain is fixed vertically and moved horizontally to a set of given fixed displacements (d = [0.0, 0.001, 0.01, 0.1, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5]). The results of the simulations include: (1) change in strain energy at each step, (2) total reaction force at the top boundary at each step, and (3) full field displacement at each step. All simulations are conducted with the FEniCS computing platform (https://fenicsproject.org). The code to reproduce these simulations is hosted on GitHub (https://github.com/elejeune11/Mechanical-MNIST/tree/master/generate_dataset)."
EMMA LEJEUNE,Mechanical MNIST - Multi-Fidelity,"Each dataset in the Mechanical MNIST collection contains the results of 70,000 (60,000 training examples + 10,000 test examples) finite element simulation of a heterogeneous material subject to large deformation. Mechanical MNIST is generated by first converting the MNIST bitmap images (http://www.pymvpa.org/datadb/mnist.html) to 2D heterogeneous blocks of material. Consistent with the MNIST bitmap ($28 \times 28$ pixels), the material domain is a $28 \times 28$ unit square. The material is Neo-Hookean with a varying modulus dictated by the input bitmap. The simulation results included here are the change in strain energy at a fixed level of applied displacement. The cases considered are as follows: *UE: uniaxial extension, full fidelity dataset (fully refined mesh, quadratic triangular elements, applied displacement is $1/2$ of a side length); *EE: equibiaxial extension, full fidelity dataset; *3D: uniaxial extension and out of plane twist, full fidelity three dimensional dataset (fully refined mesh, quadratic tetrahedral elements, applied displacement is $1/7$ of a side length, twist is $\pi/8$ radians, block thickness is $1/7$ of a side length); *UE-CM-28: uniaxial extension, $28 \times 28 \times 2$ linear triangular elements; *UE-CM-14: uniaxial extension, $14 \times 14 \times 2$ linear triangular elements; *UE-CM-7: uniaxial extension, $7 \times 7 \times 2$ linear triangular elements; *UE-CM-7-quad: uniaxial extension, $7 \times 7 \times 2$ quadratic triangular elements; *UE-CM-4: uniaxial extension, $4 \times 4 \times 2$ linear triangular elements; *UE-CM-4-quad: uniaxial extension, $4 \times 4 \times 2$ quadratic triangular elements; *UE-perturb: uniaxial extension, applied displacement is a perturbation (.001 units); *UE-CM-28-perturb: uniaxial extension, $28 \times 28 \times 2$ linear triangular elements, applied displacement is a perturbation (.001 units). All simulations are conducted with the FEniCS computing platform (https://fenicsproject.org). The code to reproduce these simulations is hosted on GitHub (https://github.com/elejeune11/Mechanical-MNIST-Transfer-Learning)."
EMMA LEJEUNE,Buckling Instability Classification (BIC),"The Buckling Instability Classification (BIC) datasets contain the results of finite element simulations where a heterogeneous column is subject to a fixed level of applied displacement and is classified as either ""Stable"" or ""Unstable."" Each model input is a 16x1 vector where the entries of the vector dictate the Young's Modulus (E) of the corresponding portion of the physical column domain. Each input file has 16 columns one for each vector entry. For each 16x1 vector input, there is a single output that indicates if the column was stable or unstable at the fixed level of applied displacement. An output value of ""0"" indicates stable, and an output value of ""1"" indicates unstable. In BIC-1, we only allow two possible discrete values for E: E=1 or E=4. In BIC-2, we allow three possible discrete values for E: E=1, E=4, or E=7. In BIC-3, we allow continuous values (to three digits of precision) of E in the range E=1–8. BIC-1 consists of 65,536 simulation results. This exhausts the entire possible input domain. BIC-2 consists of 100,000 simulation results. This is less than 1% of the entire possible input domain. BIC-3 also consists of 100,000 simulation results. This is a tiny fraction of the entire possible input domain."
EMMA LEJEUNE,"Sarc-Graph: automated segmentation, tracking, and analysis of sarcomeres in hiPSC-derived cardiomyocytes","A better fundamental understanding of human induced pluripotent stem cell-derived cardiomyocytes (hiPSC-CMs) has the potential to advance applications ranging from drug discovery to cardiac repair. Automated quantitative analysis of beating hiPSC-CMs is an important and fast developing component of the hiPSC-CM research pipeline. Here we introduce ""Sarc-Graph,"" a computational framework to segment, track, and analyze sarcomeres in fluorescently tagged hiPSC-CMs. Our framework includes functions to segment z-discs and sarcomeres, track z-discs and sarcomeres in beating cells, and perform automated spatiotemporal analysis and data visualization. In addition to reporting good performance for sarcomere segmentation and tracking with little to no parameter tuning and a short runtime, we introduce two novel analysis approaches. First, we construct spatial graphs where z-discs correspond to nodes and sarcomeres correspond to edges. This makes measuring the network distance between each sarcomere (i.e., the number of connecting sarcomeres separating each sarcomere pair) straightforward. Second, we treat tracked and segmented components as fiducial markers and use them to compute the approximate deformation gradient of the entire tracked population. This represents a new quantitative descriptor of hiPSC-CM function. We showcase and validate our approach with both synthetic and experimental movies of beating hiPSC-CMs. By publishing Sarc-Graph, we aim to make automated quantitative analysis of hiPSC-CM behavior more accessible to the broader research community."
EMMA LEJEUNE,Mechanical MNIST - Fashion,"Each dataset in the Mechanical MNIST collection contains the results of 70,000 (60,000 training examples + 10,000 test examples) finite element simulation of a heterogeneous material subject to large deformation. Mechanical MNIST - Fashion is generated by first converting the fashion MNIST bitmap images (https://github.com/zalandoresearch/fashion-mnist) to 2D heterogeneous blocks of material. Consistent with the MNIST bitmap ($28 \times 28$ pixels), the material domain is a $28 \times 28$ unit square. In “Mechanical MNIST - Fashion,” the material is Neo-Hookean with a varying modulus. In the Uniaxial Extension (UE) case, the bottom of the domain is fixed (Dirichlet boundary condition), the left and right edges of the domain are free, and the top of the domain is fixed horizontally and moved vertically to a given fixed displacement (d). In the Equibiaxial Extension (EE) case, the top of the domain is free horizontally and moved vertically to a given fixed displacement (d), the right of the domain is free vertically and moved horizontally to a given fixed displacement (d), the bottom of the domain is free horizontally and moved vertically to a given fixed displacement (-d), and the left of the domain is free vertically and moved horizontally to a given fixed displacement (-d). The results of the simulations include: (1) change in strain energy at a perturbation level step (d=0.001), and at the final applied displacement (d=14 for UE, d=7 for EE) (2) total reaction force at a perturbation level step (d=0.001 for UE, d=.0005 for EE), and at the final applied displacement (d=14 for UE, d=7 for EE), (3) full field displacement at a perturbation level step (d=0.001), and at the final applied displacement (d=14 for UE, d=7 for EE), and (4) the components of the deformation gradient (F11, F12, F21, F22) at the final applied displacement (d=14 for UE, d=7 for EE). The x-reaction (first column) and y-reaction (second column) forces are given. For the UE case, this corresponds to the top boundary. For the EE case, this correspond to the left and top boundaries. All simulations are conducted with the FEniCS computing platform (https://fenicsproject.org). The code to reproduce these simulations and import these text files is hosted on GitHub (https://github.com/elejeune11/Mechanical-MNIST-fashion)."
EMMA LEJEUNE,Mechanical MNIST – Distribution Shift,"The Mechanical MNIST – Distribution Shift dataset contains the results of finite element simulation of heterogeneous material subject to large deformation due to equibiaxial extension at a fixed boundary displacement of d = 7.0. The result provided in this dataset is the change in strain energy after this equibiaxial extension. The Mechanical MNIST dataset is generated by converting the MNIST bitmap images (28x28 pixels) with range 0 - 255 to 2D heterogeneous blocks of material (28x28 unit square) with varying modulus in range 1- s. The original bitmap images are sourced from the MNIST Digits dataset, (http://www.pymvpa.org/datadb/mnist.html) which corresponds to Mechanical MNIST – MNIST, and the EMNIST Letters dataset (https://www.nist.gov/itl/products-and-services/emnist-dataset) which correspond to Mechanical MNIST – EMNIST Letters. The Mechanical MNIST – Distribution Shift dataset is specifically designed to demonstrate three types of data distribution shift: (1) covariate shift, (2) mechanism shift, and (3) sampling bias, for all of which the training and testing environments are drawn from different distributions. For each type of data distribution shift, we have one dataset generated from the Mechanical MNIST bitmaps and one from the Mechanical MNIST – EMNIST Letters bitmaps. For the covariate shift dataset, the training dataset is collected from two environments (2500 samples from s = 100, and 2500 samples from s = 90), and the test data is collected from two additional environments (2000 samples from s = 75, and 2000 samples from s = 50). For the mechanism shift dataset, the training data is identical to the training data in the covariate shift dataset (i.e., 2500 samples from s = 100, and 2500 samples from s = 90), and the test datasets are from two additional environments (2000 samples from s = 25, and 2000 samples from s = 10).  For the sampling bias dataset, datasets are collected such that each datapoint is selected from the broader MNIST and EMNIST inputs bitmap selection by a probability which is controlled by a parameter r. The training data is collected from two environments (9800 from r = 15, and 200 from r = -2), and the test data is collected from three different environments (2000 from r = -5, 2000 from r = -10, and 2000 from r = 1).  Thus, in the end we have 6 benchmark datasets with multiple training and testing environments in each. The enclosed document “folder_description.pdf'” shows the organization of each zipped folder provided on this page. The code to reproduce these simulations is available on GitHub (https://github.com/elejeune11/Mechanical-MNIST/blob/master/generate_dataset/Equibiaxial_Extension_FEA_test_FEniCS.py). "
EMMA LEJEUNE,Mechanical MNIST - Cahn-Hilliard,"The Mechanical MNIST Cahn-Hilliard dataset contains the results of 104,813 Finite Element simulations of a heterogeneous material domain subject to large equibiaxial extension deformation. The heterogeneous domain patterns are generated from a Finite Element implementation of the Cahn-Hilliard equation. Different stripe and circle patterns are obtained by varying four simulation parameters: the initial concentration, the grid size on which the concentration is initialized, parameter $\lambda$, and $b$, the peak-to-valley value of the symmetric double-well chemical free-energy function. Binary bitmap images of 400 x 400 pixels are converted into two-dimensional meshed domains of binary material using the OpenCV library, Pygmsh, and Gmsh 4.6.0. We also include in this dataset the 104,813 patterns (37,523 from case 1, 37,680 from case 2, and 29,610 from case 3) used in the Finite Element simulations stored as binary images in text files. After pattern generation, the material domain is modeled as a unit square of Neo-Hookean binary material (high concentration areas correspond to Young's Modulus 10, low concentration areas correspond to Young's Modulus 1). For equibiaxial extension, each of the four edges of the domain is displaced to 50% of the initial domain size in the direction of the outward normal to the surface with fixed displacements (d = [0.0,0.001,0.1,0.2,0.3,0.4,0.5]). Here we provide the simulation results consisting of the following: (1) change in strain energy reported at each level of applied displacement, (2) total reaction force at the four boundaries reported at each level of applied displacement, and (3) full field displacement reported at the final applied displacement d=0.5. All Finite Element simulations are conducted with the FEniCS computing platform (https://fenicsproject.org). The code to reproduce these simulations (both pattern generation simulations and equibiaxial extension simulations) is hosted on GitHub (https://github.com/elejeune11/Mechanical-MNIST-Cahn-Hilliard). The enclosed document “description.pdf'” contains additional details."
EMMA LEJEUNE,Mechanical MNIST – Unsupervised Learning Dataset,"The Mechanical MNIST dataset collection contains Finite Element simulations of heterogeneous materials undergoing applied displacement. Here, we introduce a new benchmark dataset designed specifically for assessing unsupervised learning methods where the goal is to discover patterns from unlabeled data. To obtain this dataset, we generate displacement fields from Finite Element simulations and uniformly sample approximately 1500 displacement markers on each domain of interest. Since unsupervised learning aims to identify patterns in labeled data, we provide a dataset where the primary objective is to explore unlabeled data, while simultaneously providing “ground truth” information to ultimately evaluate the efficacy of different unsupervised learning approaches. It is important to note however, that in the intended applications of these methods, ground truth information will likely be absent, particularly in experimental studies of intricate heterogeneous soft tissue. Broadly speaking, this computationally generated dataset mimics the behavior of soft materials, while simultaneously providing ground truth information for method evaluation. In total, the dataset contains the following combinations of conditions: 6 different heterogeneous material patterns, 2 constitutive models, 4 controlled boundary conditions, and 1 random boundary condition. Here, we include the tutorials for our dataset with the name “dataset_tutorials.pdf”. This document contains the information to understand the contents of our dataset, as well as the instructions on how to use the data. The many options from our dataset should enable researchers to explore unsupervised learning methods on soft materials."
EMMA LEJEUNE,Mechanical MNIST - Uniaxial Extension,"Each dataset in the Mechanical MNIST collection contains the results of 70,000 (60,000 training examples + 10,000 test examples) finite element simulation of a heterogeneous material subject to large deformation. Mechanical MNIST is generated by first converting the MNIST bitmap images (http://www.pymvpa.org/datadb/mnist.html) to 2D heterogeneous blocks of material. Consistent with the MNIST bitmap ($28 \times 28$ pixels), the material domain is a $28 \times 28$ unit square. In “Mechanical MNIST - Uniaxial Extension,” the material is Neo-Hookean with a varying modulus. The bottom of the domain is fixed (Dirichlet boundary condition), the left and right edges of the domain are free, and the top of the domain is fixed horizontally and moved vertically to a set of given fixed displacements (d = [0.0, 0.001, 0.01, 0.1, 0.5, 1.0, 2.0, 4.0, 6.0, 8.0, 10.0, 12.0, 14.0 ]). The results of the simulations include: (1) change in strain energy at each step, (2) total reaction force at the top boundary at each step, and (3) full field displacement at each step. All simulations are conducted with the FEniCS computing platform (https://fenicsproject.org). The code to reproduce these simulations is hosted on GitHub (https://github.com/elejeune11/Mechanical-MNIST/tree/master/generate_dataset)."
EMMA LEJEUNE,Predicting mechanically driven full-field quantities of interest with deep learning-based metamodels,"Using simulation to predict the mechanical behavior of heterogeneous materials has applications ranging from topology optimization to multi-scale structural analysis. However, full-fidelity simulation techniques such as Finite Element Analysis, while effective, can be prohibitively computationally expensive when they are used to explore the massive input parameter space of heterogeneous materials. Therefore, there has been significant recent interest in machine learning-based models that, once trained, can predict mechanical behavior at a fraction of the computational cost compared to full fidelity simulations. Over the past several years, research in this area has been focused mainly on predicting single Quantities of Interest (QoIs). However, there has recently been an increased interest in a more challenging problem: predicting full-field QoI (e.g., displacement/strain fields, damage fields) for mechanical problems. Due to the added complexity of full-field information, network architectures that perform well on single QoI problems may perform relatively poorly in the full-field QoI problem setting. This problem is also challenging because, even outside the Mechanics research community, deep learning approaches to image-to-image mapping and full-field image analysis remain poorly understood. The work presented in this paper is twofold. First, we made a significant extension to the Mechanical MNIST dataset designed to enable the investigation of full-field QoI prediction. Specifically, we added Finite Element simulation results of quasi-static brittle fracture in a heterogeneous material captured with the phase-field method. This problem was chosen as a broadly relevant ”challenge problem” for full-field QoI prediction. Second, we investigated multiple Deep Neural Network architectures and subsequently established strong baseline performance for predicting full-field QoI. We found that a MultiRes-WNet architecture with straightforward data augmentation achieves 0.80% and 0.34% Mean Absolute Percentage Error on full-field displacement prediction for Equibiaxial Extension and Uniaxial Extension datasets in the Mechanical MNIST Fashion dataset, respectively. In addition, we found that our MultiRes-WNet architecture combined with a basic Convolutional Autoencoder achieves a mean score of 0.87 on the newly added Mechanical MNIST Crack Path dataset. In addition to presenting the results in this paper, we have released our model implementation and the Mechanical MNIST Crack Path dataset under open-source licenses. We anticipate that future researchers will directly use our model architecture on related datasets and potentially design models that exceed the baseline performance for predicting full-field QoI established in this paper."
EMMA LEJEUNE,Mechanically programming anisotropy in engineered muscle with actuating extracellular matrices,
EMMA LEJEUNE,Locality sensitive hashing via mechanical behavior,
EMMA LEJEUNE,SarcGraph: a Python package for analyzing the contractile behavior of pluripotent stem cell-derived cardiomyocytes,
JAMES CAMPBELL,NRXN3 Is a Novel Locus for Waist Circumference: A Genome-Wide Association Study from the CHARGE Consortium,"Central abdominal fat is a strong risk factor for diabetes and cardiovascular disease. To identify common variants influencing central abdominal fat, we conducted a two-stage genome-wide association analysis for waist circumference (WC). In total, three loci reached genome-wide significance. In stage 1, 31,373 individuals of Caucasian descent from eight cohort studies confirmed the role of FTO and MC4R and identified one novel locus associated with WC in the neurexin 3 gene [NRXN3 (rs10146997, p = 6.4×10−7)]. The association with NRXN3 was confirmed in stage 2 by combining stage 1 results with those from 38,641 participants in the GIANT consortium (p = 0.009 in GIANT only, p = 5.3×10−8 for combined analysis, n = 70,014). Mean WC increase per copy of the G allele was 0.0498 z-score units (0.65 cm). This SNP was also associated with body mass index (BMI) [p = 7.4×10−6, 0.024 z-score units (0.10 kg/m2) per copy of the G allele] and the risk of obesity (odds ratio 1.13, 95% CI 1.07–1.19; p = 3.2×10−5 per copy of the G allele). The NRXN3 gene has been previously implicated in addiction and reward behavior, lending further evidence that common forms of obesity may be a central nervous system-mediated disorder. Our findings establish that common variants in NRXN3 are associated with WC, BMI, and obesity. Author Summary Obesity is a major health concern worldwide. In the past two years, genome-wide association studies of DNA markers known as SNPs (single nucleotide polymorphisms) have identified two novel genetic factors that may help scientists better understand why some people may be more susceptible to obesity. Similarly, this paper describes results from a large scale genome-wide association analysis for obesity susceptibility genes that includes 31,373 individuals from 8 separate studies. We uncovered a new gene influencing waist circumference, the neurexin 3 gene (NRXN3), which has been previously implicated in studies of addiction and reward behavior. These findings lend further evidence that our genes may influence our desire and consumption of food and, in turn, our susceptibility to obesity."
JAMES CAMPBELL,Church and state under Henry II,
JAMES CAMPBELL,NFIA Haploinsufficiency Is Associated with a CNS Malformation Syndrome and Urinary Tract Defects,"Complex central nervous system (CNS) malformations frequently coexist with other developmental abnormalities, but whether the associated defects share a common genetic basis is often unclear. We describe five individuals who share phenotypically related CNS malformations and in some cases urinary tract defects, and also haploinsufficiency for the NFIA transcription factor gene due to chromosomal translocation or deletion. Two individuals have balanced translocations that disrupt NFIA. A third individual and two half-siblings in an unrelated family have interstitial microdeletions that include NFIA. All five individuals exhibit similar CNS malformations consisting of a thin, hypoplastic, or absent corpus callosum, and hydrocephalus or ventriculomegaly. The majority of these individuals also exhibit Chiari type I malformation, tethered spinal cord, and urinary tract defects that include vesicoureteral reflux. Other genes are also broken or deleted in all five individuals, and may contribute to the phenotype. However, the only common genetic defect is NFIA haploinsufficiency. In addition, previous analyses of Nfia−/− knockout mice indicate that Nfia deficiency also results in hydrocephalus and agenesis of the corpus callosum. Further investigation of the mouse Nfia+/− and Nfia−/− phenotypes now reveals that, at reduced penetrance, Nfia is also required in a dosage-sensitive manner for ureteral and renal development. Nfia is expressed in the developing ureter and metanephric mesenchyme, and Nfia+/− and Nfia−/− mice exhibit abnormalities of the ureteropelvic and ureterovesical junctions, as well as bifid and megaureter. Collectively, the mouse Nfia mutant phenotype and the common features among these five human cases indicate that NFIA haploinsufficiency contributes to a novel human CNS malformation syndrome that can also include ureteral and renal defects. Author Summary Central nervous system (CNS) and urinary tract abnormalities are common human malformations, but their variability and genetic complexity make it difficult to identify the responsible genes. Analysis of human chromosomal abnormalities associated with such disorders offers one approach to this problem. In five individuals described herein, a novel human syndrome that involves both CNS and urinary tract defects is associated with chromosomal disruption or deletion of NFIA, encoding a member of the Nuclear Factor I (NFI) family of transcription factors. This syndrome includes brain abnormalities (abnormal corpus callosum, hydrocephalus, ventriculomegaly, and Chiari type I malformation), spinal abnormalities (tethered spinal cord), and urinary tract abnormalities (vesicoureteral reflux). Nfia disruption in mice was already known to cause hydrocephalus and abnormal corpus callosum, and is now shown to exhibit renal defects and disturbed ureteral development. Other genes besides NFIA are also disrupted or deleted and may contribute to the observed phenotype. However, loss of one copy of NFIA is the only genetic defect common to all five patients. The authors thus provide evidence that genetic loss of NFIA contributes to a distinct CNS malformation syndrome with urinary tract defects of variable penetrance."
CHRISTOPHER MA,Two-dimensional non-line-of-sight scene estimation from a single edge occluder,
CHRISTOPHER MA,"Shifts in microbial diversity, composition, and functionality in the gut and genital microbiome during a natural SIV infection in vervet monkeys","BACKGROUND: The microbiota plays an important role in HIV pathogenesis in humans. Microbiota can impact health through several pathways such as increasing inflammation in the gut, metabolites of bacterial origin, and microbial translocation from the gut to the periphery which contributes to systemic chronic inflammation and immune activation and the development of AIDS. Unlike HIV-infected humans, SIV-infected vervet monkeys do not experience gut dysfunction, microbial translocation, and chronic immune activation and do not progress to immunodeficiency. Here, we provide the first reported characterization of the microbial ecosystems of the gut and genital tract in a natural nonprogressing host of SIV, wild vervet monkeys from South Africa. RESULTS: We characterized fecal, rectal, vaginal, and penile microbiomes in vervets from populations heavily infected with SIV from diverse locations across South Africa. Geographic site, age, and sex affected the vervet microbiome across different body sites. Fecal and vaginal microbiome showed marked stratification with three enterotypes in fecal samples and two vagitypes, which were predicted functionally distinct within each body site. External bioclimatic factors, biome type, and environmental temperature influenced microbiomes locally associated with vaginal and rectal mucosa. Several fecal microbial taxa were linked to plasma levels of immune molecules, for example, MIG was positively correlated with Lactobacillus and Escherichia/Shigella and Helicobacter, and IL-10 was negatively associated with Erysipelotrichaceae, Anaerostipes, Prevotella, and Anaerovibrio, and positively correlated with Bacteroidetes and Succinivibrio. During the chronic phase of infection, we observed a significant increase in gut microbial diversity, alterations in community composition (including a decrease in Proteobacteria/Succinivibrio in the gut) and functionality (including a decrease in genes involved in bacterial invasion of epithelial cells in the gut), and partial reversibility of acute infection-related shifts in microbial abundance observed in the fecal microbiome. As part of our study, we also developed an accurate predictor of SIV infection using fecal samples. CONCLUSIONS: The vervets infected with SIV and humans infected with HIV differ in microbial responses to infection. These responses to SIV infection may aid in preventing microbial translocation and subsequent disease progression in vervets, and may represent host microbiome adaptations to the virus. Video Abstract."
CHRISTOPHER MA,"SIVagm infection in wild African green monkeys from South Africa: epidemiology, natural history, and evolutionary considerations","Pathogenesis studies of SIV infection have not been performed to date in wild monkeys due to difficulty in collecting and storing samples on site and the lack of analytical reagents covering the extensive SIV diversity. We performed a large scale study of molecular epidemiology and natural history of SIVagm infection in 225 free-ranging AGMs from multiple locations in South Africa. SIV prevalence (established by sequencing pol, env, and gag) varied dramatically between infant/juvenile (7%) and adult animals (68%) (p,0.0001), and between adult females (78%) and males (57%). Phylogenetic analyses revealed an extensive genetic diversity, including frequent recombination events. Some AGMs harbored epidemiological linked viruses. Viruses infecting AGMs in the Free State, which are separated from those on the coastal side by the Drakensberg Mountains, formed a separate cluster in the phylogenetic trees; this observation supports a long standing presence of SIV in AGMs, at least from the time of their speciation to their Plio-Pleistocene migration. Specific primers/probes were synthesized based on the pol sequence data and viral loads (VLs) were quantified. VLs were of 104 –106 RNA copies/ml, in the range of those observed in experimentally-infected monkeys, validating the experimental approaches in natural hosts. VLs were significantly higher (107–108 RNA copies/ml) in 10 AGMs diagnosed as acutely infected based on SIV seronegativity (Fiebig II), which suggests a very active transmission of SIVagm in the wild. Neither cytokine levels (as biomarkers of immune activation) nor sCD14 levels (a biomarker of microbial translocation) were different between SIVinfected and SIV-uninfected monkeys. This complex algorithm combining sequencing and phylogeny, VL quantification, serology, and testing of surrogate markers of microbial translocation and immune activation permits a systematic investigation of the epidemiology, viral diversity and natural history of SIV infection in wild African natural hosts."
CHRISTOPHER MA,Optical calibration of the SNO+ detector in the water phase with deployed sources,"SNO+ is a large-scale liquid scintillator experiment with the primary goal of searching for neutrinoless double beta decay, and is located approximately 2 km underground in SNOLAB, Sudbury, Canada. The detector acquired data for two years as a pure water Cherenkov detector, starting in May 2017. During this period, the optical properties of the detector were measured in situ using a deployed light diffusing sphere, with the goal of improving the detector model and the energy response systematic uncertainties. The measured parameters included the water attenuation coefficients, effective attenuation coefficients for the acrylic vessel, and the angular response of the photomultiplier tubes and their surrounding light concentrators, all across different wavelengths. The calibrated detector model was validated using a deployed tagged gamma source, which showed a 0.6% variation in energy scale across the primary target volume."
CHRISTOPHER MA,Predicting attitudinal and behavioral responses to COVID-19 pandemic using machine learning,"At the beginning of 2020, COVID-19 became a global problem. Despite all the efforts to emphasize the relevance of preventive measures, not everyone adhered to them. Thus, learning more about the characteristics determining attitudinal and behavioral responses to the pandemic is crucial to improving future interventions. In this study, we applied machine learning on the multinational data collected by the International Collaboration on the Social and Moral Psychology of COVID-19 (N = 51,404) to test the predictive efficacy of constructs from social, moral, cognitive, and personality psychology, as well as socio-demographic factors, in the attitudinal and behavioral responses to the pandemic. The results point to several valuable insights. Internalized moral identity provided the most consistent predictive contribution-individuals perceiving moral traits as central to their self-concept reported higher adherence to preventive measures. Similar results were found for morality as cooperation, symbolized moral identity, self-control, open-mindedness, and collective narcissism, while the inverse relationship was evident for the endorsement of conspiracy theories. However, we also found a non-neglible variability in the explained variance and predictive contributions with respect to macro-level factors such as the pandemic stage or cultural region. Overall, the results underscore the importance of morality-related and contextual factors in understanding adherence to public health recommendations during the pandemic."
CHRISTOPHER MA,Multiple Independent Loci at Chromosome 15q25.1 Affect Smoking Quantity: a Meta-Analysis and Comparison with Lung Cancer and COPD,"Recently, genetic association findings for nicotine dependence, smoking behavior, and smoking-related diseases converged to implicate the chromosome 15q25.1 region, which includes the CHRNA5-CHRNA3-CHRNB4 cholinergic nicotinic receptor subunit genes. In particular, association with the nonsynonymous CHRNA5 SNP rs16969968 and correlates has been replicated in several independent studies. Extensive genotyping of this region has suggested additional statistically distinct signals for nicotine dependence, tagged by rs578776 and rs588765. One goal of the Consortium for the Genetic Analysis of Smoking Phenotypes (CGASP) is to elucidate the associations among these markers and dichotomous smoking quantity (heavy versus light smoking), lung cancer, and chronic obstructive pulmonary disease (COPD). We performed a meta-analysis across 34 datasets of European-ancestry subjects, including 38,617 smokers who were assessed for cigarettes-per-day, 7,700 lung cancer cases and 5,914 lung-cancer-free controls (all smokers), and 2,614 COPD cases and 3,568 COPD-free controls (all smokers). We demonstrate statistically independent associations of rs16969968 and rs588765 with smoking (mutually adjusted p-values<10−35 and >10−8 respectively). Because the risk alleles at these loci are negatively correlated, their association with smoking is stronger in the joint model than when each SNP is analyzed alone. Rs578776 also demonstrates association with smoking after adjustment for rs16969968 (p<10−6). In models adjusting for cigarettes-per-day, we confirm the association between rs16969968 and lung cancer (p<10−20) and observe a nominally significant association with COPD (p = 0.01); the other loci are not significantly associated with either lung cancer or COPD after adjusting for rs16969968. This study provides strong evidence that multiple statistically distinct loci in this region affect smoking behavior. This study is also the first report of association between rs588765 (and correlates) and smoking that achieves genome-wide significance; these SNPs have previously been associated with mRNA levels of CHRNA5 in brain and lung tissue. Author Summary Nicotine binds to cholinergic nicotinic receptors, which are composed of a variety of subunits. Genetic studies for smoking behavior and smoking-related diseases have implicated a genomic region that encodes the alpha5, alpha3, and beta4 subunits. We examined genetic data across this region for over 38,000 smokers, a subset of which had been assessed for lung cancer or chronic obstructive pulmonary disease. We demonstrate strong evidence that there are at least two statistically independent loci in this region that affect risk for heavy smoking. One of these loci represents a change in the protein structure of the alpha5 subunit. This work is also the first to report strong evidence of association between smoking and a group of genetic variants that are of biological interest because of their links to expression of the alpha5 cholinergic nicotinic receptor subunit gene. These advances in understanding the genetic influences on smoking behavior are important because of the profound public health burdens caused by smoking and nicotine addiction."
CHRISTOPHER MA,The SNO+ experiment,
CHRISTOPHER MA,Current status and future prospects of the SNO+ experiment,"SNO+ is a large liquid scintillator-based experiment located 2 km underground at SNOLAB, Sudbury, Canada. It reuses the Sudbury Neutrino Observatory detector, consisting of a 12 m diameter acrylic vessel which will be filled with about 780 tonnes of ultra-pure liquid scintillator. Designed as a multipurpose neutrino experiment, the primary goal of SNO+ is a search for the neutrinoless double-beta decay (0νββ) of ^130Te. In Phase I, the detector will be loaded with 0.3% natural tellurium, corresponding to nearly 800 kg of ^130Te, with an expected effective Majorana neutrino mass sensitivity in the region of 55–133 meV, just above the inverted mass hierarchy. Recently, the possibility of deploying up to ten times more natural tellurium has been investigated, which would enable SNO+ to achieve sensitivity deep into the parameter space for the inverted neutrino mass hierarchy in the future. Additionally, SNO+ aims to measure reactor antineutrino oscillations, low energy solar neutrinos, and geoneutrinos, to be sensitive to supernova neutrinos, and to search for exotic physics. A first phase with the detector filled with water will begin soon, with the scintillator phase expected to start after a few months of water data taking. The 0νββ Phase I is foreseen for 2017."
CHRISTOPHER MA,Scintillation light detection in the 6-m drift-length ProtoDUNE Dual Phase liquid argon TPC,"DUNE is a dual-site experiment for long-baseline neutrino oscillation studies, neutrino astrophysics and nucleon decay searches. ProtoDUNE Dual Phase (DP) is a 6  ×  6  ×  6 m 3 liquid argon time-projection-chamber (LArTPC) that recorded cosmic-muon data at the CERN Neutrino Platform in 2019-2020 as a prototype of the DUNE Far Detector. Charged particles propagating through the LArTPC produce ionization and scintillation light. The scintillation light signal in these detectors can provide the trigger for non-beam events. In addition, it adds precise timing capabilities and improves the calorimetry measurements. In ProtoDUNE-DP, scintillation and electroluminescence light produced by cosmic muons in the LArTPC is collected by photomultiplier tubes placed up to 7 m away from the ionizing track. In this paper, the ProtoDUNE-DP photon detection system performance is evaluated with a particular focus on the different wavelength shifters, such as PEN and TPB, and the use of Xe-doped LAr, considering its future use in giant LArTPCs. The scintillation light production and propagation processes are analyzed and a comparison of simulation to data is performed, improving understanding of the liquid argon properties."
CHRISTOPHER MA,Prospects for beyond the standard model physics searches at the deep underground neutrino experiment: DUNE collaboration,"The Deep Underground Neutrino Experiment (DUNE) will be a powerful tool for a variety of physics topics. The high-intensity proton beams provide a large neutrino flux, sampled by a near detector system consisting of a combination of capable precision detectors, and by the massive far detector system located deep underground. This configuration sets up DUNE as a machine for discovery, as it enables opportunities not only to perform precision neutrino measurements that may uncover deviations from the present three-flavor mixing paradigm, but also to discover new particles and unveil new interactions and symmetries beyond those predicted in the Standard Model (SM). Of the many potential beyond the Standard Model (BSM) topics DUNE will probe, this paper presents a selection of studies quantifying DUNE's sensitivities to sterile neutrino mixing, heavy neutral leptons, non-standard interactions, CPT symmetry violation, Lorentz invariance violation, neutrino trident production, dark matter from both beam induced and cosmogenic sources, baryon number violation, and other new physics topics that complement those at high-energy colliders and significantly extend the present reach."
CHRISTOPHER MA,Identification and reconstruction of low-energy electrons in the ProtoDUNE-SP detector,
ANDREW SABELHAUS,Trajectory optimization for thermally-actuated soft planar robot limbs,"Practical use of robotic manipulators made from soft materials requires generating and executing complex motions. We present the first approach for generating trajectories of a thermally-actuated soft robotic manipulator. Based on simplified approximations of the soft arm and its antagonistic shape-memory alloy actuator coils, we justify a dynamics model of a discretized rigid manipulator with joint torques proportional to wire temperature. Then, we propose a method to calibrate this model from experimental data and demonstrate that the simulation aligns well with a hardware test. Finally, we use a direct collocation optimization with the robot's nonlinear dynamics to generate feasible state-input trajectories from a desired reference. Three experiments validate our approach for a single-segment robot in hardware: first using a hand-derived reference trajectory, then with two teach-and-repeat tests. The results show promise for both open-loop motion generation as well as for future applications with feedback."
ANDREW SABELHAUS,"Editorial: materials, design, modeling and control of soft robotic artificial muscles.",
ANDREW SABELHAUS,Shape memory alloy (SMA) curator with embedded liquid metal curvature sensor for closed-loop control,"We introduce a soft robot actuator composed of a pre-stressed elastomer film embedded with shape memory alloy (SMA) and a liquid metal (LM) curvature sensor. SMA-based actuators are commonly used as electrically-powered limbs to enable walking, crawling, and swimming of soft robots. However, they are susceptible to overheating and long-term degradation if they are electrically stimulated before they have time to mechanically recover from their previous activation cycle. Here, we address this by embedding the soft actuator with a capacitive LM sensor capable of measuring bending curvature. The soft sensor is thin and elastic and can track curvature changes without significantly altering the natural mechanical properties of the soft actuator. We show that the sensor can be incorporated into a closed-loop ""bang-bang"" controller to ensure that the actuator fully relaxes to its natural curvature before the next activation cycle. In this way, the activation frequency of the actuator can be dynamically adapted for continuous, cyclic actuation. Moreover, in the special case of slower, low power actuation, we can use the embedded curvature sensor as feedback for achieving partial actuation and limiting the amount of curvature change."
ANDREW SABELHAUS,In-situ sensing and dynamics predictions for electrothermally-actuated soft robot limbs,"Untethered soft robots that locomote using electrothermally-responsive materials like shape memory alloy (SMA) face challenging design constraints for sensing actuator states. At the same time, modeling of actuator behaviors faces steep challenges, even with available sensor data, due to complex electrical-thermal-mechanical interactions and hysteresis. This article proposes a framework for in-situ sensing and dynamics modeling of actuator states, particularly temperature of SMA wires, which is used to predict robot motions. A planar soft limb is developed, actuated by a pair of SMA coils, that includes compact and robust sensors for temperature and angular deflection. Data from these sensors are used to train a neural network-based on the long short-term memory (LSTM) architecture to model both unidirectional (single SMA) and bidirectional (both SMAs) motion. Predictions from the model demonstrate that data from the temperature sensor, combined with control inputs, allow for dynamics predictions over extraordinarily long open-loop timescales (10 min) with little drift. Prediction errors are on the order of the soft deflection sensor's accuracy. This architecture allows for compact designs of electrothermally-actuated soft robots that include sensing sufficient for motion predictions, helping to bring these robots into practical application."
JUSTEEN HYDE,"Cross-jurisdictional resource sharing in local health departments: implications for services, quality, and cost","BACKGROUND: Forty one percent of local health departments in the U.S. serve jurisdictions with populations of 25,000 or less. Researchers, policymakers, and advocates have long questioned how to strengthen public health systems in smaller municipalities. Cross-jurisdictional sharing may increase quality of service, access to resources, and efficiency of resource use. OBJECTIVE: To characterize perceived strengths and challenges of independent and comprehensive sharing approaches, and to assess cost, quality, and breadth of services provided by independent and sharing health departments in Connecticut (CT) and Massachusetts (MA). METHODS: We interviewed local health directors or their designees from 15 comprehensive resource-sharing jurisdictions and 54 single-municipality jurisdictions in CT and MA using a semi-structured interview. Quantitative data were drawn from closed-ended questions in the semi-structured interviews; municipal demographic data were drawn from the American Community Survey and other public sources. Qualitative data were drawn from open-ended questions in the semi-structured interviews. RESULTS: The findings from this multistate study highlight advantages and disadvantages of two common public health service delivery models – independent and shared. Shared service jurisdictions provided more community health programs and services, and invested significantly more ($120 per thousand (1K) population vs. $69.5/1K population) on healthy food access activities. Sharing departments had more indicators of higher quality food safety inspections (FSIs), and there was a non-linear relationship between cost per FSI and number of FSI. Minimum cost per FSI was reached above the total number of FSI conducted by all but four of the jurisdictions sampled. Independent jurisdictions perceived their governing bodies to have greater understanding of the roles and responsibilities of local public health, while shared service jurisdictions had fewer staff per 1,000 population. IMPLICATIONS: There are trade-offs with sharing and remaining independent. Independent health departments serving small jurisdictions have limited resources but strong local knowledge. Multi-municipality departments have more resources but require more time and investment in governance and decision-making. When making decisions about the right service delivery model for a given municipality, careful consideration should be given to local culture and values. Some economies of scale may be achieved through resource sharing for municipalities <25,000 population."
LUCA ANNA KORITSANSZKY,Fractured identity: a framework for understanding young Asian American women's self-harm and suicidal behaviors,"Despite the high suicide rate among young Asian American women, the reasons for this phenomenon remain unclear. This qualitative study explored the family experiences of 16 young Asian American women who are children of immigrants and report a history of self-harm and/or suicidal behaviors. Our findings suggest that the participants experienced multiple types of ""disempowering parenting styles"" that are characterized as: abusive, burdening, culturally disjointed, disengaged, and gender-prescriptive parenting. Tied to these family dynamics is the double bind that participants suffer. Exposed to multiple types of negative parenting, the women felt paralyzed by opposing forces, caught between a deep desire to satisfy their parents' expectations as well as societal expectations and to simultaneously rebel against the image of ""the perfect Asian woman."" Torn by the double bind, these women developed a ""fractured identity,"" which led to the use of ""unsafe coping"" strategies. Trapped in a ""web of pain,"" the young women suffered alone and engaged in self-harm and suicidal behaviors."
NANCY NELSON,Content analysis of verbal interaction between psychiatric nurses and patients: an exploratory study,
ANDERS REPPEN,Factor learning portfolio optimization informed by continuous-time finance models,
AMELIA STANTON,The importance of assessing and addressing mental health barriers to PrEP use during pregnancy and postpartum in sub-Saharan Africa: state of the science and research priorities,"INTRODUCTION: Pregnant and postpartum women (PPW) in sub-Saharan Africa are at disproportionately high risk of HIV infection compared to non-pregnant women. When used consistently, pre-exposure prophylaxis (PrEP) can prevent HIV acquisition and transmission to the foetus or infant during these critical periods. Recent studies have demonstrated associations between mental health challenges (e.g. depression and traumatic stress associated with intimate partner violence) and decreased PrEP adherence and persistence, particularly among adolescents, younger women and women in the postpartum period. However, mental health is not currently a major focus of PrEP implementation research and programme planning for PPW. DISCUSSION: PrEP implementation programmes for PPW need to assess and address mental health barriers to consistent PrEP use to ensure effectiveness and sustainability in routine care. We highlight three key research priorities that will support PrEP adherence and persistence: (1) include mental health screening tools in PrEP implementation research with PPW, both to assess the feasibility of integrating these tools into routine antenatal and postpartum care and to ensure that limited resources are directed towards women whose symptoms may interfere most with PrEP use; (2) identify cross-cutting, transdiagnostic psychological mechanisms that affect consistent PrEP use during these periods and can realistically be targeted with intervention in resource-limited settings; and (3) develop/adapt and test interventions that target those underlying mechanisms, leveraging strategies from existing interventions that have successfully mitigated mental health barriers to antiretroviral therapy use among people with HIV. CONCLUSIONS: For PPW, implementation of PrEP should be guided by a robust understanding of the unique psychological difficulties that may act as barriers to uptake, adherence and persistence (i.e. sustained adherence over time). We strongly encourage PrEP implementation research in PPW to incorporate validated mental health screening tools and ultimately treatment in routine antenatal and postnatal care, and we stress the potential public health benefits of identifying women who face mental health barriers to PrEP use."
AMELIA STANTON,Global mental health: the role of collaboration during the COVID-19 pandemic,
AMELIA STANTON,"Mental health, social connectedness, and fear during the COVID-19 pandemic: a qualitative perspective from older women with HIV","Older women with HIV (WWH) confront significant biopsychosocial challenges that may be exacerbated by the COVID-19 pandemic. Between May 2020 and April 2021, following a resiliency intervention conducted as part of a randomized parent trial, 24 cisgender WWH (M = 58 years old) completed quantitative assessments and qualitative interviews exploring the impact of COVID-19 on mental health. Qualitative data were analyzed via rapid analysis. Most participants were Black (62.5%) and non-Hispanic or Latina (87.5%). Emergent themes included (1) increased anxiety and depression; (2) a loss of social connectedness; (3) fear of unknown interactions among COVID-19, HIV, and other comorbidities; and (4) the use of largely adaptive strategies to cope with these issues. Findings suggest that older WWH face significant COVID-19-related mental health challenges, compounding existing stressors. As the pandemic persists, it will be important to assess the impact of these stressors on wellbeing, identify effective coping strategies, and provide increased support to mitigate COVID-19-related mental health issues over time. Trial Registration: ClinicalTrials.gov identifier: NCT03071887."
AMELIA STANTON,"""I am scared, I do not want to lie"": exploring the impacts of COVID-19 on engagement in care, perceived health, relationship dynamics, and parenting among postpartum women with HIV in South Africa","BACKGROUND: COVID-19 and efforts to manage widespread infection may compromise HIV care engagement. The COVID-19-related factors linked to reduced HIV engagement have not been assessed among postpartum women with HIV, who are at heightened risk of attrition under non-pandemic circumstances. To mitigate the effects of the pandemic on care engagement and to prepare for future public health crises, it is critical to understand how COVID-19 has impacted (1) engagement in care and (2) factors that may act as barriers to care engagement. METHODS: A quantitative assessment of COVID-19-related experiences was added to a longitudinal cohort study assessing predictors of postpartum attrition from HIV care among women in South Africa. Participants (N = 266) completed the assessment at 6, 12, 18, or 24 months postpartum between June and November of 2020. Those who endorsed one or more challenge related to engagement in care (making or keeping HIV care appointments, procuring HIV medications, procuring contraception, and/or accessing immunization services for infants; n = 55) were invited to complete a brief qualitative interview, which explored the specific factors driving these challenges, as well as other impacts of COVID-19 on care engagement. Within this subset, 53 participants completed an interview; qualitative data were analyzed via rapid analysis. RESULTS: Participants described key challenges that reduced their engagement in HIV care and identified four other domains of COVID-19-related impacts: physical health, mental health, relationship with a partner or with the father of the baby, and motherhood/caring for the new baby. Within these domains, specific themes and subthemes emerged, with some positive impacts of COVID-19 also reported (e.g., increased quality time, improved communication with partner, HIV disclosure). Coping strategies for COVID-19-related challenges (e.g., acceptance, spirituality, distraction) were also discussed. CONCLUSIONS: About one in five participants reported challenges accessing HIV care, medications, or services, and they faced complex, multilayered barriers to remaining engaged. Physical health, mental health, relationships with partners, and ability to care for their infant were also affected. Given the dynamic nature of the pandemic and general uncertainty about its course, ongoing assessment of pandemic-related challenges among postpartum women is needed to avoid HIV care disruptions and to support wellbeing."
LINH TO,Innervation defects as a mechanism of childhood asthma,"Currently, asthma affects 25.7 million people in the United States and is increasing in prevalence worldwide, with young children at high risk. Recent studies show that early environmental exposure can lead to asthma and impair lung function. While the development of asthma is not fully understood, findings indicate that cigarette smoke, ozone, allergen, or viral exposure to an immature lung can induce changes in airway innervation. However, the mechanisms underlying these changes and how these changes affect lung function are unknown. Normally, lung innervation plays a role in coughing, sensing, and breathing. We found that embryonic lung innervation requires brain-derived neurotrophin factor (BDNF) signaling and postnatal lung innervation requires neurotrophin 4 (NT4) signaling. Since both of these neurotrophins signal through tyrosine kinase receptor B (TrkB), they have temporally distinct roles in airway smooth muscle (ASM) innervation. We show that neurotrophins are released from ASM and act as target-derived signals for ASM innervation. We also show that early allergen exposure in neonatal mice increase NT4/TrkB signaling leading to ASM hyper-innervation. Notably, genetic disruption and small molecule blockade of NT4/TrkB signaling in early allergen exposed neonates prevented both acute and persistent airway hyper-reactivity without affecting baseline airway function or inflammation. Furthermore, biophysical assays using lung slices and isolated ASM cells demonstrated that NT4 was required for ASM hyper-contractility induced by early-life allergen exposure. Together, our findings show that the NT4/TrkB dependent increase in innervation plays a critical role in altering the ASM phenotype during postnatal growth, thereby linking early-life allergen exposure to persistent airway dysfunction. Our findings may explain why children who are exposed to environmental insults often develop asthma later in life. Findings from this study may also provide new pathways and targets for novel allergic asthma therapies."
LINH TO,Daily labor supply and adaptive reference points,"This paper provides field evidence on how reference points adjust, a degree of freedom in reference-dependence models. Examining this in the context of cabdrivers' daily labor-supply behavior, we ask how the within-day timing of earnings affects decisions. Drivers work less in response to higher accumulated income, with a strong effect for recent earnings that gradually diminishes for earlier earnings. We estimate a structural model in which drivers work towards a reference point that adjusts to deviations from expected earnings with a lag. This dynamic view of reference dependence reconciles conflicting “neoclassical” and “behavioral” interpretations of evidence on daily labor-supply decisions."
ANDREI MAMOLEA,"Vespasian V. Pella: International Criminal Justice As A Safeguard Of Peace, 1919-1952","Vespasian V. Pella was an early twentieth century Romanian jurist who conceived and championed a system of international criminal justice that was designed to prevent war, punish atrocity, and vindicate humanity’s political and economic rights. He argued that governments had a duty to prevent economic dislocation as well as criminal acts capable of undermining international order. To construct these safeguards, Pella promoted the unification of domestic criminal law and the incorporation of international norms into domestic law. He also argued that an assembly of nations and an international criminal tribunal should play a subsidiary role by resolving disputes, imposing sanctions, and punishing aggression and violations of the laws of war. According to Pella, these institutions were necessary safeguards against the root cause of aggressive war—a small, disciplined, and ideologically rigid cadre of men who threatened to use the power of the state to manipulate the public into war. In this chapter we will excavate the ideas, people, and events that shaped Pella’s politics—the influence of his parents, the role of republican, pacifist, and socialist ideals, his desire to understand the psychological reactions of individuals and crowds, as well his response to the atrocities of the First World War. It was these atrocities that ultimately led Pella to dedicate his life to advancing what he called the “international criminal law of the future.” He did so through his scholarship, his diplomacy, and above all, through his advocacy in the organizations such as the Association International de Droit Pénale and the Bureau International pour l’Unification du Droit Pénal. In a relatively short period, Pella forged a professional consensus on previously controversial questions such as universal jurisdiction and corporate criminal liability. Yet though his ideas made great strides within the legal profession, the political support necessary to bring his system to life never materialized, either before or after the Second World War. We will examine the obstacles that Pella encountered and the underlying values of his project. Finally, the chapter challenges and overturns the many misconceptions about Pella that have proliferated in recent years and calls for further research into his life and work."
ANDREW B STEIN,"Aceso: v. 6, no. 1",
ANDREW B STEIN,Scintillation light detection in the 6-m drift-length ProtoDUNE Dual Phase liquid argon TPC,"DUNE is a dual-site experiment for long-baseline neutrino oscillation studies, neutrino astrophysics and nucleon decay searches. ProtoDUNE Dual Phase (DP) is a 6  ×  6  ×  6 m 3 liquid argon time-projection-chamber (LArTPC) that recorded cosmic-muon data at the CERN Neutrino Platform in 2019-2020 as a prototype of the DUNE Far Detector. Charged particles propagating through the LArTPC produce ionization and scintillation light. The scintillation light signal in these detectors can provide the trigger for non-beam events. In addition, it adds precise timing capabilities and improves the calorimetry measurements. In ProtoDUNE-DP, scintillation and electroluminescence light produced by cosmic muons in the LArTPC is collected by photomultiplier tubes placed up to 7 m away from the ionizing track. In this paper, the ProtoDUNE-DP photon detection system performance is evaluated with a particular focus on the different wavelength shifters, such as PEN and TPB, and the use of Xe-doped LAr, considering its future use in giant LArTPCs. The scintillation light production and propagation processes are analyzed and a comparison of simulation to data is performed, improving understanding of the liquid argon properties."
ANDREW B STEIN,Identification and reconstruction of low-energy electrons in the ProtoDUNE-SP detector,
CHARLES WILLIAMS,The effect of industrial demands on the training of office workers in business schools,
CHARLES WILLIAMS,Patterns of research utilization on patient care units,"BACKGROUND: Organizational context plays a central role in shaping the use of research by healthcare professionals. The largest group of professionals employed in healthcare organizations is nurses, putting them in a position to influence patient and system outcomes significantly. However, investigators have often limited their study on the determinants of research use to individual factors over organizational or contextual factors. METHODS: The purpose of this study was to examine the determinants of research use among nurses working in acute care hospitals, with an emphasis on identifying contextual determinants of research use. A comparative ethnographic case study design was used to examine seven patient care units (two adult and five pediatric units) in four hospitals in two Canadian provinces (Ontario and Alberta). Data were collected over a six-month period by means of quantitative and qualitative approaches using an array of instruments and extensive fieldwork. The patient care unit was the unit of analysis. Drawing on the quantitative data and using correspondence analysis, relationships between various factors were mapped using the coefficient of variation. RESULTS: Units with the highest mean research utilization scores clustered together on factors such as nurse critical thinking dispositions, unit culture (as measured by work creativity, work efficiency, questioning behavior, co-worker support, and the importance nurses place on access to continuing education), environmental complexity (as measured by changing patient acuity and re-sequencing of work), and nurses' attitudes towards research. Units with moderate research utilization clustered on organizational support, belief suspension, and intent to use research. Higher nursing workloads and lack of people support clustered more closely to units with the lowest research utilization scores. CONCLUSION: Modifiable characteristics of organizational context at the patient care unit level influences research utilization by nurses. These findings have implications for patient care unit structures and offer beginning direction for the development of interventions to enhance research use by nurses."
CHARLES WILLIAMS,Industrial accident prevention in the Boston area,
CHARLES WILLIAMS,Analysis of criticisms of high school chemistry courses,
CHARLES WILLIAMS,A survey of the relative merits of the single and double session plans of elementary school organization,
CHARLES WILLIAMS,The function of faith in the light of psychotherapy,
CHARLES WILLIAMS,"Bostonia: v. 11, no. 1-5, 7, 9-10",
CHARLES WILLIAMS,"Bostonia: v. 18, no. 1-9",
CHARLES WILLIAMS,"Bostonia: 1999-2000, no. 1-4",
CHARLES WILLIAMS,"Bostonia: v. 16, no. 1-9",
CHARLES WILLIAMS,"BMQ : Boston medical quarterly: v. 16, no. 1-4",
CHARLES WILLIAMS,"BMQ : Boston medical quarterly: v. 8, no. 1-4",
CHARLES WILLIAMS,Spengler's philosophy of history.,
CHARLES WILLIAMS,"BMQ : Boston medical quarterly: v. 2, no. 1-4",
CHARLES WILLIAMS,"Double your corners, double your fun: the doorway camera","In a built environment, wanting to see without direct line of sight is often due to being outside a doorway. The two vertical edges of the doorway provide occlusions that can be exploited for non-line-of-sight imaging by forming corner cameras. While each corner camera can separately yield a robust 1D reconstruction, joint processing suggests novelties in both forward modeling and inversion. The resulting doorway camera provides accurate and robust 2D reconstructions of the hidden scene. This work provides a novel inversion algorithm to jointly estimate two views of change in the hidden scene, using the temporal difference between photographs acquired on the visible side of the doorway. Successful reconstruction is demonstrated in a variety of real and rendered scenarios, including different hidden scenes and lighting conditions. A Cramer-Rao bound analysis is used to demonstrate the 2D resolving power of the doorway camera over other passive acquisition strategies and to motivate the novel biangular reconstruction grid."
CHARLES WILLIAMS,First M87 Event Horizon Telescope results. IV. Imaging the central supermassive black hole,
CHARLES WILLIAMS,The social implications of the Sermon on the Mount,
CHARLES WILLIAMS,EEG complexity as a biomarker for autism spectrum disorder risk,"BACKGROUND: Complex neurodevelopmental disorders may be characterized by subtle brain function signatures early in life before behavioral symptoms are apparent. Such endophenotypes may be measurable biomarkers for later cognitive impairments. The nonlinear complexity of electroencephalography (EEG) signals is believed to contain information about the architecture of the neural networks in the brain on many scales. Early detection of abnormalities in EEG signals may be an early biomarker for developmental cognitive disorders. The goal of this paper is to demonstrate that the modified multiscale entropy (mMSE) computed on the basis of resting state EEG data can be used as a biomarker of normal brain development and distinguish typically developing children from a group of infants at high risk for autism spectrum disorder (ASD), defined on the basis of an older sibling with ASD. METHODS: Using mMSE as a feature vector, a multiclass support vector machine algorithm was used to classify typically developing and high-risk groups. Classification was computed separately within each age group from 6 to 24 months. RESULTS: Multiscale entropy appears to go through a different developmental trajectory in infants at high risk for autism (HRA) than it does in typically developing controls. Differences appear to be greatest at ages 9 to 12 months. Using several machine learning algorithms with mMSE as a feature vector, infants were classified with over 80% accuracy into control and HRA groups at age 9 months. Classification accuracy for boys was close to 100% at age 9 months and remains high (70% to 90%) at ages 12 and 18 months. For girls, classification accuracy was highest at age 6 months, but declines thereafter. CONCLUSIONS: This proof-of-principle study suggests that mMSE computed from resting state EEG signals may be a useful biomarker for early detection of risk for ASD and abnormalities in cognitive development in infants. To our knowledge, this is the first demonstration of an information theoretic analysis of EEG data for biomarkers in infants at risk for a complex neurodevelopmental disorder."
CHARLES WILLIAMS,Response: infant EEG activity as a biomarker for autism: a promising approach or a false promise?,
CHARLES WILLIAMS,The authority of the teaching of Jesus,
CHARLES WILLIAMS,The social aspects of Wesley's works,
CHARLES WILLIAMS,"BMQ : Boston medical quarterly: v. 12, no. 1-4",
CHARLES WILLIAMS,Paul as a missionary,
CHARLES WILLIAMS,Cost-effectiveness of tyrosine kinase inhibitor treatment strategies for chronic myeloid leukemia in chronic phase after generic entry of imatinib in the United States,"BACKGROUND: We analyzed the cost-effectiveness of treating incident chronic myeloid leukemia in chronic phase (CML-CP) with generic imatinib when it becomes available in United States in 2016. In the year following generic entry, imatinib's price is expected to drop 70% to 90%. We hypothesized that initiating treatment with generic imatinib in these patients and then switching to the other tyrosine-kinase inhibitors (TKIs), dasatinib or nilotinib, because of intolerance or lack of effectiveness (""imatinib-first"") would be cost-effective compared with the current standard of care: ""physicians' choice"" of initiating treatment with any one of the three TKIs. METHODS: We constructed Markov models to compare the five-year cost-effectiveness of imatinib-first vs physician's choice from a US commercial payer perspective, assuming 3% annual discounting ($US 2013). The models' clinical endpoint was five-year overall survival taken from a systematic review of clinical trial results. Per-person spending on incident CML-CP treatment overall care components was estimated using Truven's MarketScan claims data. The main outcome of the models was cost per quality-adjusted life-year (QALY). We interpreted outcomes based on a willingness-to-pay threshold of $100 000/QALY. A panel of European LeukemiaNet experts oversaw the study's conduct. RESULTS: Both strategies met the threshold. Imatinib-first ($277 401, 3.87 QALYs) offered patients a 0.10 decrement in QALYs at a savings of $88 343 over five years to payers compared with physician's choice ($365 744, 3.97 QALYs). The imatinib-first incremental cost-effectiveness ratio was approximately $883 730/QALY. The results were robust to multiple sensitivity analyses. CONCLUSION: When imatinib loses patent protection and its price declines, its use will be the cost-effective initial treatment strategy for CML-CP."
CHARLES WILLIAMS,"Bostonia, first series: v. 9, 1-4",
CHARLES WILLIAMS,Social implications in modern drama,
CHARLES WILLIAMS,First Sagittarius A* Event Horizon Telescope results. V. Testing astrophysical models of the galactic center black hole,"In this paper we provide a first physical interpretation for the Event Horizon Telescope's (EHT) 2017 observations of Sgr A*. Our main approach is to compare resolved EHT data at 230 GHz and unresolved non-EHT observations from radio to X-ray wavelengths to predictions from a library of models based on time-dependent general relativistic magnetohydrodynamics simulations, including aligned, tilted, and stellar-wind-fed simulations; radiative transfer is performed assuming both thermal and nonthermal electron distribution functions. We test the models against 11 constraints drawn from EHT 230 GHz data and observations at 86 GHz, 2.2 μm, and in the X-ray. All models fail at least one constraint. Light-curve variability provides a particularly severe constraint, failing nearly all strongly magnetized (magnetically arrested disk (MAD)) models and a large fraction of weakly magnetized models. A number of models fail only the variability constraints. We identify a promising cluster of these models, which are MAD and have inclination i ≤ 30°. They have accretion rate (5.2–9.5) × 10−9 M ⊙ yr−1, bolometric luminosity (6.8–9.2) × 1035 erg s−1, and outflow power (1.3–4.8) × 1038 erg s−1. We also find that all models with i ≥ 70° fail at least two constraints, as do all models with equal ion and electron temperature; exploratory, nonthermal model sets tend to have higher 2.2 μm flux density; and the population of cold electrons is limited by X-ray constraints due to the risk of bremsstrahlung overproduction. Finally, we discuss physical and numerical limitations of the models, highlighting the possible importance of kinetic effects and duration of the simulations."
CHARLES WILLIAMS,First M87 Event Horizon Telescope results. VII. Polarization of the ring,"In 2017 April, the Event Horizon Telescope (EHT) observed the near-horizon region around the supermassive black hole at the core of the M87 galaxy. These 1.3 mm wavelength observations revealed a compact asymmetric ring-like source morphology. This structure originates from synchrotron emission produced by relativistic plasma located in the immediate vicinity of the black hole. Here we present the corresponding linear-polarimetric EHT images of the center of M87. We find that only a part of the ring is significantly polarized. The resolved fractional linear polarization has a maximum located in the southwest part of the ring, where it rises to the level of ∼15%. The polarization position angles are arranged in a nearly azimuthal pattern. We perform quantitative measurements of relevant polarimetric properties of the compact emission and find evidence for the temporal evolution of the polarized source structure over one week of EHT observations. The details of the polarimetric data reduction and calibration methodology are provided. We carry out the data analysis using multiple independent imaging and modeling techniques, each of which is validated against a suite of synthetic data sets. The gross polarimetric structure and its apparent evolution with time are insensitive to the method used to reconstruct the image. These polarimetric images carry information about the structure of the magnetic fields responsible for the synchrotron emission. Their physical interpretation is discussed in an accompanying publication."
CHARLES WILLIAMS,Resolving the inner parsec of the blazar J1924–2914 with the event horizon telescope,"The blazar J1924–2914 is a primary Event Horizon Telescope (EHT) calibrator for the Galactic center’s black hole Sagittarius A*. Here we present the first total and linearly polarized intensity images of this source obtained with the unprecedented 20 μas resolution of the EHT. J1924–2914 is a very compact flat-spectrum radio source with strong optical variability and polarization. In April 2017 the source was observed quasi-simultaneously with the EHT (April 5–11), the Global Millimeter VLBI Array (April 3), and the Very Long Baseline Array (April 28), giving a novel view of the source at four observing frequencies, 230, 86, 8.7, and 2.3 GHz. These observations probe jet properties from the subparsec to 100 pc scales. We combine the multifrequency images of J1924–2914 to study the source morphology. We find that the jet exhibits a characteristic bending, with a gradual clockwise rotation of the jet projected position angle of about 90° between 2.3 and 230 GHz. Linearly polarized intensity images of J1924–2914 with the extremely fine resolution of the EHT provide evidence for ordered toroidal magnetic fields in the blazar compact core."
CHARLES WILLIAMS,The place and function of emotion in religion,
CHARLES WILLIAMS,"Bostonia: v. 10, no. 1-10",
CHARLES WILLIAMS,"Bostonia: v. 12, no. 1-6, 8-10",
CHARLES WILLIAMS,"BMQ : Boston medical quarterly: v. 9, no. 1-4",
CHARLES WILLIAMS,"The social philosophy of William Godwin, with special reference to his religion",
CHARLES WILLIAMS,Patterns of Research Utilization on Patient Care Units,"BACKGROUND. Organizational context plays a central role in shaping the use of research by healthcare professionals. The largest group of professionals employed in healthcare organizations is nurses, putting them in a position to influence patient and system outcomes significantly. However, investigators have often limited their study on the determinants of research use to individual factors over organizational or contextual factors. METHODS. The purpose of this study was to examine the determinants of research use among nurses working in acute care hospitals, with an emphasis on identifying contextual determinants of research use. A comparative ethnographic case study design was used to examine seven patient care units (two adult and five pediatric units) in four hospitals in two Canadian provinces (Ontario and Alberta). Data were collected over a six-month period by means of quantitative and qualitative approaches using an array of instruments and extensive fieldwork. The patient care unit was the unit of analysis. Drawing on the quantitative data and using correspondence analysis, relationships between various factors were mapped using the coefficient of variation. RESULTS. Units with the highest mean research utilization scores clustered together on factors such as nurse critical thinking dispositions, unit culture (as measured by work creativity, work efficiency, questioning behavior, co-worker support, and the importance nurses place on access to continuing education), environmental complexity (as measured by changing patient acuity and re-sequencing of work), and nurses' attitudes towards research. Units with moderate research utilization clustered on organizational support, belief suspension, and intent to use research. Higher nursing workloads and lack of people support clustered more closely to units with the lowest research utilization scores. CONCLUSION. Modifiable characteristics of organizational context at the patient care unit level influences research utilization by nurses. These findings have implications for patient care unit structures and offer beginning direction for the development of interventions to enhance research use by nurses."
CHARLES WILLIAMS,"BUSM Alumni News Letter: April 1957 v. 1, no. 1",
CHARLES WILLIAMS,Comparative study of the schools for secondary instruction in Europe and America,
CHARLES WILLIAMS,A critical history of glossolalia,
CHARLES WILLIAMS,Berdyaev's philosophy of history,"The problem of this dissertation is to present a critical exposition of Nicolas Berdyaev's philosophy of history. An adequate philosophy of history will not be reached by attention to objective, abstract, and general conceptions which but lead to intellectual superstructures devoid of substantial existence, but only by concentration upon the subjective, the concrete, and the particular which are of the very substance of history. This personalistic, anthropological, existential approach sees history not as an objective process, but primarily as a spiritual event which may he apprehended inwardly through the ""historical memory"" stimulated into activity by the acceptance of religious myth, the greatest being that of the Fall. This myth is not to be accepted conventionally, naturalistically, nor literally but to be subjected to a speculative process leading to the discovery of deeper spiritual truths revelatory of inner reality. The temporal, historical, physical facts are not self-explanatory and self-sufficient but are to be comprehended in terms of the eternal, metaphysical, spiritual reality. This latter is not a static, monistic, immobile perfection but contains a tragic potential reveal1ed in a dynamic dualism.. It is most adequately portrayed in the Christian myth of the divine Hypostases and further illustrated by the German mystical doctrine of the Gottheit or Ungrund from which by a theogonic process the creator-God and freedom are manifested. God's creative task is conquering the freedom of non-being. Evil is accounted for by God's powerlessness over this recalcitrant factor. Man, the image and likeness of God and possessing freedom, is called to creative cooperation in God's work but rebels against Him only to discover his freedom illusory as he becomes enslaved to his lower nature. Man gains freedom and is restored to his original wholeness through the mediation of Christ the God-Man whose grace enables man to make freely his response to God and to help in the work of conquering non-being. Man and his historical existence are thus rooted in the eternal and make a contribution to God in whom alone their meaning is retained. The scientific study of man beginning with his immersion in nature presupposes his Fall. Ancient non-Jewish cultures lack with primitive man an historical sense in being thoroughly at home in and forming a part of nature. Opposing the resulting static or cyclical notion of history the Jews developed an historical dynamism marked by extreme futuristic expectations largely seen in terms of terrestrial fulfillment. Jesus did not fulfil these expectations and was rejected as the Messiah by most of the Jews. Christianity made historical development possible by emphasizing the freedom of the human spirit as over against the pre-Christian concern with either nature or God. Christianity delivered man from his bondage to nature but drove a wedge between natural and spiritual man. Man retired from the natural to the spiritual world where after struggle with the baser elements of his being he emerged a free human personality. The Middle Ages forged and fortified human personality by its ascetic discipline and centralized spiritual authority. Its hope to realize the Kingdom of God failed since this cannot be established without the free consent and participation of man. Berdyaev sees modern history as the testing of human liberty. Humanism rebels against medieval subjection in affirming man's self-confidence, but it ends in debasing him through severing his celestial ties and in seeing him as but a part of nature. The Renaissance rediscovered natural man and antiquity, witnessed the clash of pagan and Christian principles, and saw their partial reconciliation in the great symbolic art of the age. The Reformation affirmed man's freedom from ecclesiastical compulsion, but it debased man by denying his primal freedom before God. The Enlightenment affirmed man's self-sufficient reason, but in denying any mystery it debased man's ability to know. The French Revolution affirmed man's ability to change history but ended in denying all human rights. Romanticism affirmed man's spiritual resources but denied his ultimate destiny. Industrialism affirmed man's liberation from nature but denied his integrity and dignity. Modern history by a fatal dialectic sees humanism paradoxically be coming inhumanism; the denial of God tending to the denial of man. Berdyaev sees the present as the beginning of a new barbarism the inhumanity of which is manifest in the total war system where human lives are regarded as mere means; in capitalism under which man is enslaved and oppressed by property; in collectivism where the organization becomes the end and man, the instrument; in a morality of bestialism which permits the use of man in any way to attain inhuman or anti-human ends; in cultural manifestations in literature, science, philosophy, and theology which interpret integral man in terms of a part; in politics with sham democracy's concern with only abstract, formal political freedom and the totalitarian's rejection of all freedom; in the dictator led masses in which all individuality is ruthlessly obliterated; in the intensification of racialism and nationalism which find the existential center in entities other than human personality; in the tyranny of Caesars who rule by appeal to instinct and emotion; and in a Christianity that largely conforms to the world. This barbarism may lead on to the new Middle Ages which Berdyaev sees marked by the end of humanism, individualism, and formal liberalism; a simplified material culture; political universalism.; religious collectivism; and a social order of the syndicalist type. Berdyaev rejects the progressive and cyclical notions of history. Though Western civilization is at a crisis, he postulates the possibility of salvation through religious transfiguration. This will take place only as Christianity and creation are combined, i.e. as a dynamic, creative conception of the Church becomes actualized and incarnated in all areas of life. Man's ethical task is one of creativity as in contemplation he acquires the intuitive insight of genius and in self-sacrificing love he actualizes his inspiration. Death signifies that there is no eternity in time and that an endless temporal series is meaningless. Man's role in this situation is to forsake the creation of temporary, transitory, and corruptible goods and to devote himself to the creation of eternal, permanent, and immortal values. Hell, which results from man's separating himself from God, can be escaped only through the God-man Christ and those who are spiritually his . Paradise comes not only at the end of time but at every moment as a God-humanism by creative activity redeems, transfigures, and Christianizes the entire cosmos. The following problems and observations are some of the results of the investigation. (1) Berdyaev's philosophy of history, reflecting his own unique spiritual experience, is peculiarly personal. The best approach to his writings is through his life, and the best insight into his life is to be gained through his writings. (2) In his quest for historical insight Berdyaev stresses ""knowledge by acquaintance"" as over against ""knowledge about."" Reason is seen primarily not as abstract, but as a personal experience. This leads to an appreciation of the ways of knowing by intuition, mysticism, revelation, and faith, but fails to carry full conviction because of hesitancy to criticize rationally the claims of these experiences or to suggest ways by which their claims might he validated. (3) Berdyaev envisages the ultimate world-ground in terms of Christian supernaturalism but without making any effort to justify rationally this metaphysical outlook. However, within this tradition he makes full use of reason in attacking the inadequacies of theistic absolutism and in establishing the case for a theistic finitism. This suggests the problem as to just what is his attitude toward reason. (4) Berdyaev's recourse to the doctrine of the Ungrund involves a number of contradictions: it is in direct opposit ion to his epistemological position in being a highly rationalistic notion; while it is a negative conception to which none of the forms of thought are applicable, a number of rationalistic categories are used in its elucidation; while by means of it movement is introduced into the worldground, this is accomplished only by relegating the revealed Biblical God to a secondary position; while movement is made central, the means whereby the whole theogonic process takes place in unconvincing and inadequate. (5) Berdyaev's notion of man is daring and forms the basis of the historical process. Since man is correlative with God, his creative historical experience is meaningful in making a contribution to the very nature of reality. Yet Berdyaev seems to rob the terrestrial historical process of some of its meaning when he insists that it is the result of man's sin and fall--events which took place in a preexistent state. (6) Berdyaev's view of modern history as the record of the testing of human freedom is a well-chosen category to link together the multifarious activities of Western man. As an interpretation of modern history it is not necessarily dependent upon Berdyaev's particular epistemological and metaphysical views but would be equally valid linked with most other traditional and modern expositions of the significance of Christianity. (7) Berdyaev's analysis of the threat to contemporary man is convincing in the light of recent history. His diagnosis of its cause as lying in the realm of the spiritual receives corroboration from many other cultural historians. His prognosis of its dire future course has not yet been counteracted by the logic of events. His therapy is, however, marked by apocalyptic vision, lacks a clear and wise grasp of the issues and probabilities of the immediate future, and thus is vague in its positive suggestions as to what is to be done. (8) Berdyaev's philosophy of history is marked throughout by an effort to avoid the extremes of thought that are currently manifested in the conflict between neo-orthodoxy and naturalistic humanism. His is a constant effort to mediate between man and God, freedom and grace, nature and supernature, time and eternity."
CHARLES WILLIAMS,Analysis of the shoe industry to determine the desirability of entering the field as a manufacturer of footwear,
CHARLES WILLIAMS,"Some socio-psychological factors in the total adjustment of psychoneurotic veterans: a study of twenty-five psychoneurotic veterans referred to the Social Service Unit, Boston Regional Office Veterans Administration, July 1, 1947 to June 30, 1948",
CHARLES WILLIAMS,"Annotated bibliography of published articles, speeches and books having significance to public relations",
CHARLES WILLIAMS,Prediction of autism spectrum disorder diagnosis using nonlinear measures of language-related EEG at 6 and 12 months,"BACKGROUND: Early identification of autism spectrum disorder (ASD) provides an opportunity for early intervention and improved developmental outcomes. The use of electroencephalography (EEG) in infancy has shown promise in predicting later ASD diagnoses and in identifying neural mechanisms underlying the disorder. Given the high co-morbidity with language impairment, we and others have speculated that infants who are later diagnosed with ASD have altered language learning, including phoneme discrimination. Phoneme learning occurs rapidly in infancy, so altered neural substrates during the first year of life may serve as early, accurate indicators of later autism diagnosis. METHODS: Using EEG data collected at two different ages during a passive phoneme task in infants with high familial risk for ASD, we compared the predictive accuracy of a combination of feature selection and machine learning models at 6 months (during native phoneme learning) and 12 months (after native phoneme learning), and we identified a single model with strong predictive accuracy (100%) for both ages. Samples at both ages were matched in size and diagnoses (n = 14 with later ASD; n = 40 without ASD). Features included a combination of power and nonlinear measures across the 10‑20 montage electrodes and 6 frequency bands. Predictive features at each age were compared both by feature characteristics and EEG scalp location. Additional prediction analyses were performed on all EEGs collected at 12 months; this larger sample included 67 HR infants (27 HR-ASD, 40 HR-noASD). RESULTS: Using a combination of Pearson correlation feature selection and support vector machine classifier, 100% predictive diagnostic accuracy was observed at both 6 and 12 months. Predictive features differed between the models trained on 6- versus 12-month data. At 6 months, predictive features were biased to measures from central electrodes, power measures, and frequencies in the alpha range. At 12 months, predictive features were more distributed between power and nonlinear measures, and biased toward frequencies in the beta range. However, diagnosis prediction accuracy substantially decreased in the larger, more behaviorally heterogeneous 12-month sample. CONCLUSIONS: These results demonstrate that speech processing EEG measures can facilitate earlier identification of ASD but emphasize the need for age-specific predictive models with large sample sizes to develop clinically relevant classification algorithms."
CHARLES WILLIAMS,A study of the social and environmental factors involved in the delinquent behavior of children of more than average intelligence committed for observation at the Metropolitan State Hospital,
CHARLES WILLIAMS,"Bostonia: v. 9, no. 1-10",
CHARLES WILLIAMS,A universal power-law prescription for variability from synthetic images of black hole accretion flows,"We present a framework for characterizing the spatiotemporal power spectrum of the variability expected from the horizon-scale emission structure around supermassive black holes, and we apply this framework to a library of general relativistic magnetohydrodynamic (GRMHD) simulations and associated general relativistic ray-traced images relevant for Event Horizon Telescope (EHT) observations of Sgr A*. We find that the variability power spectrum is generically a red-noise process in both the temporal and spatial dimensions, with the peak in power occurring on the longest timescales and largest spatial scales. When both the time-averaged source structure and the spatially integrated light-curve variability are removed, the residual power spectrum exhibits a universal broken power-law behavior. On small spatial frequencies, the residual power spectrum rises as the square of the spatial frequency and is proportional to the variance in the centroid of emission. Beyond some peak in variability power, the residual power spectrum falls as that of the time-averaged source structure, which is similar across simulations; this behavior can be naturally explained if the variability arises from a multiplicative random field that has a steeper high-frequency power-law index than that of the time-averaged source structure. We briefly explore the ability of power spectral variability studies to constrain physical parameters relevant for the GRMHD simulations, which can be scaled to provide predictions for black holes in a range of systems in the optically thin regime. We present specific expectations for the behavior of the M87* and Sgr A* accretion flows as observed by the EHT."
CHARLES WILLIAMS,Millimeter light curves of Sagittarius A* observed during the 2017 Event Horizon Telescope campaign,"The Event Horizon Telescope (EHT) observed the compact radio source, Sagittarius A* (Sgr A*), in the Galactic Center on 2017 April 5–11 in the 1.3 mm wavelength band. At the same time, interferometric array data from the Atacama Large Millimeter/submillimeter Array and the Submillimeter Array were collected, providing Sgr A* light curves simultaneous with the EHT observations. These data sets, complementing the EHT very long baseline interferometry, are characterized by a cadence and signal-to-noise ratio previously unattainable for Sgr A* at millimeter wavelengths, and they allow for the investigation of source variability on timescales as short as a minute. While most of the light curves correspond to a low variability state of Sgr A*, the April 11 observations follow an X-ray flare and exhibit strongly enhanced variability. All of the light curves are consistent with a red-noise process, with a power spectral density (PSD) slope measured to be between −2 and −3 on timescales between 1 minute and several hours. Our results indicate a steepening of the PSD slope for timescales shorter than 0.3 hr. The spectral energy distribution is flat at 220 GHz, and there are no time lags between the 213 and 229 GHz frequency bands, suggesting low optical depth for the event horizon scale source. We characterize Sgr A*’s variability, highlighting the different behavior observed just after the X-ray flare, and use Gaussian process modeling to extract a decorrelation timescale and a PSD slope. We also investigate the systematic calibration uncertainties by analyzing data from independent data reduction pipelines."
CHARLES WILLIAMS,"Edge-resolved transient imaging: performance analyses, optimizations, and simulations","Edge-resolved transient imaging (ERTI) is a method for non-line-of-sight imaging that combines the use of direct time of flight for measuring distances with the azimuthal angular resolution afforded by a vertical edge occluder. Recently conceived and demonstrated for the first time, no performance analyses or optimizations of ERTI have appeared in published papers. This paper explains how the difficulty of detection of hidden scene objects with ERTI depends on a variety of parameters, including illumination power, acquisition time, ambient light, visible-side reflectivity, hidden-side reflectivity, target range, and target azimuthal angular position. Based on this analysis, optimization of the acquisition process is introduced whereby the illumination dwell times are varied to counteract decreasing signal-to-noise ratio at deeper angles into the hidden volume. Inaccuracy caused by a coaxial approximation is also analyzed and simulated."
CHARLES WILLIAMS,"Bostonia: v. 15, no. 1-10",
CHARLES WILLIAMS,"Bostonia: v. 14, no. 1-10",
CHARLES WILLIAMS,First Sagittarius A* Event Horizon Telescope results. VI. Testing the black hole metric,"Astrophysical black holes are expected to be described by the Kerr metric. This is the only stationary, vacuum, axisymmetric metric, without electromagnetic charge, that satisfies Einstein’s equations and does not have pathologies outside of the event horizon. We present new constraints on potential deviations from the Kerr prediction based on 2017 EHT observations of Sagittarius A* (Sgr A*). We calibrate the relationship between the geometrically defined black hole shadow and the observed size of the ring-like images using a library that includes both Kerr and non-Kerr simulations. We use the exquisite prior constraints on the mass-to-distance ratio for Sgr A* to show that the observed image size is within ∼10% of the Kerr predictions. We use these bounds to constrain metrics that are parametrically different from Kerr, as well as the charges of several known spacetimes. To consider alternatives to the presence of an event horizon, we explore the possibility that Sgr A* is a compact object with a surface that either absorbs and thermally reemits incident radiation or partially reflects it. Using the observed image size and the broadband spectrum of Sgr A*, we conclude that a thermal surface can be ruled out and a fully reflective one is unlikely. We compare our results to the broader landscape of gravitational tests. Together with the bounds found for stellar-mass black holes and the M87 black hole, our observations provide further support that the external spacetimes of all black holes are described by the Kerr metric, independent of their mass."
CHARLES WILLIAMS,Polarimetric properties of Event Horizon Telescope targets from ALMA,"We present the results from a full polarization study carried out with the Atacama Large Millimeter/submillimeter Array (ALMA) during the first Very Long Baseline Interferometry (VLBI) campaign, which was conducted in 2017 April in the λ3 mm and λ1.3 mm bands, in concert with the Global mm-VLBI Array (GMVA) and the Event Horizon Telescope (EHT), respectively. We determine the polarization and Faraday properties of all VLBI targets, including Sgr A*, M87, and a dozen radio-loud active galactic nuclei (AGNs), in the two bands at several epochs in a time window of 10 days. We detect high linear polarization fractions (2%–15%) and large rotation measures (RM &gt; 103.3–105.5 rad m−2), confirming the trends of previous AGN studies at millimeter wavelengths. We find that blazars are more strongly polarized than other AGNs in the sample, while exhibiting (on average) order-of-magnitude lower RM values, consistent with the AGN viewing angle unification scheme. For Sgr A* we report a mean RM of (−4.2 ± 0.3) × 105 rad m−2 at 1.3 mm, consistent with measurements over the past decade and, for the first time, an RM of (–2.1 ± 0.1) × 105 rad m−2 at 3 mm, suggesting that about half of the Faraday rotation at 1.3 mm may occur between the 3 mm photosphere and the 1.3 mm source. We also report the first unambiguous measurement of RM toward the M87 nucleus at millimeter wavelengths, which undergoes significant changes in magnitude and sign reversals on a one year timescale, spanning the range from −1.2 to 0.3 × 105 rad m−2 at 3 mm and −4.1 to 1.5 × 105 rad m−2 at 1.3 mm. Given this time variability, we argue that, unlike the case of Sgr A*, the RM in M87 does not provide an accurate estimate of the mass accretion rate onto the black hole. We put forward a two-component model, comprised of a variable compact region and a static extended region, that can simultaneously explain the polarimetric properties observed by both the EHT (on horizon scales) and ALMA (which observes the combined emission from both components). These measurements provide critical constraints for the calibration, analysis, and interpretation of simultaneously obtained VLBI data with the EHT and GMVA."
CHARLES WILLIAMS,"First Sagittarius A* Event Horizon Telescope results. IV. Variability, morphology, and black hole mass","In this paper we quantify the temporal variability and image morphology of the horizon-scale emission from Sgr A*, as observed by the EHT in 2017 April at a wavelength of 1.3 mm. We find that the Sgr A* data exhibit variability that exceeds what can be explained by the uncertainties in the data or by the effects of interstellar scattering. The magnitude of this variability can be a substantial fraction of the correlated flux density, reaching ∼100% on some baselines. Through an exploration of simple geometric source models, we demonstrate that ring-like morphologies provide better fits to the Sgr A* data than do other morphologies with comparable complexity. We develop two strategies for fitting static geometric ring models to the time-variable Sgr A* data; one strategy fits models to short segments of data over which the source is static and averages these independent fits, while the other fits models to the full data set using a parametric model for the structural variability power spectrum around the average source structure. Both geometric modeling and image-domain feature extraction techniques determine the ring diameter to be 51.8 ± 2.3 μas (68% credible intervals), with the ring thickness constrained to have an FWHM between ∼30% and 50% of the ring diameter. To bring the diameter measurements to a common physical scale, we calibrate them using synthetic data generated from GRMHD simulations. This calibration constrains the angular size of the gravitational radius to be 4.8_-0.7^+1.4 μas, which we combine with an independent distance measurement from maser parallaxes to determine the mass of Sgr A* to be 4.0_-0.6^+10^6 M⊙."
CHARLES WILLIAMS,"First Sagittarius A* Event Horizon Telescope results. II. EHT and multiwavelength observations, data processing, and calibration","We present Event Horizon Telescope (EHT) 1.3 mm measurements of the radio source located at the position of the supermassive black hole Sagittarius A* (Sgr A*), collected during the 2017 April 5–11 campaign. The observations were carried out with eight facilities at six locations across the globe. Novel calibration methods are employed to account for Sgr A*'s flux variability. The majority of the 1.3 mm emission arises from horizon scales, where intrinsic structural source variability is detected on timescales of minutes to hours. The effects of interstellar scattering on the image and its variability are found to be subdominant to intrinsic source structure. The calibrated visibility amplitudes, particularly the locations of the visibility minima, are broadly consistent with a blurred ring with a diameter of ∼50 μas, as determined in later works in this series. Contemporaneous multiwavelength monitoring of Sgr A* was performed at 22, 43, and 86 GHz and at near-infrared and X-ray wavelengths. Several X-ray flares from Sgr A* are detected by Chandra, one at low significance jointly with Swift on 2017 April 7 and the other at higher significance jointly with NuSTAR on 2017 April 11. The brighter April 11 flare is not observed simultaneously by the EHT but is followed by a significant increase in millimeter flux variability immediately after the X-ray outburst, indicating a likely connection in the emission physics near the event horizon. We compare Sgr A*’s broadband flux during the EHT campaign to its historical spectral energy distribution and find that both the quiescent emission and flare emission are consistent with its long-term behavior."
CHARLES WILLIAMS,"Bostonia, first series: v. 12, no. 1-4",
CHARLES WILLIAMS,Broadband multi-wavelength properties of M87 during the 2017 Event Horizon Telescope campaign,"In 2017, the Event Horizon Telescope (EHT) Collaboration succeeded in capturing the first direct image of the center of the M87 galaxy. The asymmetric ring morphology and size are consistent with theoretical expectations for a weakly accreting supermassive black hole of mass ∼6.5 × 109 M ⊙. The EHTC also partnered with several international facilities in space and on the ground, to arrange an extensive, quasi-simultaneous multi-wavelength campaign. This Letter presents the results and analysis of this campaign, as well as the multi-wavelength data as a legacy data repository. We captured M87 in a historically low state, and the core flux dominates over HST-1 at high energies, making it possible to combine core flux constraints with the more spatially precise very long baseline interferometry data. We present the most complete simultaneous multi-wavelength spectrum of the active nucleus to date, and discuss the complexity and caveats of combining data from different spatial scales into one broadband spectrum. We apply two heuristic, isotropic leptonic single-zone models to provide insight into the basic source properties, but conclude that a structured jet is necessary to explain M87’s spectrum. We can exclude that the simultaneous γ-ray emission is produced via inverse Compton emission in the same region producing the EHT mm-band emission, and further conclude that the γ-rays can only be produced in the inner jets (inward of HST-1) if there are strongly particle-dominated regions. Direct synchrotron emission from accelerated protons and secondaries cannot yet be excluded."
CHARLES WILLIAMS,"The medical student: v. 13, no. 1, 3-8.",
CHARLES WILLIAMS,Event Horizon Telescope imaging of the archetypal blazar 3C 279 at an extreme 20 microarcsecond resolution,"3C 279 is an archetypal blazar with a prominent radio jet that show broadband flux density variability across the entire electromagnetic spectrum. We use an ultra-high angular resolution technique – global Very Long Baseline Interferometry (VLBI) at 1.3 mm (230 GHz) – to resolve the innermost jet of 3C 279 in order to study its fine-scale morphology close to the jet base where highly variable γ-ray emission is thought to originate, according to various models. The source was observed during four days in April 2017 with the Event Horizon Telescope at 230 GHz, including the phased Atacama Large Millimeter/submillimeter Array (ALMA), at an angular resolution of ∼20 μas (at a redshift of z = 0.536 this corresponds to ∼0.13 pc  ∼ 1700 Schwarzschild radii with a black hole mass M<jats:sub>BH</jats:sub> = 8 × 10^8 M_⊙). Imaging and model-fitting techniques were applied to the data to parameterize the fine-scale source structure and its variation. We find a multicomponent inner jet morphology with the northernmost component elongated perpendicular to the direction of the jet, as imaged at longer wavelengths. The elongated nuclear structure is consistent on all four observing days and across different imaging methods and model-fitting techniques, and therefore appears robust. Owing to its compactness and brightness, we associate the northern nuclear structure as the VLBI “core”. This morphology can be interpreted as either a broad resolved jet base or a spatially bent jet. We also find significant day-to-day variations in the closure phases, which appear most pronounced on the triangles with the longest baselines. Our analysis shows that this variation is related to a systematic change of the source structure. Two inner jet components move non-radially at apparent speeds of ∼15 c and ∼20 c (∼1.3 and ∼1.7 μas day^−1, respectively), which more strongly supports the scenario of traveling shocks or instabilities in a bent, possibly rotating jet. The observed apparent speeds are also coincident with the 3C 279 large-scale jet kinematics observed at longer (cm) wavelengths, suggesting no significant jet acceleration between the 1.3 mm core and the outer jet. The intrinsic brightness temperature of the jet components are ≲10^10 K, a magnitude or more lower than typical values seen at ≥7 mm wavelengths. The low brightness temperature and morphological complexity suggest that the core region of 3C 279 becomes optically thin at short (mm) wavelengths."
CHARLES WILLIAMS,First M87 Event Horizon Telescope results. II. Array and instrumentation,"The Event Horizon Telescope (EHT) is a very long baseline interferometry (VLBI) array that comprises millimeter- and submillimeter-wavelength telescopes separated by distances comparable to the diameter of the Earth. At a nominal operating wavelength of ~1.3 mm, EHT angular resolution (λ/D) is ~25 μas, which is sufficient to resolve nearby supermassive black hole candidates on spatial and temporal scales that correspond to their event horizons. With this capability, the EHT scientific goals are to probe general relativistic effects in the strong-field regime and to study accretion and relativistic jet formation near the black hole boundary. In this Letter we describe the system design of the EHT, detail the technology and instrumentation that enable observations, and provide measures of its performance. Meeting the EHT science objectives has required several key developments that have facilitated the robust extension of the VLBI technique to EHT observing wavelengths and the production of instrumentation that can be deployed on a heterogeneous array of existing telescopes and facilities. To meet sensitivity requirements, high-bandwidth digital systems were developed that process data at rates of 64 gigabit s−1, exceeding those of currently operating cm-wavelength VLBI arrays by more than an order of magnitude. Associated improvements include the development of phasing systems at array facilities, new receiver installation at several sites, and the deployment of hydrogen maser frequency standards to ensure coherent data capture across the array. These efforts led to the coordination and execution of the first Global EHT observations in 2017 April, and to event-horizon-scale imaging of the supermassive black hole candidate in M87."
CHARLES WILLIAMS,First M87 Event Horizon Telescope results. I. The shadow of the supermassive black hole,"When surrounded by a transparent emission region, black holes are expected to reveal a dark shadow caused by gravitational light bending and photon capture at the event horizon. To image and study this phenomenon, we have assembled the Event Horizon Telescope, a global very long baseline interferometry array observing at a wavelength of 1.3 mm. This allows us to reconstruct event-horizon-scale images of the supermassive black hole candidate in the center of the giant elliptical galaxy M87. We have resolved the central compact radio source as an asymmetric bright emission ring with a diameter of 42 ± 3 μas, which is circular and encompasses a central depression in brightness with a flux ratio gsim10:1. The emission ring is recovered using different calibration and imaging schemes, with its diameter and width remaining stable over four different observations carried out in different days. Overall, the observed image is consistent with expectations for the shadow of a Kerr black hole as predicted by general relativity. The asymmetry in brightness in the ring can be explained in terms of relativistic beaming of the emission from a plasma rotating close to the speed of light around a black hole. We compare our images to an extensive library of ray-traced general-relativistic magnetohydrodynamic simulations of black holes and derive a central mass of M = (6.5 ± 0.7) × 10^9 M ⊙. Our radio-wave observations thus provide powerful evidence for the presence of supermassive black holes in centers of galaxies and as the central engines of active galactic nuclei. They also present a new tool to explore gravity in its most extreme limit and on a mass scale that was so far not accessible."
CHARLES WILLIAMS,EEG analytics for early detection of autism spectrum disorder: a data-driven approach,"Autism spectrum disorder (ASD) is a complex and heterogeneous disorder, diagnosed on the basis of behavioral symptoms during the second year of life or later. Finding scalable biomarkers for early detection is challenging because of the variability in presentation of the disorder and the need for simple measurements that could be implemented routinely during well-baby checkups. EEG is a relatively easy-to-use, low cost brain measurement tool that is being increasingly explored as a potential clinical tool for monitoring atypical brain development. EEG measurements were collected from 99 infants with an older sibling diagnosed with ASD, and 89 low risk controls, beginning at 3 months of age and continuing until 36 months of age. Nonlinear features were computed from EEG signals and used as input to statistical learning methods. Prediction of the clinical diagnostic outcome of ASD or not ASD was highly accurate when using EEG measurements from as early as 3 months of age. Specificity, sensitivity and PPV were high, exceeding 95% at some ages. Prediction of ADOS calibrated severity scores for all infants in the study using only EEG data taken as early as 3 months of age was strongly correlated with the actual measured scores. This suggests that useful digital biomarkers might be extracted from EEG measurements."
CHARLES WILLIAMS,"BMQ : Boston medical quarterly: v. 1, no. 1-4",
CHARLES WILLIAMS,First Sagittarius A* Event Horizon Telescope results. III. Imaging of the Galactic center supermassive black hole,"We present the first event-horizon-scale images and spatiotemporal analysis of Sgr A* taken with the Event Horizon Telescope in 2017 April at a wavelength of 1.3 mm. Imaging of Sgr A* has been conducted through surveys over a wide range of imaging assumptions using the classical CLEAN algorithm, regularized maximum likelihood methods, and a Bayesian posterior sampling method. Different prescriptions have been used to account for scattering effects by the interstellar medium toward the Galactic center. Mitigation of the rapid intraday variability that characterizes Sgr A* has been carried out through the addition of a “variability noise budget” in the observed visibilities, facilitating the reconstruction of static full-track images. Our static reconstructions of Sgr A* can be clustered into four representative morphologies that correspond to ring images with three different azimuthal brightness distributions and a small cluster that contains diverse nonring morphologies. Based on our extensive analysis of the effects of sparse (u, v)-coverage, source variability, and interstellar scattering, as well as studies of simulated visibility data, we conclude that the Event Horizon Telescope Sgr A* data show compelling evidence for an image that is dominated by a bright ring of emission with a ring diameter of ∼50 μas, consistent with the expected “shadow” of a 4 × 106 M⊙ black hole in the Galactic center located at a distance of 8 kpc."
CHARLES WILLIAMS,Characterizing and mitigating intraday variability: reconstructing source structure in accreting black holes with mm-VLBI,"The extraordinary physical resolution afforded by the Event Horizon Telescope has opened a window onto the astrophysical phenomena unfolding on horizon scales in two known black holes, M87* and Sgr A*. However, with this leap in resolution has come a new set of practical complications. Sgr A* exhibits intraday variability that violates the assumptions underlying Earth aperture synthesis, limiting traditional image reconstruction methods to short timescales and data sets with very sparse (u, v) coverage. We present a new set of tools to detect and mitigate this variability. We develop a data-driven, model-agnostic procedure to detect and characterize the spatial structure of intraday variability. This method is calibrated against a large set of mock data sets, producing an empirical estimator of the spatial power spectrum of the brightness fluctuations. We present a novel Bayesian noise modeling algorithm that simultaneously reconstructs an average image and statistical measure of the fluctuations about it using a parameterized form for the excess variance in the complex visibilities not otherwise explained by the statistical errors. These methods are validated using a variety of simulated data, including general relativistic magnetohydrodynamic simulations appropriate for Sgr A* and M87*. We find that the reconstructed source structure and variability are robust to changes in the underlying image model. We apply these methods to the 2017 EHT observations of M87*, finding evidence for variability across the EHT observing campaign. The variability mitigation strategies presented are widely applicable to very long baseline interferometry observations of variable sources generally, for which they provide a data-informed averaging procedure and natural characterization of inter-epoch image consistency."
CHARLES WILLIAMS,"BMQ : Boston medical quarterly: v. 5, no. 1-4",
CHARLES WILLIAMS,"Economic development, human development, and the pursuit of happiness, April 1, 2, and 3, 2004","The conference asks the questions, how can we make sure that the benefits of economic growth flow into health, education, welfare, and other aspects of human development; and what is the relationship between human development and economic development? Speakers and participants discuss the role that culture, legal and political institutions, the UN Developmental Goals, the level of decision-making, and ethics, play in development."
CHARLES WILLIAMS,"BMQ : Boston medical quarterly: v. 10, no. 1-4",
CHARLES WILLIAMS,First Sagittarius A* Event Horizon Telescope results. I. The shadow of the supermassive black hole in the center of the Milky Way,"We present the first Event Horizon Telescope (EHT) observations of Sagittarius A* (Sgr A*), the Galactic center source associated with a supermassive black hole. These observations were conducted in 2017 using a global interferometric array of eight telescopes operating at a wavelength of λ = 1.3 mm. The EHT data resolve a compact emission region with intrahour variability. A variety of imaging and modeling analyses all support an image that is dominated by a bright, thick ring with a diameter of 51.8 ± 2.3 μas (68% credible interval). The ring has modest azimuthal brightness asymmetry and a comparatively dim interior. Using a large suite of numerical simulations, we demonstrate that the EHT images of Sgr A* are consistent with the expected appearance of a Kerr black hole with mass ∼4 × 106 M ⊙, which is inferred to exist at this location based on previous infrared observations of individual stellar orbits, as well as maser proper-motion studies. Our model comparisons disfavor scenarios where the black hole is viewed at high inclination (i &gt; 50°), as well as nonspinning black holes and those with retrograde accretion disks. Our results provide direct evidence for the presence of a supermassive black hole at the center of the Milky Way, and for the first time we connect the predictions from dynamical measurements of stellar orbits on scales of 103–105 gravitational radii to event-horizon-scale images and variability. Furthermore, a comparison with the EHT results for the supermassive black hole M87* shows consistency with the predictions of general relativity spanning over three orders of magnitude in central mass."
CHARLES WILLIAMS,The development of the papacy,
CHARLES WILLIAMS,"The medical student: v. 5, no. 1-8",
CHARLES WILLIAMS,The co-ordination of the church school and public schools,
CHARLES WILLIAMS,The long-baseline neutrino experiment: exploring fundamental symmetries of the universe,"The preponderance of matter over antimatter in the early Universe, the dynamics of the supernova bursts that produced the heavy elements necessary for life and whether protons eventually decay --- these mysteries at the forefront of particle physics and astrophysics are key to understanding the early evolution of our Universe, its current state and its eventual fate. The Long-Baseline Neutrino Experiment (LBNE) represents an extensively developed plan for a world-class experiment dedicated to addressing these questions. LBNE is conceived around three central components: (1) a new, high-intensity neutrino source generated from a megawatt-class proton accelerator at Fermi National Accelerator Laboratory, (2) a near neutrino detector just downstream of the source, and (3) a massive liquid argon time-projection chamber deployed as a far detector deep underground at the Sanford Underground Research Facility. This facility, located at the site of the former Homestake Mine in Lead, South Dakota, is approximately 1,300 km from the neutrino source at Fermilab -- a distance (baseline) that delivers optimal sensitivity to neutrino charge-parity symmetry violation and mass ordering effects. This ambitious yet cost-effective design incorporates scalability and flexibility and can accommodate a variety of upgrades and contributions. With its exceptional combination of experimental configuration, technical capabilities, and potential for transformative discoveries, LBNE promises to be a vital facility for the field of particle physics worldwide, providing physicists from around the globe with opportunities to collaborate in a twenty to thirty year program of exciting science. In this document we provide a comprehensive overview of LBNE's scientific objectives, its place in the landscape of neutrino physics worldwide, the technologies it will incorporate and the capabilities it will possess."
SUSAN PHILLIPS,Shared and distinct transcriptomic cell types across neocortical areas,"The neocortex contains a multitude of cell types that are segregated into layers and functionally distinct areas. To investigate the diversity of cell types across the mouse neocortex, here we analysed 23,822 cells from two areas at distant poles of the mouse neocortex: the primary visual cortex and the anterior lateral motor cortex. We define 133 transcriptomic cell types by deep, single-cell RNA sequencing. Nearly all types of GABA (γ-aminobutyric acid)-containing neurons are shared across both areas, whereas most types of glutamatergic neurons were found in one of the two areas. By combining single-cell RNA sequencing and retrograde labelling, we match transcriptomic types of glutamatergic neurons to their long-range projection specificity. Our study establishes a combined transcriptomic and projectional taxonomy of cortical cell types from functionally distinct areas of the adult mouse cortex."
MAHESH KARRA,The effect of a postpartum IUD intervention on counseling and choice: Evidence from a cluster-randomized stepped-wedge trial in Sri Lanka,"BACKGROUND: The International Federation of Gynaecology and Obstetrics (FIGO), in collaboration with the Sri Lankan College of Obstetrics and Gynaecologists (SLCOG), launched an initiative in 2014 to institutionalize immediate postpartum IUD (PPIUD) services as a routine part of antenatal counseling and delivery room services in Sri Lanka. In this study, we evaluate the effect of the FIGO-SLCOG PPIUD intervention in six hospitals by means of a cluster-randomized stepped-wedge trial. METHODS/DESIGN: Six hospitals were randomized into two groups of three using matched pairs. Following a 3-month baseline period, the intervention was administered to the first group, while the second group received the intervention after 9 months of baseline data collection. We collected data from 39,084 women who delivered in these hospitals between September 2015 and January 2017. We conduct an intent-to-treat (ITT) analysis to determine the impact of the intervention on PPIUD counseling and choice of PPIUD, as measured by consent to receive a PPIUD, as well as PPIUD uptake (insertion following delivery). We also investigate how factors related to counseling, such as counseling timing and quality, are linked to choice of PPIUD. RESULTS: We find that the intervention increased rates of counseling, from an average counseling rate of 12% in all hospitals prior to the intervention to an average rate of 51% in all hospitals after the rollout of the intervention (0.307; 95% CI 0.148-0.465). In contrast, we find the impact of the intervention on choice of PPIUD to be less robust and mixed, with 4.1% of women choosing PPIUD prior to the intervention compared to 9.8% of women choosing PPIUD after the rollout of the intervention (0.027; 95% CI 0.000-0.054). CONCLUSIONS: This study demonstrates that incorporating PPIUD services into postpartum care is feasible and potentially effective. Taking the evidence on both counseling and choice of PPIUD together, we find that the intervention had a generally positive impact on receipt of PPIUD counseling and, to a lesser degree, on choice of the PPIUD. Nevertheless, it is clear that the intervention's effectiveness can be improved to be able to meet the demand for postpartum family planning of women. TRIAL REGISTRATION: ClinicalTrials.gov, NCT02718222 . Registered on 11 March 2016 (retrospectively registered)."
MAHESH KARRA,"Institutionalizing postpartum intrauterine device (IUD) services in Sri Lanka, Tanzania, and Nepal: study protocol for a cluster-randomized stepped-wedge trial.","BACKGROUND: During the year following the birth of a child, 40% of women are estimated to have an unmet need for contraception. The copper IUD provides safe, effective, convenient, and long-term contraceptive protection that does not interfere with breastfeeding during the postpartum period. Postpartum IUD (PPIUD) insertion should be performed by a trained provider in the early postpartum period to reduce expulsion rates and complications, but these services are not widely available. The International Federation of Obstetricians and Gynecologists (FIGO) will implement an intervention that aims to institutionalize PPIUD training as a regular part of the OB/GYN training program and to integrate it as part of the standard practice at the time of delivery in intervention hospitals. METHODS: This trial uses a cluster-randomized stepped wedge design to assess the causal effect of the FIGO intervention on the uptake and continued use of PPIUD and of the effect on subsequent pregnancy and birth. This trial also seeks to measure institutionalization of PPIUD services in study hospitals and diffusion of these services to other providers and health facilities. This study will also include a nested mixed-methods performance evaluation to describe intervention implementation. DISCUSSION: This study will provide critical evidence on the causal effects of hospital-based PPIUD provision on contraceptive choices and reproductive health outcomes, as well as on the feasibility, acceptability and longer run institutional impacts in three low- and middle-income countries. TRIAL REGISTRATION: Trial registered on March 11, 2016 with ClinicalTrials.gov, NCT02718222 ."
MAHESH KARRA,Community-based financing of family planning in developing countries: A systematic review,"In this systematic review, we gather evidence on community financing schemes and insurance programs for family planning in developing countries, and we assess the impact of these programs on primary outcomes related to contraceptive use. To identify and evaluate the research findings, we adopt a four‐stage review process that employs a weight‐of‐evidence and risk‐of‐bias analytic approach. Out of 19,138 references that were identified, only four studies were included in our final analysis, and only one study was determined to be of high quality. In the four studies, the evidence on the impact of community‐based financing on family planning and fertility outcomes is inconclusive. These limited and mixed findings suggest that either: 1) more high‐quality evidence on community‐based financing for family planning is needed before any conclusions can be made; or 2) community‐based financing for family planning may, in fact, have little or no effect on family planning outcomes."
MAHESH KARRA,Long run height and education implications of early life growth faltering: a synthetic panel analysis of 425 birth cohorts in 21 low- and middle-income countries,"BACKGROUND: We estimated the associations between exposure to early life growth faltering at the population level and adult height and education outcomes in a sample of 21 low- and middle-income countries. METHODS: We conducted a synthetic panel analysis of 425 birth cohorts across 126 regions in 21 LMICs surveyed in the Demographic and Health Surveys (DHS) both as children and as adults. Data from historic (1987-1993) DHS survey rounds were used to compute average height-for-age z-scores at the province-birth-year level. Cohort measures of early life growth were then linked to adult height and educational attainment measures collected on individuals from the same cohorts in the 2006-2014 DHS survey rounds. The primary exposure of interest was population-level early life growth (region-birth year average HAZ) and growth faltering (region-birth year stunting prevalence). Multivariable linear regression models were used to estimate the associations between adult outcomes and population-level measures of early life linear growth. RESULTS: The average cohort height-for-age z-score (HAZ) in childhood was - 1.53 [range: - 2.73, - 0.348]. In fully adjusted models, each unit increase in cohort childhood HAZ was associated with a 2.0 cm [95% CI: 1.09-2.9] increase in adult height, with larger associations for men than for women. Evidence for the association between early childhood height and adult educational attainment was found to be inconclusive (0.269, 95% CI: [- 0.68-1.22]). CONCLUSIONS: While early childhood linear growth at the cohort level appears to be highly predictive of adult height, the empirical association between early life growth and adult educational attainment seems weak and heterogeneous across countries. REGISTRATION: This study was registered on May 10, 2017 at the ISRCTN Registry ( http://www.isrctn.com ), registration number ISRCTN82438662 ."
MAHESH KARRA,Ethnolinguistic concordance and the receipt of postpartum IUD counseling services in Sri Lanka.,"CONTEXT: Ethnic and linguistic concordance are important dimensions of the patient-physician relationship, and are linked to health care disparities. However, evidence on the associations between health behavior and outcomes and patient-provider concordance is limited, especially in low- and middle-income settings. METHODS: To examine how concordance between women and their primary health midwife is associated with women's receipt of postpartum IUD counseling, observational data from a cluster-randomized trial assessing an intervention to increase postpartum IUD counseling were used. Data on 4,497 women who delivered at six hospitals in Sri Lanka between September 2015 and March 2017 were merged with data on 245 primary health midwives, and indicators of linguistic concordance, ethnic concordance and their interaction were generated. Multivariate logistic regression analyses were used to assess the associations between concordance and women's receipt of counseling. RESULTS: Women from non-Sinhalese groups in Sri Lanka face disparities in the receipt of postpartum IUD counseling. Compared with the ethnolinguistic majority (Sinhalese women who speak only Sinhala), non-Sinhalese women have lower odds of having received postpartum IUD counseling, whether they speak both Sinhala and Tamil (odds ratio, 0.6) or only Tamil (0.5). Ethnic discordance- rather than linguistic discordance-is the primary driver of this disparity. CONCLUSIONS: The findings highlight the need for interventions that aim to bridge the sociocultural gaps between providers and patients. Matching women and their providers on ethnolinguistic background may help to reduce disparities in care."
MAHESH KARRA,Birth spacing and child health trajectories,"Using longitudinal data on a cohort of over 4,000 children from four low‐ and middle‐income countries, we document the association between birth spacing and child growth trajectories. We find declines in child height at age 1 among children who are born within three years of an older sibling. However, we also observe catch‐up growth for closely spaced children as they age. We find no evidence that catch‐up growth is driven by remedial health investments after birth, suggesting substitutability in underlying biological processes. We also find that very widely spaced children (preceding birth interval of more than seven years) are similar in height at age 1 as children who are spaced three to seven years apart, but outgrow their more closely spaced counterparts as they age. However, further sibling comparisons suggest that the growth premium that is observed for very widely spaced children may be driven by unobserved confounding factors."
MAHESH KARRA,The causal effect of a family planning intervention on women's contraceptive use and birth spacing,"Studies have suggested that improving access to family planning (FP) may improve contraceptive use and reduce fertility. However, high-quality evidence, particularly from randomized implementation trials, of the effect of FP programs and interventions on longer-term fertility and birth spacing is lacking. We conduct a nonblinded, randomized, controlled trial to assess the causal impact of improved access to FP on contraceptive use and pregnancy spacing in Lilongwe, Malawi. A total of 2,143 married women aged 18 to 35 who were either pregnant or had recently given birth were recruited through home visits between September 2016 and January 2017 and were randomly assigned to an intervention arm or a control arm. The intervention arm received four services over a 2-y period: 1) up to six FP counseling sessions; 2) free transportation to an FP clinic; 3) free FP services at the clinic or financial reimbursement for FP services obtained elsewhere; and 4) treatment for contraceptive-related side effects. Contraceptive use after 2 y of intervention exposure increased by 5.9 percentage points, mainly through an increased use of contraceptive implants. The intervention group’s hazard of pregnancy was 43.5% lower 24 mo after the index birth. Our results highlight the positive impact of increased access to FP on a woman’s contraceptive use. In addition, we show that exposure to the FP intervention led to a prolongation of birth intervals among intervention women relative to control women and increased her control over birth spacing and postpartum fertility, which, in turn, may contribute to her longer-term health and well-being."
MAHESH KARRA,Location and content of counselling and acceptance of postpartum IUD in Sri Lanka.,"BACKGROUND: The immediate postpartum IUD (PPIUD) is a long-acting, reversible method of contraception that can be used safely and effectively following a birth. To appropriately facilitate the immediate postpartum insertion of IUDs, women must be informed of the method's availability and must be counselled on its benefits and risks prior to entering the delivery room. We examine the relationship between the location and quality of antenatal counselling and women's acceptance of immediate postpartum IUD (PPIUD) in four hospitals in Sri Lanka. METHODS: Data were collected between January 2015 and May 2015. Modified Poisson regressions with robust standard errors are used to assess the relationships between place of counselling, indicators of counselling quality, and PPIUD uptake following delivery. RESULTS: We find that women who were counselled in hospital antenatal clinics and admission wards were much more likely to have a PPIUD inserted than women who were counselled in field clinics or during home visits. Hospital-based counselling had higher quality indicators for providing information on PPIUD, and women were more likely to receive PPIUD information leaflets in hospital locations than in lower-tiered clinics or during home visits. Women who were counselled at hospital locations also reported a higher level of satisfaction with the counselling that they received. Receipt of hospital-based counselling was also linked to higher PPIUD uptake, in spite of the fact that women were more likely to be given information about the risks and alternatives to PPIUD in hospitals. The information about the risks of and alternatives to PPIUD, whether provided in hospital or in non-hospital settings, tended to lower the likelihood of acceptance to have a PPIUD insertion. Counselling in hospital admission wards was focused on women who had not been counselled at field clinics. CONCLUSIONS: The study findings call for efforts that improve the training of midwives who provide PPIUD counselling at field clinics and during the home visits. We also recommend that routine PPIUD counselling be conducted in hospitals, even if women have already been counselled elsewhere."
MAHESH KARRA,Adding measurement error to location data to protect subject confidentiality while allowing for consistent estimation of exposure effects,"In public use data sets, it is desirable not to report a respondent's location precisely to protect subject confidentiality. However, the direct use of perturbed location data to construct explanatory exposure variables for regression models will generally make naive estimates of all parameters biased and inconsistent. We propose an approach where a perturbation vector, consisting of a random distance at a random angle, is added to a respondent's reported geographic co‐ordinates. We show that, as long as the distribution of the perturbation is public and there is an underlying prior population density map, external researchers can construct unbiased and consistent estimates of location‐dependent exposure effects by using numerical integration techniques over all possible actual locations, although coefficient confidence intervals are wider than if the true location data were known. We examine our method by using a Monte Carlo simulation exercise and apply it to a real world example using data on perceived and actual distance to a health facility in Tanzania."
MAHESH KARRA,Effect of a postpartum family planning intervention on postpartum intrauterine device counseling and choice: evidence from a cluster-randomized trial in Tanzania,"BACKGROUND: The World Health Organization recommends postpartum family planning (PPFP) for healthy birth spacing. This study is an evaluation of an intervention that sought to improve women's access to PPFP in Tanzania. The intervention included counseling on PPFP during antenatal and delivery care and introducing postpartum intrauterine device (PPIUD) insertion as an integrated part of delivery services for women electing PPIUD in the immediate postpartum period. METHODS: This cluster-randomized controlled trial recruited 15,264 postpartum Tanzanian women aged 18 or older who delivered in one of five study hospitals between January and September 2016. We present the effectiveness of the intervention using a difference-in-differences approach to compare outcomes, receipt of PPIUD counseling and choice of PPIUD after delivery, between the pre- and post-intervention period in the treatment and control group. We also present an intervention adherence-adjusted analysis using an instrumental variables estimation. RESULTS: We estimate linear probability models to obtain effect sizes in percentage points (pp). The intervention increased PPIUD counseling by 19.8 pp (95% CI: 9.1 - 22.6 pp) and choice of PPIUD by 6.3 pp (95% CI: 2.3 - 8.0 pp). The adherence-adjusted estimates demonstrate that if all women had been counseled, we would have observed a 31.6 pp increase in choice of PPIUD (95% CI: 24.3 - 35.8 pp). Among women counseled, determinants of choosing PPIUD included receiving an informational leaflet during counseling and being counseled after admission for delivery services. CONCLUSIONS: The intervention modestly increased the rate of PPIUD counseling and choice of PPIUD, primarily due to low coverage of PPIUD counseling among women delivering in study facilities. With universal PPIUD counseling, large increases in choice of PPIUD would have been observed. Giving women informational materials on PPIUD and counseling after admission for delivery are likely to increase the proportion of women choosing PPIUD. TRIAL REGISTRATION: Registered with clinicaltrials.gov (NCT02718222) on March 24, 2016, retrospectively registered."
MAHESH KARRA,Assessing the role of women's autonomy and acceptability of intimate-partner violence in maternal health-care utilization in 63 low- and middle-income countries,"BACKGROUND: Our study investigates the associations between women's autonomy and attitudes toward the acceptability of intimate-partner violence against women (IPVAW) and maternal health-care utilization outcomes. METHODS: We combine data from 113 Demographic and Health Surveys conducted between 2003 and 2016, which give us a pooled sample of 765 169 mothers and 777 352 births from 63 countries. We generate composite scores of women's autonomy (six-point scale with reference: no contribution) and acceptability of IPVAW (five-point scale with reference: no acceptance) and assess the associations between these measures and women's use of antenatal care services and facility delivery in pooled and unique country samples. RESULTS: A change in a woman's autonomy score from 'no contribution to any decision-making domain' (a composite autonomy score of 0) to 'contribution to all decision-making domains' (a score of 6) is associated with a 31.2% increase in her odds of delivering in a facility and a 42.4% increase in her odds of receiving at least eight antenatal care visits over the course of her pregnancy. In contrast, a change in a woman's attitude towards acceptability of IPVAW from 'IPVAW is not acceptable under any scenario' (a score of 0) to 'IPVAW is acceptable in all scenarios' (a score of 5) is associated with an 8.9% decrease in her odds of delivering in a facility and a 20.3% decrease in her odds of receiving eight antenatal care visits. CONCLUSIONS: Our findings suggest that strong and significant associations exist between autonomy, acceptability of IPVAW and utilization of maternal health-care services."
MAHESH KARRA,Early-life exposure to ambient fine particulate air pollution and infant mortality: pooled evidence from 43 low- and middle-income countries,"BACKGROUND: Many low- and middle-income countries are experiencing high and increasing exposure to ambient fine particulate air pollution (PM2.5). The effect of PM2.5 on infant and child mortality is usually modelled using concentration response curves extrapolated from studies conducted in settings with low ambient air pollution, which may not capture its full effect. METHODS: We pool data on more than half a million births from 69 nationally representative Demographic and Health Surveys that were conducted in 43 low- and middle-income countries between 1998 and 2014, and we calculate early-life exposure (exposure in utero and post partum) to ambient PM2.5 using high-resolution calibrated satellite data matched to the child's place of residence. We estimate the association between the log of early-life PM2.5 exposure, both overall and separated by type, and the odds of neonatal and infant mortality, adjusting for child-level, parent-level and household-level characteristics. RESULTS: We find little evidence that early-life exposure to overall PM2.5 is associated with higher odds of mortality relative to low exposure to PM2.5. However, about half of PM2.5 is naturally occurring dust and sea-salt whereas half is from other sources, comprising mainly carbon-based compounds, which are mostly due to human activity. We find a very strong association between exposure to carbonaceous PM2.5 and infant mortality, particularly neonatal mortality, i.e. mortality in the first 28 days after birth. We estimate that, at the mean level of exposure in the sample to carbonaceous PM2.5-10.9 µg/m3-the odds of neonatal mortality are over 50% higher than in the absence of pollution. CONCLUSION: Our results suggest that the current World Health Organization guideline of limiting the overall ambient PM2.5 level to less than 10 µg/m³ should be augmented with a lower limit for harmful carbonaceous PM2.5."
MAHESH KARRA,Reframing the measurement of women’s work in the sub-Saharan African context,"This research note considers how we measure women’s work in the sub-Saharan African (SSA) context. Drawing on qualitative work conducted in Burundi, the note examines how existing measures of women’s work do not accurately capture the intensity and type of work women in SSA undertake. Transcripts from qualitative interviews suggest that women think of work to meet their roles and responsibilities within the household. The women in the interviews do not frame work as a career or a primary activity in a time-use allocation. As a result, researchers need to nest questions regarding women’s work within surveys that ask about roles and responsibilities within the household, and about how women meet these responsibilities with a financial component."
MAHESH KARRA,The effect of improved access to family planning on postpartum women: protocol for a randomized controlled trial,"BACKGROUND: The World Health Organization recommends that a woman waits at least 24 months after a live birth before getting pregnant again; however, an estimated 25% of birth intervals in low-income countries do not meet this recommendation for adequate birth spacing, and the unmet need for postpartum family planning (PPFP) services is high. Few randomized controlled trials have assessed the causal impact of access to PPFP services, and even fewer evaluations have investigated how such interventions may affect postpartum contraceptive use, birth spacing, and measures of health and well-being. OBJECTIVE: This protocol paper aims to describe a randomized controlled trial that is being conducted to identify the causal impact of an intervention to improve access to PPFP services on contraceptive use, pregnancy, and birth spacing in urban Malawi. The causal effect of the intervention will be determined by comparing outcomes for respondents who are randomly assigned to an intervention arm against outcomes for respondents who are randomly assigned to a control arm. METHODS: Married women aged 18-35 years who were either pregnant or had recently given birth were randomly assigned to either the intervention arm or control arm. Women assigned to the intervention arm received a package of services over a 2-year intervention period. Services included a brochure and up to 6 home visits from trained family planning counselors; free transportation to a high-quality family planning clinic; and financial reimbursement for family planning services, consultations, and referrals for services. Two follow-up surveys were conducted 1 and 2 years after the baseline survey. RESULTS: A total of 2143 women were randomly assigned to either the intervention arm (n=1026) or the control arm (n=1117). Data collection for the first follow-up survey began in August 2017 and was completed in February 2018. A total of 1773 women, or 82.73% of women who were eligible for follow-up, were successfully contacted and reinterviewed at the first follow-up. Data collection for the second follow-up survey began in August 2018 and was completed in February 2019. A total of 1669 women, or 77.88% of women who were eligible for follow-up, were successfully contacted and reinterviewed at the second follow-up. The analysis of the primary outcomes is ongoing and is expected to be completed in 2021. CONCLUSIONS: The results of this trial seek to fill the current knowledge gaps in the effectiveness of family planning interventions on improving fertility and health outcomes. The findings also show that the benefits of improving access to family planning are likely to extend beyond the fertility and health domain by improving other measures of women's well-being. TRIAL REGISTRATION: American Economics Association Registry Trial Number AEARCTR-0000697; https://www.socialscienceregistry.org/trials/697 Registry for International Development Impact Evaluations (RIDIE) Trial Number RIDIE-STUDY-ID-556784ed86956; https://ridie.3ieimpact.org/index.php?r=search/detailView&id=320. INTERNATIONAL REGISTERED REPORT IDENTIFIER (IRRID): DERR1-10.2196/16697."
MAHESH KARRA,Measurement of unmet need for contraception: a counterfactual approach,"Unmet need plays a critical role in reproductive health research, evaluation, and advocacy. Although conceptually straightforward, its estimation suffers from a number of methodological limitations, most notably its reliance on biased measures of women's stated fertility preferences. We propose a counterfactual-based approach to measuring unmet need at the population level. Using data from 56 countries, we calculate unmet need in a population as the difference between: (1) the observed contraceptive prevalence in the population; and (2) the calculated contraceptive prevalence in a subsample of women who are identified to be from ""ideal"" family planning environments. Women from ""ideal"" environments are selected on characteristics that signal their contraceptive autonomy and decision-making over family planning. We find significant differences between our approach and existing methods to calculating unmet need, and we observe variation across countries when comparing indicators. We argue that our indicator of unmet need is preferable to existing population-level indicators due to its independence from biases that are generated from the use of reported preference measures, the simplicity with which it can be derived, and its relevance for cross-country comparisons as well as context-specific analyses."
MAHESH KARRA,Unwanted family planning: prevalence estimates for 56 countries,"While there is a large literature on the prevalence of unmet need for family planning, there is no matching quantitative evidence on the prevalence of unwanted family planning; all contraceptive use is assumed to represent a ""met need."" This lack of evidence raises concerns that some observed contraceptive use may be undesired and coercive. We provide estimates of unwanted family planning using Demographic and Health Survey data collected from 1,546,987 women in 56 low- and middle-income countries between 2011 and 2019. We estimate the prevalence of unwanted family planning, defined as the proportion of women who report wanting a child in the next nine months but who are using contraception. We find that 12.2 percent of women have an unmet need for family planning, while 2.1 percent have unwanted family planning, with estimated prevalence rates ranging from 0.4 percent in Gambia to 7.1 percent in Jordan. About half of unwanted family planning use can be attributed to condoms, withdrawal, and abstinence. Estimating the prevalence of unwanted family planning is difficult given current data collection efforts, which are not designed for this purpose. We recommend that future surveys probe the reasons for the use of family planning."
MAHESH KARRA,User-centered counseling and male involvement in contraceptive decision making: protocol for a randomized controlled trial,"BACKGROUND: To achieve informed choice within the framework of reproductive autonomy, family planning programs have begun to adopt user-centered approaches to service provision, which highlight the individual client as the focal point of interaction and key decision maker. However, little is known about how user-centered approaches to family planning, particularly family planning counseling, shape contraceptive preferences and choices. OBJECTIVE: We conducted a multiarmed randomized controlled trial to identify the causal impact of user-centered approaches to family planning counseling on women's contraceptive decision making in urban Malawi. This study aims to determine how a tailored, preference-driven approach to family planning counseling and the involvement of male partners during the counseling process may contribute to shaping women's contraceptive preferences and choices. METHODS: Married women aged 18-35 years were recruited and randomly assigned to 1 of the 3 intervention arms or a control arm characterized by the following two interventions: an intervention arm in which women were encouraged to invite their husbands to family planning counseling (husband invitation arm) and an intervention arm in which women received targeted, tailored counseling on up to five contraceptive methods (as opposed to up to 13 contraceptive methods) that reflected women's stated preferences for contraceptive methods. Women were randomized into a control arm, T0 (no husband invitation, standard counseling); T1 (husband invitation, standard counseling); T2 (no husband invitation, targeted counseling); and T3 (husband invitation, targeted counseling). Following counseling, all women received a package of family planning services, which included free transportation to a local family planning clinic and financial reimbursement for family planning services. Follow-up surveys were conducted with women 1 month after counseling. RESULTS: A total of 785 women completed the baseline survey, and 782 eligible respondents were randomized to 1 of the 3 intervention groups or the control group (T1, n=223; T2, n=225; T3, n=228; T0, n=108). Furthermore, 98.1% (767/782) of women were contacted for follow-up. Among the 767 women who were contacted, 95.3% (731/767) completed the follow-up survey. The analysis of the primary outcomes is ongoing and is expected to be completed by the end of 2021. CONCLUSIONS: The results from this trial will fill knowledge gaps on the effectiveness of tailored family planning counseling and male involvement in family planning on women's stated and realized contraceptive preferences. More generally, the study will provide evidence on how user-centered counseling may affect women's willingness to use and continue contraception to realize their contraceptive preferences. TRIAL REGISTRATION: American Economics Association's Registry for Randomized Controlled Trials AEARCTR-0004194; https://www.socialscienceregistry.org/trials/4194/history/46808. Registry for International Development Impact Evaluations RIDIE-STUDY-ID-5ce4f42bbc2bf; https://ridie.3ieimpact.org/index.php?r=search/detailView&id=823. INTERNATIONAL REGISTERED REPORT IDENTIFIER (IRRID): DERR1-10.2196/24884."
MAHESH KARRA,"The association between age, COVID-19 symptoms, and social distancing behavior in the United States","BACKGROUND: Public health authorities recommend that people practice social distancing, especially if they have symptoms of coronavirus disease (COVID-19), or are older and more at risk of serious illness if they become infected. We test the hypothesis that these groups are following these recommendations and are more likely to undertake social distancing. METHODS: We conducted an open online survey of 4,676 U.S. adults aged 18 and older between April 4 and April 7, 2020. We model the effects of age and common COVID-19 symptoms in the last two weeks on going out of the home for non-healthcare reasons the day before taking the survey, using a logistic model and the number of close contacts (within 6 feet) that respondents had with non-household members, using a Poisson count model. Our models control for several covariates, including other flu-like symptoms, sex, education, income, whether the respondent worked in February, household size, population density in the respondent's ZIP code, state fixed effects, and the day of completion of the survey. We also weight our analyses to make the sample representative of the U.S. adult population. FINDINGS: About 52 percent of the adult United States population went out of their home the previous day. On average, adults had close contact with 1.9 non-household members. We find that having at least one COVID-19 symptom (fever, dry cough, or shortness of breath) increased the likelihood of going out the previous day and having additional close contacts with non-household members; however, the estimates were not statistically significant. When disaggregating our analysis by COVID-19 symptoms, we find no strong evidence of greater social distancing by people with a fever or cough in the last two weeks, but we do find that those who experienced shortness of breath have fewer close contacts, with an incidence rate ratio (IRR) of 0.49 (95% CI: 0.30-0.78). Having other flu-like symptoms reduces the odds of going out by 0.32 (95% CI: 0.18-0.60) and the incidence rate of having close contacts by 42 percent (IRR = 0.58; 95% CI: 0.38-0.88). We find that older people are just as likely to leave their homes as younger people, but people over the age of 50 had less than half the predicted number of close contacts than those who were younger than 30. Our approach has the limitation that the survey sample is self-selected. Our findings may therefore be subject to selection bias that is not adequately controlled for by weighting. In addition, the possibility exists of confounding of the results due to omitted variable bias. CONCLUSIONS: We provide evidence that older people are having significantly fewer close contacts than younger people, which is in line with the public health authorities' recommendations. We also find that people experiencing shortness of breath are practicing more intense social distancing. However, we find that those with two other common COVID-19 symptoms, fever and dry cough, are not engaging in greater social distancing, suggesting that increased targeting on relevant symptoms, and messaging, may be required."
MAHESH KARRA,Exploring the relationship between anemia and postpartum depression: evidence from Malawi,"PURPOSE: Study findings suggest association between anemia and postpartum depression, but available evidence is scant and inconsistent. We investigate whether anemia is related to postpartum depression among women who have recently given birth in Malawi, where anemia prevalence is high. METHODS: We use cross-sectional data from 829 women who were 18-36 years old, married, lived in Lilongwe, Malawi, and gave birth between August 2017 and February 2019. The primary outcome is postpartum depression in the year after birth, defined by the Patient Health Questionnaire-9 (PHQ-9). Anemia status was assessed using hemoglobin levels that were measured at the time of the interview. Multivariate logistic regression analyses were used to investigate the relationship between postpartum depression and anemia status. RESULTS: Our analysis sample consists of 565 women who completed the PHQ-9, tested for anemia, and had no missing values for covariates. Of these women, 37.5% had anemia (hemoglobin levels ≤ 110 g/L), and 2.7% were classified as showing symptoms of a major depressive disorder (MDD). After adjusting for potential confounders, anemia was significantly associated with increased risk of MDD (OR: 3.48, 95% CI: 1.15-10.57, p-value: 0.03). No significant associations were found between other covariates and postpartum depression. CONCLUSIONS: Our findings suggest a potential association between anemia and postpartum depression among women in Malawi. Policies that aim to improve nutrition and health outcomes for pregnant and postpartum women could generate a ""double benefit"" by both preventing anemia and reducing the risk of postpartum depression."
MAHESH KARRA,Curse of the Mummy‐ji : the influence of mothers‐in‐law on women in India †,"Restrictive social norms and strategic constraints imposed by family members can limit women's access to and benefits from social networks, especially in patrilocal societies. We characterize young married women's social networks in rural India and analyze how inter‐generational power dynamics within the household affect their network formation. Using primary data from Uttar Pradesh, we show that co‐residence with the mother‐in‐law is negatively correlated with her daughter‐in‐law's mobility and ability to form social connections outside the household, especially those related to health, fertility, and family planning. Our findings suggest that the mother‐in‐law's restrictive behavior is potentially driven by the misalignment of fertility preferences between the mother‐in‐law and the daughter‐in‐law. The lack of peers outside the household lowers the daughter‐in‐law's likelihood of visiting a family planning clinic and of using modern contraception. We find suggestive evidence that this is because outside peers (a) positively influence daughter‐in‐law's beliefs about the social acceptability of family planning and (b) enable the daughter‐in‐law to overcome mobility constraints by accompanying her to health clinics."
MAHESH KARRA,Convincing the Mummy-ji: improving mother-in-law approval of family planning in India,"Mothers-in-law, especially those in South Asia, can exert significant influence over women, often even more so than women's husbands or other household members. Using data from rural India, we first show that mothers-in-law are more likely than husbands to (i) disapprove of women's family planning use and (ii) want women to have more children, particularly sons, than women themselves want. Next, using a field experiment, we show that providing women with vouchers for subsidized family planning services not only enabled them to initiate discussions about family planning with their mothers-in-law but also increased their mothers-in-law's approval of family planning."
MAHESH KARRA,Causal language and strength of inference in academic and media articles shared in social media (CLAIMS): a systematic review,"BACKGROUND: The pathway from evidence generation to consumption contains many steps which can lead to overstatement or misinformation. The proliferation of internet-based health news may encourage selection of media and academic research articles that overstate strength of causal inference. We investigated the state of causal inference in health research as it appears at the end of the pathway, at the point of social media consumption. METHODS: We screened the NewsWhip Insights database for the most shared media articles on Facebook and Twitter reporting about peer-reviewed academic studies associating an exposure with a health outcome in 2015, extracting the 50 most-shared academic articles and media articles covering them. We designed and utilized a review tool to systematically assess and summarize studies’ strength of causal inference, including generalizability, potential confounders, and methods used. These were then compared with the strength of causal language used to describe results in both academic and media articles. Two randomly assigned independent reviewers and one arbitrating reviewer from a pool of 21 reviewers assessed each article. RESULTS: We accepted the most shared 64 media articles pertaining to 50 academic articles for review, representing 68% of Facebook and 45% of Twitter shares in 2015. Thirty-four percent of academic studies and 48% of media articles used language that reviewers considered too strong for their strength of causal inference. Seventy percent of academic studies were considered low or very low strength of inference, with only 6% considered high or very high strength of causal inference. The most severe issues with academic studies’ causal inference were reported to be omitted confounding variables and generalizability. Fifty-eight percent of media articles were found to have inaccurately reported the question, results, intervention, or population of the academic study. CONCLUSIONS: We find a large disparity between the strength of language as presented to the research consumer and the underlying strength of causal inference among the studies most widely shared on social media. However, because this sample was designed to be representative of the articles selected and shared on social media, it is unlikely to be representative of all academic and media work. More research is needed to determine how academic institutions, media organizations, and social network sharing patterns impact causal inference and language as received by the research consumer."
IVAN JULIO,Overreaction in the REITs market: new evidence from quantile autoregression approach,"Real estate investment trusts (REITs) provide portfolio diversification and tax benefits, a stable stream of income, and inflation hedging to investors. This study employs a quantile autoregression model to investigate the dependence structures of REITs’ returns across quantiles and return frequencies. This approach permits investigation of the marginal and aggregate effects of the sign and size of returns, business cycles, volatility, and REIT eras on the dependence structure of daily, weekly, and monthly REIT returns. The study documents asymmetric and misaligned dependence patterns. A bad market state is characterized by either positive or weakly negative dependence, while a good market state is generally marked by negative dependence on past returns. The results are consistent with under-reaction to good news in a bad state and overreaction to bad news in a good state. Past negative returns increase and decrease the predictability of REIT returns at lower and upper quantiles, respectively. Extreme positive returns in the lower (upper) quantiles dampen (amplify) autocorrelation of daily, weekly, and monthly REIT returns. The previous day’s REIT returns dampen autoregression more during recession periods than during non-recession periods. The marginal impact of the high volatility of daily returns supports a positive feedback trading strategy. The marginal impact of the Vintage REIT era on monthly return autocorrelation is higher than the New REIT era, suggesting that increased participation of retail and institutional investors improves market efficiency by reducing REITs’ returns predictability. Overall, the evidence supports the time-varying efficiency of the REITs markets and adaptive market hypothesis. The predictability of REIT returns is driven by the state of the market, sign, size, volatility, and frequency of returns. The results have implications for trading strategies, policies for the real estate securitization process, and investment decisions."
CHRISTOPHER T LIM,Thoracolumbar Injury Classification and Severity Score: A New Paradigm for the Treatment of Thoracolumbar Spine Trauma,"BACKGROUND Contemporary understanding of the biomechanics, natural history, and methods of treating thoracolumbar spine injuries continues to evolve. Current classification schemes of these injuries, however, can be either too simplified or overly complex for clinical use. METHODS The Spine Trauma Group was given a survey to identify similarities in treatment algorithms for common thoracolumbar injuries, as well as to identify characteristics of injury that played a key role in the decision-making process. RESULTS Based on the survey, the Spine Trauma Group has developed a classification system and an injury severity score (thoracolumbar injury classification and severity score, or TLICS), which may facilitate communication between physicians and serve as a guideline for treating these injuries. The classification system is based on the morphology of the injury, integrity of the posterior ligamentous complex, and neurological status of the patient. Points are assigned for each category, and the final total points suggest a possible treatment option. CONCLUSIONS The usefulness of this new system will have to be proven in future studies investigating inter- and intraobserver reliability, as well as long-term outcome studies for operative and nonoperative treatment methods."
RICARDO H SOUSA,The long-baseline neutrino experiment: exploring fundamental symmetries of the universe,"The preponderance of matter over antimatter in the early Universe, the dynamics of the supernova bursts that produced the heavy elements necessary for life and whether protons eventually decay --- these mysteries at the forefront of particle physics and astrophysics are key to understanding the early evolution of our Universe, its current state and its eventual fate. The Long-Baseline Neutrino Experiment (LBNE) represents an extensively developed plan for a world-class experiment dedicated to addressing these questions. LBNE is conceived around three central components: (1) a new, high-intensity neutrino source generated from a megawatt-class proton accelerator at Fermi National Accelerator Laboratory, (2) a near neutrino detector just downstream of the source, and (3) a massive liquid argon time-projection chamber deployed as a far detector deep underground at the Sanford Underground Research Facility. This facility, located at the site of the former Homestake Mine in Lead, South Dakota, is approximately 1,300 km from the neutrino source at Fermilab -- a distance (baseline) that delivers optimal sensitivity to neutrino charge-parity symmetry violation and mass ordering effects. This ambitious yet cost-effective design incorporates scalability and flexibility and can accommodate a variety of upgrades and contributions. With its exceptional combination of experimental configuration, technical capabilities, and potential for transformative discoveries, LBNE promises to be a vital facility for the field of particle physics worldwide, providing physicists from around the globe with opportunities to collaborate in a twenty to thirty year program of exciting science. In this document we provide a comprehensive overview of LBNE's scientific objectives, its place in the landscape of neutrino physics worldwide, the technologies it will incorporate and the capabilities it will possess."
STEPHANIE KUNTZ,"Chiasma: April 1972 v. 3, no. 4",
KIERSTEN STROMBOTNE,Healthcare-based food assistance programmes in the United States: a scoping review and typology,"This scoping review aimed to identify the breadth of healthcare-based food assistance programmes in the United States and organize them into a typology of programmes to provide implementation guidance to aspiring food assistance programmers in healthcare settings. We searched PubMed, Cochrane, and CINAHL databases for peer-reviewed articles published between 1 January 2010 and 31 December 2021, and mined reference lists. We used content analysis to extract programmatic details from each intervention and to qualitatively analyse intervention components to develop a typology for healthcare institutions in the United States. Eligible articles included descriptions of patient populations served and programmatic details. Articles were not required to include formal evaluations for inclusion in this scoping review. Our search resulted in 8706 abstracts, which yielded forty-three articles from thirty-five interventions. We identified three distinct programme types: direct food provision, referral, and voucher programmes. Programme type was influenced by programme goals, logistical considerations, such as staffing, food storage or refrigeration space, and existence of willing partner CBOs. Food provision programmes (n 13) were frequently permanent and leveraged partnerships with community-based organisations (CBOs) that provide food. Referral programmes (n 8) connected patients to CBOs for federal or local food assistance enrollment. Voucher programmes (n 14) prioritised provision of fruits and vegetables (n 10) and relied on a variety of clinic staff to refer patients to months-long programmes. Healthcare-based implementers can use this typology to design and maintain programmes that align with the needs of their sites and patient populations."
LAURA T. ADAM,Priorities for synthesis research in ecology and environmental science,
LAURA T. ADAM,The long-baseline neutrino experiment: exploring fundamental symmetries of the universe,"The preponderance of matter over antimatter in the early Universe, the dynamics of the supernova bursts that produced the heavy elements necessary for life and whether protons eventually decay --- these mysteries at the forefront of particle physics and astrophysics are key to understanding the early evolution of our Universe, its current state and its eventual fate. The Long-Baseline Neutrino Experiment (LBNE) represents an extensively developed plan for a world-class experiment dedicated to addressing these questions. LBNE is conceived around three central components: (1) a new, high-intensity neutrino source generated from a megawatt-class proton accelerator at Fermi National Accelerator Laboratory, (2) a near neutrino detector just downstream of the source, and (3) a massive liquid argon time-projection chamber deployed as a far detector deep underground at the Sanford Underground Research Facility. This facility, located at the site of the former Homestake Mine in Lead, South Dakota, is approximately 1,300 km from the neutrino source at Fermilab -- a distance (baseline) that delivers optimal sensitivity to neutrino charge-parity symmetry violation and mass ordering effects. This ambitious yet cost-effective design incorporates scalability and flexibility and can accommodate a variety of upgrades and contributions. With its exceptional combination of experimental configuration, technical capabilities, and potential for transformative discoveries, LBNE promises to be a vital facility for the field of particle physics worldwide, providing physicists from around the globe with opportunities to collaborate in a twenty to thirty year program of exciting science. In this document we provide a comprehensive overview of LBNE's scientific objectives, its place in the landscape of neutrino physics worldwide, the technologies it will incorporate and the capabilities it will possess."
SARAH CRANE,“It’s delightful to be married” depictions of marriage in the films of Myrna Loy and William Powell,"Myrna Loy and William Powell appeared in fourteen films together, made from 1934 to 1947, resulting in an unprecedented number of cinematic pairing within the classical Hollywood studio system. This might prompt one to wonder: what was it about their onscreen personas and characters that contributed to their success as the quintessential ‘screwball’ couple? Drawing upon genre studies, this thesis examines the performances of Loy and Powell, the comedic intertextuality developed within their filmic oeuvre, and their films’ contributions to the romantic comedy genre. The films discussed within this thesis focus mainly on Loy and Powell’s roles as married couples within ‘The Thin Man’ film series, including The Thin Man (1934), After the Thin Man (1936), Another Thin Man (1939), Shadow of the Thin Man (1941), The Thin Man Goes Home (1945), and The Song of the Thin Man (1947), as well as their stand-alone performances in the screwball and marital comedies, Libeled Lady (1936), Double Wedding (1937), I Love You Again (1940), and Love Crazy (1941). By analyzing the progression of Loy and Powell’s roles as iconic screwball and romantic comedy couples, this thesis traces the lasting cultural impact and legacy Loy and Powell’s films have had on subsequent generations of filmmakers and moviegoers."
SARAH CRANE,The eighteenth data release of the Sloan Digital Sky Surveys: targeting and first spectra from SDSS-V,"The eighteenth data release (DR18) of the Sloan Digital Sky Survey (SDSS) is the first one for SDSS-V, the fifth generation of the survey. SDSS-V comprises three primary scientific programs or “Mappers”: the Milky Way Mapper (MWM), the Black Hole Mapper (BHM), and the Local Volume Mapper. This data release contains extensive targeting information for the two multiobject spectroscopy programs (MWM and BHM), including input catalogs and selection functions for their numerous scientific objectives. We describe the production of the targeting databases and their calibration and scientifically focused components. DR18 also includes ∼25,000 new SDSS spectra and supplemental information for X-ray sources identified by eROSITA in its eFEDS field. We present updates to some of the SDSS software pipelines and preview changes anticipated for DR19. We also describe three value-added catalogs (VACs) based on SDSS-IV data that have been published since DR17, and one VAC based on the SDSS-V data in the eFEDS field."
OLGA MINAEVA,"Concussion, microvascular injury, and early tauopathy in young athletes after impact head injury and an impact concussion mouse model","The mechanisms underpinning concussion, traumatic brain injury, and chronic traumatic encephalopathy, and the relationships between these disorders, are poorly understood. We examined post-mortem brains from teenage athletes in the acute-subacute period after mild closed-head impact injury and found astrocytosis, myelinated axonopathy, microvascular injury, perivascular neuroinflammation, and phosphorylated tau protein pathology. To investigate causal mechanisms, we developed a mouse model of lateral closed-head impact injury that uses momentum transfer to induce traumatic head acceleration. Unanaesthetized mice subjected to unilateral impact exhibited abrupt onset, transient course, and rapid resolution of a concussion-like syndrome characterized by altered arousal, contralateral hemiparesis, truncal ataxia, locomotor and balance impairments, and neurobehavioural deficits. Experimental impact injury was associated with axonopathy, blood–brain barrier disruption, astrocytosis, microgliosis (with activation of triggering receptor expressed on myeloid cells, TREM2), monocyte infiltration, and phosphorylated tauopathy in cerebral cortex ipsilateral and subjacent to impact. Phosphorylated tauopathy was detected in ipsilateral axons by 24 h, bilateral axons and soma by 2 weeks, and distant cortex bilaterally at 5.5 months post-injury. Impact pathologies co-localized with serum albumin extravasation in the brain that was diagnostically detectable in living mice by dynamic contrast-enhanced MRI. These pathologies were also accompanied by early, persistent, and bilateral impairment in axonal conduction velocity in the hippocampus and defective long-term potentiation of synaptic neurotransmission in the medial prefrontal cortex, brain regions distant from acute brain injury. Surprisingly, acute neurobehavioural deficits at the time of injury did not correlate with blood–brain barrier disruption, microgliosis, neuroinflammation, phosphorylated tauopathy, or electrophysiological dysfunction. Furthermore, concussion-like deficits were observed after impact injury, but not after blast exposure under experimental conditions matched for head kinematics. Computational modelling showed that impact injury generated focal point loading on the head and seven-fold greater peak shear stress in the brain compared to blast exposure. Moreover, intracerebral shear stress peaked before onset of gross head motion. By comparison, blast induced distributed force loading on the head and diffuse, lower magnitude shear stress in the brain. We conclude that force loading mechanics at the time of injury shape acute neurobehavioural responses, structural brain damage, and neuropathological sequelae triggered by neurotrauma. These results indicate that closed-head impact injuries, independent of concussive signs, can induce traumatic brain injury as well as early pathologies and functional sequelae associated with chronic traumatic encephalopathy. These results also shed light on the origins of concussion and relationship to traumatic brain injury and its aftermath."
MARK HOWE,Unique contributions of parvalbumin and cholinergic interneurons in organizing striatal networks during movement,"Striatal pavalbumin (PV) and cholinergic (CHI) interneurons are poised to play major roles in behavior by coordinating the networks of medium spiny cells that relay motor output. However, the small numbers and scattered distribution of these cells has made it difficult to directly assess their contribution to activity in networks of MSNs during behavior. Here, we build upon recent improvements in single cell calcium imaging combined with optogenetics to test the capacity of PVs and CHIs to affect MSN activity and behavior in mice engaged in voluntarily locomotion. We find that PVs and CHIs have unique effects on MSN activity and dissociable roles in supporting movement. PV cells facilitate movement by refining the activation of MSN networks responsible for movement execution. CHIs, in contrast, synchronize activity within MSN networks to signal the end of a movement bout. These results provide new insights into the striatal network activity that supports movement."
MARK HOWE,"Targeted micro-fiber arrays for measuring and manipulating localized multi-scale neural dynamics over large, deep brain volumes during behavior","Neural population dynamics relevant for behavior vary over multiple spatial and temporal scales across 3-dimensional volumes. Current optical approaches lack the spatial coverage and resolution necessary to measure and manipulate naturally occurring patterns of large-scale, distributed dynamics within and across deep brain regions such as the striatum. We designed a new micro-fiber array and imaging approach capable of chronically measuring and optogenetically manipulating local dynamics across over 100 targeted locations simultaneously in head-fixed and freely moving mice. We developed a semi-automated micro-CT based strategy to precisely localize positions of each optical fiber. This highly-customizable approach enables investigation of multi-scale spatial and temporal patterns of cell-type and neurotransmitter specific signals over arbitrary 3-D volumes at a spatial resolution and coverage previously inaccessible. We applied this method to resolve rapid dopamine release dynamics across the striatum volume which revealed distinct, modality specific spatiotemporal patterns in response to salient sensory stimuli extending over millimeters of tissue. Targeted optogenetics through our fiber arrays enabled flexible control of neural signaling on multiple spatial scales, better matching endogenous signaling patterns, and spatial localization of behavioral function across large circuits."
ERIC MARKS,Analysis of OD Flows (Raw Data),"In a recent paper, Structural Analysis of Network Traffic Flows, we analyzed the set of Origin Destination traffic flows from the Sprint-Europe and Abilene backbone networks. This report presents the complete set of results from analyzing data from both networks. The results in this report are specific to the Sprint-1 and Abilene datasets studied in the above paper. The following results are presented here: 1 Rows of Principal Matrix (V) 2 1.1 Sprint-1 Dataset ................................ 2 1.2 Abilene Dataset.................................. 9 2 Set of Eigenflows 14 2.1 Sprint-1 Dataset.................................. 14 2.2 Abilene Dataset................................... 21 3 Classifying Eigenflows 26 3.1 Sprint-1 Dataset.................................. 26 3.2 Abilene Datase.................................... 44"
ERIC MARKS,Structural Analysis of Network Traffic Flows,"Network traffic arises from the superposition of Origin-Destination (OD) flows. Hence, a thorough understanding of OD flows is essential for modeling network traffic, and for addressing a wide variety of problems including traffic engineering, traffic matrix estimation, capacity planning, forecasting and anomaly detection. However, to date, OD flows have not been closely studied, and there is very little known about their properties. We present the first analysis of complete sets of OD flow timeseries, taken from two different backbone networks (Abilene and Sprint-Europe). Using Principal Component Analysis (PCA), we find that the set of OD flows has small intrinsic dimension. In fact, even in a network with over a hundred OD flows, these flows can be accurately modeled in time using a small number (10 or less) of independent components or dimensions. We also show how to use PCA to systematically decompose the structure of OD flow timeseries into three main constituents: common periodic trends, short-lived bursts, and noise. We provide insight into how the various constituents contribute to the overall structure of OD flows and explore the extent to which this decomposition varies over time."
ERIC MARKS,Adaptation of Dopamine Neurons' Mismatch Sensitivity Is not Compatible With Reinforcement Learning Theory,Recent electrophysical data inspired the claim that dopaminergic neurons adapt their mismatch sensitivities to reflect variances of expected rewards. This contradicts reward prediction error theory and most basal ganglia models. Application of learning principles points to a testable alternative interpretation-of the same data-that is compatible with existing theory.
ERIC MARKS,Graph Wavelets for Spatial Traffic Analysis,"A number of problems in network operations and engineering call for new methods of traffic analysis. While most existing traffic analysis methods are fundamentally temporal, there is a clear need for the analysis of traffic across multiple network links — that is, for spatial traffic analysis. In this paper we give examples of problems that can be addressed via spatial traffic analysis. We then propose a formal approach to spatial traffic analysis based on the wavelet transform. Our approach (graph wavelets) generalizes the traditional wavelet transform so that it can be applied to data elements connected via an arbitrary graph topology. We explore the necessary and desirable properties of this approach and consider some of its possible realizations. We then apply graph wavelets to measurements from an operating network. Our results show that graph wavelets are very useful for our motivating problems; for example, they can be used to form highly summarized views of an entire network's traffic load, to gain insight into a network's global traffic response to a link failure, and to localize the extent of a failure event within the network."
ERIC MARKS,Meta-analysis and systematic review of skin graft donor-site dressings with future guidelines.,"Background: Many types of split-thickness skin graft (STSG) donor-site dressings are available with little consensus from the literature on the optimal dressing type. The purpose of this systematic review was to analyze the most recent outcomes regarding moist and nonmoist dressings for STSG donor sites. Methods: A comprehensive systematic review was conducted across PubMed/MEDLINE, EMBASE, and Cochrane Library databases to search for comparative studies evaluating different STSG donor-site dressings in adult subjects published between 2008 and 2017. The quality of randomized controlled trials was assessed using the Jadad scale. Data were collected on donor-site pain, rate of epithelialization, infection rate, cosmetic appearance, and cost. Meta-analysis was performed for reported pain scores. Results: A total of 41 articles were included comparing 44 dressings. Selected studies included analysis of donor-site pain (36 of 41 articles), rate of epithelialization (38 of 41), infection rate (25 of 41), cosmetic appearance (20 of 41), and cost (10 of 41). Meta-analysis revealed moist dressings result in lower pain (pooled effect size = 1.44). A majority of articles (73%) reported better reepithelialization rates with moist dressings. Conclusion: The literature on STSG donor-site dressings has not yet identified an ideal dressing. Although moist dressings provide superior outcomes with regard to pain control and wound healing, there continues to be a lack of standardization. The increasing commercial availability and marketing of novel dressings necessitates the development of standardized research protocols to design better comparison studies and assess true efficacy."
ERIC MARKS,Familiarity Discrimination of Radar Pulses,"The ARTMAP-FD neural network performs both identification (placing test patterns in classes encountered during training) and familiarity discrimination (judging whether a test pattern belongs to any of the classes encountered during training). The performance of ARTMAP-FD is tested on radar pulse data obtained in the field, and compared to that of the nearest-neighbor-based NEN algorithm and to a k > 1 extension of NEN."
ERIC MARKS,Classification of Incomplete Data Using the Fuzzy ARTMAP Neural Network,"The fuzzy ARTMAP neural network is used to classify data that is incomplete in one or more ways. These include a limited number of training cases, missing components, missing class labels, and missing classes. Modifications for dealing with such incomplete data are introduced, and performance is assessed on an emitter identification task using a data base of radar pulses"
ERIC MARKS,The Role of the Applied Epidemiologist in Armed Conflict,"BACKGROUND: Applied epidemiologists are increasingly working in areas of insecurity and active conflict to define the health risks, suggest feasible means to reduce these risks and, monitor the capacity and reconstruction of the public health system. In 2001, The Carter Center and the United States Institute for Peace sponsored a conference within which ""Violence and Health"" was discussed and a working group on applied epidemiology formed. The group was tasked to describe the skills that are essential to effective functioning in these settings and thereby provide guidance to the applied epidemiology training programs. METHODS: We conducted a literature review and consultation of a convenience sample of practitioners of applied epidemiology with experience in conflict areas. RESULTS AND CONCLUSIONS: The health programs designed to prevent and mitigate conflict are in their early stages of implementation and the evaluation measures for success are still being defined. The practice of epidemiology in conflict must occur within a larger humanitarian and political context to be effective. The skills required extend beyond the normal epidemiological training that focuses on the valid collection and interpretation of data and fall into two general categories: (1) Conducting a thorough assessment of the conflict setting in order to design more effective public health action in conflict settings, and (2) Communicating effectively to guide health program implementation, to advocate for needed policy changes and to facilitate interagency coordination. These are described and illustrated using examples from different countries."
ERIC MARKS,A What-and-Where Fusion Neural Network for Recognition and Tracking of Multiple Radar Emitters,"A neural network recognition and tracking system is proposed for classification of radar pulses in autonomous Electronic Support Measure systems. Radar type information is combined with position-specific information from active emitters in a scene. Type-specific parameters of the input pulse stream are fed to a neural network classifier trained on samples of data collected in the field. Meanwhile, a clustering algorithm is used to separate pulses from different emitters according to position-specific parameters of the input pulse stream. Classifier responses corresponding to different emitters are separated into tracks, or trajectories, one per active emitter, allowing for more accurate identification of radar types based on multiple views of emitter data along each emitter trajectory. Such a What-and-Where fusion strategy is motivated by a similar subdivision of labor in the brain. The fuzzy ARTMAP neural network is used to classify streams of pulses according to radar type using their functional parameters. Simulation results obtained with a radar pulse data set indicate that fuzzy AIUMAP compares favorably to several other approaches when performance is measured in terms of accuracy and computational complexity. Incorporation into fuzzy ARTMAP of negative match tracking (from ARTMAP-IC) facilitated convergence during training with this data set. Other modifications improved classification of data that include missing input pattern components and missing training classes. Fuzzy ARTMAP was combined with a bank of Kalman filters to group pulses transmitted from different emitters based on their position-specific parameters, and with a module to accumulate evidence from fuzzy ARTMAP responses corresponding to the track defined for each emitter. Simulation results demonstrate that the system provides a high level of performance on complex, incomplete and overlapping radar data."
ERIC MARKS,"D-cycloserine augmentation of exposure-based cognitive behavior therapy for anxiety, obsessive-compulsive, and posttraumatic stress disorders: a systematic review and meta-analysis of individual participant data","Importance: Whether and under which conditions D-cycloserine (DCS) augments the effects of exposure-based cognitive behavior therapy for anxiety, obsessive-compulsive, and posttraumatic stress disorders is unclear. Objective: To clarify whether DCS is superior to placebo in augmenting the effects of cognitive behavior therapy for anxiety, obsessive-compulsive, and posttraumatic stress disorders and to evaluate whether antidepressants interact with DCS and the effect of potential moderating variables. Data Sources: PubMed, EMBASE, and PsycINFO were searched from inception to February 10, 2016. Reference lists of previous reviews and meta-analyses and reports of randomized clinical trials were also checked. Study Selection: Studies were eligible for inclusion if they were (1) double-blind randomized clinical trials of DCS as an augmentation strategy for exposure-based cognitive behavior therapy and (2) conducted in humans diagnosed as having specific phobia, social anxiety disorder, panic disorder with or without agoraphobia, obsessive-compulsive disorder, or posttraumatic stress disorder. Data Extraction and Synthesis: Raw data were obtained from the authors and quality controlled. Data were ranked to ensure a consistent metric across studies (score range, 0-100). We used a 3-level multilevel model nesting repeated measures of outcomes within participants, who were nested within studies. Results: Individual participant data were obtained for 21 of 22 eligible trials, representing 1047 of 1073 eligible participants. When controlling for antidepressant use, participants receiving DCS showed greater improvement from pretreatment to posttreatment (mean difference, -3.62; 95% CI, -0.81 to -6.43; P = .01; d = -0.25) but not from pretreatment to midtreatment (mean difference, -1.66; 95% CI, -4.92 to 1.60; P = .32; d = -0.14) or from pretreatment to follow-up (mean difference, -2.98, 95% CI, -5.99 to 0.03; P = .05; d = -0.19). Additional analyses showed that participants assigned to DCS were associated with lower symptom severity than those assigned to placebo at posttreatment and at follow-up. Antidepressants did not moderate the effects of DCS. None of the prespecified patient-level or study-level moderators was associated with outcomes. Conclusions and Relevance: D-cycloserine is associated with a small augmentation effect on exposure-based therapy. This effect is not moderated by the concurrent use of antidepressants. Further research is needed to identify patient and/or therapy characteristics associated with DCS response."
ERIC MARKS,Land and cryosphere products from Suomi NPP VIIRS: overview and status,"[1] The Visible Infrared Imaging Radiometer Suite (VIIRS) instrument was launched in October 2011 as part of the Suomi National Polar-Orbiting Partnership (S-NPP). The VIIRS instrument was designed to improve upon the capabilities of the operational Advanced Very High Resolution Radiometer and provide observation continuity with NASA's Earth Observing System's Moderate Resolution Imaging Spectroradiometer (MODIS). Since the VIIRS first-light images were received in November 2011, NASA- and NOAA-funded scientists have been working to evaluate the instrument performance and generate land and cryosphere products to meet the needs of the NOAA operational users and the NASA science community. NOAA's focus has been on refining a suite of operational products known as Environmental Data Records (EDRs), which were developed according to project specifications under the National Polar-Orbiting Environmental Satellite System. The NASA S-NPP Science Team has focused on evaluating the EDRs for science use, developing and testing additional products to meet science data needs, and providing MODIS data product continuity. This paper presents to-date findings of the NASA Science Team's evaluation of the VIIRS land and cryosphere EDRs, specifically Surface Reflectance, Land Surface Temperature, Surface Albedo, Vegetation Indices, Surface Type, Active Fires, Snow Cover, Ice Surface Temperature, and Sea Ice Characterization. The study concludes that, for MODIS data product continuity and earth system science, an enhanced suite of land and cryosphere products and associated data system capabilities are needed beyond the EDRs currently available from the VIIRS."
ERIC MARKS,Network kriging,"Network service providers and customers are often concerned with aggregate performance measures that span multiple network paths. Unfortunately, forming such network-wide measures can be difficult, due to the issues of scale involved. In particular, the number of paths grows too rapidly with the number of endpoints to make exhaustive measurement practical. As a result, it is of interest to explore the feasibility of methods that dramatically reduce the number of paths measured in such situations while maintaining acceptable accuracy. We cast the problem as one of statistical prediction—in the spirit of the so-called ‘kriging’ problem in spatial statistics—and show that end-to-end network properties may be accurately predicted in many cases using a surprisingly small set of carefully chosen paths. More precisely, we formulate a general framework for the prediction problem, propose a class of linear predictors for standard quantities of interest (e.g., averages, totals, differences) and show that linear algebraic methods of subset selection may be used to effectively choose which paths to measure. We characterize the performance of the resulting methods, both analytically and numerically. The success of our methods derives from the low effective rank of routing matrices as encountered in practice, which appears to be a new observation in its own right with potentially broad implications on network measurement generally."
ERIC MARKS,"Bostonia: 1999-2000, no. 1-4",
ERIC MARKS,Comparison of Classifiers for Radar Emitter Type Identification,"ARTMAP neural network classifiers are considered for the identification of radar emitter types from their waveform parameters. These classifiers can represent radar emitter type classes with one or more prototypes, perform on-line incremental learning to account for novelty encountered in the field, and process radar pulse streams at high speed, making them attractive for real-time applications such as electronic support measures (ESM). The performance of four ARTMAP variants- ARTMAP (Stage 1), ARTMAP-IC, fuzzy ARTMAP and Gaussian ARTMAP - is assessed with radar data gathered in the field. The k nearest neighbor (kNN) and radial basis function (RDF) classifiers are used for reference. Simulation results indicate that fuzzy ARTMAP and Gaussian ARTMAP achieve an average classification rate consistently higher than that of the other ARTMAP classifers and comparable to that of kNN and RBF. ART-EMAP, ARTMAP-IC and fuzzy ARTMAP require fewer training epochs than Gaussian ARTMAP and RBF, and substantially fewer prototype vectors (thus, smaller physical memory requirements and faster fielded performance) than Gaussian ARTMAP, RBF and kNN. Overall, fuzzy ART MAP performs at least as well as the other classifiers in both accuracy and computational complexity, and better than each of them in at least one of these aspects of performance. Incorporation into fuzzy ARTMAP of the MT- feature of ARTMAP-IC is found to be essential for convergence during on-line training with this data set."
ERIC MARKS,A statistical framework for efficient monitoring of end-to-end network properties,"Network service providers and customers are often concerned with aggregate performance measures that span multiple network paths. Unfortunately, forming such network-wide measures can be difficult, due to the issues of scale involved. In particular, the number of paths grows too rapidly with the number of endpoints to make exhaustive measurement practical. As a result, it is of interest to explore the feasibility of methods that dramatically reduce the number of paths measured in such situations while maintaining acceptable accuracy. In previous work we have proposed a statistical framework for efficiently addressing this problem, in the context of additive metrics such as delay and loss rate, for which the per-path metric is a sum of per-link measures (possibly under appropriate transformation). The key to our method lies in the observation and exploitation of the fact that network paths show significant redundancy (sharing of common links). In this paper we make three contributions: (1) we generalize the framework to make it more immediately applicable to network measurements encountered in practice; (2) we demonstrate that the observed path redundancy upon which our method is based is robust to variation in key network conditions and characteristics, including the presence of link failures; and (3) we show how the framework may be applied to address three practical problems of interest to network providers and customers, using data from an operating network. In particular, we show how appropriate selection of small sets of path measurements can be used to accurately estimate network-wide averages of path delays, to reliably detect network anomalies, and to effectively make a choice between alternative sub-networks, as a customer choosing between two providers or two ingress points into a provider network."
ERIC MARKS,"Excavation of an obsidian craft workshop at Teotihuacan, Mexico","The original research by the Teotihuacan Mapping Project (TMP) identified a large number of obsidian workshops within Teotihuacan based on surface concentrations of production debris. Clark (1986b) questioned the validity of these identifications and called for subsurface excavation to confirm the presence of in situ workshop locales. This article summarizes the results from the excavation of one of the obsidian workshops identified in the Tlajinga district of Teotihuacan at Compound 17:S3E1 (Compound 17). We describe the excavations, discuss the lithic technology, and examine the subsurface contexts in terms of what they tell us about in situ obsidian craft activity. Excavations confirm that Compound 17 was a locus of large-scale obsidian craft production during the Classic period. While only a single test case, these results suggest that surface remains at Teotihuacan can be a useful guide in identifying craft production areas when they are confirmed through subsurface testing."
ERIC MARKS,Kinship ties across the lifespan in human communities,"A hypothesis for the evolution of long post-reproductive lifespans in the human lineage involves asymmetries in relatedness between young immigrant females and the older females in their new groups. In these circumstances, inter-generational reproductive conflicts between younger and older females are predicted to resolve in favour of the younger females, who realize fewer inclusive fitness benefits from ceding reproduction to others. This conceptual model anticipates that immigrants to a community initially have few kin ties to others in the group, gradually showing greater relatedness to group members as they have descendants who remain with them in the group. We examine this prediction in a cross-cultural sample of communities, which vary in their sex-biased dispersal patterns and other aspects of social organization. Drawing on genealogical and demographic data, the analysis provides general but not comprehensive support for the prediction that average relatedness of immigrants to other group members increases as they age. In rare cases, natal members of the community also exhibit age-related increases in relatedness. We also find large variation in the proportion of female group members who are immigrants, beyond simple traditional considerations of patrilocality or matrilocality, which raises questions about the circumstances under which this hypothesis of female competition are met. We consider possible explanations for these heterogenous results, and we address methodological considerations that merit increased attention for research on kinship and reproductive conflict in human societies.This article is part of the theme issue ‘The evolution of female-biased kinship in humans and other mammals’."
ERIC MARKS,Suicide-Related Behaviors in Older Patients with New Anti-Epileptic Drug Use: Data from the VA Hospital System,"BACKGROUND: The U.S. Food and Drug Administration (FDA) recently linked antiepileptic drug (AED) exposure to suicide-related behaviors based on meta-analysis of randomized clinical trials. We examined the relationship between suicide-related behaviors and different AEDs in older veterans receiving new AED monotherapy from the Veterans Health Administration (VA), controlling for potential confounders. METHODS: VA and Medicare databases were used to identify veterans 66 years and older, who received a) care from the VA between 1999 and 2004, and b) an incident AED (monotherapy) prescription. Previously validated ICD-9-CM codes were used to identify suicidal ideation or behavior (suicide-related behaviors cases), epilepsy, and other conditions previously associated with suicide-related behaviors. Each case was matched to controls based on prior history of suicide-related behaviors, year of AED prescription, and epilepsy status. RESULTS: The strongest predictor of suicide-related behaviors (N = 64; Controls N = 768) based on conditional logistic regression analysis was affective disorder (depression, anxiety, or post-traumatic stress disorder (PTSD); Odds Ratio 4.42, 95% CI 2.30 to 8.49) diagnosed before AED treatment. Increased suicide-related behaviors were not associated with individual AEDs, including the most commonly prescribed AED in the US - phenytoin. CONCLUSION: Our extensive diagnostic and treatment data demonstrated that the strongest predictor of suicide-related behaviors for older patients newly treated with AED monotherapy was a previous diagnosis of affective disorder. Additional, research using a larger sample is needed to clearly determine the risk of suicide-related behaviors among less commonly used AEDs."
ERIC MARKS,"Genome-wide association studies of serum magnesium, potassium, and sodium concentrations identify six loci influencing serum magnesium levels","Magnesium, potassium, and sodium, cations commonly measured in serum, are involved in many physiological processes including energy metabolism, nerve and muscle function, signal transduction, and fluid and blood pressure regulation. To evaluate the contribution of common genetic variation to normal physiologic variation in serum concentrations of these cations, we conducted genome-wide association studies of serum magnesium, potassium, and sodium concentrations using ∼2.5 million genotyped and imputed common single nucleotide polymorphisms (SNPs) in 15,366 participants of European descent from the international CHARGE Consortium. Study-specific results were combined using fixed-effects inverse-variance weighted meta-analysis. SNPs demonstrating genome-wide significant (p<5×10−8) or suggestive associations (p<4×10−7) were evaluated for replication in an additional 8,463 subjects of European descent. The association of common variants at six genomic regions (in or near MUC1, ATP2B1, DCDC5, TRPM6, SHROOM3, and MDS1) with serum magnesium levels was genome-wide significant when meta-analyzed with the replication dataset. All initially significant SNPs from the CHARGE Consortium showed nominal association with clinically defined hypomagnesemia, two showed association with kidney function, two with bone mineral density, and one of these also associated with fasting glucose levels. Common variants in CNNM2, a magnesium transporter studied only in model systems to date, as well as in CNNM3 and CNNM4, were also associated with magnesium concentrations in this study. We observed no associations with serum sodium or potassium levels exceeding p<4×10−7. Follow-up studies of newly implicated genomic loci may provide additional insights into the regulation and homeostasis of human serum magnesium levels. Author Summary Magnesium, potassium, and sodium are involved in important physiological processes. To better understand how common genetic variation may contribute to inter-individual differences in serum concentrations of these electrolytes, we evaluated single nucleotide polymorphisms (SNPs) across the genome in association with serum magnesium, potassium, and sodium levels in 15,366 participants of European descent from the CHARGE Consortium. We then verified the associations in an additional 8,463 study participants. Six different genomic regions contain variants that are reproducibly associated with serum magnesium levels, and only one of the regions had been previously known to influence serum magnesium concentrations in humans. The identified SNPs also show association with clinically defined hypomagnesemia, and some of them with traits that have been linked to serum magnesium levels, including kidney function, fasting glucose, and bone mineral density. We further provide evidence for a physiological role of magnesium transporters in humans which have previously only been studied in model systems. None of the SNPs evaluated in our study are significantly associated with serum levels of sodium or potassium. Additional studies are needed to investigate the underlying molecular mechanisms in order to help us understand the contribution of these newly identified regions to magnesium homeostasis."
ERIC MARKS,"Caribbean Corals in Crisis: Record Thermal Stress, Bleaching, and Mortality in 2005","BACKGROUND. The rising temperature of the world's oceans has become a major threat to coral reefs globally as the severity and frequency of mass coral bleaching and mortality events increase. In 2005, high ocean temperatures in the tropical Atlantic and Caribbean resulted in the most severe bleaching event ever recorded in the basin. METHODOLOGY/PRINCIPAL FINDINGS. Satellite-based tools provided warnings for coral reef managers and scientists, guiding both the timing and location of researchers' field observations as anomalously warm conditions developed and spread across the greater Caribbean region from June to October 2005. Field surveys of bleaching and mortality exceeded prior efforts in detail and extent, and provided a new standard for documenting the effects of bleaching and for testing nowcast and forecast products. Collaborators from 22 countries undertook the most comprehensive documentation of basin-scale bleaching to date and found that over 80% of corals bleached and over 40% died at many sites. The most severe bleaching coincided with waters nearest a western Atlantic warm pool that was centered off the northern end of the Lesser Antilles. CONCLUSIONS/SIGNIFICANCE. Thermal stress during the 2005 event exceeded any observed from the Caribbean in the prior 20 years, and regionally-averaged temperatures were the warmest in over 150 years. Comparison of satellite data against field surveys demonstrated a significant predictive relationship between accumulated heat stress (measured using NOAA Coral Reef Watch's Degree Heating Weeks) and bleaching intensity. This severe, widespread bleaching and mortality will undoubtedly have long-term consequences for reef ecosystems and suggests a troubled future for tropical marine ecosystems under a warming climate."
ERIC MARKS,Integrated Assessment of Genomic Correlates of Protein Evolutionary Rate,"Rates of evolution differ widely among proteins, but the causes and consequences of such differences remain under debate. With the advent of high-throughput functional genomics, it is now possible to rigorously assess the genomic correlates of protein evolutionary rate. However, dissecting the correlations among evolutionary rate and these genomic features remains a major challenge. Here, we use an integrated probabilistic modeling approach to study genomic correlates of protein evolutionary rate in Saccharomyces cerevisiae. We measure and rank degrees of association between (i) an approximate measure of protein evolutionary rate with high genome coverage, and (ii) a diverse list of protein properties (sequence, structural, functional, network, and phenotypic). We observe, among many statistically significant correlations, that slowly evolving proteins tend to be regulated by more transcription factors, deficient in predicted structural disorder, involved in characteristic biological functions (such as translation), biased in amino acid composition, and are generally more abundant, more essential, and enriched for interaction partners. Many of these results are in agreement with recent studies. In addition, we assess information contribution of different subsets of these protein properties in the task of predicting slowly evolving proteins. We employ a logistic regression model on binned data that is able to account for intercorrelation, non-linearity, and heterogeneity within features. Our model considers features both individually and in natural ensembles (""meta-features"") in order to assess joint information contribution and degree of contribution independence. Meta-features based on protein abundance and amino acid composition make strong, partially independent contributions to the task of predicting slowly evolving proteins; other meta-features make additional minor contributions. The combination of all meta-features yields predictions comparable to those based on paired species comparisons, and approaching the predictive limit of optimal lineage-insensitive features. Our integrated assessment framework can be readily extended to other correlational analyses at the genome scale. Author Summary Proteins encoded within a given genome are known to evolve at drastically different rates. Through recent large-scale studies, researchers have measured a wide variety of properties for all proteins in yeast. We are interested to know how these properties relate to one another and to what extent they explain evolutionary rate variation. Protein properties are a heterogeneous mix, a factor which complicates research in this area. For example, some properties (e.g., protein abundance) are numerical, while others (e.g., protein function) are descriptive; protein properties may also suffer from noise and hidden redundancies. We have addressed these issues within a flexible and robust statistical framework. We first ranked a large list of protein properties by the strength of their relationships with evolutionary rate; this confirms many known evolutionary relationships and also highlights several new ones. Similar protein properties were then grouped and applied to predict slowly evolving proteins. Some of these groups were as effective as paired species comparison in making correct predictions, although in both cases a great deal of evolutionary rate variation remained to be explained. Our work has helped to refine the set of protein properties that researchers should consider as they investigate the mechanisms underlying protein evolution."
ERIC MARKS,"Chiasma: November 1969 v. 1, no. 1",
ERIC MARKS,"Chiasma: March 1970 v. 1, no. 2",
ERIC MARKS,GJ 1252 b: A 1.2 R ⊕ planet transiting an M3 dwarf at 20.4 pc,"We report the discovery of GJ 1252 b, a planet with a radius of 1.193 ± 0.074 R⊕ and an orbital period of 0.52 days around an M3-type star (0.381 ± 0.019 M⊙, 0.391 ± 0.020 R⊙) located 20.385 ± 0.019 pc away. We use TESS data, ground-based photometry and spectroscopy, Gaia astrometry, and high angular resolution imaging to show that the transit signal seen in the TESS data must originate from a transiting planet. We do so by ruling out all false positive scenarios that attempt to explain the transit signal as originating from an eclipsing stellar binary. Precise Doppler monitoring also leads to a tentative mass measurement of 2.09 ± 0.56 M⊕. The host star proximity, brightness (V = 12.19 mag, K = 7.92 mag), low stellar activity, and the system’s short orbital period make this planet an attractive target for detailed characterization, including precise mass measurement, looking for other objects in the system, and planet atmosphere characterization."
ERIC MARKS,Cancer classification and metastasis,
ERIC MARKS,Reproductive inequality in humans and other mammals,"To address claims of human exceptionalism, we determine where humans fit within the greater mammalian distribution of reproductive inequality. We show that humans exhibit lower reproductive skew (i.e., inequality in the number of surviving offspring) among males and smaller sex differences in reproductive skew than most other mammals, while nevertheless falling within the mammalian range. Additionally, female reproductive skew is higher in polygynous human populations than in polygynous nonhumans mammals on average. This patterning of skew can be attributed in part to the prevalence of monogamy in humans compared to the predominance of polygyny in nonhuman mammals, to the limited degree of polygyny in the human societies that practice it, and to the importance of unequally held rival resources to women's fitness. The muted reproductive inequality observed in humans appears to be linked to several unusual characteristics of our species-including high levels of cooperation among males, high dependence on unequally held rival resources, complementarities between maternal and paternal investment, as well as social and legal institutions that enforce monogamous norms."
ERIC MARKS,"The L 98-59 system: three transiting, terrestrial-size planets orbiting a nearby M dwarf","We report the Transiting Exoplanet Survey Satellite (TESS) discovery of three terrestrial-size planets transiting L 98-59 (TOI-175, TIC 307210830)—a bright M dwarf at a distance of 10.6 pc. Using the Gaia-measured distance and broadband photometry, we find that the host star is an M3 dwarf. Combined with the TESS transits from three sectors, the corresponding stellar parameters yield planet radii ranging from 0.8 R ⊕ to 1.6 R ⊕. All three planets have short orbital periods, ranging from 2.25 to 7.45 days with the outer pair just wide of a 2:1 period resonance. Diagnostic tests produced by the TESS Data Validation Report and the vetting package DAVE rule out common false-positive sources. These analyses, along with dedicated follow-up and the multiplicity of the system, lend confidence that the observed signals are caused by planets transiting L 98-59 and are not associated with other sources in the field. The L 98-59 system is interesting for a number of reasons: the host star is bright (V = 11.7 mag, K = 7.1 mag) and the planets are prime targets for further follow-up observations including precision radial-velocity mass measurements and future transit spectroscopy with the James Webb Space Telescope; the near-resonant configuration makes the system a laboratory to study planetary system dynamical evolution; and three planets of relatively similar size in the same system present an opportunity to study terrestrial planets where other variables (age, metallicity, etc.) can be held constant. L 98-59 will be observed in four more TESS sectors, which will provide a wealth of information on the three currently known planets and have the potential to reveal additional planets in the system."
ERIC MARKS,"Bostonia: 2000-2001, no. 1-4",
ERIC MARKS,"Bostonia: 2003-2004, no. 1-4",
ERIC MARKS,"Bostonia: 2002-2003, no.1-4",
ERIC MARKS,"Bostonia: 1998-1999, no. 1, 3-4",
ERIC MARKS,The first habitable-zone Earth-sized planet from TESS. II. Spitzer confirms TOI-700 d,"We present Spitzer 4.5 μm observations of the transit of TOI-700 d, a habitable-zone Earth-sized planet in a multiplanet system transiting a nearby M-dwarf star (TIC 150428135, 2MASS J06282325–6534456). TOI-700 d has a radius of 1.144_-0.061^+0.062R_⨁ and orbits within its host star's conservative habitable zone with a period of 37.42 days (T eq ~ 269 K). TOI-700 also hosts two small inner planets (R b = 1.037_-0.064^+0.065R_⨁ and R c = 2.65_-0.15^+0.16R_⨁) with periods of 9.98 and 16.05 days, respectively. Our Spitzer observations confirm the Transiting Exoplanet Survey Satellite (TESS) detection of TOI-700 d and remove any remaining doubt that it is a genuine planet. We analyze the Spitzer light curve combined with the 11 sectors of TESS observations and a transit of TOI-700 c from the LCOGT network to determine the full system parameters. Although studying the atmosphere of TOI-700 d is not likely feasible with upcoming facilities, it may be possible to measure the mass of TOI-700 d using state-of-the-art radial velocity (RV) instruments (expected RV semiamplitude of ~70 cm s^−1)."
ERIC MARKS,The revised TESS Input Catalog and candidate target list,"We describe the catalogs assembled and the algorithms used to populate the revised TESS Input Catalog (TIC), based on the incorporation of the Gaia second data release. We also describe a revised ranking system for prioritizing stars for 2 minute cadence observations, and we assemble a revised Candidate Target List (CTL) using that ranking. The TIC is available on the Mikulski Archive for Space Telescopes server, and an enhanced CTL is available through the Filtergraph data visualization portal system at http://filtergraph.vanderbilt.edu/tess_ctl."
ERIC MARKS,The long-baseline neutrino experiment: exploring fundamental symmetries of the universe,"The preponderance of matter over antimatter in the early Universe, the dynamics of the supernova bursts that produced the heavy elements necessary for life and whether protons eventually decay --- these mysteries at the forefront of particle physics and astrophysics are key to understanding the early evolution of our Universe, its current state and its eventual fate. The Long-Baseline Neutrino Experiment (LBNE) represents an extensively developed plan for a world-class experiment dedicated to addressing these questions. LBNE is conceived around three central components: (1) a new, high-intensity neutrino source generated from a megawatt-class proton accelerator at Fermi National Accelerator Laboratory, (2) a near neutrino detector just downstream of the source, and (3) a massive liquid argon time-projection chamber deployed as a far detector deep underground at the Sanford Underground Research Facility. This facility, located at the site of the former Homestake Mine in Lead, South Dakota, is approximately 1,300 km from the neutrino source at Fermilab -- a distance (baseline) that delivers optimal sensitivity to neutrino charge-parity symmetry violation and mass ordering effects. This ambitious yet cost-effective design incorporates scalability and flexibility and can accommodate a variety of upgrades and contributions. With its exceptional combination of experimental configuration, technical capabilities, and potential for transformative discoveries, LBNE promises to be a vital facility for the field of particle physics worldwide, providing physicists from around the globe with opportunities to collaborate in a twenty to thirty year program of exciting science. In this document we provide a comprehensive overview of LBNE's scientific objectives, its place in the landscape of neutrino physics worldwide, the technologies it will incorporate and the capabilities it will possess."
KAYHAN BATMANGHELICH,"Dividing and conquering a BlackBox to a mixture of interpretable models: route, interpret, repeat",
KAYHAN BATMANGHELICH,Adversarial consistency for single domain generalization in medical image segmentation,
KAYHAN BATMANGHELICH,Anatomy-guided weakly-supervised abnormality localization in chest X-rays,
MARK BUN,Approximate degree in classical and quantum computing,"In this book, the authors survey what is known about a particularly natural notion of approximation by polynomials, capturing pointwise approximation over the real numbers."
MARK BUN,Controlling privacy loss in sampling schemes: an analysis of stratified and cluster sampling,"Sampling schemes are fundamental tools in statistics, survey design, and algorithm design. A fundamental result in differential privacy is that a differentially private mechanism run on a simple random sample of a population provides stronger privacy guarantees than the same algorithm run on the entire population. However, in practice, sampling designs are often more complex than the simple, data-independent sampling schemes that are addressed in prior work. In this work, we extend the study of privacy amplification results to more complex, data-dependent sampling schemes. We find that not only do these sampling schemes often fail to amplify privacy, they can actually result in privacy degradation. We analyze the privacy implications of the pervasive cluster sampling and stratified sampling paradigms, as well as provide some insight into the study of more general sampling designs"
MARK BUN,Multiclass versus binary differentially private PAC learning,We show a generic reduction from multiclass differentially private PAC learning to binary private PAC learning. We apply this transformation to a recently proposed binary private PAC learner to obtain a private multiclass learner with sample complexity that has a polynomial dependence on the multiclass Littlestone dimension and a poly-logarithmic dependence on the number of classes. This yields a doubly exponential improvement in the dependence on both parameters over learners from previous work. Our proof extends the notion of 𝚿-dimension defined in work of Ben-David et al. [5] to the online setting and explores its general properties.
MARK BUN,Differentially private correlation clustering,"Correlation clustering is a widely used technique in unsupervised machine learning. Motivated by applications where individual privacy is a concern, we initiate the study of differentially private correlation clustering. We propose an algorithm that achieves subquadratic additive error compared to the optimal cost. In contrast, straightforward adaptations of existing non-private algorithms all lead to a trivial quadratic error. Finally, we give a lower bound showing that any pure differentially private algorithm for correlation clustering requires additive error of Ω (n)."
MARK BUN,Sign-rank can increase under intersection,
MARK BUN,Strong memory lower bounds for learning natural models,
MARK BUN,"Efficient, noise-tolerant, and private learning via boosting","We introduce a simple framework for designing private boosting algorithms. We give natural conditions under which these algorithms are differentially private, efficient, and noise-tolerant PAC learners. To demonstrate our framework, we use it to construct noise-tolerant and private PAC learners for large-margin halfspaces whose sample complexity does not depend on the dimension. We give two sample complexity bounds for our large-margin halfspace learner. One bound is based only on differential privacy, and uses this guarantee as an asset for ensuring generalization. This first bound illustrates a general methodology for obtaining PAC learners from privacy, which may be of independent interest. The second bound uses standard techniques from the theory of large-margin classification (the fat-shattering dimension) to match the best known sample complexity for differentially private learning of large-margin halfspaces, while additionally tolerating random label noise."
MARK BUN,New oracle-efficient algorithms for private synthetic data release,"We present three new algorithms for constructing differentially private synthetic data—a sanitized version of a sensitive dataset that approximately preserves the answers to a large collection of statistical queries. All three algorithms are oracle-efficient in the sense that they are computationally efficient when given access to an optimization oracle. Such an oracle can be implemented using many existing (non-private) optimization tools such as sophisticated integer program solvers. While the accuracy of the synthetic data is contingent on the oracle’s optimization performance, the algorithms satisfy differential privacy even in the worst case. For all three algorithms, we provide theoretical guarantees for both accuracy and privacy. Through empirical evaluation, we demonstrate that our methods scale well with both the dimensionality of the data and the number of queries. Compared to the state-of-the-art method High-Dimensional Matrix Mechanism McKenna et al. (2018), our algorithms provide better accuracy in the large workload and high privacy regime (corresponding to low privacy loss ε)."
MARK BUN,An equivalence between private classification and online prediction,
MARK BUN,Private hypothesis selection,"We provide a differentially private algorithm for hypothesis selection. Given samples from an unknown probability distribution P and a set of m probability distributions H, the goal is to output, in a ε-differentially private manner, a distribution from H whose total variation distance to P is comparable to that of the best such distribution (which we denote by α). The sample complexity of our basic algorithm is O(log m/α^2 + log m/αε), representing a minimal cost for privacy when compared to the non-private algorithm. We also can handle infinite hypothesis classes H by relaxing to (ε, δ)-differential privacy. We apply our hypothesis selection algorithm to give learning algorithms for a number of natural distribution classes, including Gaussians, product distributions, sums of independent random variables, piecewise polynomials, and mixture classes. Our hypothesis selection procedure allows us to generically convert a cover for a class to a learning algorithm, complementing known learning lower bounds which are in terms of the size of the packing number of the class. As the covering and packing numbers are often closely related, for constant α, our algorithms achieve the optimal sample complexity for many classes of interest. Finally, we describe an application to private distribution-free PAC learning."
MARK BUN,The large-error approximate degree of AC^0,
MARK BUN,Guest column,The approximate degree of a Boolean function f captures how well f can be approximated pointwise by low-degree polynomials. This article surveys what we know about approximate degree and illustrates some of its applications in theoretical computer science.
MARK BUN,When is memorization of irrelevant training data necessary for high-accuracy learning?,"Modern machine learning models are complex and frequently encode surprising amounts of information about individual inputs. In extreme cases, complex models appear to memorize entire input examples, including seemingly irrelevant information (social security numbers from text, for example). In this paper, we aim to understand whether this sort of memorization is necessary for accurate learning. We describe natural prediction problems in which every sufficiently accurate training algorithm must encode, in the prediction model, essentially all the information about a large subset of its training examples. This remains true even when the examples are high-dimensional and have entropy much higher than the sample size, and even when most of that information is ultimately irrelevant to the task at hand. Further, our results do not depend on the training algorithm or the class of models used for learning. Our problems are simple and fairly natural variants of the next-symbol prediction and the cluster labeling tasks. These tasks can be seen as abstractions of image- and text-related prediction problems. To establish our results, we reduce from a family of one-way communication problems for which we prove new information complexity lower bounds."
MARK BUN,A computational separation between private learning and online learning,"A recent line of work has shown a qualitative equivalence between differentially private PAC learning and online learning: A concept class is privately learnable if and only if it is online learnable with a finite mistake bound. However, both directions of this equivalence incur significant losses in both sample and computational efficiency. Studying a special case of this connection, Gonen, Hazan, and Moran (NeurIPS 2019) showed that uniform or highly sample-efficient pure-private learners can be time-efficiently compiled into online learners. We show that, assuming the existence of one-way functions, such an efficient conversion is impossible even for general pure-private learners with polynomial sample complexity. This resolves a question of Neel, Roth, and Wu (FOCS 2019)."
MARK BUN,Strong memory bounds for learning natural models,
MARK BUN,"Stability is stable: connections between replicability, privacy, and adaptive generalization",
MARK BUN,Not all learnable distribution classes are privately learnable,
MARK BUN,Private PAC learning may be harder than online learning,
MARK BUN,Continual release of differentially private synthetic data,
MARK BUN,Hypothesis selection with memory constraints,
MARK BUN,Approximate degree lower bounds for oracle identification problems,
RICHARD M SHERVA,Multiple Independent Loci at Chromosome 15q25.1 Affect Smoking Quantity: a Meta-Analysis and Comparison with Lung Cancer and COPD,"Recently, genetic association findings for nicotine dependence, smoking behavior, and smoking-related diseases converged to implicate the chromosome 15q25.1 region, which includes the CHRNA5-CHRNA3-CHRNB4 cholinergic nicotinic receptor subunit genes. In particular, association with the nonsynonymous CHRNA5 SNP rs16969968 and correlates has been replicated in several independent studies. Extensive genotyping of this region has suggested additional statistically distinct signals for nicotine dependence, tagged by rs578776 and rs588765. One goal of the Consortium for the Genetic Analysis of Smoking Phenotypes (CGASP) is to elucidate the associations among these markers and dichotomous smoking quantity (heavy versus light smoking), lung cancer, and chronic obstructive pulmonary disease (COPD). We performed a meta-analysis across 34 datasets of European-ancestry subjects, including 38,617 smokers who were assessed for cigarettes-per-day, 7,700 lung cancer cases and 5,914 lung-cancer-free controls (all smokers), and 2,614 COPD cases and 3,568 COPD-free controls (all smokers). We demonstrate statistically independent associations of rs16969968 and rs588765 with smoking (mutually adjusted p-values<10−35 and >10−8 respectively). Because the risk alleles at these loci are negatively correlated, their association with smoking is stronger in the joint model than when each SNP is analyzed alone. Rs578776 also demonstrates association with smoking after adjustment for rs16969968 (p<10−6). In models adjusting for cigarettes-per-day, we confirm the association between rs16969968 and lung cancer (p<10−20) and observe a nominally significant association with COPD (p = 0.01); the other loci are not significantly associated with either lung cancer or COPD after adjusting for rs16969968. This study provides strong evidence that multiple statistically distinct loci in this region affect smoking behavior. This study is also the first report of association between rs588765 (and correlates) and smoking that achieves genome-wide significance; these SNPs have previously been associated with mRNA levels of CHRNA5 in brain and lung tissue. Author Summary Nicotine binds to cholinergic nicotinic receptors, which are composed of a variety of subunits. Genetic studies for smoking behavior and smoking-related diseases have implicated a genomic region that encodes the alpha5, alpha3, and beta4 subunits. We examined genetic data across this region for over 38,000 smokers, a subset of which had been assessed for lung cancer or chronic obstructive pulmonary disease. We demonstrate strong evidence that there are at least two statistically independent loci in this region that affect risk for heavy smoking. One of these loci represents a change in the protein structure of the alpha5 subunit. This work is also the first to report strong evidence of association between smoking and a group of genetic variants that are of biological interest because of their links to expression of the alpha5 cholinergic nicotinic receptor subunit gene. These advances in understanding the genetic influences on smoking behavior are important because of the profound public health burdens caused by smoking and nicotine addiction."
DAVID PIMENTEL,An oxindole efflux inhibitor potentiates azoles and impairs virulence in the fungal pathogen Candida auris,"Candida auris is an emerging fungal pathogen that exhibits resistance to multiple drugs, including the most commonly prescribed antifungal, fluconazole. Here, we use a combinatorial screening approach to identify a bis-benzodioxolylindolinone (azoffluxin) that synergizes with fluconazole against C. auris. Azoffluxin enhances fluconazole activity through the inhibition of efflux pump Cdr1, thus increasing intracellular fluconazole levels. This activity is conserved across most C. auris clades, with the exception of clade III. Azoffluxin also inhibits efflux in highly azole-resistant strains of Candida albicans, another human fungal pathogen, increasing their susceptibility to fluconazole. Furthermore, azoffluxin enhances fluconazole activity in mice infected with C. auris, reducing fungal burden. Our findings suggest that pharmacologically targeting Cdr1 in combination with azoles may be an effective strategy to control infection caused by azole-resistant isolates of C. auris."
DAVID PIMENTEL,Scintillation light detection in the 6-m drift-length ProtoDUNE Dual Phase liquid argon TPC,"DUNE is a dual-site experiment for long-baseline neutrino oscillation studies, neutrino astrophysics and nucleon decay searches. ProtoDUNE Dual Phase (DP) is a 6  ×  6  ×  6 m 3 liquid argon time-projection-chamber (LArTPC) that recorded cosmic-muon data at the CERN Neutrino Platform in 2019-2020 as a prototype of the DUNE Far Detector. Charged particles propagating through the LArTPC produce ionization and scintillation light. The scintillation light signal in these detectors can provide the trigger for non-beam events. In addition, it adds precise timing capabilities and improves the calorimetry measurements. In ProtoDUNE-DP, scintillation and electroluminescence light produced by cosmic muons in the LArTPC is collected by photomultiplier tubes placed up to 7 m away from the ionizing track. In this paper, the ProtoDUNE-DP photon detection system performance is evaluated with a particular focus on the different wavelength shifters, such as PEN and TPB, and the use of Xe-doped LAr, considering its future use in giant LArTPCs. The scintillation light production and propagation processes are analyzed and a comparison of simulation to data is performed, improving understanding of the liquid argon properties."
DAVID PIMENTEL,Identification and reconstruction of low-energy electrons in the ProtoDUNE-SP detector,
LUIS BALLESTEROS,Black swans and the social value of corporate disaster giving,
LUIS BALLESTEROS,Masters of disasters? An empirical analysis of how societies benefit from corporate disaster aid,"Corporations have become increasingly influential within societies around the world, while the relative capacity of national governments to meet large social needs has waned. Consequentially, firms are being asked to adopt responsibilities that have traditionally fallen to governments, aid agencies, and other types of organizations. There are questions, though, about whether or not this is beneficial for society. We study this in the context of disaster relief and recovery; an area where companies account for a growing share of aid as compared to traditional providers. Drawing on the dynamic capabilities literature, we argue that firms are better-equipped than other types of organizations to sense areas of need following a disaster, seize response opportunities, and reconfigure resources for fast, effective relief efforts. As such, we predict that—while traditional aid providers are important for disaster recovery—relief will arrive faster, and nations will recover more fully when locally active firms account for a larger share of disaster aid. We test our predictions with a proprietary dataset comprising information on every natural disaster and reported aid donation worldwide from 2003 to 2013. Our analysis uses a novel, quasi-experimental technique known as the synthetic control method and shows that nations benefit greatly from corporate involvement when disaster strikes."
LUIS BALLESTEROS,The internal allocation of rights within the firm and the creation of competitive advantages,
LUIS BALLESTEROS,Post-disaster fluctuations in innovation: evidence from Hurricane Katrina,"Theoretically motivated by evidence that large exogenous shocks can produce enduring fluctuations in economic behavior, this study shows that counties affected by Hurricane Katrina exhibit substantial post-disaster increases in the quantity and quality of patenting compared to unaffected counterfactual counties. The significance of the matched-sample difference-in-differences estimates rises in damage severity, persists up to 10 years, and is robust to factors such as a post-disaster economic rebound, changes in the demand for specific innovation, aid and investment, wealth, education, and traditional explanations of innovative activity, such as firm R&D, institutions and market factors, but enhanced by inventor density and collaboration. Using georeferenced histories of inventors, the analyses trace the “Katrina effect” and control for selective migration and company affiliation. The findings point to an understudied driver of the geography of innovation."
LUIS BALLESTEROS,Is labor productivity more sensitive to corporate philanthropy towards welfare shocks or chronic conditions?,"Do increases in labor productivity that follow from corporate philanthropy depend on the societal causes to which firms donate? Integrating insights from psychological research showing that individuals respond more charitably towards beneficiaries who experience a welfare shock (e.g., those afflicted by disasters) than beneficiaries in a chronic state of low welfare (e.g., those living in poverty), we develop and test the argument that employees exert more effort at work when their firm’s philanthropy targets welfare loss than when philanthropy targets chronic conditions. Using longitudinal data on corporate philanthropy from large U.S. companies, we present identification strategies that consistently support our argument. Our estimates suggest that, on average, a 6.63 percent greater increase in marginal labor productivity occurs when companies donate towards welfare loss after sudden shocks—such as epidemics, natural disasters, and terrorist attacks—vis-à-vis donations to chronic conditions like poverty and homelessness. This correlation survives accounting for a vector of joint fixed effects and time-varying controls as well as a battery of robustness checks. The findings suggest that the targets of philanthropic donations are important for the ways in which corporate giving acts as a non-pecuniary incentive."
LUIS BALLESTEROS,Institutional disruptions and the philanthropy of multinational firms,"This paper studies philanthropy by multinational enterprises (MNEs) during institutional disruptions—the sudden and unexpected, temporary, and systemic breakdowns in market-oriented institutions. The central argument is that, under institutional disruptions, MNEs aim to restore factors that are essential for the market to function, such as infrastructure and labor markets, and the strength of the market restoration motive is positively associated with the economic importance of the affected country to the MNE. Analyses of donations from 2,000 MNEs headquartered in 63 countries in the aftermath of 265 major epidemics, natural disasters, and terrorist attacks affecting 129 countries suggest that the economic importance of the country to the firm strongly explains donations. Country market concentration, public aid, and the country’s regulatory quality moderate this effect. These associations are robust to a matching method; a vector of firm-, country-, and event-specific time-varying and -constant variables; and alternative motives, such as reputation, altruism, media salience, market standing, and poverty-gap avoidance. They offer evidence that company philanthropy in the aftermath of institutional disruptions may deviate from predicted behavior under stable conditions. Particularly, the findings contest the expectation that philanthropy rises in market competition. Monopolistic firms are comparatively large donors and may act as an economic stop-loss mechanism during large disruptions."
LUIS BALLESTEROS,Firms economic reliance to national markets and the corporate provision of public goods,"When firms decide to engage in the provision of collective goods that benefit social welfare (i.e., to behave prosocially), they may consider the economic relevance of such goods for their own market operation. The bigger the stake of the firm in a given market, the greater its reliance on the market’s collective goods (e.g., communication networks, transportation infrastructure). Therefore, a market’s relative importance for a firm should be a significant predictor of corporate pro-social behavior—an association that is not explained by theories on social preferences or strategic considerations. I test this argument by constructing a measure of corporate economic reliance on market systems based on the literature on club goods and analyzing data on corporations’ philanthropic responses to 3,115 natural disasters between 2003 and 2013, inclusive. I show that accounting for variation in economic reliance leads to a more accurate prediction of the frequency and magnitude of corporate pro-social behavior than widely invoked arguments rooted in the strategic philanthropy and institutional literatures, which neglect such firm-market connection."
LUIS BALLESTEROS,How the uncertainty associated with social issues influences the financial returns of corporate philanthropy,"This study examines whether the varying financial returns to philanthropy can be explained by the uncertainty associated with the issues to which a firm donates. We start with the premise that stakeholders react favorably to donations they view as effective and appropriate for specific social needs, which can lead to financial advantages for the donor firm. However, the reliance on various cues for such assessments may differ based on the uncertainty surrounding social issues. For stable issues, where the social need and redress strategies are relatively clear and direct, we expect that proximate cues such as the donation amount and a firm’s donation experience are likely indicators of philanthropic effectiveness, thereby predicting its financial returns. Conversely, when donations target uncertain issues where the social need is unclear or evolving, these cues become less informative, prompting stakeholders to consider broader cues, such as firm reputation. Our analysis introduces a method for measuring the country- and time-specific uncertainty of issues and applies it to evaluate donations from the world’s largest 2,000 firms from 2007 to 2018. The significance of our study is underscored by the increasing engagement of firms in social issues fraught with high uncertainty."
LUIS BALLESTEROS,The influence of MNE’s local reputation on the financial rents from responding to large disasters,"I explore the financial implications of multinational enterprises’ (MNEs) philanthropic responses to international disasters. While MNEs have been the fastest-growing sector in disaster relief philanthropy, there is ambiguity surrounding the financial consequences of their donations. Using a firm’s country reputation as a theoretical lens, I posit that stakeholders are more influenced by a company’s pre-existing reputation than the actual donation amount when its social value is ambiguous. Employing a staggered difference-in-differences approach on data from 2005 to 2019, I find that initial donors with favorable country reputations experience unexpected gains compared to initial donors with an unfavorable reputation. Additionally, subsequent firms matching the donations of initial donors with good reputations also observed positive revenue effects. However, the impact is less clear for subsequent donors that diverged in donation amounts. The findings underscore the significance of MNE reputation in influencing stakeholder perceptions and consequent financial outcomes in disaster philanthropy. Furthermore, they challenge the prevailing notion that swift, large donations invariably lead to positive financial results, offering insights for more strategic philanthropic engagements in volatile contexts."
LUIS BALLESTEROS,Unleashing innovation in the wake of disaster: patenting increases post-Hurricane Katrina,"Theoretically motivated by evidence that large exogenous shocks can produce enduring fluctuations in economic behavior, this study shows that counties affected by Hurricane Katrina exhibit substantial post-disaster increases in the quantity and quality of patenting compared to unaffected counterfactual counties. The significance of the matched-sample difference-in-differences estimates rises in damage severity, persists up to 10 years, and is robust to factors such as a post-disaster economic rebound, changes in the demand for specific innovation, aid and investment, wealth, education, and traditional explanations of innovative activity, such as firm R&D, institutions, and market factors, but enhanced by inventor density and collaboration. Using georeferenced histories of inventors, the analyses trace the “Katrina effect” and control for selective migration and company affiliation. The findings point to an understudied environmental driver of innovation."
LUIS BALLESTEROS,Organizational decision making under uncertainty shocks,"In line with the fallacy of riskification of uncertainty by which decision makers believe that the effects of unpredictable phenomena can be captured accurately by probability distributions, organizational scholars commonly treat the organizational inefficiency in dealing with uncertainty shocks—exogenous hazards whose welfare effects spread across industries and markets, such as natural disasters, terrorist attacks, and financial crises—as a problem of risk management. This is problematic because the consequences of uncertainty shocks outstrip the predictability capacity for the average manager and entail a greater complexity of internal and external factors. Moreover, their uniqueness makes translating experience into learning far more difficult. We seek to address this inadequate approach with a theoretical framework that captures the multidimensional complexity of organizations preparing for, coping with, and recovering from exogenous uncertain disruption. We bring together the literatures on cognitive psychology that suggest that biases and heuristics drive behavior under uncertainty, a Neo-Carnegie perspective that indicates that organizational structure and strategy regulate these behavioral factors, and institutional theory that points to stakeholder and institutional dynamics affecting economic incentives to invest in prevention and business continuity. Taken together, this article offers the foundation for a behaviorally plausible, decision-centered perspective on organizational decision-making under uncertainty."
MICHAEL KAYE,A systematic search of Zwicky Transient Facility data for ultracompact binary LISA-detectable gravitational-wave sources,"Using photometry collected with the Zwicky Transient Facility, we are conducting an ongoing survey for binary systems with short orbital periods (P_b < 1 hr) with the goal of identifying new gravitational-wave sources detectable by the upcoming Laser Interferometer Space Antenna (LISA). We present a sample of 15 binary systems discovered thus far, with orbital periods ranging from 6.91 to 56.35 minutes. Of the 15 systems, seven are eclipsing systems that do not show signs of significant mass transfer. Additionally, we have discovered two AM Canum Venaticorum systems and six systems exhibiting primarily ellipsoidal variations in their lightcurves. We present follow-up spectroscopy and high-speed photometry confirming the nature of these systems, estimates of their LISA signal-to-noise ratios, and a discussion of their physical characteristics."
MICHAEL KAYE,A new class of large-amplitude radial-mode hot subdwarf pulsators,"Using high-cadence observations from the Zwicky Transient Facility at low Galactic latitudes, we have discovered a new class of pulsating, hot compact stars. We have found four candidates, exhibiting blue colors (g − r ≤ −0.1 mag), pulsation amplitudes of >5%, and pulsation periods of 200–475 s. Fourier transforms of the light curves show only one dominant frequency. Phase-resolved spectroscopy for three objects reveals significant radial velocity, T eff, and log(g) variations over the pulsation cycle, which are consistent with large-amplitude radial oscillations. The mean T eff and log(g) for these stars are consistent with hot subdwarf B (sdB) effective temperatures and surface gravities. We calculate evolutionary tracks using MESA and adiabatic pulsations using GYRE for low-mass, helium-core pre-white dwarfs (pre-WDs) and low-mass helium-burning stars. Comparison of low-order radial oscillation mode periods with the observed pulsation periods show better agreement with the pre-WD models. Therefore, we suggest that these new pulsators and blue large-amplitude pulsators (BLAPs) could be members of the same class of pulsators, composed of young ≈0.25–0.35 M ⊙ helium-core pre-WDs."
REBECCA BOURGAULT,"Thinking in, through, and with art: challenges, discoveries, and knowledge construction in graduate arts-based research","This panel presentation composed of instructors will discuss the design and experience of the collaborative facilitation of an arts-based research capstone course created in 2018 for an online graduate program in Art Education. Guided by the question,” in what ways were our students invited to explore, embed, and integrate the arts in their research?” we will discuss how the course design functioned as a creative map that provided direction yet allowed a flexible structure for student guidance. We will elaborate on the artistic and epistemological challenges students encountered in contextualizing their practice within a broader research sphere. As they worked, they aimed at maintaining “internal consistency and coherence that represent[ed] a strong and seamless relationship between purpose and method” (Cole & Knowles, 2008. p.67). Examples of students’ artistic research will illustrate the concepts presented."
REBECCA BOURGAULT,"A pedagogy of presence: attending to context, process, being, and belonging",
REBECCA BOURGAULT,The shapeshifting and boundary crossings of socially engaged art,"Socially-engaged art practices are understood to borrow from several disciplinary territories where they coexist and cross over into contexts that, in the process of engaging in civic work and quotidian actions, occlude their identity as art and aesthetic practices. The article examines the complications arising from these overlapping ontologies through a socially engaged project where the author acts and performs as an artist-scholar-facilitator, adopting, alongside the participants, multiple identities that are dependent on changing perspectives and conditions. Arguing for a different ethical orientation to research, the inquiry into this community practice further interrogates the wrangle between the expectations that symbolic capital is accrued by artists engaged in these practices and the invisible agency of quiet activism that offers potent alternative forms of resistance."
REBECCA BOURGAULT,"Across the Bridge: A story of community, sociality, and art education","The article examines the planning, development, and outcome of an experiential learning project that brought together undergraduate studio art students and the workers of a power plant about to shut down. As one of the instructors for the project, I reflect on how our emergent pedagogical methods interfaced or conflicted with students interests, and plant employees. Principles of phenomenological research inspired my early steps to the study. However, its operative conceptual framework follows the thoughts of socially engaged artists Suzanne Lacy (2010) and Pablo Helguera (2011), guiding an analysis of the relationships between students and workers with instructors as observer-participants. I investigate how these roles and relations developed through different modalities that ranged from familial sentiments to memorializing impulses, including the industrial conditions that inspired various sensual and aesthetic student responses. I argue that the production of artwork as autonomous objects, which constituted the self-evident outcome of this community-focused experience, contributed only a transactional materiality to the project, and that the relational exchanges from which transformative experiences originated, offered unrivaled creative possibilities."
REBECCA BOURGAULT,"Stories of community practice, artistic ambivalence, and emergent pedagogies","The reflections and questions discussed in the paper emerged from a teaching artist experience in community-art that led to the examination of the contrasting values between the disciplinary paradigms of social practices, community-based and participatory arts, and that of the contemporary art world aesthetics. As goals of art for social justice often contradict the perception of artistic merit based on aesthetic quality, working at the intersection of artistic creation and community development demands a shift in perspectives. The position demands going beyond one’s artistic ambivalences, to include participants in a reciprocal relationship, attentive to the fact that any goals of empowerment inherently conceal a power structure. Models of interaction borrowed from prefigurative pedagogies, pedagogies of contingencies inspire the elaboration of a pedagogy of presence that allows for the unfolding of a process anchored in integrity, quiet activism, and the heuristic purpose of art."
REBECCA BOURGAULT,Presence: the search for wisdom in a socially engaged art education project,"A socially engaged art pedagogical and relational experiment brings to light the necessity to trust the unfolding of personal and communal change as elusive yet credible value. Social practices of art, including art for social justice often situate the goals of their artistic project in qualities of relational exchanges. This paper reviews an experiment led through a pedagogy of presence for an open art studio at a homeless shelter for women. Narrated through the structure of Ground, Path, and Fruition, a Shambala conceptualization of life’s change, the paper is written from a personal, and philosophical storytelling approach. It situates the pedagogy of presence in the art studio as a shared method of discovery, experienced differently by every participant. Part social art practice inflected with quiet activism, part meditation, and borrowing from theories of adult learning, the qualities of presence at the open studio offered a centring counterpoise to the precarious living situation experienced by participants, enhancing human connections through shared artmaking, listening, and a sense of social belonging."
HECTOR MARQUEZ,Generation of a purified iPSC-derived smooth muscle-like population for cell sheet engineering,"Induced pluripotent stem cells (iPSCs) provide a potential source for the derivation of smooth muscle cells (SMCs); however, current approaches are limited by the production of heterogeneous cell types and a paucity of tools or markers for tracking and purifying candidate SMCs. Here, we develop murine and human iPSC lines carrying fluorochrome reporters (Acta2hrGFP and ACTA2eGFP, respectively) that identify Acta2+/ACTA2+ cells as they emerge in vitro in real time during iPSC-directed differentiation. We find that Acta2hrGFP+ and ACTA2eGFP+ cells can be sorted to purity and are enriched in markers characteristic of an immature or synthetic SMC. We characterize the resulting GFP+ populations through global transcriptomic profiling and functional studies, including the capacity to form engineered cell sheets. We conclude that these reporter lines allow for generation of sortable, live iPSC-derived Acta2+/ACTA2+ cells highly enriched in smooth muscle lineages for basic developmental studies, tissue engineering, or future clinical regenerative applications."
JENNIFER LEE,"Multichannel presence, boon or curse?: A comparison in price, loyalty, regret, and disappointment","We question the conventional notion that multichannel presence is always advantageous. The study shows inconsistent pricing in on- and offline stores results in consumers’ regret and disappointment, which in turn influence their post-purchase behaviors. We focus on how complex emotional responses in varying price/loyalty contexts may adversely affect the retailer."
JENNIFER LEE,Auxin and tryptophan homeostasis are facilitated by the ISS1/VAS1 aromatic aminotransferase in arabidopsis,"Indole-3-acetic acid (IAA) plays a critical role in regulating numerous aspects of plant growth and development. While there is much genetic support for tryptophan-dependent (Trp-D) IAA synthesis pathways, there is little genetic evidence for tryptophan-independent (Trp-I) IAA synthesis pathways. Using Arabidopsis, we identified two mutant alleles of ISS1 ( I: ndole S: evere S: ensitive) that display indole-dependent IAA overproduction phenotypes including leaf epinasty and adventitious rooting. Stable isotope labeling showed that iss1, but not WT, uses primarily Trp-I IAA synthesis when grown on indole-supplemented medium. In contrast, both iss1 and WT use primarily Trp-D IAA synthesis when grown on unsupplemented medium. iss1 seedlings produce 8-fold higher levels of IAA when grown on indole and surprisingly have a 174-fold increase in Trp. These findings indicate that the iss1 mutant's increase in Trp-I IAA synthesis is due to a loss of Trp catabolism. ISS1 was identified as At1g80360, a predicted aromatic aminotransferase, and in vitro and in vivo analysis confirmed this activity. At1g80360 was previously shown to primarily carry out the conversion of indole-3-pyruvic acid to Trp as an IAA homeostatic mechanism in young seedlings. Our results suggest that in addition to this activity, in more mature plants ISS1 has a role in Trp catabolism and possibly in the metabolism of other aromatic amino acids. We postulate that this loss of Trp catabolism impacts the use of Trp-D and/or Trp-I IAA synthesis pathways."
JENNIFER LEE,Clinicopathological evaluation of chronic traumatic encephalopathy in players of American football,"IMPORTANCE: Players of American football may be at increased risk of long-term neurological conditions, particularly chronic traumatic encephalopathy (CTE). OBJECTIVE: To determine the neuropathological and clinical features of deceased football players with CTE. DESIGN, SETTING, AND PARTICIPANTS: Case series of 202 football players whose brains were donated for research. Neuropathological evaluations and retrospective telephone clinical assessments (including head trauma history) with informants were performed blinded. Online questionnaires ascertained athletic and military history. EXPOSURES: Participation in American football at any level of play. MAIN OUTCOMES AND MEASURES: Neuropathological diagnoses of neurodegenerative diseases, including CTE, based on defined diagnostic criteria; CTE neuropathological severity (stages I to IV or dichotomized into mild [stages I and II] and severe [stages III and IV]); informant-reported athletic history and, for players who died in 2014 or later, clinical presentation, including behavior, mood, and cognitive symptoms and dementia. RESULTS: Among 202 deceased former football players (median age at death, 66 years [interquartile range, 47-76 years]), CTE was neuropathologically diagnosed in 177 players (87%; median age at death, 67 years [interquartile range, 52-77 years]; mean years of football participation, 15.1 [SD, 5.2]), including 0 of 2 pre–high school, 3 of 14 high school (21%), 48 of 53 college (91%), 9 of 14 semiprofessional (64%), 7 of 8 Canadian Football League (88%), and 110 of 111 National Football League (99%) players. Neuropathological severity of CTE was distributed across the highest level of play, with all 3 former high school players having mild pathology and the majority of former college (27 [56%]), semiprofessional (5 [56%]), and professional (101 [86%]) players having severe pathology. Among 27 participants with mild CTE pathology, 26 (96%) had behavioral or mood symptoms or both, 23 (85%) had cognitive symptoms, and 9 (33%) had signs of dementia. Among 84 participants with severe CTE pathology, 75 (89%) had behavioral or mood symptoms or both, 80 (95%) had cognitive symptoms, and 71 (85%) had signs of dementia. CONCLUSIONS AND RELEVANCE: In a convenience sample of deceased football players who donated their brains for research, a high proportion had neuropathological evidence of CTE, suggesting that CTE may be related to prior participation in football."
JENNIFER LEE,How do consumers choose offline shops on online platforms? An investigation of interactive consumer decision processing in diagnosis-and-cure markets,"PURPOSE: The purpose of this paper is twofold: (1) to understand the process and consequences of the two-way communication between consumers and businesses on online-to-offline (O2O) diagnosis-and-cure services platforms and (2) to examine how consumer request-specific factors and service quote-specific factors influence consumer decisions in the interactive marketing context. DESIGN/METHODOLOGY/APPROACH: The study analyzes a dataset of 17,878 service requests and 57,867 price quotes obtained from an O2O platform bridging consumers and automotive repair shops. On the platform, consumers request service quotes by uploading the description of automotive damage and multiple service providers suggest price quotes. The authors formulated a logit model to examine consumer decisions of responding service quotes. FINDINGS: This paper finds that (1) consumers receiving more severe diagnostic results are more likely to respond to the price quotes, and (2) diagnostic severity and inconsistency moderate the impacts of geographic distance, shop size, and quote price on consumers' responses to the service quotes. RESEARCH LIMITATIONS/IMPLICATIONS: This paper fills the gap in the literature by advancing the consumer decision processing model to address the interactive shopping experience on O2O diagnosis-and-cure services platforms. The findings are limited by the data and the research context. PRACTICAL IMPLICATIONS: For marketing practitioners, the empirical results imply specific positioning and targeting strategies for markets with informational and geographic barriers to expand the market scope and customer base. ORIGINALITY/VALUE: The present work is the first to examine the consumer decision process on O2O diagnosis-and-cure service platforms. It adds value to the literature by investigating how consumers update their problem awareness through the service request-specific factors (i.e. diagnostic severity and diagnostic inconsistency) and how the request-specific factors moderate the impacts of the quote-specific factors (i.e. shop distance, shop size and quote price) on consumers' responses to price quote. The conceptual model and empirical findings provide theoretical and practical values for e-commerce researchers and practitioners."
JENNIFER LEE,Characterizing double-back stutter in low to multi-copy number regimes in forensically relevant STR loci,"Modern DNA analysis is possible due to the discovery of repeating microsatellite regions in DNA and successful implementation of the polymerase chain reaction (PCR) in laboratories. PCR amplification chemistries that contain short tandem repeat (STR) loci are sensitive. As a result, the discrimination power within human identification sciences has increased in recent years. Despite these advances, cellular admixtures are commonly collected, and the resultant “DNA mixture profile” is difficult to interpret as it is often encumbered by low-signals and allele drop-out. Regularly detected PCR artifacts can further complicate interpretation. One commonly encountered artifact is stutter, the result of strand slippage during PCR. Stutter can be of two types: forward and reverse. Reverse stutter (or back stutter) is the most prevalent and is one repeat unit shorter (n - 1) than the template strand. In contrast, forward stutter is one repeat unit longer (n + 1). If a reverse stutter amplicon is produced there is the distinct possibility that a stutter product of stutter may occur. This artifact is usually referred to as double-back stutter (DBS) or n - 2 stutter. Recently there has been renewed interest in examining signal approaching baseline levels. As the sensitivity of the process improves, so does the probability of detecting DBS. Therefore, studies that examine the peak height distributions, rarity, stutter signal-to-noise distances and the general impact of DBS on the signal are warranted. Models simulating PCR, and the entire forensic DNA process, have been created by this laboratory. The work presented herein builds upon a preexisting model; specifically, the dynamic model was extended such that DNA profiles consisting of 21 autosomal STRs, consistent with the GlobalFilerTM multiplex, are simulated. Furthermore, this expansion incorporated a three-type Galton-Watson branching process allowing DBS to be added to the simulated electropherogram (EPG). The in silico model was used to simulate the amplification of a 1:43 and 1:73 mixture at a total DNA concentration of 0.3 and 0.5 ng, respectively. We chose these extreme mixture ratios because the signal from these minor contributors would be most susceptible to DBS effects from the major contributor. A total of 1200 alleles from each contributor were simulated at each target, and effects of DBS on the signal from the minor contributor were characterized. At 0.3 and 0.5 ng both the noise and stutter signal histograms are right-skewed and a Kolmogorov-Smirnov (KS) test indicates that the noise and DBS were significantly different (p-value < 4x10-6). The average peak height of DBS for all loci in both scenarios were less than 50 RFU (Relative Fluorescence Units), and the DBS ratios ranged from 0.29 to 2.15% of the main allele, with the median ratios less than 0.5%. A per locus analytical threshold (AT) was calculated for both the 0.3 and 0.5 ng targets using two k-values: 3 and 4. The k-value is chosen based on the Type I risk assessment, wherein increasing the k-value increases AT. The percentage of DBS peaks greater than AT when k = 3 for the mixtures amplified at 0.3 and 0.5 ng ranged from 0 to 7.08% and 0 to 10.50%, respectively. Interestingly, when k = 4 the percentage of DBS peaks greater than AT for 0.3 and 0.5 ng reduced to 0 to 1.08% and 0 to 0.17%, respectively. This suggests that modeling DBS in continuous systems may not be necessary if the laboratory continues to rely on a system that requires an AT of sufficient strength. However, with the advent of Bayesian or machine learning-based approaches to analyzing EPGs, thus removing AT in its entirety, a complete understanding of the prevalence of DBS is necessary. This work shows that DBS from an extreme major using our laboratory protocols is not likely to be in the same signal regime as the signal from alleles; however, it does show that signal from DBS is significantly different from noise. Therefore, the software expert pair should be carefully considered during the validation stage and laboratories should consider DBS during interpretation, especially if enhanced post-PCR parameters are implemented into the forensic laboratory process."
JENNIFER LEE,Factors associated with lifestyle counseling provided to patients with prehypertension,"Prehypertension is an increasingly common diagnosis in this country and is a predictor of a future hypertension diagnosis. Current guidelines recommend lifestyle modifications for prehypertension treatment. This study attempts to evaluate the provider, physician, and patient-related factors that affect whether physicians provide lifestyle counseling to patients with prehypertension. This study is a cross-sectional, retrospective cohort study using the 2007–2010 datasets from the National Ambulatory Medical Care Survey. The analysis sample included 2,804 patient visit records of prehypertensive patients. The outcome variable is whether any lifestyle counseling that included smoking cessation, dietary change, and/or weight loss was provided. A logistic regression model was constructed to assess the effects that patient, physician and provider characteristics had on the outcome variable. Out of the total analysis sample of 2,804, 30% of the patients received at least one form of lifestyle counseling. Patient factors that were statistically significant included: having diabetes (odds ratio (OR)=2.32, 95% confidence interval (CI), 1.70–3.17), being over-weight (OR=1.64, 95% CI: 1.25 – 2.15), being identified as smokers (OR=1.65, 95% CI: 1.19–2.29). Significant provider characteristics included solo practices (OR=0.67 95% CI: 0.50–0.90); having electronic health record system with a patient problem list feature (OR=1.46, 95% CI: 1.08–1.98), electronic reminders of clinical guidelines (OR=1.42, 95% CI: 1.03–1.96), and medical/surgical physician specialty (OR=0.67, 95% CI: 0.45–0.99). Among the financial factors, only the percent revenue from Medicaid was significant, with the 26–75% Medicaid revenue having an OR of 0.45 (95% CI: 0.29–0.71) compared with the 0–25% reference level."
JENNIFER LEE,Autism screening and diagnosis in low resource settings: Challenges and opportunities to enhance research and services worldwide.,"Most research into the epidemiology, etiology, clinical manifestations, diagnosis and treatment of autism is based on studies in high income countries. Moreover, within high income countries, individuals of high socioeconomic status are disproportionately represented among participants in autism research. Corresponding disparities in access to autism screening, diagnosis, and treatment exist globally. One of the barriers perpetuating this imbalance is the high cost of proprietary tools for diagnosing autism and for delivering evidence-based therapies. Another barrier is the high cost of training of professionals and para-professionals to use the tools. Open-source and open access models provide a way to facilitate global collaboration and training. Using these models and technologies, the autism scientific community and clinicians worldwide should be able to work more effectively and efficiently than they have to date to address the global imbalance in autism knowledge and at the same time advance our understanding of autism and our ability to deliver cost-effective services to everyone in need."
JENNIFER LEE,Sevoflurane induces coherent slow-delta oscillations in rats,"Although general anesthetics are routinely administered to surgical patients to induce loss of consciousness, the mechanisms underlying anesthetic-induced unconsciousness are not fully understood. In rats, we characterized changes in the extradural EEG and intracranial local field potentials (LFPs) within the prefrontal cortex (PFC), parietal cortex (PC), and central thalamus (CT) in response to progressively higher doses of the inhaled anesthetic sevoflurane. During induction with a low dose of sevoflurane, beta/low gamma (12-40 Hz) power increased in the frontal EEG and PFC, PC and CT LFPs, and PFC-CT and PFC-PFC LFP beta/low gamma coherence increased. Loss of movement (LOM) coincided with an abrupt decrease in beta/low gamma PFC-CT LFP coherence. Following LOM, cortically coherent slow-delta (0.1-4 Hz) oscillations were observed in the frontal EEG and PFC, PC and CT LFPs. At higher doses of sevoflurane sufficient to induce loss of the righting reflex, coherent slow-delta oscillations were dominant in the frontal EEG and PFC, PC and CT LFPs. Dynamics similar to those observed during induction were observed as animals emerged from sevoflurane anesthesia. We conclude that the rat is a useful animal model for sevoflurane-induced EEG oscillations in humans, and that coherent slow-delta oscillations are a correlate of sevoflurane-induced behavioral arrest and loss of righting in rats."
JENNIFER LEE,Racial/ethnic disparities in type 2 diabetes remission after bariatric surgery,"BACKGROUND: Previous studies have shown that there are racial disparities in type 2 diabetes (T2DM) remission following bariatric surgery, with African-Americans (AA) in particular experiencing a subsequent relapse. In recent years, some have attributed these findings to racial differences in fasting insulin levels, with AA having higher levels, as increasing evidence for an alternate model of T2DM pathophysiology gains support. In this model, basal hyperinsulinemia is considered a primary event in T2DM disease development, rather than a compensatory response to increased insulin resistance. This study aimed to compare glycemic outcomes after bariatric surgery in different races, namely African-Americans (AA), Hispanic-Americans (HA), and Caucasian-Americans (CA), and to determine whether there were any associated changes in insulin levels and insulin resistance that may lend support to this revised model of T2DM pathophysiology. METHODS: A retrospective medical record review of 1,326 patients (389 AA, 179 HA, and 758 CA) who underwent bariatric surgery at Boston Medical Center (BMC) from 2004 to 2015 was conducted. Baseline characteristics and maximum percent weight loss were compared using one-way ANOVA and Chi-square tests of independence. Changes in mean glycated hemoglobin (HbA1c), insulin levels, insulin resistance (HOMA-IR), and blood glucose levels were analyzed using linear mixed models, overall and by racial group. The same procedures were conducted in both the overall patient population and a T2DM subpopulation. RESULTS: Over an 11-year postoperative observation period, all racial groups underwent a significant decrease in HbA1c (P<0.001) within the first two years following surgery. While HbA1c levels remained stable in CA and HA, they began to rise at 2 years in AA only (P=0.043). Additionally, analyses of covariates, including age at surgery (P=0.005), initial BMI (P<0.001), and maximum weight loss (P=0.049), revealed that all three were significant factors affecting mean HbA1c levels. However, when included in the mixed model, the race x time interaction effect on mean HbA1c remained significant. There was also a significant overall decrease in both insulin and HOMA-IR. When stratified by race, analysis of the T2DM population showed that insulin levels began to increase again by the 2nd year after surgery in AA, while in CA and HA they continued to decrease and subsequently stabilize. Analysis of the total patient population showed that HOMA-IR levels in AA, as well as in CA and HA, continued to decrease at this 2-year time point. Decreases in blood glucose levels after surgery were significant overall (P<0.001), but not significant when stratified by race. CONCLUSIONS: After the initial “metabolic reset” that occurs within the first 2 years after bariatric surgery, during which HbA1c levels normalize in the vast majority of patients, it was observed only in the AA population that there was a steady increase in HbA1c to levels near those recorded at baseline. This coincided with an observation of increasing insulin levels despite decreasing insulin resistance seen in AA only. Our results suggest that current discussions regarding a revised model of T2DM pathophysiology, in which hyperinsulinemia precedes insulin resistance, may help explain the racial disparities in glycemic control observed in both post-surgical and non-surgical contexts of T2DM outcome. However, future prospective studies are needed to further the preliminary results of this study."
JENNIFER LEE,Cooperative Stimulation of Dendritic Cells by Cryptococcus neoformans Mannoproteins and CpG Oligodeoxynucleotides,"While mannosylation targets antigens to mannose receptors on dendritic cells (DC), the resultant immune response is suboptimal. We hypothesized that the addition of toll-like receptor (TLR) ligands would enhance the DC response to mannosylated antigens. Cryptococcus neoformans mannoproteins (MP) synergized with CpG-containing oligodeoxynucleotides to stimulate enhanced production of proinflammatory cytokines and chemokines from murine conventional and plasmacytoid DC. Synergistic stimulation required the interaction of mannose residues on MP with the macrophage mannose receptor (MR), CD206. Moreover, synergy with MP was observed with other TLR ligands, including tripalmitoylated lipopeptide (Pam3CSK4), polyinosine-polycytidylic acid (pI:C), and imiquimod. Finally, CpG enhanced MP-specific MHC II-restricted CD4+ T-cell responses by a mechanism dependent upon DC expression of CD206 and TLR9. These data suggest a rationale for vaccination strategies that combine mannosylated antigens with TLR ligands and imply that immune responses to naturally mannosylated antigens on pathogens may be greatly augmented if TLR and MR are cooperatively stimulated."
JENNIFER LEE,Triple-Negative Breast Cancers are Increased in Black Women Regardless of Age or Body Mass Index,"INTRODUCTION. We investigated clinical and pathologic features of breast cancers (BC) in an unselected series of patients diagnosed in a tertiary care hospital serving a diverse population. We focused on triple-negative (Tneg) tumours (oestrogen receptor (ER), progesterone receptor (PR) and HER2 negative), which are associated with poor prognosis. METHODS. We identified female patients with invasive BC diagnosed between 1998 and 2006, with data available on tumor grade, stage, ER, PR and HER2 status, and patient age, body mass index (BMI) and self-identified racial/ethnic group. We determined associations between patient and tumour characteristics using contingency tables and multivariate logistic regression. RESULTS. 415 cases were identified. Patients were racially and ethnically diverse (born in 44 countries, 36% white, 43% black, 10% Hispanic and 11% other). 47% were obese (BMI > 30 kg/m2). 72% of tumours were ER+ and/or PR+, 20% were Tneg and 13% were HER2+. The odds of having a Tneg tumour were 3-fold higher (95% CI 1.6, 5.5; p = 0.0001) in black compared with white women. Tneg tumours were equally common in black women diagnosed before and after age 50 (31% vs 29%; p = NS), and who were obese and non-obese (29% vs 31%; p = NS). Considering all patients, as BMI increased, the proportion of Tneg tumours decreased (p = 0.08). CONCLUSIONS. Black women of diverse background have 3-fold more Tneg tumours than non-black women, regardless of age and BMI. Other factors must determine tumour subtype. The higher prevalence of Tneg tumours in black women in all age and weight categories likely contributes to black women's unfavorable breast cancer prognosis."
JENNIFER LEE,Consumer behavior on an online-to-offline platform: an empirical investigation of the automotive repair service market,"This study uses a unique dataset from O2O platform business in the automotive repair services. The platform is a mobile-based service where consumers upload the information about their auto damage such as photos and written description, and the repair shops respond to such requests informing the price quote, location of the store, written comments, and contact information. Consumers can also to initiate communications with the business before the store visit through the mobile application or via phone calls. The study serves three objectives. First, to investigate how the platform helps consumers receive services customized to their situations, we test the effect of consumer’s automotive damage severity on their likeliness to contact the provider through the platform. We also test how physical distance to the shop affects consumer behavior and how its impact is moderated by consumer-specific situations. Lastly, as the open platform enables transparent and rapid delivery of information to consumers, we examine the effects of quote price and the responsive delivery of service quotes, i.e., quote timeliness. Below are sample hypotheses. H3: The effect of repair shop distance on consumers’ probability of responding to a service quote weakens as the severity of damage increases. H4: Consumers are more likely to respond to a service quote with high price competitiveness (which has a lower price compared to the average of all quotes). H7: The effect of quote timeliness on consumers’ probability of responding to a service quote weakens as the severity of damage increases. Since our data include a binary outcome (i.e., responded to a quote or not), we formulate consumer behaviors using a logit model (e.g., Guadagni and Little 1983; Malhotra 1984; Train 2009). We applied maximum likelihood estimation method to estimate the parameters in our logit model. u_ij=β_0+β_1 DemageSeverity_i+β_2 ShopDistance_ij+β_3 PriceDiff_ij + β_4 TimeElapsed_ij +β_5 NoPrevQuotes_ij+β_6 ShopClass_ij +〖 β〗_7 ShopDistance_ij∙DemageSeverity_i+β_8 PriceDiff_ij∙DemageSeverity_i + β_9 TimeElapsed_ij∙DamageSeverity_i +〖 ε〗_ij. The research provides contributions to the academy and the practice. We fill the gap in literature by discussing the fundamental characteristics of businesses adopting both O2O and open platform models. We also take the first step to empirically test the impact of various types of store and consumer-specific information on consumer decisions, advancing the understanding of digital consumers. It is found that consumers’ high involvement in the purchase situation motivates them to proactively utilize the digital platform to contact the service providers. Also, they preferred the timely quotes regardless of the involvement level, which implies that the digital consumers have tendency to make quick choices under all situations."
JENNIFER LEE,New era signals and customer review platforms: conceptual and empirical analysis,"Thanks to the development of the customer review platforms, which now exist for nearly all service categories, customers have access to more objective product information free from firms’ self-interest. The emerging market environment calls for further academic examination of how customers look for peer-provided signals when facing difficulty in evaluating product/service quality. Extending the signaling literature (Kirmani and Rao, 2000; Akerlof 1970; Saboo and Grewal 2013), the objectives of the present work are a) to introduce a novel concept of customer-provided New era signals as solution for quality evaluation problems (specifically, adverse selection and moral hazard), b) to empirically test a model of how different types of the New era signals (expert advisor vs. referent advisor) are sought by customers when facing such problems, and finally c) to examine how customers’ latent needs (cognitive knowledge vs. social trust) emerge in those circumstances. Based on the novel concept of New era signals, we conduct empirical studies to examine how customers look for New era signals (expert leader vs. referent leader) under different types of uncertainty condition based on their perceived risk (adverse selection vs. moral hazard). We conducted two experiments and adopted ANOVA for both studies. Study 1 was designed to evaluate the participants’ need for objective knowledge and trust based on the type of information asymmetry problem they are focusing on (adverse selection vs. moral hazard). The results find support for our hypotheses indicating that when focusing on adverse selection, participants are more likely to feel the need for objective knowledge whereas when focusing on moral hazard, participants are more likely to feel the need for trust. Study 2 was designed to evaluate the participants’ need for objective knowledge and trust as well as their likelihood to seek New era signals with high expert power and referent power based on the type of information asymmetry problem they are focusing on (adverse selection vs. moral hazard). The results find support for our hypotheses indicating that when focusing on adverse selection, participants are more likely to feel the need for objective knowledge and more likely to seek New era signals with high expert power whereas when focusing on moral hazard, participants are more likely to feel the need for trust. The participants’ likelihood of seeking New era signals with high referent power was not supported."
JENNIFER LEE,"Design, Conduct and Challenges of a Clinical Trial Utilizing Elastic Light Scattering Spectroscopy in the Thyroid",
JENNIFER LEE,The eighteenth data release of the Sloan Digital Sky Surveys: targeting and first spectra from SDSS-V,"The eighteenth data release (DR18) of the Sloan Digital Sky Survey (SDSS) is the first one for SDSS-V, the fifth generation of the survey. SDSS-V comprises three primary scientific programs or “Mappers”: the Milky Way Mapper (MWM), the Black Hole Mapper (BHM), and the Local Volume Mapper. This data release contains extensive targeting information for the two multiobject spectroscopy programs (MWM and BHM), including input catalogs and selection functions for their numerous scientific objectives. We describe the production of the targeting databases and their calibration and scientifically focused components. DR18 also includes ∼25,000 new SDSS spectra and supplemental information for X-ray sources identified by eROSITA in its eFEDS field. We present updates to some of the SDSS software pipelines and preview changes anticipated for DR19. We also describe three value-added catalogs (VACs) based on SDSS-IV data that have been published since DR17, and one VAC based on the SDSS-V data in the eFEDS field."
JENNIFER LEE,The long-baseline neutrino experiment: exploring fundamental symmetries of the universe,"The preponderance of matter over antimatter in the early Universe, the dynamics of the supernova bursts that produced the heavy elements necessary for life and whether protons eventually decay --- these mysteries at the forefront of particle physics and astrophysics are key to understanding the early evolution of our Universe, its current state and its eventual fate. The Long-Baseline Neutrino Experiment (LBNE) represents an extensively developed plan for a world-class experiment dedicated to addressing these questions. LBNE is conceived around three central components: (1) a new, high-intensity neutrino source generated from a megawatt-class proton accelerator at Fermi National Accelerator Laboratory, (2) a near neutrino detector just downstream of the source, and (3) a massive liquid argon time-projection chamber deployed as a far detector deep underground at the Sanford Underground Research Facility. This facility, located at the site of the former Homestake Mine in Lead, South Dakota, is approximately 1,300 km from the neutrino source at Fermilab -- a distance (baseline) that delivers optimal sensitivity to neutrino charge-parity symmetry violation and mass ordering effects. This ambitious yet cost-effective design incorporates scalability and flexibility and can accommodate a variety of upgrades and contributions. With its exceptional combination of experimental configuration, technical capabilities, and potential for transformative discoveries, LBNE promises to be a vital facility for the field of particle physics worldwide, providing physicists from around the globe with opportunities to collaborate in a twenty to thirty year program of exciting science. In this document we provide a comprehensive overview of LBNE's scientific objectives, its place in the landscape of neutrino physics worldwide, the technologies it will incorporate and the capabilities it will possess."
JONATHAN S JAY,"Bostonia: v. 61, no. 1-2, 4",
JONATHAN S JAY,"First Sagittarius A* Event Horizon Telescope results. II. EHT and multiwavelength observations, data processing, and calibration","We present Event Horizon Telescope (EHT) 1.3 mm measurements of the radio source located at the position of the supermassive black hole Sagittarius A* (Sgr A*), collected during the 2017 April 5–11 campaign. The observations were carried out with eight facilities at six locations across the globe. Novel calibration methods are employed to account for Sgr A*'s flux variability. The majority of the 1.3 mm emission arises from horizon scales, where intrinsic structural source variability is detected on timescales of minutes to hours. The effects of interstellar scattering on the image and its variability are found to be subdominant to intrinsic source structure. The calibrated visibility amplitudes, particularly the locations of the visibility minima, are broadly consistent with a blurred ring with a diameter of ∼50 μas, as determined in later works in this series. Contemporaneous multiwavelength monitoring of Sgr A* was performed at 22, 43, and 86 GHz and at near-infrared and X-ray wavelengths. Several X-ray flares from Sgr A* are detected by Chandra, one at low significance jointly with Swift on 2017 April 7 and the other at higher significance jointly with NuSTAR on 2017 April 11. The brighter April 11 flare is not observed simultaneously by the EHT but is followed by a significant increase in millimeter flux variability immediately after the X-ray outburst, indicating a likely connection in the emission physics near the event horizon. We compare Sgr A*’s broadband flux during the EHT campaign to its historical spectral energy distribution and find that both the quiescent emission and flare emission are consistent with its long-term behavior."
JONATHAN S JAY,Event Horizon Telescope imaging of the archetypal blazar 3C 279 at an extreme 20 microarcsecond resolution,"3C 279 is an archetypal blazar with a prominent radio jet that show broadband flux density variability across the entire electromagnetic spectrum. We use an ultra-high angular resolution technique – global Very Long Baseline Interferometry (VLBI) at 1.3 mm (230 GHz) – to resolve the innermost jet of 3C 279 in order to study its fine-scale morphology close to the jet base where highly variable γ-ray emission is thought to originate, according to various models. The source was observed during four days in April 2017 with the Event Horizon Telescope at 230 GHz, including the phased Atacama Large Millimeter/submillimeter Array (ALMA), at an angular resolution of ∼20 μas (at a redshift of z = 0.536 this corresponds to ∼0.13 pc  ∼ 1700 Schwarzschild radii with a black hole mass M<jats:sub>BH</jats:sub> = 8 × 10^8 M_⊙). Imaging and model-fitting techniques were applied to the data to parameterize the fine-scale source structure and its variation. We find a multicomponent inner jet morphology with the northernmost component elongated perpendicular to the direction of the jet, as imaged at longer wavelengths. The elongated nuclear structure is consistent on all four observing days and across different imaging methods and model-fitting techniques, and therefore appears robust. Owing to its compactness and brightness, we associate the northern nuclear structure as the VLBI “core”. This morphology can be interpreted as either a broad resolved jet base or a spatially bent jet. We also find significant day-to-day variations in the closure phases, which appear most pronounced on the triangles with the longest baselines. Our analysis shows that this variation is related to a systematic change of the source structure. Two inner jet components move non-radially at apparent speeds of ∼15 c and ∼20 c (∼1.3 and ∼1.7 μas day^−1, respectively), which more strongly supports the scenario of traveling shocks or instabilities in a bent, possibly rotating jet. The observed apparent speeds are also coincident with the 3C 279 large-scale jet kinematics observed at longer (cm) wavelengths, suggesting no significant jet acceleration between the 1.3 mm core and the outer jet. The intrinsic brightness temperature of the jet components are ≲10^10 K, a magnitude or more lower than typical values seen at ≥7 mm wavelengths. The low brightness temperature and morphological complexity suggest that the core region of 3C 279 becomes optically thin at short (mm) wavelengths."
JONATHAN S JAY,First M87 Event Horizon Telescope results. II. Array and instrumentation,"The Event Horizon Telescope (EHT) is a very long baseline interferometry (VLBI) array that comprises millimeter- and submillimeter-wavelength telescopes separated by distances comparable to the diameter of the Earth. At a nominal operating wavelength of ~1.3 mm, EHT angular resolution (λ/D) is ~25 μas, which is sufficient to resolve nearby supermassive black hole candidates on spatial and temporal scales that correspond to their event horizons. With this capability, the EHT scientific goals are to probe general relativistic effects in the strong-field regime and to study accretion and relativistic jet formation near the black hole boundary. In this Letter we describe the system design of the EHT, detail the technology and instrumentation that enable observations, and provide measures of its performance. Meeting the EHT science objectives has required several key developments that have facilitated the robust extension of the VLBI technique to EHT observing wavelengths and the production of instrumentation that can be deployed on a heterogeneous array of existing telescopes and facilities. To meet sensitivity requirements, high-bandwidth digital systems were developed that process data at rates of 64 gigabit s−1, exceeding those of currently operating cm-wavelength VLBI arrays by more than an order of magnitude. Associated improvements include the development of phasing systems at array facilities, new receiver installation at several sites, and the deployment of hydrogen maser frequency standards to ensure coherent data capture across the array. These efforts led to the coordination and execution of the first Global EHT observations in 2017 April, and to event-horizon-scale imaging of the supermassive black hole candidate in M87."
JONATHAN S JAY,First M87 Event Horizon Telescope results. I. The shadow of the supermassive black hole,"When surrounded by a transparent emission region, black holes are expected to reveal a dark shadow caused by gravitational light bending and photon capture at the event horizon. To image and study this phenomenon, we have assembled the Event Horizon Telescope, a global very long baseline interferometry array observing at a wavelength of 1.3 mm. This allows us to reconstruct event-horizon-scale images of the supermassive black hole candidate in the center of the giant elliptical galaxy M87. We have resolved the central compact radio source as an asymmetric bright emission ring with a diameter of 42 ± 3 μas, which is circular and encompasses a central depression in brightness with a flux ratio gsim10:1. The emission ring is recovered using different calibration and imaging schemes, with its diameter and width remaining stable over four different observations carried out in different days. Overall, the observed image is consistent with expectations for the shadow of a Kerr black hole as predicted by general relativity. The asymmetry in brightness in the ring can be explained in terms of relativistic beaming of the emission from a plasma rotating close to the speed of light around a black hole. We compare our images to an extensive library of ray-traced general-relativistic magnetohydrodynamic simulations of black holes and derive a central mass of M = (6.5 ± 0.7) × 10^9 M ⊙. Our radio-wave observations thus provide powerful evidence for the presence of supermassive black holes in centers of galaxies and as the central engines of active galactic nuclei. They also present a new tool to explore gravity in its most extreme limit and on a mass scale that was so far not accessible."
JONATHAN S JAY,First Sagittarius A* Event Horizon Telescope results. I. The shadow of the supermassive black hole in the center of the Milky Way,"We present the first Event Horizon Telescope (EHT) observations of Sagittarius A* (Sgr A*), the Galactic center source associated with a supermassive black hole. These observations were conducted in 2017 using a global interferometric array of eight telescopes operating at a wavelength of λ = 1.3 mm. The EHT data resolve a compact emission region with intrahour variability. A variety of imaging and modeling analyses all support an image that is dominated by a bright, thick ring with a diameter of 51.8 ± 2.3 μas (68% credible interval). The ring has modest azimuthal brightness asymmetry and a comparatively dim interior. Using a large suite of numerical simulations, we demonstrate that the EHT images of Sgr A* are consistent with the expected appearance of a Kerr black hole with mass ∼4 × 106 M ⊙, which is inferred to exist at this location based on previous infrared observations of individual stellar orbits, as well as maser proper-motion studies. Our model comparisons disfavor scenarios where the black hole is viewed at high inclination (i &gt; 50°), as well as nonspinning black holes and those with retrograde accretion disks. Our results provide direct evidence for the presence of a supermassive black hole at the center of the Milky Way, and for the first time we connect the predictions from dynamical measurements of stellar orbits on scales of 103–105 gravitational radii to event-horizon-scale images and variability. Furthermore, a comparison with the EHT results for the supermassive black hole M87* shows consistency with the predictions of general relativity spanning over three orders of magnitude in central mass."
JONATHAN S JAY,Curbing the epidemic of community firearm violence after the Bruen decision,"The Supreme Court’s decision in New York State Rifle & Pistol Association Inc. v. Bruen undermines the ability of cities and states to regulate firearms safety. Nonetheless, we remain hopeful that firearm violence can decline even after the Bruen decision. Several promising public health approaches have gained broader adoption in recent years. This essay examines the key drivers of community firearm violence and reviews promising strategies to reverse those conditions, including community violence intervention (CVI) programs and place-based and structural interventions."
JONATHAN S JAY,The long-baseline neutrino experiment: exploring fundamental symmetries of the universe,"The preponderance of matter over antimatter in the early Universe, the dynamics of the supernova bursts that produced the heavy elements necessary for life and whether protons eventually decay --- these mysteries at the forefront of particle physics and astrophysics are key to understanding the early evolution of our Universe, its current state and its eventual fate. The Long-Baseline Neutrino Experiment (LBNE) represents an extensively developed plan for a world-class experiment dedicated to addressing these questions. LBNE is conceived around three central components: (1) a new, high-intensity neutrino source generated from a megawatt-class proton accelerator at Fermi National Accelerator Laboratory, (2) a near neutrino detector just downstream of the source, and (3) a massive liquid argon time-projection chamber deployed as a far detector deep underground at the Sanford Underground Research Facility. This facility, located at the site of the former Homestake Mine in Lead, South Dakota, is approximately 1,300 km from the neutrino source at Fermilab -- a distance (baseline) that delivers optimal sensitivity to neutrino charge-parity symmetry violation and mass ordering effects. This ambitious yet cost-effective design incorporates scalability and flexibility and can accommodate a variety of upgrades and contributions. With its exceptional combination of experimental configuration, technical capabilities, and potential for transformative discoveries, LBNE promises to be a vital facility for the field of particle physics worldwide, providing physicists from around the globe with opportunities to collaborate in a twenty to thirty year program of exciting science. In this document we provide a comprehensive overview of LBNE's scientific objectives, its place in the landscape of neutrino physics worldwide, the technologies it will incorporate and the capabilities it will possess."
ADAM THOMAS LABADORF,Widespread perturbation of ETS factor binding sites in cancer,"Although >90% of somatic mutations reside in non-coding regions, few have been reported as cancer drivers. To predict driver non-coding variants (NCVs), we present a transcription factor (TF)-aware burden test based on a model of coherent TF function in promoters. We apply this test to NCVs from the Pan-Cancer Analysis of Whole Genomes cohort and predict 2555 driver NCVs in the promoters of 813 genes across 20 cancer types. These genes are enriched in cancer-related gene ontologies, essential genes, and genes associated with cancer prognosis. We find that 765 candidate driver NCVs alter transcriptional activity, 510 lead to differential binding of TF-cofactor regulatory complexes, and that they primarily impact the binding of ETS factors. Finally, we show that different NCVs within a promoter often affect transcriptional activity through shared mechanisms. Our integrated computational and experimental approach shows that cancer NCVs are widespread and that ETS factors are commonly disrupted."
YI JI,Deep spectral learning for label-free optical imaging oximetry with uncertainty quantification,"Measurement of blood oxygen saturation (sO2) by optical imaging oximetry provides invaluable insight into local tissue functions and metabolism. Despite different embodiments and modalities, all label-free optical-imaging oximetry techniques utilize the same principle of sO2-dependent spectral contrast from haemoglobin. Traditional approaches for quantifying sO2 often rely on analytical models that are fitted by the spectral measurements. These approaches in practice suffer from uncertainties due to biological variability, tissue geometry, light scattering, systemic spectral bias, and variations in the experimental conditions. Here, we propose a new data-driven approach, termed deep spectral learning (DSL), to achieve oximetry that is highly robust to experimental variations and, more importantly, able to provide uncertainty quantification for each sO2 prediction. To demonstrate the robustness and generalizability of DSL, we analyse data from two visible light optical coherence tomography (vis-OCT) setups across two separate in vivo experiments on rat retinas. Predictions made by DSL are highly adaptive to experimental variabilities as well as the depth-dependent backscattering spectra. Two neural-network-based models are tested and compared with the traditional least-squares fitting (LSF) method. The DSL-predicted sO2 shows significantly lower mean-square errors than those of the LSF. For the first time, we have demonstrated en face maps of retinal oximetry along with a pixel-wise confidence assessment. Our DSL overcomes several limitations of traditional approaches and provides a more flexible, robust, and reliable deep learning approach for in vivo non-invasive label-free optical oximetry."
YI JI,First born model for reflection-mode Fourier ptychographic microscopy,We validate a first Born approximation based model for Reflection-mode Fourier ptychography under the semi-infinite boundary condition. Our model enables optical thickness and absorption recovery with enhanced resolution from thin samples.
YI JI,Inverse scattering for reflection intensity phase microscopy,"Reflection phase imaging provides label-free, high-resolution characterization of biological samples, typically using interferometric-based techniques. Here, we investigate reflection phase microscopy from intensity-only measurements under diverse illumination. We evaluate the forward and inverse scattering model based on the first Born approximation for imaging scattering objects above a glass slide. Under this design, the measured field combines linear forward-scattering and height-dependent nonlinear back-scattering from the object that complicates object phase recovery. Using only the forward-scattering, we derive a linear inverse scattering model and evaluate this model's validity range in simulation and experiment using a standard reflection microscope modified with a programmable light source. Our method provides enhanced contrast of thin, weakly scattering samples that complement transmission techniques. This model provides a promising development for creating simplified intensity-based reflection quantitative phase imaging systems easily adoptable for biological research."
YI JI,"Multiplexed intensity diffraction tomography (mIDT) for dynamic, label-free volumetric biological imaging",We present multiplexed Intensity Diffraction Tomography (mIDT) for live biological sample imaging. We achieve 100X imaging speed improvements using model-based multiplexed LED illuminations that maintain high-quality 3D object reconstructions.
YI JI,Nanosecond-resolution photothermal dynamic imaging via MHZ digitization and match filtering,"Photothermal microscopy has enabled highly sensitive label-free imaging of absorbers, from metallic nanoparticles to chemical bonds. Photothermal signals are conventionally detected via modulation of excitation beam and demodulation of probe beam using lock-in amplifier. While convenient, the wealth of thermal dynamics is not revealed. Here, we present a lock-in free, mid-infrared photothermal dynamic imaging (PDI) system by MHz digitization and match filtering at harmonics of modulation frequency. Thermal-dynamic information is acquired at nanosecond resolution within single pulse excitation. Our method not only increases the imaging speed by two orders of magnitude but also obtains four-fold enhancement of signal-to-noise ratio over lock-in counterpart, enabling high-throughput metabolism analysis at single-cell level. Moreover, by harnessing the thermal decay difference between water and biomolecules, water background is effectively separated in mid-infrared PDI of living cells. This ability to nondestructively probe chemically specific photothermal dynamics offers a valuable tool to characterize biological and material specimens."
YI JI,"Missing voices: examining how misinformation-susceptible individuals from underrepresented communities engage, perceive, and combat science misinformation","This study examines how misinformation-susceptible individuals from historically excluded and marginalized communities engage with science topics (e.g., climate change, vaccines, and health/wellness) and interpret misinformation and corrective intervention strategies. Two focus groups reveal that most participants are highly distrustful of authority figures, celebrity endorsements, and fact-checking strategies to combat misinformation. As one of the first studies to explore underrepresented community members’ experiences with science misinformation, findings reveal structural and institutional power dynamics that impede access to accurate information and indicate how missing voices must be included in the efforts at media and information literacy initiatives."
YI JI,The eighteenth data release of the Sloan Digital Sky Surveys: targeting and first spectra from SDSS-V,"The eighteenth data release (DR18) of the Sloan Digital Sky Survey (SDSS) is the first one for SDSS-V, the fifth generation of the survey. SDSS-V comprises three primary scientific programs or “Mappers”: the Milky Way Mapper (MWM), the Black Hole Mapper (BHM), and the Local Volume Mapper. This data release contains extensive targeting information for the two multiobject spectroscopy programs (MWM and BHM), including input catalogs and selection functions for their numerous scientific objectives. We describe the production of the targeting databases and their calibration and scientifically focused components. DR18 also includes ∼25,000 new SDSS spectra and supplemental information for X-ray sources identified by eROSITA in its eFEDS field. We present updates to some of the SDSS software pipelines and preview changes anticipated for DR19. We also describe three value-added catalogs (VACs) based on SDSS-IV data that have been published since DR17, and one VAC based on the SDSS-V data in the eFEDS field."
YI JI,Scintillation light detection in the 6-m drift-length ProtoDUNE Dual Phase liquid argon TPC,"DUNE is a dual-site experiment for long-baseline neutrino oscillation studies, neutrino astrophysics and nucleon decay searches. ProtoDUNE Dual Phase (DP) is a 6  ×  6  ×  6 m 3 liquid argon time-projection-chamber (LArTPC) that recorded cosmic-muon data at the CERN Neutrino Platform in 2019-2020 as a prototype of the DUNE Far Detector. Charged particles propagating through the LArTPC produce ionization and scintillation light. The scintillation light signal in these detectors can provide the trigger for non-beam events. In addition, it adds precise timing capabilities and improves the calorimetry measurements. In ProtoDUNE-DP, scintillation and electroluminescence light produced by cosmic muons in the LArTPC is collected by photomultiplier tubes placed up to 7 m away from the ionizing track. In this paper, the ProtoDUNE-DP photon detection system performance is evaluated with a particular focus on the different wavelength shifters, such as PEN and TPB, and the use of Xe-doped LAr, considering its future use in giant LArTPCs. The scintillation light production and propagation processes are analyzed and a comparison of simulation to data is performed, improving understanding of the liquid argon properties."
YI JI,Prospects for beyond the standard model physics searches at the deep underground neutrino experiment: DUNE collaboration,"The Deep Underground Neutrino Experiment (DUNE) will be a powerful tool for a variety of physics topics. The high-intensity proton beams provide a large neutrino flux, sampled by a near detector system consisting of a combination of capable precision detectors, and by the massive far detector system located deep underground. This configuration sets up DUNE as a machine for discovery, as it enables opportunities not only to perform precision neutrino measurements that may uncover deviations from the present three-flavor mixing paradigm, but also to discover new particles and unveil new interactions and symmetries beyond those predicted in the Standard Model (SM). Of the many potential beyond the Standard Model (BSM) topics DUNE will probe, this paper presents a selection of studies quantifying DUNE's sensitivities to sterile neutrino mixing, heavy neutral leptons, non-standard interactions, CPT symmetry violation, Lorentz invariance violation, neutrino trident production, dark matter from both beam induced and cosmogenic sources, baryon number violation, and other new physics topics that complement those at high-energy colliders and significantly extend the present reach."
YI JI,Identification and reconstruction of low-energy electrons in the ProtoDUNE-SP detector,
MARIA KAMENETSKA,Local Defects in colloidal quantum dot thin films measured via spatially resolved multi-modal optoelectronic spectroscopy.,"The morphology, chemical composition, and electronic uniformity of thin-film solution-processed optoelectronics are believed to greatly affect device performance. Although scanning probe microscopies can address variations on the micrometer scale, the field of view is still limited to well under the typical device area, as well as the size of extrinsic defects introduced during fabrication. Herein, a micrometer-resolution 2D characterization method with millimeter-scale field of view is demonstrated, which simultaneously collects photoluminescence spectra, photocurrent transients, and photovoltage transients. This high-resolution morphology mapping is used to quantify the distribution and strength of the local optoelectronic property variations in colloidal quantum dot solar cells due to film defects, physical damage, and contaminants across nearly the entire test device area, and the extent to which these variations account for overall performance losses. It is found that macroscopic defects have effects that are confined to their localized areas, rarely prove fatal for device performance, and are largely not responsible for device shunting. Moreover, quantitative analysis based on statistical partitioning methods of such data is used to show how defect identification can be automated while identifying variations in underlying properties such as mobilities and recombination strengths and the mechanisms by which they govern device behavior."
MARIA KAMENETSKA,Formation and evolution of metallocene single-molecule circuits with direct gold-π links,"Single-molecule circuits with group 8 metallocenes are formed without additional linker groups in scanning tunneling microscope-based break junction (STMBJ) measurements at cryogenic and room-temperature conditions with gold (Au) electrodes. We investigate the nature of this direct gold-π binding motif and its effect on molecular conductance and persistence characteristics during junction evolution. The measurement technique under cryogenic conditions tracks molecular plateaus through the full cycle of extension and compression. Analysis reveals that junction persistence when the metal electrodes are pushed together correlates with whether electrodes are locally sharp or blunt, suggesting distinct scenarios for metallocene junction formation and evolution. The top and bottom surfaces of the “barrel”-shaped metallocenes present the electron-rich π system of cyclopentadienyl rings, which interacts with the gold electrodes in two distinct ways. An undercoordinated gold atom on a sharp tip forms a donor–acceptor bond to a specific carbon atom in the ring. However, a small, flat patch on a dull tip can bind more strongly to the ring as a whole through van der Waals interactions. Density functional theory (DFT)-based calculations of model electrode structures provide an atomic-scale picture of these scenarios, demonstrating the role of these bonding motifs during junction evolution and showing that the conductance is relatively independent of tip atomic-scale structure. The nonspecific interaction of the cyclopentadienyl rings with the electrodes enables extended conductance plateaus, a mechanism distinct from that identified for the more commonly studied, rod-shaped organic molecular wires."
MARIA KAMENETSKA,Atomically precise binding conformations of adenine and its variants on gold using single molecule conductance signatures,"We demonstrate single molecule conductance as a sensitive and atomically precise probe of binding configurations of adenine and its biologically relevant variants on gold. By combining experimental measurements and density functional theory (DFT) calculations of single molecule–metal junction structures in aqueous conditions, we determine for the first time that robust binding of adenine occurs in neutral or basic pH when the molecule is deprotonated at the imidazole moiety. The molecule binds through the donation of the electron lone pairs from the imidazole nitrogen atoms, N7 and N9, to the gold electrodes. In addition, the pyrimidine ring nitrogen, N3, can bind concurrently and strengthen the overall metal–molecule interaction. The amine does not participate in binding to gold in contrast to most other amine-terminated molecular wires due to the planar geometry of the nucleobase. DFT calculations reveal the importance of interface charge transfer in stabilizing the experimentally observed binding configurations. We demonstrate that biologically relevant variants of adenine, 6-methyladenine and 2′-deoxyadenosine, have distinct conductance signatures. These results lay the foundation for biosensing on gold using single molecule conductance readout."
BRIAN WILLIAMS,Species interactions differ in their genetic robustness,"Conflict and cooperation between bacterial species drive the composition and function of microbial communities. Stability of these emergent properties will be influenced by the degree to which species' interactions are robust to genetic perturbations. We use genome-scale metabolic modeling to computationally analyze the impact of genetic changes when Escherichia coli and Salmonella enterica compete, or cooperate. We systematically knocked out in silico each reaction in the metabolic network of E. coli to construct all 2583 mutant stoichiometric models. Then, using a recently developed multi-scale computational framework, we simulated the growth of each mutant E. coli in the presence of S. enterica. The type of interaction between species was set by modulating the initial metabolites present in the environment. We found that the community was most robust to genetic perturbations when the organisms were cooperating. Species ratios were more stable in the cooperative community, and community biomass had equal variance in the two contexts. Additionally, the number of mutations that have a substantial effect is lower when the species cooperate than when they are competing. In contrast, when mutations were added to the S. enterica network the system was more robust when the bacteria were competing. These results highlight the utility of connecting metabolic mechanisms and studies of ecological stability. Cooperation and conflict alter the connection between genetic changes and properties that emerge at higher levels of biological organization."
BRIAN WILLIAMS,"Bostonia: 1999-2000, no. 1-4",
BRIAN WILLIAMS,Butterfly genome reveals promiscuous exchange of mimicry adaptations among species,"The evolutionary importance of hybridization and introgression has long been debated1. Hybrids are usually rare and unfit, but even infrequent hybridization can aid adaptation by transferring beneficial traits between species. Here we use genomic tools to investigate introgression in Heliconius, a rapidly radiating genus of neotropical butterflies widely used in studies of ecology, behaviour, mimicry and speciation2,3,4,5. We sequenced the genome of Heliconius melpomene and compared it with other taxa to investigate chromosomal evolution in Lepidoptera and gene flow among multiple Heliconius species and races. Among 12,669 predicted genes, biologically important expansions of families of chemosensory and Hox genes are particularly noteworthy. Chromosomal organization has remained broadly conserved since the Cretaceous period, when butterflies split from the Bombyx (silkmoth) lineage. Using genomic resequencing, we show hybrid exchange of genes between three co-mimics, Heliconius melpomene, Heliconius timareta and Heliconius elevatus, especially at two genomic regions that control mimicry pattern. We infer that closely related Heliconius species exchange protective colour-pattern genes promiscuously, implying that hybridization has an important role in adaptive radiation."
BRIAN WILLIAMS,Genetic underpinnings of increased BMI and its association with late midlife cognitive abilities,"OBJECTIVES: First, we test for differences in various cognitive abilities across trajectories of body mass index (BMI) over the later life course. Second, we examine whether genetic risk factors for unhealthy BMIs-assessed via polygenic risk scores (PRS)-predict cognitive abilities in late-life. METHODS: The study used a longitudinal sample of Vietnam veteran males to explore the associations between BMI trajectories, measured across four time points, and later cognitive abilities. The sample of 977 individuals was drawn from the Vietnam Era Twin Study of Aging. Cognitive abilities evaluated included executive function, abstract reasoning, episodic memory, processing speed, verbal fluency, and visual spatial ability. Multilevel linear regression models were used to estimate the associations between BMI trajectories and cognitive abilities. Then, BMI PRS was added to the models to evaluate polygenic associations with cognitive abilities. RESULTS: There were no significant differences in cognitive ability between any of the BMI trajectory groups. There was a significant inverse relationship between BMI-PRS and several cognitive ability measures. DISCUSSION: While no associations emerged for BMI trajectories and cognitive abilities at the phenotypic levels, BMI PRS measures did correlate with key cognitive domains. Our results suggest possible polygenic linkages cutting across key components of the central and peripheral nervous system."
BRIAN WILLIAMS,"Canvass: a crowd-sourced, natural-product screening library for exploring biological space",
BRIAN WILLIAMS,Imaging X-ray polarimetry explorer: prelaunch,"Launched on 2021 December 9, the Imaging X-ray Polarimetry Explorer (IXPE) is a NASA Small Explorer Mission in collaboration with the Italian Space Agency (ASI). The mission will open a new window of investigation—imaging x-ray polarimetry. The observatory features three identical telescopes, each consisting of a mirror module assembly with a polarization-sensitive imaging x-ray detector at the focus. A coilable boom, deployed on orbit, provides the necessary 4-m focal length. The observatory utilizes a three-axis-stabilized spacecraft, which provides services such as power, attitude determination and control, commanding, and telemetry to the ground. During its 2-year baseline mission, IXPE will conduct precise polarimetry for samples of multiple categories of x-ray sources, with follow-on observations of selected targets."
BRIAN WILLIAMS,Transformational leadership in a high school choral program,"The purpose of this study was to examine a high school choral program to discover how the leadership behaviors of the teacher contributed to the success of the program. The teacher's leadership behaviors were examined through the framework of Transformational Leadership. Criteria for the selection of this program included a recent performance at a state music conference, consistent high ratings at choral performance evaluations, and the inclusion of one chorus where students continued to sing but did not audition into the top chorus. Primary participants included student members in the three choruses and their director. Secondary participants included the orchestra director and the choral director from the feeder middle school. Data were collected through interviews, observations ofrehearsals and concerts, and artifacts during one academic year of instruction. Data were coded and analyzed and primary themes included: (a) a supportive classroom atmosphere; (b) repertoire selection; (c) developing independent musicians; and (d) creating a chorus community. Results ofthis study indicate that the director demonstrated numerous leadership behaviors that intersect with those found in Transformational Leadership. These included: setting clear, challenging goals; holding high expectations for music reading and performance; providing support in a friendly classroom environment; establishing a sense of community in the chorus; and, genuinely caring for the needs of students. These transformative leadership behaviors led to students who demonstrated musical independence. Suggestions for the profession include implementing transformational leadership behaviors into undergraduate music education courses, which can provide opportunities for preservice teachers to reflect upon and develop their leadership skills to a higher level before they enter the profession."
BRIAN WILLIAMS,Toroidal prefactorization algebras associated to holomorphic fibrations and a relationship to vertex algebras,"Let $X$ be a complex manifold, $\pi: E \rightarrow X$ a locally trivial holomorphic fibration with fiber $F$, and $\mathfrak{g}$ a Lie algebra with an invariant symmetric form. We associate to this data a holomorphic prefactorization algebra $\mathcal{F}_{\mathfrak{g}, \pi}$ on $X$ in the formalism of Costello-Gwilliam. When $X=\mathbb{C}$, $\mathfrak{g}$ is simple, and $F$ is a smooth affine variety, we extract from $\mathcal{F}_{\mathfrak{g}, \pi}$ a vertex algebra which is a vacuum module for the universal central extension of the Lie algebra $\mathfrak{g} \otimes H^{0}(F, \mathcal{O})[z,z^{-1}]$. As a special case, when $F$ is an algebraic torus $(\mathbb{C}^{*})^n$, we obtain a vertex algebra naturally associated to an $(n+1)$--toroidal algebra, generalizing the affine vacuum module."
BRIAN WILLIAMS,Rapid HIV Testing Program Implementation: Lessons from the Emergency Department,"BACKGROUND. The US Centers for Disease Control and Prevention (CDC) guidelines and the World Health Organization (WHO) both recommend HIV testing in health-care settings. However, neither organization provides prescriptive details regarding how these recommendations should be adapted into clinical practice in an emergency department. METHODS. We have implemented an HIV-testing program in the ED of a major academic medical center within the scope of the Universal Screening for HIV Infection in the Emergency Room (USHER) Trial—a randomized clinical trial evaluating the feasibility and cost-effectiveness of HIV screening in this setting. RESULTS AND CONCLUSION. Drawing on our collective experiences in establishing programs domestically and internationally, we offer a practical framework of lessons learned so that others poised to embark on such HIV testing programs may benefit from our experiences."
BRIAN WILLIAMS,A connection between colony biomass and death in Caribbean Reef-Building Corals,"Increased sea-surface temperatures linked to warming climate threaten coral reef ecosystems globally. To better understand how corals and their endosymbiotic dinoflagellates (Symbiodinium spp.) respond to environmental change, tissue biomass and Symbiodinium density of seven coral species were measured on various reefs approximately every four months for up to thirteen years in the Upper Florida Keys, United States (1994–2007), eleven years in the Exuma Cays, Bahamas (1995–2006), and four years in Puerto Morelos, Mexico (2003–2007). For six out of seven coral species, tissue biomass correlated with Symbiodinium density. Within a particular coral species, tissue biomasses and Symbiodinium densities varied regionally according to the following trends: Mexico≥Florida Keys≥Bahamas. Average tissue biomasses and symbiont cell densities were generally higher in shallow habitats (1–4 m) compared to deeper-dwelling conspecifics (12–15 m). Most colonies that were sampled displayed seasonal fluctuations in biomass and endosymbiont density related to annual temperature variations. During the bleaching episodes of 1998 and 2005, five out of seven species that were exposed to unusually high temperatures exhibited significant decreases in symbiotic algae that, in certain cases, preceded further decreases in tissue biomass. Following bleaching, Montastraea spp. colonies with low relative biomass levels died, whereas colonies with higher biomass levels survived. Bleaching- or disease-associated mortality was also observed in Acropora cervicornis colonies; compared to A. palmata, all A. cervicornis colonies experienced low biomass values. Such patterns suggest that Montastraea spp. and possibly other coral species with relatively low biomass experience increased susceptibility to death following bleaching or other stressors than do conspecifics with higher tissue biomass levels."
BRIAN WILLIAMS,The cusp plasma imaging detector (CuPID) cubesat observatory: instrumentation,"The Cusp Plasma Imaging Detector (CuPID) CubeSat observatory is a 6U CubeSat designed to observe solar wind charge exchange in magnetospheric cusps to test competing theories of magnetic reconnection at the Earth's magnetopause. The CuPID is equipped with three instruments, namely, a wide field-of-view (4.6° × 4.6°) soft x-ray telescope, a micro-dosimeter suite, and an engineering magnetometer optimized for the science operation. The instrument suite has been tested and calibrated in relevant environments, demonstrating successful design. The testing and calibration of these instruments produced metrics and coefficients that will be used to create the CuPID mission's data product."
BRIAN WILLIAMS,On the need for International Solar Terrestrial Program Next (ISTPNext),
BRIAN WILLIAMS,Proceedings of the Sixth International Workshop on Web Caching and Content Distribution,"OVERVIEW: The International Web Content Caching and Distribution Workshop (WCW) is a premiere technical meeting for researchers and practitioners interested in all aspects of content caching, distribution and delivery on the Internet. The 2001 WCW meeting was held on the Boston University Campus. Building on the successes of the five previous WCW meetings, WCW01 featured a strong technical program and record participation from leading researchers and practitioners in the field. This report includes all the technical papers presented at WCW'01. NOTE: Proceedings of WCW'01 are published by Elsevier. Hard copies of these proceedings can be purchased through the workshop organizers. As a service to the community, electronic copies of all WCW'01 papers are accessible through Technical Report BUCS‐TR‐2001‐017, available from the Boston University Computer Science Technical Report Archives at http://www.cs.bu.edu/techreps. [Ed.note: URL outdated. Use http://www.bu.edu/cs/research/technical-reports or http://hdl.handle.net/2144/1455 in this repository to access the reports.]"
BRIAN WILLIAMS,The trans-heliospheric survey,"Context.Though the solar wind is characterized by spatial and temporal variability across a wide range of scales, long-term averages of in situ measurements have revealed clear radial trends: changes in average values of basic plasma parameters (e.g., density, temperature, and speed) and a magnetic field with a distance from the Sun. Aims.To establish our current understanding of the solar wind's average expansion through the heliosphere, data from multiple spacecraft needed to be combined and standardized into a single dataset. Methods.In this study, data from twelve heliospheric and planetary spacecraft - Parker Solar Probe (PSP), Helios 1 and 2, Mariner 2 and 10, Ulysses, Cassini, Pioneer 10 and 11, New Horizons, and Voyager 1 and 2 - were compiled into a dataset spanning over three orders of magnitude in heliocentric distance. To avoid introducing artifacts into this composite dataset, special attention was given to the solar cycle, spacecraft heliocentric elevation, and instrument calibration. Results.The radial trend in each parameter was found to be generally well described by a power-law fit, though up to two break points were identified in each fit. Conclusions.These radial trends are publicly released here to benefit research groups in the validation of global heliospheric simulations and in the development of new deep-space missions such as Interstellar Probe."
BRIAN WILLIAMS,Tracing MYC expression for small molecule discovery,"Our inability to effectively ""drug"" targets such as MYC for therapeutic purposes requires the development of new approaches. We report on the implementation of a phenotype-based assay for monitoring MYC expression in multiple myeloma cells. The open reading frame (ORF) encoding an unstable variant of GFP was engineered immediately downstream of the MYC ORF using CRISPR/Cas9, resulting in co-expression of both proteins from the endogenous MYC locus. Using fluorescence readout as a surrogate for MYC expression, we implemented a pilot screen in which ∼10,000 compounds were prosecuted. Among known MYC expression inhibitors, we identified cardiac glycosides and cytoskeletal disruptors to be quite potent. We demonstrate the power of CRISPR/Cas9 engineering in establishing phenotype-based assays to identify gene expression modulators."
BRIAN WILLIAMS,The eighteenth data release of the Sloan Digital Sky Surveys: targeting and first spectra from SDSS-V,"The eighteenth data release (DR18) of the Sloan Digital Sky Survey (SDSS) is the first one for SDSS-V, the fifth generation of the survey. SDSS-V comprises three primary scientific programs or “Mappers”: the Milky Way Mapper (MWM), the Black Hole Mapper (BHM), and the Local Volume Mapper. This data release contains extensive targeting information for the two multiobject spectroscopy programs (MWM and BHM), including input catalogs and selection functions for their numerous scientific objectives. We describe the production of the targeting databases and their calibration and scientifically focused components. DR18 also includes ∼25,000 new SDSS spectra and supplemental information for X-ray sources identified by eROSITA in its eFEDS field. We present updates to some of the SDSS software pipelines and preview changes anticipated for DR19. We also describe three value-added catalogs (VACs) based on SDSS-IV data that have been published since DR17, and one VAC based on the SDSS-V data in the eFEDS field."
BRIAN WILLIAMS,Hemispheric symmetry and asymmetry of poleward moving radar auroral forms (PMRAFs) and associated polar cap patches during a geomagnetic storm,"Introduction: Magnetopause reconnection is known to impact the dayside ionosphere by driving fast ionospheric flows, auroral transients, and high-density plasma structures named polar cap patches. However, most of the observed reconnection impact is limited to one hemisphere, and a question arises as to how symmetric the impact is between hemispheres. Methods: We address the question using interhemispheric observations of poleward moving radar auroral forms (PMRAFs), which are a “fossil” signature of magnetopause reconnection, during a geomagnetic storm. We are particularly interested in the temporal repetition and spatial structure of PMRAFs, which are directly affected by the temporal and spatial variation of magnetopause reconnection. PMRAFs are detected and traced using SuperDARN complemented by DMSP, Swarm, and GPS TEC measurements. Results: The results show that PMRAFs occurred repetitively on time scales of about 10 min. They were one-to-one related to pulsed ionospheric flows, and were collocated with polar cap patches embedded in a Tongue of Ionization. The temporal repetition of PMRAFs exhibited a remarkably high degree of correlation between hemispheres, indicating that PMRAFs were produced at a similar rate, or even in close synchronization, in the two hemispheres. However, the spatial structure exhibited significant hemispherical asymmetry. In the Northern Hemisphere, PMRAFs/patches had a dawn-dusk elongated cigar shape that extended &amp;gt;1,000 km, at times reaching &amp;gt;2,000 km, whereas in the Southern Hemisphere, PMRAFs/patches were 2–3 times shorter. Conclusion: The interesting symmetry and asymmetry of PMRAFs suggests that both magnetopause reconnection and local ionospheric conditions play important roles in determining the degree of symmetry of PMRAFs/patches."
BRIAN WILLIAMS,"Bostonia: 2003-2004, no. 1-4",
BRIAN WILLIAMS,The long-baseline neutrino experiment: exploring fundamental symmetries of the universe,"The preponderance of matter over antimatter in the early Universe, the dynamics of the supernova bursts that produced the heavy elements necessary for life and whether protons eventually decay --- these mysteries at the forefront of particle physics and astrophysics are key to understanding the early evolution of our Universe, its current state and its eventual fate. The Long-Baseline Neutrino Experiment (LBNE) represents an extensively developed plan for a world-class experiment dedicated to addressing these questions. LBNE is conceived around three central components: (1) a new, high-intensity neutrino source generated from a megawatt-class proton accelerator at Fermi National Accelerator Laboratory, (2) a near neutrino detector just downstream of the source, and (3) a massive liquid argon time-projection chamber deployed as a far detector deep underground at the Sanford Underground Research Facility. This facility, located at the site of the former Homestake Mine in Lead, South Dakota, is approximately 1,300 km from the neutrino source at Fermilab -- a distance (baseline) that delivers optimal sensitivity to neutrino charge-parity symmetry violation and mass ordering effects. This ambitious yet cost-effective design incorporates scalability and flexibility and can accommodate a variety of upgrades and contributions. With its exceptional combination of experimental configuration, technical capabilities, and potential for transformative discoveries, LBNE promises to be a vital facility for the field of particle physics worldwide, providing physicists from around the globe with opportunities to collaborate in a twenty to thirty year program of exciting science. In this document we provide a comprehensive overview of LBNE's scientific objectives, its place in the landscape of neutrino physics worldwide, the technologies it will incorporate and the capabilities it will possess."
CHIARA MARGARIA,Dynamic communication with biased senders,"We study dynamic games in which senders with state-independent payoffs communicate to a single receiver. Senders' private information evolves according to an aperiodic and irreducible Markov chain. We prove an analog of a folk theorem—that any feasible and individually rational payoff can be approximated in a perfect Bayesian equilibrium if players are sufficiently patient. In particular, there are equilibria in which the receiver makes perfectly informed decisions in almost every period, even if no informative communication can be sustained in the stage game. We conclude that repeated interaction can overcome strategic limits of communication."
CHIARA MARGARIA,Learning and payoff externalities in an investment game,"This paper examines the interplay of informational and payoff externalities in a two-player irreversible investment game. Each player learns about the quality of his project by observing a private signal and the action of his opponent. I characterize the unique symmetric equilibrium in a timing game that features a second-mover advantage, allowing for arbitrary correlation in project qualities. Despite private learning, the game reduces to a stochastic war of attrition. In contrast to the case of purely informational externalities, all investments happen at the same real time instant—irrespective of the sign of the correlation—and beliefs never get trapped in a no-learning region, provided that the second-mover advantage is sufficiently high."
CHIARA MARGARIA,Exit dilemma: the role of private learning on firm survival,"We study the exit of duopolists from a stochastically declining market. Firms privately learn about the market conditions from observing the stochastic arrival of customers, while exit decisions are publicly observed. A larger firm is more likely to have customers and hence has better information about the market conditions. We provide sufficient conditions for either the smaller or the larger firm to be the first to exit in the unique equilibrium. Because of observational learning, exiting may be a firm's dominant action since continuing operation would bring too optimistic news to the rival, leading it to further postpone its exit. (JEL D21, D43, D83, L21, L25)"
MAUREEN DUBREUIL,Risk of myocardial infarction with use of selected nonsteroidal anti-inflammatory drugs in spondyloarthritis patients,"BACKGROUND: Spondyloarthritis (SpA) is associated with increased risk of myocardial infarction (MI); the risk may be due to the underlying inflammatory disease, or also due to medications that increase MI risk, such as certain non-steroidal anti-inflammatory drugs (NSAIDs). OBJECTIVES: 1. To describe the risk of myocardial infarction (MI) among patients with spondyloarthritis who are prescribed NSAIDs 2. To compare the pattern of MI risk with specific NSAID use among spondyloarthritis patients with the pattern of risk among patients with osteoarthritis (OA) METHODS: Nested case-control studies were performed using 1994–2015 data from The Health Improvement Network (THIN). Underlying cohorts included adult patients with incident SpA or OA had >1 NSAID prescriptions and no history of MI. In each cohort, we matched cases of incident MI to four controls without MI. NSAID use was categorized as: (A) current (prescription end date 0–180 days prior to index date), (B) recent (181–365 days), or (C) remote (>365 days). We performed conditional logistic regression to compare the odds of current or recent NSAID use relative to remote use of any NSAID, considering diclofenac and naproxen specifically. RESULTS: Within the SpA cohort of 8140 and the OA cohort of 244,399, there were 115 and 6287 MI cases, respectively. After adjustment, among SpA subjects, current diclofenac use was associated with an OR of 3.05 (95% CI 1.48–6.29; Table 2) for MI. Naproxen use was not associated with any increase (adjusted OR 1.25, 95% CI 0.56–2.78). A ratio of ORs for SpA/diclofenac relative to OA/diclofenac was 2.35 (1.10–4.90)."
WANZHENG HU,Transiently enhanced interlayer tunneling in optically driven high-Tc superconductors,"Recent pump-probe experiments reported an enhancement of superconducting transport along the c axis of underdoped YBa2Cu3O6+δ (YBCO), induced by a midinfrared optical pump pulse tuned to a specific lattice vibration. To understand this transient nonequilibrium state, we develop a pump-probe formalism for a stack of Josephson junctions, and we consider the tunneling strengths in the presence of modulation with an ultrashort optical pulse. We demonstrate that a transient enhancement of the Josephson coupling can be obtained for pulsed excitation and that this can be even larger than in a continuously driven steady state. Especially interesting is the conclusion that the effect is largest when the material is parametrically driven at a frequency immediately above the plasma frequency, in agreement with what is found experimentally. For bilayer Josephson junctions, an enhancement similar to that experimentally is predicted below the critical temperature Tc. This model reproduces the essential features of the enhancement measured below Tc. To reproduce the experimental results above Tc, we will explore extensions of this model, such as in-plane and amplitude fluctuations, elsewhere."
WANZHENG HU,Transient gap generation in BaFe2As2 driven by coherent lattice vibrations,"Iron-based superconductors provide a rich platform to investigate the interplay between unconventional superconductivity, nematicity, and magnetism. The electronic structure and the magnetic properties of iron-based superconductors are highly sensitive to the pnictogen height. Coherent excitation of the A1g phonon by femtosecond laser directly modulates the pnictogen height, which has been used to control the physical properties of iron-based superconductors. Previous studies show that the driven A1g phonon resulted in a transient increase of the pnictogen height in BaFe2As2, favoring an enhanced Fe magnetic moment. However, there are no direct observations on either the enhanced Fe magnetic moments or the enhanced spin-density wave (SDW) gap. Here, we use time-resolved broadband terahertz spectroscopy to investigate the dynamics of BaFe2As2 in the A1g phonon-driven state. Below the SDW transition temperature, we observe a transient gap generation at early-time delays. A similar transient feature is observed in the normal state up to room temperature."
JONATHAN HUGGINS,Robust inference and model criticism using bagged posteriors,"Standard Bayesian inference is known to be sensitive to model misspecification, leading to unreliable uncertainty quantification and poor predictive performance. However, finding generally applicable and computationally feasible methods for robust Bayesian inference under misspecification has proven to be a difficult challenge. An intriguing, easy-to-use, and widely applicable approach is to use bagging on the Bayesian posterior (“Bayes- Bag”); that is, to use the average of posterior distributions conditioned on bootstrapped datasets. In this paper, we develop the asymptotic theory of BayesBag, propose a model–data mismatch index for model criticism using BayesBag, and empirically validate our theory and methodology on synthetic and real-world data in linear regression, sparse logistic regression, and a hierarchical mixed effects model. We find that in the presence of significant misspecification, BayesBag yields more reproducible inferences and has better predictive accuracy than the standard Bayesian posterior; on the other hand, when the model is correctly specified, BayesBag produces superior or equally good results. Overall, our results demonstrate that BayesBag combines the attractive modeling features of standard Bayesian inference with the distributional robustness properties of frequentist methods, providing benefits over both Bayes alone and the bootstrap alone."
JONATHAN HUGGINS,Robust and reproducible model selection using bagged posteriors,"Bayesian model selection is premised on the assumption that the data are generated from one of the postulated models, however, in many applications, all of these models are incorrect. When two or more models provide a nearly equally good t to the data, Bayesian model selection can be highly unstable, potentially leading to self-contradictory ndings. In this paper, we explore using bagging on the posterior distribution (\BayesBag"") when performing model selection { that is, averaging the posterior model probabilities over many bootstrapped datasets. We provide theoreti- cal results characterizing the asymptotic behavior of the standard posterior and the BayesBag posterior under misspeci cation, in the model selection setting. We empir- ically assess the BayesBag approach on synthetic and real-world data in (i) feature selection for linear regression and (ii) phylogenetic tree reconstruction. Our theory and experiments show that in the presence of misspeci cation, BayesBag provides (a) greater reproducibility and (b) greater accuracy in selecting the correct model, compared to the standard Bayesian posterior; on the other hand, under correct speci- cation, BayesBag is slightly more conservative than the standard posterior. Overall, our results demonstrate that BayesBag provides an easy-to-use and widely applicable approach that improves upon standard Bayesian model selection by making it more stable and reproducible."
JONATHAN HUGGINS,Statistical inference with stochastic gradient algorithms,"The tuning of stochastic gradient algorithms (SGAs) for optimization and sampling is often based on heuristics and trial-and-error rather than generalizable theory. We address this theory–practice gap by characterizing the large-sample statistical asymptotics of SGAs via a joint step-size–sample-size scaling limit. We show that iterate averaging with a large fixed step size is robust to the choice of tuning parameters and asymptotically has covariance proportional to that of the MLE sampling distribution. We also prove a Bernstein–von Mises-like theorem to guide tuning, including for generalized posteriors that are robust to model misspecification. Numerical experiments validate our results and recommendations in realistic finite-sample regimes. Our work lays the foundation for a systematic analysis of other stochastic gradient Markov chain Monte Carlo algorithms for a wide range of models."
JONATHAN HUGGINS,"Independent finite approximations for Bayesian nonparametric inference: construction, error bounds, and practical implications","Bayesian nonparametrics based on completely random measures (CRMs)offers a flexible modeling approach when the number of clusters or latent componentsin a dataset is unknown. However, managing the infinite dimensionality of CRMs oftenleads to slow computation. Practical inference typically relies on either integrating outthe infinite-dimensional parameter or using afinite approximation: a truncated finiteapproximation (TFA) or an independent finite approximation (IFA). The atom weightsof TFAs are constructed sequentially, while the atoms of IFAs are independent, which(1) make them well-suited for parallel and distributed computation and (2) facilitatesmore convenient inference schemes. While IFAs have been developed in certain spe-cial cases in the past, there has not yet been a general template for construction or asystematic comparison to TFAs. We show how to construct IFAs for approximating dis-tributions in a large family of CRMs, encompassing all those typically used in practice.We quantify the approximation error between IFAs and the target nonparametric prior,and prove that, in the worst-case, TFAs provide more component-efficient approxima-tions than IFAs. However, in experiments on image denoising and topic modeling taskswith real data, we find that the error of Bayesian approximation methods overwhelmsany finite approximation error, and IFAs perform very similarly to TFAs."
JONATHAN HUGGINS,"Robust, accurate stochastic optimization for variational inference","We consider the problem of fitting variational posterior approximations using stochastic optimization methods. The performance of these approximations depends on (1) how well the variational family matches the true posterior distribution, (2) the choice of divergence, and (3) the optimization of the variational objective. We show that even in the best-case scenario when the exact posterior belongs to the assumed variational family, common stochastic optimization methods lead to poor variational approximations if the problem dimension is moderately large. We also demonstrate that these methods are not robust across diverse model types. Motivated by these findings, we develop a more robust and accurate stochastic optimization framework by viewing the underlying optimization algorithm as producing a Markov chain. Our approach is theoretically motivated and includes a diagnostic for convergence and a novel stopping rule, both of which are robust to noisy evaluations of the objective function. We show empirically that the proposed framework works well on a diverse set of models: it can automatically detect stochastic optimization failure or inaccurate variational approximation."
JONATHAN HUGGINS,Calibrated model criticism using split predictive checks,"Checking how well a fitted model explains the data is one of the most fundamental parts of a Bayesian data analysis. However, existing model checking methods suffer from trade-offs between being well-calibrated, automated, and computationally efficient. To overcome these limitations, we propose split predictive checks (SPCs), which combine the ease-of-use and speed of posterior predictive checks with the good calibration properties of predictive checks that rely on model-specific derivations or inference schemes. We develop an asymptotic theory for two types of SPCs: single SPCs and the divided SPC. Our results demonstrate that they offer complementary strengths: single SPCs provide superior power in the small-data regime or when the misspecification is significant and divided SPCs provide superior power as the dataset size increases or when the form of misspecification is more subtle. We validate the finite-sample utility of SPCs through extensive simulation experiments in exponential family and hierarchical models, and provide four real-data examples where SPCs offer novel insights and additional flexibility beyond what is available when using posterior predictive checks."
JONATHAN HUGGINS,"Robust, automated, and accurate black-box variational inference","Black-box variational inference (BBVI) now sees widespread use in machine learning and statistics as a fast yet flexible alternative to Markov chain Monte Carlo methods for approximate Bayesian inference. However, stochastic optimization methods for BBVI remain unreliable and require substantial expertise and hand-tuning to apply effectively. In this paper, we propose Robust, Automated, and Accurate BBVI (RAABBVI), a framework for reliable BBVI optimization. RAABBVI is based on rigorously justified automation techniques, includes just a small number of intuitive tuning parameters, and detects inaccurate estimates of the optimal variational approximation. RAABBVI adaptively decreases the learning rate by detecting convergence of the fixed--learning-rate iterates, then estimates the symmetrized Kullback--Leiber (KL) divergence between the current variational approximation and the optimal one. It also employs a novel optimization termination criterion that enables the user to balance desired accuracy against computational cost by comparing (i) the predicted relative decrease in the symmetrized KL divergence if a smaller learning were used and (ii) the predicted computation required to converge with the smaller learning rate. We validate the robustness and accuracy of RAABBVI through carefully designed simulation studies and on a diverse set of real-world model and data examples."
JONATHAN HUGGINS,Validated variational inference via practical posterior error bounds,"Variational inference has become an increasingly attractive fast alternative to Markov chain Monte Carlo methods for approximate Bayesian inference. However, a major obstacle to the widespread use of variational methods is the lack of post-hoc accuracy measures that are both theoretically justified and computationally efficient. In this paper, we provide rigorous bounds on the error of posterior mean and uncertainty estimates that arise from full-distribution approximations, as in variational inference. Our bounds are widely applicable, as they require only that the approximating and exact posteriors have polynomial moments. Our bounds are also computationally efficient for variational inference because they require only standard values from variational objectives, straightforward analytic calculations, and simple Monte Carlo estimates. We show that our analysis naturally leads to a new and improved workflow for validated variational inference. Finally, we demonstrate the utility of our proposed workflow and error bounds on a robust regression problem and on a real-data example with a widely used multilevel hierarchical model."
JONATHAN HUGGINS,Bidirectional contact tracing could dramatically improve COVID-19 control,"Contact tracing is critical to controlling COVID-19, but most protocols only ""forward-trace"" to notify people who were recently exposed. Using a stochastic branching-process model, we find that ""bidirectional"" tracing to identify infector individuals and their other infectees robustly improves outbreak control. In our model, bidirectional tracing more than doubles the reduction in effective reproduction number (Reff) achieved by forward-tracing alone, while dramatically increasing resilience to low case ascertainment and test sensitivity. The greatest gains are realised by expanding the manual tracing window from 2 to 6 days pre-symptom-onset or, alternatively, by implementing high-uptake smartphone-based exposure notification; however, to achieve the performance of the former approach, the latter requires nearly all smartphones to detect exposure events. With or without exposure notification, our results suggest that implementing bidirectional tracing could dramatically improve COVID-19 control."
JONATHAN HUGGINS,Challenges and opportunities in high-dimensional variational inference,"We explore the limitations of and best practices for using black-box variational inference to estimate posterior summaries of the model parameters. By taking an importance sampling perspective, we are able to explain and empirically demonstrate: 1) why the intuitions about the behavior of approximate families and divergences for low-dimensional posteriors fail for higher-dimensional posteriors, 2) how we can diagnose the pre-asymptotic reliability of variational inference in practice by examining the behavior of the density ratios (i.e., importance weights), 3) why the choice of variational objective is not as relevant for higher-dimensional posteriors, and 4) why, although flexible variational families can provide some benefits in higher dimensions, they also introduce additional optimization challenges. Based on these findings, for high-dimensional posteriors we recommend using the exclusive KL divergence that is most stable and easiest to optimize, and then focusing on improving the variational family or using model parameter transformations to make the posterior more similar to the approximating family. Our results also show that in low to moderate dimensions, heavy-tailed variational families and mass-covering divergences can increase the chances that the approximation can be improved by importance sampling."
JONATHAN HUGGINS,"The Mutational signature comprehensive analysis toolkit (musicatk) for the discovery, prediction, and exploration of mutational signatures","Mutational signatures are patterns of somatic alterations in the genome caused by carcinogenic exposures or aberrant cellular processes. To provide a comprehensive workflow for preprocessing, analysis, and visualization of mutational signatures, we created the Mutational Signature Comprehensive Analysis Toolkit (musicatk) package. musicatk enables users to select different schemas for counting mutation types and to easily combine count tables from different schemas. Multiple distinct methods are available to deconvolute signatures and exposures or to predict exposures in individual samples given a pre-existing set of signatures. Additional exploratory features include the ability to compare signatures to the Catalogue Of Somatic Mutations In Cancer (COSMIC) database, embed tumors in two dimensions with uniform manifold approximation and projection, cluster tumors into subgroups based on exposure frequencies, identify differentially active exposures between tumor subgroups, and plot exposure distributions across user-defined annotations such as tumor type. Overall, musicatk will enable users to gain novel insights into the patterns of mutational signatures observed in cancer cohorts. SIGNIFICANCE: The musicatk package empowers researchers to characterize mutational signatures and tumor heterogeneity with a comprehensive set of preprocessing utilities, discovery and prediction tools, and multiple functions for downstream analysis and visualization."
RYAN GOH,Asymptotic approximation of a modified compressible Navier-Stokes system,"We study the effects of localization on the long time asymptotics of a modified compressible Navier-Stokes system (mcNS) inspired by the previous work of Hoff and Zumbrun. We introduce a new decomposition of the momentum field into its irrotational and incompressible parts, and a new method for approximating solutions of jointly hyperbolic-parabolic equations in terms of Hermite functions in which $n^{th}$ order approximations can be computed for solutions with $n^{th}$ order moments. We then obtain existence of solutions to the mcNS system in weighted spaces and, based on the decay rates obtained for the various pieces of the solutions, determine the optimal choice of asymptotic approximation with respect to the various localization assumptions, which in certain cases can be evaluated explicitly in terms of Hermite functions."
RYAN GOH,Spectral stability of pattern-forming fronts in the complex Ginzburg–Landau equation with a quenching mechanism,"We consider pattern-forming fronts in the complex Ginzburg-Landau equation with a traveling spatial heterogeneity which destabilizes, or quenches, the trivial ground state while progressing through the domain. We consider the regime where the heterogeneity propagates with speed c just below the linear invasion speed of the pattern-forming front in the associated homogeneous system. In this situation, the front locks to the interface of the heterogeneity leaving a long intermediate state lying near the unstable ground state, possibly allowing for growth of perturbations. This manifests itself in the spectrum of the linearization about the front through the accumulation of eigenvalues onto the absolute spectrum associated with the unstable ground state. As the quench speed c increases towards the linear invasion speed, the absolute spectrum stabilizes with the same rate at which eigenvalues accumulate onto it allowing us to rigorously establish spectrally stability of the front in L2(R). The presence of unstable absolute spectrum poses a technical challenge as spatial eigenvalues along the intermediate state no longer admit a hyperbolic splitting and standard tools such as exponential dichotomies are unavailable. Instead, we projectivize the linear flow, and use Riemann surface unfolding in combination with a superposition principle to study the evolution of subspaces as solutions to the associated matrix Riccati differential equation on the Grassmannian manifold. Eigenvalues can then be identified as the roots of the meromorphic Riccati-Evans function, and can be located using winding number and parity arguments."
RYAN GOH,Strain and defects in oblique stripe growth,We study stripe formation in two-dimensional systems under directional quenching in a phase-diffusion approximation including non-adiabatic boundary effects. We find stripe formation through simple traveling waves for all angles relative to the quenching line using an analytic continuation procedure. We also present comprehensive analytical asymptotic formulas in limiting cases of small and large angles as well as small and large quenching rates. Of particular interest is a regime of small angle and slow quenching rate which is well described by the glide motion of a boundary dislocation along the quenching line. A delocalization bifurcation of this dislocation leads to a sharp decrease of strain created in the growth process at small angles. We complement our results with numerical continuation reliant on a boundary-integral formulation. We also compare results in the phase-diffusion approximation numerically to quenched stripe formation in an anisotropic Swift Hohenberg equation.
RYAN GOH,"Dissecting the dynamics of signaling events in the BMP, WNT, and NODAL cascade during self-organized fate patterning in human gastruloids.","During gastrulation, the pluripotent epiblast self-organizes into the 3 germ layers-endoderm, mesoderm and ectoderm, which eventually form the entire embryo. Decades of research in the mouse embryo have revealed that a signaling cascade involving the Bone Morphogenic Protein (BMP), WNT, and NODAL pathways is necessary for gastrulation. In vivo, WNT and NODAL ligands are expressed near the site of gastrulation in the posterior of the embryo, and knockout of these ligands leads to a failure to gastrulate. These data have led to the prevailing view that a signaling gradient in WNT and NODAL underlies patterning during gastrulation; however, the activities of these pathways in space and time have never been directly observed. In this study, we quantify BMP, WNT, and NODAL signaling dynamics in an in vitro model of human gastrulation. Our data suggest that BMP signaling initiates waves of WNT and NODAL signaling activity that move toward the colony center at a constant rate. Using a simple mathematical model, we show that this wave-like behavior is inconsistent with a reaction-diffusion-based Turing system, indicating that there is no stable signaling gradient of WNT/NODAL. Instead, the final signaling state is homogeneous, and spatial differences arise only from boundary effects. We further show that the durations of WNT and NODAL signaling control mesoderm differentiation, while the duration of BMP signaling controls differentiation of CDX2-positive extra-embryonic cells. The identity of these extra-embryonic cells has been controversial, and we use RNA sequencing (RNA-seq) to obtain their transcriptomes and show that they closely resemble human trophoblast cells in vivo. The domain of BMP signaling is identical to the domain of differentiation of these trophoblast-like cells; however, neither WNT nor NODAL forms a spatial pattern that maps directly to the mesodermal region, suggesting that mesoderm differentiation is controlled dynamically by the combinatorial effect of multiple signals. We synthesize our data into a mathematical model that accurately recapitulates signaling dynamics and predicts cell fate patterning upon chemical and physical perturbations. Taken together, our study shows that the dynamics of signaling events in the BMP, WNT, and NODAL cascade in the absence of a stable signaling gradient control fate patterning of human gastruloids."
KEVIN C THOMAS,A systematic search of Zwicky Transient Facility data for ultracompact binary LISA-detectable gravitational-wave sources,"Using photometry collected with the Zwicky Transient Facility, we are conducting an ongoing survey for binary systems with short orbital periods (P_b < 1 hr) with the goal of identifying new gravitational-wave sources detectable by the upcoming Laser Interferometer Space Antenna (LISA). We present a sample of 15 binary systems discovered thus far, with orbital periods ranging from 6.91 to 56.35 minutes. Of the 15 systems, seven are eclipsing systems that do not show signs of significant mass transfer. Additionally, we have discovered two AM Canum Venaticorum systems and six systems exhibiting primarily ellipsoidal variations in their lightcurves. We present follow-up spectroscopy and high-speed photometry confirming the nature of these systems, estimates of their LISA signal-to-noise ratios, and a discussion of their physical characteristics."
KEVIN C THOMAS,A new class of Roche lobe–filling hot subdwarf binaries,"We present the discovery of the second binary with a Roche lobe–filling hot subdwarf transferring mass to a white dwarf (WD) companion. This 56 minute binary was discovered using data from the Zwicky Transient Facility. Spectroscopic observations reveal an He-sdOB star with an effective temperature of Teff = 33,700 ± 1000 K and a surface gravity of log(g) = 5.54 ± 0.11. The GTC+HiPERCAM light curve is dominated by the ellipsoidal deformation of the He-sdOB star and shows an eclipse of the He-sdOB by an accretion disk as well as a weak eclipse of the WD. We infer a He-sdOB mass of MsdOB = 0.41 ± 0.04 M⊙ and a WD mass of MWD = 0.68 ± 0.05 M⊙. The weak eclipses imply a WD blackbody temperature of 63,000 ± 10,000 K and a radius RWD = 0.0148 ± 0.0020 R⊙ as expected for a WD of such high temperature. The He-sdOB star is likely undergoing hydrogen shell burning and will continue transferring mass for ≈1 Myr at a rate of 10−9 M⊙ yr−1, which is consistent with the high WD temperature. The hot subdwarf will then turn into a WD and the system will merge in ≈30 Myr. We suggest that Galactic reddening could bias discoveries toward preferentially finding Roche lobe–filling systems during the short-lived shell-burning phase. Studies using reddening-corrected samples should reveal a large population of helium core–burning hot subdwarfs with Teff ≈ 25,000 K in binaries of 60–90 minutes with WDs. Though not yet in contact, these binaries would eventually come into contact through gravitational-wave emission and explode as a subluminous thermonuclear supernova or evolve into a massive single WD."
KEVIN C THOMAS,The first ultracompact Roche lobe–filling hot subdwarf binary,"We report the discovery of the first short-period binary in which a hot subdwarf star (sdOB) filled its Roche lobe and started mass transfer to its companion. The object was discovered as part of a dedicated high-cadence survey of the Galactic plane named the Zwicky Transient Facility and exhibits a period of P = 39.3401(1) minutes, making it the most compact hot subdwarf binary currently known. Spectroscopic observations are consistent with an intermediate He-sdOB star with an effective temperature of T_eff = 42,400 ± 300 K and a surface gravity of log(g) = 5.77 ± 0.05. A high signal-to-noise ratio GTC+HiPERCAM light curve is dominated by the ellipsoidal deformation of the sdOB star and an eclipse of the sdOB by an accretion disk. We infer a low-mass hot subdwarf donor with a mass MsdOB = 0.337 ± 0.015 M_⊙ and a white dwarf accretor with a mass MWD = 0.545 ± 0.020 M_⊙. Theoretical binary modeling indicates the hot subdwarf formed during a common envelope phase when a 2.5–2.8 M_⊙ star lost its envelope when crossing the Hertzsprung gap. To match its current P_orb, T_eff, log(g), and masses, we estimate a post–common envelope period of P_orb ≈ 150 minutes and find that the sdOB star is currently undergoing hydrogen shell burning. We estimate that the hot subdwarf will become a white dwarf with a thick helium layer of ≈0.1 M_⊙, merge with its carbon/oxygen white dwarf companion after ≈17 Myr, and presumably explode as a thermonuclear supernova or form an R CrB star."
KEVIN C THOMAS,A new class of large-amplitude radial-mode hot subdwarf pulsators,"Using high-cadence observations from the Zwicky Transient Facility at low Galactic latitudes, we have discovered a new class of pulsating, hot compact stars. We have found four candidates, exhibiting blue colors (g − r ≤ −0.1 mag), pulsation amplitudes of >5%, and pulsation periods of 200–475 s. Fourier transforms of the light curves show only one dominant frequency. Phase-resolved spectroscopy for three objects reveals significant radial velocity, T eff, and log(g) variations over the pulsation cycle, which are consistent with large-amplitude radial oscillations. The mean T eff and log(g) for these stars are consistent with hot subdwarf B (sdB) effective temperatures and surface gravities. We calculate evolutionary tracks using MESA and adiabatic pulsations using GYRE for low-mass, helium-core pre-white dwarfs (pre-WDs) and low-mass helium-burning stars. Comparison of low-order radial oscillation mode periods with the observed pulsation periods show better agreement with the pre-WD models. Therefore, we suggest that these new pulsators and blue large-amplitude pulsators (BLAPs) could be members of the same class of pulsators, composed of young ≈0.25–0.35 M ⊙ helium-core pre-WDs."
KEVIN C THOMAS,"Canvass: a crowd-sourced, natural-product screening library for exploring biological space",
KEVIN C THOMAS,Imaging X-ray polarimetry explorer: prelaunch,"Launched on 2021 December 9, the Imaging X-ray Polarimetry Explorer (IXPE) is a NASA Small Explorer Mission in collaboration with the Italian Space Agency (ASI). The mission will open a new window of investigation—imaging x-ray polarimetry. The observatory features three identical telescopes, each consisting of a mirror module assembly with a polarization-sensitive imaging x-ray detector at the focus. A coilable boom, deployed on orbit, provides the necessary 4-m focal length. The observatory utilizes a three-axis-stabilized spacecraft, which provides services such as power, attitude determination and control, commanding, and telemetry to the ground. During its 2-year baseline mission, IXPE will conduct precise polarimetry for samples of multiple categories of x-ray sources, with follow-on observations of selected targets."
KEVIN C THOMAS,"First Sagittarius A* Event Horizon Telescope results. II. EHT and multiwavelength observations, data processing, and calibration","We present Event Horizon Telescope (EHT) 1.3 mm measurements of the radio source located at the position of the supermassive black hole Sagittarius A* (Sgr A*), collected during the 2017 April 5–11 campaign. The observations were carried out with eight facilities at six locations across the globe. Novel calibration methods are employed to account for Sgr A*'s flux variability. The majority of the 1.3 mm emission arises from horizon scales, where intrinsic structural source variability is detected on timescales of minutes to hours. The effects of interstellar scattering on the image and its variability are found to be subdominant to intrinsic source structure. The calibrated visibility amplitudes, particularly the locations of the visibility minima, are broadly consistent with a blurred ring with a diameter of ∼50 μas, as determined in later works in this series. Contemporaneous multiwavelength monitoring of Sgr A* was performed at 22, 43, and 86 GHz and at near-infrared and X-ray wavelengths. Several X-ray flares from Sgr A* are detected by Chandra, one at low significance jointly with Swift on 2017 April 7 and the other at higher significance jointly with NuSTAR on 2017 April 11. The brighter April 11 flare is not observed simultaneously by the EHT but is followed by a significant increase in millimeter flux variability immediately after the X-ray outburst, indicating a likely connection in the emission physics near the event horizon. We compare Sgr A*’s broadband flux during the EHT campaign to its historical spectral energy distribution and find that both the quiescent emission and flare emission are consistent with its long-term behavior."
KEVIN C THOMAS,Final results on Se-82 double beta decay to the ground state of Kr-82 from the NEMO-3 experiment,"Using data from the NEMO-3 experiment, we have measured the two-neutrino double beta decay (2𝜈𝛽𝛽) half-life of ^82 Se as 𝑇2𝜈1/2=[9.39±0.17( stat )±0.58( syst )]×10^19 y under the single-state dominance hypothesis for this nuclear transition. The corresponding nuclear matrix element is ∣∣𝑀^2𝜈∣∣=0.0498±0.0016. In addition, a search for neutrinoless double beta decay (0𝜈𝛽𝛽) using 0.93 kg of ^82 Se observed for a total of 5.25 y has been conducted and no evidence for a signal has been found. The resulting half-life limit of 𝑇^0𝜈1/2>2.5×10^23 y (90% C.L. ) for the light neutrino exchange mechanism leads to a constraint on the effective Majorana neutrino mass of ⟨𝑚𝜈⟩<(1.2−3.0) eV , where the range reflects 0𝜈𝛽𝛽 nuclear matrix element values from different calculations. Furthermore, constraints on lepton number violating parameters for other 0𝜈𝛽𝛽 mechanisms, such as right-handed currents, majoron emission and R-parity violating supersymmetry modes have been set."
KEVIN C THOMAS,Constructing custom-made radiotranscriptomic signatures of vascular inflammation from routine CT angiograms: a prospective outcomes validation study in COVID-19,"BACKGROUND: Direct evaluation of vascular inflammation in patients with COVID-19 would facilitate more efficient trials of new treatments and identify patients at risk of long-term complications who might respond to treatment. We aimed to develop a novel artificial intelligence (AI)-assisted image analysis platform that quantifies cytokine-driven vascular inflammation from routine CT angiograms, and sought to validate its prognostic value in COVID-19. METHODS: For this prospective outcomes validation study, we developed a radiotranscriptomic platform that uses RNA sequencing data from human internal mammary artery biopsies to develop novel radiomic signatures of vascular inflammation from CT angiography images. We then used this platform to train a radiotranscriptomic signature (C19-RS), derived from the perivascular space around the aorta and the internal mammary artery, to best describe cytokine-driven vascular inflammation. The prognostic value of C19-RS was validated externally in 435 patients (331 from study arm 3 and 104 from study arm 4) admitted to hospital with or without COVID-19, undergoing clinically indicated pulmonary CT angiography, in three UK National Health Service (NHS) trusts (Oxford, Leicester, and Bath). We evaluated the diagnostic and prognostic value of C19-RS for death in hospital due to COVID-19, did sensitivity analyses based on dexamethasone treatment, and investigated the correlation of C19-RS with systemic transcriptomic changes. FINDINGS: Patients with COVID-19 had higher C19-RS than those without (adjusted odds ratio [OR] 2·97 [95% CI 1·43-6·27], p=0·0038), and those infected with the B.1.1.7 (alpha) SARS-CoV-2 variant had higher C19-RS values than those infected with the wild-type SARS-CoV-2 variant (adjusted OR 1·89 [95% CI 1·17-3·20] per SD, p=0·012). C19-RS had prognostic value for in-hospital mortality in COVID-19 in two testing cohorts (high [≥6·99] vs low [<6·99] C19-RS; hazard ratio [HR] 3·31 [95% CI 1·49-7·33], p=0·0033; and 2·58 [1·10-6·05], p=0·028), adjusted for clinical factors, biochemical biomarkers of inflammation and myocardial injury, and technical parameters. The adjusted HR for in-hospital mortality was 8·24 (95% CI 2·16-31·36, p=0·0019) in patients who received no dexamethasone treatment, but 2·27 (0·69-7·55, p=0·18) in those who received dexamethasone after the scan, suggesting that vascular inflammation might have been a therapeutic target of dexamethasone in COVID-19. Finally, C19-RS was strongly associated (r=0·61, p=0·00031) with a whole blood transcriptional module representing dysregulation of coagulation and platelet aggregation pathways. INTERPRETATION: Radiotranscriptomic analysis of CT angiography scans introduces a potentially powerful new platform for the development of non-invasive imaging biomarkers. Application of this platform in routine CT pulmonary angiography scans done in patients with COVID-19 produced the radiotranscriptomic signature C19-RS, a marker of cytokine-driven inflammation driving systemic activation of coagulation and responsible for adverse clinical outcomes, which predicts in-hospital mortality and might allow targeted therapy. FUNDING: Engineering and Physical Sciences Research Council, British Heart Foundation, Oxford BHF Centre of Research Excellence, Innovate UK, NIHR Oxford Biomedical Research Centre, Wellcome Trust, Onassis Foundation."
KEVIN C THOMAS,Event Horizon Telescope imaging of the archetypal blazar 3C 279 at an extreme 20 microarcsecond resolution,"3C 279 is an archetypal blazar with a prominent radio jet that show broadband flux density variability across the entire electromagnetic spectrum. We use an ultra-high angular resolution technique – global Very Long Baseline Interferometry (VLBI) at 1.3 mm (230 GHz) – to resolve the innermost jet of 3C 279 in order to study its fine-scale morphology close to the jet base where highly variable γ-ray emission is thought to originate, according to various models. The source was observed during four days in April 2017 with the Event Horizon Telescope at 230 GHz, including the phased Atacama Large Millimeter/submillimeter Array (ALMA), at an angular resolution of ∼20 μas (at a redshift of z = 0.536 this corresponds to ∼0.13 pc  ∼ 1700 Schwarzschild radii with a black hole mass M<jats:sub>BH</jats:sub> = 8 × 10^8 M_⊙). Imaging and model-fitting techniques were applied to the data to parameterize the fine-scale source structure and its variation. We find a multicomponent inner jet morphology with the northernmost component elongated perpendicular to the direction of the jet, as imaged at longer wavelengths. The elongated nuclear structure is consistent on all four observing days and across different imaging methods and model-fitting techniques, and therefore appears robust. Owing to its compactness and brightness, we associate the northern nuclear structure as the VLBI “core”. This morphology can be interpreted as either a broad resolved jet base or a spatially bent jet. We also find significant day-to-day variations in the closure phases, which appear most pronounced on the triangles with the longest baselines. Our analysis shows that this variation is related to a systematic change of the source structure. Two inner jet components move non-radially at apparent speeds of ∼15 c and ∼20 c (∼1.3 and ∼1.7 μas day^−1, respectively), which more strongly supports the scenario of traveling shocks or instabilities in a bent, possibly rotating jet. The observed apparent speeds are also coincident with the 3C 279 large-scale jet kinematics observed at longer (cm) wavelengths, suggesting no significant jet acceleration between the 1.3 mm core and the outer jet. The intrinsic brightness temperature of the jet components are ≲10^10 K, a magnitude or more lower than typical values seen at ≥7 mm wavelengths. The low brightness temperature and morphological complexity suggest that the core region of 3C 279 becomes optically thin at short (mm) wavelengths."
KEVIN C THOMAS,First M87 Event Horizon Telescope results. II. Array and instrumentation,"The Event Horizon Telescope (EHT) is a very long baseline interferometry (VLBI) array that comprises millimeter- and submillimeter-wavelength telescopes separated by distances comparable to the diameter of the Earth. At a nominal operating wavelength of ~1.3 mm, EHT angular resolution (λ/D) is ~25 μas, which is sufficient to resolve nearby supermassive black hole candidates on spatial and temporal scales that correspond to their event horizons. With this capability, the EHT scientific goals are to probe general relativistic effects in the strong-field regime and to study accretion and relativistic jet formation near the black hole boundary. In this Letter we describe the system design of the EHT, detail the technology and instrumentation that enable observations, and provide measures of its performance. Meeting the EHT science objectives has required several key developments that have facilitated the robust extension of the VLBI technique to EHT observing wavelengths and the production of instrumentation that can be deployed on a heterogeneous array of existing telescopes and facilities. To meet sensitivity requirements, high-bandwidth digital systems were developed that process data at rates of 64 gigabit s−1, exceeding those of currently operating cm-wavelength VLBI arrays by more than an order of magnitude. Associated improvements include the development of phasing systems at array facilities, new receiver installation at several sites, and the deployment of hydrogen maser frequency standards to ensure coherent data capture across the array. These efforts led to the coordination and execution of the first Global EHT observations in 2017 April, and to event-horizon-scale imaging of the supermassive black hole candidate in M87."
KEVIN C THOMAS,First M87 Event Horizon Telescope results. I. The shadow of the supermassive black hole,"When surrounded by a transparent emission region, black holes are expected to reveal a dark shadow caused by gravitational light bending and photon capture at the event horizon. To image and study this phenomenon, we have assembled the Event Horizon Telescope, a global very long baseline interferometry array observing at a wavelength of 1.3 mm. This allows us to reconstruct event-horizon-scale images of the supermassive black hole candidate in the center of the giant elliptical galaxy M87. We have resolved the central compact radio source as an asymmetric bright emission ring with a diameter of 42 ± 3 μas, which is circular and encompasses a central depression in brightness with a flux ratio gsim10:1. The emission ring is recovered using different calibration and imaging schemes, with its diameter and width remaining stable over four different observations carried out in different days. Overall, the observed image is consistent with expectations for the shadow of a Kerr black hole as predicted by general relativity. The asymmetry in brightness in the ring can be explained in terms of relativistic beaming of the emission from a plasma rotating close to the speed of light around a black hole. We compare our images to an extensive library of ray-traced general-relativistic magnetohydrodynamic simulations of black holes and derive a central mass of M = (6.5 ± 0.7) × 10^9 M ⊙. Our radio-wave observations thus provide powerful evidence for the presence of supermassive black holes in centers of galaxies and as the central engines of active galactic nuclei. They also present a new tool to explore gravity in its most extreme limit and on a mass scale that was so far not accessible."
KEVIN C THOMAS,Detailed studies of ^100Mo two-neutrino double beta decay in NEMO-3,"The full data set of the NEMO-3 experiment has been used to measure the half-life of the two-neutrino double beta decay of ^100Mo to the ground state of ^100Ru, 𝑇_1/2=[6.81±0.01( stat )^+0.38_−0.40( syst )]×10^18 year. The two-electron energy sum, single electron energy spectra and distribution of the angle between the electrons are presented with an unprecedented statistics of 5×10^5 events and a signal-to-background ratio of ∼ 80. Clear evidence for the Single State Dominance model is found for this nuclear transition. Limits on Majoron emitting neutrinoless double beta decay modes with spectral indices of n=2,3,7, as well as constraints on Lorentz invariance violation and on the bosonic neutrino contribution to the two-neutrino double beta decay mode are obtained."
KEVIN C THOMAS,The eighteenth data release of the Sloan Digital Sky Surveys: targeting and first spectra from SDSS-V,"The eighteenth data release (DR18) of the Sloan Digital Sky Survey (SDSS) is the first one for SDSS-V, the fifth generation of the survey. SDSS-V comprises three primary scientific programs or “Mappers”: the Milky Way Mapper (MWM), the Black Hole Mapper (BHM), and the Local Volume Mapper. This data release contains extensive targeting information for the two multiobject spectroscopy programs (MWM and BHM), including input catalogs and selection functions for their numerous scientific objectives. We describe the production of the targeting databases and their calibration and scientifically focused components. DR18 also includes ∼25,000 new SDSS spectra and supplemental information for X-ray sources identified by eROSITA in its eFEDS field. We present updates to some of the SDSS software pipelines and preview changes anticipated for DR19. We also describe three value-added catalogs (VACs) based on SDSS-IV data that have been published since DR17, and one VAC based on the SDSS-V data in the eFEDS field."
KEVIN C THOMAS,First Sagittarius A* Event Horizon Telescope results. I. The shadow of the supermassive black hole in the center of the Milky Way,"We present the first Event Horizon Telescope (EHT) observations of Sagittarius A* (Sgr A*), the Galactic center source associated with a supermassive black hole. These observations were conducted in 2017 using a global interferometric array of eight telescopes operating at a wavelength of λ = 1.3 mm. The EHT data resolve a compact emission region with intrahour variability. A variety of imaging and modeling analyses all support an image that is dominated by a bright, thick ring with a diameter of 51.8 ± 2.3 μas (68% credible interval). The ring has modest azimuthal brightness asymmetry and a comparatively dim interior. Using a large suite of numerical simulations, we demonstrate that the EHT images of Sgr A* are consistent with the expected appearance of a Kerr black hole with mass ∼4 × 106 M ⊙, which is inferred to exist at this location based on previous infrared observations of individual stellar orbits, as well as maser proper-motion studies. Our model comparisons disfavor scenarios where the black hole is viewed at high inclination (i &gt; 50°), as well as nonspinning black holes and those with retrograde accretion disks. Our results provide direct evidence for the presence of a supermassive black hole at the center of the Milky Way, and for the first time we connect the predictions from dynamical measurements of stellar orbits on scales of 103–105 gravitational radii to event-horizon-scale images and variability. Furthermore, a comparison with the EHT results for the supermassive black hole M87* shows consistency with the predictions of general relativity spanning over three orders of magnitude in central mass."
KEVIN C THOMAS,The revised TESS Input Catalog and candidate target list,"We describe the catalogs assembled and the algorithms used to populate the revised TESS Input Catalog (TIC), based on the incorporation of the Gaia second data release. We also describe a revised ranking system for prioritizing stars for 2 minute cadence observations, and we assemble a revised Candidate Target List (CTL) using that ranking. The TIC is available on the Mikulski Archive for Space Telescopes server, and an enhanced CTL is available through the Filtergraph data visualization portal system at http://filtergraph.vanderbilt.edu/tess_ctl."
KEVIN C THOMAS,MAGIC and H.E.S.S. detect VHE gamma rays from the blazar OT081 for the first time: a deep multiwavelength study,
KEVIN C THOMAS,The long-baseline neutrino experiment: exploring fundamental symmetries of the universe,"The preponderance of matter over antimatter in the early Universe, the dynamics of the supernova bursts that produced the heavy elements necessary for life and whether protons eventually decay --- these mysteries at the forefront of particle physics and astrophysics are key to understanding the early evolution of our Universe, its current state and its eventual fate. The Long-Baseline Neutrino Experiment (LBNE) represents an extensively developed plan for a world-class experiment dedicated to addressing these questions. LBNE is conceived around three central components: (1) a new, high-intensity neutrino source generated from a megawatt-class proton accelerator at Fermi National Accelerator Laboratory, (2) a near neutrino detector just downstream of the source, and (3) a massive liquid argon time-projection chamber deployed as a far detector deep underground at the Sanford Underground Research Facility. This facility, located at the site of the former Homestake Mine in Lead, South Dakota, is approximately 1,300 km from the neutrino source at Fermilab -- a distance (baseline) that delivers optimal sensitivity to neutrino charge-parity symmetry violation and mass ordering effects. This ambitious yet cost-effective design incorporates scalability and flexibility and can accommodate a variety of upgrades and contributions. With its exceptional combination of experimental configuration, technical capabilities, and potential for transformative discoveries, LBNE promises to be a vital facility for the field of particle physics worldwide, providing physicists from around the globe with opportunities to collaborate in a twenty to thirty year program of exciting science. In this document we provide a comprehensive overview of LBNE's scientific objectives, its place in the landscape of neutrino physics worldwide, the technologies it will incorporate and the capabilities it will possess."
JONATHAN GREENACRE,Failure of mobile money services: standards for systemic risk,"With an overview analysing the mobile money system, this article provides a preliminary criterion for determining potential systemic risk of collapse of a mobile money firm (MM firm). The article has two main components: first, defining systemic risk, it clarifies that systemic risk is likely to arise through a delay in returning customers’ funds from an MM firm in insolvency proceedings. Second, the article points out that determining whether this delay will have systemic consequences ought to consider diverse elements, particularly the range of components of the economy which could be impacted by the failure of an MM firm, the size of the failing MM firm, available substitutes for mobile money, and interconnections with the remainder of the economy. Future research should explore this topic in greater depth and begin developing regulatory tools that can address potential systemic consequences of failure, such as accelerated bankruptcy regimes."
JONATHAN GREENACRE,Resolving liquidity problems in mobile money,
PETER EVERETT,Event Horizon Telescope imaging of the archetypal blazar 3C 279 at an extreme 20 microarcsecond resolution,"3C 279 is an archetypal blazar with a prominent radio jet that show broadband flux density variability across the entire electromagnetic spectrum. We use an ultra-high angular resolution technique – global Very Long Baseline Interferometry (VLBI) at 1.3 mm (230 GHz) – to resolve the innermost jet of 3C 279 in order to study its fine-scale morphology close to the jet base where highly variable γ-ray emission is thought to originate, according to various models. The source was observed during four days in April 2017 with the Event Horizon Telescope at 230 GHz, including the phased Atacama Large Millimeter/submillimeter Array (ALMA), at an angular resolution of ∼20 μas (at a redshift of z = 0.536 this corresponds to ∼0.13 pc  ∼ 1700 Schwarzschild radii with a black hole mass M<jats:sub>BH</jats:sub> = 8 × 10^8 M_⊙). Imaging and model-fitting techniques were applied to the data to parameterize the fine-scale source structure and its variation. We find a multicomponent inner jet morphology with the northernmost component elongated perpendicular to the direction of the jet, as imaged at longer wavelengths. The elongated nuclear structure is consistent on all four observing days and across different imaging methods and model-fitting techniques, and therefore appears robust. Owing to its compactness and brightness, we associate the northern nuclear structure as the VLBI “core”. This morphology can be interpreted as either a broad resolved jet base or a spatially bent jet. We also find significant day-to-day variations in the closure phases, which appear most pronounced on the triangles with the longest baselines. Our analysis shows that this variation is related to a systematic change of the source structure. Two inner jet components move non-radially at apparent speeds of ∼15 c and ∼20 c (∼1.3 and ∼1.7 μas day^−1, respectively), which more strongly supports the scenario of traveling shocks or instabilities in a bent, possibly rotating jet. The observed apparent speeds are also coincident with the 3C 279 large-scale jet kinematics observed at longer (cm) wavelengths, suggesting no significant jet acceleration between the 1.3 mm core and the outer jet. The intrinsic brightness temperature of the jet components are ≲10^10 K, a magnitude or more lower than typical values seen at ≥7 mm wavelengths. The low brightness temperature and morphological complexity suggest that the core region of 3C 279 becomes optically thin at short (mm) wavelengths."
PETER EVERETT,First M87 Event Horizon Telescope results. II. Array and instrumentation,"The Event Horizon Telescope (EHT) is a very long baseline interferometry (VLBI) array that comprises millimeter- and submillimeter-wavelength telescopes separated by distances comparable to the diameter of the Earth. At a nominal operating wavelength of ~1.3 mm, EHT angular resolution (λ/D) is ~25 μas, which is sufficient to resolve nearby supermassive black hole candidates on spatial and temporal scales that correspond to their event horizons. With this capability, the EHT scientific goals are to probe general relativistic effects in the strong-field regime and to study accretion and relativistic jet formation near the black hole boundary. In this Letter we describe the system design of the EHT, detail the technology and instrumentation that enable observations, and provide measures of its performance. Meeting the EHT science objectives has required several key developments that have facilitated the robust extension of the VLBI technique to EHT observing wavelengths and the production of instrumentation that can be deployed on a heterogeneous array of existing telescopes and facilities. To meet sensitivity requirements, high-bandwidth digital systems were developed that process data at rates of 64 gigabit s−1, exceeding those of currently operating cm-wavelength VLBI arrays by more than an order of magnitude. Associated improvements include the development of phasing systems at array facilities, new receiver installation at several sites, and the deployment of hydrogen maser frequency standards to ensure coherent data capture across the array. These efforts led to the coordination and execution of the first Global EHT observations in 2017 April, and to event-horizon-scale imaging of the supermassive black hole candidate in M87."
PETER EVERETT,First M87 Event Horizon Telescope results. I. The shadow of the supermassive black hole,"When surrounded by a transparent emission region, black holes are expected to reveal a dark shadow caused by gravitational light bending and photon capture at the event horizon. To image and study this phenomenon, we have assembled the Event Horizon Telescope, a global very long baseline interferometry array observing at a wavelength of 1.3 mm. This allows us to reconstruct event-horizon-scale images of the supermassive black hole candidate in the center of the giant elliptical galaxy M87. We have resolved the central compact radio source as an asymmetric bright emission ring with a diameter of 42 ± 3 μas, which is circular and encompasses a central depression in brightness with a flux ratio gsim10:1. The emission ring is recovered using different calibration and imaging schemes, with its diameter and width remaining stable over four different observations carried out in different days. Overall, the observed image is consistent with expectations for the shadow of a Kerr black hole as predicted by general relativity. The asymmetry in brightness in the ring can be explained in terms of relativistic beaming of the emission from a plasma rotating close to the speed of light around a black hole. We compare our images to an extensive library of ray-traced general-relativistic magnetohydrodynamic simulations of black holes and derive a central mass of M = (6.5 ± 0.7) × 10^9 M ⊙. Our radio-wave observations thus provide powerful evidence for the presence of supermassive black holes in centers of galaxies and as the central engines of active galactic nuclei. They also present a new tool to explore gravity in its most extreme limit and on a mass scale that was so far not accessible."
PETER EVERETT,"The L 98-59 system: three transiting, terrestrial-size planets orbiting a nearby M dwarf","We report the Transiting Exoplanet Survey Satellite (TESS) discovery of three terrestrial-size planets transiting L 98-59 (TOI-175, TIC 307210830)—a bright M dwarf at a distance of 10.6 pc. Using the Gaia-measured distance and broadband photometry, we find that the host star is an M3 dwarf. Combined with the TESS transits from three sectors, the corresponding stellar parameters yield planet radii ranging from 0.8 R ⊕ to 1.6 R ⊕. All three planets have short orbital periods, ranging from 2.25 to 7.45 days with the outer pair just wide of a 2:1 period resonance. Diagnostic tests produced by the TESS Data Validation Report and the vetting package DAVE rule out common false-positive sources. These analyses, along with dedicated follow-up and the multiplicity of the system, lend confidence that the observed signals are caused by planets transiting L 98-59 and are not associated with other sources in the field. The L 98-59 system is interesting for a number of reasons: the host star is bright (V = 11.7 mag, K = 7.1 mag) and the planets are prime targets for further follow-up observations including precision radial-velocity mass measurements and future transit spectroscopy with the James Webb Space Telescope; the near-resonant configuration makes the system a laboratory to study planetary system dynamical evolution; and three planets of relatively similar size in the same system present an opportunity to study terrestrial planets where other variables (age, metallicity, etc.) can be held constant. L 98-59 will be observed in four more TESS sectors, which will provide a wealth of information on the three currently known planets and have the potential to reveal additional planets in the system."
PETER EVERETT,First Sagittarius A* Event Horizon Telescope results. I. The shadow of the supermassive black hole in the center of the Milky Way,"We present the first Event Horizon Telescope (EHT) observations of Sagittarius A* (Sgr A*), the Galactic center source associated with a supermassive black hole. These observations were conducted in 2017 using a global interferometric array of eight telescopes operating at a wavelength of λ = 1.3 mm. The EHT data resolve a compact emission region with intrahour variability. A variety of imaging and modeling analyses all support an image that is dominated by a bright, thick ring with a diameter of 51.8 ± 2.3 μas (68% credible interval). The ring has modest azimuthal brightness asymmetry and a comparatively dim interior. Using a large suite of numerical simulations, we demonstrate that the EHT images of Sgr A* are consistent with the expected appearance of a Kerr black hole with mass ∼4 × 106 M ⊙, which is inferred to exist at this location based on previous infrared observations of individual stellar orbits, as well as maser proper-motion studies. Our model comparisons disfavor scenarios where the black hole is viewed at high inclination (i &gt; 50°), as well as nonspinning black holes and those with retrograde accretion disks. Our results provide direct evidence for the presence of a supermassive black hole at the center of the Milky Way, and for the first time we connect the predictions from dynamical measurements of stellar orbits on scales of 103–105 gravitational radii to event-horizon-scale images and variability. Furthermore, a comparison with the EHT results for the supermassive black hole M87* shows consistency with the predictions of general relativity spanning over three orders of magnitude in central mass."
JIANYU HAN,The cross section of the monetary policy announcement premium,"Using the expected option-implied variance reduction to measure the sensitivity of stock returns to monetary policy announcement surprises, this paper shows monetary policy announcements require significant risk compensation in the cross section of equity returns. We develop a parsimonious equilibrium model in which FOMC announcements reveal the Federal Reserve’s private information about its interest-rate target, which affects the private sector’s expectation about the long-run growth-rate of the economy. Our model accounts for the dynamics of implied variances and the cross section of the monetary policy announcement premium realized around FOMC announcement days."
JIANYU HAN,Information-driven volatility,"Modern asset pricing theory predicts an unambiguously positive relationship between volatility and expected returns. Empirically, however, realized volatility in the past often predicts expected returns in the future with a negative sign, as exemplified by the volatility-managed portfolios of Moreira and Muir (2017). Theoretically, we show that information-driven volatility induces a negative correlation between past realized volatility and expected volatility and expected returns in the future. We develop a simple asset pricing model based on this intuition and demonstrate that our model can account for several volatility-related asset pricing puzzles such as the return on volatility managed portfolios, the “variance risk premium” return predictability (Bollerslev, Tauchen, and Zhou, 2009), and the predictability of returns by implied volatility reduction on macroeconomic announcement days."
JIANYU HAN,Ambiguity and information processing in a model of intermediary asset pricing,"This paper incorporates ambiguity and information processing constraints into a model of intermediary asset pricing. Financial intermediaries are assumed to possess greater information processing capacity. Households purchase this capacity, and then delegate their investment decisions to intermediaries. As in He and Krishnamurthy (2012), the delegation contract is constrained by a moral hazard problem, which gives rise to a minimum capital requirement. Both households and intermediaries have a preference for robustness, reflecting ambiguity about asset returns (Hansen and Sargent (2008)). We show that ambiguity aversion tightens the capital constraint, and amplifies its effects. Detection error probabilities are used to discipline the degree of ambiguity aversion. The model can explain both the unconditional moments of asset returns and their state dependence, even with DEPs in excess of 20%."
JIANYU HAN,"Announcements, expectations, and stock returns with asymmetric information","Revisions of consensus forecasts of macroeconomic variables positively predict announcement day forecast errors, whereas stock market returns on forecast revision days predict announcement day returns in the opposite direction. A dynamic noisy rational expectations model with periodic macroeconomic announcements quantitatively accounts for these findings. Under asymmetric information, average beliefs are not Bayesian: they underweight new information and positively predict subsequent belief errors. In addition, stock prices are partly driven by noise, and therefore negatively predict returns on announcement days when noise is revealed and the market corrects itself."
JIANYU HAN,Information acquisition and the pre-announcement drift,"We present a dynamic Grossman-Stiglitz model with endogenous information acquisition to explain the pre-FOMC announcement drift. Because FOMC announcements reveal substantial information about the economy, investors’ incentives to acquire information are particularly strong days ahead of the announcements. Information acquisition partially resolves the uncertainty for uninformed traders, and under generalized risk sensitive preferences (Ai and Bansal, 2018), lower the discount rate and results in a stock market run-up. Because our theory does not rely on the leakage of information, it can simultaneously explain the low realized volatility during the pre-FOMC announcement period and the lack of a positive correlation between pre-and post-announcement returns."
XI LING,Determining the twist angle of stacked MoS2 layers using machine learning‐assisted low‐frequency interlayer Raman fingerprints,"The investigation of twisted stacked few‐layer MoS2 has revealed novel electronic, optical, and vibrational properties over an extended period. For the successful integration of twisted stacked few‐layer MoS2 into a wide range of applications, it is crucial to employ a noninvasive, versatile technique for characterizing the layered architecture of these complex structures. In this work, we introduce a machine learning‐assisted low‐frequency Raman spectroscopy method to characterize the twist angle of few‐layer stacked MoS2 samples. A feedforward neural network (FNN) is utilized to analyze the low‐frequency breathing mode as a function of the twist angle. Moreover, using finite difference method (FDM) and density functional theory (DFT) calculations, we show that the low‐frequency Raman spectra of MoS2 are mainly influenced by the effect of the nearest and second nearest layers. A new improved linear chain model (TA‐LCM) with taking the twist angle into the consideration is developed to understand the interlayer breathing modes of stacked few‐layer MoS2. This approach can be extended to other 2D materials systems and provides an intelligent way to investigate naturally stacked and twisted interlayer interactions."
ANDREW HENRY,Friends of hot Jupiters. II. No correspondence between hot-Jupiter spin-orbit misalignment and the incidence of directly imaged stellar companions,"Multi-star systems are common, yet little is known about a stellar companion's influence on the formation and evolution of planetary systems. For instance, stellar companions may have facilitated the inward migration of hot Jupiters toward to their present day positions. Many observed short-period gas giant planets also have orbits that are misaligned with respect to their star's spin axis, which has also been attributed to the presence of a massive outer companion on a non-coplanar orbit. We present the results of a multi-band direct imaging survey using Keck NIRC2 to measure the fraction of short-period gas giant planets found in multi-star systems. Over three years, we completed a survey of 50 targets (""Friends of Hot Jupiters"") with 27 targets showing some signature of multi-body interaction (misaligned or eccentric orbits) and 23 targets in a control sample (well-aligned and circular orbits). We report the masses, projected separations, and confirmed common proper motion for the 19 stellar companions found around 17 stars. Correcting for survey incompleteness, we report companion fractions of 48% ± 9%, 47% ± 12%, and 51% ± 13% in our total, misaligned/eccentric, and control samples, respectively. This total stellar companion fraction is 2.8σ larger than the fraction of field stars with companions approximately 50-2000 AU. We observe no correlation between misaligned/eccentric hot Jupiter systems and the incidence of stellar companions. Combining this result with our previous radial velocity survey, we determine that 72% ± 16% of hot Jupiters are part of multi-planet and/or multi-star systems."
ANDREW HENRY,The magic of crowd acclamations and the cult of amulets in late antiquity,"This project examines the prevalence of acclamation formulas inscribed on late Roman amulets. It argues that acclamations in amuletic form were believed to have the power to protect due to the powers that people in late antiquity attributed to actual acclamatory performance. Crowds shouted acclamations in public gatherings and liturgical processions for a variety of pragmatic reasons, including voicing dissent against an emperor or in response to natural disasters. Late antique texts also demonstrate that crowds were thought to possess extraordinary, even miraculous, qualities when shouting acclamations. Crowds chanted “Kyrie Eleison” to ward off plagues. Chanting “Holy, Holy, Holy” was believed to invoke angelic presence. This late antique culture of attributing efficacy to acclamatory performance should direct how we interpret the appearance of acclamations on amulets. These amulets illustrate the range of acclamations that constituted a veritable repertoire of efficacious formulas which could be materialized in portable or visual form for protection. Whereas prior scholarship on such acclamations focused primarily on their political and theological meanings, this thesis demonstrates how their functions extended well beyond communication. Inscribing an amulet with a popular chant recalls the memory of these powerful performances and extends that power in material form. This thesis further illustrates that this material efficacy of amulets could be integrated into the built environment when inscribed on monumental building stones or as graffiti in the streets of late antique towns."
ANDREW HENRY,A Music index of periodicals for the first six months of 1948 /,
ANDREW HENRY,"BMQ : Boston medical quarterly: v. 12, no. 1-4",
ANDREW HENRY,"Scope: v. 1, no. 1-8",
ANDREW HENRY,The role of incentive-based instruments and social equity in conservation conflict interventions,"Conflicts between biodiversity conservation and other human activities are multifaceted. Understanding farmer preferences for various conflict mitigation strategies is therefore critical. We developed a novel interactive game around farmer land management decisions across 18 villages in Gabon to examine responses to three elephant conflict mitigation options: use of elephant deterrent methods, flat-rate subsidy, and agglomeration payments rewarding coordinated action for setting land aside for elephants. We found that all three policies significantly reduced participants’ inclinations to engage in lethal control. Use of deterrents and agglomeration payments were also more likely to reduce decisions to kill elephants in situations where levels of social equity were higher. Only the two monetary incentives increased farmers’ predisposition to provide habitats for elephants, suggesting that incentive-based instruments were conducive to pro-conservation behavior; different subsidy levels did not affect responses. Likewise, neither participants’ socioeconomic characteristics nor their real-life experiences of crop damage by elephants affected game decisions. Killing behavior in the games was 64% lower in villages influenced by protected areas than in villages surrounded by logging concessions, highlighting the need to address conservation conflicts beyond protected areas. Our study shows the importance of addressing underlying social conflicts, specifically equity attitudes, prior to, or alongside addressing material losses."
ANDREW HENRY,"Twenty questions about design behavior for sustainability, report of the International Expert Panel on behavioral science for design","How behavioral scientists, engineers, and architects can work together to advance how we all understand and practice design—in order to enhance sustainability in the built environment, and beyond."
ANDREW HENRY,"BMQ : Boston medical quarterly: v. 6, no. 1-4",
ANDREW HENRY,"BMQ : Boston medical quarterly: v. 5, no. 1-4",
ANDREW HENRY,"Friends of hot Jupiters. IV. Stellar companions beyond 50 au might facilitate giant planet formation, but most are unlikely to cause Kozai-Lidov migration","Stellar companions can influence the formation and evolution of planetary systems, but there are currently few observational constraints on the properties of planet-hosting binary star systems. We search for stellar companions around 77 transiting hot Jupiter systems to explore the statistical properties of this population of companions as compared to field stars of similar spectral type. After correcting for survey incompleteness, we find that $47 \% \pm 7 \% $ of hot Jupiter systems have stellar companions with semimajor axes between 50 and 2000 au. This is 2.9 times larger than the field star companion fraction in this separation range, with a significance of $4.4\sigma $. In the 1–50 au range, only ${3.9}_{-2.0}^{+4.5} \% $ of hot Jupiters host stellar companions, compared to the field star value of $16.4 \% \pm 0.7 \% $, which is a $2.7\sigma $ difference. We find that the distribution of mass ratios for stellar companions to hot Jupiter systems peaks at small values and therefore differs from that of field star binaries which tend to be uniformly distributed across all mass ratios. We conclude that either wide separation stellar binaries are more favorable sites for gas giant planet formation at all separations, or that the presence of stellar companions preferentially causes the inward migration of gas giant planets that formed farther out in the disk via dynamical processes such as Kozai–Lidov oscillations. We determine that less than 20% of hot Jupiters have stellar companions capable of inducing Kozai–Lidov oscillations assuming initial semimajor axes between 1 and 5 au, implying that the enhanced companion occurrence is likely correlated with environments where gas giants can form efficiently."
ANDREW HENRY,"BMQ : Boston medical quarterly: v. 10, no. 1-4",
ANDREW HENRY,Scintillation light detection in the 6-m drift-length ProtoDUNE Dual Phase liquid argon TPC,"DUNE is a dual-site experiment for long-baseline neutrino oscillation studies, neutrino astrophysics and nucleon decay searches. ProtoDUNE Dual Phase (DP) is a 6  ×  6  ×  6 m 3 liquid argon time-projection-chamber (LArTPC) that recorded cosmic-muon data at the CERN Neutrino Platform in 2019-2020 as a prototype of the DUNE Far Detector. Charged particles propagating through the LArTPC produce ionization and scintillation light. The scintillation light signal in these detectors can provide the trigger for non-beam events. In addition, it adds precise timing capabilities and improves the calorimetry measurements. In ProtoDUNE-DP, scintillation and electroluminescence light produced by cosmic muons in the LArTPC is collected by photomultiplier tubes placed up to 7 m away from the ionizing track. In this paper, the ProtoDUNE-DP photon detection system performance is evaluated with a particular focus on the different wavelength shifters, such as PEN and TPB, and the use of Xe-doped LAr, considering its future use in giant LArTPCs. The scintillation light production and propagation processes are analyzed and a comparison of simulation to data is performed, improving understanding of the liquid argon properties."
ANDREW HENRY,Prospects for beyond the standard model physics searches at the deep underground neutrino experiment: DUNE collaboration,"The Deep Underground Neutrino Experiment (DUNE) will be a powerful tool for a variety of physics topics. The high-intensity proton beams provide a large neutrino flux, sampled by a near detector system consisting of a combination of capable precision detectors, and by the massive far detector system located deep underground. This configuration sets up DUNE as a machine for discovery, as it enables opportunities not only to perform precision neutrino measurements that may uncover deviations from the present three-flavor mixing paradigm, but also to discover new particles and unveil new interactions and symmetries beyond those predicted in the Standard Model (SM). Of the many potential beyond the Standard Model (BSM) topics DUNE will probe, this paper presents a selection of studies quantifying DUNE's sensitivities to sterile neutrino mixing, heavy neutral leptons, non-standard interactions, CPT symmetry violation, Lorentz invariance violation, neutrino trident production, dark matter from both beam induced and cosmogenic sources, baryon number violation, and other new physics topics that complement those at high-energy colliders and significantly extend the present reach."
ANDREW HENRY,Identification and reconstruction of low-energy electrons in the ProtoDUNE-SP detector,
ANDREW HENRY,The long-baseline neutrino experiment: exploring fundamental symmetries of the universe,"The preponderance of matter over antimatter in the early Universe, the dynamics of the supernova bursts that produced the heavy elements necessary for life and whether protons eventually decay --- these mysteries at the forefront of particle physics and astrophysics are key to understanding the early evolution of our Universe, its current state and its eventual fate. The Long-Baseline Neutrino Experiment (LBNE) represents an extensively developed plan for a world-class experiment dedicated to addressing these questions. LBNE is conceived around three central components: (1) a new, high-intensity neutrino source generated from a megawatt-class proton accelerator at Fermi National Accelerator Laboratory, (2) a near neutrino detector just downstream of the source, and (3) a massive liquid argon time-projection chamber deployed as a far detector deep underground at the Sanford Underground Research Facility. This facility, located at the site of the former Homestake Mine in Lead, South Dakota, is approximately 1,300 km from the neutrino source at Fermilab -- a distance (baseline) that delivers optimal sensitivity to neutrino charge-parity symmetry violation and mass ordering effects. This ambitious yet cost-effective design incorporates scalability and flexibility and can accommodate a variety of upgrades and contributions. With its exceptional combination of experimental configuration, technical capabilities, and potential for transformative discoveries, LBNE promises to be a vital facility for the field of particle physics worldwide, providing physicists from around the globe with opportunities to collaborate in a twenty to thirty year program of exciting science. In this document we provide a comprehensive overview of LBNE's scientific objectives, its place in the landscape of neutrino physics worldwide, the technologies it will incorporate and the capabilities it will possess."
ANDREA MERRILL,Head & Neck Optical Diagnostics: Vision of the Future of Surgery,Review paper and Proceedings of the Inaugural Meeting of the Head and Neck Optical Diagnostics Society (HNODS) on March 14th 2009 at University College London. The aim of our research must be to provide breakthrough translational research which can be applied clinically in the immediate rather than the near future. We are fortunate that this is indeed a possibility and may fundamentally change current clinical and surgical practice to improve our patients' lives.
GARRETT JOHNSON,First M87 Event Horizon Telescope results. III. Data processing and calibration,"We present the calibration and reduction of Event Horizon Telescope (EHT) 1.3 mm radio wavelength observations of the supermassive black hole candidate at the center of the radio galaxy M87 and the quasar 3C 279, taken during the 2017 April 5–11 observing campaign. These global very long baseline interferometric observations include for the first time the highly sensitive Atacama Large Millimeter/submillimeter Array (ALMA); reaching an angular resolution of 25 μas, with characteristic sensitivity limits of ~1 mJy on baselines to ALMA and ~10 mJy on other baselines. The observations present challenges for existing data processing tools, arising from the rapid atmospheric phase fluctuations, wide recording bandwidth, and highly heterogeneous array. In response, we developed three independent pipelines for phase calibration and fringe detection, each tailored to the specific needs of the EHT. The final data products include calibrated total intensity amplitude and phase information. They are validated through a series of quality assurance tests that show consistency across pipelines and set limits on baseline systematic errors of 2% in amplitude and 1° in phase. The M87 data reveal the presence of two nulls in correlated flux density at ~3.4 and ~8.3 Gλ and temporal evolution in closure quantities, indicating intrinsic variability of compact structure on a timescale of days, or several light-crossing times for a few billion solar-mass black hole. These measurements provide the first opportunity to image horizon-scale structure in M87."
GARRETT JOHNSON,First M87 Event Horizon Telescope results. V. Physical origin of the asymmetric ring,"The Event Horizon Telescope (EHT) has mapped the central compact radio source of the elliptical galaxy M87 at 1.3 mm with unprecedented angular resolution. Here we consider the physical implications of the asymmetric ring seen in the 2017 EHT data. To this end, we construct a large library of models based on general relativistic magnetohydrodynamic (GRMHD) simulations and synthetic images produced by general relativistic ray tracing. We compare the observed visibilities with this library and confirm that the asymmetric ring is consistent with earlier predictions of strong gravitational lensing of synchrotron emission from a hot plasma orbiting near the black hole event horizon. The ring radius and ring asymmetry depend on black hole mass and spin, respectively, and both are therefore expected to be stable when observed in future EHT campaigns. Overall, the observed image is consistent with expectations for the shadow of a spinning Kerr black hole as predicted by general relativity. If the black hole spin and M87's large scale jet are aligned, then the black hole spin vector is pointed away from Earth. Models in our library of non-spinning black holes are inconsistent with the observations as they do not produce sufficiently powerful jets. At the same time, in those models that produce a sufficiently powerful jet, the latter is powered by extraction of black hole spin energy through mechanisms akin to the Blandford-Znajek process. We briefly consider alternatives to a black hole for the central compact object. Analysis of existing EHT polarization data and data taken simultaneously at other wavelengths will soon enable new tests of the GRMHD models, as will future EHT campaigns at 230 and 345 GHz."
GARRETT JOHNSON,First M87 Event Horizon Telescope results. VI. The shadow and mass of the central black hole,"We present measurements of the properties of the central radio source in M87 using Event Horizon Telescope data obtained during the 2017 campaign. We develop and fit geometric crescent models (asymmetric rings with interior brightness depressions) using two independent sampling algorithms that consider distinct representations of the visibility data. We show that the crescent family of models is statistically preferred over other comparably complex geometric models that we explore. We calibrate the geometric model parameters using general relativistic magnetohydrodynamic (GRMHD) models of the emission region and estimate physical properties of the source. We further fit images generated from GRMHD models directly to the data. We compare the derived emission region and black hole parameters from these analyses with those recovered from reconstructed images. There is a remarkable consistency among all methods and data sets. We find that >50% of the total flux at arcsecond scales comes from near the horizon, and that the emission is dramatically suppressed interior to this region by a factor >10, providing direct evidence of the predicted shadow of a black hole. Across all methods, we measure a crescent diameter of 42 ± 3 μas and constrain its fractional width to be <0.5. Associating the crescent feature with the emission surrounding the black hole shadow, we infer an angular gravitational radius of GM/Dc^2 = 3.8 ± 0.4 μas. Folding in a distance measurement of {16.8}_{-0.7}^{+0.8}{Mpc} gives a black hole mass of M = 6.5 ± 0.2{| }_{stat} ± 0.7{| }_{sys} × {10}^{9} {M}_{odot }. This measurement from lensed emission near the event horizon is consistent with the presence of a central Kerr black hole, as predicted by the general theory of relativity."
GARRETT JOHNSON,The Event Horizon general relativistic magnetohydrodynamic code comparison project,"Recent developments in compact object astrophysics, especially the discovery of merging neutron stars by LIGO, the imaging of the black hole in M87 by the Event Horizon Telescope, and high- precision astrometry of the Galactic Center at close to the event horizon scale by the GRAVITY experiment motivate the development of numerical source models that solve the equations of general relativistic magnetohydrodynamics (GRMHD). Here we compare GRMHD solutions for the evolution of a magnetized accretion flow where turbulence is promoted by the magnetorotational instability from a set of nine GRMHD codes: Athena++, BHAC, Cosmos++, ECHO, H-AMR, iharm3D, HARM-Noble, IllinoisGRMHD, and KORAL. Agreement among the codes improves as resolution increases, as measured by a consistently applied, specially developed set of code performance metrics. We conclude that the community of GRMHD codes is mature, capable, and consistent on these test problems."
GARRETT JOHNSON,Privacy-centric digital advertising: implications for research,"Yesterday's digital advertising relied on cross-website and cross-app user identity to measure, target, and optimize ads. Spurred by regulatory pressure, today's digital advertising is evolving to become more privacy-protective. Apple and Google are leading this movement by sunsetting old technologies and building more privacy-centric alternatives. Marketing academics and practitioners, in turn, must learn to adapt to this new reality. We outline these new advertising approaches and their implications for advertising strategy, targeting, and measurement. We propose key questions and an agenda for researchers to help shape the privacy-centric future of digital advertising."
GARRETT JOHNSON,Gravitational test beyond the first post-Newtonian order with the shadow of the M87 black hole,"The 2017 Event Horizon Telescope (EHT) observations of the central source in M87 have led to the first measurement of the size of a black-hole shadow. This observation offers a new and clean gravitational test of the black-hole metric in the strong-field regime. We show analytically that spacetimes that deviate from the Kerr metric but satisfy weak-field tests can lead to large deviations in the predicted black-hole shadows that are inconsistent with even the current EHT measurements. We use numerical calculations of regular, parametric, non-Kerr metrics to identify the common characteristic among these different parametrizations that control the predicted shadow size. We show that the shadow-size measurements place significant constraints on deviation parameters that control the second post-Newtonian and higher orders of each metric and are, therefore, inaccessible to weak-field tests. The new constraints are complementary to those imposed by observations of gravitational waves from stellar-mass sources."
GARRETT JOHNSON,First M87 Event Horizon Telescope results. IV. Imaging the central supermassive black hole,
GARRETT JOHNSON,Verification of radiative transfer schemes for the EHT,"The Event Horizon Telescope (EHT) Collaboration has recently produced the first resolved images of the central supermassive black hole in the giant elliptical galaxy M87. Here we report on tests of the consistency and accuracy of the general relativistic radiative transfer codes used within the collaboration to model M87* and Sgr A*. We compare and evaluate (1) deflection angles for equatorial null geodesics in a Kerr spacetime; (2) images calculated from a series of simple, parameterized matter distributions in the Kerr metric using simplified emissivities and absorptivities; (3) for a subset of codes, images calculated from general relativistic magnetohydrodynamics simulations using different realistic synchrotron emissivities and absorptivities; (4) observables for the 2017 configuration of EHT, including visibility amplitudes and closure phases. The error in total flux is of order 1% when the codes are run with production numerical parameters. The dominant source of discrepancies for small camera distances is the location and detailed setup of the software ""camera"" that each code uses to produce synthetic images. We find that when numerical parameters are suitably chosen and the camera is sufficiently far away the images converge and that for given transfer coefficients, numerical uncertainties are unlikely to limit parameter estimation for the current generation of EHT observations. The purpose of this paper is to describe a verification and comparison of EHT radiative transfer codes. It is not to verify EHT models more generally."
GARRETT JOHNSON,Monitoring the mmorphology of M87* in 2009–2017 with the Event Horizon Telescope,"The Event Horizon Telescope (EHT) has recently delivered the first resolved images of M87*, the supermassive black hole in the center of the M87 galaxy. These images were produced using 230 GHz observations performed in April 2017. Additional observations are required to investigate the persistence of the primary image feature – a ring with azimuthal brightness asymmetry – and to quantify the image variability on event horizon scales. To address this need, we analyze M87* data collected with prototype EHT arrays in 2009, 2011, 2012, and 2013. While these observations do not contain enough information to produce images, they are sufficient to constrain simple geometric models. We develop a modeling approach based on the framework utilized for the 2017 EHT data analysis and validate our procedures using synthetic data. Applying the same approach to the observational data sets, we find the M87* morphology in 2009–2017 to be consistent with a persistent asymmetric ring of 40 as diameter. The position angle of peak intensity varies in time. In particular, we find a significant difference between the position angle measured in 2013 and 2017. These variations are in broad agreement with predictions of a subset of general relativistic magnetohydrodynamic simulations. We show that quantifying the variability across multiple observational epochs has the potential to constrain physical properties of the source, such as the accretion state or the black hole spin."
GARRETT JOHNSON,THEMIS: a parameter estimation framework for the Event Horizon Telescope,"The Event Horizon Telescope (EHT) provides the unprecedented ability to directly resolve the structure and dynamics of black hole emission regions on scales smaller than their horizons. This has the potential to critically probe the mechanisms by which black holes accrete and launch outflows, and the structure of supermassive black hole spacetimes. However, accessing this information is a formidable analysis challenge for two reasons. First, the EHT natively produces a variety of data types that encode information about the image structure in nontrivial ways; these are subject to a variety of systematic effects associated with very long baseline interferometry and are supplemented by a wide variety of auxiliary data on the primary EHT targets from decades of other observations. Second, models of the emission regions and their interaction with the black hole are complex, highly uncertain, and computationally expensive to construct. As a result, the scientific utilization of EHT observations requires a flexible, extensible, and powerful analysis framework. We present such a framework, Themis, which defines a set of interfaces between models, data, and sampling algorithms that facilitates future development. We describe the design and currently existing components of Themis, how Themis has been validated thus far, and present additional analyses made possible by Themis that illustrate its capabilities. Importantly, we demonstrate that Themis is able to reproduce prior EHT analyses, extend these, and do so in a computationally efficient manner that can efficiently exploit modern high-performance computing facilities. Themis has already been used extensively in the scientific analysis and interpretation of the first EHT observations of M87."
GARRETT JOHNSON,First Sagittarius A* Event Horizon Telescope results. V. Testing astrophysical models of the galactic center black hole,"In this paper we provide a first physical interpretation for the Event Horizon Telescope's (EHT) 2017 observations of Sgr A*. Our main approach is to compare resolved EHT data at 230 GHz and unresolved non-EHT observations from radio to X-ray wavelengths to predictions from a library of models based on time-dependent general relativistic magnetohydrodynamics simulations, including aligned, tilted, and stellar-wind-fed simulations; radiative transfer is performed assuming both thermal and nonthermal electron distribution functions. We test the models against 11 constraints drawn from EHT 230 GHz data and observations at 86 GHz, 2.2 μm, and in the X-ray. All models fail at least one constraint. Light-curve variability provides a particularly severe constraint, failing nearly all strongly magnetized (magnetically arrested disk (MAD)) models and a large fraction of weakly magnetized models. A number of models fail only the variability constraints. We identify a promising cluster of these models, which are MAD and have inclination i ≤ 30°. They have accretion rate (5.2–9.5) × 10−9 M ⊙ yr−1, bolometric luminosity (6.8–9.2) × 1035 erg s−1, and outflow power (1.3–4.8) × 1038 erg s−1. We also find that all models with i ≥ 70° fail at least two constraints, as do all models with equal ion and electron temperature; exploratory, nonthermal model sets tend to have higher 2.2 μm flux density; and the population of cold electrons is limited by X-ray constraints due to the risk of bremsstrahlung overproduction. Finally, we discuss physical and numerical limitations of the models, highlighting the possible importance of kinetic effects and duration of the simulations."
GARRETT JOHNSON,First M87 Event Horizon Telescope results. VII. Polarization of the ring,"In 2017 April, the Event Horizon Telescope (EHT) observed the near-horizon region around the supermassive black hole at the core of the M87 galaxy. These 1.3 mm wavelength observations revealed a compact asymmetric ring-like source morphology. This structure originates from synchrotron emission produced by relativistic plasma located in the immediate vicinity of the black hole. Here we present the corresponding linear-polarimetric EHT images of the center of M87. We find that only a part of the ring is significantly polarized. The resolved fractional linear polarization has a maximum located in the southwest part of the ring, where it rises to the level of ∼15%. The polarization position angles are arranged in a nearly azimuthal pattern. We perform quantitative measurements of relevant polarimetric properties of the compact emission and find evidence for the temporal evolution of the polarized source structure over one week of EHT observations. The details of the polarimetric data reduction and calibration methodology are provided. We carry out the data analysis using multiple independent imaging and modeling techniques, each of which is validated against a suite of synthetic data sets. The gross polarimetric structure and its apparent evolution with time are insensitive to the method used to reconstruct the image. These polarimetric images carry information about the structure of the magnetic fields responsible for the synchrotron emission. Their physical interpretation is discussed in an accompanying publication."
GARRETT JOHNSON,First M87 Event Horizon Telescope results. VIII. Magnetic field structure near The Event Horizon,"Event Horizon Telescope (EHT) observations at 230 GHz have now imaged polarized emission around the supermassive black hole in M87 on event-horizon scales. This polarized synchrotron radiation probes the structure of magnetic fields and the plasma properties near the black hole. Here we compare the resolved polarization structure observed by the EHT, along with simultaneous unresolved observations with the Atacama Large Millimeter/submillimeter Array, to expectations from theoretical models. The low fractional linear polarization in the resolved image suggests that the polarization is scrambled on scales smaller than the EHT beam, which we attribute to Faraday rotation internal to the emission region. We estimate the average density n_e ∼ 10^4–7 cm^−3, magnetic field strength B ∼ 1–30 G, and electron temperature T_e ∼ (1–12) × 10^10 K of the radiating plasma in a simple one-zone emission model. We show that the net azimuthal linear polarization pattern may result from organized, poloidal magnetic fields in the emission region. In a quantitative comparison with a large library of simulated polarimetric images from general relativistic magnetohydrodynamic (GRMHD) simulations, we identify a subset of physical models that can explain critical features of the polarimetric EHT observations while producing a relativistic jet of sufficient power. The consistent GRMHD models are all of magnetically arrested accretion disks, where near-horizon magnetic fields are dynamically important. We use the models to infer a mass accretion rate onto the black hole in M87 of (3–20) × 10^−4 M ⊙ yr^−1."
GARRETT JOHNSON,Resolving the inner parsec of the blazar J1924–2914 with the event horizon telescope,"The blazar J1924–2914 is a primary Event Horizon Telescope (EHT) calibrator for the Galactic center’s black hole Sagittarius A*. Here we present the first total and linearly polarized intensity images of this source obtained with the unprecedented 20 μas resolution of the EHT. J1924–2914 is a very compact flat-spectrum radio source with strong optical variability and polarization. In April 2017 the source was observed quasi-simultaneously with the EHT (April 5–11), the Global Millimeter VLBI Array (April 3), and the Very Long Baseline Array (April 28), giving a novel view of the source at four observing frequencies, 230, 86, 8.7, and 2.3 GHz. These observations probe jet properties from the subparsec to 100 pc scales. We combine the multifrequency images of J1924–2914 to study the source morphology. We find that the jet exhibits a characteristic bending, with a gradual clockwise rotation of the jet projected position angle of about 90° between 2.3 and 230 GHz. Linearly polarized intensity images of J1924–2914 with the extremely fine resolution of the EHT provide evidence for ordered toroidal magnetic fields in the blazar compact core."
GARRETT JOHNSON,A universal power-law prescription for variability from synthetic images of black hole accretion flows,"We present a framework for characterizing the spatiotemporal power spectrum of the variability expected from the horizon-scale emission structure around supermassive black holes, and we apply this framework to a library of general relativistic magnetohydrodynamic (GRMHD) simulations and associated general relativistic ray-traced images relevant for Event Horizon Telescope (EHT) observations of Sgr A*. We find that the variability power spectrum is generically a red-noise process in both the temporal and spatial dimensions, with the peak in power occurring on the longest timescales and largest spatial scales. When both the time-averaged source structure and the spatially integrated light-curve variability are removed, the residual power spectrum exhibits a universal broken power-law behavior. On small spatial frequencies, the residual power spectrum rises as the square of the spatial frequency and is proportional to the variance in the centroid of emission. Beyond some peak in variability power, the residual power spectrum falls as that of the time-averaged source structure, which is similar across simulations; this behavior can be naturally explained if the variability arises from a multiplicative random field that has a steeper high-frequency power-law index than that of the time-averaged source structure. We briefly explore the ability of power spectral variability studies to constrain physical parameters relevant for the GRMHD simulations, which can be scaled to provide predictions for black holes in a range of systems in the optically thin regime. We present specific expectations for the behavior of the M87* and Sgr A* accretion flows as observed by the EHT."
GARRETT JOHNSON,Millimeter light curves of Sagittarius A* observed during the 2017 Event Horizon Telescope campaign,"The Event Horizon Telescope (EHT) observed the compact radio source, Sagittarius A* (Sgr A*), in the Galactic Center on 2017 April 5–11 in the 1.3 mm wavelength band. At the same time, interferometric array data from the Atacama Large Millimeter/submillimeter Array and the Submillimeter Array were collected, providing Sgr A* light curves simultaneous with the EHT observations. These data sets, complementing the EHT very long baseline interferometry, are characterized by a cadence and signal-to-noise ratio previously unattainable for Sgr A* at millimeter wavelengths, and they allow for the investigation of source variability on timescales as short as a minute. While most of the light curves correspond to a low variability state of Sgr A*, the April 11 observations follow an X-ray flare and exhibit strongly enhanced variability. All of the light curves are consistent with a red-noise process, with a power spectral density (PSD) slope measured to be between −2 and −3 on timescales between 1 minute and several hours. Our results indicate a steepening of the PSD slope for timescales shorter than 0.3 hr. The spectral energy distribution is flat at 220 GHz, and there are no time lags between the 213 and 229 GHz frequency bands, suggesting low optical depth for the event horizon scale source. We characterize Sgr A*’s variability, highlighting the different behavior observed just after the X-ray flare, and use Gaussian process modeling to extract a decorrelation timescale and a PSD slope. We also investigate the systematic calibration uncertainties by analyzing data from independent data reduction pipelines."
GARRETT JOHNSON,Selective dynamical imaging of interferometric data,"Recent developments in very long baseline interferometry (VLBI) have made it possible for the Event Horizon Telescope (EHT) to resolve the innermost accretion flows of the largest supermassive black holes on the sky. The sparse nature of the EHT’s (u, v)-coverage presents a challenge when attempting to resolve highly time-variable sources. We demonstrate that the changing (u, v)-coverage of the EHT can contain regions of time over the course of a single observation that facilitate dynamical imaging. These optimal time regions typically have projected baseline distributions that are approximately angularly isotropic and radially homogeneous. We derive a metric of coverage quality based on baseline isotropy and density that is capable of ranking array configurations by their ability to produce accurate dynamical reconstructions. We compare this metric to existing metrics in the literature and investigate their utility by performing dynamical reconstructions on synthetic data from simulated EHT observations of sources with simple orbital variability. We then use these results to make recommendations for imaging the 2017 EHT Sgr A* data set."
GARRETT JOHNSON,First Sagittarius A* Event Horizon Telescope results. VI. Testing the black hole metric,"Astrophysical black holes are expected to be described by the Kerr metric. This is the only stationary, vacuum, axisymmetric metric, without electromagnetic charge, that satisfies Einstein’s equations and does not have pathologies outside of the event horizon. We present new constraints on potential deviations from the Kerr prediction based on 2017 EHT observations of Sagittarius A* (Sgr A*). We calibrate the relationship between the geometrically defined black hole shadow and the observed size of the ring-like images using a library that includes both Kerr and non-Kerr simulations. We use the exquisite prior constraints on the mass-to-distance ratio for Sgr A* to show that the observed image size is within ∼10% of the Kerr predictions. We use these bounds to constrain metrics that are parametrically different from Kerr, as well as the charges of several known spacetimes. To consider alternatives to the presence of an event horizon, we explore the possibility that Sgr A* is a compact object with a surface that either absorbs and thermally reemits incident radiation or partially reflects it. Using the observed image size and the broadband spectrum of Sgr A*, we conclude that a thermal surface can be ruled out and a fully reflective one is unlikely. We compare our results to the broader landscape of gravitational tests. Together with the bounds found for stellar-mass black holes and the M87 black hole, our observations provide further support that the external spacetimes of all black holes are described by the Kerr metric, independent of their mass."
GARRETT JOHNSON,Polarimetric properties of Event Horizon Telescope targets from ALMA,"We present the results from a full polarization study carried out with the Atacama Large Millimeter/submillimeter Array (ALMA) during the first Very Long Baseline Interferometry (VLBI) campaign, which was conducted in 2017 April in the λ3 mm and λ1.3 mm bands, in concert with the Global mm-VLBI Array (GMVA) and the Event Horizon Telescope (EHT), respectively. We determine the polarization and Faraday properties of all VLBI targets, including Sgr A*, M87, and a dozen radio-loud active galactic nuclei (AGNs), in the two bands at several epochs in a time window of 10 days. We detect high linear polarization fractions (2%–15%) and large rotation measures (RM &gt; 103.3–105.5 rad m−2), confirming the trends of previous AGN studies at millimeter wavelengths. We find that blazars are more strongly polarized than other AGNs in the sample, while exhibiting (on average) order-of-magnitude lower RM values, consistent with the AGN viewing angle unification scheme. For Sgr A* we report a mean RM of (−4.2 ± 0.3) × 105 rad m−2 at 1.3 mm, consistent with measurements over the past decade and, for the first time, an RM of (–2.1 ± 0.1) × 105 rad m−2 at 3 mm, suggesting that about half of the Faraday rotation at 1.3 mm may occur between the 3 mm photosphere and the 1.3 mm source. We also report the first unambiguous measurement of RM toward the M87 nucleus at millimeter wavelengths, which undergoes significant changes in magnitude and sign reversals on a one year timescale, spanning the range from −1.2 to 0.3 × 105 rad m−2 at 3 mm and −4.1 to 1.5 × 105 rad m−2 at 1.3 mm. Given this time variability, we argue that, unlike the case of Sgr A*, the RM in M87 does not provide an accurate estimate of the mass accretion rate onto the black hole. We put forward a two-component model, comprised of a variable compact region and a static extended region, that can simultaneously explain the polarimetric properties observed by both the EHT (on horizon scales) and ALMA (which observes the combined emission from both components). These measurements provide critical constraints for the calibration, analysis, and interpretation of simultaneously obtained VLBI data with the EHT and GMVA."
GARRETT JOHNSON,"First Sagittarius A* Event Horizon Telescope results. IV. Variability, morphology, and black hole mass","In this paper we quantify the temporal variability and image morphology of the horizon-scale emission from Sgr A*, as observed by the EHT in 2017 April at a wavelength of 1.3 mm. We find that the Sgr A* data exhibit variability that exceeds what can be explained by the uncertainties in the data or by the effects of interstellar scattering. The magnitude of this variability can be a substantial fraction of the correlated flux density, reaching ∼100% on some baselines. Through an exploration of simple geometric source models, we demonstrate that ring-like morphologies provide better fits to the Sgr A* data than do other morphologies with comparable complexity. We develop two strategies for fitting static geometric ring models to the time-variable Sgr A* data; one strategy fits models to short segments of data over which the source is static and averages these independent fits, while the other fits models to the full data set using a parametric model for the structural variability power spectrum around the average source structure. Both geometric modeling and image-domain feature extraction techniques determine the ring diameter to be 51.8 ± 2.3 μas (68% credible intervals), with the ring thickness constrained to have an FWHM between ∼30% and 50% of the ring diameter. To bring the diameter measurements to a common physical scale, we calibrate them using synthetic data generated from GRMHD simulations. This calibration constrains the angular size of the gravitational radius to be 4.8_-0.7^+1.4 μas, which we combine with an independent distance measurement from maser parallaxes to determine the mass of Sgr A* to be 4.0_-0.6^+10^6 M⊙."
GARRETT JOHNSON,"First Sagittarius A* Event Horizon Telescope results. II. EHT and multiwavelength observations, data processing, and calibration","We present Event Horizon Telescope (EHT) 1.3 mm measurements of the radio source located at the position of the supermassive black hole Sagittarius A* (Sgr A*), collected during the 2017 April 5–11 campaign. The observations were carried out with eight facilities at six locations across the globe. Novel calibration methods are employed to account for Sgr A*'s flux variability. The majority of the 1.3 mm emission arises from horizon scales, where intrinsic structural source variability is detected on timescales of minutes to hours. The effects of interstellar scattering on the image and its variability are found to be subdominant to intrinsic source structure. The calibrated visibility amplitudes, particularly the locations of the visibility minima, are broadly consistent with a blurred ring with a diameter of ∼50 μas, as determined in later works in this series. Contemporaneous multiwavelength monitoring of Sgr A* was performed at 22, 43, and 86 GHz and at near-infrared and X-ray wavelengths. Several X-ray flares from Sgr A* are detected by Chandra, one at low significance jointly with Swift on 2017 April 7 and the other at higher significance jointly with NuSTAR on 2017 April 11. The brighter April 11 flare is not observed simultaneously by the EHT but is followed by a significant increase in millimeter flux variability immediately after the X-ray outburst, indicating a likely connection in the emission physics near the event horizon. We compare Sgr A*’s broadband flux during the EHT campaign to its historical spectral energy distribution and find that both the quiescent emission and flare emission are consistent with its long-term behavior."
GARRETT JOHNSON,Event Horizon Telescope imaging of the archetypal blazar 3C 279 at an extreme 20 microarcsecond resolution,"3C 279 is an archetypal blazar with a prominent radio jet that show broadband flux density variability across the entire electromagnetic spectrum. We use an ultra-high angular resolution technique – global Very Long Baseline Interferometry (VLBI) at 1.3 mm (230 GHz) – to resolve the innermost jet of 3C 279 in order to study its fine-scale morphology close to the jet base where highly variable γ-ray emission is thought to originate, according to various models. The source was observed during four days in April 2017 with the Event Horizon Telescope at 230 GHz, including the phased Atacama Large Millimeter/submillimeter Array (ALMA), at an angular resolution of ∼20 μas (at a redshift of z = 0.536 this corresponds to ∼0.13 pc  ∼ 1700 Schwarzschild radii with a black hole mass M<jats:sub>BH</jats:sub> = 8 × 10^8 M_⊙). Imaging and model-fitting techniques were applied to the data to parameterize the fine-scale source structure and its variation. We find a multicomponent inner jet morphology with the northernmost component elongated perpendicular to the direction of the jet, as imaged at longer wavelengths. The elongated nuclear structure is consistent on all four observing days and across different imaging methods and model-fitting techniques, and therefore appears robust. Owing to its compactness and brightness, we associate the northern nuclear structure as the VLBI “core”. This morphology can be interpreted as either a broad resolved jet base or a spatially bent jet. We also find significant day-to-day variations in the closure phases, which appear most pronounced on the triangles with the longest baselines. Our analysis shows that this variation is related to a systematic change of the source structure. Two inner jet components move non-radially at apparent speeds of ∼15 c and ∼20 c (∼1.3 and ∼1.7 μas day^−1, respectively), which more strongly supports the scenario of traveling shocks or instabilities in a bent, possibly rotating jet. The observed apparent speeds are also coincident with the 3C 279 large-scale jet kinematics observed at longer (cm) wavelengths, suggesting no significant jet acceleration between the 1.3 mm core and the outer jet. The intrinsic brightness temperature of the jet components are ≲10^10 K, a magnitude or more lower than typical values seen at ≥7 mm wavelengths. The low brightness temperature and morphological complexity suggest that the core region of 3C 279 becomes optically thin at short (mm) wavelengths."
GARRETT JOHNSON,First M87 Event Horizon Telescope results. II. Array and instrumentation,"The Event Horizon Telescope (EHT) is a very long baseline interferometry (VLBI) array that comprises millimeter- and submillimeter-wavelength telescopes separated by distances comparable to the diameter of the Earth. At a nominal operating wavelength of ~1.3 mm, EHT angular resolution (λ/D) is ~25 μas, which is sufficient to resolve nearby supermassive black hole candidates on spatial and temporal scales that correspond to their event horizons. With this capability, the EHT scientific goals are to probe general relativistic effects in the strong-field regime and to study accretion and relativistic jet formation near the black hole boundary. In this Letter we describe the system design of the EHT, detail the technology and instrumentation that enable observations, and provide measures of its performance. Meeting the EHT science objectives has required several key developments that have facilitated the robust extension of the VLBI technique to EHT observing wavelengths and the production of instrumentation that can be deployed on a heterogeneous array of existing telescopes and facilities. To meet sensitivity requirements, high-bandwidth digital systems were developed that process data at rates of 64 gigabit s−1, exceeding those of currently operating cm-wavelength VLBI arrays by more than an order of magnitude. Associated improvements include the development of phasing systems at array facilities, new receiver installation at several sites, and the deployment of hydrogen maser frequency standards to ensure coherent data capture across the array. These efforts led to the coordination and execution of the first Global EHT observations in 2017 April, and to event-horizon-scale imaging of the supermassive black hole candidate in M87."
GARRETT JOHNSON,First M87 Event Horizon Telescope results. I. The shadow of the supermassive black hole,"When surrounded by a transparent emission region, black holes are expected to reveal a dark shadow caused by gravitational light bending and photon capture at the event horizon. To image and study this phenomenon, we have assembled the Event Horizon Telescope, a global very long baseline interferometry array observing at a wavelength of 1.3 mm. This allows us to reconstruct event-horizon-scale images of the supermassive black hole candidate in the center of the giant elliptical galaxy M87. We have resolved the central compact radio source as an asymmetric bright emission ring with a diameter of 42 ± 3 μas, which is circular and encompasses a central depression in brightness with a flux ratio gsim10:1. The emission ring is recovered using different calibration and imaging schemes, with its diameter and width remaining stable over four different observations carried out in different days. Overall, the observed image is consistent with expectations for the shadow of a Kerr black hole as predicted by general relativity. The asymmetry in brightness in the ring can be explained in terms of relativistic beaming of the emission from a plasma rotating close to the speed of light around a black hole. We compare our images to an extensive library of ray-traced general-relativistic magnetohydrodynamic simulations of black holes and derive a central mass of M = (6.5 ± 0.7) × 10^9 M ⊙. Our radio-wave observations thus provide powerful evidence for the presence of supermassive black holes in centers of galaxies and as the central engines of active galactic nuclei. They also present a new tool to explore gravity in its most extreme limit and on a mass scale that was so far not accessible."
GARRETT JOHNSON,First Sagittarius A* Event Horizon Telescope results. III. Imaging of the Galactic center supermassive black hole,"We present the first event-horizon-scale images and spatiotemporal analysis of Sgr A* taken with the Event Horizon Telescope in 2017 April at a wavelength of 1.3 mm. Imaging of Sgr A* has been conducted through surveys over a wide range of imaging assumptions using the classical CLEAN algorithm, regularized maximum likelihood methods, and a Bayesian posterior sampling method. Different prescriptions have been used to account for scattering effects by the interstellar medium toward the Galactic center. Mitigation of the rapid intraday variability that characterizes Sgr A* has been carried out through the addition of a “variability noise budget” in the observed visibilities, facilitating the reconstruction of static full-track images. Our static reconstructions of Sgr A* can be clustered into four representative morphologies that correspond to ring images with three different azimuthal brightness distributions and a small cluster that contains diverse nonring morphologies. Based on our extensive analysis of the effects of sparse (u, v)-coverage, source variability, and interstellar scattering, as well as studies of simulated visibility data, we conclude that the Event Horizon Telescope Sgr A* data show compelling evidence for an image that is dominated by a bright ring of emission with a ring diameter of ∼50 μas, consistent with the expected “shadow” of a 4 × 106 M⊙ black hole in the Galactic center located at a distance of 8 kpc."
GARRETT JOHNSON,Characterizing and mitigating intraday variability: reconstructing source structure in accreting black holes with mm-VLBI,"The extraordinary physical resolution afforded by the Event Horizon Telescope has opened a window onto the astrophysical phenomena unfolding on horizon scales in two known black holes, M87* and Sgr A*. However, with this leap in resolution has come a new set of practical complications. Sgr A* exhibits intraday variability that violates the assumptions underlying Earth aperture synthesis, limiting traditional image reconstruction methods to short timescales and data sets with very sparse (u, v) coverage. We present a new set of tools to detect and mitigate this variability. We develop a data-driven, model-agnostic procedure to detect and characterize the spatial structure of intraday variability. This method is calibrated against a large set of mock data sets, producing an empirical estimator of the spatial power spectrum of the brightness fluctuations. We present a novel Bayesian noise modeling algorithm that simultaneously reconstructs an average image and statistical measure of the fluctuations about it using a parameterized form for the excess variance in the complex visibilities not otherwise explained by the statistical errors. These methods are validated using a variety of simulated data, including general relativistic magnetohydrodynamic simulations appropriate for Sgr A* and M87*. We find that the reconstructed source structure and variability are robust to changes in the underlying image model. We apply these methods to the 2017 EHT observations of M87*, finding evidence for variability across the EHT observing campaign. The variability mitigation strategies presented are widely applicable to very long baseline interferometry observations of variable sources generally, for which they provide a data-informed averaging procedure and natural characterization of inter-epoch image consistency."
GARRETT JOHNSON,The polarized image of a synchrotron-emitting ring of gas orbiting a black hole,"Synchrotron radiation from hot gas near a black hole results in a polarized image. The image polarization is determined by effects including the orientation of the magnetic field in the emitting region, relativistic motion of the gas, strong gravitational lensing by the black hole, and parallel transport in the curved spacetime. We explore these effects using a simple model of an axisymmetric, equatorial accretion disk around a Schwarzschild black hole. By using an approximate expression for the null geodesics derived by Beloborodov and conservation of the Walker–Penrose constant, we provide analytic estimates for the image polarization. We test this model using currently favored general relativistic magnetohydrodynamic simulations of M87*, using ring parameters given by the simulations. For a subset of these with modest Faraday effects, we show that the ring model broadly reproduces the polarimetric image morphology. Our model also predicts the polarization evolution for compact flaring regions, such as those observed from Sgr A* with GRAVITY. With suitably chosen parameters, our simple model can reproduce the EVPA pattern and relative polarized intensity in Event Horizon Telescope images of M87*. Under the physically motivated assumption that the magnetic field trails the fluid velocity, this comparison is consistent with the clockwise rotation inferred from total intensity images."
GARRETT JOHNSON,First Sagittarius A* Event Horizon Telescope results. I. The shadow of the supermassive black hole in the center of the Milky Way,"We present the first Event Horizon Telescope (EHT) observations of Sagittarius A* (Sgr A*), the Galactic center source associated with a supermassive black hole. These observations were conducted in 2017 using a global interferometric array of eight telescopes operating at a wavelength of λ = 1.3 mm. The EHT data resolve a compact emission region with intrahour variability. A variety of imaging and modeling analyses all support an image that is dominated by a bright, thick ring with a diameter of 51.8 ± 2.3 μas (68% credible interval). The ring has modest azimuthal brightness asymmetry and a comparatively dim interior. Using a large suite of numerical simulations, we demonstrate that the EHT images of Sgr A* are consistent with the expected appearance of a Kerr black hole with mass ∼4 × 106 M ⊙, which is inferred to exist at this location based on previous infrared observations of individual stellar orbits, as well as maser proper-motion studies. Our model comparisons disfavor scenarios where the black hole is viewed at high inclination (i &gt; 50°), as well as nonspinning black holes and those with retrograde accretion disks. Our results provide direct evidence for the presence of a supermassive black hole at the center of the Milky Way, and for the first time we connect the predictions from dynamical measurements of stellar orbits on scales of 103–105 gravitational radii to event-horizon-scale images and variability. Furthermore, a comparison with the EHT results for the supermassive black hole M87* shows consistency with the predictions of general relativity spanning over three orders of magnitude in central mass."
MICHAEL ALBRO,Raman spectroscopy reveals new insights into the zonal organization of native and tissue-engineered articular cartilage,"Tissue architecture is intimately linked with its functions, and loss of tissue organization is often associated with pathologies. The intricate depth-dependent extracellular matrix (ECM) arrangement in articular cartilage is critical to its biomechanical functions. In this study, we developed a Raman spectroscopic imaging approach to gain new insight into the depth-dependent arrangement of native and tissue-engineered articular cartilage using bovine tissues and cells. Our results revealed previously unreported tissue complexity into at least six zones above the tidemark based on a principal component analysis and k-means clustering analysis of the distribution and orientation of the main ECM components. Correlation of nanoindentation and Raman spectroscopic data suggested that the biomechanics across the tissue depth are influenced by ECM microstructure rather than composition. Further, Raman spectroscopy together with multivariate analysis revealed changes in the collagen, glycosaminoglycan, and water distributions in tissue-engineered constructs over time. These changes were assessed using simple metrics that promise to instruct efforts toward the regeneration of a broad range of tissues with native zonal complexity and functional performance."
MICHAEL ALBRO,Raman spectroscopic imaging for quantification of depth-dependent and local heterogeneities in native and engineered cartilage,"Articular cartilage possesses a remarkable, mechanically-robust extracellular matrix (ECM) that is organized and distributed throughout the tissue to resist physiologic strains and provide low friction during articulation. The ability to characterize the make-up and distribution of the cartilage ECM is critical to both understand the process by which articular cartilage undergoes disease-related degeneration and to develop novel tissue repair strategies to restore tissue functionality. However, the ability to quantitatively measure the spatial distribution of cartilage ECM constituents throughout the tissue has remained a major challenge. In this experimental investigation, we assessed the analytical ability of Raman micro-spectroscopic imaging to semi-quantitatively measure the distribution of the major ECM constituents in cartilage tissues. Raman spectroscopic images were acquired of two distinct cartilage tissue types that possess large spatial ECM gradients throughout their depth: native articular cartilage explants and large engineered cartilage tissue constructs. Spectral acquisitions were processed via multivariate curve resolution to decompose the ""fingerprint"" range spectra (800-1800 cm-1) to the component spectra of GAG, collagen, and water, giving rise to the depth dependent concentration profile of each constituent throughout the tissues. These Raman spectroscopic acquired-profiles exhibited strong agreement with profiles independently acquired via direct biochemical assaying of spatial tissue sections. Further, we harness this spectroscopic technique to evaluate local heterogeneities through the depth of cartilage. This work represents a powerful analytical validation of the accuracy of Raman spectroscopic imaging measurements of the spatial distribution of biochemical components in a biological tissue and shows that it can be used as a valuable tool for quantitatively measuring the distribution and organization of ECM constituents in native and engineered cartilage tissue specimens."
DENNIS WUERTHNER,Introduction,
DENNIS WUERTHNER,"Unhappy confucians, take heed! reading seoljam kim siseup’s geumo sinhwa as anti-religious propaganda-fiction","Against the backdrop of different texts from the collected writings of Kim Siseup (dharma-name Seoljam), this article offers an against-the-grain reading of Kim’s famous collection of strange tales Geumo sinhwa (New Tales of the Golden Turtle). It is hypothesized that Kim’s life as well as his fictional and nonfictional literature can be viewed in the tradition of earlier Chinese and Korean anti-Buddhist Neo-Confucian thinkers such as Cheng Hao, Zhu Xi, or Jeong Dojeon. Through close-reading and by discussing such issues as funerary rites, burial practices, “unhappy” Confucians, and the persuasive power of storytelling, the author aims to show that Geumo sinhwa may be understood as a piece of narrative anti-religious propaganda-fiction meant to dissuade a specific 1460s younger Korean Neo-Confucian readership from turning toward seemingly soothing religion, and as an agenda-driven work designed to thwart a revival of Buddhism on the state level."
CHARALAMPOS TSOURAKAKIS,Opinion dynamics with varying susceptibility to persuasion,"A long line of work in social psychology has studied variations in people's susceptibility to persuasion -- the extent to which they are willing to modify their opinions on a topic. This body of literature suggests an interesting perspective on theoretical models of opinion formation by interacting parties in a network: in addition to considering interventions that directly modify people's intrinsic opinions, it is also natural to consider interventions that modify people's susceptibility to persuasion. In this work, we adopt a popular model for social opinion dynamics, and we formalize the opinion maximization and minimization problems where interventions happen at the level of susceptibility. We show that modeling interventions at the level of susceptibility lead to an interesting family of new questions in network opinion dynamics. We find that the questions are quite different depending on whether there is an overall budget constraining the number of agents we can target or not. We give a polynomial-time algorithm for finding the optimal target-set to optimize the sum of opinions when there are no budget constraints on the size of the target-set. We show that this problem is NP-hard when there is a budget, and that the objective function is neither submodular nor supermodular. Finally, we propose a heuristic for the budgeted opinion optimization and show its efficacy at finding target-sets that optimize the sum of opinions compared on real world networks, including a Twitter network with real opinion estimates."
CHARALAMPOS TSOURAKAKIS,Opinion dynamics optimization by varying susceptibility,"A long line of work in social psychology has studied variations in people’s susceptibility to persuasion—the extent to which they are willing to modify their opinions on a topic. This body of literature suggests an interesting perspective on theoretical models of opinion formation by interacting parties in a network: in addition to considering interventions that directly modify people’s intrinsic opinions, it is also natural to consider interventions that modify people’s susceptibility to persuasion. In this work, motivated by this fact, we propose an influence optimization problem. Specifically, we adopt a popular model for social opinion dynamics, where each agent has some fixed innate opinion, and a resistance that measures the importance it places on its innate opinion; agents influence one another’s opinions through an iterative process. Under certain conditions, this iterative process converges to some equilibrium opinion vector. For the unbudgeted variant of the problem, the goal is to modify the resistance of any number of agents (within some given range) such that the sum of the equilibrium opinions is minimized; for the budgeted variant, in addition the algorithm is given upfront a restriction on the number of agents whose resistance may be modified. We prove that the objective function is in general non-convex. Hence, formulating the problem as a convex program as in an early version of this work (Abebe et al., KDD’18) might have potential correctness issues. We instead analyze the structure of the objective function, and show that any local optimum is also a global optimum, which is somehow surprising as the objective function might not be convex. Furthermore, we combine the iterative process and the local search paradigm to design very efficient algorithms that can solve the unbudgeted variant of the problem optimally on large-scale graphs containing millions of nodes. Finally, we propose and evaluate experimentally a family of heuristics for the budgeted variant of the problem."
CHARALAMPOS TSOURAKAKIS,Finding densest k-connected subgraphs,"Dense subgraph discovery is an important graph-mining primitive with a variety of real-world applications. One of the most well-studied optimization problems for dense subgraph discovery is the densest subgraph problem, where given an edge-weighted undirected graph G = (V, E, w) we are asked to find S ⊆ V that maximizes the density d (S), i.e., half the weighted average degree of the induced subgraph G [S]. This problem can be solved exactly in polynomial time and well-approximately in almost linear time. However, a densest subgraph has a structural drawback, namely, the subgraph may not be robust to vertex/edge failure. Indeed, a densest subgraph may not be well-connected, which implies that the subgraph may be disconnected by removing only a few vertices/edges within it. In this paper, we provide an algorithmic framework to find a dense subgraph that is well-connected in terms of vertex/edge connectivity. Specifically, we introduce the following problems: given a graph G = (V, E, w) and a positive integer/real k, we are asked to find S ⊆ V that maximizes the density d (S) under the constraint that G [S] is k-vertex/edge-connected. For both problems, we propose polynomial-time (bicriteria and ordinary) approximation algorithms, using classic Mader’s theorem in graph theory and its extensions."
CHARALAMPOS TSOURAKAKIS,TwitterMancer: predicting user interactions on Twitter,"This paper investigates the interplay between different types of user interactions on Twitter, with respect to predicting missing or unseen interactions. For example, given a set of retweet interactions between Twitter users, how accurately can we predict reply interactions? Is it more difficult to predict retweet or quote interactions between a pair of accounts? Also, how important is time locality, and which features of interaction patterns are most important to enable accurate prediction of specific Twitter interactions? Our empirical study of Twitter interactions contributes initial answers to these questions.We have crawled an extensive data set of Greek-speaking Twitter accounts and their follow, quote, retweet, reply interactions over a period of a month. We find we can accurately predict many interactions of Twitter users. Interestingly, the most predictive features vary with the user profiles, and are not the same across all users. For example, for a pair of users that interact with a large number of other Twitter users, we find that certain “higher-dimensional” triads, i.e., triads that involve multiple types of interactions, are very informative, whereas for less active Twitter users, certain in-degrees and out-degrees play a major role. Finally, we provide various other insights on Twitter user behavior. Our code and data are available at https://github.com/twittermancer."
CHARALAMPOS TSOURAKAKIS,Query-efficient correlation clustering,"Correlation clustering is arguably the most natural formulation of clustering. Given n objects and a pairwise similarity measure, the goal is to cluster the objects so that, to the best possible extent, similar objects are put in the same cluster and dissimilar objects are put in different clusters. A main drawback of correlation clustering is that it requires as input the Θ(n2) pairwise similarities. This is often infeasible to compute or even just to store. In this paper we study query-efficient algorithms for correlation clustering. Specifically, we devise a correlation clustering algorithm that, given a budget of Q queries, attains a solution whose expected number of disagreements is at most , where is the optimal cost for the instance. Its running time is O(Q), and can be easily made non-adaptive (meaning it can specify all its queries at the outset and make them in parallel) with the same guarantees. Up to constant factors, our algorithm yields a provably optimal trade-off between the number of queries Q and the worst-case error attained, even for adaptive algorithms. Finally, we perform an experimental study of our proposed method on both synthetic and real data, showing the scalability and the accuracy of our algorithm."
CHARALAMPOS TSOURAKAKIS,Flowless: extracting densest subgraphs without flow computations,"The problem of finding dense components of a graph is a major primitive in graph mining and data analysis. The densest subgraph problem (DSP) that asks to find a subgraph with maximum average degree forms a basic primitive in dense subgraph discovery with applications ranging from community detection to unsupervised discovery of biological network modules [16]. The DSP is exactly solvable in polynomial time using maximum flows [14, 17, 22]. Due to the high computational cost of maximum flows, Charikar’s greedy approximation algorithm is usually preferred in practice due to its linear time and linear space complexity [3, 8]. It constitutes a key algorithmic idea in scalable solutions for large-scale dynamic graphs [5, 7]. However, its output density can be a factor 2 off the optimal solution. In this paper we design Greedy++, an iterative peeling algorithm that improves upon the performance of Charikar’s greedy algorithm significantly. Our iterative greedy algorithm is able to output near-optimal and optimal solutions fast by adding a few more passes to Charikar’s greedy algorithm. Furthermore Greedy++ is more robust against the structural heterogeneities (e.g., skewed degree distributions) in real-world datasets. An additional property of our algorithm is that it is able to assess quickly, without computing maximum flows, whether Charikar’s approximation quality on a given graph instance is closer to the worst case theoretical guarantee of or to optimality. We also demonstrate that our method has significant efficiency advantage over the maximum flow based exact optimal algorithm. For example, our algorithm achieves ∼ 145 × speedup on average across a variety of real-world graphs while finding subgraphs of density that are at least 90% as dense as the densest subgraph."
CHARALAMPOS TSOURAKAKIS,Optimal learning of joint alignments with a faulty oracle,"We consider the following problem, which is useful in applications such as joint image and shape alignment. The goal is to recover n discrete variables gi ∈ {0, . . . , k − 1} (up to some global offset) given noisy observations of a set of their pairwise differences {(gi − gj) mod k}; specifically, with probability 1 k + 𝛿 for some 𝛿> 0 one obtains the correct answer, and with the remaining probability one obtains a uniformly random incorrect answer. We consider a learning-based formulation where one can perform a query to observe a pairwise difference, and the goal is to perform as few queries as possible while obtaining the exact joint alignment. We provide an easy-to-implement, time efficient algorithm that performs O (n lg n k𝛿^2 ) queries, and recovers the joint alignment with high probability. We also show that our algorithm is optimal by proving a general lower bound that holds for all non-adaptive algorithms. Our work improves significantly recent work by Chen and Cand´es [CC16], who view the problem as a constrained principal components analysis problem that can be solved using the power method. Specifically, our approach is simpler both in the algorithm and the analysis, and provides additional insights into the problem structure."
CHARALAMPOS TSOURAKAKIS,Clustering with a faulty oracle,"Clustering, i.e., finding groups in the data, is a problem that permeates multiple fields of science and engineering. Recently, the problem of clustering with a noisy oracle has drawn attention due to various applications including crowdsourced entity resolution [33], and predicting signs of interactions in large-scale online social networks [20, 21]. Here, we consider the following fundamental model for two clusters as proposed by Mitzenmacher and Tsourakakis [28], and Mazumdar and Saha [25]; there exist n items, belonging to two unknown groups. We are allowed to query any pair of nodes whether they belong to the same cluster or not, but the answer to the query is corrupted with some probability . Let 1 > δ = 1 − 2q > 0 be the bias. In this work, we provide a polynomial time algorithm that recovers all signs correctly with high probability in the presence of noise with queries. This is the best known result for this problem for all but tiny δ, improving on the current state-of-the-art due to Mazumdar and Saha [25]."
CHARALAMPOS TSOURAKAKIS,Node-differentially private estimation of the number of connected components,
CHARALAMPOS TSOURAKAKIS,Densest diverse subgraphs: how to plan a successful cocktail party with diversity,
CHARALAMPOS TSOURAKAKIS,Learning mixtures of Markov Chains with quality guarantees,
CHARALAMPOS TSOURAKAKIS,Markovletics: methods and a novel application for learning continuous-time Markov chain mixtures,"Sequential data naturally arises from user engagement on digital platforms like social media, music streaming services, and web navigation, encapsulating evolving user preferences and behaviors through continuous information streams. A notable unresolved task in stochastic processes is learning mixtures of continuous-time Markov chains (CTMCs). While there is progress in learning mixtures of discrete-time Markov chains with recovery guarantees [GKV16,ST23,KTT2023], the continuous scenario uncovers unique unexplored challenges. The intrigue in CTMC mixtures stems from their ability to model intricate continuous-time stochastic processes prevalent in various fields including social media, finance, and biology. In this study, we introduce a novel framework for exploring CTMCs, emphasizing the influence of observed trails' length and mixture parameters on problem regimes, which demands specific algorithms. Through thorough experimentation, we examine the impact of discretizing continuous-time trails on the learnability of the continuous-time mixture, given that these processes are often observed via discrete, resource-demanding observations. Our comparative analysis with leading methods explores sample complexity and the trade-off between the number of trails and their lengths, offering crucial insights for method selection in different problem instances. We apply our algorithms on an extensive collection of Lastfm's user-generated trails spanning three years, demonstrating the capability of our algorithms to differentiate diverse user preferences. We also pioneer the use of CTMC mixtures on a basketball passing dataset to unveil intricate offensive tactics of NBA teams. This underscores the pragmatic utility and versatility of our proposed framework."
MICHAEL WASSERMAN,The long-baseline neutrino experiment: exploring fundamental symmetries of the universe,"The preponderance of matter over antimatter in the early Universe, the dynamics of the supernova bursts that produced the heavy elements necessary for life and whether protons eventually decay --- these mysteries at the forefront of particle physics and astrophysics are key to understanding the early evolution of our Universe, its current state and its eventual fate. The Long-Baseline Neutrino Experiment (LBNE) represents an extensively developed plan for a world-class experiment dedicated to addressing these questions. LBNE is conceived around three central components: (1) a new, high-intensity neutrino source generated from a megawatt-class proton accelerator at Fermi National Accelerator Laboratory, (2) a near neutrino detector just downstream of the source, and (3) a massive liquid argon time-projection chamber deployed as a far detector deep underground at the Sanford Underground Research Facility. This facility, located at the site of the former Homestake Mine in Lead, South Dakota, is approximately 1,300 km from the neutrino source at Fermilab -- a distance (baseline) that delivers optimal sensitivity to neutrino charge-parity symmetry violation and mass ordering effects. This ambitious yet cost-effective design incorporates scalability and flexibility and can accommodate a variety of upgrades and contributions. With its exceptional combination of experimental configuration, technical capabilities, and potential for transformative discoveries, LBNE promises to be a vital facility for the field of particle physics worldwide, providing physicists from around the globe with opportunities to collaborate in a twenty to thirty year program of exciting science. In this document we provide a comprehensive overview of LBNE's scientific objectives, its place in the landscape of neutrino physics worldwide, the technologies it will incorporate and the capabilities it will possess."
LEONARDO MARTINEZ,MAGIC and H.E.S.S. detect VHE gamma rays from the blazar OT081 for the first time: a deep multiwavelength study,
CHRISTINE E PHILLIPS,Shared and distinct transcriptomic cell types across neocortical areas,"The neocortex contains a multitude of cell types that are segregated into layers and functionally distinct areas. To investigate the diversity of cell types across the mouse neocortex, here we analysed 23,822 cells from two areas at distant poles of the mouse neocortex: the primary visual cortex and the anterior lateral motor cortex. We define 133 transcriptomic cell types by deep, single-cell RNA sequencing. Nearly all types of GABA (γ-aminobutyric acid)-containing neurons are shared across both areas, whereas most types of glutamatergic neurons were found in one of the two areas. By combining single-cell RNA sequencing and retrograde labelling, we match transcriptomic types of glutamatergic neurons to their long-range projection specificity. Our study establishes a combined transcriptomic and projectional taxonomy of cortical cell types from functionally distinct areas of the adult mouse cortex."
CHRISTINE E PHILLIPS,Clinicopathological evaluation of chronic traumatic encephalopathy in players of American football,"IMPORTANCE: Players of American football may be at increased risk of long-term neurological conditions, particularly chronic traumatic encephalopathy (CTE). OBJECTIVE: To determine the neuropathological and clinical features of deceased football players with CTE. DESIGN, SETTING, AND PARTICIPANTS: Case series of 202 football players whose brains were donated for research. Neuropathological evaluations and retrospective telephone clinical assessments (including head trauma history) with informants were performed blinded. Online questionnaires ascertained athletic and military history. EXPOSURES: Participation in American football at any level of play. MAIN OUTCOMES AND MEASURES: Neuropathological diagnoses of neurodegenerative diseases, including CTE, based on defined diagnostic criteria; CTE neuropathological severity (stages I to IV or dichotomized into mild [stages I and II] and severe [stages III and IV]); informant-reported athletic history and, for players who died in 2014 or later, clinical presentation, including behavior, mood, and cognitive symptoms and dementia. RESULTS: Among 202 deceased former football players (median age at death, 66 years [interquartile range, 47-76 years]), CTE was neuropathologically diagnosed in 177 players (87%; median age at death, 67 years [interquartile range, 52-77 years]; mean years of football participation, 15.1 [SD, 5.2]), including 0 of 2 pre–high school, 3 of 14 high school (21%), 48 of 53 college (91%), 9 of 14 semiprofessional (64%), 7 of 8 Canadian Football League (88%), and 110 of 111 National Football League (99%) players. Neuropathological severity of CTE was distributed across the highest level of play, with all 3 former high school players having mild pathology and the majority of former college (27 [56%]), semiprofessional (5 [56%]), and professional (101 [86%]) players having severe pathology. Among 27 participants with mild CTE pathology, 26 (96%) had behavioral or mood symptoms or both, 23 (85%) had cognitive symptoms, and 9 (33%) had signs of dementia. Among 84 participants with severe CTE pathology, 75 (89%) had behavioral or mood symptoms or both, 80 (95%) had cognitive symptoms, and 71 (85%) had signs of dementia. CONCLUSIONS AND RELEVANCE: In a convenience sample of deceased football players who donated their brains for research, a high proportion had neuropathological evidence of CTE, suggesting that CTE may be related to prior participation in football."
EMILY PATRICIA STEPHEN,Characterizing dynamically evolving functional networks in humans with application to speech,"Understanding how communication between brain areas evolves to support dynamic function remains a fundamental challenge in neuroscience. One approach to this question is functional connectivity analysis, in which statistical coupling measures are employed to detect signatures of interactions between brain regions. Because the brain uses multiple communication mechanisms at different temporal and spatial scales, and because the neuronal signatures of communication are often weak, powerful connectivity inference methodologies require continued development specific to these challenges. Here we address the challenge of inferring task-related functional connectivity in brain voltage recordings. We first develop a framework for detecting changes in statistical coupling that occur reliably in a task relative to a baseline period. The framework characterizes the dynamics of connectivity changes, allows inference on multiple spatial scales, and assesses statistical uncertainty. This general framework is modular and applicable to a wide range of tasks and research questions. We demonstrate the flexibility of the framework in the second part of this thesis, in which we refine the coupling statistics and hypothesis tests to improve statistical power and test different proposed connectivity mechanisms. In particular, we introduce frequency domain coupling measures and define test statistics that exploit theoretical properties and capture known sampling variability. The resulting test statistics use correlation, coherence, canonical correlation, and canonical coherence to infer task-related changes in coupling. Because canonical correlation and canonical coherence are not commonly used in functional connectivity analyses, we derive the theoretical values and statistical estimators for these measures. In the third part of this thesis, we present a sample application of these techniques to electrocorticography data collected during an overt reading task. We discuss the challenges that arise with task-related human data, which is often noisy and underpowered, and present functional connectivity results in the context of traditional and contemporary within-electrode analytics. In two of nine subjects we observe time-domain and frequency-domain network changes that accord with theoretical models of information routing during motor processing. Taken together, this work contributes a methodological framework for inferring task-related functional connectivity across spatial and temporal scales, and supports insight into the rapid, dynamic functional coupling of human speech."
EMILY PATRICIA STEPHEN,Reproductive inequality in humans and other mammals,"To address claims of human exceptionalism, we determine where humans fit within the greater mammalian distribution of reproductive inequality. We show that humans exhibit lower reproductive skew (i.e., inequality in the number of surviving offspring) among males and smaller sex differences in reproductive skew than most other mammals, while nevertheless falling within the mammalian range. Additionally, female reproductive skew is higher in polygynous human populations than in polygynous nonhumans mammals on average. This patterning of skew can be attributed in part to the prevalence of monogamy in humans compared to the predominance of polygyny in nonhuman mammals, to the limited degree of polygyny in the human societies that practice it, and to the importance of unequally held rival resources to women's fitness. The muted reproductive inequality observed in humans appears to be linked to several unusual characteristics of our species-including high levels of cooperation among males, high dependence on unequally held rival resources, complementarities between maternal and paternal investment, as well as social and legal institutions that enforce monogamous norms."
EMILY PATRICIA STEPHEN,"The L 98-59 system: three transiting, terrestrial-size planets orbiting a nearby M dwarf","We report the Transiting Exoplanet Survey Satellite (TESS) discovery of three terrestrial-size planets transiting L 98-59 (TOI-175, TIC 307210830)—a bright M dwarf at a distance of 10.6 pc. Using the Gaia-measured distance and broadband photometry, we find that the host star is an M3 dwarf. Combined with the TESS transits from three sectors, the corresponding stellar parameters yield planet radii ranging from 0.8 R ⊕ to 1.6 R ⊕. All three planets have short orbital periods, ranging from 2.25 to 7.45 days with the outer pair just wide of a 2:1 period resonance. Diagnostic tests produced by the TESS Data Validation Report and the vetting package DAVE rule out common false-positive sources. These analyses, along with dedicated follow-up and the multiplicity of the system, lend confidence that the observed signals are caused by planets transiting L 98-59 and are not associated with other sources in the field. The L 98-59 system is interesting for a number of reasons: the host star is bright (V = 11.7 mag, K = 7.1 mag) and the planets are prime targets for further follow-up observations including precision radial-velocity mass measurements and future transit spectroscopy with the James Webb Space Telescope; the near-resonant configuration makes the system a laboratory to study planetary system dynamical evolution; and three planets of relatively similar size in the same system present an opportunity to study terrestrial planets where other variables (age, metallicity, etc.) can be held constant. L 98-59 will be observed in four more TESS sectors, which will provide a wealth of information on the three currently known planets and have the potential to reveal additional planets in the system."
EMILY PATRICIA STEPHEN,The first habitable-zone Earth-sized planet from TESS. II. Spitzer confirms TOI-700 d,"We present Spitzer 4.5 μm observations of the transit of TOI-700 d, a habitable-zone Earth-sized planet in a multiplanet system transiting a nearby M-dwarf star (TIC 150428135, 2MASS J06282325–6534456). TOI-700 d has a radius of 1.144_-0.061^+0.062R_⨁ and orbits within its host star's conservative habitable zone with a period of 37.42 days (T eq ~ 269 K). TOI-700 also hosts two small inner planets (R b = 1.037_-0.064^+0.065R_⨁ and R c = 2.65_-0.15^+0.16R_⨁) with periods of 9.98 and 16.05 days, respectively. Our Spitzer observations confirm the Transiting Exoplanet Survey Satellite (TESS) detection of TOI-700 d and remove any remaining doubt that it is a genuine planet. We analyze the Spitzer light curve combined with the 11 sectors of TESS observations and a transit of TOI-700 c from the LCOGT network to determine the full system parameters. Although studying the atmosphere of TOI-700 d is not likely feasible with upcoming facilities, it may be possible to measure the mass of TOI-700 d using state-of-the-art radial velocity (RV) instruments (expected RV semiamplitude of ~70 cm s^−1)."
MICHAEL SALINS,On the Smoluchowski-Kramers approximation for a system with infinite degrees of freedom exposed to a magnetic field,"We study the validity of the so-called Smoluchowski–Kramers approximation for a two dimensional system of stochastic partial differential equations, subject to a constant magnetic field. Since the small mass limit does not yield to the solution of the corresponding first order system, we regularize our problem by adding a small friction. We show that in this case the Smoluchowski–Kramers approximation holds. We also give a justification of the regularization, by showing that the regularized problems provide a good approximation to the original ones."
MICHAEL SALINS,Smoluchowski–Kramers approximation for the damped stochastic wave equation with multiplicative noise in any spatial dimension,We show that the solutions to the damped stochastic wave equation converge pathwise to the solution of a stochastic heat equation. This is called the Smoluchowski–Kramers approximation. Cerrai and Freidlin have previously demonstrated that this result holds in the cases where the system is exposed to additive noise in any spatial dimension or when the system is exposed to multiplicative noise and the spatial dimension is one. The current paper proves that the Smoluchowski–Kramers approximation is valid in any spatial dimension when the system is exposed to multiplicative noise.
MICHAEL SALINS,Metastability and exit problems for systems of stochastic reaction-diffusion equations,"In this paper we develop a metastability theory for a class of stochastic reaction-diffusion equations exposed to small multiplicative noise. We consider the case where the unperturbed reaction-diffusion equation features multiple asymptotically stable equilibria. When the system is exposed to small stochastic perturbations, it is likely to stay near one equilibrium for a long period of time, but will eventually transition to the neighborhood of another equilibrium. We are interested in studying the exit time from the full domain of attraction (in a function space) surrounding an equilibrium and therefore do not assume that the domain of attraction features uniform attraction to the equilibrium. This means that the boundary of the domain of attraction is allowed to contain saddles and limit cycles. Our method of proof is purely infinite dimensional, i.e., we do not go through finite dimensional approximations. In addition, we address the multiplicative noise case and we do not impose gradient type of assumptions on the nonlinearity. We prove large deviations logarithmic asymptotics for the exit time and for the exit shape, also characterizing the most probable set of shapes of solutions at the time of exit from the domain of attraction."
MICHAEL SALINS,Large deviations and averaging for systems of slow–fast reaction–diffusion equations,"We study a large deviation principle for a system of stochastic reaction--diffusion equations (SRDEs) with a separation of fast and slow components and small noise in the slow component. The derivation of the large deviation principle is based on the weak convergence method in infinite dimensions, which results in studying averaging for controlled SRDEs. By appropriate choice of the parameters, the fast process and the associated control that arises from the weak convergence method decouple from each other. We show that in this decoupling case one can use the weak convergence method to characterize the limiting process via a ""viable pair"" that captures the limiting controlled dynamics and the effective invariant measure simultaneously. The characterization of the limit of the controlled slow-fast processes in terms of viable pair enables us to obtain a variational representation of the large deviation action functional. Due to the infinite--dimensional nature of our set--up, the proof of tightness as well as the analysis of the limit process and in particular the proof of the large deviations lower bound is considerably more delicate here than in the finite--dimensional situation. Smoothness properties of optimal controls in infinite dimensions (a necessary step for the large deviations lower bound) need to be established. We emphasize that many issues that are present in the infinite dimensional case, are completely absent in finite dimensions."
MICHAEL SALINS,On the Smoluchowski Kramers approximation for SPDEs and its interplay with large deviations and long time behavior,"We discuss here the validity of the small mass limit (the so-called Smoluchowski-Kramers approximation) on a fixed time interval for a class of semi-linear stochastic wave equations, both in the case of the presence of a constant friction term and in the case of the presence of a constant magnetic field. We also consider the small mass limit in an infinite time interval and we see how the approximation is stable in terms of the invariant measure and of the large deviation estimates and the exit problem from a bounded domain of the space of square integrable functions."
MICHAEL SALINS,Rare event simulation via importance sampling for linear SPDE's,"The goal of this paper is to develop provably efficient importance sampling Monte Carlo methods for the estimation of rare events within the class of linear stochastic partial differential equations (SPDEs). We find that if a spectral gap of appropriate size exists, then one can identify a lower dimensional manifold where the rare event takes place. This allows one to build importance sampling changes of measures that perform provably well even pre-asymptotically (i.e. for small but non-zero size of the noise) without degrading in performance due to infinite dimensionality or due to long simulation time horizons. Simulation studies supplement and illustrate the theoretical results."
MICHAEL SALINS,Smoluchowski-Kramers approximation and large deviations for infinite-dimensional nongradient systems with applications to the exit problem,"In this paper, we study the quasi-potential for a general class of damped semilinear stochastic wave equations. We show that as the density of the mass converges to zero, the infimum of the quasi-potential with respect to all possible velocities converges to the quasi-potential of the corresponding stochastic heat equation, that one obtains from the zero mass limit. This shows in particular that the Smoluchowski–Kramers approximation is not only valid for small time, but in the zero noise limit regime, can be used to approximate long-time behaviors such as exit time and exit place from a basin of attraction."
MICHAEL SALINS,On uniqueness and blowup properties for a class of second order SDEs,"As the first step for approaching the uniqueness and blowup properties of the solutions of the stochastic wave equations with multiplicative noise, we analyze the conditions for the uniqueness and blowup properties of the solution (X𝗍,Y𝗍) of the equations dX𝗍=Y𝗍dt, dY𝗍=|X𝗍|ᵅdB𝗍, (X₀,Y₀)=(x₀,y₀). In particular, we prove that solutions are nonunique if 0<α<1 and (x₀,y₀)=(0,0) and unique if 1/2<α and (x₀,y₀)≠(0,0). We also show that blowup in finite time holds if α>1 and (x₀,y₀)≠(0,0)."
MICHAEL SALINS,Uniform large deviation principles for Banach space valued stochastic differential equations,"We prove a large deviation principle (LDP) for a general class of Banach space valued stochastic differential equations (SDE) that is uniform with respect to initial conditions in bounded subsets of the Banach space. A key step in the proof is showing that a uniform large deviation principle over compact sets is implied by a uniform over compact sets Laplace principle. Because bounded subsets of infinite dimensional Banach spaces are in general not relatively compact in the norm topology, we embed the Banach space into its double dual and utilize the weak-$\star $ compactness of closed bounded sets in the double dual space. We prove that a modified version of our stochastic differential equation satisfies a uniform Laplace principle over weak-$\star $ compact sets and consequently a uniform over bounded sets large deviation principle. We then transfer this result back to the original equation using a contraction principle. The main motivation for this uniform LDP is to generalize results of Freidlin and Wentzell concerning the behavior of finite dimensional SDEs. Here we apply the uniform LDP to study the asymptotics of exit times from bounded sets of Banach space valued small noise SDE, including reaction diffusion equations with multiplicative noise and $2$-dimensional stochastic Navier-Stokes equations with multiplicative noise."
MICHAEL SALINS,Equivalences and counterexamples between several definitions of the uniform large deviations principle,"This paper explores the equivalences between four definitions of uniform large deviations principles and uniform Laplace principles found in the literature. Counterexamples are presented to illustrate the differences between these definitions and specific conditions are described under which these definitions are equivalent to each other. A fifth definition called the equicontinuous uniform Laplace principle (EULP) is proposed and proven to be equivalent to Freidlin and Wentzell's definition of a uniform large deviations principle. Sufficient conditions that imply a measurable function of infinite dimensional Wiener process satisfies an EULP using the variational methods of Budhiraja, Dupuis and Maroulas are presented. This theory is applied to prove that a family of Hilbert space valued stochastic equations exposed to multiplicative noise satisfy a uniform large deviations principle that is uniform over all initial conditions in bounded subsets of the Hilbert space. This is an improvement over previous weak convergence methods which can only prove uniformity over compact sets."
MICHAEL SALINS,"Markov processes with spatial delay: path space characterization, occupation time and properties","In this paper, we study one-dimensional Markov processes with spatial delay. Since the seminal work of Feller, we know that virtually any one-dimensional, strong, homogeneous, continuous Markov process can be uniquely characterized via its infinitesimal generator and the generator’s domain of definition. Unlike standard diffusions like Brownian motion, processes with spatial delay spend positive time at a single point of space. Interestingly, the set of times that a delay process spends at its delay point is nowhere dense and forms a positive measure Cantor set. The domain of definition of the generator has restrictions involving second derivatives. In this paper we provide a pathwise characterization for processes with delay in terms of an SDE and an occupation time formula involving the symmetric local time. This characterization provides an explicit Doob–Meyer decomposition, demonstrating that such processes are semi-martingales and that all of stochastic calculus including Itô formula and Girsanov formula applies. We also establish an occupation time formula linking the time that the process spends at a delay point with its symmetric local time there. A physical example of a stochastic dynamical system with delay is lastly presented and analyzed."
MICHAEL SALINS,On the Smoluchowski-Kramers approximation for a system with infinite degrees of freedom exposed to a magnetic field,"We study the validity of the so-called Smoluchowski-Kramers approximation for a two dimensional system of stochastic partial differential equations, subject to a constant magnetic field. As the small mass limit does not yield to the solution of the corresponding first order system, we regularize our problem by adding a small friction. We show that in this case the Smoluchowski-Kramers approximation holds. We also give a justification of the regularization, by showing that the regularized problems provide a good approximation to the original ones."
MICHAEL SALINS,Moderate deviations for systems of slow-fast stochastic reaction-diffusion equations,"The goal of this paper is to study the Moderate Deviation Principle (MDP) for a system of stochastic reaction-diffusion equations with a time-scale separation in slow and fast components and small noise in the slow component. Based on weak convergence methods in infinite dimensions and related stochastic control arguments, we obtain an exact form for the moderate deviations rate function in different regimes as the small noise and time-scale separation parameters vanish. Many issues that come up due to the infinite dimensionality of the problem are completely absent in their finite-dimensional counterpart. In comparison to corresponding Large Deviation Principles, the moderate deviation scaling necessitates a more delicate approach to establishing tightness and properly identifying the limiting behavior of the underlying controlled problem. The latter involves regularity properties of a solution of an associated elliptic Kolmogorov equation on Hilbert space along with a finite-dimensional approximation argument."
MICHAEL SALINS,Systems of small-noise stochastic reaction-diffusion equations satisfy a large deviations principle that is uniform over all initial data,"This paper proves three uniform large deviations results for a system of stochastic reaction--diffusion equations exposed to small multiplicative noise. If the reaction term can be written as the sum of a decreasing function and a Lipschitz continuous function and the multiplicative noise term is Lipschitz continuous, then the system satisfies a large deviations principle that is uniform over bounded subsets of initial data. Under the stronger assumption that the multiplicative noise term is uniformly bounded, the large deviations principle is uniform over all initial data, not just bounded sets. Alternatively, if the reaction term features super-linear dissipativity, like odd-degree polynomials with negative leading terms do, and the multiplicative noise term is unbounded, but does not grow too quickly, then the large deviations principle is uniform over all initial data."
MICHAEL SALINS,On dynamical systems perturbed by a null-recurrent fast motion: the continuous coefficient case with independent driving noises,"An ordinary differential equation perturbed by a null-recurrent diffusion will be considered in the case where the averaging type perturbation is strong only when a fast motion is close to the origin. The normal deviations of these solutions from the averaged motion are studied, and a central limit type theorem is proved. The limit process satisfies a linear equation driven by a Brownian motion time changed by the local time of the fast motion."
MICHAEL SALINS,Existence and uniqueness of global solutions to the stochastic heat equation with superlinear drift on an unbounded spatial domain,"We prove the existence and uniqueness of global solutions to the semilinear stochastic heat equation on an unbounded spatial domain with forcing terms that grow superlinearly and satisfy an Osgood condition R 1/|f(u)|du = +∞ along with additional restrictions. For example, consider the forcing f(u) = u log(e e + |u|) log(log(e e + |u|)). A new dynamic weighting procedure is introduced to control the solutions, which are unbounded in space."
MICHAEL SALINS,Global solutions for the stochastic reaction-diffusion equation with super-linear multiplicative noise and strong dissipativity,"A condition is identified that implies that solutions to the stochastic reaction-diffusion equation ∂u ∂t = Au + f(u) + σ(u)W˙ on a bounded spatial domain never explode. We consider the case where σ grows polynomially and f is polynomially dissipative, meaning that f strongly forces solutions toward finite values. This result demonstrates the role that the deterministic forcing term f plays in preventing explosion"
MICHAEL SALINS,Systems of small-noise stochastic reaction–diffusion equations satisfy a large deviations principle that is uniform over all initial data,"Large deviations principles characterize the exponential decay rates of the probabilities of rare events. Cerrai and Röckner (2004) proved that systems of stochastic reaction–diffusion equations satisfy a large deviations principle that is uniform over bounded sets of initial data. This paper proves uniform large deviations results for a system of stochastic reaction–diffusion equations in a more general setting than Cerrai and Röckner. Furthermore, this paper identifies two common situations where the large deviations principle is uniform over unbounded sets of initial data, enabling the characterization of Freidlin–Wentzell exit time and exit shape asymptotics from unbounded sets."
MICHAEL SALINS,Moderate deviations for systems of slow–fast stochastic reaction–diffusion equations,
MICHAEL SALINS,Global solutions to the stochastic reaction-diffusion equation with superlinear accretive reaction term and superlinear multiplicative noise term on a bounded spatial domain,We describe sufficient conditions on the reaction terms and multiplicative noise terms of a stochastic reaction-diffusion equation that guarantee that the solutions never explode. Both the reaction term and multiplicative noise terms are allowed to grow superlinearly.
MICHAEL SALINS,Importance sampling for stochastic reaction–diffusion equations in the moderate deviation regime,"We develop a provably efficient importance sampling scheme that estimates exit probabilities of solutions to small-noise stochastic reaction–diffusion equations from scaled neighborhoods of a stable equilibrium. The moderate deviation scaling allows for a local approximation of the nonlinear dynamics by their linearized version. In addition, we identify a finite-dimensional subspace where exits take place with high probability. Using stochastic control and variational methods we show that our scheme performs well both in the zero noise limit and pre-asymptotically. Simulation studies for stochastically perturbed bistable dynamics illustrate the theoretical results."
CATHI A THOMAS,Highly challenging balance program reduces fall rate in Parkinson disease,"BACKGROUND AND PURPOSE: There is a paucity of effective treatment options to reduce falls in Parkinson disease (PD). Although a variety of rehabilitative approaches have been shown to improve balance, evidence of a reduction in falls has been mixed. Prior balance trials suggest that programs with highly challenging exercises had superior outcomes. We investigated the effects of a theory-driven, progressive, highly challenging group exercise program on fall rate, balance, and fear of falling. METHODS: Twenty-three subjects with PD participated in this randomized cross-over trial. Subjects were randomly allocated to 3 months of active balance exercises or usual care followed by the reverse. During the active condition, subjects participated in a progressive, highly challenging group exercise program twice weekly for 90 minutes. Outcomes included a change in fall rate over the 3-month active period and differences in balance (Mini-Balance Evaluation Systems Test [Mini-BESTest]), and fear of falling (Falls Efficacy Scale-International [FES-I]) between active and usual care conditions. RESULTS: The effect of time on falls was significant (regression coefficient = -0.015 per day, P < 0.001). The estimated rate ratio comparing incidence rates at time points 1 month apart was 0.632 (95% confidence interval, 0.524-0.763). Thus, there was an estimated 37% decline in fall rate per month (95% confidence interval, 24%-48%). Improvements were also observed on the Mini-BESTest (P = 0.037) and FES-I (P = 0.059). DISCUSSION AND CONCLUSIONS: The results of this study show that a theory-based, highly challenging, and progressive exercise program was effective in reducing falls, improving balance, and reducing fear of falling in PD.Video abstract available for more insights from the authors (see Supplemental Digital Content 1, http://links.lww.com/JNPT/A120). TRIAL REGISTRATION: ClinicalTrials.gov NCT02302144."
CATHI A THOMAS,Cognitive-behavioral therapy for anxiety in Parkinson's disease,"Parkinson's disease (PD) is characterized by motor symptoms, but nonmotor symptoms also significantly impair daily functioning and reduce quality of life. Anxiety is prevalent and debilitating in PD, but remains understudied and undertreated. Much affective research in PD focuses on depression rather than anxiety, and as such, there are no evidence-based treatments for anxiety in this population. Cognitive-behavioral therapy (CBT) has shown promise for treating depression in PD and may be efficacious for anxiety. This exploratory study implemented a multiple-baseline single-case experimental design to evaluate the utility and feasibility of CBT for individuals with PD who also met criteria for a DSM-5 anxiety disorder ( n = 9). Participants were randomized to a 2-, 4-, or 6-week baseline phase, followed by 12 CBT sessions, and two post treatment assessments (immediately post treatment and 6-week follow-up). Multiple outcome measures of anxiety and depression were administered weekly during baseline and intervention. Weekly CBT sessions were conducted in-person ( n = 5) or via secure videoconferencing ( n = 4). At post treatment, seven of the nine participants showed significant reductions in anxiety and/or depression, with changes functionally related to treatment and most improvements maintained at 6-week follow-up. Effects of CBT on secondary outcomes varied across participants, with preliminary evidence for reduction in fear of falling. Adherence and retention were high, as were treatment satisfaction and acceptability. The findings of this pilot study provide preliminary evidence for the utility of CBT as a feasible treatment for anxiety and comorbid depressive symptoms in PD and highlight the potential of telehealth interventions for mood in this population."
CATHI A THOMAS,"IMPACT: The Journal of the Center for Interdisciplinary Teaching and Learning. Volume 10, Issue 1, Winter 2021","In this issue, a central question explored is, what kinds of programs and approaches can enhance interdisciplinary teaching and student learning? The essays in this issue explore this question in distinct and insightful ways. Grounded in her own experiences developing and running a Latin American and Caribbean Studies minor, one contributor argues that the minor enhances students’ interdisciplinary learning by exposing students to ethnic and racial difference, enriches student understanding of the depth and breadth of geo-cultural diversity, and prepares students to engage and work in multicultural settings. Writing together, two health educators highlight how various applications of service-learning pedagogy, such as traditional vs. online classroom approaches to service learning, application of service-learning strategies in the context of health education and health promotion, via internship courses and funded service projects, and the role of service-learning in enhancing core areas of responsibilities for certified health education specialists (CHES), can be a powerful interdisciplinary teaching and learning tool in health education. Finally, two faculty from the University of Tennessee interested in the Biglan/Becher taxonomy of disciplines, collaboratively show how the Biglan/Becher taxonomy of disciplines can be used to analyze disciplinary interrelationships in STEAM (STEM + Arts), with the ultimate goal of categorizing ways STEAM approaches can facilitate student learning in higher education. Our Impact book reviewers inform readers about new interdisciplinary and ground-breaking work in the under-researched area of parental incarceration, one author’s suggestions for how to teach undergraduates and still feel good about it, notes from a white professor in terms of teaching about race and racism in the college classroom, and, finally, another author’s arguments about how democracy can handle climate change."
CATHI A THOMAS,"Design of the WHIP-PD study: a phase II, twelve-month, dual-site, randomized controlled trial evaluating the effects of a cognitive-behavioral approach for promoting enhanced walking activity using mobile health technology in people with Parkinson-disease","BACKGROUND: Parkinson disease (PD) is a debilitating and chronic neurodegenerative disease resulting in ambulation difficulties. Natural walking activity often declines early in disease progression despite the relative stability of motor impairments. In this study, we propose a paradigm shift with a ""connected behavioral approach"" that targets real-world walking using cognitive-behavioral training and mobile health (mHealth) technology. METHODS/DESIGN: The Walking and mHealth to Increase Participation in Parkinson Disease (WHIP-PD) study is a twelve-month, dual site, two-arm, randomized controlled trial recruiting 148 participants with early to mid-stage PD. Participants will be randomly assigned to connected behavioral or active control conditions. Both conditions will include a customized program of goal-oriented walking, walking-enhancing strengthening exercises, and eight in-person visits with a physical therapist. Participants in the connected behavioral condition also will (1) receive cognitive-behavioral training to promote self-efficacy for routine walking behavior and (2) use a mHealth software application to manage their program and communicate remotely with their physical therapist. Active control participants will receive no cognitive-behavioral training and manage their program on paper. Evaluations will occur at baseline, three-, six-, and twelve-months and include walking assessments, self-efficacy questionnaires, and seven days of activity monitoring. Primary outcomes will include the change between baseline and twelve months in overall amount of walking activity (mean number of steps per day) and amount of moderate intensity walking activity (mean number of minutes per day in which > 100 steps were accumulated). Secondary outcomes will include change in walking capacity as measured by the six-minute walk test and ten-meter walk test. We also will examine if self-efficacy mediates change in amount of walking activity and if change in amount of walking activity mediates change in walking capacity. DISCUSSION: We expect this study to show the connected behavioral approach will be more effective than the active control condition in increasing the amount and intensity of real-world walking activity and improving walking capacity. Determining effective physical activity interventions for persons with PD is important for preserving mobility and essential for maintaining quality of life. Clinical trials registration NCT03517371, May 7, 2018. TRIAL REGISTRATION: ClinicalTrials.gov: NCT03517371. Date of registration: May 7, 2018. Protocol version: Original."
MICHAEL S KAIN,Rocaglates as dual-targeting agents for experimental cerebral malaria,"Cerebral malaria (CM) is a severe and rapidly progressing complication of infection by Plasmodium parasites that is associated with high rates of mortality and morbidity. Treatment options are currently few, and intervention with artemisinin (Art) has limited efficacy, a problem that is compounded by the emergence of resistance to Art in Plasmodium parasites. Rocaglates are a class of natural products derived from plants of the Aglaia genus that have been shown to interfere with eukaryotic initiation factor 4A (eIF4A), ultimately blocking initiation of protein synthesis. Here, we show that the rocaglate CR-1-31B perturbs association of Plasmodium falciparum eIF4A (PfeIF4A) with RNA. CR-1-31B shows potent prophylactic and therapeutic antiplasmodial activity in vivo in mouse models of infection with Plasmodium berghei (CM) and Plasmodium chabaudi (blood-stage malaria), and can also block replication of different clinical isolates of P. falciparum in human erythrocytes infected ex vivo, including drug-resistant P. falciparum isolates. In vivo, a single dosing of CR-1-31B in P. berghei-infected animals is sufficient to provide protection against lethality. CR-1-31B is shown to dampen expression of the early proinflammatory response in myeloid cells in vitro and dampens the inflammatory response in vivo in P. berghei-infected mice. The dual activity of CR-1-31B as an antiplasmodial and as an inhibitor of the inflammatory response in myeloid cells should prove extremely valuable for therapeutic intervention in human cases of CM."
ERIC DEVINE,Sub-continental-scale carbon stocks of individual trees in African drylands,"The distribution of dryland trees and their density, cover, size, mass and carbon content are not well known at sub-continental to continental scales1-14. This information is important for ecological protection, carbon accounting, climate mitigation and restoration efforts of dryland ecosystems15-18. We assessed more than 9.9 billion trees derived from more than 300,000 satellite images, covering semi-arid sub-Saharan Africa north of the Equator. We attributed wood, foliage and root carbon to every tree in the 0-1,000 mm year-1 rainfall zone by coupling field data19, machine learning20-22, satellite data and high-performance computing. Average carbon stocks of individual trees ranged from 0.54 Mg C ha-1 and 63 kg C tree-1 in the arid zone to 3.7 Mg C ha-1 and 98 kg tree-1 in the sub-humid zone. Overall, we estimated the total carbon for our study area to be 0.84 (±19.8%) Pg C. Comparisons with 14 previous TRENDY numerical simulation studies23 for our area found that the density and carbon stocks of scattered trees have been underestimated by three models and overestimated by 11 models, respectively. This benchmarking can help understand the carbon cycle and address concerns about land degradation24-29. We make available a linked database of wood mass, foliage mass, root mass and carbon stock of each tree for scientists, policymakers, dryland-restoration practitioners and farmers, who can use it to estimate farmland tree carbon stocks from tablets or laptops."
ERIC DEVINE,Inhibition of translation initiation factor eIF4a inactivates heat shock factor 1 (HSF1) and exerts anti-leukemia activity in AML,"Eukaryotic initiation factor 4A (eIF4A), the enzymatic core of the eIF4F complex essential for translation initiation, plays a key role in the oncogenic reprogramming of protein synthesis, and thus is a putative therapeutic target in cancer. As important component of its anticancer activity, inhibition of translation initiation can alleviate oncogenic activation of HSF1, a stress-inducible transcription factor that enables cancer cell growth and survival. Here, we show that primary acute myeloid leukemia (AML) cells exhibit the highest transcript levels of eIF4A1 compared to other cancer types. eIF4A inhibition by the potent and specific compound rohinitib (RHT) inactivated HSF1 in these cells, and exerted pronounced in vitro and in vivo anti-leukemia effects against progenitor and leukemia-initiating cells, especially those with FLT3-internal tandem duplication (ITD). In addition to its own anti-leukemic activity, genetic knockdown of HSF1 also sensitized FLT3-mutant AML cells to clinical FLT3 inhibitors, and this synergy was conserved in FLT3 double-mutant cells carrying both ITD and tyrosine kinase domain mutations. Consistently, the combination of RHT and FLT3 inhibitors was highly synergistic in primary FLT3-mutated AML cells. Our results provide a novel therapeutic rationale for co-targeting eIF4A and FLT3 to address the clinical challenge of treating FLT3-mutant AML."
KINH VU,Sessions,
KINH VU,Opening ceremony,
KINH VU,Opening ceremonies of The Conversation on Music Education,
KINH VU,A voice for the voiceless: lessons from a Hmong community's approach to music and self-expression,"Since the turn of the century, the world has witnessed a rise in violence promulgated on American soil. From terrorism to bullying, citizens across the United States are left wondering, ""What’s next or what can I do about it?"" I imagine that I am not alone in feeling powerlessness, out of control, and sometimes apathetic about the constant newsfeed heralding bad news both at home and abroad. With this kind ofuncertainty, it is no surprise that our students might feel just as overwhelmed and confused as we teachers. What, then, can music educators do to be a voice for and with students and how will their songs be a voice for those who will not or cannot sing songs of their own? This essay is an account of how I connect what I learned in a Hmong community of rappers and poets to music education, what critical pedagogy might mean for music educators, and how teachers can employ ""voice for the voiceless"" strategies with their ensemble or general music students."
KINH VU,Closing ceremony,
KINH VU,A pedagogy of love,
SUSAN WHITE,Task force on immigration and higher education in Central Massachusetts,"In August 2007, the Colleges of Worcester Consortium, Inc. created a task force to examine the issue of immigration and higher education in Central Massachusetts. It has become increasingly clear from recent demographic and economic studies and projections that the population in the northeast, and certainly in Central Massachusetts, is showing minimal growth. There is evidence that a decline in the “native-born” population is caused by significant out-migration due to a number of factors, including the high cost of living, limited career opportunities and a declining birth rate. The limited population growth that is evident is due primarily to the recent influx of immigrants to this area, with the most significant numbers in Worcester coming from Ghana, Brazil, the Dominican Republic, Kenya, El Salvador, Albania and Liberia. It is also clear that the area’s economy is becoming more knowledge-based with an increasing percentage of all new jobs requiring some form of postsecondary education. According to the 2007 Massachusetts Department of Workforce Development’s Job Vacancy Survey, 38 percent of current job vacancies in Massachusetts require an associate’s degree or higher. This represents an increase from 30 percent in 2003. Consequently, the level of education that the immigrant population attains is of vital importance to everyone—not only to immigrant students and their families but also to the economic well-being of the entire region. The Task Force was charged with researching the barriers to higher education faced by this new wave of immigrants and suggesting recommendations to address those barriers. The 36-member Task Force was made up of representatives from Consortium member institutions; federal, state and local governments; community and faithbased organizations; the Worcester Public Schools; the Massachusetts Board of Higher Education; and the Massachusetts Immigrant and Refugee Advocacy (MIRA) Coalition. Meetings were held over six months, during which the Task Force identified three main barriers faced by immigrant communities in accessing higher education, and sub-committees were created to work on each of these. Speakers were invited to present on topics of interest. Two public hearings were held, the first of which was conducted at Worcester State College in October. It attracted community representatives, as well as college and high school faculty and administrators. The second hearing, held at the downtown branch of Quinsigamond Community College (QCC) in December, was attended by immigrants (English for Speakers of Other Languages – ESOL and GED) students as well as QCC staff."
SUSAN WHITE,Role of Viral Hemagglutinin Glycosylation in Anti-Influenza Activities of Recombinant Surfactant Protein D,"BACKGROUND. Surfactant protein D (SP-D) plays an important role in innate defense against influenza A viruses (IAVs) and other pathogens. METHODS. We tested antiviral activities of recombinant human SP-D against a panel of IAV strains that vary in glycosylation sites on their hemagglutinin (HA). For these experiments a recombinant version of human SP-D of the Met11, Ala160 genotype was used after it was characterized biochemically and structurally. RESULTS. Oligosaccharides at amino acid 165 on the HA in the H3N2 subtype and 104 in the H1N1 subtype are absent in collectin-resistant strains developed in vitro and are important for mediating antiviral activity of SP-D; however, other glycans on the HA of these viral subtypes also are involved in inhibition by SP-D. H3N2 strains obtained shortly after introduction into the human population were largely resistant to SP-D, despite having the glycan at 165. H3N2 strains have become steadily more sensitive to SP-D over time in the human population, in association with addition of other glycans to the head region of the HA. In contrast, H1N1 strains were most sensitive in the 1970s-1980s and more recent strains have become less sensitive, despite retaining the glycan at 104. Two H5N1 strains were also resistant to inhibition by SP-D. By comparing sites of glycan attachment on sensitive vs. resistant strains, specific glycan sites on the head domain of the HA are implicated as important for inhibition by SP-D. Molecular modeling of the glycan attachment sites on HA and the carbohydrate recognition domain of SPD are consistent with these observations. CONCLUSION. Inhibition by SP-D correlates with presence of several glycan attachment sites on the HA. Pandemic and avian strains appear to lack susceptibility to SP-D and this could be a contributory factor to their virulence."
MICHAELA MCSWEENEY,Following logical realism where it leads,"Logical realism is the view that there is logical structure in the world. I argue that, if logical realism is true, then we are deeply ignorant of that logical structure: either we can’t know which of our logical concepts accurately capture it, or none of our logical concepts accurately capture it at all. I don’t suggest abandoning logical realism, but instead discuss how realists should adjust their methodology in the face of this ignorance."
MICHAELA MCSWEENEY,Logical realism and the metaphysics of logic,"‘Logical Realism’ is taken to mean many different things. I argue that if reality has a privileged structure, then a view I call metaphysical logical realism is true. The view says that, first, there is ‘One True Logic’; second, that the One True Logic is made true by the mind‐and‐language‐independent world; and third, that the mind‐and‐language‐independent world makes it the case that the One True Logic is better than any other logic at capturing the structure of reality. Along the way, I discuss a few alternatives, and clarify two distinct kinds of metaphysical logical realism."
MICHAELA MCSWEENEY,An epistemic account of metaphysical equivalence,
MICHAELA MCSWEENEY,Debunking logical grounding: distinguishing metaphysics from semantics,"Many philosophers take purportedly logical cases of ground (such as a true disjunction being grounded in its true disjunct(s)) to be obvious cases, and indeed such cases have been used to motivate the existence of and importance of ground. I argue against this. I do so by motivating two kinds of semantic determination relations. Intuitions of logical ground track these semantic relations. Moreover, our knowledge of semantics for (e.g.) first order logic can explain why we have such intuitions. And, I argue, neither semantic relation can be a species of ground, even on a quite broad conception of what ground is. Hence, without a positive argument for taking so-called ‘logical ground’ to be something distinct from a semantic determination relation, we should cease treating logical cases as cases of ground."
MICHAELA MCSWEENEY,Theories as recipes: third-order virtue and vice,"A basic way of evaluating metaphysical theories is to ask whether they give satisfying (not necessarily truthful!) answers to the questions they set out to resolve. I propose an account of “third-order” virtue that tells us what it takes for certain kinds of metaphysical theories to do so. We should think of these theories as recipes. I identify three good-making features of recipes and show that they translate to third-order theoretical virtues. I apply the view to two theories—mereological universalism and plenitudinous platonism—and draw out their third-order virtues and vices. One lesson is that there is an important difference between essentially and non-essentially third-order vicious theories. I also argue that if a theory is essentially third-order vicious, it cannot be assessed for more standard “second-order” theoretical virtues and vices, like parsimony. This motivates the idea that third-order virtues are distinct from second-order ones. Finally, I suggest that the relationship between truth, progress, and third-order virtue is more complex than it seems"
RACHEL EPSTEIN,"Screening for hepatitis C virus among adolescents and emerging adults in federally qualified health centers in the United States, 2012–2017","INTRODUCTION: Despite rising hepatitis C virus (HCV) incidence in the United States in recent years among young adults, little data describe HCV testing in youth. My objective was to characterize the HCV care cascade in adolescents and emerging adults in a large US sample and to describe the association between diagnosed substance use disorders (SUDs) and HCV testing. METHODS: In this retrospective cohort study, I describe HCV care cascade outcomes for youth 13–21 years old seen at least once from 1/2012–9/2017 at an OCHIN-participating federally qualified health center. Using electronic health record data, I analyzed odds of HCV testing by number of concurrent diagnosed SUDs associated with HCV risk (those associated with injection or intranasal use: opioids, amphetamines, and cocaine). RESULTS: Among 269,124 youth who met inclusion criteria, (54.7% female, 62.5% non-white, mean age [SD] at testing 18.5 [2.2] years), 6812 (2.5%) were tested for HCV antibody, 122/6812 (1.8%) of those tested were anti-HCV positive, and of anti-HCV positive youth, 75.4% had additional diagnostic testing. Only 1 had documented HCV treatment. Each additional HCV risk-associated SUD was associated with higher odds of HCV testing, particularly in younger (OR 9.12, 95% CI 6.78, 12.4 in 13–15 year-olds, and OR 8.37, 95% CI 7.48, 9.36 in 16–18 year-olds) compared with older youth (OR 3.9, 95% CI 3.59, 4.24 in 19–21 year-olds). CONCLUSION: This study highlights important gaps in recommended HCV testing during the current opioid crisis. As the first step in the care cascade, addressing missed testing opportunities is critical for reducing hepatitis C burden."
KATHERINE JENNINGS,Evaluation of association of HNF1B variants with diverse cancers: collaborative analysis of data from 19 genome-wide association studies,"BACKGROUND. Genome-wide association studies have found type 2 diabetes-associated variants in the HNF1B gene to exhibit reciprocal associations with prostate cancer risk. We aimed to identify whether these variants may have an effect on cancer risk in general versus a specific effect on prostate cancer only. METHODOLOGY/PRINCIPAL FINDINGS. In a collaborative analysis, we collected data from GWAS of cancer phenotypes for the frequently reported variants of HNF1B, rs4430796 and rs7501939, which are in linkage disequilibrium (r2=0.76, HapMap CEU). Overall, the analysis included 16 datasets on rs4430796 with 19,640 cancer cases and 21,929 controls; and 21 datasets on rs7501939 with 26,923 cases and 49,085 controls. Malignancies other than prostate cancer included colorectal, breast, lung and pancreatic cancers, and melanoma. Meta-analysis showed large between-dataset heterogeneity that was driven by different effects in prostate cancer and other cancers. The per-T2D-risk-allele odds ratios (95% confidence intervals) for rs4430796 were 0.79 (0.76, 0.83)] per G allele for prostate cancer (p<10-15 for both); and 1.03 (0.99, 1.07) for all other cancers. Similarly for rs7501939 the per-T2D-risk-allele odds ratios (95% confidence intervals) were 0.80 (0.77, 0.83) per T allele for prostate cancer (p<10-15 for both); and 1.00 (0.97, 1.04) for all other cancers. No malignancy other than prostate cancer had a nominally statistically significant association. CONCLUSIONS/SIGNIFICANCE. The examined HNF1B variants have a highly specific effect on prostate cancer risk with no apparent association with any of the other studied cancer types."
JAMES CHAPMAN,1998 Editorial Panel,
JAMES CHAPMAN,"Bostonia: v. 32, no. 1-3",
BARBARA L EDWARDS,"Bostonia: v. 17, no. 1-9",
BARBARA L EDWARDS,CD4+ and CD8+ T cells and antibodies are associated with protection against Delta vaccine breakthrough infection: a nested case-control study within the PITCH study,"Defining correlates of protection against severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) vaccine breakthrough infection informs vaccine policy for booster doses and future vaccine designs. Existing studies demonstrate humoral correlates of protection, but the role of T cells in protection is still unclear. In this study, we explore antibody and T cell immune responses associated with protection against Delta variant vaccine breakthrough infection in a well-characterized cohort of UK Healthcare Workers (HCWs). We demonstrate evidence to support a role for CD4+ and CD8+ T cells as well as antibodies against Delta vaccine breakthrough infection. In addition, our results suggest a potential role for cross-reactive T cells in vaccine breakthrough."
BARBARA L EDWARDS,The long-baseline neutrino experiment: exploring fundamental symmetries of the universe,"The preponderance of matter over antimatter in the early Universe, the dynamics of the supernova bursts that produced the heavy elements necessary for life and whether protons eventually decay --- these mysteries at the forefront of particle physics and astrophysics are key to understanding the early evolution of our Universe, its current state and its eventual fate. The Long-Baseline Neutrino Experiment (LBNE) represents an extensively developed plan for a world-class experiment dedicated to addressing these questions. LBNE is conceived around three central components: (1) a new, high-intensity neutrino source generated from a megawatt-class proton accelerator at Fermi National Accelerator Laboratory, (2) a near neutrino detector just downstream of the source, and (3) a massive liquid argon time-projection chamber deployed as a far detector deep underground at the Sanford Underground Research Facility. This facility, located at the site of the former Homestake Mine in Lead, South Dakota, is approximately 1,300 km from the neutrino source at Fermilab -- a distance (baseline) that delivers optimal sensitivity to neutrino charge-parity symmetry violation and mass ordering effects. This ambitious yet cost-effective design incorporates scalability and flexibility and can accommodate a variety of upgrades and contributions. With its exceptional combination of experimental configuration, technical capabilities, and potential for transformative discoveries, LBNE promises to be a vital facility for the field of particle physics worldwide, providing physicists from around the globe with opportunities to collaborate in a twenty to thirty year program of exciting science. In this document we provide a comprehensive overview of LBNE's scientific objectives, its place in the landscape of neutrino physics worldwide, the technologies it will incorporate and the capabilities it will possess."
DEEPAK KUMAR,"SoK: hate, harassment, and the changing landscape of online abuse","We argue that existing security, privacy, and antiabuse protections fail to address the growing threat of online hate and harassment. In order for our community to understand and address this gap, we propose a taxonomy for reasoning about online hate and harassment. Our taxonomy draws on over 150 interdisciplinary research papers that cover disparate threats ranging from intimate partner violence to coordinated mobs. In the process, we identify seven classes of attacks—such as toxic content and surveillance—that each stem from different attacker capabilities and intents. We also provide longitudinal evidence from a three-year survey that hate and harassment is a pervasive, growing experience for online users, particularly for at-risk communities like young adults and people who identify as LGBTQ+. Responding to each class of hate and harassment requires a unique strategy and we highlight five such potential research directions that ultimately empower individuals, communities, and platforms to do so."
CHRISTOPHER SCHMITT,Genetic variation and gene expression across multiple tissues and developmental stages in a nonhuman primate,"By analyzing multitissue gene expression and genome-wide genetic variation data in samples from a vervet monkey pedigree, we generated a transcriptome resource and produced the first catalog of expression quantitative trait loci (eQTLs) in a nonhuman primate model. This catalog contains more genome-wide significant eQTLs per sample than comparable human resources and identifies sex- and age-related expression patterns. Findings include a master regulatory locus that likely has a role in immune function and a locus regulating hippocampal long noncoding RNAs (lncRNAs), whose expression correlates with hippocampal volume. This resource will facilitate genetic investigation of quantitative traits, including brain and behavioral phenotypes relevant to neuropsychiatric disorders."
CHRISTOPHER SCHMITT,Localized population divergence of vervet monkeys ( Chlorocebus spp.) in South Africa: Evidence from mtDNA,"OBJECTIVES—Vervet monkeys are common in most tree-rich areas of South Africa, but their absence from grassland and semi-desert areas of the country suggest potentially restricted and mosaic local population patterns that may have relevance to local phenotype patterns and selection. A portion of the mtDNA control region was sequenced to study patterns of genetic differentiation. MATERIALS AND METHODS—DNA was extracted and mtDNA sequences were obtained from 101 vervet monkeys at 15 localities which represent both an extensive (widely across the distribution range) and intensive (more than one troop at most of the localities) sampling strategy. Analyses utilized Arlequin 3.1, MEGA 6, BEAST v1.5.2 and Network V3.6.1 RESULTS—The dataset contained 26 distinct haplotypes, with six populations fixed for single haplotypes. Pairwise P-distance among population pairs showed significant differentiation among most population pairs, but with non-significant differences among populations within some regions. Populations were grouped into three broad clusters in a maximum likelihood phylogenetic tree and a haplotype network. These clusters correspond to (i) north-western, northern and northeastern parts of the distribution range as well as the northern coastal belt; (ii) central areas of the country; and (iii) southern part of the Indian Ocean coastal belt, and adjacent inland areas. DISCUSSION—Apparent patterns of genetic structure correspond to current and past distribution of suitable habitat, geographic barriers to gene flow, geographic distance and female philopatry. However, further work on nuclear markers and other genomic data is necessary to confirm these results."
CHRISTOPHER SCHMITT,The static allometry of sexual and non-sexual traits in vervet monkeys,"Sexual traits vary tremendously in static allometry. This variation may be explained in part by body size-related differences in the strength of selection. We tested this hypothesis in two populations of vervet monkeys, using estimates of the level of condition dependence for different morphological traits as a proxy for body size-related variation in the strength of selection. In support of the hypothesis, we found that the steepness of allometric slopes increased with the level of condition dependence. One trait of particular interest, the penis, had shallow allometric slopes and low levels of condition dependence, in agreement with one of the most consistent patterns yet detected in the study of allometry, namely that of genitalia exhibiting shallow allometries."
CHRISTOPHER SCHMITT,ACE2 and TMPRSS2 variation in savanna monkeys (Chlorocebus spp.): potential risk for zoonotic/anthroponotic transmission of SARS-CoV-2 and a potential model for functional studies,"The COVID-19 pandemic, caused by the coronavirus SARS-CoV-2, has devastated health infrastructure around the world. Both ACE2 (an entry receptor) and TMPRSS2 (used by the virus for spike protein priming) are key proteins to SARS-CoV-2 cell entry, enabling progression to COVID-19 in humans. Comparative genomic research into critical ACE2 binding sites, associated with the spike receptor binding domain, has suggested that African and Asian primates may also be susceptible to disease from SARS-CoV-2 infection. Savanna monkeys (Chlorocebus spp.) are a widespread non-human primate with well-established potential as a bi-directional zoonotic/anthroponotic agent due to high levels of human interaction throughout their range in sub-Saharan Africa and the Caribbean. To characterize potential functional variation in savanna monkey ACE2 and TMPRSS2, we inspected recently published genomic data from 245 savanna monkeys, including 163 wild monkeys from Africa and the Caribbean and 82 captive monkeys from the Vervet Research Colony (VRC). We found several missense variants. One missense variant in ACE2 (X:14,077,550; Asp30Gly), common in Ch. sabaeus, causes a change in amino acid residue that has been inferred to reduce binding efficiency of SARS-CoV-2, suggesting potentially reduced susceptibility. The remaining populations appear as susceptible as humans, based on these criteria for receptor usage. All missense variants observed in wild Ch. sabaeus populations are also present in the VRC, along with two splice acceptor variants (at X:14,065,076) not observed in the wild sample that are potentially disruptive to ACE2 function. The presence of these variants in the VRC suggests a promising model for SARS-CoV-2 infection and vaccine and therapy development. In keeping with a One Health approach, characterizing actual susceptibility and potential for bi-directional zoonotic/anthroponotic transfer in savanna monkey populations may be an important consideration for controlling COVID-19 epidemics in communities with frequent human/non-human primate interactions that, in many cases, may have limited health infrastructure."
CHRISTOPHER SCHMITT,Variable responses of human and non-human primate gut microbiomes to a Western diet,"BACKGROUND: The human gut microbiota interacts closely with human diet and physiology. To better understand the mechanisms behind this relationship, gut microbiome research relies on complementing human studies with manipulations of animal models, including non-human primates. However, due to unique aspects of human diet and physiology, it is likely that host-gut microbe interactions operate differently in humans and non-human primates. RESULTS: Here, we show that the human microbiome reacts differently to a high-protein, high-fat Western diet than that of a model primate, the African green monkey, or vervet (Chlorocebus aethiops sabaeus). Specifically, humans exhibit increased relative abundance of Firmicutes and reduced relative abundance of Prevotella on a Western diet while vervets show the opposite pattern. Predictive metagenomics demonstrate an increased relative abundance of genes associated with carbohydrate metabolism in the microbiome of only humans consuming a Western diet. CONCLUSIONS: These results suggest that the human gut microbiota has unique properties that are a result of changes in human diet and physiology across evolution or that may have contributed to the evolution of human physiology. Therefore, the role of animal models for understanding the relationship between the human gut microbiota and host metabolism must be re-focused."
CHRISTOPHER SCHMITT,Comparative growth and static allometry in the genus Chlorocebus,"Characterizing variation in growth across populations is critical to understanding multiple aspects of development in primates, including within-taxon developmental plasticity and the evolution of life history patterns. Growth in wild primates has often been reported and directly compared across larger taxonomic groups and within social groups, but comparisons are rarely investigated across widely dispersed populations of a single taxon. With the Vervet Phenome-Genome Project and the International Vervet Research Consortium, we trapped 936 vervet monkeys of all ages representing three populations (Kenyan pygerythrus, South African pygerythrus, and sabaeus from St. Kitts & Nevis). We gathered 10 different body measurements from each including mass, body breadth and length, segmental limb lengths, and chest circumference. To gain a better understanding of how ontogenetic patterns vary in these populations, we calculated bivariate allometry coefficients, derived using PCA on log-transformed and z-standardized trait values, and compared them to isometric vector coefficients. Within all population samples, around weaning age most traits showed a negative allometric relationship to body length. As each population ages, however, distinct patterns emerge, showing population differences in onset and intensity of growth among traits. In concordance with other analyses on growth in these populations, our results suggest that there exist relative differences in patterns of growth between Chlorocebus populations, further suggesting selection for unique developmental pathways in each."
CHRISTOPHER SCHMITT,"Shifts in microbial diversity, composition, and functionality in the gut and genital microbiome during a natural SIV infection in vervet monkeys","BACKGROUND: The microbiota plays an important role in HIV pathogenesis in humans. Microbiota can impact health through several pathways such as increasing inflammation in the gut, metabolites of bacterial origin, and microbial translocation from the gut to the periphery which contributes to systemic chronic inflammation and immune activation and the development of AIDS. Unlike HIV-infected humans, SIV-infected vervet monkeys do not experience gut dysfunction, microbial translocation, and chronic immune activation and do not progress to immunodeficiency. Here, we provide the first reported characterization of the microbial ecosystems of the gut and genital tract in a natural nonprogressing host of SIV, wild vervet monkeys from South Africa. RESULTS: We characterized fecal, rectal, vaginal, and penile microbiomes in vervets from populations heavily infected with SIV from diverse locations across South Africa. Geographic site, age, and sex affected the vervet microbiome across different body sites. Fecal and vaginal microbiome showed marked stratification with three enterotypes in fecal samples and two vagitypes, which were predicted functionally distinct within each body site. External bioclimatic factors, biome type, and environmental temperature influenced microbiomes locally associated with vaginal and rectal mucosa. Several fecal microbial taxa were linked to plasma levels of immune molecules, for example, MIG was positively correlated with Lactobacillus and Escherichia/Shigella and Helicobacter, and IL-10 was negatively associated with Erysipelotrichaceae, Anaerostipes, Prevotella, and Anaerovibrio, and positively correlated with Bacteroidetes and Succinivibrio. During the chronic phase of infection, we observed a significant increase in gut microbial diversity, alterations in community composition (including a decrease in Proteobacteria/Succinivibrio in the gut) and functionality (including a decrease in genes involved in bacterial invasion of epithelial cells in the gut), and partial reversibility of acute infection-related shifts in microbial abundance observed in the fecal microbiome. As part of our study, we also developed an accurate predictor of SIV infection using fecal samples. CONCLUSIONS: The vervets infected with SIV and humans infected with HIV differ in microbial responses to infection. These responses to SIV infection may aid in preventing microbial translocation and subsequent disease progression in vervets, and may represent host microbiome adaptations to the virus. Video Abstract."
CHRISTOPHER SCHMITT,"SIVagm infection in wild African green monkeys from South Africa: epidemiology, natural history, and evolutionary considerations","Pathogenesis studies of SIV infection have not been performed to date in wild monkeys due to difficulty in collecting and storing samples on site and the lack of analytical reagents covering the extensive SIV diversity. We performed a large scale study of molecular epidemiology and natural history of SIVagm infection in 225 free-ranging AGMs from multiple locations in South Africa. SIV prevalence (established by sequencing pol, env, and gag) varied dramatically between infant/juvenile (7%) and adult animals (68%) (p,0.0001), and between adult females (78%) and males (57%). Phylogenetic analyses revealed an extensive genetic diversity, including frequent recombination events. Some AGMs harbored epidemiological linked viruses. Viruses infecting AGMs in the Free State, which are separated from those on the coastal side by the Drakensberg Mountains, formed a separate cluster in the phylogenetic trees; this observation supports a long standing presence of SIV in AGMs, at least from the time of their speciation to their Plio-Pleistocene migration. Specific primers/probes were synthesized based on the pol sequence data and viral loads (VLs) were quantified. VLs were of 104 –106 RNA copies/ml, in the range of those observed in experimentally-infected monkeys, validating the experimental approaches in natural hosts. VLs were significantly higher (107–108 RNA copies/ml) in 10 AGMs diagnosed as acutely infected based on SIV seronegativity (Fiebig II), which suggests a very active transmission of SIVagm in the wild. Neither cytokine levels (as biomarkers of immune activation) nor sCD14 levels (a biomarker of microbial translocation) were different between SIVinfected and SIV-uninfected monkeys. This complex algorithm combining sequencing and phylogeny, VL quantification, serology, and testing of surrogate markers of microbial translocation and immune activation permits a systematic investigation of the epidemiology, viral diversity and natural history of SIV infection in wild African natural hosts."
CHRISTOPHER SCHMITT,Transmission of Staphylococcus aureus from humans to green monkeys in The Gambia as revealed by whole-genome sequencing,"Staphylococcus aureus is an important pathogen of humans and animals. We genome sequenced 90 S. aureus isolates from The Gambia: 46 isolates from invasive disease in humans, 13 human carriage isolates, and 31 monkey carriage isolates. We inferred multiple anthroponotic transmissions of S. aureus from humans to green monkeys (Chlorocebus sabaeus) in The Gambia over different time scales. We report a novel monkey-associated clade of S. aureus that emerged from a human-to-monkey switch estimated to have occurred 2,700 years ago. Adaptation of this lineage to the monkey host is accompanied by the loss of phage-carrying genes that are known to play an important role in human colonization. We also report recent anthroponotic transmission of the well-characterized human lineages sequence type 6 (ST6) and ST15 to monkeys, probably because of steadily increasing encroachment of humans into the monkeys' habitat. Although we have found no evidence of transmission of S. aureus from monkeys to humans, as the two species come into ever-closer contact, there might be an increased risk of additional interspecies exchanges of potential pathogens. IMPORTANCE: The population structures of Staphylococcus aureus in humans and monkeys in sub-Saharan Africa have been previously described using multilocus sequence typing (MLST). However, these data lack the power to accurately infer details regarding the origin and maintenance of new adaptive lineages. Here, we describe the use of whole-genome sequencing to detect transmission of S. aureus between humans and nonhuman primates and to document the genetic changes accompanying host adaptation. We note that human-to-monkey switches tend to be more common than the reverse and that a novel monkey-associated clade is likely to have emerged from such a switch approximately 2,700 years ago. Moreover, analysis of the accessory genome provides important clues as to the genetic changes underpinning host adaptation and, in particular, shows that human-to-monkey switches tend to be associated with the loss of genes known to confer adaptation to the human host."
CHRISTOPHER SCHMITT,Ancient hybridization and strong adaptation to viruses across African vervet monkey populations,"Vervet monkeys are among the most widely distributed nonhuman primates, show considerable phenotypic diversity, and have long been an important biomedical model for a variety of human diseases and in vaccine research. Using whole-genome sequencing data from 163 vervets sampled from across Africa and the Caribbean, we find high diversity within and between taxa and clear evidence that taxonomic divergence was reticulate rather than following a simple branching pattern. A scan for diversifying selection across taxa identifies strong and highly polygenic selection signals affecting viral processes. Furthermore, selection scores are elevated in genes whose human orthologs interact with HIV and in genes that show a response to experimental simian immunodeficiency virus (SIV) infection in vervet monkeys but not in rhesus macaques, suggesting that part of the signal reflects taxon-specific adaptation to SIV."
CHRISTOPHER SCHMITT,Evidence of strong stabilizing effects on the evolution of boreoeutherian (Mammalia) dental proportions.,"The dentition is an extremely important organ in mammals with variation in timing and sequence of eruption, crown morphology, and tooth size enabling a range of behavioral, dietary, and functional adaptations across the class. Within this suite of variable mammalian dental phenotypes, relative sizes of teeth reflect variation in the underlying genetic and developmental mechanisms. Two ratios of postcanine tooth lengths capture the relative size of premolars to molars (premolar-molar module, PMM), and among the three molars (molar module component, MMC), and are known to be heritable, independent of body size, and to vary significantly across primates. Here, we explore how these dental traits vary across mammals more broadly, focusing on terrestrial taxa in the clade of Boreoeutheria (Euarchontoglires and Laurasiatheria). We measured the postcanine teeth of N = 1,523 boreoeutherian mammals spanning six orders, 14 families, 36 genera, and 49 species to test hypotheses about associations between dental proportions and phylogenetic relatedness, diet, and life history in mammals. Boreoeutherian postcanine dental proportions sampled in this study carry conserved phylogenetic signal and are not associated with variation in diet. The incorporation of paleontological data provides further evidence that dental proportions may be slower to change than is dietary specialization. These results have implications for our understanding of dental variation and dietary adaptation in mammals."
CHRISTOPHER SCHMITT,Adaptive genetic variation at three loci in South African vervet monkeys (Chlorocebus pygerythrus) and the role of selection within primates,"Vervet monkeys (Chlorocebus pygerythrus) are one of the most widely distributed non-human primate species found in South Africa. They occur across all the South African provinces, inhabiting a large variety of habitats. These habitats vary sufficiently that it can be assumed that various factors such as pathogen diversity could influence populations in different ways. In turn, these factors could lead to varied levels of selection at specific fitness linked loci. The Toll-like receptor (TLR) gene family, which play an integral role in vertebrate innate immunity, is a group of fitness linked loci which has been the focus of much research. In this study, we assessed the level of genetic variation at partial sequences of two TLR loci (TLR4 and 7) and a reproductively linked gene, acrosin (ACR), across the different habitat types within the vervet monkey distribution range. Gene variation and selection estimates were also made among 11–21 primate species. Low levels of genetic variation for all three gene regions were observed within vervet monkeys, with only two polymorphic sites identified for TLR4, three sites for TLR7 and one site for ACR. TLR7 variation was positively correlated with high mean annual rainfall, which was linked to increased pathogen abundance. The observed genetic variation at TLR4 might have been influenced by numerous factors including pathogens and climatic conditions. The ACR exonic regions showed no variation in vervet monkeys, which could point to the occurrence of a selective sweep. The TLR4 and TLR7 results for the among primate analyses was mostly in line with previous studies, indicating a higher rate of evolution for TLR4. Within primates, ACR coding regions also showed signs of positive selection, which was congruent with previous reports on mammals. Important additional information to the already existing vervet monkey knowledge base was gained from this study, which can guide future research projects on this highly researched taxon as well as help conservation agencies with future management planning involving possible translocations of this species."
CHRISTOPHER SCHMITT,The genome of the vervet ( Chlorocebus aethiops sabaeus ),"We describe a genome reference of the African green monkey or vervet (Chlorocebus aethiops). This member of the Old World monkey (OWM) superfamily is uniquely valuable for genetic investigations of simian immunodeficiency virus (SIV), for which it is the most abundant natural host species, and of a wide range of health-related phenotypes assessed in Caribbean vervets (C. a. sabaeus), whose numbers have expanded dramatically since Europeans introduced small numbers of their ancestors from West Africa during the colonial era. We use the reference to characterize the genomic relationship between vervets and other primates, the intra-generic phylogeny of vervet subspecies, and genome-wide structural variations of a pedigreed C. a. sabaeus population. Through comparative analyseswith human and rhesus macaque, we characterize at high resolution the unique chromosomal fission events that differentiate the vervets and their close relatives from most other catarrhine primates, in whom karyotype is highly conserved. We also provide a summary of transposable elements and contrast these with the rhesus macaque and human. Analysis of sequenced genomes representing each of the main vervet subspecies supports previously hypothesized relationships between these populations, which range across most of sub-Saharan Africa, while uncovering high levels of genetic diversity within each. Sequence-based analyses of major histocompatibility complex (MHC) polymorphisms reveal extremely low diversity in Caribbean C. a. sabaeus vervets, compared to vervets from putatively ancestral West African regions. In the C. a. sabaeus research population, we discover the first structural variations that are, in some cases, predicted to have a deleterious effect; future studies will determine the phenotypic impact of these variations."
CHRISTOPHER SCHMITT,Obesity and obesogenic growth are both highly heritable and modified by diet in a nonhuman primate model the African green monkey (Chlorocebus aethiops sabaeus),"OBJECTIVE: In humans, the ontogeny of obesity throughout the life course and the genetics underlying it has been historically difficult to study. We compared, in a non-human primate model, the lifelong growth trajectories of obese and non-obese adults to assess the heritability of and map potential genomic regions implicated in growth and obesity. STUDY POPULATION: A total of 905 African green monkeys, or vervets (Chlorocebus aethiops sabaeus) (472 females, 433 males) from a pedigreed captive colony. METHODS: We measured fasted body weight (BW), crown-to-rump length (CRL), body-mass index (BMI) and waist circumference (WC) from 2000 to 2015. We used a longitudinal clustering algorithm to detect obesogenic growth, and logistic growth curves implemented in nonlinear mixed effects models to estimate three growth parameters. We used maximum likelihood variance decomposition methods to estimate the genetic contributions to obesity-related traits and growth parameters, including a test for the effects of a calorie-restricted dietary intervention. We used multipoint linkage analysis to map implicated genomic regions. RESULTS: All measurements were significantly influenced by sex, and with the exception of WC, also influenced by maternal and post-natal diet. Chronic obesity outcomes were significantly associated with a pattern of extended growth duration with slow growth rates for BW. After accounting for environmental influences, all measurements were found to have a significant genetic component to variability. Linkage analysis revealed several regions suggested to be linked to obesity-related traits that are also implicated in human obesity and metabolic disorders. CONCLUSIONS: As in humans, growth patterns in vervets have a significant impact on adult obesity and are largely under genetic control with some evidence for maternal and dietary programming. These results largely mirror findings from human research, but reflect shorter developmental periods, suggesting that the vervet offers a strong genetic model for elucidating the ontogeny of human obesity."
CHRISTOPHER SCHMITT,Post-release survival rates and welfare of rehabilitated vervet monkeys in Malawi,"Research on primate rehabilitation-release (R&R) is limited, and released troop mortality is generally high. We investigated factors affecting survival and welfare of a rehabilitant troop of vervet monkeys (Chlorocebus pygerythrus rufoviridis) released in Malawi in 2016. Using 9 months of pre- and post-release data from the Lilongwe Wildlife Trust (LWT) and linear modeling, survival analysis, and social network analysis, we considered several potential factors influencing survival. The LWT troop survival rate was 36% and results suggest high ranking individuals, juveniles, and highly socially connected individuals were more likely to survive. Mortality patterns suggest released troops may benefit from platform feeders that encourage greater canopy use, more time at the release site before the rainy season when predation is more common, and predator-awareness training. Future studies using behavioral diversity to assess welfare should use detailed ethograms to capture unique behaviors. LWT’s extensive pre- and post-release monitoring provides vital insight into the troop’s survival. Other rehabilitation centers should follow this strategy to help improve primate R&R programs."
CHRISTOPHER SCHMITT,Tevatron-for-LHC report: preparations for discoveries,"This is the ""TeV4LHC"" report of the ""Physics Landscapes"" Working Group, focused on facilitating the start-up of physics explorations at the LHC by using the experience gained at the Tevatron. We present experimental and theoretical results that can be employed to probe various scenarios for physics beyond the Standard Model."
CHRISTOPHER SCHMITT,The long-baseline neutrino experiment: exploring fundamental symmetries of the universe,"The preponderance of matter over antimatter in the early Universe, the dynamics of the supernova bursts that produced the heavy elements necessary for life and whether protons eventually decay --- these mysteries at the forefront of particle physics and astrophysics are key to understanding the early evolution of our Universe, its current state and its eventual fate. The Long-Baseline Neutrino Experiment (LBNE) represents an extensively developed plan for a world-class experiment dedicated to addressing these questions. LBNE is conceived around three central components: (1) a new, high-intensity neutrino source generated from a megawatt-class proton accelerator at Fermi National Accelerator Laboratory, (2) a near neutrino detector just downstream of the source, and (3) a massive liquid argon time-projection chamber deployed as a far detector deep underground at the Sanford Underground Research Facility. This facility, located at the site of the former Homestake Mine in Lead, South Dakota, is approximately 1,300 km from the neutrino source at Fermilab -- a distance (baseline) that delivers optimal sensitivity to neutrino charge-parity symmetry violation and mass ordering effects. This ambitious yet cost-effective design incorporates scalability and flexibility and can accommodate a variety of upgrades and contributions. With its exceptional combination of experimental configuration, technical capabilities, and potential for transformative discoveries, LBNE promises to be a vital facility for the field of particle physics worldwide, providing physicists from around the globe with opportunities to collaborate in a twenty to thirty year program of exciting science. In this document we provide a comprehensive overview of LBNE's scientific objectives, its place in the landscape of neutrino physics worldwide, the technologies it will incorporate and the capabilities it will possess."
TAWNYA SMITH,Sessions,
TAWNYA SMITH,New perspectives on research,"The Graduate Research Session at the conference will be held on Thursday March 1st from 4-5pm. At this session, graduate students from institutions from across the state will present a series of lightning talks where each presenter will briefly share the purpose and findings of their research study, and share a few implications for music education practice. The graduate student panel will be seated in a circle in order to facilitate sharing. Non-presenting attendees will be seated in an outer circle which will then be integrated with the presenters during the Q&A portion of the session, in order promote the free-sharing of ideas between all in ttendance. In order to highlight a few examples of the exciting projects being presented, Yank’l Garcia and Nicholas Quigley, master’s students at Boston University, briefly introduce their research projects below. Please join us to learn about the fresh and exciting topics that graduate student researchers are focusing upon within the field of music education."
TAWNYA SMITH,Competitive comparison in music: influences upon self-efficacy beliefs by gender,"This study profiles gender differences in instrumental performance self-efficacy perceptions of high school students (N = 87) over the course of a three-day orchestra festival in which students competed against one another for rank-based seating and then rehearsed and performed as a group. Reported self-beliefs rose significantly for the sample over the course of the festival. Self-efficacy beliefs of females were significantly lower than those of males before the seating audition and first rehearsal, but were no longer different by the midpoint of the festival. Survey free-response data were coded according to Bandura's (1997 Bandura, A. 1997. Self-efficacy: The Exercise of Control. New York: W. H. Freeman.) four sources of self-efficacy. A 52% drop in the frequency of student comments regarding competitive comparison appeared at the same point in which female self-efficacy beliefs were no longer different from those of males. Results support past research to suggest that males and females may respond differently to rank-based competition versus social support."
TAWNYA SMITH,Research that serves: building active teacher-researcher partnerships,"The impact of scholarly research in education on educational practice in classrooms remains low (Admiraal, Buijs, Claessens, Honing, & Karkdijk, 2017). As a result, educational practices in schools remain tied to practical wisdom, rather than educational theories that have been developed and tested in classrooms. This research to practice gap, as it is widely known, is attributed to beliefs that scholars in higher education tend to examine problems that teachers in schools find irrelevant. Classroom teachers contend that because scholars’ primary purpose is to publish, they tend to aim toward generalizations rather than to focus on the improvement of relevant educational practice. Gore and Gitlin (2004) argued that existing tensions between researchers and practitioners may be related to long-standing traditions of framing educational research in such a way that classroom teachers are positioned as “users” rather than “producers” of knowledge."
TAWNYA SMITH,The self-directed learning of adult music students: a comparison of teacher approaches and student needs,"Adult music learners may expect to be more independent and therefore more inclined to engage in self-directed learning than younger learners; however, adults may not feel encouraged or supported to self-direct. In this qualitative study, the relationships between six adult instrumentalists and their teachers were examined using Grow’s Staged Self-Directed Learning (SSDL) Model to determine if there was congruence or a mismatch between individual student learning needs and their teachers’ strategies. Teachers reported a willingness to accommodate the self-direction needs of their students. Even in cases of teacher–student mismatch, more direction from the teacher was welcomed when they encountered technical difficulty or an unfamiliar style. Students who reported that they had little experience playing were more inclined to have low to moderate levels of self-direction, whereas more advanced players reported intermediate to high levels of self-direction. Cultural expectations were found to play an important role in determining if students desire to be self-directed learners. The quality of the teacher–student relationship and communication were both found to be an important determinant of successful collaboration. These findings suggest that the theoretical application of the SSDL model could provide teachers a means to assess and discern their adult students’ learning needs."
TAWNYA SMITH,"IMPACT: The Journal of the Center for Interdisciplinary Teaching and Learning. Volume 13, Issue 1, Winter 2024","Welcome to the Winter 2024 issue of Impact: The Journal of the Center of Interdisciplinary Teaching & Learning. The following essays explore interdisciplinary connections that link musical ideas and experiences to the environments that humans and non-human species inhabit. Readers will quickly note the range of approaches adopted in these essays, including insights from teacher training programs, psychology, critical theory, and the performing arts. Despite the differences, each essay is informed by an underlying assumption that musical engagement can help us make better sense of our current moment of ecological crisis."
TAWNYA SMITH,Research to Practice: Issues of Relevance for Massachusetts Music Educators,
TAWNYA SMITH,Research to practice: bridging the gap,
REBECCA COPELAND,Focus: Summer 2018,
JOSHUA PETERSON,"IMPACT: The Journal of the Center for Interdisciplinary Teaching and Learning. Volume 6, Issue 2, Summer 2017","In this issue, podcasts are looked at as a pedagogical game changer. Using the award-wining podcast Serial as their catalyst, this issue's essayists look at podcast's emerging role in higher education, how multimodal learning can help students find their voices, the podcast's place in the curriculum at a criminal justice college, and how podcasts can inspire students to reflectively assess their own writing. Our reviewers take a critical look at the podcasts Welcome to Night Vale and Revisionist History."
JOSHUA PETERSON,A super-earth and sub-neptune transiting the late-type M Dwarf LP 791-18,"Planets occur most frequently around cool dwarfs, but only a handful of specific examples are known to orbit the latest-type M stars. Using TESS photometry, we report the discovery of two planets transiting the low-mass star called LP 791-18 (identified by TESS as TOI 736). This star has spectral type M6V, effective temperature 2960 K, and radius 0.17 R ⊙, making it the third-coolest star known to host planets. The two planets straddle the radius gap seen for smaller exoplanets; they include a 1.1R ⊕ planet on a 0.95 day orbit and a 2.3R ⊕ planet on a 5 day orbit. Because the host star is small the decrease in light during these planets' transits is fairly large (0.4% and 1.7%). This has allowed us to detect both planets' transits from ground-based photometry, refining their radii and orbital ephemerides. In the future, radial velocity observations and transmission spectroscopy can both probe these planets' bulk interior and atmospheric compositions, and additional photometric monitoring would be sensitive to even smaller transiting planets."
AARON WILLIAM YOUNG,Event Horizon Telescope imaging of the archetypal blazar 3C 279 at an extreme 20 microarcsecond resolution,"3C 279 is an archetypal blazar with a prominent radio jet that show broadband flux density variability across the entire electromagnetic spectrum. We use an ultra-high angular resolution technique – global Very Long Baseline Interferometry (VLBI) at 1.3 mm (230 GHz) – to resolve the innermost jet of 3C 279 in order to study its fine-scale morphology close to the jet base where highly variable γ-ray emission is thought to originate, according to various models. The source was observed during four days in April 2017 with the Event Horizon Telescope at 230 GHz, including the phased Atacama Large Millimeter/submillimeter Array (ALMA), at an angular resolution of ∼20 μas (at a redshift of z = 0.536 this corresponds to ∼0.13 pc  ∼ 1700 Schwarzschild radii with a black hole mass M<jats:sub>BH</jats:sub> = 8 × 10^8 M_⊙). Imaging and model-fitting techniques were applied to the data to parameterize the fine-scale source structure and its variation. We find a multicomponent inner jet morphology with the northernmost component elongated perpendicular to the direction of the jet, as imaged at longer wavelengths. The elongated nuclear structure is consistent on all four observing days and across different imaging methods and model-fitting techniques, and therefore appears robust. Owing to its compactness and brightness, we associate the northern nuclear structure as the VLBI “core”. This morphology can be interpreted as either a broad resolved jet base or a spatially bent jet. We also find significant day-to-day variations in the closure phases, which appear most pronounced on the triangles with the longest baselines. Our analysis shows that this variation is related to a systematic change of the source structure. Two inner jet components move non-radially at apparent speeds of ∼15 c and ∼20 c (∼1.3 and ∼1.7 μas day^−1, respectively), which more strongly supports the scenario of traveling shocks or instabilities in a bent, possibly rotating jet. The observed apparent speeds are also coincident with the 3C 279 large-scale jet kinematics observed at longer (cm) wavelengths, suggesting no significant jet acceleration between the 1.3 mm core and the outer jet. The intrinsic brightness temperature of the jet components are ≲10^10 K, a magnitude or more lower than typical values seen at ≥7 mm wavelengths. The low brightness temperature and morphological complexity suggest that the core region of 3C 279 becomes optically thin at short (mm) wavelengths."
AARON WILLIAM YOUNG,First M87 Event Horizon Telescope results. II. Array and instrumentation,"The Event Horizon Telescope (EHT) is a very long baseline interferometry (VLBI) array that comprises millimeter- and submillimeter-wavelength telescopes separated by distances comparable to the diameter of the Earth. At a nominal operating wavelength of ~1.3 mm, EHT angular resolution (λ/D) is ~25 μas, which is sufficient to resolve nearby supermassive black hole candidates on spatial and temporal scales that correspond to their event horizons. With this capability, the EHT scientific goals are to probe general relativistic effects in the strong-field regime and to study accretion and relativistic jet formation near the black hole boundary. In this Letter we describe the system design of the EHT, detail the technology and instrumentation that enable observations, and provide measures of its performance. Meeting the EHT science objectives has required several key developments that have facilitated the robust extension of the VLBI technique to EHT observing wavelengths and the production of instrumentation that can be deployed on a heterogeneous array of existing telescopes and facilities. To meet sensitivity requirements, high-bandwidth digital systems were developed that process data at rates of 64 gigabit s−1, exceeding those of currently operating cm-wavelength VLBI arrays by more than an order of magnitude. Associated improvements include the development of phasing systems at array facilities, new receiver installation at several sites, and the deployment of hydrogen maser frequency standards to ensure coherent data capture across the array. These efforts led to the coordination and execution of the first Global EHT observations in 2017 April, and to event-horizon-scale imaging of the supermassive black hole candidate in M87."
AARON WILLIAM YOUNG,First M87 Event Horizon Telescope results. I. The shadow of the supermassive black hole,"When surrounded by a transparent emission region, black holes are expected to reveal a dark shadow caused by gravitational light bending and photon capture at the event horizon. To image and study this phenomenon, we have assembled the Event Horizon Telescope, a global very long baseline interferometry array observing at a wavelength of 1.3 mm. This allows us to reconstruct event-horizon-scale images of the supermassive black hole candidate in the center of the giant elliptical galaxy M87. We have resolved the central compact radio source as an asymmetric bright emission ring with a diameter of 42 ± 3 μas, which is circular and encompasses a central depression in brightness with a flux ratio gsim10:1. The emission ring is recovered using different calibration and imaging schemes, with its diameter and width remaining stable over four different observations carried out in different days. Overall, the observed image is consistent with expectations for the shadow of a Kerr black hole as predicted by general relativity. The asymmetry in brightness in the ring can be explained in terms of relativistic beaming of the emission from a plasma rotating close to the speed of light around a black hole. We compare our images to an extensive library of ray-traced general-relativistic magnetohydrodynamic simulations of black holes and derive a central mass of M = (6.5 ± 0.7) × 10^9 M ⊙. Our radio-wave observations thus provide powerful evidence for the presence of supermassive black holes in centers of galaxies and as the central engines of active galactic nuclei. They also present a new tool to explore gravity in its most extreme limit and on a mass scale that was so far not accessible."
AARON WILLIAM YOUNG,First Sagittarius A* Event Horizon Telescope results. I. The shadow of the supermassive black hole in the center of the Milky Way,"We present the first Event Horizon Telescope (EHT) observations of Sagittarius A* (Sgr A*), the Galactic center source associated with a supermassive black hole. These observations were conducted in 2017 using a global interferometric array of eight telescopes operating at a wavelength of λ = 1.3 mm. The EHT data resolve a compact emission region with intrahour variability. A variety of imaging and modeling analyses all support an image that is dominated by a bright, thick ring with a diameter of 51.8 ± 2.3 μas (68% credible interval). The ring has modest azimuthal brightness asymmetry and a comparatively dim interior. Using a large suite of numerical simulations, we demonstrate that the EHT images of Sgr A* are consistent with the expected appearance of a Kerr black hole with mass ∼4 × 106 M ⊙, which is inferred to exist at this location based on previous infrared observations of individual stellar orbits, as well as maser proper-motion studies. Our model comparisons disfavor scenarios where the black hole is viewed at high inclination (i &gt; 50°), as well as nonspinning black holes and those with retrograde accretion disks. Our results provide direct evidence for the presence of a supermassive black hole at the center of the Milky Way, and for the first time we connect the predictions from dynamical measurements of stellar orbits on scales of 103–105 gravitational radii to event-horizon-scale images and variability. Furthermore, a comparison with the EHT results for the supermassive black hole M87* shows consistency with the predictions of general relativity spanning over three orders of magnitude in central mass."
ESHED OHN-BAR,Learning to drive anywhere,"Human drivers can seamlessly adapt their driving decisions across geographical locations with diverse conditions and rules of the road, e.g., left vs. right-hand traffic. In contrast, existing models for autonomous driving have been thus far only deployed within restricted operational domains, i.e., without accounting for varying driving behaviors across locations or model scalability. In this work, we propose AnyD, a single geographically-aware conditional imitation learning (CIL) model that can efficiently learn from heterogeneous and globally distributed data with dynamic environmental, traffic, and social characteristics. Our key insight is to introduce a high-capacity geo-location-based channel attention mechanism that effectively adapts to local nuances while also flexibly modeling similarities among regions in a data-driven manner. By optimizing a contrastive imitation objective, our proposed approach can efficiently scale across the inherently imbalanced data distributions and location-dependent events. We demonstrate the benefits of our AnyD agent across multiple datasets, cities, and scalable deployment paradigms, i.e., centralized, semi-supervised, and distributed agent training. Specifically, AnyD outperforms CIL baselines by over 14% in open-loop evaluation and 30% in closed-loop testing on CARLA."
ESHED OHN-BAR,XVO: generalized visual odometry via cross-modal self-training,
ESHED OHN-BAR,Coaching a teachable student,
CASSANDRA M PIERRE,Revitalizing the infection prevention workforce with a fellowship program for underrepresented groups,"Infection preventionist (IP) positions are difficult to fill, and future workforce shortages are anticipated. The IP field has less racial and ethnic diversity than the general nursing workforce or patient population. A targeted fellowship program for underrepresented groups allowed the recruitment and training of IPs while avoiding staffing shortages."
GABRIEL KOCH OCKER,Tensor decompositions of higher-order correlations by nonlinear Hebbian learning,"Biological synaptic plasticity exhibits nonlinearities that are not accounted for by classic Hebbian learning rules. Here, we introduce a simple family of generalized nonlinear Hebbian learning rules. We study the computations implemented by their dynamics in the simple setting of a neuron receiving feedforward inputs. These nonlinear Hebbian rules allow a neuron to learn tensor decompositions of its higher-order input correlations. The particular input correlation decomposed and the form of the decomposition depend on the location of nonlinearities in the plasticity rule. For simple, biologically motivated parameters, the neuron learns eigenvectors of higher-order input correlation tensors. We prove that tensor eigenvectors are attractors and determine their basins of attraction. We calculate the volume of those basins, showing that the dominant eigenvector has the largest basin of attraction. We then study arbitrary learning rules and find that any learning rule that admits a finite Taylor expansion into the neural input and output also has stable equilibria at generalized eigenvectors of higher-order input correlation tensors. Nonlinearities in synaptic plasticity thus allow a neuron to encode higher-order input correlations in a simple fashion."
GABRIEL KOCH OCKER,Reconciling functional differences in populations of neurons recorded with two-photon imaging and electrophysiology,"Extracellular electrophysiology and two-photon calcium imaging are widely used methods for measuring physiological activity with single cell resolution across large populations of neurons in the brain. While these two modalities have distinct advantages and disadvantages, neither provides complete, unbiased information about the underlying neural population. Here, we compare evoked responses in visual cortex recorded in awake mice under highly standardized conditions using either imaging or electrophysiology. Across all stimulus conditions tested, we observe a larger fraction of responsive neurons in electrophysiology and higher stimulus selectivity in calcium imaging. This work explores which data transformations are most useful for explaining these modality specific discrepancies. We show that the higher selectivity in imaging can be partially reconciled by applying a spikes-to-calcium forward model to the electrophysiology data. However, the forward model could not reconcile differences in responsiveness without sub selecting neurons based on event rate or level of signal contamination. This suggests that differences in responsiveness more likely reflect neuronal sampling bias or cluster merging artifacts during spike sorting of electrophysiological recordings, rather than flaws in event detection from fluorescence time series. This work establishes the dominant impacts of the two modalities9 respective biases on a set of functional metrics that are fundamental for characterizing sensory-evoked responses."
GABRIEL KOCH OCKER,"Retraction: dynamics of stochastic integrate-and-fire networks [Phys. Rev. X 12, 041007 (2022)]",
GABRIEL KOCH OCKER,Republished: dynamics of stochastic integrate-and-fire networks,
JULIAN R. GONZALEZ,MAGIC and H.E.S.S. detect VHE gamma rays from the blazar OT081 for the first time: a deep multiwavelength study,
ERIC ROSEEN,Frequent back pain and subsequent mortality among older community-dwelling white women in the study of osteoporotic fractures (SOF),"INTRODUCTION: While the impact of back pain on morbidity in older adults is well-understood, the influence of back pain on mortality is unclear. Back pain is the leading cause of disability worldwide, and disability is associated with elevated risk of mortality. Thus, we hypothesized that older women with persistent back pain would have a higher risk of mortality over 16-years of follow-up, compared to those with no back pain among participants in the Study of Osteoporotic Fractures (SOF), a large multisite cohort study. Furthermore, we hypothesized that limitations of Instrumental Activities of Daily Living (IADLs) would mediate a greater proportion of the mortality risk, compared to two objective measures of physical function, walking and chair stand speed. METHODS: The analytic sample included 8,321 SOF participants (mean age 71.5, SD=5.1) who answered back pain questions at baseline (1986–87) and visit 2 (1989–90). We created a four-category back pain variable by combining responses at two interviews (baseline and visit 2). Categories included: no back pain (24%), non-persistent back pain (23%), and infrequent (44%) or frequent (9%) persistent back pain. Participants were followed from visit 2 through visit 9 (2006–08). Death was confirmed with receipt of death certificates. Using Cox-proportional hazards, we calculated hazard ratios (HR) and 95% confidence intervals (CI) for all-cause and cause-specific mortality with the ‘no back pain’ participants as our reference group. We adjusted for age, sociodemographic characteristics, self-reported general health, smoking status, comorbid conditions (e.g., prevalent vertebral fractures, osteoarthritis, hip pain, diabetes, hypertension), previous stroke, history of breast cancer, hospitalizations and falls in the previous year. We evaluated self-reported IADL limitations, slow walking speed, and slow chair stand time as a priori mediators of back pain and subsequent mortality. RESULTS: A total of 4975 women (55.8%) died over the follow-up period. A higher proportion of women with frequent persistent back pain died (65.8%) compared to those with no back pain (53.5%) (adjusted HR = 1.24; 95% CI 1.11 to 1.39). We observed an increase in cardiovascular (adjusted HR = 1.34; 95% CI 1.12 to 1.62) and cancer (adjusted HR = 1.33; 95% CI 1.03 to 1.71) mortality. No increased risk was observed for other back pain groups. A larger proportion of the association was mediated by IADL limitations (47%), compared to poor performance on chair stand (27%) and walking speed (24%). CONCLUSION: Compared to older women with no back pain, those with frequent persistent back pain had an increased risk of mortality, which underscores the importance of developing safe interventions to address and prevent this condition. Therapies that address IADL limitations or improve physical function (e.g. walking speed, chair stand) may be ideal for preventing early death in individuals with back pain."
ALBERTO CRUZ-MARTIN,"Cosmology intertwined: a review of the particle physics, astrophysics, and cosmology associated with the cosmological tensions and anomalies",
ALBERTO CRUZ-MARTIN,Songbird organotypic culture as an in vitro model for interrogating sparse sequencing networks,"Sparse sequences of neuronal activity are fundamental features of neural circuit computation; however, the underlying homeostatic mechanisms remain poorly understood. To approach these questions, we have developed a method for cellular-resolution imaging in organotypic cultures of the adult zebra finch brain, including portions of the intact song circuit. These in vitro networks can survive for weeks, and display mature neuron morphologies. Neurons within the organotypic slices exhibit a diversity of spontaneous and pharmacologically induced activity that can be easily monitored using the genetically encoded calcium indicator GCaMP6. In this study, we primarily focus on the classic song sequence generator HVC and the surrounding areas. We describe proof of concept experiments including physiological, optical, and pharmacological manipulation of these exposed networks. This method may allow the cellular rules underlying sparse, stereotyped neural sequencing to be examined with new degrees of experimental control."
SHIVELY SMITH,Focus: Summer 2020,
SHIVELY SMITH,"Brief reflections on the “head, heart and hand” legacy of Dr. Clarice J. Martin through the social-conscious literary voice of Anna Julia Cooper",
SHIVELY SMITH,Useful assessment and evaluation in language education,
XI LIU,"Computer-based tracking, analysis, and visualization of linguistically significant nonmanual events in American Sign Language (ASL)","Our linguistically annotated American Sign Language (ASL) corpora have formed a basis for research to automate detection by computer of essential linguistic information conveyed through facial expressions and head movements. We have tracked head position and facial deformations, and used computational learning to discern specific grammatical markings. Our ability to detect, identify, and temporally localize the occurrence of such markings in ASL videos has recently been improved by incorporation of (1) new techniques for deformable model-based 3D tracking of head position and facial expressions, which provide significantly better tracking accuracy and recover quickly from temporary loss of track due to occlusion; and (2) a computational learning approach incorporating 2-level Conditional Random Fields (CRFs), suited to the multi-scale spatio-temporal characteristics of the data, which analyses not only low-level appearance characteristics, but also the patterns that enable identification of significant gestural components, such as periodic head movements and raised or lowered eyebrows. Here we summarize our linguistically motivated computational approach and the results for detection and recognition of nonmanual grammatical markings; demonstrate our data visualizations, and discuss the relevance for linguistic research; and describe work underway to enable such visualizations to be produced over large corpora and shared publicly on the Web."
XI LIU,Sequential optimization for efficient high-quality object proposal generation,"We are motivated by the need for a generic object proposal generation algorithm which achieves good balance between object detection recall, proposal localization quality and computational efficiency. We propose a novel object proposal algorithm, BING ++, which inherits the virtue of good computational efficiency of BING [1] but significantly improves its proposal localization quality. At high level we formulate the problem of object proposal generation from a novel probabilistic perspective, based on which our BING++ manages to improve the localization quality by employing edges and segments to estimate object boundaries and update the proposals sequentially. We propose learning the parameters efficiently by searching for approximate solutions in a quantized parameter space for complexity reduction. We demonstrate the generalization of BING++ with the same fixed parameters across different object classes and datasets. Empirically our BING++ can run at half speed of BING on CPU, but significantly improve the localization quality by 18.5 and 16.7 percent on both VOC2007 and Microhsoft COCO datasets, respectively. Compared with other state-of-the-art approaches, BING++ can achieve comparable performance, but run significantly faster."
JESSICA MAXHAM PISEGNA,"Rethinking residue, an investigation of pharyngeal residue on flexible endoscopic evaluation of swallowing: the past, present, and future directions","This dissertation investigated measures of pharyngeal residue as seen on flexible endoscopic evaluation of swallowing (FEES). Research in this area of deglutology has been stalled due to measurement problems. The particular aims of this project were to compare visual analog scale ratings to categorical ratings of residue on FEES, and to investigate various measurement aspects. METHODS: Speech language pathologists were asked to rate residue from 81 swallows on FEES that demonstrated a wide range of residue severity for thin liquid, applesauce, and cracker boluses. A total of 33 clinicians rated the amount of residue at the time point after the first swallow, twice in a randomized fashion: the first time on a visual analog scale (VAS) and the second time categorically on a five point Likert scale. The results were analyzed for (1) inter/intra-rater agreement, (2) correlations between ratings and residue severity for each rating method, and (3) clusters of ratings to better define the scales and their clinical significance. A total of 2,673 VAS ratings and 2,673 categorical ratings were collected. RESULTS: (1) Both inter- and intra-rater reliability met acceptable levels of agreement, although intra-rater reliability on VAS ratings were slightly higher (r=0.8–0.9) than categorical ratings (k=0.7–0.8). Expert ratings were not significantly different from other clinicians’ ratings for any severity of any of the 3 boluses. (2) Residue ratings fit best on a curvilinear model; a quadratic fit of the data significantly improved the r2 values for each bolus type. (3) An increased residue amount, rated on either the VAS or categorical scale, was significantly associated with worse penetration-aspiration scale scores, but no significant relationship was found between the two methods of residue ratings and measures of quality of life or diet. Novel computerized methods are proposed for future measurement pursuits. CONCLUSION: The results of this dissertation suggest that residue is best measured on a scale with unequal intervals, and clinicians can be reliable in rating overall amount of residue on FEES after the first swallow. Novel computerized measurement approaches are useful building blocks for future research. It is hoped that with better measurement will come better understanding of residue, its risks, and consequences."
JESSICA MAXHAM PISEGNA,The efficacy of the Masako (tongue-hold) maneuver,"Purpose: Clinicians commonly recommend the tongue-hold maneuver, also called the Masako, as an exercise to strengthen swallowing muscles. Although this exercise is widely used, limited empirical data support this maneuver as an effective exercise. The goal ofthe present study is to observe, over multiple sessions, the effects ofthe tongue- hold maneuver as a 6-week exercise in subjects with dysphagia. The results ofthis study will help to address whether the tongue-hold maneuver is beneficial and, if so, which muscle groups are strengthened by this exercise. Methods: Five subjects with dysphagia and one healthy adult performed a set oftongue- hold maneuvers 3 times a day, 5 days per week, for 6 weeks. The number o f repetitions per set was individually calculated based on 80% of the maximal repetitions until fatigue. At baseline and 6 weeks, 4 measures were observed: a subject-reported quality-of-life swallowing scale, lingual strength, the amount of residue in the valleculae, and the pressures generated by pharyngeal muscles during a normal swallow. Four healthy adults who did not perform the tongue-hold maneuver were used as controls for the lingual measures, completing the measures of lingual strength at baseline, 3 weeks, and 6 weeks. Results: No overt trends in the subject-reported swallowing scale were noted; after 6 weeks of exercise, about half ranked their swallowing as worse and half ranked their swallowing as better. The treatment group demonstrated a non-significant overall2.3% increase in anteromedian lingual strength and 8.4% increase in posteromedian lingual strength. These changes did not set the treatment group apart from the control group, who demonstrated an increase of3.8% and 6.3% in the anteromedian and posteromedian positions, respectively. Regarding pharyngeal residue, 2 subjects did not show any changes in residue scores. However, the other 3 subjects demonstrated reduced residue in the valleculae with a cracker bolus. Out ofthe 3 subjects who were measured with manometry, 2 demonstrated higher oropharyngeal pressures on normal swallows after 6 weeks of exercise, although great variability was present. These results are limited by the small sample size and heterogeneity of the treatment group, as well as high variability in instrumental measurements. Conclusion: This study investigated the tongue-hold maneuver as an exercise and provides preliminary support for its use, with caution. Specifically, clinicians should be sure to prescribe regimens that fatigue swallowing muscles and push them past normal use. When using the Iowa Oral Performance Instrument (IOPI) as a tool, clinicians should also keep in mind that a learning effect is likely to occur over the first few trials. This pilot study suggests that clinicians should continue to prescribe the tongue-hold maneuver as an exercise with caution, as some patients may benefit from it while others may not. Further investigation is required."
MICHAEL RICHARD ULRICH,Quantum biology revisited,"Photosynthesis is a highly optimized process from which valuable lessons can be learned about the operating principles in nature. Its primary steps involve energy transport operating near theoretical quantum limits in efficiency. Recently, extensive research was motivated by the hypothesis that nature used quantum coherences to direct energy transfer. This body of work, a cornerstone for the field of quantum biology, rests on the interpretation of small-amplitude oscillations in two-dimensional electronic spectra of photosynthetic complexes. This Review discusses recent work reexamining these claims and demonstrates that interexciton coherences are too short lived to have any functional significance in photosynthetic energy transfer. Instead, the observed long-lived coherences originate from impulsively excited vibrations, generally observed in femtosecond spectroscopy. These efforts, collectively, lead to a more detailed understanding of the quantum aspects of dissipation. Nature, rather than trying to avoid dissipation, exploits it via engineering of exciton-bath interaction to create efficient energy flow."
MICHAEL RICHARD ULRICH,"Twenty questions about design behavior for sustainability, report of the International Expert Panel on behavioral science for design","How behavioral scientists, engineers, and architects can work together to advance how we all understand and practice design—in order to enhance sustainability in the built environment, and beyond."
ALEXANDER CHANG,"First Sagittarius A* Event Horizon Telescope results. IV. Variability, morphology, and black hole mass","In this paper we quantify the temporal variability and image morphology of the horizon-scale emission from Sgr A*, as observed by the EHT in 2017 April at a wavelength of 1.3 mm. We find that the Sgr A* data exhibit variability that exceeds what can be explained by the uncertainties in the data or by the effects of interstellar scattering. The magnitude of this variability can be a substantial fraction of the correlated flux density, reaching ∼100% on some baselines. Through an exploration of simple geometric source models, we demonstrate that ring-like morphologies provide better fits to the Sgr A* data than do other morphologies with comparable complexity. We develop two strategies for fitting static geometric ring models to the time-variable Sgr A* data; one strategy fits models to short segments of data over which the source is static and averages these independent fits, while the other fits models to the full data set using a parametric model for the structural variability power spectrum around the average source structure. Both geometric modeling and image-domain feature extraction techniques determine the ring diameter to be 51.8 ± 2.3 μas (68% credible intervals), with the ring thickness constrained to have an FWHM between ∼30% and 50% of the ring diameter. To bring the diameter measurements to a common physical scale, we calibrate them using synthetic data generated from GRMHD simulations. This calibration constrains the angular size of the gravitational radius to be 4.8_-0.7^+1.4 μas, which we combine with an independent distance measurement from maser parallaxes to determine the mass of Sgr A* to be 4.0_-0.6^+10^6 M⊙."
ALEXANDER CHANG,Omicron infection following vaccination enhances a broad spectrum of immune responses dependent on infection history,"Pronounced immune escape by the SARS-CoV-2 Omicron variant has resulted in many individuals possessing hybrid immunity, generated through a combination of vaccination and infection. Concerns have been raised that omicron breakthrough infections in triple-vaccinated individuals result in poor induction of omicron-specific immunity, and that prior SARS-CoV-2 infection is associated with immune dampening. Taking a broad and comprehensive approach, we characterize mucosal and blood immunity to spike and non-spike antigens following BA.1/BA.2 infections in triple mRNA-vaccinated individuals, with and without prior SARS-CoV-2 infection. We find that most individuals increase BA.1/BA.2/BA.5-specific neutralizing antibodies following infection, but confirm that the magnitude of increase and post-omicron titres are higher in the infection-naive. In contrast, significant increases in nasal responses, including neutralizing activity against BA.5 spike, are seen regardless of infection history. Spike-specific T cells increase only in infection-naive vaccinees; however, post-omicron T cell responses are significantly higher in the previously-infected, who display a maximally induced response with a highly cytotoxic CD8+ phenotype following their 3rd mRNA vaccine dose. Responses to non-spike antigens increase significantly regardless of prior infection status. These findings suggest that hybrid immunity induced by omicron breakthrough infections is characterized by significant immune enhancement that can help protect against future omicron variants."
ALEXANDER CHANG,Event Horizon Telescope imaging of the archetypal blazar 3C 279 at an extreme 20 microarcsecond resolution,"3C 279 is an archetypal blazar with a prominent radio jet that show broadband flux density variability across the entire electromagnetic spectrum. We use an ultra-high angular resolution technique – global Very Long Baseline Interferometry (VLBI) at 1.3 mm (230 GHz) – to resolve the innermost jet of 3C 279 in order to study its fine-scale morphology close to the jet base where highly variable γ-ray emission is thought to originate, according to various models. The source was observed during four days in April 2017 with the Event Horizon Telescope at 230 GHz, including the phased Atacama Large Millimeter/submillimeter Array (ALMA), at an angular resolution of ∼20 μas (at a redshift of z = 0.536 this corresponds to ∼0.13 pc  ∼ 1700 Schwarzschild radii with a black hole mass M<jats:sub>BH</jats:sub> = 8 × 10^8 M_⊙). Imaging and model-fitting techniques were applied to the data to parameterize the fine-scale source structure and its variation. We find a multicomponent inner jet morphology with the northernmost component elongated perpendicular to the direction of the jet, as imaged at longer wavelengths. The elongated nuclear structure is consistent on all four observing days and across different imaging methods and model-fitting techniques, and therefore appears robust. Owing to its compactness and brightness, we associate the northern nuclear structure as the VLBI “core”. This morphology can be interpreted as either a broad resolved jet base or a spatially bent jet. We also find significant day-to-day variations in the closure phases, which appear most pronounced on the triangles with the longest baselines. Our analysis shows that this variation is related to a systematic change of the source structure. Two inner jet components move non-radially at apparent speeds of ∼15 c and ∼20 c (∼1.3 and ∼1.7 μas day^−1, respectively), which more strongly supports the scenario of traveling shocks or instabilities in a bent, possibly rotating jet. The observed apparent speeds are also coincident with the 3C 279 large-scale jet kinematics observed at longer (cm) wavelengths, suggesting no significant jet acceleration between the 1.3 mm core and the outer jet. The intrinsic brightness temperature of the jet components are ≲10^10 K, a magnitude or more lower than typical values seen at ≥7 mm wavelengths. The low brightness temperature and morphological complexity suggest that the core region of 3C 279 becomes optically thin at short (mm) wavelengths."
ALEXANDER CHANG,First M87 Event Horizon Telescope results. II. Array and instrumentation,"The Event Horizon Telescope (EHT) is a very long baseline interferometry (VLBI) array that comprises millimeter- and submillimeter-wavelength telescopes separated by distances comparable to the diameter of the Earth. At a nominal operating wavelength of ~1.3 mm, EHT angular resolution (λ/D) is ~25 μas, which is sufficient to resolve nearby supermassive black hole candidates on spatial and temporal scales that correspond to their event horizons. With this capability, the EHT scientific goals are to probe general relativistic effects in the strong-field regime and to study accretion and relativistic jet formation near the black hole boundary. In this Letter we describe the system design of the EHT, detail the technology and instrumentation that enable observations, and provide measures of its performance. Meeting the EHT science objectives has required several key developments that have facilitated the robust extension of the VLBI technique to EHT observing wavelengths and the production of instrumentation that can be deployed on a heterogeneous array of existing telescopes and facilities. To meet sensitivity requirements, high-bandwidth digital systems were developed that process data at rates of 64 gigabit s−1, exceeding those of currently operating cm-wavelength VLBI arrays by more than an order of magnitude. Associated improvements include the development of phasing systems at array facilities, new receiver installation at several sites, and the deployment of hydrogen maser frequency standards to ensure coherent data capture across the array. These efforts led to the coordination and execution of the first Global EHT observations in 2017 April, and to event-horizon-scale imaging of the supermassive black hole candidate in M87."
ALEXANDER CHANG,First M87 Event Horizon Telescope results. I. The shadow of the supermassive black hole,"When surrounded by a transparent emission region, black holes are expected to reveal a dark shadow caused by gravitational light bending and photon capture at the event horizon. To image and study this phenomenon, we have assembled the Event Horizon Telescope, a global very long baseline interferometry array observing at a wavelength of 1.3 mm. This allows us to reconstruct event-horizon-scale images of the supermassive black hole candidate in the center of the giant elliptical galaxy M87. We have resolved the central compact radio source as an asymmetric bright emission ring with a diameter of 42 ± 3 μas, which is circular and encompasses a central depression in brightness with a flux ratio gsim10:1. The emission ring is recovered using different calibration and imaging schemes, with its diameter and width remaining stable over four different observations carried out in different days. Overall, the observed image is consistent with expectations for the shadow of a Kerr black hole as predicted by general relativity. The asymmetry in brightness in the ring can be explained in terms of relativistic beaming of the emission from a plasma rotating close to the speed of light around a black hole. We compare our images to an extensive library of ray-traced general-relativistic magnetohydrodynamic simulations of black holes and derive a central mass of M = (6.5 ± 0.7) × 10^9 M ⊙. Our radio-wave observations thus provide powerful evidence for the presence of supermassive black holes in centers of galaxies and as the central engines of active galactic nuclei. They also present a new tool to explore gravity in its most extreme limit and on a mass scale that was so far not accessible."
ALEXANDER CHANG,The polarized image of a synchrotron-emitting ring of gas orbiting a black hole,"Synchrotron radiation from hot gas near a black hole results in a polarized image. The image polarization is determined by effects including the orientation of the magnetic field in the emitting region, relativistic motion of the gas, strong gravitational lensing by the black hole, and parallel transport in the curved spacetime. We explore these effects using a simple model of an axisymmetric, equatorial accretion disk around a Schwarzschild black hole. By using an approximate expression for the null geodesics derived by Beloborodov and conservation of the Walker–Penrose constant, we provide analytic estimates for the image polarization. We test this model using currently favored general relativistic magnetohydrodynamic simulations of M87*, using ring parameters given by the simulations. For a subset of these with modest Faraday effects, we show that the ring model broadly reproduces the polarimetric image morphology. Our model also predicts the polarization evolution for compact flaring regions, such as those observed from Sgr A* with GRAVITY. With suitably chosen parameters, our simple model can reproduce the EVPA pattern and relative polarized intensity in Event Horizon Telescope images of M87*. Under the physically motivated assumption that the magnetic field trails the fluid velocity, this comparison is consistent with the clockwise rotation inferred from total intensity images."
ALEXANDER CHANG,First Sagittarius A* Event Horizon Telescope results. I. The shadow of the supermassive black hole in the center of the Milky Way,"We present the first Event Horizon Telescope (EHT) observations of Sagittarius A* (Sgr A*), the Galactic center source associated with a supermassive black hole. These observations were conducted in 2017 using a global interferometric array of eight telescopes operating at a wavelength of λ = 1.3 mm. The EHT data resolve a compact emission region with intrahour variability. A variety of imaging and modeling analyses all support an image that is dominated by a bright, thick ring with a diameter of 51.8 ± 2.3 μas (68% credible interval). The ring has modest azimuthal brightness asymmetry and a comparatively dim interior. Using a large suite of numerical simulations, we demonstrate that the EHT images of Sgr A* are consistent with the expected appearance of a Kerr black hole with mass ∼4 × 106 M ⊙, which is inferred to exist at this location based on previous infrared observations of individual stellar orbits, as well as maser proper-motion studies. Our model comparisons disfavor scenarios where the black hole is viewed at high inclination (i &gt; 50°), as well as nonspinning black holes and those with retrograde accretion disks. Our results provide direct evidence for the presence of a supermassive black hole at the center of the Milky Way, and for the first time we connect the predictions from dynamical measurements of stellar orbits on scales of 103–105 gravitational radii to event-horizon-scale images and variability. Furthermore, a comparison with the EHT results for the supermassive black hole M87* shows consistency with the predictions of general relativity spanning over three orders of magnitude in central mass."
JEFFREY ALLEN,The effect of extreme response and non-extreme response styles on testing measurement invariance,"Extreme and non-extreme response styles (RSs) are prevalent in survey research using Likert-type scales. Their effects on measurement invariance (MI) in the context of confirmatory factor analysis are systematically investigated here via a Monte Carlo simulation study. Using the parameter estimates obtained from analyzing a 2007 Trends in International Mathematics and Science Study data set, a population model was constructed. Original and contaminated data with one of two RSs were generated and analyzed via multi-group confirmatory factor analysis with different constraints of MI. The results indicated that the detrimental effects of response style on MI have been underestimated. More specifically, these two RSs had a substantially negative impact on both model fit and parameter recovery, suggesting that the lack of MI between groups may have been caused by the RSs, not the measured factors of focal interest. Practical implications are provided to help practitioners to detect RSs and determine whether RSs are a serious threat to MI."
JEFFREY ALLEN,Imaging X-ray polarimetry explorer: prelaunch,"Launched on 2021 December 9, the Imaging X-ray Polarimetry Explorer (IXPE) is a NASA Small Explorer Mission in collaboration with the Italian Space Agency (ASI). The mission will open a new window of investigation—imaging x-ray polarimetry. The observatory features three identical telescopes, each consisting of a mirror module assembly with a polarization-sensitive imaging x-ray detector at the focus. A coilable boom, deployed on orbit, provides the necessary 4-m focal length. The observatory utilizes a three-axis-stabilized spacecraft, which provides services such as power, attitude determination and control, commanding, and telemetry to the ground. During its 2-year baseline mission, IXPE will conduct precise polarimetry for samples of multiple categories of x-ray sources, with follow-on observations of selected targets."
MING ZHANG,First M87 Event Horizon Telescope results. V. Physical origin of the asymmetric ring,"The Event Horizon Telescope (EHT) has mapped the central compact radio source of the elliptical galaxy M87 at 1.3 mm with unprecedented angular resolution. Here we consider the physical implications of the asymmetric ring seen in the 2017 EHT data. To this end, we construct a large library of models based on general relativistic magnetohydrodynamic (GRMHD) simulations and synthetic images produced by general relativistic ray tracing. We compare the observed visibilities with this library and confirm that the asymmetric ring is consistent with earlier predictions of strong gravitational lensing of synchrotron emission from a hot plasma orbiting near the black hole event horizon. The ring radius and ring asymmetry depend on black hole mass and spin, respectively, and both are therefore expected to be stable when observed in future EHT campaigns. Overall, the observed image is consistent with expectations for the shadow of a spinning Kerr black hole as predicted by general relativity. If the black hole spin and M87's large scale jet are aligned, then the black hole spin vector is pointed away from Earth. Models in our library of non-spinning black holes are inconsistent with the observations as they do not produce sufficiently powerful jets. At the same time, in those models that produce a sufficiently powerful jet, the latter is powered by extraction of black hole spin energy through mechanisms akin to the Blandford-Znajek process. We briefly consider alternatives to a black hole for the central compact object. Analysis of existing EHT polarization data and data taken simultaneously at other wavelengths will soon enable new tests of the GRMHD models, as will future EHT campaigns at 230 and 345 GHz."
MING ZHANG,The effect of static stretch on elastin degradation in arteries,"Previously we have shown that gradual changes in the structure of elastin during an elastase treatment can lead to important transition stages in the mechanical behavior of arteries. However, in vivo arteries are constantly being loaded due to systolic and diastolic pressures and so understanding the effects of loading on the enzymatic degradation of elastin in arteries is important. With biaxial tensile testing, we measured the mechanical behavior of porcine thoracic aortas digested with a mild solution of purified elastase (5 U/mL) in the presence of a static stretch. Arterial mechanical properties and biochemical composition were analyzed to assess the effects of mechanical stretch on elastin degradation. As elastin is being removed, the dimensions of the artery increase by more than 20% in both the longitude and circumference directions. Elastin assays indicate a faster rate of degradation when stretch was present during the digestion. A simple exponential decay fitting confirms the time constant for digestion with stretch (0.11 ± 0.04 h(-1)) is almost twice that of digestion without stretch (0.069 ± 0.028 h(-1)). The transition from J-shaped to S-shaped stress vs. strain behavior in the longitudinal direction generally occurs when elastin content is reduced by about 60%. Multiphoton image analysis confirms the removal/fragmentation of elastin and also shows that the collagen fibers are closely intertwined with the elastin lamellae in the medial layer. After removal of elastin, the collagen fibers are no longer constrained and become disordered. Release of amorphous elastin during the fragmentation of the lamellae layers is observed and provides insights into the process of elastin degradation. Overall this study reveals several interesting microstructural changes in the extracellular matrix that could explain the resulting mechanical behavior of arteries with elastin degradation."
MING ZHANG,Experimental and modeling study of collagen scaffolds with the effects of crosslinking and fiber alignment,"Collagen type I scaffolds are commonly used due to its abundance, biocompatibility, and ubiquity. Most applications require the scaffolds to operate under mechanical stresses. Therefore understanding and being able to control the structural-functional integrity of collagen scaffolds becomes crucial. Using a combined experimental and modeling approach, we studied the structure and function of Type I collagen gel with the effects of spatial fiber alignment and crosslinking. Aligned collagen scaffolds were created through the flow of magnetic particles enmeshed in collagen fibrils to mimic the anisotropy seen in native tissue. Inter- and intra- molecular crosslinking was modified chemically with Genipin to further improve the stiffness of collagen scaffolds. The anisotropic mechanical properties of collagen scaffolds were characterized using a planar biaxial tensile tester and parallel plate rheometer. The tangent stiffness from biaxial tensile test is two to three orders of magnitude higher than the storage moduli from rheological measurements. The biphasic nature of collagen gel was discussed and used to explain the mechanical behavior of collagen scaffolds under different types of mechanical tests. An anisotropic hyperelastic constitutive model was used to capture the characteristics of the stress-strain behavior exhibited by collagen scaffolds."
MING ZHANG,Regulating global capital flows for long-run development,"This report is the product of the Pardee Center Task Force on Regulating Global Capital Flows for Long-Run Development convened in September 2011 on behalf of the Pardee Center’s Global Economic Governance Initiative led by Kevin P. Gallagher, Associate Professor of International Relations at Boston University. Gallagher co-chaired the Task Force along with Stephany Griffith-Jones and José Antonio Ocampo of the Initiative for Policy Dialogue (IPD) at Columbia University. With contributions from a dozen prominent scholars and practitioners in the field of global finance and development, the report is intended to contribute expert knowledge to an important and very timely debate concerning whether and how nations can use what have been traditionally referred to as capital controls (classified in the report as ‘capital account regulations’ or CARs) to prevent and mitigate financial crises caused by short-term speculative capital flows in developing countries. Based on discussions among members at the September 2011 meeting, the report posits that there is a clear rationale for capital account regulations in the wake of the 2008 financial crisis, that the design and monitoring of such regulations is essential for their effectiveness, and that a limited amount of global and regional cooperation would be useful to ensure that CARs can form an effective part of the macroeconomic policy toolkit. The protocol for deploying capital account regulations in developing countries that is put forth here stands in stark contrast to a set of guidelines for the use of capital controls endorsed by the board of the International Monetary Fund (IMF) in March 2011. However the Task Force’s recommendations are more in sync with the set of “coherent conclusions” on capital account regulations endorsed by the G-20 in November 2011. Our hope is that this Pardee Center Task Force Report will help inform the discussions and decisions of policymakers and the IMF as they move forward on this issue under the rubric of the G-20 recommendations."
MING ZHANG,Sequential optimization for efficient high-quality object proposal generation,"We are motivated by the need for a generic object proposal generation algorithm which achieves good balance between object detection recall, proposal localization quality and computational efficiency. We propose a novel object proposal algorithm, BING ++, which inherits the virtue of good computational efficiency of BING [1] but significantly improves its proposal localization quality. At high level we formulate the problem of object proposal generation from a novel probabilistic perspective, based on which our BING++ manages to improve the localization quality by employing edges and segments to estimate object boundaries and update the proposals sequentially. We propose learning the parameters efficiently by searching for approximate solutions in a quantized parameter space for complexity reduction. We demonstrate the generalization of BING++ with the same fixed parameters across different object classes and datasets. Empirically our BING++ can run at half speed of BING on CPU, but significantly improve the localization quality by 18.5 and 16.7 percent on both VOC2007 and Microhsoft COCO datasets, respectively. Compared with other state-of-the-art approaches, BING++ can achieve comparable performance, but run significantly faster."
MING ZHANG,First Sagittarius A* Event Horizon Telescope results. V. Testing astrophysical models of the galactic center black hole,"In this paper we provide a first physical interpretation for the Event Horizon Telescope's (EHT) 2017 observations of Sgr A*. Our main approach is to compare resolved EHT data at 230 GHz and unresolved non-EHT observations from radio to X-ray wavelengths to predictions from a library of models based on time-dependent general relativistic magnetohydrodynamics simulations, including aligned, tilted, and stellar-wind-fed simulations; radiative transfer is performed assuming both thermal and nonthermal electron distribution functions. We test the models against 11 constraints drawn from EHT 230 GHz data and observations at 86 GHz, 2.2 μm, and in the X-ray. All models fail at least one constraint. Light-curve variability provides a particularly severe constraint, failing nearly all strongly magnetized (magnetically arrested disk (MAD)) models and a large fraction of weakly magnetized models. A number of models fail only the variability constraints. We identify a promising cluster of these models, which are MAD and have inclination i ≤ 30°. They have accretion rate (5.2–9.5) × 10−9 M ⊙ yr−1, bolometric luminosity (6.8–9.2) × 1035 erg s−1, and outflow power (1.3–4.8) × 1038 erg s−1. We also find that all models with i ≥ 70° fail at least two constraints, as do all models with equal ion and electron temperature; exploratory, nonthermal model sets tend to have higher 2.2 μm flux density; and the population of cold electrons is limited by X-ray constraints due to the risk of bremsstrahlung overproduction. Finally, we discuss physical and numerical limitations of the models, highlighting the possible importance of kinetic effects and duration of the simulations."
MING ZHANG,Resolving the inner parsec of the blazar J1924–2914 with the event horizon telescope,"The blazar J1924–2914 is a primary Event Horizon Telescope (EHT) calibrator for the Galactic center’s black hole Sagittarius A*. Here we present the first total and linearly polarized intensity images of this source obtained with the unprecedented 20 μas resolution of the EHT. J1924–2914 is a very compact flat-spectrum radio source with strong optical variability and polarization. In April 2017 the source was observed quasi-simultaneously with the EHT (April 5–11), the Global Millimeter VLBI Array (April 3), and the Very Long Baseline Array (April 28), giving a novel view of the source at four observing frequencies, 230, 86, 8.7, and 2.3 GHz. These observations probe jet properties from the subparsec to 100 pc scales. We combine the multifrequency images of J1924–2914 to study the source morphology. We find that the jet exhibits a characteristic bending, with a gradual clockwise rotation of the jet projected position angle of about 90° between 2.3 and 230 GHz. Linearly polarized intensity images of J1924–2914 with the extremely fine resolution of the EHT provide evidence for ordered toroidal magnetic fields in the blazar compact core."
MING ZHANG,Capital account liberalization in China: a cautionary tale,"This policy brief synthesizes some of the main themes and policy recommendations discussed at a February 2014 workshop of the Pardee Task Force for Regulating Capital Flows at Boston University, and presented in this report, though the specific recommendations discussed in this brief are our own. The main message is that China would do well to draw lessons from both the economics literature and country experiences with capital account liberalization. Such an approach would guide China to adopt a carefully sequenced and cautionary approach to capital account liberalization."
MING ZHANG,A universal power-law prescription for variability from synthetic images of black hole accretion flows,"We present a framework for characterizing the spatiotemporal power spectrum of the variability expected from the horizon-scale emission structure around supermassive black holes, and we apply this framework to a library of general relativistic magnetohydrodynamic (GRMHD) simulations and associated general relativistic ray-traced images relevant for Event Horizon Telescope (EHT) observations of Sgr A*. We find that the variability power spectrum is generically a red-noise process in both the temporal and spatial dimensions, with the peak in power occurring on the longest timescales and largest spatial scales. When both the time-averaged source structure and the spatially integrated light-curve variability are removed, the residual power spectrum exhibits a universal broken power-law behavior. On small spatial frequencies, the residual power spectrum rises as the square of the spatial frequency and is proportional to the variance in the centroid of emission. Beyond some peak in variability power, the residual power spectrum falls as that of the time-averaged source structure, which is similar across simulations; this behavior can be naturally explained if the variability arises from a multiplicative random field that has a steeper high-frequency power-law index than that of the time-averaged source structure. We briefly explore the ability of power spectral variability studies to constrain physical parameters relevant for the GRMHD simulations, which can be scaled to provide predictions for black holes in a range of systems in the optically thin regime. We present specific expectations for the behavior of the M87* and Sgr A* accretion flows as observed by the EHT."
MING ZHANG,Millimeter light curves of Sagittarius A* observed during the 2017 Event Horizon Telescope campaign,"The Event Horizon Telescope (EHT) observed the compact radio source, Sagittarius A* (Sgr A*), in the Galactic Center on 2017 April 5–11 in the 1.3 mm wavelength band. At the same time, interferometric array data from the Atacama Large Millimeter/submillimeter Array and the Submillimeter Array were collected, providing Sgr A* light curves simultaneous with the EHT observations. These data sets, complementing the EHT very long baseline interferometry, are characterized by a cadence and signal-to-noise ratio previously unattainable for Sgr A* at millimeter wavelengths, and they allow for the investigation of source variability on timescales as short as a minute. While most of the light curves correspond to a low variability state of Sgr A*, the April 11 observations follow an X-ray flare and exhibit strongly enhanced variability. All of the light curves are consistent with a red-noise process, with a power spectral density (PSD) slope measured to be between −2 and −3 on timescales between 1 minute and several hours. Our results indicate a steepening of the PSD slope for timescales shorter than 0.3 hr. The spectral energy distribution is flat at 220 GHz, and there are no time lags between the 213 and 229 GHz frequency bands, suggesting low optical depth for the event horizon scale source. We characterize Sgr A*’s variability, highlighting the different behavior observed just after the X-ray flare, and use Gaussian process modeling to extract a decorrelation timescale and a PSD slope. We also investigate the systematic calibration uncertainties by analyzing data from independent data reduction pipelines."
MING ZHANG,Capital account liberalization in China: the need for a balanced approach,"This is the third report stemming from the Pardee Center Task Force on Regulating Capital Flows for Long-Run Development, a project of the Global Economic Governance Initiative (GEGI) at Boston University. This report is the collective work of experts examining the benefits and risks of accelerated capital account liberalization in China. The contributing authors – all leading scholars and practitioners from around the world (listed below) – met at Boston University in February 2014 to discuss the experiences of other emerging market countries that liberalized the capital account to glean lessons for China as it considers this delicate task. This volume is an outcome from that meeting, presenting the authors’ perspectives on important aspects of capital account liberalization that China should pay special attention to, not only for its own sake, but also in consideration of the potential impacts that China’s actions may have on other emerging markets and the global economy overall."
MING ZHANG,The H-index of a network node and its relation to degree and coreness,"Identifying influential nodes in dynamical processes is crucial in understanding network structure and function. Degree, H-index and coreness are widely used metrics, but previously treated as unrelated. Here we show their relation by constructing an operator , in terms of which degree, H-index and coreness are the initial, intermediate and steady states of the sequences, respectively. We obtain a family of H-indices that can be used to measure a node’s importance. We also prove that the convergence to coreness can be guaranteed even under an asynchronous updating process, allowing a decentralized local method of calculating a node’s coreness in large-scale evolving networks. Numerical analyses of the susceptible-infected-removed spreading dynamics on disparate real networks suggest that the H-index is a good tradeoff that in many cases can better quantify node influence than either degree or coreness."
MING ZHANG,RNN training along locally optimal trajectoriesvia Frank-Wolfe algorithm,"We propose a novel and efficient training method for RNNs by iteratively seeking a local minima on the loss surface within a small region, and leverage this directional vector for the update, in an outer-loop. We propose to utilize the Frank-Wolfe (FW) algorithm in this context. Although, FW implicitly involves normalized gradients, which can lead to a slow convergence rate, we develop a novel RNN training method that, surprisingly, even with the additional cost, the overall training cost is empirically observed to be lower than backpropagation. Our method leads to a new Frank-Wolfe method, that is in essence an SGD algorithm with a restart scheme. We prove that under certain conditions our algorithm has a sublinear convergence rate of O (1/ϵ) for ϵ error. We then conduct empirical experiments on several benchmark datasets including those that exhibit long-term dependencies, and show significant performance improvement. We also experiment with deep RNN architectures and show efficient training performance. Finally, we demonstrate that our training method is robust to noisy data."
MING ZHANG,First Sagittarius A* Event Horizon Telescope results. VI. Testing the black hole metric,"Astrophysical black holes are expected to be described by the Kerr metric. This is the only stationary, vacuum, axisymmetric metric, without electromagnetic charge, that satisfies Einstein’s equations and does not have pathologies outside of the event horizon. We present new constraints on potential deviations from the Kerr prediction based on 2017 EHT observations of Sagittarius A* (Sgr A*). We calibrate the relationship between the geometrically defined black hole shadow and the observed size of the ring-like images using a library that includes both Kerr and non-Kerr simulations. We use the exquisite prior constraints on the mass-to-distance ratio for Sgr A* to show that the observed image size is within ∼10% of the Kerr predictions. We use these bounds to constrain metrics that are parametrically different from Kerr, as well as the charges of several known spacetimes. To consider alternatives to the presence of an event horizon, we explore the possibility that Sgr A* is a compact object with a surface that either absorbs and thermally reemits incident radiation or partially reflects it. Using the observed image size and the broadband spectrum of Sgr A*, we conclude that a thermal surface can be ruled out and a fully reflective one is unlikely. We compare our results to the broader landscape of gravitational tests. Together with the bounds found for stellar-mass black holes and the M87 black hole, our observations provide further support that the external spacetimes of all black holes are described by the Kerr metric, independent of their mass."
MING ZHANG,"First Sagittarius A* Event Horizon Telescope results. IV. Variability, morphology, and black hole mass","In this paper we quantify the temporal variability and image morphology of the horizon-scale emission from Sgr A*, as observed by the EHT in 2017 April at a wavelength of 1.3 mm. We find that the Sgr A* data exhibit variability that exceeds what can be explained by the uncertainties in the data or by the effects of interstellar scattering. The magnitude of this variability can be a substantial fraction of the correlated flux density, reaching ∼100% on some baselines. Through an exploration of simple geometric source models, we demonstrate that ring-like morphologies provide better fits to the Sgr A* data than do other morphologies with comparable complexity. We develop two strategies for fitting static geometric ring models to the time-variable Sgr A* data; one strategy fits models to short segments of data over which the source is static and averages these independent fits, while the other fits models to the full data set using a parametric model for the structural variability power spectrum around the average source structure. Both geometric modeling and image-domain feature extraction techniques determine the ring diameter to be 51.8 ± 2.3 μas (68% credible intervals), with the ring thickness constrained to have an FWHM between ∼30% and 50% of the ring diameter. To bring the diameter measurements to a common physical scale, we calibrate them using synthetic data generated from GRMHD simulations. This calibration constrains the angular size of the gravitational radius to be 4.8_-0.7^+1.4 μas, which we combine with an independent distance measurement from maser parallaxes to determine the mass of Sgr A* to be 4.0_-0.6^+10^6 M⊙."
MING ZHANG,A critical bioenergetic switch is regulated by IGF2 during murine cartilage development,"Long bone growth requires the precise control of chondrocyte maturation from proliferation to hypertrophy during endochondral ossification, but the bioenergetic program that ensures normal cartilage development is still largely elusive. We show that chondrocytes have unique glucose metabolism signatures in these stages, and they undergo bioenergetic reprogramming from glycolysis to oxidative phosphorylation during maturation, accompanied by an upregulation of the pentose phosphate pathway. Inhibition of either oxidative phosphorylation or the pentose phosphate pathway in murine chondrocytes and bone organ cultures impaired hypertrophic differentiation, suggesting that the appropriate balance of these pathways is required for cartilage development. Insulin-like growth factor 2 (IGF2) deficiency resulted in a profound increase in oxidative phosphorylation in hypertrophic chondrocytes, suggesting that IGF2 is required to prevent overactive glucose metabolism and maintain a proper balance of metabolic pathways. Our results thus provide critical evidence of preference for a bioenergetic pathway in different stages of chondrocytes and highlight its importance as a fundamental mechanism in skeletal development."
MING ZHANG,"First Sagittarius A* Event Horizon Telescope results. II. EHT and multiwavelength observations, data processing, and calibration","We present Event Horizon Telescope (EHT) 1.3 mm measurements of the radio source located at the position of the supermassive black hole Sagittarius A* (Sgr A*), collected during the 2017 April 5–11 campaign. The observations were carried out with eight facilities at six locations across the globe. Novel calibration methods are employed to account for Sgr A*'s flux variability. The majority of the 1.3 mm emission arises from horizon scales, where intrinsic structural source variability is detected on timescales of minutes to hours. The effects of interstellar scattering on the image and its variability are found to be subdominant to intrinsic source structure. The calibrated visibility amplitudes, particularly the locations of the visibility minima, are broadly consistent with a blurred ring with a diameter of ∼50 μas, as determined in later works in this series. Contemporaneous multiwavelength monitoring of Sgr A* was performed at 22, 43, and 86 GHz and at near-infrared and X-ray wavelengths. Several X-ray flares from Sgr A* are detected by Chandra, one at low significance jointly with Swift on 2017 April 7 and the other at higher significance jointly with NuSTAR on 2017 April 11. The brighter April 11 flare is not observed simultaneously by the EHT but is followed by a significant increase in millimeter flux variability immediately after the X-ray outburst, indicating a likely connection in the emission physics near the event horizon. We compare Sgr A*’s broadband flux during the EHT campaign to its historical spectral energy distribution and find that both the quiescent emission and flare emission are consistent with its long-term behavior."
MING ZHANG,Event Horizon Telescope imaging of the archetypal blazar 3C 279 at an extreme 20 microarcsecond resolution,"3C 279 is an archetypal blazar with a prominent radio jet that show broadband flux density variability across the entire electromagnetic spectrum. We use an ultra-high angular resolution technique – global Very Long Baseline Interferometry (VLBI) at 1.3 mm (230 GHz) – to resolve the innermost jet of 3C 279 in order to study its fine-scale morphology close to the jet base where highly variable γ-ray emission is thought to originate, according to various models. The source was observed during four days in April 2017 with the Event Horizon Telescope at 230 GHz, including the phased Atacama Large Millimeter/submillimeter Array (ALMA), at an angular resolution of ∼20 μas (at a redshift of z = 0.536 this corresponds to ∼0.13 pc  ∼ 1700 Schwarzschild radii with a black hole mass M<jats:sub>BH</jats:sub> = 8 × 10^8 M_⊙). Imaging and model-fitting techniques were applied to the data to parameterize the fine-scale source structure and its variation. We find a multicomponent inner jet morphology with the northernmost component elongated perpendicular to the direction of the jet, as imaged at longer wavelengths. The elongated nuclear structure is consistent on all four observing days and across different imaging methods and model-fitting techniques, and therefore appears robust. Owing to its compactness and brightness, we associate the northern nuclear structure as the VLBI “core”. This morphology can be interpreted as either a broad resolved jet base or a spatially bent jet. We also find significant day-to-day variations in the closure phases, which appear most pronounced on the triangles with the longest baselines. Our analysis shows that this variation is related to a systematic change of the source structure. Two inner jet components move non-radially at apparent speeds of ∼15 c and ∼20 c (∼1.3 and ∼1.7 μas day^−1, respectively), which more strongly supports the scenario of traveling shocks or instabilities in a bent, possibly rotating jet. The observed apparent speeds are also coincident with the 3C 279 large-scale jet kinematics observed at longer (cm) wavelengths, suggesting no significant jet acceleration between the 1.3 mm core and the outer jet. The intrinsic brightness temperature of the jet components are ≲10^10 K, a magnitude or more lower than typical values seen at ≥7 mm wavelengths. The low brightness temperature and morphological complexity suggest that the core region of 3C 279 becomes optically thin at short (mm) wavelengths."
MING ZHANG,First M87 Event Horizon Telescope results. I. The shadow of the supermassive black hole,"When surrounded by a transparent emission region, black holes are expected to reveal a dark shadow caused by gravitational light bending and photon capture at the event horizon. To image and study this phenomenon, we have assembled the Event Horizon Telescope, a global very long baseline interferometry array observing at a wavelength of 1.3 mm. This allows us to reconstruct event-horizon-scale images of the supermassive black hole candidate in the center of the giant elliptical galaxy M87. We have resolved the central compact radio source as an asymmetric bright emission ring with a diameter of 42 ± 3 μas, which is circular and encompasses a central depression in brightness with a flux ratio gsim10:1. The emission ring is recovered using different calibration and imaging schemes, with its diameter and width remaining stable over four different observations carried out in different days. Overall, the observed image is consistent with expectations for the shadow of a Kerr black hole as predicted by general relativity. The asymmetry in brightness in the ring can be explained in terms of relativistic beaming of the emission from a plasma rotating close to the speed of light around a black hole. We compare our images to an extensive library of ray-traced general-relativistic magnetohydrodynamic simulations of black holes and derive a central mass of M = (6.5 ± 0.7) × 10^9 M ⊙. Our radio-wave observations thus provide powerful evidence for the presence of supermassive black holes in centers of galaxies and as the central engines of active galactic nuclei. They also present a new tool to explore gravity in its most extreme limit and on a mass scale that was so far not accessible."
MING ZHANG,First Sagittarius A* Event Horizon Telescope results. III. Imaging of the Galactic center supermassive black hole,"We present the first event-horizon-scale images and spatiotemporal analysis of Sgr A* taken with the Event Horizon Telescope in 2017 April at a wavelength of 1.3 mm. Imaging of Sgr A* has been conducted through surveys over a wide range of imaging assumptions using the classical CLEAN algorithm, regularized maximum likelihood methods, and a Bayesian posterior sampling method. Different prescriptions have been used to account for scattering effects by the interstellar medium toward the Galactic center. Mitigation of the rapid intraday variability that characterizes Sgr A* has been carried out through the addition of a “variability noise budget” in the observed visibilities, facilitating the reconstruction of static full-track images. Our static reconstructions of Sgr A* can be clustered into four representative morphologies that correspond to ring images with three different azimuthal brightness distributions and a small cluster that contains diverse nonring morphologies. Based on our extensive analysis of the effects of sparse (u, v)-coverage, source variability, and interstellar scattering, as well as studies of simulated visibility data, we conclude that the Event Horizon Telescope Sgr A* data show compelling evidence for an image that is dominated by a bright ring of emission with a ring diameter of ∼50 μas, consistent with the expected “shadow” of a 4 × 106 M⊙ black hole in the Galactic center located at a distance of 8 kpc."
MING ZHANG,Characterizing and mitigating intraday variability: reconstructing source structure in accreting black holes with mm-VLBI,"The extraordinary physical resolution afforded by the Event Horizon Telescope has opened a window onto the astrophysical phenomena unfolding on horizon scales in two known black holes, M87* and Sgr A*. However, with this leap in resolution has come a new set of practical complications. Sgr A* exhibits intraday variability that violates the assumptions underlying Earth aperture synthesis, limiting traditional image reconstruction methods to short timescales and data sets with very sparse (u, v) coverage. We present a new set of tools to detect and mitigate this variability. We develop a data-driven, model-agnostic procedure to detect and characterize the spatial structure of intraday variability. This method is calibrated against a large set of mock data sets, producing an empirical estimator of the spatial power spectrum of the brightness fluctuations. We present a novel Bayesian noise modeling algorithm that simultaneously reconstructs an average image and statistical measure of the fluctuations about it using a parameterized form for the excess variance in the complex visibilities not otherwise explained by the statistical errors. These methods are validated using a variety of simulated data, including general relativistic magnetohydrodynamic simulations appropriate for Sgr A* and M87*. We find that the reconstructed source structure and variability are robust to changes in the underlying image model. We apply these methods to the 2017 EHT observations of M87*, finding evidence for variability across the EHT observing campaign. The variability mitigation strategies presented are widely applicable to very long baseline interferometry observations of variable sources generally, for which they provide a data-informed averaging procedure and natural characterization of inter-epoch image consistency."
MING ZHANG,First Sagittarius A* Event Horizon Telescope results. I. The shadow of the supermassive black hole in the center of the Milky Way,"We present the first Event Horizon Telescope (EHT) observations of Sagittarius A* (Sgr A*), the Galactic center source associated with a supermassive black hole. These observations were conducted in 2017 using a global interferometric array of eight telescopes operating at a wavelength of λ = 1.3 mm. The EHT data resolve a compact emission region with intrahour variability. A variety of imaging and modeling analyses all support an image that is dominated by a bright, thick ring with a diameter of 51.8 ± 2.3 μas (68% credible interval). The ring has modest azimuthal brightness asymmetry and a comparatively dim interior. Using a large suite of numerical simulations, we demonstrate that the EHT images of Sgr A* are consistent with the expected appearance of a Kerr black hole with mass ∼4 × 106 M ⊙, which is inferred to exist at this location based on previous infrared observations of individual stellar orbits, as well as maser proper-motion studies. Our model comparisons disfavor scenarios where the black hole is viewed at high inclination (i &gt; 50°), as well as nonspinning black holes and those with retrograde accretion disks. Our results provide direct evidence for the presence of a supermassive black hole at the center of the Milky Way, and for the first time we connect the predictions from dynamical measurements of stellar orbits on scales of 103–105 gravitational radii to event-horizon-scale images and variability. Furthermore, a comparison with the EHT results for the supermassive black hole M87* shows consistency with the predictions of general relativity spanning over three orders of magnitude in central mass."
XUEPING FAN,Inhibitory Effects of Robo2 on Nephrin: A Crosstalk between Positive and Negative Signals Regulating Podocyte Structure,"Robo2 is the cell surface receptor for the repulsive guidance cue Slit and is involved in axon guidance and neuronal migration. Nephrin is a podocyte slit- diaphragm protein that functions in the kidney glomerular filtration barrier. Here, we report that Robo2 is expressed at the basal surface of mouse podocytes and colocalizes with nephrin. Biochemical studies indicate that Robo2 forms a complex with nephrin in the kidney through adaptor protein Nck. In contrast to the role of nephrin that promotes actin polymerization, Slit2-Robo2 signaling inhibits nephrin-induced actin polymerization. In addition, the amount of F-actin associated with nephrin is increased in Robo2 knockout mice that develop an altered podocyte foot process structure. Genetic interaction study further reveals that loss of Robo2 alleviates the abnormal podocyte structural pheno- type in nephrin null mice. These results suggest that Robo2 signaling acts as a negative regulator on neph- rin to influence podocyte foot process architecture."
SURESH KALATHUR,Applying Data Mining Techniques Over Big Data,"With rapid development of information technology, data flows in different variety of formats - sensors data, tweets, photos, raw data, and unstructured data. Statistics show that there were 800,000 Petabytes stored in the world in 2000. Today Internet is about 1.8 Zettabytes (Zettabytes is 10^21), and this number will reach 35 Zettabytes by 2020. With that, data management systems are not able to scale to this huge amount of raw, unstructured data, which what is called today big data. In this present study, we show the basic concept and design of big data tools, algorthims [sic] and techniques. We compare the classical data mining algorithms with big data algorthims [sic] by using hadoop/MapReuce [sic] as the core implemention [sic] of big-data for scalable algorthims. [sic] We implemented K-means and A-priori algorthim [sic] by using Hadoop/MapReduce on 5 nodes cluster of hadoop. We also show their performance for Gigabytes of data. Finally, we explore NoSQL (Not Only SQL) databases for semi-structured, massively large-scale of data using MongoDB as an example. Then, we show the performance between HDFS (Hadoop Distributed File System) and MongoDB data stores for these two algorithms."
MAURA WALKER,Associations of accelerometer-measured physical activity and sedentary time with chronic kidney disease: The Framingham Heart Study,"BACKGROUND: Few studies examined the individual and conjoint associations of accelerometer-measured physical activity (PA) and sedentary times with the prevalence of chronic kidney disease (CKD) among older adults. METHODS: We evaluated 1,268 Framingham Offspring Study participants (mean age 69.2 years, 53.8% women) between 2011 and 2014. CKD was defined as an estimated glomerular filtration rate (eGFR) <60 ml/min/1.732 and/or urine albumin-to-creatinine ratio (UACR) ≥25/35 μg/mg (men/women). We used multivariable logistic regression models to relate time spent being sedentary and active with the odds of CKD. We then performed compositional data analysis to estimate the change in the eGFR and UACR when a fixed proportion of time in one activity behavior (among the following: moderate to vigorous physical activity [MVPA], light intensity physical activity [LIPA], and sedentary) is reallocated to another activity behavior. RESULTS: Overall, 258 participants had prevalent CKD (20.4%; 120 women). Higher total PA ([MVPA+LIPA], adjusted-odds ratio [OR] per 30 minutes/day increase, 0.86; 95% CI, 0.78-0.96) and higher LIPA (OR per 30 minutes/day increase, 0.87; 95% CI, 0.76-0.99) were associated with lower odds of CKD. Additionally, higher sedentary time (OR per 30 minutes/day increase, 1.16; 95% CI, 1.04-1.29) was associated with higher odds of CKD. Reallocating 5% of the time from LIPA to sedentary was associated with the largest predicted difference in eGFR (-1.06 ml/min/1.73m2). Reallocating 1% of time spent in MVPA to sedentary status predicted the largest difference in UACR (14.37 μg/mg). CONCLUSION: The findings suggest that increasing LIPA and maintaining MVPA at the expense of sedentary time may be associated with a lower risk of CKD in community-based older adults."
SHAMIRAN MAKO,Ethno-cultural and religious identity of Syrian Orthodox Christians,"Many Middle Eastern Christian groups identify or have been identified with pre-lslamic peoples in the Middle East: the Copts with Ancient Egypt, the Nestorians with Assyria, the Maronites with Phoenicians and some Rum Onhodox and other Christians with pre-lslamic Arab tribes. The concern of this study is the Syrian Orthodox Christians or Jacobite(s) (named after the 6th century Monophysite Christian bishop Yacoub Burd'ono or Jacob Baradaeus of Urfa/Osrohene/Edessa), specifically those whose ancestry stems from the Tur Abdin region of Turkey, Diyarbekir, Mardin, Urfa, and Harput/Elazig."
SHAMIRAN MAKO,Institutionalizing exclusion: de-Ba‘thification in post-2003 Iraq,
SAM GONZALEZ,Scintillation light detection in the 6-m drift-length ProtoDUNE Dual Phase liquid argon TPC,"DUNE is a dual-site experiment for long-baseline neutrino oscillation studies, neutrino astrophysics and nucleon decay searches. ProtoDUNE Dual Phase (DP) is a 6  ×  6  ×  6 m 3 liquid argon time-projection-chamber (LArTPC) that recorded cosmic-muon data at the CERN Neutrino Platform in 2019-2020 as a prototype of the DUNE Far Detector. Charged particles propagating through the LArTPC produce ionization and scintillation light. The scintillation light signal in these detectors can provide the trigger for non-beam events. In addition, it adds precise timing capabilities and improves the calorimetry measurements. In ProtoDUNE-DP, scintillation and electroluminescence light produced by cosmic muons in the LArTPC is collected by photomultiplier tubes placed up to 7 m away from the ionizing track. In this paper, the ProtoDUNE-DP photon detection system performance is evaluated with a particular focus on the different wavelength shifters, such as PEN and TPB, and the use of Xe-doped LAr, considering its future use in giant LArTPCs. The scintillation light production and propagation processes are analyzed and a comparison of simulation to data is performed, improving understanding of the liquid argon properties."
SAM GONZALEZ,Prospects for beyond the standard model physics searches at the deep underground neutrino experiment: DUNE collaboration,"The Deep Underground Neutrino Experiment (DUNE) will be a powerful tool for a variety of physics topics. The high-intensity proton beams provide a large neutrino flux, sampled by a near detector system consisting of a combination of capable precision detectors, and by the massive far detector system located deep underground. This configuration sets up DUNE as a machine for discovery, as it enables opportunities not only to perform precision neutrino measurements that may uncover deviations from the present three-flavor mixing paradigm, but also to discover new particles and unveil new interactions and symmetries beyond those predicted in the Standard Model (SM). Of the many potential beyond the Standard Model (BSM) topics DUNE will probe, this paper presents a selection of studies quantifying DUNE's sensitivities to sterile neutrino mixing, heavy neutral leptons, non-standard interactions, CPT symmetry violation, Lorentz invariance violation, neutrino trident production, dark matter from both beam induced and cosmogenic sources, baryon number violation, and other new physics topics that complement those at high-energy colliders and significantly extend the present reach."
SAM GONZALEZ,Identification and reconstruction of low-energy electrons in the ProtoDUNE-SP detector,
SAMIA HESNI,How to disrupt a social script,"Social scripts, like A gives a compliment, B says ‘thank you’, pervade and shape natural language discourse and social interactions. Scripts usually promote cooperation between conversational participants, but not always. For example, if A pays B a ‘compliment’ like ‘nice legs’, A puts B in a double bind of either abiding by the compliment script by saying ‘thank you’ and being humiliated, or breaking the script and risking escalation. In this paper, I take a philosophical lens to the notion of a social script. I give a theoretical overview of what it would mean to disrupt a social script and explain why and when it is prudential to do so. Then I give several examples of disruptions of social scripts. This essay makes four key contributions to the philosophical literature on social scripts: (1) it introduces a new distinction between interpersonal and structural scripts; (2) it illuminates how interpersonal social scripts can be pernicious by creating a double bind; (3) it analyzes what it is to disrupt a social script; and (4) in doing so, it challenges the orthodoxy about the relationship between cooperation and disruption in political action."
SAMIA HESNI,Philosophical intuitions about socially significant language,"As we theorize about philosophy of language that bears on social and political issues, it is worth revisiting the methodological question of how we as theorists rely on our philosophical and linguistic intuitions, and what assumptions underlie our justification of such a reliance. Two threads in the philosophical literature are relevant to this question: the discussion of situatedness in feminist epistemology and the debate about philosophical expertise and philosophical intuitions. I argue that philosophers examining social and political philosophy of language should be careful—perhaps more careful than we have been—when we rely on our intuitions to draw conclusions about socially significant language, such as racist, sexist, homophobic, and other derogatory speech. I don’t claim we should give up relying on our intuitions. Instead, I argue that we should be more explicit that our intuitions are limited, and open to the possibility that they might not align with the intuitions of those who have more experience with the kinds of speech we are analyzing. As a result, we might find that the conclusions we draw from our intuitions have to be revised or qualified."
JESSICA SIMES,School closures significantly reduced arrests of Black and Latinx urban youth,"Police arrests are common events for youth of color, contributing to increased risk of arrest in adulthood and population health inequities. Although schools are important sites for youth criminalization, research focuses on within-school mechanisms, with limited analysis of hot spots policing in surrounding school areas. Using COVID-19 school closures as an interruption to police activity and in-person school attendance, we estimate Black youth weekly arrests fell from 43.6 to 16.8 per 100,000, vs. 3.57 to 2.17 per 100,000 among White youth. Youth arrest rates declined during two school closure periods: at the start of the pandemic and Summer 2019. A spatial analysis shows Black and Latinx youth experience a higher percentage of arrests near schools than White youth. Our findings show school closures significantly reduce arrests of urban youth of color, and reforms addressing youth criminalization and structural racism should consider the joint spatial context of schools and policing."
JESSICA SIMES,Place and punishment: the spatial context of incarceration,"OBJECTIVES: Research on race and urban poverty views incarceration as a new and important aspect of social disadvantage in inner-city neighborhoods. However, in quantitative studies of the spatial distribution of imprisonment across neighborhoods, the pattern outside urban areas has not been examined. This paper offers a unique analysis of disaggregated prison admissions and investigates the spatial concentrations and levels of admissions for the entire state of Massachusetts. METHODS: Spatial regressions estimate census tract-level prison admission rates in relation to racial demographics, social and economic disadvantage, arrest rates, and violent crime; an analysis of outlier neighborhoods examines the surprisingly high admission rates in small cities. FINDINGS: Regression analysis yields three findings. First, incarceration is highly spatially concentrated: census tracts covering 15% of the state’s population account for half of all prison admissions. Second, across urban and non-urban areas, incarceration is strongly related to concentrated disadvantage and the share of the black population, even after controlling for arrest and crime rates. Third, the analysis shows admission rates in small urban satellite cities and suburbs comprise the highest rates in the sample and far exceed model predictions. CONCLUSION: Mass incarceration emerged not just to manage distinctively urban social problems but was characteristic of a broader mode of governance evident in communities often far-removed from deep inner-city poverty. These notably high levels and concentrations in small cities should be accounted for when developing theories of concentrated disadvantage or policies designed to ameliorate the impacts of mass incarceration on communities."
JESSICA SIMES,Racial disparities in neighborhood arrest rates during the COVID-19 pandemic,"Systemic racism in police contact is an important driver of health inequities among the U.S. urban population. Hyper-policing and police violence in marginalized communities have risen to the top of the national policy agenda, particularly since protests in 2020. How did pandemic conditions impact policing? We assess neighborhood racial disparities in arrests after COVID-19 stay-at-home orders in Boston, Charleston, Pittsburgh, and San Francisco census tracts (January 2019-August 2020). Using interrupted time series models with census tract fixed effects, we report arrest rates across tract racial and ethnic compositions. In the week following stay orders, overall arrest rates were 66% (95% CI: 51-77%) lower on average. Although arrest rates steadily increased thereafter, most tracts did not reach pre-pandemic arrest levels. However, despite declines in nearly all census tracts, the magnitude of racial inequities in arrests remained unchanged. During the initial weeks of the pandemic, arrest rates declined significantly in areas with higher Black populations, but absolute rates in Black neighborhoods remain higher than pre-pandemic arrest rates in White neighborhoods. These findings support urban policy reforms that reconsider police capacity and presence, particularly as a mechanism for enforcing public health ordinances and reducing racial disparities."
JESSICA SIMES,Mental health disparities in solitary confinement,"Harsh prison conditions have been widely examined for their effects on the mental health of incarcerated people. Few studies of punishment examine how mental health status could expose individuals to greater risk of harsh and punitive treatment in the criminal justice system. With prisoners confined to their cells for up to 23 hours each day, often deprived of visitors or phone calls, solitary confinement is an important case for studying both harsh treatment and cumulative disadvantage. Routinely used as punishment for prison misconduct, the quasi-legal process leading to solitary confinement may be subject to the same forces that criminalize the mentally ill in community settings, and drive disparities in treatment. Analyzing a large administrative dataset showing admissions to solitary confinement, we find very high rates of punitive isolation among those with serious mental illness that result from the cumulative effects of disciplinary tickets and disciplinary hearings, in which long periods of solitary confinement are disproportionately dispensed to the mentally ill. We estimate that prisoners with serious mental illness could expect to spend three to four times longer in solitary confinement than a similar person with no history of mental illness. These findings suggest the stigma of dangerousness follows individuals into prison, providing new evidence of how the criminalization of mental health conditions also accompany greater severity of incarceration."
JESSICA SIMES,Place after prison: neighborhood attachment and attainment during reentry,"Over 600,000 people leave prison and become residents of neighborhoods across the United States annually. Using a longitudinal survey of people returning to Greater Boston, this study examines disparities in neighborhood attainment after prison. Accounting for levels of pre-prison neighborhood disadvantage, Black and Hispanic respondents moved into significantly more disadvantaged areas than whites. Forty percent of respondents initially moved to only one of two Boston community areas. Housing is an important neighborhood sorting mechanism: living in concentrated disadvantage was more likely for those residing in household arrangements with family or friends, or in emergency or transitional housing. Significantly, neighborhood residence was not attained by all: a quarter of respondents left prison and entered formal institutional settings or lived in extreme social marginality throughout Boston. Housing insecurity, re-incarceration, and profound racial disparities in neighborhood context explain the ecological structure of social inequality in urban neighborhoods in an era of mass incarceration."
JESSICA SIMES,The population prevalence of solitary confinement,"Solitary confinement is a severe form of incarceration closely associated with long-lasting psychological harm and poor post-release outcomes. Estimating the population prevalence, we find that 11% of all black men in Pennsylvania, born 1986 to 1989, were incarcerated in solitary confinement by age 32. Reflecting large racial disparities, the population prevalence is only 3.4% for Latinos and 1.4% for white men. About 9% of black men in the state cohort were held in solitary for more than 15 consecutive days, violating the United Nations standards for minimum treatment of incarcerated people. Nearly 1 in 100 black men experienced solitary for a year or longer by age 32. Racial disparities are similar for women, but rates are lower. A decomposition shows that black men’s high risk of solitary confinement stems primarily from their high imprisonment rate. Findings suggest that harsh conditions of U.S. incarceration have population-level effects on black men’s well-being."
JESSICA SIMES,Solitary confinement and institutional harm,
JESSICA SIMES,"Out-of-control criminal justice: the systems improvement solution for more safety, justice, accountability, and efficiency",
JESSICA SIMES,The consequences of Medicaid expansion under the Affordable Care Act for police arrests,"BACKGROUND & METHODS: National protests in the summer of 2020 drew attention to the significant presence of police in marginalized communities. Recent social movements have called for substantial police reforms, including “defunding the police,” a phrase originating from a larger, historical abolition movement advocating that public investments be redirected away from the criminal justice system and into social services and health care. Although research has demonstrated the expansive role of police to respond a broad range of social problems and health emergencies, existing research has yet to fully explore the capacity for health insurance policy to influence rates of arrest in the population. To fill this gap, we examine the potential effect of Medicaid expansion under the Affordable Care Act (ACA) on arrests in 3,035 U.S. counties. We compare county-level arrests using FBI Uniform Crime Reporting (UCR) Program Data before and after Medicaid expansion in 2014–2016, relative to counties in non-expansion states. We use difference-in-differences (DID) models to estimate the change in arrests following Medicaid expansion for overall arrests, and violent, drug, and low-level arrests. RESULTS: Police arrests significantly declined following the expansion of Medicaid under the ACA. Medicaid expansion produced a 20–32% negative difference in overall arrests rates in the first three years. We observe the largest negative differences for drug arrests: we find a 25–41% negative difference in drug arrests in the three years following Medicaid expansion, compared to non-expansion counties. We observe a 19–29% negative difference in arrests for violence in the three years after Medicaid expansion, and a decrease in low-level arrests between 24–28% in expansion counties compared to non-expansion counties. Our main results for drug arrests are robust to multiple sensitivity analyses, including a state-level model. CONCLUSIONS: Evidence in this paper suggests that expanded Medicaid insurance reduced police arrests, particularly drug-related arrests. Combined with research showing the harmful health consequences of chronic policing in disadvantaged communities, greater insurance coverage creates new avenues for individuals to seek care, receive treatment, and avoid criminalization. As police reform is high on the agenda at the local, state, and federal level, our paper supports the perspective that broad health policy reforms can meaningfully reduce contact with the criminal justice system under historic conditions of mass criminalization."
JESSICA SIMES,Prenatal healthcare after sentencing reform: heterogeneous effects for prenatal healthcare access and equity,"BACKGROUND: High rates of imprisonment in the U.S. have significant health, social, and economic consequences, particularly for marginalized communities. This study examines imprisonment as a contextual driver of receiving prenatal care by evaluating whether early and adequate prenatal care improved after Pennsylvania’s criminal sentencing reform reduced prison admissions. METHODS: We linked individual-level birth certificate microdata on births (n = 999,503) in Pennsylvania (2009–2015), to monthly county-level rates of prison admissions. We apply an interrupted time series approach that contrasts post-policy changes in early and adequate prenatal care across counties where prison admissions were effectively reduced or continued to rise. We then tested whether prenatal care improvements were stronger among Black birthing people and those with lower levels of educational attainment. RESULTS: In counties where prison admissions declined the most after the policy, early prenatal care increased from 69.0% to 73.2%, and inadequate prenatal care decreased from 18.1% to 15.9%. By comparison, improvements in early prenatal care were smaller in counties where prison admissions increased the most post-policy (73.5 to 76.4%) and there was no change to prenatal care inadequacy (14.4% pre and post). We find this pattern of improvements to be particularly strong among Black birthing people and those with lower levels of educational attainment. CONCLUSIONS: Pennsylvania’s sentencing reforms were associated with small advancements in racial and socioeconomic equity in prenatal care."
JESSICA SIMES,Solitary confinement and the U.S. prison boom,"Solitary confinement is a harsh form custody involving isolation from the general prison population and highly restricted access to visitation and programs. Using detailed prison records covering 30 years of practices in Kansas (1985–2014), we find solitary confinement is a normal event during imprisonment: 38 percent of whites and 46 percent of blacks experienced solitary confinement during their prison term. Long stays in solitary confinement were rare in the late 1980s with no detectable racial disparities, but a sharp increase in capacity after a new prison opening began an era of long-term isolation that most heavily impacted black young adults. A decomposition analysis indicates the increase in the length of stay in solitary confinement almost entirely explains the growth in the proportion of people held in solitary confinement. Our results provide new evidence of increasingly punitive prison conditions and previously unmeasured forms of inequality during the prison boom."
JESSICA SIMES,Drug use in the year after prison,"With poor health and widespread drug problems in the U.S. prison population, post-prison drug use provides an important measure of both public health and social integration following incarceration. We study the correlates of drug use with data from the Boston Reentry Study (BRS), a survey of men and women interviewed four times over the year after prison release. The BRS data allow an analysis of legal and illegal drug use, and the correlation between them. We find that illegal drug use is associated with histories of drug problems and childhood trauma. Use of medications is associated with poor physical health and a history of mental illness. Legal and illegal drug use are not strongly correlated. Results suggest that in a Medicaid expansion state where health coverage is widely provided to people leaving prison, formerly-incarcerated men and women use medications, not illegal drugs, to address their health needs."
JESSICA SIMES,The ecology of race and punishment across cities,"In an era of mass incarceration in the United States, neighborhood context plays a significant role in demographic patterns of imprisonment. This paper examines the preprison neighborhood environment of racial and ethnic groups within the Massachusetts prison admission population. The data include over 12,000 prison records of individuals sentenced to state prison for a criminal offense between 2009 and 2014. Findings indicate significant spatial variation across racial groups: The most disadvantaged preprison neighborhoods exist in small cities outside of Boston. Whites and Hispanics who enter prison from small cities, suburbs, and rural towns in Massachusetts lived in significantly more concentrated disadvantage than their counterparts in Boston. However, black men and women coming from Boston and small cities lived in the greatest concentrated disadvantage among the black admission population. Black‐ and Hispanic‐incarcerated people lived in significantly higher levels of concentrated disadvantage as compared to the average neighborhood of white‐incarcerated people. Results indicate that the prison population is drawn from a diverse set of communities, and suggest that an understanding of the full picture of differences in neighborhood context may play an important role in understanding community‐level conditions of mass incarceration and inform interventions aimed at ameliorating the community‐level impacts of imprisonment."
JOSHUA DAVIES,"A meta-analysis of pharmacotherapy for social anxiety disorder: an examination of efficacy, moderators, and mediators","INTRODUCTION: Social anxiety disorder (SAD) is among the most prevalent mental disorders, associated with impaired functioning and poor quality of life. Pharmacotherapy is the most widely utilized treatment option. The current study provides an updated meta-analytic review of the efficacy of pharmacotherapy and examines moderators and mediators of treatment efficacy. Areas Covered: A comprehensive search of the current literature yielded 52 randomized, pill placebo-controlled trials of pharmacotherapy for adults diagnosed with SAD. Data on potential mediators of treatment outcome were collected, as well as data necessary to calculate pooled correlation matrices to compute indirect effects. Expert Opinion: The overall effect size of pharmacotherapy for SAD is small to medium (Hedges' g = 0.41). Effect sizes were not moderated by age, sex, length of treatment, initial severity, risk of study bias, or publication year. Furthermore, reductions in symptoms mediated pharmacotherapy's effect on quality of life. Support was found for reverse mediation. Future directions may include sustained efforts to examine treatment mechanisms of pharmacotherapy using rigorous longitudinal methodology to better establish temporal precedence."
JOSHUA DAVIES,The long-baseline neutrino experiment: exploring fundamental symmetries of the universe,"The preponderance of matter over antimatter in the early Universe, the dynamics of the supernova bursts that produced the heavy elements necessary for life and whether protons eventually decay --- these mysteries at the forefront of particle physics and astrophysics are key to understanding the early evolution of our Universe, its current state and its eventual fate. The Long-Baseline Neutrino Experiment (LBNE) represents an extensively developed plan for a world-class experiment dedicated to addressing these questions. LBNE is conceived around three central components: (1) a new, high-intensity neutrino source generated from a megawatt-class proton accelerator at Fermi National Accelerator Laboratory, (2) a near neutrino detector just downstream of the source, and (3) a massive liquid argon time-projection chamber deployed as a far detector deep underground at the Sanford Underground Research Facility. This facility, located at the site of the former Homestake Mine in Lead, South Dakota, is approximately 1,300 km from the neutrino source at Fermilab -- a distance (baseline) that delivers optimal sensitivity to neutrino charge-parity symmetry violation and mass ordering effects. This ambitious yet cost-effective design incorporates scalability and flexibility and can accommodate a variety of upgrades and contributions. With its exceptional combination of experimental configuration, technical capabilities, and potential for transformative discoveries, LBNE promises to be a vital facility for the field of particle physics worldwide, providing physicists from around the globe with opportunities to collaborate in a twenty to thirty year program of exciting science. In this document we provide a comprehensive overview of LBNE's scientific objectives, its place in the landscape of neutrino physics worldwide, the technologies it will incorporate and the capabilities it will possess."
ADAM B HALL,"Canvass: a crowd-sourced, natural-product screening library for exploring biological space",
ADAM B HALL,The eighteenth data release of the Sloan Digital Sky Surveys: targeting and first spectra from SDSS-V,"The eighteenth data release (DR18) of the Sloan Digital Sky Survey (SDSS) is the first one for SDSS-V, the fifth generation of the survey. SDSS-V comprises three primary scientific programs or “Mappers”: the Milky Way Mapper (MWM), the Black Hole Mapper (BHM), and the Local Volume Mapper. This data release contains extensive targeting information for the two multiobject spectroscopy programs (MWM and BHM), including input catalogs and selection functions for their numerous scientific objectives. We describe the production of the targeting databases and their calibration and scientifically focused components. DR18 also includes ∼25,000 new SDSS spectra and supplemental information for X-ray sources identified by eROSITA in its eFEDS field. We present updates to some of the SDSS software pipelines and preview changes anticipated for DR19. We also describe three value-added catalogs (VACs) based on SDSS-IV data that have been published since DR17, and one VAC based on the SDSS-V data in the eFEDS field."
CHAD WILLIAM FARRIS,Effects of hypertension and age on cerebrovasculature and cognition in the monkey,"Cognitive impairments in memory and executive function are commonly observed with aging in humans. Age-related cognitive decline is exacerbated by hypertension, but it is less clear whether hypertension can lead to cognitive impairment in young adults. Rhesus monkeys evidence cognitive impairment with age that parallels that seen in humans and have been established as an appropriate animal model to examine the neurobiological basis of age-related cognitive decline. The development of a technique to produce chronic hypertension in the monkey has permitted the expansion of this model to evaluate the effects of hypertension in both young adult and aged monkeys. We used this model to examine whether hypertension alone is sufficient to cause impairment in learning and memory in the young adult. In order to gain a better understanding of the etiology of cognitive decline with hypertension or age, we then evaluated the integrity of cerebral microvasculature in hypertensive and in aged cognitively characterized rhesus monkeys using magnetic resonance imaging to measure cerebral blood volume (CBV) and image analysis on serial brain sections immunohistochemically stained for type IV collagen to quantify the microvasculature. Lastly, we utilized dynamic R2-star mapping to assess hippocampal vasoreactivity to carbon dioxide challenge from early adulthood to old age. The results demonstrated that induction of hypertension in young adult monkeys impaired relearning of a previously learned rule, but not visual recognition memory. Morphological measurement showed an increase in vascularity of the cingulum bundle with age but not in three other preselected areas, but this change did not correlate with cognitive impairments. MRI measurement of CBV in gray matter increased with duration of hypertension, but did not correlate with cognitive function. Functional vasoreactivity to carbon dioxide challenge was reduced in the right hippocampus, but not the left with age. The right hippocampal vasoreactivity, but not left, correlated with performance on two of three hippocampal dependent memory tasks in middle-aged animals. Together, the results of these studies suggest that hypertension in young adulthood can impair some aspects of cognition, and while both structural and functional microvascular alterations occur with aging or hypertension, only functional alterations correlate with cognitive function."
LINDA MIRIAM AFIFI,The neural basis of behavioral recovery of chronic visuo-spatial neglect using repetitive transcranial magnetic stimulation,"High frequency repetitive Transcranial Magnetic Stimulation (rTMS) has been used clinically on perilesional cortex to attenuate the neurological consequences of cerebral injuries. Presently the clinical benefits of rTMS remain controversial due to the high level of inter-individual variability in response to treatment. We explored the behavioral efficacy of multisession rTMS and studied the structural and metabolic characteristics underlying the recovery of visuo-spatial functions after unilateral parietal injury. A group of adult cats (n=12) underwent focal lesions in a region of the posterior parietal cortex (pMS, posterior middle suprasylvian area) leading to contralesional visuo-spatial deficits. Two and a half months post-injury, cats were treated with 7 consecutive rounds of 10 daily session of high frequency rTMS applied on an intact perilesional area (aMS, anterior middle suprasylvian area). We characterized two populations of animals: a group of 'Responders' (n=6) displaying significant visual detection improvements in contralesional hemispace and a group of 'Non-responders' (n=6) that did not show significant improvements and suffered unexpected decreases in ipsilesional performance. Detailed lesion analyses revealed no differences between the two groups neither in the amount of total spared cortex nor in the level of residual metabolic bactivity within areas of perilesional tissue. Additionally, we used 14C-2-deoxyglucose (2DG) to measure the metabolic impact of the aMS area on discrete nodes of the visuo-spatial network. When compared individually, none of the 60 regions proved to have significantly higher metabolic uptake in either group. Interestingly however, a cross-correlation analysis of metabolic activity throughout all sampled cortical and subcortical areas revealed that 'Responders' displayed a more metabolically cross-correlated visuo-spatial network than 'Nonresponders'. Furthermore, we found the aMS cortex in the 'Non-responders' group to hold no significant cross-correlations within the visuo-spatial network, indicating that the site of rTMS stimulation held no functional relationship with network regions. We conclude that multisession perilesional rTMS has the potential to induce behavioral ameliorations in chronic brain injury but the ability to consistently induce adaptive outcomes remains highly variable across animals. In addition, functional correlations indicate that rTMS-mediated recovery is dependent on the stimulated aMS cortex and its metabolic coupling with other nodes within visuo-spatial networks."
LUKE GLOWACKI,Norm violations and punishments across human societies,"Punishments for norm violations are hypothesised to be a crucial component of the maintenance of cooperation in humans but are rarely studied from a comparative perspective. We investigated the degree to which punishment systems were correlated with socioecology and cultural history. We took data from the Standard Cross-Cultural Sample database and coded ethnographic documents from a sample of 131 largely non-industrial societies. We recorded whether punishment for norm violations concerned adultery, religion, food, rape or war cowardice and whether sanctions were reputational, physical, material or execution. We used Bayesian phylogenetic regression modelling to test for culture-level covariation. We found little evidence of phylogenetic signals in evidence for punishment types, suggesting that punishment systems change relatively quickly over cultural evolutionary history. We found evidence that reputational punishment was associated with egalitarianism and the absence of food storage; material punishment was associated with the presence of food storage; physical punishment was moderately associated with greater dependence on hunting; and execution punishment was moderately associated with social stratification. Taken together, our results suggest that the role and kind of punishment vary both by the severity of the norm violation, but also by the specific socio-economic system of the society."
FILIPE MAIA,Focus: Summer 2020,
FILIPE MAIA,Betrayed by accent: theological notes on a racist worldsound,"This volume brings sustainability studies into creative and constructive conversation with actions, practices, and worldviews from religion and theology supportive of the vision and work of the UN SDGs."
FILIPE MAIA,The rise of the common: spiritual revival and political revolution in the Wesleyan movement,"The hope of the gospel that is at the heart of the Methodist evangelical holiness traditions needs to be tested and verified in concrete transformations, which will be spelled out in the chapters of this book.Contributors: Cliff Bird, ..."
FILIPE MAIA,The Pan-Amazon Synod,"The Pan-Amazon Synod concluded on October 27, 2019 with a call for a renewed Roman Catholic presence in Amazonia, the recognition of the need for an “integral ecological conversion,” the importance of developing indigenous theologies and inculturated liturgies, and the call for the recognition of women into the permanent diaconate of the Church. This essay details some of the discussion topics addressed by the Synod and locates these conversations in the context of the turbulent Brazilian political scenario, where the Synod has been attacked by political authorities."
FILIPE MAIA,"A theology of the drug war: globalization, violence, and salvation",
FILIPE MAIA,Alter-carnation: notes on cannibalism and coloniality in the Brazilian context,"Beyond Man reimagines the meaning and potential of a philosophy of religion that better attends to the inextricable links among religion, racism, and colonialism."
JONATHAN BUONOCORE,Evaluation of the Public Health Impacts of Traffic Congestion: A Health Risk Assessment,"BACKGROUND: Traffic congestion is a significant issue in urban areas in the United States and around the world. Previous analyses have estimated the economic costs of congestion, related to fuel and time wasted, but few have quantified the public health impacts or determined how these impacts compare in magnitude to the economic costs. Moreover, the relative magnitudes of economic and public health impacts of congestion would be expected to vary significantly across urban areas, as a function of road infrastructure, population density, and atmospheric conditions influencing pollutant formation, but this variability has not been explored. METHODS: In this study, we evaluate the public health impacts of ambient exposures to fine particulate matter (PM2.5) concentrations associated with a business-as-usual scenario of predicted traffic congestion. We evaluate 83 individual urban areas using traffic demand models to estimate the degree of congestion in each area from 2000 to 2030. We link traffic volume and speed data with the MOBILE6 model to characterize emissions of PM2.5 and particle precursors attributable to congestion, and we use a source-receptor matrix to evaluate the impact of these emissions on ambient PM2.5 concentrations. Marginal concentration changes are related to a concentration-response function for mortality, with a value of statistical life approach used to monetize the impacts. RESULTS: We estimate that the monetized value of PM2.5-related mortality attributable to congestion in these 83 cities in 2000 was approximately $31 billion (2007 dollars), as compared with a value of time and fuel wasted of $60 billion. In future years, the economic impacts grow (to over $100 billion in 2030) while the public health impacts decrease to $13 billion in 2020 before increasing to $17 billion in 2030, given increasing population and congestion but lower emissions per vehicle. Across cities and years, the public health impacts range from more than an order of magnitude less to in excess of the economic impacts. CONCLUSIONS: Our analyses indicate that the public health impacts of congestion may be significant enough in magnitude, at least in some urban areas, to be considered in future evaluations of the benefits of policies to mitigate congestion."
JONATHAN BUONOCORE,Home is where the pipeline ends: characterization of volatile organic compounds present in natural gas at the point of the residential end user,"The presence of volatile organic compounds (VOCs) in unprocessed natural gas (NG) is well documented; however, the degree to which VOCs are present in NG at the point of end use is largely uncharacterized. We collected 234 whole NG samples across 69 unique residential locations across the Greater Boston metropolitan area, Massachusetts. NG samples were measured for methane (CH4), ethane (C2H6), and nonmethane VOC (NMVOC) content (including tentatively identified compounds) using commercially available USEPA analytical methods. Results revealed 296 unique NMVOC constituents in end use NG, of which 21 (or approximately 7%) were designated as hazardous air pollutants. Benzene (bootstrapped mean = 164 ppbv; SD = 16; 95% CI: 134-196) was detected in 95% of samples along with hexane (98% detection), toluene (94%), heptane (94%), and cyclohexane (89%), contributing to a mean total concentration of NMVOCs in distribution-grade NG of 6.0 ppmv (95% CI: 5.5-6.6). While total VOCs exhibited significant spatial variability, over twice as much temporal variability was observed, with a wintertime NG benzene concentration nearly eight-fold greater than summertime. By using previous NG leakage data, we estimated that 120-356 kg/yr of annual NG benzene emissions throughout Greater Boston are not currently accounted for in emissions inventories, along with an unaccounted-for indoor portion. NG-odorant content (tert-butyl mercaptan and isopropyl mercaptan) was used to estimate that a mean NG-CH4 concentration of 21.3 ppmv (95% CI: 16.7-25.9) could persist undetected in ambient air given known odor detection thresholds. This implies that indoor NG leakage may be an underappreciated source of both CH4 and associated VOCs."
BENJAMIN MARX,The institutional foundations of religious politics: evidence from Indonesia,"Why do religious politics thrive in some societies but not others? This paper explores the institutional foundations of this process in Indonesia, the world’s largest Muslim country. We show that an important Islamic institution, the waqf, fostered the entrenchment of Islamism at a critical juncture. In the early 1960s, rural elites transferred large amounts of land into waqf—a type of inalienable charitable trust—to avoid expropriation by the government as part of a major land reform effort. We exploit policy rules to show that greater intensity of the planned reform led to more prevalent waqf land and Islamic institutions endowed as such, including mosques and religious schools. After Indonesia’s democratic transition, the Islamist movement leveraged these endowments to confront the secular state and expand the influence of religion in public life. We identify lasting effects on electoral support for Islamist parties, the adoption of sharia regulations, and the size of the religious sector. These changes do not come from higher religiosity, but are instead driven by distinct views about the role of religion in government. However, this also comes with economic costs, particularly in agriculture where large waqf endowments reduce productivity. Overall, our findings shed new light on the origins and consequences of Islamism."
JOHN PAN,Isolation and synthesis of novel meroterpenoids from rhodomyrtus tomentosa: investigation of a reactive enetrione intermediate,"Rhodomyrtusials A-C, the first examples of triketone-sesquiterpene meroterpenoids featuring a unique 6/5/5/9/4 fused pentacyclic ring system were isolated from Rhodomyrtus tomentosa, along with several biogenetically-related dihydropyran isomers. Two bis-furans and one dihydropyran isomer showed acetylcholinesterase (AChE) inhibitory activity. Structures of the isolates were unambiguously established by a combination of spectroscopic data, ECD analysis, and total synthesis. Bioinspired total syntheses of six isolates were achieved in six steps utilizing a reactive enetrione intermediate generated in situ from a readily available hydroxy-endoperoxide precursor."
JOHN PAN,"Cosmology intertwined: a review of the particle physics, astrophysics, and cosmology associated with the cosmological tensions and anomalies",
JOHN PAN,The eighteenth data release of the Sloan Digital Sky Surveys: targeting and first spectra from SDSS-V,"The eighteenth data release (DR18) of the Sloan Digital Sky Survey (SDSS) is the first one for SDSS-V, the fifth generation of the survey. SDSS-V comprises three primary scientific programs or “Mappers”: the Milky Way Mapper (MWM), the Black Hole Mapper (BHM), and the Local Volume Mapper. This data release contains extensive targeting information for the two multiobject spectroscopy programs (MWM and BHM), including input catalogs and selection functions for their numerous scientific objectives. We describe the production of the targeting databases and their calibration and scientifically focused components. DR18 also includes ∼25,000 new SDSS spectra and supplemental information for X-ray sources identified by eROSITA in its eFEDS field. We present updates to some of the SDSS software pipelines and preview changes anticipated for DR19. We also describe three value-added catalogs (VACs) based on SDSS-IV data that have been published since DR17, and one VAC based on the SDSS-V data in the eFEDS field."
JAMES JOSEPH HERMES,The eighteenth data release of the Sloan Digital Sky Surveys: targeting and first spectra from SDSS-V,"The eighteenth data release (DR18) of the Sloan Digital Sky Survey (SDSS) is the first one for SDSS-V, the fifth generation of the survey. SDSS-V comprises three primary scientific programs or “Mappers”: the Milky Way Mapper (MWM), the Black Hole Mapper (BHM), and the Local Volume Mapper. This data release contains extensive targeting information for the two multiobject spectroscopy programs (MWM and BHM), including input catalogs and selection functions for their numerous scientific objectives. We describe the production of the targeting databases and their calibration and scientifically focused components. DR18 also includes ∼25,000 new SDSS spectra and supplemental information for X-ray sources identified by eROSITA in its eFEDS field. We present updates to some of the SDSS software pipelines and preview changes anticipated for DR19. We also describe three value-added catalogs (VACs) based on SDSS-IV data that have been published since DR17, and one VAC based on the SDSS-V data in the eFEDS field."
ANDREY FRADKIN,Competition avoidance vs herding in job search: evidence from large-scale field experiments on an online job board,
ANDREY FRADKIN,Do incentives to review help the market? Evidence from a field experiment on Airbnb,"Online reviews are typically written by volunteers and, consequently, accurate information about seller quality may be underprovided. We study the extent of this under-provision in a randomized experiment conducted by Airbnb. In the treatment, buyers are offered a coupon to review listings that have no prior reviews. The treatment induces additional reviews, which are more negative on average. Induced reviews do not change nights sold, although they affect the types of transactions that occur. Measures of transaction quality for treated sellers do not improve. We show how market conditions and the design of the reputation system can explain our findings."
HENRI LEE,Climate change and the kidney,"The worldwide increase in temperature has resulted in a marked increase in heat waves (heat extremes) that carries a markedly increased risk for morbidity and mortality. The kidney has a unique role not only in protecting the host from heat and dehydration but also is an important site of heat-associated disease. Here we review the potential impact of global warming and heat extremes on kidney diseases. High temperatures can result in increased core temperatures, dehydration, and blood hyperosmolality. Heatstroke (both clinical and subclinical whole-body hyperthermia) may have a major role in causing both acute kidney disease, leading to increased risk of acute kidney injury from rhabdomyolysis, or heat-induced inflammatory injury to the kidney. Recurrent heat and dehydration can result in chronic kidney disease (CKD) in animals and theoretically plays a role in epidemics of CKD developing in hot regions of the world where workers are exposed to extreme heat. Heat stress and dehydration also has a role in kidney stone formation, and poor hydration habits may increase the risk for recurrent urinary tract infections. The resultant social and economic consequences include disability and loss of productivity and employment. Given the rise in world temperatures, there is a major need to better understand how heat stress can induce kidney disease, how best to provide adequate hydration, and ways to reduce the negative effects of chronic heat exposure."
HENRI LEE,"Bostonia: v. 16, no. 1-9",
HENRI LEE,"Bostonia: v. 10, no. 1-10",
HENRI LEE,A systems approach to the evolution of antibiotic resistance,"Antibiotic-resistant bacterial strains continually arise and their increasing prevalence poses significant clinical and societal challenges. Functional analyses of resistant mutants and the study of general stress responses perturbed by antibiotic treatment have yielded valuable insights into how resistance arises through mutations. However, less is known about the population dynamics and communal interactions that underlie the development of resistance through mutations. In this work, we utilize systems approaches to study the functional dynamics of bacterial populations evolving antibiotic resistance. We follow a continuous culture of Escherichia coli facing increasing levels of antibiotic and show that the vast majority of isolates are less resistant than the population as a whole. We find that the few highly resistant mutants improve the survival of the populations less resistant constituents, in part, by producing indole, a signaling molecule generated by actively growing and unstressed cells. We show, through transcriptional profiling, that indole serves to turn on drug efflux pumps and oxidative stress protective mechanisms. The indole production comes at a fitness cost to the highly resistant isolates, and wholegenome sequencing reveals that this bacterial altruism is enabled by drug-resistance mutations unrelated to indole production. This work establishes a population-based resistance mechanism constituting a form of kin selection whereby a small number of resistant mutants can, at some cost to themselves, provide protection to other more vulnerable cells, enhancing the survival capacity of the overall population in stressful environments. Deeper studies into cooperative strategies bacteria use to evade antibiotics may prove critical for the rational design of more effective antimicrobial interventions."
HENRI LEE,Measurement of the positive muon anomalous magnetic moment to 0.46 ppm,"We present the first results of the Fermilab National Accelerator Laboratory (FNAL) Muon g-2 Experiment for the positive muon magnetic anomaly a_{μ}≡(g_{μ}-2)/2. The anomaly is determined from the precision measurements of two angular frequencies. Intensity variation of high-energy positrons from muon decays directly encodes the difference frequency ω_{a} between the spin-precession and cyclotron frequencies for polarized muons in a magnetic storage ring. The storage ring magnetic field is measured using nuclear magnetic resonance probes calibrated in terms of the equivalent proton spin precession frequency ω[over ˜]_{p}^{'} in a spherical water sample at 34.7 °C. The ratio ω_{a}/ω[over ˜]_{p}^{'}, together with known fundamental constants, determines a_{μ}(FNAL)=116 592 040(54)×10^{-11} (0.46 ppm). The result is 3.3 standard deviations greater than the standard model prediction and is in excellent agreement with the previous Brookhaven National Laboratory (BNL) E821 measurement. After combination with previous measurements of both μ^{+} and μ^{-}, the new experimental average of a_{μ}(Exp)=116 592 061(41)×10^{-11} (0.35 ppm) increases the tension between experiment and theory to 4.2 standard deviations."
HENRI LEE,Scintillation light detection in the 6-m drift-length ProtoDUNE Dual Phase liquid argon TPC,"DUNE is a dual-site experiment for long-baseline neutrino oscillation studies, neutrino astrophysics and nucleon decay searches. ProtoDUNE Dual Phase (DP) is a 6  ×  6  ×  6 m 3 liquid argon time-projection-chamber (LArTPC) that recorded cosmic-muon data at the CERN Neutrino Platform in 2019-2020 as a prototype of the DUNE Far Detector. Charged particles propagating through the LArTPC produce ionization and scintillation light. The scintillation light signal in these detectors can provide the trigger for non-beam events. In addition, it adds precise timing capabilities and improves the calorimetry measurements. In ProtoDUNE-DP, scintillation and electroluminescence light produced by cosmic muons in the LArTPC is collected by photomultiplier tubes placed up to 7 m away from the ionizing track. In this paper, the ProtoDUNE-DP photon detection system performance is evaluated with a particular focus on the different wavelength shifters, such as PEN and TPB, and the use of Xe-doped LAr, considering its future use in giant LArTPCs. The scintillation light production and propagation processes are analyzed and a comparison of simulation to data is performed, improving understanding of the liquid argon properties."
HENRI LEE,Identification and reconstruction of low-energy electrons in the ProtoDUNE-SP detector,
HENRI LEE,The long-baseline neutrino experiment: exploring fundamental symmetries of the universe,"The preponderance of matter over antimatter in the early Universe, the dynamics of the supernova bursts that produced the heavy elements necessary for life and whether protons eventually decay --- these mysteries at the forefront of particle physics and astrophysics are key to understanding the early evolution of our Universe, its current state and its eventual fate. The Long-Baseline Neutrino Experiment (LBNE) represents an extensively developed plan for a world-class experiment dedicated to addressing these questions. LBNE is conceived around three central components: (1) a new, high-intensity neutrino source generated from a megawatt-class proton accelerator at Fermi National Accelerator Laboratory, (2) a near neutrino detector just downstream of the source, and (3) a massive liquid argon time-projection chamber deployed as a far detector deep underground at the Sanford Underground Research Facility. This facility, located at the site of the former Homestake Mine in Lead, South Dakota, is approximately 1,300 km from the neutrino source at Fermilab -- a distance (baseline) that delivers optimal sensitivity to neutrino charge-parity symmetry violation and mass ordering effects. This ambitious yet cost-effective design incorporates scalability and flexibility and can accommodate a variety of upgrades and contributions. With its exceptional combination of experimental configuration, technical capabilities, and potential for transformative discoveries, LBNE promises to be a vital facility for the field of particle physics worldwide, providing physicists from around the globe with opportunities to collaborate in a twenty to thirty year program of exciting science. In this document we provide a comprehensive overview of LBNE's scientific objectives, its place in the landscape of neutrino physics worldwide, the technologies it will incorporate and the capabilities it will possess."
SAMUEL KRISHNA DASARATHA,Virus dynamics with behavioral responses,
ELAINE L LEE,The long-baseline neutrino experiment: exploring fundamental symmetries of the universe,"The preponderance of matter over antimatter in the early Universe, the dynamics of the supernova bursts that produced the heavy elements necessary for life and whether protons eventually decay --- these mysteries at the forefront of particle physics and astrophysics are key to understanding the early evolution of our Universe, its current state and its eventual fate. The Long-Baseline Neutrino Experiment (LBNE) represents an extensively developed plan for a world-class experiment dedicated to addressing these questions. LBNE is conceived around three central components: (1) a new, high-intensity neutrino source generated from a megawatt-class proton accelerator at Fermi National Accelerator Laboratory, (2) a near neutrino detector just downstream of the source, and (3) a massive liquid argon time-projection chamber deployed as a far detector deep underground at the Sanford Underground Research Facility. This facility, located at the site of the former Homestake Mine in Lead, South Dakota, is approximately 1,300 km from the neutrino source at Fermilab -- a distance (baseline) that delivers optimal sensitivity to neutrino charge-parity symmetry violation and mass ordering effects. This ambitious yet cost-effective design incorporates scalability and flexibility and can accommodate a variety of upgrades and contributions. With its exceptional combination of experimental configuration, technical capabilities, and potential for transformative discoveries, LBNE promises to be a vital facility for the field of particle physics worldwide, providing physicists from around the globe with opportunities to collaborate in a twenty to thirty year program of exciting science. In this document we provide a comprehensive overview of LBNE's scientific objectives, its place in the landscape of neutrino physics worldwide, the technologies it will incorporate and the capabilities it will possess."
SARAH DAVIES,De novo transcriptome assembly of the clown anemonefish (Amphiprion percula): a new resource to study the evolution of fish color,"A fundamental question of evolutionary biology is, why are some animals conspicuously colored? This question may be addressed from both a proximate (genetic and ontogenetic) and ultimate (adaptive value and evolutionary origins) perspective, and integrating these perspectives can provide further insights. Over the last few decades we have made great advances in understanding the causes of conspicuous coloration in terrestrial systems, e.g., birds and butterflies, but we still know relatively little about the causes of conspicuous, “poster” coloration in coral reef fishes. Of all coral reef fishes, the clownfish Amphiprion percula, is perhaps the most conspicuously colored, possessing a bright orange body with three iridescent white bars bordered with pitch black. Here, we review what is known about the proximate and ultimate causes of the conspicuous coloration of clownfishes Amphiprion sp.: coloration has a heritable genetic basis; coloration is influenced by development and environment; coloration has multiple plausible signaling functions; there is a phylogenetic component to coloration. Subsequently, to provide new insights into the genetic mechanisms and potential functions of A. percula coloration we (i) generate the first de novo transcriptome for this species, (ii) conduct differential gene expression analyses across different colored epidermal tissues, and (iii) conduct gene ontology (GO) enrichment analyses to characterize function of these differentially expressed genes. BUSCO indicated that transcriptome assembly was successful and many genes were found to be differentially expressed between epidermal tissues of different colors. In orange tissue, relative to white and black, many GO terms associated with muscle were over-represented. In white tissue, relative to orange and black tissue, there were very few over- or under-represented GO terms. In black tissue, relative to orange and white, many GO terms related to immune function were over-represented, supporting the hypothesis that black (melanin) coloration may serve a protective function. Overall, this study presents the assembly of the A. percula transcriptome, and represents a first step in an integrative investigation of the proximate and ultimate causes of conspicuous coloration of this iconic coral reef fish."
SARAH DAVIES,Resilience of Atlantic slippersnail Crepidula fornicata larvae in the face of severe coastal acidification,"Globally, average oceanic pH is dropping, and it will continue to decline into the foreseeable future. This ocean acidification (OA) will exacerbate the natural fluctuations in pH that nearshore ecosystems currently experience daily, potentially pushing marine organisms to their physiological limits. Adults of Crepidula fornicata (the Atlantic slippersnail) have proven remarkably resilient to many environmental changes, which is perhaps not surprising considering that they are common intertidally, have a geographically large native range, and have been extremely successful at invading coastal waters in many other parts of the world. However, the larvae of C. fornicata have been shown to be somewhat more vulnerable than adults to the effects of reduced pH. Research to date has focused on the physiological impacts of OA on C. fornicata larvae; few studies have explored shifts in gene expression resulting from changes in pH. In the present study, we examined the response of young (4-day old) C. fornicata larvae to two extreme OA treatments (pH 7.5 and 7.6) relative to pH 8.0, documenting both phenotypic and genome-wide gene expression responses. We found that rearing larvae at reduced pH had subtle influences on gene expression, predominantly involving downregulation of genes related to growth and metabolism, accompanied by significantly reduced shell growth rates only for larvae reared at pH 7.5. Additionally, 10-day old larvae that had been reared at the two lower pH levels were far less likely to metamorphose within 6 h when exposed to inducer. However, all larvae eventually reached similarly high levels of metamorphosis 24 h after settlement induction. Finally, there were no observed impacts of OA on larval mortality. Taken together, our results indicate that far future OA levels have observable, but not severe, impacts on C. fornicata larvae, which is consistent with the resilience of this invasive snail across rapidly changing nearshore ecosystems. We propose that future work should delve further into the physiological and transcriptomic responses of all life history stages to gain a more comprehensive understanding of how OA impacts the littoral gastropod C. fornicata."
SARAH DAVIES,"Military culture in Senegambia and the origins of the tirailleurs Sénégalais army, 1750-1910","This project traces the historical evolution of warfare and military recruitment in the Senegambia region. It investigates the conscription and recruitment of indigenous troops and their service in royal and jihādist forces, irregular armed groups, and the French colonial military. Whether through the ceɗɗo armies protecting the states of the former Jolof Empire, the sòfa soldiers who fought in jihāds in the interior, or the French-recruited tirailleurs sénégalais, engaging in regular warfare was one of few paths to personal autonomy. Men who embraced a corporate military identity within the caste systems of Senegambia gained power through complex patron-client relationships with civil and religious authorities. For those whose lives were defined by kinship networks, soldiers formed their own stable social category. A second line of inquiry identifies a subset of soldiers known as volontaires sénégalais, professional soldiers who were so integral to the success of the French colonial army in campaigns in the region that they were given compensation and rations on par with European troops, a de facto admission of their military importance. Enlisted Senegalese men became interpreters, porters, recruiters, spies, policemen, soldiers, and non-commissioned officers, playing a decisive role in combat in the territory that would become modern-day Senegal as well as other West African states and kingdoms, particularly Dahomey. Further, this study asks questions about colonial as well as indigenous power relations and caste identity, examining the ways in which access to political and military power structures affected ethnic, caste, and class relationships. It considers the caste identity of Senegalese men who fought in the various realms that make up present-day Senegal and provides a re-examination of their status as “slaves.” Moreover, it focuses on the development of military culture within these groups, the tactics employed in inter-state conflicts and between indigenous states and a burgeoning French colonial army, and the emergence of war making as a vocation. Drawing on studies of martial and organizational culture, this project reorients our understanding of patron-client relationships and provides a new lens through which to view the development of military identity among indigenous troops from Senegambia."
SARAH DAVIES,Varied effects of algal symbionts on transcription factor NF-κB in a sea anemone and a coral: possible roles in symbiosis and thermotolerance,"Many cnidarians, including the reef-building corals, undergo symbiotic mutualisms with photosynthetic dinoflagellate algae of the family Symbiodiniaceae. These partnerships are sensitive to temperature extremes, which cause symbiont loss and increased coral mortality. Previous studies have implicated host immunity and specifically immunity transcription factor NF-κB as having a role in the maintenance of the cnidarian-algal symbiosis. Here we have further investigated a possible role for NF-κB in establishment and loss of symbiosis in various strains of the anemone Exaiptasia (Aiptasia) and in the coral Pocillopora damicornis. Our results show that NF-κB expression is reduced in Aiptasia larvae and adults that host certain algae strains. Treatment of Aiptasia larvae with a known symbiosis-promoting cytokine, transforming growth factor β, also led to decreased NF-κB expression. We also show that aposymbiotic Aiptasia (with high NF-κB expression) have increased survival following infection with the pathogenic bacterium Serratia marcescens as compared to symbiotic Aiptasia (low NF-κB expression). Furthermore, a P. damicornis coral colony hosting Durusdinium spp. (formerly clade D) symbionts had higher basal NF-κB expression and decreased heat-induced bleaching as compared to two individuals hosting Cladocopium spp. (formerly clade C) symbionts. Lastly, genome-wide gene expression profiling and genomic promoter analysis identified putative NF-κB target genes that may be involved in thermal bleaching, symbiont maintenance, and/or immune protection in P. damicornis. Our results provide further support for the hypothesis that modulation of NF-κB and immunity plays a role in some, but perhaps not all, cnidarian-Symbiodiniaceae partnerships as well as in resistance to pathogens and bleaching."
SARAH DAVIES,Consensus guidelines for advancing coral holobiont genome and specimen voucher deposition,"Coral research is being ushered into the genomic era. To fully capitalize on the potential discoveries from this genomic revolution, the rapidly increasing number of high-quality genomes requires effective pairing with rigorous taxonomic characterizations of specimens and the contextualization of their ecological relevance. However, to date there is no formal framework that genomicists, taxonomists, and coral scientists can collectively use to systematically acquire and link these data. Spurred by the recently announced “Coral symbiosis sensitivity to environmental change hub” under the “Aquatic Symbiosis Genomics Project” - a collaboration between the Wellcome Sanger Institute and the Gordon and Betty Moore Foundation to generate gold-standard genome sequences for coral animal hosts and their associated Symbiodiniaceae microalgae (among the sequencing of many other symbiotic aquatic species) - we outline consensus guidelines to reconcile different types of data. The metaorganism nature of the coral holobiont provides a particular challenge in this context and is a key factor to consider for developing a framework to consolidate genomic, taxonomic, and ecological (meta)data. Ideally, genomic data should be accompanied by taxonomic references, i.e., skeletal vouchers as formal morphological references for corals and strain specimens in the case of microalgal and bacterial symbionts (cultured isolates). However, exhaustive taxonomic characterization of all coral holobiont member species is currently not feasible simply because we do not have a comprehensive understanding of all the organisms that constitute the coral holobiont. Nevertheless, guidelines on minimal, recommended, and ideal-case descriptions for the major coral holobiont constituents (coral animal, Symbiodiniaceae microalgae, and prokaryotes) will undoubtedly help in future referencing and will facilitate comparative studies. We hope that the guidelines outlined here, which we will adhere to as part of the Aquatic Symbiosis Genomics Project sub-hub focused on coral symbioses, will be useful to a broader community and their implementation will facilitate cross- and meta-data comparisons and analyses."
SARAH DAVIES,Gene expression of endangered coral (Orbicella spp.) in flower garden banks National Marine Sanctuary after Hurricane Harvey,"About 190 km south of the Texas–Louisiana border, the East and West Flower Garden Banks (FGB) have maintained > 50% coral cover with infrequent and minor incidents of disease or bleaching since monitoring began in the 1970s. However, a mortality event, affecting 5.6 ha (2.6% of the area) of the East FGB, occurred in late July 2016 and coincided with storm-generated freshwater runoff extending offshore and over the reef system. To capture the immediate effects of storm-driven freshwater runoff on coral and symbiont physiology, we leveraged the heavy rainfall associated with Hurricane Harvey in late August 2017 by sampling FGB corals at two time points: September 2017, when surface water salinity was reduced (∼34 ppt); and 1 month later when salinity had returned to typical levels (∼36 ppt in October 2017). Tissue samples (N = 47) collected midday were immediately preserved for gene expression profiling from two congeneric coral species (Orbicella faveolata and Orbicella franksi) from the East and West FGB to determine the physiological consequences of storm-derived runoff. In the coral, differences between host species and sampling time points accounted for the majority of differentially expressed genes. Gene ontology enrichment for genes differentially expressed immediately after Hurricane Harvey indicated increases in cellular oxidative stress responses. Although tissue loss was not observed on FGB reefs following Hurricane Harvey, our results suggest that poor water quality following this storm caused FGB corals to experience sub-lethal stress. We also found dramatic expression differences across sampling time points in the coral’s algal symbiont, Breviolum minutum. Some of these differentially expressed genes may be involved in the symbionts’ response to changing environments, including a group of differentially expressed post-transcriptional RNA modification genes. In this study, we cannot disentangle the effects of reduced salinity from the collection time point, so these expression patterns could also be related to seasonality. These findings highlight the urgent need for continued monitoring of these reef systems to establish a baseline for gene expression of healthy corals in the FGB system across seasons, as well as the need for integrated solutions to manage stormwater runoff in the Gulf of Mexico."
SARAH DAVIES,Exposure to global change and microplastics elicits an immune response in an endangered coral,"Global change is increasing seawater temperatures and decreasing oceanic pH, driving declines of coral reefs globally. Coral ecosystems are also impacted by local stressors, including microplastics, which are ubiquitous on reefs. While the independent effects of these global and local stressors are well-documented, their interactions remain less explored. Here, we examine the independent and combined effects of global change (ocean warming and acidification) and microplastics exposures on gene expression (GE) and microbial community composition in the endangered coral Acropora cervicornis. Nine genotypes were fragmented and maintained in one of four experimental treatments: 1) ambient conditions (ambient seawater, no microplastics; AMB); 2) microplastics treatment (ambient seawater, microplastics; MP); 3) global change conditions (warm and acidic conditions, no microplastics; OAW); and 4) multistressor treatment (warm and acidic conditions with microplastics; OAW+MP) for 22 days, after which corals were sampled for genome-wide GE profiling and ITS2 and 16S metabarcoding. Overall A. cervicornis GE responses to all treatments were subtle; however, corals in the multistressor treatment exhibited the strongest GE responses, and genes associated with innate immunity were overrepresented in this treatment. ITS2 analyses confirmed that all coral were associated with Symbiodinium ‘fitti’ and 16S analyses revealed similar microbiomes dominated by the bacterial associate Aquarickettsia, suggesting that these A. cervicornis fragments exhibited remarkably low variability in algal and bacterial community compositions. Future work should focus on functional differences across microbiomes, especially Aquarickettsia and viruses, in these responses. Overall, results suggest that when local stressors are coupled with global change, these interacting stressors present unique challenges to this endangered coral species."
SARAH DAVIES,"Genomic comparison of the temperate coral Astrangia poculata with tropical corals yields insights into winter quiescence, innate immunity, and sexual reproduction","Facultatively symbiotic corals provide important experimental models to explore the establishment, maintenance, and breakdown of the mutualism between corals and members of the algal family Symbiodiniaceae. The temperate coral Astrangia poculata is one such model as it is not only facultatively symbiotic, but also occurs across a broad temperature and latitudinal gradient. Here, we report the de novo chromosome-scale assembly and annotation of the A. poculata genome. Though widespread segmental/tandem duplications of genomic regions were detected, we did not find strong evidence of a whole genome duplication (WGD) event. Comparison of the gene arrangement between A. poculata and the tropical coral Acropora millepora revealed 56.38% of the orthologous genes were conserved in syntenic blocks despite ~415 million years of divergence. Gene families related to sperm hyperactivation and innate immunity, including lectins, were found to contain more genes in A. millepora relative to A. poculata. Sperm hyperactivation in A. millepora is expected given the extreme requirements of gamete competition during mass spawning events in tropical corals, while lectins are important in the establishment of coral-algal symbiosis. By contrast, gene families involved in sleep promotion, feeding suppression, and circadian sleep/wake cycle processes were expanded in A. poculata. These expanded gene families may play a role in A. poculata’s ability to enter a dormancy-like state (“winter quiescence”) to survive freezing temperatures at the northern edges of the species’ range."
SARAH DAVIES,Exposure duration modulates the response of Caribbean corals to global change stressors,"Global change, including rising temperatures and acidification, threatens corals globally. Although bleaching events reveal fine-scale patterns of resilience, traits enabling persistence under global change remain elusive. We conducted a 95-d controlled-laboratory experiment investigating how duration of exposure to warming (~28, 31°C), acidification (pCO2 ~ 343 [present day], ~663 [end of century], ~3109 [extreme] μatm), and their combination influences physiology of reef-building corals (Siderastrea siderea, Pseudodiploria strigosa) from two reef zones on the Belize Mesoamerican Barrier Reef System. Every 30 d, net calcification rate, host protein and carbohydrate, chlorophyll a, and symbiont density were quantified for the same coral individual to characterize acclimation potential under global change. Coral physiologies of the two species were differentially affected by stressors and exposure duration was found to modulate these responses. Siderastrea siderea exhibited resistance to end of century pCO2 and temperature stress, but calcification was negatively affected by extreme pCO2. However, S. siderea calcification rates remained positive after 95 d of extreme pCO2 conditions, suggesting acclimation. In contrast, P. strigosa was more negatively influenced by elevated temperatures, which reduced most physiological parameters. An exception was nearshore P. strigosa, which maintained calcification rates under elevated temperature, suggesting local adaptation to the warmer environment of their natal reef zone. This work highlights how tracking coral physiology across various exposure durations can capture acclimatory responses to global change stressors."
SARAH DAVIES,The eighteenth data release of the Sloan Digital Sky Surveys: targeting and first spectra from SDSS-V,"The eighteenth data release (DR18) of the Sloan Digital Sky Survey (SDSS) is the first one for SDSS-V, the fifth generation of the survey. SDSS-V comprises three primary scientific programs or “Mappers”: the Milky Way Mapper (MWM), the Black Hole Mapper (BHM), and the Local Volume Mapper. This data release contains extensive targeting information for the two multiobject spectroscopy programs (MWM and BHM), including input catalogs and selection functions for their numerous scientific objectives. We describe the production of the targeting databases and their calibration and scientifically focused components. DR18 also includes ∼25,000 new SDSS spectra and supplemental information for X-ray sources identified by eROSITA in its eFEDS field. We present updates to some of the SDSS software pipelines and preview changes anticipated for DR19. We also describe three value-added catalogs (VACs) based on SDSS-IV data that have been published since DR17, and one VAC based on the SDSS-V data in the eFEDS field."
SARAH DAVIES,Coral symbiodinium community composition across the Belize Mesoamerican barrier reef system is influenced by host species and thermal variability,
SARAH DAVIES,Starvation decreases immunity and immune regulatory factor NF-κB in the starlet sea anemone Nematostella vectensis,"Lack of proper nutrition (malnutrition) or the complete absence of all food (starvation) have important consequences on the physiology of all organisms. In many cases, nutritional status affects immunity, but, for the most part, the relationship between nutrition and immunity has been limited to studies in vertebrates and terrestrial invertebrates. Herein, we describe a positive correlation between nutrition and immunity in the sea anemone Nematostella vectensis. Gene expression profiling of adult fed and starved anemones showed downregulation of many genes involved in nutrient metabolism and cellular respiration, as well as immune-related genes, in starved animals. Starved adult anemones also had reduced protein levels and DNA-binding activity of immunity-related transcription factor NF-κB. Starved juvenile anemones had increased sensitivity to bacterial infection and also had lower NF-κB protein levels, as compared to fed controls. Weighted Gene Correlation Network Analysis (WGCNA) revealed significantly correlated gene networks that were inversely associated with starvation. Based on the WGCNA and a reporter gene assay, we identified TRAF3 as a likely NF-κB target gene in N. vectensis. Overall, these experiments demonstrate a correlation between nutrition and immunity in a basal marine metazoan, and the results have implications for the survival of marine organisms as they encounter changing environments."
SARAH DAVIES,The long-baseline neutrino experiment: exploring fundamental symmetries of the universe,"The preponderance of matter over antimatter in the early Universe, the dynamics of the supernova bursts that produced the heavy elements necessary for life and whether protons eventually decay --- these mysteries at the forefront of particle physics and astrophysics are key to understanding the early evolution of our Universe, its current state and its eventual fate. The Long-Baseline Neutrino Experiment (LBNE) represents an extensively developed plan for a world-class experiment dedicated to addressing these questions. LBNE is conceived around three central components: (1) a new, high-intensity neutrino source generated from a megawatt-class proton accelerator at Fermi National Accelerator Laboratory, (2) a near neutrino detector just downstream of the source, and (3) a massive liquid argon time-projection chamber deployed as a far detector deep underground at the Sanford Underground Research Facility. This facility, located at the site of the former Homestake Mine in Lead, South Dakota, is approximately 1,300 km from the neutrino source at Fermilab -- a distance (baseline) that delivers optimal sensitivity to neutrino charge-parity symmetry violation and mass ordering effects. This ambitious yet cost-effective design incorporates scalability and flexibility and can accommodate a variety of upgrades and contributions. With its exceptional combination of experimental configuration, technical capabilities, and potential for transformative discoveries, LBNE promises to be a vital facility for the field of particle physics worldwide, providing physicists from around the globe with opportunities to collaborate in a twenty to thirty year program of exciting science. In this document we provide a comprehensive overview of LBNE's scientific objectives, its place in the landscape of neutrino physics worldwide, the technologies it will incorporate and the capabilities it will possess."
MICHAEL CASSIDY,Distributing expertise to integrate computational thinking practices,
MICHAEL CASSIDY,"D-cycloserine augmentation of exposure-based cognitive behavior therapy for anxiety, obsessive-compulsive, and posttraumatic stress disorders: a systematic review and meta-analysis of individual participant data","Importance: Whether and under which conditions D-cycloserine (DCS) augments the effects of exposure-based cognitive behavior therapy for anxiety, obsessive-compulsive, and posttraumatic stress disorders is unclear. Objective: To clarify whether DCS is superior to placebo in augmenting the effects of cognitive behavior therapy for anxiety, obsessive-compulsive, and posttraumatic stress disorders and to evaluate whether antidepressants interact with DCS and the effect of potential moderating variables. Data Sources: PubMed, EMBASE, and PsycINFO were searched from inception to February 10, 2016. Reference lists of previous reviews and meta-analyses and reports of randomized clinical trials were also checked. Study Selection: Studies were eligible for inclusion if they were (1) double-blind randomized clinical trials of DCS as an augmentation strategy for exposure-based cognitive behavior therapy and (2) conducted in humans diagnosed as having specific phobia, social anxiety disorder, panic disorder with or without agoraphobia, obsessive-compulsive disorder, or posttraumatic stress disorder. Data Extraction and Synthesis: Raw data were obtained from the authors and quality controlled. Data were ranked to ensure a consistent metric across studies (score range, 0-100). We used a 3-level multilevel model nesting repeated measures of outcomes within participants, who were nested within studies. Results: Individual participant data were obtained for 21 of 22 eligible trials, representing 1047 of 1073 eligible participants. When controlling for antidepressant use, participants receiving DCS showed greater improvement from pretreatment to posttreatment (mean difference, -3.62; 95% CI, -0.81 to -6.43; P = .01; d = -0.25) but not from pretreatment to midtreatment (mean difference, -1.66; 95% CI, -4.92 to 1.60; P = .32; d = -0.14) or from pretreatment to follow-up (mean difference, -2.98, 95% CI, -5.99 to 0.03; P = .05; d = -0.19). Additional analyses showed that participants assigned to DCS were associated with lower symptom severity than those assigned to placebo at posttreatment and at follow-up. Antidepressants did not moderate the effects of DCS. None of the prespecified patient-level or study-level moderators was associated with outcomes. Conclusions and Relevance: D-cycloserine is associated with a small augmentation effect on exposure-based therapy. This effect is not moderated by the concurrent use of antidepressants. Further research is needed to identify patient and/or therapy characteristics associated with DCS response."
DARAE KO,Prevention of venous thromboembolism after hip and knee replacement among older adults,"BACKGROUND: Venous thromboembolism (VTE) after total hip and knee replacement is a major patient safety threat, and pharmacologic prophylaxis is generally recommended. However, clinicians disagree on the optimal prophylaxis agent. OBJECTIVE: We sought to compare effectiveness and safety of aspirin versus anticoagulants for VTE prophylaxis after hip and knee replacement. METHODS: We identified patients aged ≥ 65 years undergoing elective hip and knee replacement during 2011–2013 in 59 hospitals nationwide and included in the Institute for Health Metrics clinical database. We limited our analysis to the patients with electronic medical record of discharge prophylaxis medication. Patients were categorized into either anticoagulant or aspirin only group. VTE, major hemorrhage, cardiovascular event, and death were identified from electronic databases and validated by physician review of the source documents. We compared the 90-day risk of VTE and the composite outcome (VTE, cardiovascular events, major hemorrhage, and death) in the anticoagulant group to the aspirin group using Cox proportional hazards analysis. RESULTS: Study sample included 5648 patients with mean age 73 years. The overall number of outcome events was low. There were 35 VTE events (0.61%), 15 (0.26%) cardiovascular events, 17 major hemorrhages (0.30%), and 18 deaths (0.31%). In multivariable analysis adjusting for patient demographics and cardiovascular disease and risk factors, anticoagulation therapy was associated with decreased risk of VTE (HR 0.76, 95% CI 0.35–1.68) and the composite outcome (HR 0.80, 95% CI 0.48–1.32) although not statistically significant. CONCLUSION: Among older adults undergoing elective total hip and knee replacement in community hospitals, the 90-day risk of VTE and the composite outcome was low. We did not find any statistical difference in the risk of the outcomes between anticoagulation and aspirin. Future studies with a larger sample size and different patient population are needed to validate our results."
SHANA BURROWES,Revitalizing the infection prevention workforce with a fellowship program for underrepresented groups,"Infection preventionist (IP) positions are difficult to fill, and future workforce shortages are anticipated. The IP field has less racial and ethnic diversity than the general nursing workforce or patient population. A targeted fellowship program for underrepresented groups allowed the recruitment and training of IPs while avoiding staffing shortages."
